{"title": "Motion meets Attention: Video Motion Prompts", "authors": ["Qixiang Chen", "Lei Wang", "Piotr Koniusz", "Tom Gedeon"], "abstract": "Videos contain rich spatio-temporal information. Traditional methods for extracting motion, used in tasks such as action recognition, often rely on visual contents rather than precise motion features. This phenomenon is referred to as 'blind motion extraction' behavior, which proves inefficient in capturing motions of interest due to a lack of motion-guided cues. Recently, attention mechanisms have enhanced many computer vision tasks by effectively highlighting salient visual areas. Inspired by this, we propose using a modified Sigmoid function with learnable slope and shift parameters as an attention mechanism to activate and modulate motion signals derived from frame differencing maps. This approach generates a sequence of attention maps that enhance the processing of motion-related video content. To ensure temporally continuity and smoothness of the attention maps, we apply pair-wise temporal attention variation regularization to remove unwanted motions (e.g., noise) while preserving important ones. We then perform Hadamard product between each pair of attention maps and the original video frames to highlight the evolving motions of interest over time. These highlighted motions, termed video motion prompts, are subsequently used as inputs to the model instead of the original video frames. We formalize this process as a motion prompt layer and incorporate the regularization term into the loss function to learn better motion prompts. This layer serves as an adapter between the model and the video data, bridging the gap between traditional 'blind motion extraction' and the extraction of relevant motions of interest. Experimentally, we demonstrate that our plug-and-play motion prompt layer, despite its simplicity and lightweight nature, can be seamlessly integrated into existing architectures such as SlowFast, X3D and TimeSformer, achieving state-of-the-art performance on various benchmarks, including the large-scale FineGym and MPII Cooking 2 for fine-grained action recognition.", "sections": [{"title": "1. Introduction", "content": "Video-based research has gained popularity over the past several years due to its extensive applications in human-computer interaction, smart video surveillance, sports, and health-care Wang et al. (2020); Tang et al. (2023). Videos contain rich information: spatially, they include visual contents such as objects, human subjects, and scene layouts; temporally, they show the dynamics of how these objects and humans interact and evolve over time. Early works focused on using expert-designed, handcrafted descriptors to extract spatio-temporal information from videos Dalal et al. (2006); Scovanner et al. (2007); Kl\u00e4ser et al. (2008); Wang and Schmid (2013). However, while they were carefully designed, they could only handle simple contexts and were unable to generalize to other datasets even within the same domain. The major issue is that most descriptors do not focus on motions and heavily rely on visual contents. Compared to human vision system in extracting information, they are now considered outdated, even though some are still in use Wang and Schmid (2013).\nDeep learning has significantly advanced video-based research due to its end-to-end learnable nature Simonyan and Zisserman (2014); Tran et al. (2015); Feichtenhofer et al. (2016); Carreira and Zisserman (2018). New architectures, such as CNNs, RNNs, and transformers O\u2019shea and Nash (2015); Sherstinsky (2020); Vaswani et al. (2017), along with components like normalization layers, skip connections, and dropout Ba et al. (2016); He et al. (2016); Srivastava et al. (2014), and strategies like large-scale pretraining and transfer learning Carreira and Zisserman (2018); Zhuang et al. (2020), have greatly supported the evolution of modern video processing techniques. The attention mechanism has recently joined convolutional layers, MLPs, and RNNs as a fundamental building block Vaswani et al. (2017). Initially used within transformers in natural language processing, the attention mechanism is now effectively applied to image and video processing tasks Dosovitskiy et al. (2021); Arnab et al. (2021). It helps models focus on the most relevant parts of an image, relevant frames, spatial regions within frames, or significant scenes, thereby understanding both the visual contents and temporal dynamics. However, there are several challenges with attention mechanisms in video processing Guo et al. (2022); Brauwers and Frasincar (2021); Niu et al. (2021): (i) they are computationally intensive due to the calculation of spatio-temporal attention weights, (ii) scalability issues when handling videos of varying lengths, and (iii) capturing temporal dependencies is challenging, as the model must focus not only on spatial features within frames but also on the temporal relationships between frames. Moreover, while attentions provide some level of interpretability by highlighting important regions or frames, interpreting why certain regions or frames receive higher attention can be difficult in complex video tasks. Additionally, attention mechanisms must be robust to variations in video quality, lighting, occlusions, and other environmental factors. Despite large-scale datasets allowing attention-driven models to capture a wide range of spatio-temporal patterns, their generalization to unseen video data remains challenging."}, {"title": "2. Approach", "content": "First, we present our notation. An overview of our motion prompt layer is provided in Fig. 2. In Appendix B, we provide preliminary information on activation functions, a mathematical view of attention mechanisms, and the power normalization family.\nNotation. Let $I_T$ stand for the index set $\\{1,2,\\ldots,T\\}$. Scalars are in regular fonts; vectors are denoted by lowercase boldface letters, e.g., $\\mathbf{x}$; matrices by uppercase boldface, e.g., $\\mathbf{X}$; tensors by calligraphic letters, e.g., $\\mathcal{X}$. Let $\\mathcal{X} \\in \\mathbb{R}^{d_1\\times d_2 \\times d_3}$ denote a third-order tensor, using the Matlab convention, we refer to its t-th slice as $\\mathbf{X}_{:,:,t}$, which is a $d_1 \\times d_2$ matrix."}, {"title": "2.1. Learnable Power Normalization", "content": "Frame differencing maps. For a $T$-frame video $\\mathbf{X} = [\\mathbf{F}_1, \\mathbf{F}_2,\\ldots,\\mathbf{F}_T] \\in \\mathbb{R}^{H \\times W \\times 3 \\times T}$ where $\\mathbf{F}_t \\in \\mathbb{R}^{H \\times W \\times 3}$ ($t \\in I_T$), $H$ and $W$ denote the frame height and width, respectively, we first convert it into a grayscale video sequence $\\mathbf{X}' = [\\mathbf{F}'_1, \\mathbf{F}'_2, \\ldots, \\mathbf{F}'_T] \\in \\mathbb{R}^{H \\times W \\times T}$. After normalizing the pixel values between 0 and 1, we compute the frame differencing maps between consecutive frames, resulting in $\\mathbf{D} = [\\mathbf{D}_1, \\mathbf{D}_2, \\ldots, \\mathbf{D}_{T-1}] \\in \\mathbb{R}^{H \\times W \\times (T-1)}$, where $\\mathbf{D}_t = \\mathbf{F}'_{t+1} - \\mathbf{F}'_{t}$ ($t \\in I_{T-1}$). Note that the pixel values in $\\mathbf{D}_t$ can be either positive or negative. Positive values indicate areas where the pixel intensity has increased from frame $t$ to frame $t+1$, while negative values indicate areas where the pixel intensity has decreased. Note that the pixel values in frame differencing maps are in the range of [-1,1].\nThe frame differencing maps $\\mathbf{D}$, record a sequence of motions between consecutive video frames, capturing both foreground and background motions, as well as noisy patterns. Depending on the task, we aim to enhance the motions of interest while suppressing the rest. For example, in human action recognition, we want to amplify the motions associated with human actions and reduce irrelevant motions. Below, we introduce our learnable Sigmoid activation function as a Power Normalization (PN) function for motion modulation. Fig. 3"}, {"title": "Learnable slope and shift parameters", "content": "We opt for a modified Sigmoid function with learnable slope $a$ and shift $b$ as a PN function (see Fig. 3 (a)) on frame differencing maps. For a given frame differencing map $\\mathbf{D}$, we define:\n$$f(\\mathbf{D}) = \\frac{1}{1+ e^{-a(\\mathbf{D}-b)}}$$\nThis element-wise function maps the pixel values of frame differencing maps from [-1,1] to [0,1]. For simplicity, we set $a > 0$ so that $f(\\cdot)$ is a monotone increasing function. The parameter $a$ controls the slope of the Sigmoid function\u00b9, and it influences how sharply the function transitions from its minimum to its maximum value (see Fig. 3(b)). The parameter $b$ acts as the threshold or the point of inflection of the Sigmoid function\u00b2, and it determines the position of the Sigmoid curve along the $\\mathbf{D}$ axis. We allow $b$ to shift either left or right, hence it can be positive or negative (see Fig. 3(c)). However, directly learning the parameters $a$ and $b$ presents challenges, such as initialization sensitivity and uncertainty in the parameter search space. To address these issues, we design the following two mapping functions to learn $m$ and $n$ instead:\n$$a(m) = \\frac{\\alpha}{\\beta|tanh(m)| + \\epsilon}$$\\\n$$b(n) = \\gamma tanh(n)$$\nHere, $tanh(\\cdot)$ is the hyperbolic tangent function, which maps any given $m$ and $n$ to values between -1 and 1. This alignment with the pixel value range in frame differencing maps"}, {"title": "Upper and lower bound", "content": "Consider the term inside the exponential: $A = a(m)(\\mathbf{D} \u2013 b(n))$, we first examine the derivative of $A$ with respect to $\\mathbf{D}$: $\\frac{\\partial A}{\\partial \\mathbf{D}} = a(m) = \\frac{\\alpha}{\\beta|tanh(m)|+\\epsilon}$. Since this derivative is always positive, $A$ is monotonically increasing with respect to $\\mathbf{D}$. The scaling factor $a(m)$ ranges from $\\frac{\\alpha}{\\beta+\\epsilon}$ to $\\frac{\\alpha}{\\epsilon}$ and $\\mathbf{D} \u2013 b(n)$ ranges from $-1 - \\gamma$ to $1+ \\gamma$; hence, the lower and upper bounds of $A$ are $-\\frac{\\alpha(1+\\gamma)}{\\beta+\\epsilon}$ and $\\frac{\\alpha(1+\\gamma)}{\\epsilon}$, respectively.\nThe Sigmoid function $\\sigma(A) = \\frac{1}{1+e^{-A}}$ approaches 0 as value in $A$ becomes large and negative (e.g., $\\sigma(-5(1+0.6)) \\approx 0.0$), and 1 as value in $A$ becomes large and positive (e.g., $\\sigma(5(1+0.6)) \\approx 1.0$). Consequently, for values in $\\mathbf{D}$ in the interval [-1, 1], the function $f(\\mathbf{D})$ is always bounded within [0, 1]."}, {"title": "Well-behaved power normalization", "content": "Eq. (3) is continuous and smooth for all real values of $\\mathbf{D}$, $m$, and $n$. The scaling factor $a(m)$ is always positive, and the exponential term is well-defined, producing a valid, finite value for all input values. The Sigmoid function $\\sigma(A) = \\frac{1}{1+e^{-A}}$ maps any real number to the interval [0, 1], ensuring that $f(\\mathbf{D})$ produces values within this range and thereby maintaining proper normalization.\nTherefore, Eq. (3) is a well-behaved PN function given: (i) It is continuous and smooth. (ii) The output values are properly normalized within the range [0,1]. (iii) The Sigmoid function ensures that the function maps real numbers to a bounded interval, maintaining normalization."}, {"title": "2.2. Motion Prompt Layer: An Adapter", "content": "Video motion prompts. Eq. (3) modulates the motions in frame differencing maps via the learnable $m$ and $n$, resulting in a sequence of normalized frame differencing maps with pixel values ranging from 0 to 1. This element-wise PN process can also be viewed as activating the motions of interest, guided by the generic loss function, e.g., cross-entropy, hence we call this the attention map. The PN process produces a sequence of attention maps: $\\mathbf{A}' = [f(\\mathbf{D}_1), f(\\mathbf{D}_2),\\ldots, f(\\mathbf{D}_{T-1})] \\in \\mathbb{R}^{H \\times W \\times (T-1)}$, spatially highlighting regions where motions are of interest (e.g., 1) and dampening motions that are not of interest (e.g., 0); temporally showing the evolution of attention maps over time. We then duplicate each attention map three times, resulting in $\\mathbf{A}'^{(3)} = [f^{(3)}(\\mathbf{D}_1), f^{(3)}(\\mathbf{D}_2),\\cdots, f^{(3)}(\\mathbf{D}_{T-1})] \\in \\mathbb{R}^{H \\times W \\times 3 \\times (T-1)}$, where $f^{(3)}(\\mathbf{D}_t) \\in \\mathbb{R}^{H \\times W \\times 3}$ and $t\\in I_{T-1}$. We perform a channel-wise Hadamard product between each duplicated attention map $f^{(3)}(\\mathbf{D}_t)$ and the original video frame $\\mathbf{F}_{(t+1)}$ so that attention attends to each channel of the original video frame, resulting in a sequence of highlighted video frames:\n$$\\mathbf{Z} = \\mathbf{A}'^{(3)} \\odot \\mathbf{X}_{:,:,2:}$$\n$$= [f^{(3)}(\\mathbf{D}_1), f^{(3)}(\\mathbf{D}_2), \\ldots, f^{(3)}(\\mathbf{D}_{T-1})] \\odot [\\mathbf{F}_2,\\ldots, \\mathbf{F}_T]$$\n$$= [f^{(3)}(\\mathbf{D}_1) \\odot \\mathbf{F}_2, f^{(3)}(\\mathbf{D}_2) \\odot \\mathbf{F}_3, \\ldots, f^{(3)}(\\mathbf{D}_{T-1}) \\odot \\mathbf{F}_T],$$\nwhere $\\odot$ denotes the Hadamard (element-wise) product. $\\mathbf{Z} \\in \\mathbb{R}^{H \\times W \\times 3 \\times (T-1)}$ denotes the newly generated video, referred to as Video Motion Prompts (VMPs), which are motion-dependent, attention-driven, and provide rich motion cues. $f^{(3)}(\\mathbf{D}_t) \\odot \\mathbf{F}_{(t+1)} \\in \\mathbb{R}^{H \\times W \\times 3}$ shows the motion prompt for the t-th frame. Below, we show that our video motion prompt generation process is, in fact, connected to the attention mechanism."}, {"title": "Connecting to attention mechanism", "content": "We rewrite $f^{(3)}(\\mathbf{D}_t)$ using the Sigmoid function $\\sigma^{(3)}(\\cdot)$, replace the frame differencing map $\\mathbf{D}_t$ with the grayscale conversion function $h(\\cdot)$ as $h(\\mathbf{F}_{t+1}) - h(\\mathbf{F}_{t})$, and rewrite Eq (4) in the form of per-frame motion prompt (we use Eq. (2) for the scaling factor and shift parameter and omit $m$ and $n$ for simplicity):\n$$\\mathbf{Z}_t = f^{(3)}(\\mathbf{D}_t) \\odot \\mathbf{F}_{(t+1)}$$\n$$= \\sigma^{(3)}(a[h(\\mathbf{F}_{t+1}) \u2013 h(\\mathbf{F}_{t}) \u2013 b]) \\odot \\mathbf{F}_{(t+1)}$$\nwhere $A = a[h(\\mathbf{F}_{t+1}) \u2013 h(\\mathbf{F}_{t}) \u2013 b]$ can be viewed as an attention matrix with the shifting parameter $b$ modulating the pixel intensity changes between each pair of grayscaled consecutive frames. The Sigmoid (denoted as $\\sigma^{(3)}$) outputs are similar to the Softmax function outputs in the standard attention mechanism (as in Eq. (9) of Appendix B), which transforms raw attention scores to highlight the most important parts of the h-transformed frame differencing maps. Since we operate on each pixel in frame differencing maps to highlight all relevant motion pixels, there is no need to satisfy the criterion that attention weights sum up to 1, as required by the Softmax function. $\\mathbf{F}_{(t+1)}$ is analogous to the value matrix $\\mathbf{V}$ in Eq. (10) of Appendix B. This shows that our motion prompt generation process is analogous to the standard attention mechanism.\nUnlike the traditional attention mechanism, our attention mechanism offers (i) lightweight computation with only two learnable parameters, (ii) interpretability, as the learnable scaling factor and shift parameter have well-explainable functionalities in the motion modulation process, and (iii) generalizability, as $A$ is motion-dependent, relying on frame differencing maps rather than being dataset-dependent."}, {"title": "Temporal attention variation regularization", "content": "To ensure temporally the smoothness and continuity of generated attention maps, we introduce temporal attention variation regularization on pair-wise attention maps:\n$$V = \\frac{1}{T-2} \\sum_{t=1}^{T-2}||f(\\mathbf{D}_{t+1}) \u2013 f(\\mathbf{D}_{t})||_F^2,$$\nwhere $||\\cdot||_F$ denotes the Frobenius norm. This regularization ensures that each pair of consecutive attention maps has fewer variations in pixel values, promoting the generated attention maps to be temporally smooth while maintaining spatially the regions of motion that are of interest.\nWe design the video motion prompt generation process as a single layer with two learnable parameters that amplify relevant motions while attenuating irrelevant movements. Eq. (6) is incorporated into the original loss function $\\mathcal{L}_{ori}$, such as cross-entropy loss for action recognition, used in models like SlowFast and TimeSformer backbones:\n$$\\mathcal{L} = \\mathcal{L}_{ori} + \\lambda V,$$\nwhere $\\lambda$ is a penalty parameter that controls the strength of this regularization, balancing the trade-off between temporal smoothness and the maintenance of spatially significant motion regions. We simply insert our motion prompt layer between the video input and the model architecture, using Eq. (7) as the loss function to learn the VMPs as new inputs. The entire model can be learned in an end-to-end manner or fine-tuned on specific layers, including the learning of our motion prompt layer."}, {"title": "3. Experiment", "content": null}, {"title": "3.1. Setup", "content": "Dataset. For generic action recognition, we choose the popular and challenging HMDB-51 Kuehne et al. (2011), which features significant camera and background motion. For fine-grained action recognition, we select the large-scale MPII Cooking 2 (2,881,616 frames, resolution 1624\u00d71224) Rohrbach et al. (2015) and FineGym Shao et al. (2020) (Gym99 v1.1: 26,320/8,521 for train/val set, respectively) datasets. FineGym focuses on human actions performed in a gym environment, capturing a wide range of activities with fine granularity (99 classes). In contrast, the MPII Cooking 2 dataset specializes in cooking-related actions (67 classes). We adhere to their standard evaluation protocols in our experiments.\nImplementation. The motion prompt layer is initialized with a normal distribution, hav-ing a mean of 1\u00d710-5 and a standard deviation of 1. The penalty parameter for temporal attention variation regularization is selected from the range [1e-4, 10]. We use SlowFast Fe-ichtenhofer et al. (2019), X3D Feichtenhofer (2020), and TimeSformer Bertasius et al. (2021) as backbones. All experiments use SGD as the optimizer (e.g., with momentum 0.9). The learning rate (e.g., 0.005), weight decay (e.g., 0.0001), decay strategy (e.g., step decay, or cosine decay with a warm-up), and the number of sampled video frames per video follow those specified in the original papers. Note that our layer requires an additional frame to ensure that the resulting motion prompts have exactly the same length as the original input video frames (see Eq. (4)). We fine-tune models pretrained on Kinetics-400 Kay et al. (2017) as our baseline. All experiments are conducted using the Tesla V100 GPU. Below, we present our evaluations and analysis."}, {"title": "3.2. Evaluation", "content": "Analysis of learnable slope and shift. We present the learning process of $a$ and $b$ versus the number of fine-tuning epochs in Fig. 4 with varying regularization penalty parameter \u03bb. We use FineGym with TimeSformer pretrained on Kinetics-400 for the fine-tuning and learning of our motion prompts. As shown in the figure, choosing bigger A results in both slope and shift parameters quickly approaching their lower bounds. Using smaller Aor set \u03bb to 0 results in the noisy and fluctuated learning process for $b$ (red and purple lines in Fig. 4 (b)), and $a$ tends to be slightly bigger (green line in Fig. 4 (a)). Overall, on FineGym, $a$ and $b$ tend to be small, that is to consider some negative motions in the frame differencing maps while ensuring a smooth transition rather than an increase in steepness. This is reasonable as FineGym is captured by moving cameras, hence all positive motions should be considered with varying degrees of attention. The optimal value of A is 2.5, and the learned values are $a$ = 11.04 and $b$ = -0.59, resulting in a performance gain of 0.8% compared to the baseline.\nLearned a and b across various datasets and backbones. We visualize the pairs of $a$ and $b$ from top performers per dataset using the TimeSformer backbone in Fig. 4 (c). We notice that MPII Cooking 2 tends to have higher $a$ and $b$ values. This is attributed to the dataset being captured by a static camera, making it easier to extract motions related to cooking activities, e.g., with a steep slope. On FineGym, both slope and shift tend to be smaller compared to HMDB-51 split 1 (HMDB-s1). This is because FineGym focuses specifically on gymnastic activities, where significant camera motions occur due to player localization and tracking. In Fig. 4 (d), we observe that the learned $a$ and $b$ vary significantly across different backbones. Moreover, using motion prompts on the SlowFast fast-only (SlowFast-f) stream results in a steeper slope compared to the SlowFast slow-only (SlowFast-s) stream. This is because the fast-only stream samples more frames, offering richer and smoother temporal information that facilitates easier access to motions of interest.\nVisualizations of attention maps and motion prompts. We visualize frame difference maps, learned attention maps, and motion prompts in Fig. 5. We also include the original video frames for comparison. As shown in the figure, we observe discrepancies between consecutive frames, attributable to the frame sampling strategy commonly used in video processing tasks. The frame differencing maps show noticeable noise; blue indicates nega-tive motions while orange shows positive motions, especially in videos captured by moving cameras like HMDB-51 and FineGym. Conversely, in static camera scenes such as MPII Cooking 2, the background appears clean. Consequently, in the generated attention maps, the background is depicted in lighter orange, suggesting lower attention scores and less im-portance relative to the action. In contrast, in scenarios with moving cameras, background motions appear more significant, reflected by darker red shades in the attention maps, indicating higher attention scores. Interestingly, the generated motion prompts reveal rich action information, compared to original frames."}, {"title": "With and without temporal attention variation regularization", "content": "Fig. 6 shows a comparison of with and without the use of temporal attention variation regularization. We observe that without the regularization term, the generated attention maps are quite noisy, especially in the background. However, with regularization, the attention maps contain much less noise. This demonstrates that our regularization term contributes to generating smooth and clean attention maps, thereby improving the quality of motion prompts. We also observe that the attention maps exhibit several interesting patterns: (i) they highlight the motion regions in the current frame, (ii) they capture potential movements from the previous frame, and (iii) they attend to background scenes affected by camera motions. These observations indicate that our attention maps, guided by only two learnable parameters, effectively highlight visual contents of interest while capturing dynamics over short periods of time. More visualizations and discussions for MPII Cooking 2 (Fig. 12, 13 and 14) and HMDB-51 (Fig. 15 and 16) can be found in Appendix \u0415."}, {"title": "Generic action recognition", "content": "Our evaluations on HMDB-51 are summarized in Table 2 (left). Using TimeSformer as the backbone and integrating our motion prompt layer, we achieve accuracy improvements of 1.5%, 1.2%, and 0.7% for split 1, 2 and 3, respectively. On average, this results in a performance gain of 1.1%. Our VMPs show consistent improve-ments across various recent action recognition backbones, including SlowFast and X3D."}, {"title": "Fine-grained action recognition", "content": "In Table 2 (right), we report performance gains on MPII Cooking 2, with Top-1 mean average precision improvements of 3.9% for SlowFast (both streams with VMPs), 0.7% for X3D, and 6.0% for TimeSformer. For FineGym, TimeSformer shows a 0.8% increase in Top-1 accuracy. The TimeSformer backbone consis-tently outperforms the X3D backbone on both datasets, indicating that model performance and the benefits of VMPs depend on the backbone's ability to handle motion."}, {"title": "4. Conclusion", "content": "We introduce video motion prompts to enhance action recognition. We use a modified Sig-moid activation function with learnable slope and shift as a power normalization function on frame differencing maps to activate motions as attention maps. Additionally, we introduce the temporal attention variation regularization term to generate more accurate and smooth motion prompts. We formalize the entire process as a single motion prompt layer acting as an adapter, resulting in improved performance across various benchmarks and backbones."}, {"title": "Appendix A. Related Work", "content": "Below, we review closely related work on motion extraction, attention mechanisms, prompts, and adapter layers for video processing. We also highlight the significant differences between our work and these studies.\nMotion. Optical flow computed between consecutive video frames is a widely used secondary input for video processing Simonyan and Zisserman (2014); Carreira and Zisserman (2018); Wang and Koniusz (2024), complementing the use of conventional videos Wang et al. (2018, 2021). Dynamic images, which record spatio-temporal information in a single frame, were introduced in Bilen et al. (2016). A channel sampling method that combines the R, G, or B channels of consecutive frames into a single frame for better motion capture was proposed in Kim et al. (2022). Recently, Taylor videos have been introduced to capture dominant motions in videos for action recognition Wang et al. (2024).\nUnlike these approaches, our motion prompts are (i) lightweight, with only two extra learnable parameters, (ii) dependent on motions guided by frame differencing maps, and (iii) driven by attention mechanisms that highlight the spatio-temporal motion regions over time.\nAttention. To improve feature representations, attention mechanisms capture the rela-tionships between tokens. Attention mechanisms have been efficiently incorporated into transformers, including self-attention Vaswani et al. (2017); Dosovitskiy et al. (2021) and cross-attention Lin et al. (2021); Hashiguchi and Tamaki (2022); Chen et al. (2021); Wei et al. (2020); Wang and Koniusz (2023), among others.\nOur attention mechanism, however, differs from traditional approaches. It is lightweight and does not require the learning of attention matrices. We use a sequence of frame differ-encing maps to record video dynamics, along with a newly introduced regularization term, to learn spatio-temporally smooth attention maps. Consequently, our attention mechanism is motion-dependent rather than dataset-dependent. Furthermore, compared to existing works, we use an activation function as power normalization to modulate motions, with learnable slope and shift parameters for controlling motion strengths and thresholding. This makes our attention mechanism more transparent and interpretable.\nPrompt. Prompt engineering has gained significant interest with advancements in image and video processing. Representative works include the use of prompt templates Radford et al. (2021), textual prompts for video content description Hu et al. (2022), incorporating video features into language models as prompts Yang et al. (2022), and learnable continuous prompt vectors as virtual tokens Ju et al. (2022). Visual prompts include methods such as visual prompt tuning Jia et al. (2022), fine-grained visual prompting Yang et al. (2024), among others.\nTo the best of our knowledge, none of these works consider using a sequence of refined motions as motion prompts to enhance video processing tasks. Our motion prompts are defined as a sequence of video frames with highlighted spatio-temporally smooth motion regions per frame. Our motion prompts are learnable and form a plug-and-play motion prompt layer. They are optimized using the original loss function.\nAdapter. There are several layers and mechanisms in neural networks that can be consid-ered as adapters between the data input and the model itself. Embedding layers, commonly used in NLP and recommendation systems, convert categorical data or tokens into dense"}, {"title": "Appendix B. Preliminary", "content": "Below, we refer to the preliminary works used in the paper.\nActivation function. Activation functions such as Sigmoid, Tanh, ReLU, and Softmax are among the most popular and commonly used in neural networks. These functions can be either linear or non-linear, depending on their formulation and the context in which they are applied. A review of activation functions in deep learning can be found in Nwankpa et al. (2018); Dubey et al. (2022). A logistic function is a common S-shaped curve (sigmoid curve) with the equation:\n$$f(x) = \\frac{L}{1+ e^{-a(x-b)}}$$\nwhere $L$ is the carrying capacity, $a$ is the logistic growth rate (the steepness of the curve), and $b$ is the point at which the output transitions from below $L/2$ to above $L/2$. For values of $x$ much less than $b$, $f(x)$ is close to 0 (or the lower bound), and for values of $x$ much greater than $b$, $f(x)$ is close to $L$ (or the upper bound). This characteristic makes $b$ act as a kind of threshold in the Sigmoid function. The standard logistic function is when $L = 1, a = 1, and b = 0$.\nBoth the Sigmoid and Softmax functions introduce non-linearity; however, the Softmax function additionally provides a means to interpret the neural network's output as proba-bilities. In attention mechanisms, the Softmax function is applied to a set of scores (often called attention scores or logits) to produce a probability distribution over the elements in the sequence. This ensures that the importance (attention) weights sum to 1.\nA mathematical view of attention. The Vision Transformer (ViT) applies the attention mechanism to image processing by first dividing an image into n patches and then treating these patches as input tokens $X \\in \\mathbb{R}^{n \\times d}$. The self-attention mechanism involves three main components: query $Q$, key $K$ and value $V$ matrices, which are computed as linear projections of the input matrix $X$. The self-attention scores are then computed as the scaled dot-product of the query and key matrices, and the resulting attention matrix is"}, {"title": "Appendix C. Boundedness and Differentiability", "content": "Below we proof the boundedness and differentiability for both $a(m)$ and $b(n)$.\nBoundedness. The hyperbolic tangent function $tanh(m)$ has a range of (-1", "m": "a'(m) = \\frac{\u03b1 g'(m)"}, {"g(m)": 2}, "where $g(m) = \u03b2|tanh(m)|+\u03f5$. The derivative of the hyperbolic tangent function is $tanh'(m) = 1 \u2013 tanh^2(m)$. The derivative of $|tanh(m)|$ is generally given by $\\frac{d|tanh(m)|}{dm} = sgn(tanh(m)) \\cdot tanh'(m) = sgn(tanh(m)) \\cdot (1 \u2013 tanh^2(m))$. Therefore, $g'(m)$ is: $g'(m) = \u03b2 \\cdot sgn(tanh(m)) \\cdot (1 \u2013 tanh^2(m))$. Thus, the derivative of a(m) is: $a'(m) =\\frac{-\u03b1 \u03b2 sgn(tanh(m)) (1 - tanh^2(m))}{[\u03b2|tanh(m)| + \u03f5"], "n": "b'(n) = \u03b3 \\cdot tanh'(n) = \u03b3 \\cdot (1 \u2013 tanh^2(n))$. Since tanh(n) is differentiable", "by": "b'(n) = \u03b3 \\cdot"}