{"title": "LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation", "authors": ["Ziyao Zhang", "Yanlin Wang", "Chong Wang", "Jiachi Chen", "Zibin Zheng"], "abstract": "Abstract-Code generation aims to automatically generate code from input requirements, significantly enhancing development efficiency. Recent large language models (LLMs) based approaches have shown promising results and revolutionized code generation task. Despite the promising performance, LLMs often generate contents with hallucinations, especially for the code generation scenario requiring the handling of complex contextual dependen- cies in practical development process. Although previous study has analyzed hallucinations in LLM-powered code generation, the study is limited to standalone function generation. In this paper, we conduct an empirical study to study the phenomena, mechanism, and mitigation of LLM hallucinations within more practical and complex development contexts in repository-level generation scenario. First, we manually examine the code generation results from six mainstream LLMs to establish a hallucination taxonomy of LLM-generated code. Next, we elaborate on the phenomenon of hallucinations, analyze their distribution across different models. We then analyze causes of hallucinations and identify four potential factors contributing to hallucinations. Finally, we propose an RAG-based mitigation method, which demonstrates consistent effectiveness in all studied LLMs. The replication package including code, data, and experimental results is available at https: //github.com/DeepSoftwareAnalytics/LLMCodingHallucination.", "sections": [{"title": "I. INTRODUCTION", "content": "Code generation is an automation technology aimed at effi- ciently producing code from specifications described in natural language. This process significantly reduces the manual coding workload for developers [6], [9], [10], allowing them to focus more on solving advanced technical challenges and engaging in innovative tasks. Recent developments have introduced a variety of large language models (LLMs) [14]\u2013[16], [18]-[25], [37], [44], [51] built upon the Transformer architecture [1]. These models, trained on extensive code corpora, can automatically generate code from natural language inputs and have shown high efficacy in code generation. For example, GPT-4 has achieved state-of-the-art results on evaluation benchmarks such as HumanEval [15] and MBPP [17], demonstrating high functional correctness, particularly in generating standalone functions based on detailed specifications.\nHowever, in practical development scenarios, the require- ments for code generation are more complex than sim- ply generating standalone functions from detailed specifica- tions [59]. To address this complexity, new benchmarks, such as CoderEval [59], ClassEval [60], and EvoCodeBench [61], have been proposed to better reflect real-world repository-level development scenarios. Evaluations based on these benchmarks have revealed that LLMs face challenges in generating non- standalone functions with contextual dependencies, such as calls to user-defined functions and project-defined data protocol. While these benchmarks provide valuable insights into the effectiveness of LLMs in practical code generation, they pri- marily focus on functional correctness as measured by test case pass rates and lack a thorough analysis of underlying failure causes. To bridge this gap, this work aims to systematically investigate issues in practical LLM-based code generation from the perspective of hallucinations.\nHallucination is a significant issue for state-of-the-art gener- ative LLMs [7]. For general natural language tasks, LLM hallucinations have been explored to a certain extent [5], [7], [8], [69] and are typically categorized into three types: Input-Conflicting Hallucination, Fact-Conflicting Hallucination, and Context-Conflicting Hallucination [69]. In the domain of code generation, Liu et al. [75] conducted a study to analyze hallucinations in LLM-powered code generation and established a taxonomy that aligns with these three categories. While Liu et al.'s study provided insightful findings, it is based on benchmarks (i.e., HumanEval [15] and DS-1000 [76]) for stan- dalone function/script generation instead. Our work, however, focuses on hallucinations within more practical and complex development contexts in repository-level generation scenarios. Additionally, their study primarily categorized hallucinations from a problem-presentation perspective to uncover fine-grained code-semantic issues, resulting in categories such as Dead Code and Repetition. In this work, we investigate hallucinations from a holistic perspective in terms of phenomena, mechanism, and mitigation. We believe that our study can complement the findings by Liu et al., providing a broader understanding of hallucinations in LLM-based code generation.\nIn this work, we conduct an empirical study to uncover the status quo and root causes of hallucinations in LLM-based code generation within real-world projects. The study aims at answering the following research questions (RQs):\n\u2022 RQ1 (Hallucination Taxonomy): What are the specific manifestations of hallucinations in practical code generation, and how are they distributed?\n\u2022 RQ2 (LLM Comparison): How do different LLMS compare in terms of hallucination occurrences and patterns?\n\u2022 RQ3 (Root Causes): What are the root causes of halluci- nations in practical LLM-based code generation?"}, {"title": "II. BACKGROUND & RELATED WORK", "content": "For developers, a realistic scenario is to use a code repository to write code, which is very common in practice [66]. For example, due to security and functionality considerations, companies often only build code warehouses internally. The code repository provides many private APIs that are not seen by the language model and are not public on any code hosting platform. Therefore, it is worth exploring whether pre-trained language models can adapt to real development needs and generate correct and efficient code. In real-world development scenarios, the development of a function not only relies on the text description and function signature of the function, but also requires calling a custom API in the code repository. Such non-independent functions are commonly found in real- world generation scenarios. By analyzing the 100 most popular projects written in Java and Python on GitHub [59], previous work found that dependent functions account for more than 70% of the functions in open source projects. In order to better simulate real development scenarios and to check the correctness of LLMs, CoderEval [59], ClassEval [60], and EvoCodeBench [61] collected code snippets and text descriptions from real code repositories and used test cases to check the correctness of the code repositories in their corresponding environments.However, the performance of the model on these benchmarks is extremely poor. LLMs cannot generate correct code based on the problem description, and the model prefers to generate independent code segments rather than using existing functions in the current development scenario.\nIn the field of natural language processing (NLP), hallu- cination refers specifically to situations where the content produced by a language model in the process of generating text is inconsistent with the given input or expected output environment, lacks meaning, or violates the facts [68]. This kind of phenomenon is particularly prominent in text generation models, especially in tasks such as text completion, summary generation, and machine translation. The output of the model must maintain a high degree of consistency and authenticity to ensure its practicality and reliability. Hallucination phenomena can be divided into the following categories according to their nature [69]: (1) Input-Conflicting Hallucinations: When the text generated by the model deviates from the original input source, input-conflicting hallucinations will occur. This illusion may result from the model's incorrect parsing or inaccurate internal representation of the input information, causing the output content to deviate from the intent and context of the source input. (2) Context-Conflicting Hallucinations: This type of hallucination occurs when the text generated by the model is contradictory or inconsistent with its previously generated content. Contextual conflict hallucinations reflect the model's challenges in maintaining textual coherence and consistency, which may be due to the model's insufficient processing of"}, {"title": "III. EVALUATION SETUP", "content": "To better simulate practical development scenarios, we use a set of coding tasks from real-world Python repositories based on the CoderEval benchmark [59]. CoderEval comprises 230 Python code generation tasks, extracted from a diverse set of Python repositories. Each task consists of a natural language description, a ground-truth code snippet, and a set of test cases, along with the project environment context associated with the task.\nWe utilize several mainstream LLMs to perform code generation for the studied programming tasks. The LLMs being used cover both open-source and closed-source models and span various parameter sizes, listed as follows:\n\u2022 ChatGPT [55]: ChatGPT is a versatile text generation model for multilingualism with powerful code generation capabilities, we use the GPT-3.5-Turbo in our experiments.\n\u2022 CodeGen [53]: CodeGen is a family of auto-regressive lan- guage models for program synthesis with several different versions. To better accomplish the generation task, we use the CodeGen-350M-Mono model.\n\u2022 PanGu-a [44]: PanGu-a can perform code generation tasks in multiple languages. We use the PanGu-a-2.6B model.\n\u2022 DeepSeekCoder [51]: DeepSeekCoder performs well in open source models across multiple programming languages and various benchmarks. We use the DeepSeekCoder-6.7B base model.\n\u2022 CodeLlama [14]: CodeLlama is a set of pre-trained and fine-tuned generative text models ranging in size from 7 to 34 billion parameters. We use the CodeLlama-7b-Python-hf model.\n\u2022 StarCoder2 [18]: StarCoder2 is a family of open code- oriented models for large languages, providing three scales of models, we use the StarCoder2-7B model.\nFor each task, we use the LLMs to generate 10 code snippets by employing the nuclear sampling strategy and setting temperature to 0.6, following the same setting as CoderEval.\nIn order to analyze the hallucination types in the LLM- generated code, we manually perform open coding [65] on the generated code to obtain the hallucination taxonomy.\n(1) Initial Open Coding. Firstly, in the initial open-coding stage, we select 10% of the 230 coding tasks in CoderEval Python dataset for preliminary analysis. We randomly collect 23 generative tasks from CoderEval, we employ CodeGen, Pangu- a, ChatGPT, DeepSeekCoder, CodeLlama, and StarCoder2, with each model generating ten code snippets for each code generation task, culminating in a total of 1,380 code snippets to be analysed for hallucination taxonomy framework. For each code snippet, we test it in the actual development environment corresponding to the task to determine its correctness. On this basis, the two authors will compare the differences between the ground-truth and LLMs generated code snippets and discuss and record possible hallucination phenomena.\n(2) Preliminary Taxonomy Construction. Secondly, we document possible hallucinations in the generated code and the location of the hallucination content. Several different hallucinations may occur within a single code snippet. All annotators are required to discuss the codes and define the code's hallucinatory taxonomy. In this process, we classify similar hallucination to create a preliminary taxonomy that illustrates the various hallucination types and their meanings in the code generated by LLMs.\n(3) Full Taxonomy Construction. Finally, after obtaining the categorisation criteria, the remaining code snippets will be independently annotated by three newly invited volunteers with extensive Python programming experience, two with more than ten years of experience and one with four years of programming experience. If new types of hallucinations arise that are not covered by the current taxonomy, annotators are required to write descriptions of the hallucinations to allow further discussion to establish new types and enhance the taxonomy."}, {"title": "IV. EVALUATION RESULTS", "content": "In this section, we present the evaluation results and answer the three aforementioned research questions.\nThe overall LLM coding hallucination taxonomy we obtained from Section III-C is presented in Figure 1. Through manual annotation, we identify three primary hallucination categories: Task Requirement Conflicts, Factual Knowledge Conflicts, and Project Context Conflicts, which can be further divided into eight specific types. Note that our three primary categories align well with the hallucination types in the general domain [68]. Task requirement conflicts correspond to input-conflicting hallucinations in the general domain, indicating that the generated code does not meet the functional or non-functional requirements of the coding tasks. Factual knowledge conflicts correspond to knowledge-conflicting hallucinations in the gen- eral domain, indicating that the generated code does not comply with background knowledge, library/framework knowledge, or API knowledge. Project context conflicts correspond to context-conflicting hallucinations in the general domain, indicating that the generated code incorrectly uses project contexts, including environments, dependencies, and resources. In the following, we present the detailed hallucination types in our taxonomy.\n1) Task Requirement Conflicts (43.53%): In the general domain, input-conflicting hallucinations occur when the an- swers generated by LLMs deviate from the original intentions of user inputs [7]. In the context of code generation tasks, the primary intentions of inputs typically revolve around the functional and non-functional requirements of the coding tasks. When the code generated by LLMs does not align with these requirements, hallucinations related to Task Requirement Conflicts occur. Specifically, these conflicts can be categorized into two types: Functional Requirement Violation and Non- functional Requirement Violation.\nBesides functional requirements, developers often have non-functional requirements for the generated code, such as security concerns or performance considerations. These non-functional requirements are usually more implicit than functional requirements and are not described in the input natural language descriptions. Our open coding annotation reveals that non-functional requirements in coding tasks can be mainly divided into the following aspects: Security, Performance, Style, and Code Smell. Generated code that violates these non-functional requirements may introduce safety risks or increase the maintenance complexity of the corresponding project.\nSpecifically, on the security side, the generated code may introduce vulnerabilities such as unsanitized inputs, which can lead to insecure deserialization or SQL injection attacks. As shown in Figure 4, the ground-truth code uses the safe_load function to safely read YAML files. In contrast, the LLM- generated code utilizes the load function, thereby introducing a potential security risk. Regarding performance, the generated code may lack optimization for execution efficiency, for example, by using inefficient loop structures that lead to unnecessary overhead in computing and memory resources. Style violations often occur when the generated code fails to follow established programming conventions or style guides, such as inconsistent naming conventions or inappropriate code layout, which can negatively affect code readability and maintainability. Code smell violations include issues such as overly complex functions or excessive use of global variables, which increase the complexity and potential risks associated with future maintenance.\n2) Factual Knowledge Conflicts (31.91%): In the field of NLP, the term \"factual conflicts\" refers to content generated by LLMs that does not align with established knowledge or facts about the real world. Practical software development similarly relies on various types and levels of factual knowledge to produce correct code. Consequently, when LLMs fail to accurately understand and apply background knowledge [67], li- brary/framework knowledge, or API knowledge, hallucinations on Factual Knowledge Conflicts arise. We further divide this hallucination category into three types: Background Knowledge Conflicts, Library Knowledge Conflicts, and API Knowledge Conflicts.\n1https://en.wikipedia.org/wiki/AUTOSAR\n2https://ocfl.io/1.1/spec/#storage-root\nLibrary Knowledge Conflicts (2.68%). In modern software development, developers frequently employ frameworks or"}, {"title": "V. MITIGATION APPROACH", "content": "The aforementioned root causes of hallucinations in code generated by LLMs can be traced back to three main factors at the inference stage: incorrect or insufficient understanding for task requirements, the lack of factual knowledge pertinent to the generation tasks, and the inability to access the necessary code and non-code resources from the repository. These limitations create substantial challenges for LLMs in code generation in practical development settings. There are many previous works investing LLM-based code generation [26]\u2013[28], [28], [30], [31], [33]-[35], we draw inspiration from existing work [62] on repository-level code generation and explore the feasibility of applying retrieval-augmented generation (RAG) to mitigate hallucinations.The idea is that by providing LLMs with code snippets relevant to the current task, they can better understand the requirements and gain awareness of specific factual knowledge and project contexts.\nTo implement the RAG method, we first collect all code repositories from the CoderEval dataset and follow Re- poCoder's method [62] to construct the retrieval corpora. Specifically, for each repository, we apply a sliding window to scan all the source files in it. This scanning process extracts consecutive lines of code based on a predefined window size. The sliding window moves by a fixed number of lines (slicing step) at each iteration to ensure complete coverage of the code. We adhere to RepoCoder's parameter settings, with a window size of 20 lines and a sliding step of 2 lines. To prevent answer leakage, code lines containing or following the ground-truth code are excluded from the scanning process. Once all files are processed, a retrieval corpus of code snippets is generated for the repository.\nWe employ a sparse bag-of-words (BOW) model for our retrieval mechanism, which simplifies gauging similarity be- tween textual data. This model transmutes both the query and the candidate code snippets into sets of tokens, which are compared using the Jaccard index. The Jaccard index measures the similarity between two sets by dividing the size of their intersection by the size of their union, we choose the code snippet that retrieves the top ten scores each time to return as the prompt for the LLMs.\nWe evaluate the effectiveness of the RAG-based mitigation method with the six LLMs: CodeGen, PanGu-a, ChatGPT, DeepSeekCoder, CodeLlama, and StarCoder2 on the CodeEval dataset. We compared our RAG-based mitigation method with the Raw method. In the Raw method, we only provide LLMs basic docstrings and function signatures. In the RAG-based mitigation, when providing docstrings and function signatures, we will obtain ten related code snippets from the above- constructed retrieval library through a similarity algorithm as prompts and provide them to LLMs. We use the Pass@1 metric to assess the functionality correctness of the generated code snippets according to test cases. As shown in Table I, the Pass@1 scores of all six models are consistently improved with the RAG-based mitigation method. Note that the performance improvement in our experiments is modest, as the mitigation method we explored is preliminary. We consider this experiment as an pilot study to explore the potential effectiveness of RAG- based mitigation. In future work, there are more methods worth studying, such as model fine-tuning and multi-agent framework with tool using, etc.\nTo further illustrate the effectiveness of the hallucination mitigation, we conduct two case studies. As shown in Figure 12, in the Raw method, which only provides a docstring and a function signature, CodeGen incorrectly uses the replace function and fails to convert scripts to one-line commands. In contrast, with the RAG-based method, CodeGen correctly uses the splitlines function, aligning with the ground- truth and successfully addressing the requirement. In addition, the RAG-based method can also effectively mitigate Project Context Conflicts. As shown in Figure 13, in the Raw method, ChatGPT attempts to use the self.items.popitem() API, which does not exist in the repository, leading to hallucinated generation. In contrast, with the RAG-based"}, {"title": "VI. DISCUSSION", "content": "We provide implications for future research on the halluci- nations in practical LLM-based code generation.\nDeveloping hallucination identification techniques: Through our study, we find 3 major categories of hallucinations in the LLM-based code generation. Some hallucinations can be detected by using static analysis (e.g., undefined variables) or dynamic test execution (runtime errors or test failures), making it relatively easy for developers to recognize and locate the relevant code issues. However, certain hallucinations, such as incomplete functionality and security issues, are very difficult for developers to detect and correct, as they can likely pass static checks and all test cases. As a result, LLM-generated code containing these hallucinations may be introduced into development projects and even real production environments, leading to unreliable software systems and severe security risks. Existing hallucination localization approaches [71], [72] based on LLM self-feedback methods can detect hallucinations to a certain extent. However, these approaches heavily rely on the current model's capabilities and cannot address the fundamental limitations imposed by the training corpora. Therefore, in future work, researchers may consider developing more effective techniques to quickly and precisely identify and localize hallucinations in LLM-generated code.\nDeveloping more effective hallucination mitigation tech- niques: In Section V, we explore the feasibility of applying a lightweight RAG-based method to mitigate hallucinations in LLM-based code generation. While the method demonstrates effectiveness in mitigating hallucinations such as undefined attributes, the potentials of RAG need to be further explored. For example, we only construct retrieval corpus using current code repository, leading to the augmented information is insufficient to mitigate many hallucinations such as background knowledge conflicts. In the future, we can integrate more comprehensive knowledge sources like online search engines, API documents, and StackOverflow discussions. In addition to RAG techniques, other methods such as input query refinement [4], [49] and multi-agent systems [73] can also be leveraged to achieve an iterative process of (i) clarifying task requirements, (ii) generating code, (iii) running test cases, and (iv) mitigating hallucinations. To achieve this, we need to design the appropriate interaction protocols between agents and relevant tools (e.g., search engines and static analysis tools) and apply suitable prompting strategies."}, {"title": "VII. THREATS TO VALIDITY", "content": "Threats to external validity mainly con- cern the generalizability of our findings. We focused on Python when exploring the taxonomy and root causes of hallucinations in LLM-based code generation due to its simplicity and ease of use. Constructing hallucination taxonomies for other programming languages and comparing them with our current taxonomy is a valuable future direction. Another potential threat is the limited scale of the adopted CoderEval dataset, which contains only 230 coding tasks. To mitigate this, we selected six LLMs and had each generate 10 code snippets for each task to ensure a sufficient number of annotations.\nThreats to internal validity primarily concern the manual annotation process in taxonomy con- struction. A key issue is the absence of formal inter-rater reliability measure for annotating hallucinations. To address this, discrepancies were discussed and resolved in annotator meetings to ensure a consistent annotation protocol, with each identified hallucination receiving a mutually agreed-upon label. Additionally, to ensure consistency in our findings, one author reviewed all labeled data. Another potential threat is model bias during the annotation process. To mitigate this, we mixed the generation results of the six models before annotation.\nThreats to construct validity are related to evaluating our hallucination mitigation approach. To alleviate these threats, we conducted experiments on six models using test cases available in the CoderEval dataset, a standard method for evaluating the correctness of generated code."}, {"title": "VIII. CONCLUSION", "content": "In this paper, we conduct an empirical study on code- generated hallucinations of large models in the practical"}]}