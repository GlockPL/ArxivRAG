{"title": "TACKLING THE ABSTRACTION AND REASONING CORPUS WITH VISION TRANSFORMERS: THE IMPORTANCE OF 2D REPRESENTATION, POSITIONS, AND OBJECTS", "authors": ["Wenhao Li", "Yudong Xu", "Scott Sanner", "Elias B. Khalil"], "abstract": "The Abstraction and Reasoning Corpus (ARC) is a popular benchmark focused on visual reasoning in the evaluation of Artificial Intelligence systems. In its original framing, an ARC task requires solving a program synthesis problem over small 2D images using a few input-output training pairs. In this work, we adopt the recently popular data-driven approach to the ARC and ask whether a Vision Transformer (ViT) can learn the implicit mapping, from input image to output image, that underlies the task. We show that a ViT-otherwise a state-of-the-art model for images-fails dramatically on most ARC tasks even when trained on one million examples per task. This points to an inherent representational deficiency of the ViT architecture that makes it incapable of uncovering the simple structured mappings underlying the ARC tasks. Building on these insights, we propose VITARC, a ViT-style architecture that unlocks some of the visual reasoning capabilities required by the ARC. Specifically, we use a pixel-level input representation, design a spatially-aware tokenization scheme, and introduce a novel object-based positional encoding that leverages automatic segmentation, among other enhancements. Our task-specific VITARC models achieve a test solve rate close to 100% on more than half of the 400 public ARC tasks strictly through supervised learning from input-output grids. This calls attention to the importance of imbuing the powerful (Vision) Transformer with the correct inductive biases for abstract visual reasoning that are critical even when the training data is plentiful and the mapping is noise-free. Hence, VITARC provides a strong foundation for future research in visual reasoning using transformer-based architectures.", "sections": [{"title": "INTRODUCTION", "content": "Developing systems that are capable of performing abstract reasoning has been a long-standing challenge in Artificial Intelligence (AI). Abstract Visual Reasoning (AVR) tasks require AI models to discern patterns and underlying rules within visual content, offering a rigorous test for evaluating AI systems. Unlike other visual reasoning benchmarks such as Visual Question Answering (VQA) (Antol et al., 2015) and Visual Commonsense Reasoning (VCR) (Kahou et al., 2018) that rely on natural language inputs or knowledge of real-world physical properties, AVR tasks do not include any text or background knowledge. Instead, they focus purely on visual abstraction and pattern recognition (Ma\u0142ki\u0144ski & Ma\u0144dziuk, 2023). One prominent example of AVR is the Abstraction and Reasoning Corpus (ARC) (Chollet, 2019), which is designed to evaluate an AI's capacity for generalization in abstract reasoning. Each ARC task involves transforming input grids into output grids by identifying a hidden mapping often requiring significant reasoning beyond mere pattern matching (cf. Figure 3). While the ARC's original setting is one of few-shot learning, there has been recent interest in studying the ARC in a data-rich setting where task-specific input-output samples can be generated (Hodel, 2024), allowing for the evaluation of deep learning-based solutions."}, {"title": "RELATED WORK", "content": "Abstract Visual Reasoning (AVR) is an emerging field that seeks to measure machine \u201cintelligence\" (Ma\u0142ki\u0144ski & Ma\u0144dziuk, 2023). Unlike many popular studies that focus on visual reasoning with multi-modal input (Antol et al., 2015; Johnson et al., 2017; Zellers et al., 2019; Bakhtin et al., 2019; Li et al., 2024), AVR focuses on reasoning tasks where the inputs are strictly images. The goal of AVR tasks is to discover abstract visual concepts and apply them to new settings. While"}, {"title": "VANILLA VISION TRANSFORMER FOR THE ARC: AN INITIAL APPROACH", "content": "We first implement a vanilla Vision Transformer architecture as detailed in Dosovitskiy et al. (2021) and Touvron et al. (2021) as a solver for the ARC. Consider an input image I divided into P \u00d7 P non-overlapping patches. Each patch pi is flattened in raster order and indexed by i before being projected into a d-dimensional embedding space. Let hi denote the initial input to the Transformer for patch pi. For the n-th Transformer layer, n \u2208 {1, ..., N}, and for a single attention head, the following operations are performed:\n\n$h_i^0 = E_{p_i} + E_{pos_i}$\n\n$h_i^{l'} = LayerNorm(h_i^{l-1})$\n\n$q^{l}, k^{l}, v^{l}_i = h_i^{l'}W_i^{q_i}, h_i^{l'}W_i^{k_i}, h_i^{l'}W_i^{v_i}$\n\n$A_{ij}^l = \\frac{q_i^l k_j^l}{\\sqrt{d}}$\n\n$o_i^{l} = \\sum_j Softmax(A_{ij}^l) h_j^{l-1} + h_i^{l-1}$\n\n$f_i^{l} = FeedForward(LayerNorm(o_i^l)) + o_i^l$\n\n$h^l = LayerNorm(f_i^l)$"}, {"title": "EXPERIMENTS", "content": "Data. To evaluate ViT's reasoning capabilities comprehensively, we treat each of the 400 public training ARC tasks as an individual AVR problem. We generate a dataset of 1 million input-output pairs per task using the RE-ARC generator (Hodel, 2024) and train all of our models (the vanilla ViT and VITARC models) in a supervised manner from scratch.\nHyperparameters and training protocol. The ViT baseline consists of three layers with eight attention heads and a hidden dimension of 128. We trained the model on various single-core GPU nodes, including P100, V100, and T4, using a batch size of 8 for one epoch. We chose to train for one epoch because most models showed signs of convergence within the epoch. Due to computational resource limitations, we evaluated our major milestone models on the full set of 400 tasks. However, for the ablation studies hereafter, we used a randomly sampled subset of 100 tasks. For more details on the training process, please refer to Appendix B.\u00b9\nEvaluation metric. We evaluate the model primarily on the percentage of solved instances, using a strict criterion: an instance is considered solved only if all generated pixels, including padding and border tokens, exactly match the ground truth. This approach is stricter than the original ARC metric which permits up to three candidate solutions.\nResults. Figure 2 shows that the vanilla ViT performs poorly: a significant percentage of tasks have a near 0% solve rate despite the million training examples per task. This points to fundamental limitations of the ViT architecture that inhibit abstract visual reasoning. In the following sections, we analyze failure cases and investigate methods for enhancing the visual reasoning ability of ViT."}, {"title": "VISUAL TOKENS: A BETTER REPRESENTATION FOR VIT", "content": "The basic version of our VITARC framework builds on the vanilla ViT but includes three simple yet highly effective changes to the representation of the ARC grids. We refer to these changes as visual tokens to emphasize a departure from the language-based tokenization perspective in the particular setting of the ARC.\nOur implementation is available at https://github.com/khalil-research/ViTARC.git"}, {"title": "Border tokens for spatial awareness.", "content": "The implementation of 2D padding did not completely alleviate the previously observed failure cases. We further observed that for some tasks, when the output is cropped to the true grid dimensions, the predictions within the valid region are correct, underscoring the importance of proper boundary handling. We show an example in Figure 8 of Appendix A. Inspired by the use of end-of-sequence (EOS) tokens like </s> in Natural Language Processing (NLP), we introduce border tokens to explicitly define the grid boundaries (cf. Figure 1):\nNewline tokens (<arc_n1>) mark row transitions in the hmax \u00d7 Wmax grid.\nEnd-of-grid tokens (<arc_endxgrid>, <arc_endygrid>, and <arc_endxygrid>) delineate the true h \u00d7 w grid boundaries."}, {"title": "2D Absolute Positional Encoding.", "content": "With the introduction of 2D padding and border tokens, our setup now operates on fixed-size, two-dimensional input-output pairs that are aligned with a universal (x, y) coordinate system. This allows us to adopt existing positional encoding (PE) strategies from the literature (see Section 2). After empirical analysis, we implement a (non-learned) 2D sinusoidal APE for VITARC, which is defined as follows:\n\n$Sinusoid(p) = \\begin{cases}sin(\\frac{p}{10000^{2k/d}})\\cr cos(\\frac{p}{10000^{2k/d}})\\end{cases} k = 0,...,d/2,$\n\n$Epos(x,y) = concat (sinusoid(x), sinusoid(y)),$"}, {"title": "ANALYSIS", "content": "While ViTARC-VT delivers strong results-approximately 40% of ARC tasks achieved over 90% solved test instances there remain certain tasks where the model struggles. Specifically, around 10% of ARC tasks have less than 5% of test instances solved, even after training on a large dataset containing one million examples per task. Closer examination reveals that tasks involving complex visual structures, such as concave shapes, holes, or subgrids, are consistently problematic. These challenges highlight certain architectural limitations, particularly the model's difficulty in segmenting multi-colored objects, where positional information should ideally play a more dominant role.\nTo better understand this behavior, we refer back to Equation (1): $h_i^l = E_{p_i} + E_{pos_i}$. In this setup, the absolute positional encoding, $E_{pos_i}$, is directly added to the input embedding, $E_{p_i}$, so that it adjusts the token's representation without overwhelming its semantic content. This works effectively in NLP tasks, where the semantic meaning of tokens generally takes precedence over their position. However, in vision tasks, especially those requiring detailed visual reasoning, spatial"}, {"title": "RECENTERING POSITIONS & OBJECTS FOR SPATIAL REASONING IN VIT", "content": "Our observations on the failure cases of ViTARC-VT lead us to implement further enhancements to tackle tasks with complex visual structures by better encapsulating the positional information of pixels and objects."}, {"title": "Positional Encoding Mixer (PEmixer).", "content": "To better balance the importance of positional information and tokens, we modify Equation (1) by learning weight vectors for the encodings, i.e.,\n\n$h_i^l = \\alpha E_{p_i} + \\beta E_{pos_i},$"}, {"title": "2D Relative Positional Encoding (2D-RPE).", "content": "Motivated by the example in Figure 6, we aim to enable the model to distinguish between pixels in different spatial regions, such as the color-3 (green) pixel in the cyan box versus the one in the yellow box. In this example, the positional difference between the two pixels is just 1 along the y-coordinate. APE encodes this difference as a small shift;"}, {"title": "Object-based Positional Encoding (OPE).", "content": "For tasks involving multi-colored objects, or more generally, tasks that require objectness priors (Chollet, 2019), external sources of knowledge about object abstractions can be integrated into the model. We inject this information through a novel object-based positional encoding. We extend the 2D sinusoidal APE defined in Equation (9) by introducing the object index o as an additional component to the pixel coordinates (x, y). This results in a modified positional encoding:\n\n$Epos (0,x,y) = concat (sinusoid(0), sinusoid(x), sinusoid(y)).$"}, {"title": "RESULTS", "content": "We arrive at our final model, ViTARC, which contains all the improvements mentioned in Section 4 and Section 5. As shown in Figure 2, the model is a significant improvement over both the baseline ViT-Vanilla and ViTARC-VT due to the proposed positional enhancements.\nFurthermore, Figure 5(b) highlights the generalization of these improvements across tasks, with an additional 9.02% increase in solved instances compared to ViTARC-VT. ViTARC-VT itself already achieved a significant boost over ViT-Vanilla, culminating in a total improvement of 57.36% over the baseline ViT-Vanilla.\nFigure 7 further illustrates the impact of each enhancement on task performance. All three contribute to the overall improvement, with 2D-RPE providing the largest gain, followed by PEmixer and OPE. Notably, without 2D-RPE, the model's performance drops below that of ViTARC-VT. This occurs because OPE, while effective in specific tasks, is not consistently reliable. In these cases, ViTARC must fall back on the (x, y) embeddings from 2D-APE, which are less expressive due to their lower dimensionality compared to ViTARC-VT. The inclusion of 2D-RPE recovers these positional signals at the attention level, ensuring robust performance even when object-based cues are insufficient.\nFor a comprehensive breakdown of the task-level performance and the numerical details of these ablations, please refer to Appendix C.2."}, {"title": "CONCLUSION", "content": "This paper introduced VITARC, a Vision Transformer architecture designed to address the unique challenges posed by the Abstraction and Reasoning Corpus. A key finding of our work is that positional information plays a critical role in visual reasoning tasks. While often overlooked when adapting transformers from NLP to vision, our results demonstrate that even simple enhancements to positional encoding can significantly improve performance on ARC tasks. Furthermore, we show that incorporating object indices as additional positional information via OPEs provides a meaningful improvement in handling complex spatial relationships in ARC tasks.\nIn pixel-perfect generative tasks like ARC, where resizing or patching methods are not applicable, we demonstrate that padding can be effectively managed through the introduction of additional border tokens. This innovation allows the model to maintain spatial consistency and correctly reason about grid boundaries, leading to more accurate pixel-level transformations."}, {"title": null, "content": "Our results also show that VITARC can learn complex transformation rules at the pixel level, providing a promising direction for handling the abstract reasoning required in ARC. However, it is important to note that VITARC solves task-specific instances of ARC in a data-driven approach, treating each ARC task independently. This method does not fully solve ARC, which requires the ability to generalize across different tasks a challenge that remains open for future research.\nIn summary, this work highlights the importance of positional information and object-based encodings in abstract visual reasoning that leads to our novel contribution of the VITARC architecture. VITARC advances the application of Vision Transformers for pixel-level reasoning and suggests further avenues for improving generalization capabilities in models tackling visual reasoning tasks."}, {"title": "VANILLA VIT FAILURE ANALYSIS", "content": "This section provides a comprehensive overview of the training setup, including hyperparameters, hardware specifications, and other relevant details regarding the training process.\nOur model consists of 3 layers with 8 attention heads and a hidden dimension of 128. The model was trained on various single-core GPU nodes, including P100, V100, and T4, with a batch size of 8 for 1 epoch. The typical training time per task ranges from 6 to 10 hours (wall clock).\nThe dataset was generated using Hodel's generators (Hodel, 2024), producing 1 million samples, which were then split into training, validation, and test sets with 998,000, 1,000, and 1,000 instances, respectively. The generation time varies between 3 and 12 hours, depending on the task. A fixed random seed (1230) was used for both dataset generation and model training to ensure reproducibility.\nDue to computational resource constraints, the ablation study was performed on a randomly sampled subset of 100 tasks from the total 400, also selected using seed 1230."}, {"title": "FULL RESULTS FOR TASK-SPECIFIC ACCURACIES", "content": "MAIN MODELS ON FULL 400 TASKS"}]}