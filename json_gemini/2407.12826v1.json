{"title": "Assessing the Effectiveness of GPT-40 in Climate Change Evidence Synthesis and Systematic Assessments: Preliminary Insights", "authors": ["Elphin Tom Joe", "Sai Dileep Koneru", "Christine J Kirchhoff"], "abstract": "In this research short, we examine the potential of using GPT-40, a state-of-the-art large language model (LLM) to undertake evidence synthesis and systematic assessment tasks. Traditional workflows for such tasks involve large groups of domain experts who manually review and synthesize vast amounts of literature. The exponential growth of scientific literature and recent advances in LLMs provide an opportunity to complementing these traditional workflows with new age tools. We assess the efficacy of GPT-40 to do these tasks on a sample from the dataset created by the Global Adaptation Mapping Initiative (GAMI) where we check the accuracy of climate change adaptation related feature extraction from the scientific literature across three levels of expertise. Our results indicate that while GPT-40 can achieve high accuracy in low-expertise tasks like geographic location identification, their performance in intermediate and high-expertise tasks, such as stakeholder identification and assessment of depth of the adaptation response, is less reliable. The findings motivate the need for designing assessment workflows that utilize the strengths of models like GPT-40 while also providing refinements to improve their performance on these tasks.", "sections": [{"title": "1 Introduction", "content": "Climate change is one of the most pressing challenges that several regions across the world have to face in the coming decades (Lee et al., 2023). Adapting to climate change is essential for ensuring long-term sustainability (Styczynski et al., 2014). For decision-makers to effectively respond to this challenge, they must carefully plan their strategies based on well-documented and assessed climate adaptation evidence. This involves reviewing a vast array of scientific documents and case studies that detail adaptation efforts in different regions. The Intergovernmental Panel on Climate Change (IPCC) has formalized the assessment of this evidence through the publication of time-sensitive reports. These reports play a crucial role in informing international treaties and country-specific legislative actions.\nWhile traditionally such assessments relied on domain experts working voluntarily in teams tasked with annotating the documents for specific aspects of climate change, this is changing of late with the incorporation of machine learning in the evidence gathering and synthesis process (Berrang-Ford et al., 2021; Sietsma et al., 2024). This is because the exponential growth of scientific literature over time has made the process of managing and synthesizing the evidence increasingly challenging (Bornmann et al., 2021). Automating the annotation of large volumes of scientific data can save valuable researcher time and reduce the assessment cycle, allowing decision-makers to receive quicker and more up-to-date information.\nRecent advancements in neural network methods have shown to be very useful in processing documents and extracting useful information from documents in open domain data such as Wikipedia articles (Martinez-Rodriguez et al., 2020). However, scientific documents present unique challenges due to their complex domain-specific terminologies and concepts. Addressing these challenges requires training models with high-quality, human-labeled data at a sufficient scale. Recent advances in Large Language Models (LLMs) have shown promise in overcoming these challenges. Their diverse training across various topics has made them effective at extracting information from scientific documents and supporting researchers in enhancing time efficiency. Although they perform well in extracting information from scientific documents in certain domains, they still struggle to fully understand the evidence presented in these articles (Koneru et al., 2023).\nGiven the importance of accurate information ex-"}, {"title": "2 Related work", "content": "In recent years, LLMs have gained significant attention across the world due to their ability to complete tasks on which no explicit training was provided. The variety of applications being explored by this technology has spawned a new interest in deploying them across different domains such as medical science, business, education etc. Several studies have investigated the use of LLMs as text annotators, primarily in open domain settings (Ding et al., 2022; He et al., 2023). However, there is limited research on evaluating their performance in applications requiring domain expertise, specifically information extraction from scientific articles (Dagdelen et al., 2024). Studies have experimented with incorporating LLMs into data annotation pipelines, particularly for annotating texts that require domain expertise. These efforts have shown potential to reduce the time and overall cost of annotation (Goel et al., 2023). For instance, LLMs have been used to extract information about nanorod structure procedures from scientific texts (Walker et al., 2023) and to extract information from clinical trials (Ghosh et al., 2024).\nIn the context of Climate Change research, recent work such as the creation of Expert Confidence in Climate Statements (CLIMATEX) dataset"}, {"title": "3 Method", "content": ""}, {"title": "3.1 Dataset", "content": "The dataset for this study has been sourced from the Global Adaptation Mapping Initiative (GAMI) (Berrang-Ford et al., 2021) a global effort led by IPCC scientists to systematically collect and assess the evidence in peer-reviewed literature on climate adaptation progress. This dataset consisted of twenty five features such as Geographic Location, Adaptation Response Type, Implementation Tools etc. labeled by climate change adaptation experts from 1,682 peer-reviewed articles that met an inclusion criteria defined by the group of scientists leading the initiative. The curation of this dataset was rigorous wherein each peer-reviewed article was assigned to two human-labelers with climate change adaptation expertise and any conflict between any of the features labelled by them was resolved by a senior expert selected for their extensive experience in climate adaptation. Further details on the dataset creation can be found in the original article (Berrang-Ford et al., 2021).\nIt is important to note that the 1,682 peer-reviewed articles were divided into focus groups such as cities, food, health, etc. to help administer the labelling as well as the systematic assessment exercise. For the purposes of this study, we focus on a sample (n =586) of the GAMI database i.e. adaptation responses documented only in the food sector focus group. The publications in this focus group consist information related to climate change adaptation responses aimed at ensuring food security and sustaining related livelihoods. Having"}, {"title": "3.2 Task Description", "content": "To assess the utility of GPT-40, we used three features on the sampled GAMI dataset where each feature reflects a specific level of expertise and domain knowledge in order to accurately capture information regarding the adaptation response. Table 1 outlines the features categorized by their complexity based on the level of expertise needed for accurate extraction, along with illustrative examples for each feature.\n\u2022 For the low expert level we chose the extraction of geographical country where the climate change adaptation response occurred. This is similar to identifying location using open domain standard NLP tasks such as NER. Although an article may discuss multiple countries that are not relevant to the specific adaptation response in sections such as the introduction or related work, the model must accurately extract the specific location where the adaptation response occurred. This task can be viewed as classic information retrieval, and we evaluate the model using precision and recall metrics.\n\u2022 The medium or intermediate expertise level fea-"}, {"title": "3.3 Experiments", "content": "We prompted GPT-40\u00b9 for this task by first converting the PDF files to markdown format using LlamaParse\u00b2. To guide the model effectively, we included an intermediate verification step in the prompts. Specifically, the model was first asked to identify the climate change adaptation response and then to recognize that stakeholders involved in this response. The complete prompt used is provided in the Appendix A. Additionally, to understand the model's reasoning and identify differences between the model's outputs and human annotations, we asked the model to provide excerpts that it used to justify its extractions. This approach allowed us to analyze the model's rationale where it diverged from human annotations. The model was prompted under default settings."}, {"title": "4 Results", "content": "To evaluate the information extraction capabilities of GPT-40 in the context of climate change data from scientific publications, we compared the annotation agreement between human-created labels and the information extracted by GPT-40. Table 2 presents a summary of the evaluation metrics, and our findings are detailed below:\nLow Expertise Tasks such as extracting the geographic regions where adaptation responses occurred, GPT-40 demonstrated high agreement with human annotators. Specifically, GPT-4o achieved a precision score of 0.88 and a recall of 0.9. In instances of disagreement, manual checks revealed that GPT-40 often provided more specific information, extracting exact countries while human annotators tended to group countries together. These results align with findings from studies that used LLMs for NER tasks, suggesting consistent performance across different domains (Goel et al., 2023)."}, {"title": "5 Limitations", "content": "In this study, we tried to cover a diverse set of information extraction tasks in the context of climate change adaptation research to understand the feasi-"}, {"title": "6 Conclusion", "content": "From our study we find that there are opportunities and challenges for the deployment of pre-trained LLMs in the climate evidence synthesis and assessments. We assessed the efficiency of GPT-40's role as an annotator and find that tasks requiring beyond low levels of expertise are challenging for GPT-40. Future work should explore using methods to integrate knowledge for medium expertise level and learning from human feedback to improve the model performance on extraction on information that requires high levels of expertise. Further, models with such capacity when trained on task specific data, could play a complementary role in the task of adaptation tracking by governments and global agencies and eventually help in timely securing funding for necessary adaptation responses. However, it is important to emphasize that these models cannot completely replace the expert-driven process, rather a human-in-the loop system would be extremely beneficial for ensuring the integrity and effectiveness of this process."}]}