{"title": "Assessing the Effectiveness of GPT-40 in Climate Change Evidence Synthesis and Systematic Assessments: Preliminary Insights", "authors": ["Elphin Tom Joe", "Sai Dileep Koneru", "Christine J Kirchhoff"], "abstract": "In this research short, we examine the poten- tial of using GPT-40, a state-of-the-art large language model (LLM) to undertake evidence synthesis and systematic assessment tasks. Tra- ditional workflows for such tasks involve large groups of domain experts who manually review and synthesize vast amounts of literature. The exponential growth of scientific literature and recent advances in LLMs provide an opportu- nity to complementing these traditional work- flows with new age tools. We assess the efficacy of GPT-40 to do these tasks on a sample from the dataset created by the Global Adaptation Mapping Initiative (GAMI) where we check the accuracy of climate change adaptation related feature extraction from the scientific literature across three levels of expertise. Our results indicate that while GPT-40 can achieve high accuracy in low-expertise tasks like geographic location identification, their performance in in- termediate and high-expertise tasks, such as stakeholder identification and assessment of depth of the adaptation response, is less reliable. The findings motivate the need for designing assessment workflows that utilize the strengths of models like GPT-40 while also providing refinements to improve their performance on these tasks.", "sections": [{"title": "1 Introduction", "content": "Climate change is one of the most pressing chal- lenges that several regions across the world have to face in the coming decades (Lee et al., 2023). Adapting to climate change is essential for ensuring long-term sustainability (Styczynski et al., 2014). For decision-makers to effectively respond to this challenge, they must carefully plan their strategies based on well-documented and assessed climate adaptation evidence. This involves reviewing a vast array of scientific documents and case studies that detail adaptation efforts in different regions. The Intergovernmental Panel on Climate Change (IPCC) has formalized the assessment of this ev- idence through the publication of time-sensitive reports. These reports play a crucial role in in- forming international treaties and country-specific legislative actions.\nWhile traditionally such assessments relied on domain experts working voluntarily in teams tasked with annotating the documents for specific aspects of climate change, this is changing of late with the incorporation of machine learning in the evi- dence gathering and synthesis process (Berrang- Ford et al., 2021; Sietsma et al., 2024). This is be- cause the exponential growth of scientific literature over time has made the process of managing and synthesizing the evidence increasingly challenging (Bornmann et al., 2021). Automating the annota- tion of large volumes of scientific data can save valuable researcher time and reduce the assessment cycle, allowing decision-makers to receive quicker and more up-to-date information.\nRecent advancements in neural network methods have shown to be very useful in processing docu- ments and extracting useful information from doc- uments in open domain data such as Wikipedia ar- ticles (Martinez-Rodriguez et al., 2020). However, scientific documents present unique challenges due to their complex domain-specific terminologies and concepts. Addressing these challenges requires training models with high-quality, human-labeled data at a sufficient scale. Recent advances in Large Language Models (LLMs) have shown promise in overcoming these challenges. Their diverse train- ing across various topics has made them effective at extracting information from scientific documents and supporting researchers in enhancing time effi- ciency. Although they perform well in extracting information from scientific documents in certain domains, they still struggle to fully understand the evidence presented in these articles (Koneru et al., 2023).\nGiven the importance of accurate information ex-"}, {"title": "2 Related work", "content": "In recent years, LLMs have gained significant atten- tion across the world due to their ability to complete tasks on which no explicit training was provided. The variety of applications being explored by this technology has spawned a new interest in deploy- ing them across different domains such as medical science, business, education etc. Several studies have investigated the use of LLMs as text annota- tors, primarily in open domain settings (Ding et al., 2022; He et al., 2023). However, there is limited research on evaluating their performance in applica- tions requiring domain expertise, specifically infor- mation extraction from scientific articles (Dagdelen et al., 2024). Studies have experimented with in- corporating LLMs into data annotation pipelines, particularly for annotating texts that require domain expertise. These efforts have shown potential to re- duce the time and overall cost of annotation (Goel et al., 2023). For instance, LLMs have been used to extract information about nanorod structure proce- dures from scientific texts (Walker et al., 2023) and to extract information from clinical trials (Ghosh et al., 2024).\nIn the context of Climate Change research, re- cent work such as the creation of Expert Confi- dence in Climate Statements (CLIMATEX) dataset"}, {"title": "3 Method", "content": null}, {"title": "3.1 Dataset", "content": "The dataset for this study has been sourced from the Global Adaptation Mapping Initiative (GAMI) (Berrang-Ford et al., 2021) a global effort led by IPCC scientists to systematically collect and assess the evidence in peer-reviewed literature on climate adaptation progress. This dataset consisted of twenty five features such as Geographic Loca- tion, Adaptation Response Type, Implementation Tools etc. labeled by climate change adaptation experts from 1,682 peer-reviewed articles that met an inclusion criteria defined by the group of sci- entists leading the initiative. The curation of this dataset was rigorous wherein each peer-reviewed article was assigned to two human-labelers with climate change adaptation expertise and any con- flict between any of the features labelled by them was resolved by a senior expert selected for their extensive experience in climate adaptation. Further details on the dataset creation can be found in the original article (Berrang-Ford et al., 2021).\nIt is important to note that the 1,682 peer- reviewed articles were divided into focus groups such as cities, food, health, etc. to help administer the labelling as well as the systematic assessment exercise. For the purposes of this study, we focus on a sample (n =586) of the GAMI database i.e. adaptation responses documented only in the food sector focus group. The publications in this focus group consist information related to climate change adaptation responses aimed at ensuring food se- curity and sustaining related livelihoods. Having"}, {"title": "3.2 Task Description", "content": "To assess the utility of GPT-40, we used three fea- tures on the sampled GAMI dataset where each feature reflects a specific level of expertise and do- main knowledge in order to accurately capture in- formation regarding the adaptation response. Table 1 outlines the features categorized by their com- plexity based on the level of expertise needed for accurate extraction, along with illustrative exam- ples for each feature.\n\u2022 For the low expert level we chose the extrac- tion of geographical country where the climate change adaptation response occurred. This is similar to identifying location using open domain standard NLP tasks such as NER. Although an article may discuss multiple countries that are not relevant to the specific adaptation response in sections such as the introduction or related work, the model must accurately extract the specific location where the adaptation response occurred. This task can be viewed as classic information retrieval, and we evaluate the model using preci- sion and recall metrics.\n\u2022 The medium or intermediate expertise level fea-"}, {"title": "4 Results", "content": "To evaluate the information extraction capabilities of GPT-40 in the context of climate change data from scientific publications, we compared the an- notation agreement between human-created labels and the information extracted by GPT-40. Table 2 presents a summary of the evaluation metrics, and our findings are detailed below:\nLow Expertise Tasks such as extracting the ge- ographic regions where adaptation responses oc- curred, GPT-40 demonstrated high agreement with human annotators. Specifically, GPT-4o achieved a precision score of 0.88 and a recall of 0.9. In instances of disagreement, manual checks revealed that GPT-40 often provided more specific informa- tion, extracting exact countries while human anno- tators tended to group countries together. These results align with findings from studies that used LLMs for NER tasks, suggesting consistent perfor- mance across different domains (Goel et al., 2023).\nFurthermore, this specificity indicates the potential of GPT-40 to enhance the granularity of extracted data in low-expertise tasks.\nIntermediate Expertise Tasks of identifying stakeholders involved in adaptation responses, GPT-40 effectively captured the primary stakehold- ers but also extracted extraneous information and occasionally misclassified categories. The perfor- mance metrics for this level included a micro F1 score of 0.54 (macro: 0.30), precision of 0.40 (macro: 0.27), and recall of 0.83 (macro: 0.33). Manual checks of the disagreements highlighted that GPT-4o sometimes misidentified and extracted stakeholders mentioned in introductory sections or other parts of the text, which were not relevant to the specific adaptation measures being discussed in the document (high recall). Given the task re- quires the model to use a taxonomy to classify the stakeholders, the model's performance on this task suggests that improvements can be made by inte- grating prompting methods that elicit reasoning and verification capabilities.\nHigh Expertise Tasks For high expertise feature extraction, our evaluation reveals that the model, in some cases (10.4% of the time), provides indi- vidual assessments for each adaptation response rather than an aggregate assessment of the impact of a set of responses. This behavior complicated the evaluation process. To address this, we isolated these instances and focused our evaluation on cases where the model provided a depth evaluation for the aggregate of set of adaptation responses. We treated the evaluation as a multi-class classification problem. In this context, GPT-4o achieved an ac- curacy of 22.7% and a micro-averaged F1 score of 0.22 (macro F1: 0.17). Closer examination of the instances of disagreement revealed that, in all the cases with minimal agreement, GPT-40 exhibited a more optimistic view compared to human annota- tors, often overestimating the impact of adaptation responses. This discrepancy is likely due to the generalized nature of the model's training and in- struction tuning on a wide range of tasks. These findings highlight significant challenges in using GPT-40 for tasks that require a deep understanding of complex and nuanced information."}, {"title": "5 Limitations", "content": "In this study, we tried to cover a diverse set of in- formation extraction tasks in the context of climate change adaptation research to understand the feasi-"}, {"title": "6 Conclusion", "content": "From our study we find that there are opportunities and challenges for the deployment of pre-trained LLMs in the climate evidence synthesis and as- sessments. We assessed the efficiency of GPT-40's role as an annotator and find that tasks requiring beyond low levels of expertise are challenging for GPT-40. Future work should explore using meth- ods to integrate knowledge for medium expertise level and learning from human feedback to improve the model performance on extraction on informa- tion that requires high levels of expertise. Further, models with such capacity when trained on task specific data, could play a complementary role in the task of adaptation tracking by governments and global agencies and eventually help in timely se- curing funding for necessary adaptation responses. However, it is important to emphasize that these models cannot completely replace the expert-driven process, rather a human-in-the loop system would be extremely beneficial for ensuring the integrity and effectiveness of this process."}, {"title": "A Appendix", "content": "Here we provide the prompt we have used for in- formation extraction."}]}