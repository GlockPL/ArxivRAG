{"title": "Adapting While Learning:\nGrounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation", "authors": ["Bohan Lyu", "Yadi Cao", "Duncan Watson-Parris", "Leon Bergen", "Taylor Berg-Kirkpatrick", "Rose Yu"], "abstract": "Large Language Models (LLMs) demonstrate\npromising capabilities in solving simple scien-\ntific problems but often produce hallucinations\nfor complex ones. While integrating LLMs with\ntools can increase reliability, this approach typi-\ncally results in over-reliance on tools, diminish-\ning the model's ability to solve simple problems\nthrough basic reasoning. In contrast, human ex-\nperts first assess problem complexity using do-\nmain knowledge before choosing an appropri-\nate solution approach. Inspired by this human\nproblem-solving process, we propose a novel two-\ncomponent fine-tuning method. In the first com-\nponent World Knowledge Distillation (WKD),\nLLMs learn directly from solutions generated\nusing tool's information to internalize domain\nknowledge. In the second component Tool Usage\nAdaptation (TUA), we partition problems into\neasy and hard categories based on the model's\ndirect answering accuracy. While maintaining the\nsame alignment target for easy problems as in\nWKD, we train the model to intelligently switch\nto tool usage for more challenging problems.\nWe validate our method on six scientific bench-\nmark datasets, spanning mathematics, climate sci-\nence and epidemiology. On average, our models\ndemonstrate a 28.18% improvement in answer ac-\ncuracy and a 13.89% increase in tool usage preci-\nsion across all datasets, surpassing state-of-the-art\nmodels including GPT-40 and Claude-3.5.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) (Brown, 2020; Dubey\net al., 2024) have demonstrated impressive capabilities\nacross various scientific domains, from answering general\nquestions (Lu et al., 2022; Zhang et al., 2024b) to contribut-\ning to scientific discoveries (Ma et al., 2024; Kumar et al.,\n2023; Liu et al.). However, scholars note that their abilities\nare capped at approximately high-school levels (Rein et al.,\n2024; Cobbe et al., 2021; Hendrycks et al.).\nCompared to direct reasoning alone, leveraging scientific\ntools such as physics-based simulators enables the handling\nof more complex problem-solving tasks. Following this\ntrajectory, integrating LLMs with such specialized tools rep-\nresents a natural evolution to enhance their problem-solving\ncapabilities (Schick et al., 2023; Qin et al., 2024; Liu et al.).\nHowever, when trained solely on tool usage, LLMs tend to\nover-rely on tools, even for problems solvable through ba-\nsic reasoning. This not only increases computational costs\ndue to resource-intensive scientific tools but also limits the\nmodel's ability to internalize knowledge.\nDrawing inspiration from human expert behavior (Payne\net al., 1993; Stevenson et al., 1986; Kruger & Dunning,\n1999), we observe that experts first assess a scientific prob-\nlem's complexity before deciding whether to employ basic\nreasoning or specialized tools. We aim to instill similar\nadaptive capabilities in LLMs, developing them into reliable\nand accurate assistants across scientific domains. While\nprevious works have explored prompt engineering and post-\nprocessing techniques (Li et al., 2024; Wan et al., 2024;\nWang et al., 2024a; Zheng et al., 2024a) to improve infer-\nence accuracy and efficiency, none have focused on training\nmodels to make adaptive decisions about tool usage.\nTo this end, we propose a novel training paradigm consisting\nof two components. The first component, World Knowl-\nedge Distillation (WKD), uses supervised fine-tuning and\npreference learning to align a pre-trained LLM with highly\naccurate solutions generated using information from exter-\nnal tools, aiming to internalize scientific knowledge. In\nthe second component, Tool Usage Adaptation (TUA), we\nevaluate the LLM's direct answering ability and classify\nquestions as easy or hard based on the model's accuracy.\nWhile maintaining the same alignment target for easy ques-\ntions, we train the model to follow external tool traces for\nhard questions, enabling intelligent switching based on prob-\nlem complexity."}, {"title": "2. Related Work", "content": "LLM Alignment Alignment techniques aim to make\nLLMs behave in accordance with human values, using\nmethods such as supervised fine-tuning (SFT) (Zhang et al.,\n2024a) and reinforcement learning (RL). RL methods can\nbe categorized into online (Rafailov et al., 2024; Meng et al.,\n2024) and offline (Schulman et al., 2017; Ouyang et al.,\n2022; Lee et al., 2023; Bai et al., 2022) algorithms. Both\nonline and offline algorithms convert the non-trivial scoring\nof text outputs into pairwise preference orderings.\nWe employ SFT for all questions in our dataset. Addition-\nally, we utilize RL to learn preferences between different\nproposals for open-ended questions.\nGrounding LLMs for Scientific Problems Previous\nwork has sought to ground LLMs using domain-specific\nknowledge across various scientific fields: climate sci-\nence (Thulke et al., 2024), biomedical science (Luo et al.,\n2022), molecular science (Chithrananda et al., 2020), and\ngeneral science (Zhang et al., 2024b; Taylor et al., 2022).\nMost of these approaches heavily rely on expert annotations\nor distillation from stronger models and face scalability\nlimitations due to computational and expert labor costs.\nThese constraints highlight the need to integrate scientific\ntools into both data generation and training processes.\nLLM as Tool User Researchers have integrated LLMs\nwith various tools (Schick et al., 2023; Tang et al., 2023;\nPatil et al., 2023; Qin et al., 2023b; Wang et al., 2024c),\nwhich fall into two broad categories: 1. tools with time-\nvarying results, such as search engines (Nakano et al., 2022;\nQin et al., 2023a) and social media platforms (Park et al.,\n2023; Ye et al., 2023); 2. tools with consistent results, such\nas compilers, physics-based simulators (Ma et al., 2024;\nKumar et al., 2023; Liu et al.; Bran et al., 2023; Huang\net al., 2024), and scientific knowledge bases (Kraus et al.,\n2023; Koldunov & Jung, 2024; Thulke et al., 2024; Vaghefi\net al., 2023). While these approaches leverage LLMs' tool-\nusing capabilities, they do not enhance the LLMs' inherent\ndomain knowledge. Furthermore, existing studies have not\naddressed training LLMs to make intelligent decisions about\ntool usage based on problem complexity, often resulting in\nover-reliance on the tools covered during training."}, {"title": "3. Methodology", "content": "To equip LLMs with both internalized knowledge and adap-\ntive tool usage capabilities, we propose a two-component\ntraining paradigm: World Knowledge Distillation (WKD)\nand Tool Usage Adaptation (TUA). The first component\nequips the model with domain-specific expertise through\nsupervised fine-tuning and preference optimization. The\nsecond component trains the model to intelligently leverage\nexternal tools based on task complexity. Together, these\ncomponents enable the model to balance direct reasoning\nwith effective tool use, enhanced accuracy and reliability\nin scientific problem-solving. Figure 1 illustrates our fine-\ntuning pipeline and its intended target."}, {"title": "3.1. Solution Generation with Tools", "content": "Our approach integrates professional tools E, such as\nphysics simulators, with LLMs for high-accuracy answer\ngeneration. We employ templates for both questions and\ncorresponding tool traces t for solution generation. At each\nstep in t, we instruct the LLM to run the simulator using\na specific format, i.e., a system prompt Pf instructing and\nforcing tool usage. After gathering the returned information\n{Ie}t from tool usage traces t, the LLM generates the solu-\ntion y using its policy \u03c0 combined with the context provided\nin question x. We denote this whole process as:\n$y \\sim \\pi(\\cdot | x, \\{I_e\\}_t, P_f)$.\n(1)"}, {"title": "3.2. World Knowledge Distillation (WKD)", "content": "Following the generation of these solutions, we proceed\nto directly fine-tune the target LLM. The alignment loss\nbetween the generated answer and (1) is defined as:\n$J_{Direct}(\\theta, D, P) =$\n$\\bull E_{x\\sim D,y\\sim \\pi(\\cdot|x\\{I_e\\}_t,P_f)} [log \\pi_{\\theta}(y | x, P)],$\n(2)\nwhere D represents the training dataset. The loss for World\nKnowledge Distillation (WKD) is then:\n$J_{WKD}(\\theta, D) = J_{Direct}(\\theta, D, P_n),$\n(3)\nwhere Pn is the prompt that allows no tool usage.\nThe aim of WKD is to prompt the LLM to generate solu-\ntions directly, without relying on tools. This is the ideal case\nif achievable. However, when problems are too challeng-\ning and hallucination may occur, we switch to Tool Usage\nAdaptation (TUA), which trains the LLM to intelligently\nselect tools to use based on problem complexity."}, {"title": "3.3. Tool Usage Adaptation (TUA)", "content": "The TUA begins by evaluating the fine-tuned LLMs after\nWKD on the questions of a benchmark. For each question,\nwe sample an ensemble of directly generated answers to\ncalculate the accuracy rate. Based on a predefined threshold\nof the accuracy rate, we partition the questions into two\nsubsets: Deasy, problems the LLM can solve directly, and\nDhard, the remaining ones.\nFor Deasy, we keep the alignment target as in (2). However,\nfor Dhard, we switch the alignment target to the augmented\nsolution with tool usage traces and train the LLM to follow\nthese traces accurately. In this case, the alignment loss for\ncorrect traces is:\n$J_{Trace}(\\theta, D, P) =$\n$- E_{x\\sim D,t\\sim \\pi(\\cdot|x,E,P_f)} log \\pi_{\\theta}(t | x, E, P).$\n(4)\nThe combined training loss considering both easy and hard\nquestions is:\n$J_{TUA} (\\theta, D_{easy}, D_{hard}) =$\n$\\lambda J_{Direct}(\\theta, D_{easy}, P_i) + (1 - \\lambda) J_{Trace}(\\theta, D_{hard}, P_i),$\n(5)\nwhere Pi is a prompt that allows LLMs to intelligently\nchoose whether to utilize external tools or not. \\lambda adjusts\nthe weight between the two subsets to prevent extreme pro-\nportion distribution. In our case, we set \\lambda = 0.5 as our\ndataset's easy and hard questions have proportions at the\nsame magnitude. Note that we apply the same prompt Pi\nacross both easy and hard questions to train the model's\nability to determine when to leverage tools."}, {"title": "3.4. Knowledge Consistency Across Prompt Strategies", "content": "In our settings, certain knowledge required for directly an-\nswering questions should be learned under both prompting\nstrategies: Pn during WKD,and Pi during TUA and dur-\ning deployment. Recent research (Zeng et al., 2024) has\nhighlighted a critical challenge: knowledge acquired under\none prompting strategy may not readily transfer to another,\noften resulting in significant performance degradation. We\nencountered similar issues in our preliminary experiments\nwhen attempting to alternate between WKD and TUA within\ntraining iterations.\nTo mitigate this issue, we propose leveraging a mixed loss\nthat simultaneously considers both WKD and TUA objec-\ntives, thereby maintaining consistent knowledge across dif-\nferent prompting strategies.\nFor each epoch, we first partition the dataset into easy and\nhard questions following the procedure outlined in Sec-\ntion 3.3. The mixed loss function is defined as:\n$J_{Mix} (\\theta, D, D_{easy}, D_{hard}) =$\n$\\alpha J_{WKD}(\\theta, D) + (1 - \\alpha) J_{TUA}(\\theta, D_{easy}, D_{hard}),$\n(6)\nwhere \u03b1 is a hyperparameter that balances the learning under\nPn and Pi. In our experiments, we consistently set \u03b1 =\n0.5. We note that this term differs fundamentally from a\nre-weighting of the loss terms in Equation 5, as the latter\nonly takes the input as Pi."}, {"title": "4. Experiments", "content": "4.1. Dataset\nWe employ two existing public datasets, MATH and\nSciBench, and construct four new scientific datasets for our\nexperiments: Mujoco, Partial Differential Equations (PDEs),\nClimate Science, and Epidemiology. Detailed statistics of\nall datasets are available in Appendix A.1.\nOur custom dataset construction follows a systematic\npipeline. First, we design domain-specific question tem-\nplates based on both the expert consultation and the emu-\nlator functionality. We then generate individual questions\nby sampling parameters within scientifically valid ranges.\nFinally, we produce corresponding solutions using LLMs\nand tool usage traces, as detailed in Section 3.2.\nOur custom datasets primarily comprise multiple-choice\nquestions (MCQs), with the Climate dataset additionally\nincorporating open-ended questions (such as policy propos-\nals for climate change mitigation). Open-ended questions,\nlacking definitive \u201cgold answers\", necessitate a modified\npipeline for preference learning\u2014details of which will be\npresented in Section 5.6. The publicly available MATH and\nSciBench datasets consist exclusively of numerical prob-\nlems.\nMATH MATH (Hendrycks et al.) is a collection of\nhigh-school-level competition mathematics problems, cat-\negorized into five difficulty levels. Following previous\nwork (Qian et al., 2023), we utilize problems from the test\nset with numerical answers to evaluate our methods.\nSciBench SciBench (Wang et al., 2024b) is a challenging\ncollegiate-level benchmark of scientific problems covering\nMathematics, Physics, and Chemistry.\nMujoco We developed this dataset to address problems\nrelated to rigid- and soft-body dynamics, utilizing the Mu-\njoco physics engine (Todorov et al., 2012). Unlike idealized\nscenarios common in textbooks, these problems incorpo-\nrate real-world complexities such as stiffness, damping, and\nvarious types of collisions and frictional forces.\nPartial Differential Equations (PDEs) We created in-\nhouse numerical solvers and design questions in heat trans-\nfer, chemical engineering, population simulation, etc. These\nquestions require solving 1D or 2D PDEs.\nClimate Science We designed a dataset of climate science\nproblems centered around a neural surrogate model (Niu\net al., 2024). The model accepts inputs including time,\nclimate scenario (e.g., ssp126, ssp245), and emissions of\ngreenhouse gases (CO2, CH4) and aerosol gases (BC, SO2),\noutputting the corresponding earth surface temperature.\nEpidemiology We constructed this dataset of epi-\ndemiological problems using a state-of-the-art surrogate\nmodel (Wu et al., 2023). The model inputs include 28-\nday, multi-dimensional features for each California county,"}, {"title": "4.2. Experiment Setup", "content": "Models We used Llama-3.1-8B-Instruct (Dubey\net al., 2024) as the base model for our training scheme.\nWe also conducted extensive evaluations of other state-\nof-the-art (SOTA) open and closed-source models, includ-\ning GPT40, GPT4o-mini,Claude-3.5-Sonnet, and\nLlama-3.1-70B-Instruct.\nComparisons We primarily report the performance of the\nuntrained base model both with and without tool usage, as\nwell as its performance after training with our method. Ad-\nditionally, we conducted ablation studies to analyze the in-\nfluence of various factors, such as the necessity of WKD and\nTUA and the proportion of noisy data in the training dataset.\nExperiments with Pn were conducted under a zero-shot set-\nting for our custom-created datasets and MATH, and under a\nfew-shot setting for SciBench with officially provided shots.\nFor experiments with Pf and Pi, we sampled correct tool\ntraces generated by GPT-40 as few-shot prompts.\nTools We employed different tools for each dataset. For\nMujoco, we used the official APIs. In the case of PDEs,\nwe utilized in-house numerical solvers. For Climate and\nEpidemiology datasets, we employed APIs wrapping the\ncorresponding neural surrogate models. For open-ended\ndatasets, we used a Python code interpreter."}, {"title": "4.3. Evaluation Metrics", "content": "We primarily evaluate two types of accuracy: Answer Accu-\nracy and Tool Usage Accuracy."}, {"title": "5. Results", "content": "5.1. Answer Accuracy\nWe report the answer accuracy comparison across all\ndatasets in Table 1. Our method substantially outperforms\nall baselines on custom-created datasets that were not com-\nmonly covered in pre-training. While our model does not\nsurpass current state-of-the-art models on public datasets, it\nshows significant improvement over the base model without\nfine-tuning. This performance gap on public benchmarks\nis likely due to the larger parameter count and specific op-\ntimization of state-of-the-art models on the open-source\ndatasets.\n5.2. Tool Usage Accuracy\nWe present the tool usage accuracy in Table 2. Overall,\nour trained model achieves the best tool usage accuracy\nacross all datasets, except for SciBench where it ranks sec-\nond, demonstrating the ability to make intelligent decisions\non tool usage. In contrast, other models exhibit accuracy\naround 50%$\\frac{1}{2} \\cdot (0 + 100\\%)$ , indicating two typical cases:\neither over-reliance on tools or never attempting to use them\n(empirical proof is in Appendix C.3).\nBesides the advantage shown in Table 2, we further in-\nvestigate the tool usage decisions on the MATH dataset,\nwhich provides a prior label of difficulty levels in Figure 8.\nOur trained model exhibits a reasonable increase in tool us-\nage with growing question difficulty, while the base model\nshows an over-reliance on tools regardless of difficulty. In\ncontrast, Claude-3.5 demonstrates more confidence in an-\nswering directly for both easy and hard questions, possibly\nbecause MATH is a public dataset and the model has seen\nsimilar questions during training."}, {"title": "5.3. Functionality of Sub-components", "content": "Figure 4 presents an ablation study on the functionality of\nWKD and TUA by evaluating the proportion of the four tool\nusage decisions (EN, ET, HN, HT) on the Climate dataset.\nWe observe that omitting either component leads to tool\nover-reliance. Moreover, without WKD, the model exhibits\nthe lowest answer accuracy, as it is never trained on the\ndistilled knowledge directly."}, {"title": "5.4. Functionality of Prompt Strategies", "content": "Figure 5. Different models' performance on the PDE Dataset: com-\nparing pre- and post-training, with and without tool usage.\nWe present the model's performance on the PDE dataset\nin Figure 5 before and after training, and with Pn and Pi,\nrespectively.\nThe model Ours-Pn demonstrates performance comparable\nto Base-Pf, both showing a significant improvement over\nthe base model. This similarity indicates successful inter-\nnalization of distilled knowledge from tools. The transition\nfrom Ours-Pn to Ours-Pi showcases further improvement\nin answer accuracy, resulting from the model's enhanced\nability to intelligently switch to tools for harder questions."}, {"title": "5.5. Resistance to Noisy Data brought by TUA", "content": "Figure 6. Model Performance vs. Noise Level: Comparison be-\ntween our two-component method and SFT-only approaches on\nClimate dataset.\nGenerating solutions via LLMs or human expert annota-\ntion inevitably introduces noise. We examine how model\ntrained with our method performs with increasing noise on\nthe Climate dataset. We compare the model trained with our\ntwo-component method, against a model that only under-\ngoes SFT for comparison. The results are shown in Figure 6.\nThe WKD-only model's performance degrades drastically\nwith increasing levels of noise, as the underlying distribution\nbecomes polluted. However, this does not significantly\nimpact the trained model with P\u2081. The model judges these\npolluted questions as harder and opts to use tools to ensure\naccuracy, demonstrating the robustness of our approach.\nAs noise levels increase, the performance of the SFT-only\nmethod declines, while models trained with our method\ndemonstrate robust performance."}, {"title": "5.6. Extension to Open-ended Questions", "content": "We extend our approach to open-ended questions, specifi-\ncally addressing problems with two key characteristics: 1.\nExpensive verification, reflecting the high cost of experi-\nmental or simulation as tools. This constraint precludes\nthe use of model-based reinforcement learning, which would\notherwise be a mature solution. 2. High penalties for violat-\ning feasibility constraints, mirroring real-world applications\nwhere failed plans incur unaffordable costs (e.g., aircraft\ndesign). Such questions necessitate modifications to both\ndata generation and the training pipeline to incorporate pref-\nerence learning with our method:\nModified Data Generation We prompt the LLM to gen-\nerate an ensemble of solutions for a given problem (e.g.,\nproposing a marine route between two ports that minimizes\nfuture temperature rise). These solutions are evaluated using\nprofessional tools (e.g., a neural surrogate model for climate\nscience) to derive pre-defined metrics L, such as a penalty\nfunction of temperature rise. We then form pair-wise rank-\nings for subsequent preference learning. We expand the\ndefinition of trace t' to include: 1) ensemble generation, 2)\nensemble evaluation with tools, and 3) ranking.\nEnhanced Training Pipeline\n\u2022 WKD: We switch to preference alignment using DPO\nbased on pair-wise rankings of proposals. For infeasi-\nble proposals, we assign -\u221e to their L to effectively\nfilter them out from the ensembles.\n\u2022 Easy/Hard Problem Determination: We gauge prob-\nlem difficulty by setting a quality threshold and measur-\ning the frequency of generated feasible and qualified\nproposals rather than answer accuracy rates.\n\u2022 TUA: For hard problems, the LLM adopts a more\ncautious approach, generating and evaluating multi-\nple proposals with tools (within a budget cap) before\nselecting the best. This process naturally uses the ex-\npanded trace definition t', in contrast to MCQs where\na single tool call suffices.\nWe denote the model trained with our original approach as\n\"Ours\" and the one with preference learning as \u201cOurs-RL\u201d."}, {"title": "6. Conclusion and Future Works", "content": "We introduced a novel two-component fine-tuning approach\nto enhance Large Language Models (LLMs) in solving\nscientific problems of varying complexity. Our approach\nequips LLMs with the ability to intelligently choose be-\ntween using appropriate tools or conducting basic reasoning\nindependently by assessing problem difficulty using their\ninternalized knowledge, resembling human expert problem-\nsolving strategies. Experiments across diverse datasets\ndemonstrate that our fine-tuning method significantly im-\nproves a smaller base model's performance, enabling it to\nsurpass larger models such as GPT-40 and Claude-3.5. On\naverage, our fine-tuned models achieve a 28.18% increase in\nanswer accuracy and a 13.89% improvement in tool usage\nprecision across all datasets.\nWe expect our method to serve as a paradigm and foundation\nfor creating reliable AI scientific assistants, and we note sev-\neral promising directions for future investigation: Our cur-\nrent approach requires domain-specific fine-tuning, future\nresearch could explore methods for unifying cross-domain\ntraining in related scientific fields. Incorporating step-wise\nadaptive tool utilization, i.e., adaptive decision-making on\ntool usage at each step, could significantly reduce human\npreprocessing workload. Finally, expanding our method to\nhandle multi-modal inputs and outputs would broaden its\napplicability to settings where data extends beyond textual\nformats."}, {"title": "A. Dataset Details", "content": "We utilize two existing public datasets, MATH and SciBench, alongside four custom scientific datasets that we developed:\nMujoco, Partial Differential Equations (PDEs), Climate Science, and Epidemiology. Below, we provide detailed descriptions\nof the datasets, along with the tools employed to construct and evaluate them."}, {"title": "A.1. Dataset Statistics", "content": "Table 5 shows the statistics of the seven datasets used in our experiments. For our custom datasets (Mujoco, PDE, Climate,\nand Epidemiology), we show the number of scenarios and question templates used to generate the problems. The existing\ndatasets (MATH and SciBench) are from established benchmarks that do not provide information about scenarios and\ntemplates."}, {"title": "A.2. Dataset Components", "content": "A.2.1. MUJOCO\nWe developed the Mujoco dataset to address problems in rigid- and soft-body dynamics. This dataset is based on the Mujoco\nphysics engine (Todorov et al., 2012), which simulates realistic physics scenarios. Previous work introduced a dataset\ncomprising 39 qualitative questions and trained LLMs to solve them using MuJoCo simulations. However, this benchmark\nhas proven to be too simplistic for current models, which can achieve 100% accuracy with ease. To address this limitation,\nwe have developed a new dataset consisting of 8 distinct scenarios of different complexity based on a public turtorial 1. Each\nscenario contains an average of 14.5 adjustable parameters, including variables such as the initial position and velocity of\nobjects, time constants, damping ratios, friction coefficients, and the gravitational acceleration of the environment.\nA.2.2. PDE (PARTIAL DIFFERENTIAL EQUATION)\nThe PDE dataset focuses on solving partial differential equations in fields such as heat transfer, chemical engineering, and\npopulation dynamics. We wrote 1-D and 2-D partial differential equation solvers for the diffusion process, which can be set\nwith different variables like diffusion coefficient and size of the field, and different kinds of initial situations and boundary\nsituations with different parameters.\nA.2.3. CLIMATE\nThe Climate Science dataset comprises problems related to predicting earth surface temperature changes based on climate\nscenarios. The dataset is built using a neural surrogate model (Niu et al., 2024) that integrates data across multiple fidelity\nlevels for robust climate modeling. The model utilizes 12 climate driver variables as input, encompassing total emissions\nof greenhouse gases (CO2, CH4) and the first five principal components of global aerosol gas (BC, SO2) distributions,\nderived from a 72x96 global grid. The output predicts air temperature 2 meters above the Earth's surface at a global\nscale. The model spans historical data from 1850-2015 and projects future scenarios from 2015 to 2100 under four Shared\nSocioeconomic Pathways (SSPs): ssp126, ssp245, ssp370, and ssp585. These scenarios range from sustainable development\nwith low challenges to mitigation and adaptation (ssp126) to fossil-fueled development with high challenges to mitigation\nand adaptation (ssp585), representing a spectrum of potential future climate states and associated societal responses.\nA.2.4. EPIDEMIOLOGY\nThe Epidemiology dataset focuses on simulating disease spread and predicting epidemiological states over time. This dataset\nis based on a state-of-the-art surrogate model (Wu et al., 2023) that predicts disease progression using multi-dimensional\ninput features. For the California epidemic scenario, the input consists of two components: 1. county-level data for 58\ncounties, including 24 features per county per day over 28 days, 2. 10 initial state-level features. The output predicts 10\nstate-level features for each of the next 28 days.\nA.2.5. \u039c\u0391\u03a4\u0397\nMATH (Hendrycks et al.) is a widely used benchmark that consists of high-school-level mathematics competition problems.\nThe dataset covers various topics such as algebra, geometry, and number theory, and is divided into five difficulty levels. It\nremains challenging compared with another renowned math dataset GSM8K (Cobbe et al., 2021), where current 7B LLMS\nalready achieve over 80% accuracy. Following previous work (Qian et al., 2023), we utilize problems from the MATH test\nset with definite numerical answers to evaluate our methods.\nA.2.6. SCIBENCH\nSciBench (Wang et al., 2024b) is a collegiate-level benchmark that includes scientific problems in fields such as Mathematics,\nPhysics, and Chemistry. Like MATH, the problems are numerical and focus on real-world scientific applications. We use\nthe SciBench dataset to evaluate models on complex numerical problems."}, {"title": "A.4. Prompt Examples", "content": "Pn for our custom-created datasets:\nAnswer the following question. Your answer should be in the following format:\nSolution: <Your solution process>\nAnswer: <Your answer, one of A/B/C/D>\nQuestion: {{question}}\nPi for our custom-created datasets:\nGiven the following functions, please respond with a JSON for a function call with its\nproper arguments that best answers the given prompt.\nRespond in the format {\"name\": function name, \"parameters\": dictionary of argument name\nand its value}. Do not use variables.\n{{functions}}\nIf you don't know the answer, you can use the tool to help you. If you can answer the\nproblem without the tool, answer the problem directly.\nQuestion: {{question}}\nPn for SciBench and MATH:\nAnswer the following question. Your answer should be in the following format:\nSolution: <Your solution process>\nAnswer: <Your answer, a pure number>\nQuestion: {{question}}\nPi for SciBench and MATH:\nPlease answer the following question. You can write code to solve the problem or give the\nanswer directly. When answering, you should first give the Solution then give the Answer\nThe answer should be a pure number without LaTeX or unit signs. Each time, you should\neither write code or answer the question. Your final answer should be in one of the\nfollowing formats:\nIf you want to write code, your answer should be in the following format:\nThought: <Your thought>\nAction: write_and_run_code\nCode:\n'python\n<Your code>\nIf you want to answer the question, you should answer in the following format:\nThought: <Your thought>\nAction: answer_question\nSolution: <Your solution>\nAnswer: <Your answer>\nQuestion: {{question}}"}, {"title": "A.5. Solution Examples", "content": "A.5.1. EXAMPLE FROM THE CLIMATE DATASET\nProblem:\nQuestion: How much will the temperature of Ewo in 2068 under ssp126 change if the emission of CO2 is\ndecreased by -25%?\n(A) -0.13081711\n(B) -0.16986465\n(C) -0.09639389\n(D) -0.05745936\nCorrect Answer: B\nProblem-solving without tool usage:\nTo answer this question", "model": "Select a suitable climate model that has been run under the ssp126 scenario.\n2. Select a location: Identify the location of Ewo (presumably a small town in the Democratic Republic of\nCongo).\n3. Run the climate model: Run the climate model for the year 2068 under the ssp126 scenario with a -25%\nreduction in CO2 emissions.\n4. Extract the temperature change: Extract the temperature change at the location of Ewo for the year 2068 from\nthe climate model output.\n5. Compare with options: Compare the extracted temperature change with the provided options to select the\ncorrect answer.\nHowever", "answer.\nSolution": "Run the climate model for the year 2068 under the ssp126 scenario with a -25% reduction in CO2\nemissions and extract the temperature change at the location of Ewo.\nTherefore", "usage.\nAssistant": "Call query_lat_and_lon with parameters {'city_name': 'Ewo'"}, "nFeedback from query_lat_and_lon: latitude: -0.8742, longitude: 14.8167.\nAssistant: Call diy_greenhouse with parameters {'longitude': 14.8167, 'latitude': -0.8742, 'setting': 'ssp126',\n'year': 2068, 'delta_CO2': -0.25, 'delta_CH4': 0}\nFeedback from diy_greenhouse: The temperature is 27.52265167236328.\nAssistant: Call future_temperature"]}