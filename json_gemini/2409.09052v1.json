{"title": "OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in Computed Tomography", "authors": ["Youzhu Jin", "Yichen Zhang"], "abstract": "Multimodal large language models (MLLMs) have achieved significant success in the general field of image processing. Their emerging task generalization and free-form conversational capabilities can greatly facilitate medical diagnostic assistance, helping patients better understand their conditions and enhancing doctor-patient trust. Computed Tomography (CT) is a non-invasive imaging technique used to capture the internal mechanisms of a patient's condition and is widely utilized. However, in past research, the complex textural features of this imaging data have made accurate interpretation by algorithms challenging, impeding the performance of general LLMs in diagnostic assistance. To address this, we developed OrthoDoc, a MLLM designed for CT diagnostics. OrthoDoc is trained on 120,000 CT images and diagnostic reports and includes a Retrieval-Augmented Generation(RAG) module capable of effectively mitigating model hallucinations. This module is informed by extensive medical literature, textbooks, and explanatory data. Thus, OrthoDoc not only processes complex CT images but also stores, understands, and reasons over medical knowledge and language. In extensive experiments, OrthoDoc outperforms commercial models led by GPT-4, demonstrating superior diagnostic capabilities and accuracy. Specifically, OrthoDoc significantly surpasses existing models in the diagnosis of common orthopedic conditions such as fractures, arthritis, and tumors. Additionally, OrthoDoc exhibits robust generalization and stability when handling rare and complex cases.", "sections": [{"title": "I. INTRODUCTION", "content": "With the rapid advancement of artificial intelligence technology, there is a growing research interest in using AI systems to assist in medical image diagnostics. [1]-[4] Ideally, AI models can handle multiple modalities of medical data, including patients' basic vital signs, pathological slide data, and computed tomography (CT) data. These systems aim to assist doctors in real-world diagnostics by engaging in natural language dialogue, thus providing comprehensive support across various medical scenarios.\nTraditional AI models for medical image interpretation often fall short of the precision required for effective diagnostic assistance. These models are typically limited to specific tasks such as medical image classification and segmentation, which restricts their utility in broader diagnostic contexts.\n[5]-[7] Moreover, they often lack the capability to engage in free-form conversational interactions, which is crucial for nuanced medical consultations. Multimodal large language models (MLLMs), exemplified by ChatGPT, possess powerful natural language understanding and generation capabilities. Their ability to handle multimodal data-integrating text, images, and other forms of data opens up new possibilities for advanced diagnostic assistance.\nHowever, the intelligent interpretation of CT images presents unique challenges. Unlike general image recognition tasks, medical image analysis requires models to have extensive prior knowledge of complex medical images. Specifically, fractures exhibit distinct features on CT images, and models must not only accurately identify these features but also discern subtle differences among various types of fractures and understand their clinical significance. Traditional models often lack this depth of medical knowledge, leading to suboptimal performance in real-world applications.\nAdditionally, the textual information used in CT medical diagnostics is highly specialized and complex. In a series of zero-shot experiments, representative open-source and commercial models frequently encountered significant hallucination issues. These models often generated inaccurate or misleading content when dealing with medical domain-specific terminology and complex diagnostic reports. Such inaccuracies pose significant risks to medical decision-making, highlighting the need for models that can generate precise and reliable outputs.\nTo address these challenges, we developed OrthoDoc, a multimodal large model specifically designed for CT diagnostics. OrthoDoc is trained on a diverse dataset comprising 120,000 CT images and their corresponding diagnostic reports. This extensive training enables the model to master a wide range of medical imaging features and acquire comprehensive diagnostic knowledge. We incorporated medical professional texts from a broad array of data sources, particularly those rich in orthopedic and diagnostic knowledge. Additionally, we integrated an automated knowledge graph construction and retrieval component. This enhancement allows the model to support its text reasoning process with a robust RAG module, effectively eliminating severe hallucinations.\nOrthoDoc is capable of processing complex CT images and generating detailed diagnostic reports in natural language, offering valuable diagnostic information and treatment recommendations for physicians. This multimodal capability allows OrthoDoc to excel in real clinical environments, significantly improving diagnostic accuracy and efficiency. In a series of experiments, OrthoDoc outperformed existing open-source and commercial models, achieving over 91% accuracy in identifying common orthopedic conditions such as fractures, arthritis, and tumors. Moreover, OrthoDoc demonstrated exceptional generalization ability, effectively handling rare conditions and complex cases, further proving its practical utility in clinical applications.\nIn summary, the main contributions of this paper are as follows:\n\u2022 Enhanced Diagnostic Accuracy Through Multimodal Integration: OrthoDoc employs a comprehensive multimodal approach by integrating 120,000 CT images with corresponding diagnostic reports. This integration enables effective handling of complex CT images, enhancing diagnostic accuracy.\n\u2022 Mitigation of Text Generation Hallucinations in Diagnostic Models: To address common hallucination issues in traditional models, OrthoDoc introduces a RAG module. This module assists in generating precise diagnostic reports and reduces misleading outputs, particularly when handling complex or rare cases."}, {"title": "II. RELATED WORK", "content": "In recent years, multimodal large models, particularly vision-language models, have garnered extensive attention in the field of medical diagnosis [9]\u2013[12]. These models combine image and text data through sophisticated neural network architectures to achieve cross-modal information fusion. For instance, Vision-Language Pre-training (VLP) models, by jointly learning from large-scale medical images and clinical records, have significantly enhanced the understanding of medical images and the quality of medical text generation [1]\u2013[4], [8]. Research indicates that these models can effectively extract useful information from complex medical images and align it semantically with clinical text, thereby providing more accurate support for medical diagnosis.\nFurthermore, recent work has started focusing on improving the performance of vision-language models in medical diagnosis by incorporating RAG [13]\u2013[17] and CoT techniques. RAG technology combines the model's generative capabilities with an information retrieval module, allowing it to quickly retrieve relevant information and generate more accurate diagnostic suggestions from a large volume of medical literature and case studies. This approach effectively reduces the model's knowledge gaps, providing more targeted support in complex medical scenarios. Meanwhile, CoT technology enhances the model's logical consistency and coherence in medical text generation by guiding it through step-by-step reasoning during the generation process. The introduction of these techniques not only improves the model's accuracy in medical diagnosis but also offers new solutions for addressing the details and reasoning complexity in medical text generation."}, {"title": "III. ORTHODOC", "content": "In this section, we will introduce the two-phase specialization process of OrthoDoc: multimodal fine-tuning,the RAG module and the CoT module. The overall training pipeline and capabilities of OrthoDoc are illustrated in the figure below.\nMultimodal Fine-tuning\nThe multimodal fine-tuning phase of OrthoDoc involves two distinct stages: (1) multimodal training using CT image-text pairs and (2) training the text encoder on an instruction-tuning dataset. Each stage plays a crucial role in enhancing OrthoDoc's ability to accurately interpret medical images and generate precise diagnostic reports.\n1) Multimodal Training Using CT Image-Text Pairs: Data Preparation The first stage focuses on multimodal training using a dataset of 120,000 CT images paired with detailed diagnostic reports. These reports contain annotations of key features, diagnoses, and treatment recommendations, offering a comprehensive context for each image. The dataset covers a wide range of conditions, from common orthopedic issues like fractures, arthritis, and tumors to rare and complex cases.\nFeature Extraction In this stage, OrthoDoc leverages ResNet-101 for extracting features from the CT images. These ResNet are pre-trained on large-scale image datasets and then fine-tuned on our specific CT image dataset. The fine-tuning process adjusts the weights of the ResNet to better capture the unique characteristics of medical images, such as bone density variations and fracture patterns.\nText Embedding Simultaneously, the diagnostic reports are processed using advanced natural language processing (NLP) techniques. We utilize transformer-based models, BERT, to generate embeddings for the diagnostic texts. These embeddings capture the semantic meaning of the reports, including medical terminologies and context-specific information critical for accurate diagnosis.\nMultimodal Integration The extracted image features and text embeddings are integrated into a unified representation through a cross-modal attention mechanism. This mechanism aligns relevant information from both modalities, enabling OrthoDoc to correlate visual features in the CT images with textual descriptions in the diagnostic reports. This integration significantly enhances the model's understanding of the medical context.\nFine-tuning Process OrthoDoc undergoes supervised training using the prepared dataset, with the objective of minimizing a loss function that measures the discrepancy between the model's predictions and the actual diagnostic annotations. Techniques such as gradient descent and backpropagation are employed to iteratively update the model parameters, gradually improving diagnostic accuracy.\n2) Training the Text Encoder on Instruction-Tuning Dataset: Instruction-Tuning Dataset The second stage involves training the text encoder on an instruction-tuning dataset. This dataset comprises a variety of medical instructions and queries, paired with corresponding diagnostic texts and responses. The goal is to enhance OrthoDoc's ability to understand and generate natural language responses in a medical context.\nText Encoder Fine-tuning The text encoder, which has been previously trained on the diagnostic reports, is further fine-tuned on the instruction-tuning dataset. This process involves adjusting the encoder to better understand and generate text based on specific medical instructions and queries. Techniques such as masked language modeling and sequence-to-sequence learning are used to refine the encoder's capabilities.\nGraphRAG for OrthoDoc\nIn addressing the challenge of hallucinations in medical text generation, we employ a sophisticated approach using GraphRAG specifically adapted for orthopedic documentation. The integration of GraphRAG is essential for enhancing the accuracy and relevance of the generated medical texts by leveraging a RAG mechanism alongside specialized medical knowledge. [18]\nGraphRAG significantly mitigates the risk of hallucinations\u2014instances where models generate inaccurate or misleading information\u2014by incorporating a robust retrieval component within the text generation framework. This is crucial in the field of orthopedics, where precision and reliability are paramount. GraphRAG ensures that the generated content is not only accurate but also contextually appropriate, addressing the specific needs of orthopedic documentation. The model utilizes a curated database of authoritative medical texts to guide its generation process, including key resources such as Apley's System of Orthopaedics and Fractures (9th ed), Orthopedic Textbook, Introduction to Orthopedic Anatomy, and Osteology (Standardized training for residents). These texts provide a comprehensive foundation of orthopedic knowledge, ensuring that the model's outputs are grounded in current and relevant information.\nThe GraphRAG approach begins with retrieving pertinent information from this specialized medical knowledge base based on the input query. This retrieval step ensures that the generated text is supported by up-to-date and relevant data. Subsequently, GraphRAG utilizes a graph-based method to represent and integrate the relationships between various medical concepts and conditions. This contextual graph helps the model comprehend the intricate connections within orthopedic knowledge, thereby enhancing the coherence and accuracy of the generated text.\nDuring the text generation phase, the model integrates the retrieved information and contextual understanding from the graph. Before finalizing the output, it reassesses the generated text to ensure it aligns with professional medical standards and accurately reflects the retrieved data. This process not only improves the fidelity of the generated content but also ensures that the output provides valuable insights and recommendations based on the latest medical knowledge.\nIn summary, GraphRAG's application in our study involves a sophisticated inference process where the model retrieves relevant information from authoritative medical texts, constructs a contextual graph to represent the relationships between medical concepts, and generates text that is both accurate and contextually appropriate. This approach addresses the challenge of hallucinations in medical text generation, delivering reliable and actionable orthopedic documentation.\nCoT for OrthoDoc\nIn developing a sophisticated text generation model for orthopedic documentation, implementing Chain-of-Thought (CoT) techniques is essential [19]. CoT allows the model to produce detailed and coherent long-form text reports by structuring its reasoning process effectively. This approach enables the model to generate comprehensive reports that reflect a thorough exploration of a patient's condition, akin to the methodical process a physician might use during a clinical discussion.\nA well-structured orthopedic report begins with a thorough patient background section. This part provides essential details about the patient's demographics, medical history, and presenting complaints. It sets the stage for understanding the context of the patient's condition by including information such as age, gender, occupation, and any pre-existing medical conditions relevant to the current orthopedic issue.\nFollowing the background, the clinical presentation section details the patient's current symptoms, including their onset, progression, and any notable physical examination findings. This section is crucial for describing the nature of the pain, mobility issues, and functional limitations the patient is experiencing. Accurate and detailed descriptions in this section help in setting a clear understanding of the patient's current state.\nThe diagnostic process section outlines the steps taken to diagnose the condition, explaining the rationale behind selecting specific diagnostic tests or imaging studies. It should include the results from these tests, such as X-rays, MRIs, or CT scans, and discuss their implications for the diagnosis. This section helps in documenting the process of arriving at a diagnosis and the evidence supporting it.\nOnce the diagnostic results are established, the diagnosis and assessment section clearly states the diagnosis, supported by the findings from the diagnostic process. If necessary, it includes a differential diagnosis, providing reasoning for ruling out other possible conditions. This section ensures that the diagnosis is well-supported and justified.\nThe treatment plan section details the proposed management strategies, covering both conservative and surgical options if applicable. It should outline the rationale behind the chosen treatment strategy, including the expected benefits, potential risks, and anticipated outcomes. Additionally, it should address any recommended follow-up or rehabilitation protocols to ensure comprehensive management of the condition.\nPatient education and recommendations follow, providing guidance on managing the condition, understanding treatment options, and any lifestyle modifications or precautions necessary. This section is vital for ensuring that the patient is well-informed about their condition and the steps they need to take.\nFinally, the conclusion summarizes the key points of the report, reinforcing the diagnosis and treatment plan. It may also highlight any next steps or additional consultations required, ensuring that the report provides a clear and complete overview of the patient's condition and management plan.\nTo generate such a detailed and coherent long-form report, the model must follow a structured CoT process. This process starts with gathering all relevant patient data and medical history, which informs the content of the report. The model then organizes this information into a logical structure corresponding to the report sections, ensuring a natural flow of content. For each section, the model engages in detailed reasoning, explaining choices and supporting the text with relevant information. After developing the individual sections, the model synthesizes the information to produce a cohesive report, integrating details from different parts to create a unified narrative. Finally, the model reviews and refines the report to ensure it meets medical standards and effectively communicates the necessary information.\nAll generated results will be collected and organized through an automated LaTeX pipeline. This pipeline will systematically format the text into a well-designed report template, ensuring that the final document is professionally presented and adheres to high standards of readability and clarity. By integrating the automated LaTeX pipeline, we ensure that the final reports are not only comprehensive and accurate but also consistently formatted and ready for clinical use."}, {"title": "IV. EVALUATION", "content": "In this section, we evaluate our model's performance across several critical dimensions: its superiority in orthopedic CT diagnostics, the generalization of its multimodal capabilities, the effectiveness of the RAG module, the efficacy of the CoT module, and the robustness of large models in handling counterfactual scenarios. Each aspect is assessed through specific experiments designed to provide comprehensive insights into the model's strengths and limitations.\nEvaluating the Model's Superiority in Orthopedic CT Diagnostics\nTo rigorously assess the model's diagnostic accuracy and its superiority in interpreting orthopedic CT scans, we designed a comprehensive experiment involving a comparative analysis with several leading multimodal large models. The dataset used consists of orthopedic CT images annotated with known diagnoses, including conditions such as fractures, dislocations, and degenerative diseases. Each image is labeled and verified by expert radiologists, providing a solid basis for evaluation.\nOur experiment focuses on two primary tasks: condition identification and report generation. In condition identification, the model is evaluated on its ability to accurately detect and classify various orthopedic conditions present in the CT images. For report generation, the model must produce detailed diagnostic reports based on its findings.\nWe compared our model against five prominent multimodal large models: MedVision Transformer (MedViT) [20], MediBERT [21], PathBERT [22], and ClinicalBERT [23]. The performance of these models is assessed using metrics such as accuracy, sensitivity, specificity, and F1-score.\nFor the experimental procedure, each model, including ours, is trained using the orthopedic CT dataset with a focus on diagnosing orthopedic conditions. Subsequently, the models generate diagnostic predictions and reports for a separate test set of CT images. These predictions and reports are evaluated based on accuracy, sensitivity, specificity, and F1-score. Statistical analyses are conducted to determine any significant performance differences. The results are then organized into detailed tables and charts to provide a clear comparison of performance metrics across all models. This approach ensures a comprehensive evaluation of our model's diagnostic capabilities and highlights its effectiveness relative to other state-of-the-art systems."}, {"title": "V. CONCLUSION", "content": "The research achievements of OrthoDoc represent a milestone in the field of multimodal large language models (MLLMs). Through training on 120,000 CT images and their diagnostic reports, OrthoDoc not only excels in image processing but also demonstrates exceptional capabilities in medical knowledge, language understanding, and reasoning. In extensive experiments, OrthoDoc has surpassed existing commercial models, including GPT-4, in diagnostic accuracy and the quality of report generation, especially in diagnosing common orthopedic diseases such as fractures, arthritis, and tumors. These achievements not only showcase OrthoDoc's potential in medical image diagnosis but also provide new directions for future medical assistance technology.\nThe innovation of the OrthoDoc model lies in its integration of a RAG module. This module, supported by extensive medical literature, textbooks, and explanatory data, effectively reduces hallucinations and improves the accuracy of text generation. Additionally, OrthoDoc employs multimodal fine-tuning and CoT techniques, further enhancing the coherence and completeness of medical text generation, making it more practical in clinical applications. The application of these technologies not only improves OrthoDoc's diagnostic capabilities but also provides valuable experience for the development of other medical assistance systems.\nOrthoDoc's robustness and generalization ability in handling rare and complex cases demonstrate its potential application value in real clinical environments. The model's high accuracy and deep understanding of medical terminology and diagnostic processes enable it to provide doctors with detailed diagnostic information and treatment recommendations, significantly improving the efficiency and accuracy of diagnoses. These characteristics suggest that OrthoDoc will play an important role in future clinical practice, especially in assisting doctors with complex diagnoses and treatment decisions.\nDespite OrthoDoc's outstanding performance in current research, there is still room for further improvement and expansion. Future work can focus on expanding the model's training dataset to cover a wider range of medical fields and case types. Continuously optimizing the RAG and CoT modules to enhance the model's adaptability and flexibility in more complex clinical scenarios is also crucial. Additionally, exploring the integration of OrthoDoc with other medical assistance technologies, such as wearable devices and telemedicine platforms, is a potential direction for future research. These improvements will further enhance OrthoDoc's practicality and impact, making it an indispensable auxiliary tool in the medical field."}]}