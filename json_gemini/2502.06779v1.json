{"title": "KARST: Multi-Kernel Kronecker Adaptation with Re-Scaling Transmission for Visual Classification", "authors": ["Yue Zhu", "Haiwen Diao", "Shang Gao", "Long Chen", "Huchuan Lu"], "abstract": "Fine-tuning pre-trained vision models for specific tasks is a common practice in computer vision. However, this process becomes more expensive as models grow larger. Recently, parameter-efficient fine-tuning (PEFT) methods have emerged as a popular solution to improve training efficiency and reduce storage needs by tuning additional low-rank modules within pre-trained backbones. Despite their advantages, they struggle with limited representation capabilities and misalignment with pre-trained intermediate features. To address these issues, we introduce an innovative Multi-Kernel Kronecker Adaptation with Re-Scaling Transmission (KARST) for various recognition tasks. Specifically, its multi-kernel design extends Kronecker projections horizontally and separates adaptation matrices into multiple complementary spaces, reducing parameter dependency and creating more compact subspaces. Besides, it incorporates extra learnable re-scaling factors to better align with pre-trained feature distributions, allowing for more flexible and balanced feature aggregation. Extensive experiments validate that our KARST outperforms other PEFT counterparts with a negligible inference cost due to its re-parameterization characteristics.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, large pre-trained foundational models have exhibited strong representation capabilities in various downstream domains. They are trained on extensive in-domain or web-scale data, enabling them to generalize effectively across various downstream tasks. However, as their model size grows, fully fine-tuning them for specific tasks becomes increasingly expensive and impractical. To address it, parameter-efficient transfer learning (PETL) methods have gained traction, significantly reducing the computational and storage costs of fine-tuning large models. Among them, Partially-tuning approaches focus on training a subset of pre-trained parameters for downstream domains. Besides, Prompt-based methods attempt to integrate several trainable prompt tokens into model inputs, while Adapter-based studies try to insert several multi-layer perceptrons (MLPs) into transformer blocks, either after or in parallel with multi-head self-attention and feedforward modules for domain adaptation.\nIn contrast, LoRA stands out as a popular strategy with a re-parameterization property, allowing seamless integration into pre-trained models and bringing no extra computational costs during inference. Inspire by this, the subsequent research has introduced advanced techniques like factorized bilinear matrices, compactor decomposition, Kronecker products, and tensor factorization to further enhance adaptation capabilities. Other studies explore multiple parallel layers to enhance the transfer representations. However, they suffer from two key issues: (1) how to create more compact and diverse feature spaces for complex domain adaptation, and (2) how to maintain consistency in intermediate distributions with pre-trained parameters.\nTo address this, we introduce a novel approach called Multi-Kernel Kronecker Adaptation with Re-Scaling Transmission (KARST) for visual classification. Specifically, our KARST leverages adaptive and flexible combinations to create compact mapping subspaces and build powerful feature patterns. Additionally, learnable re-scaling factors are employed to align diverse feature representations with the feature distributions of pre-trained parameters. Extensive experiments validate that our KARST not only outperforms existing PETL approaches and fully fine-tuning strategy, but also exhibits strong capability across different network backbones with minimal inference costs due to its re-parameterization characteristics."}, {"title": "II. METHODOLOGY", "content": "Mathematically, the Kronecker product extends the outer product from vectors to matrices and represents the tensor product in the standard basis, denoted by the symbol . Given the matrix $A \\in \\mathbb{R}^{P_1 \\times q_1}$ and the matrix $B \\in \\mathbb{R}^{P_2 \\times q_2}$, it transforms $A \\otimes B$ into a new matrix with $(P_1 \\cdot P_2) \\times (q_1 \\cdot q_2)$. Note that each element of the Kronecker product $A \\otimes B$ can be represented as follows:"}, {"title": "A. Preliminary", "content": "Existing PETL strategies typically freeze pre-trained weights during fine-tuning to avoid overfitting and preserve the generalized patterns learned during the pre-training phase. However, the updated weights alert the value distribution of intermediate features compared to the pre-training phase. The misalignment between pre-trained layers and their corresponding input space limits the model's ability to adapt effectively to specific downstream tasks. Therefore, it is crucial to adjust the merging features adaptively to better align with the pre-trained mapping subspaces.\nTo address this challenge, we introduce a simple yet effective strategy to adjust the merged features from the pre-trained and Kronecker layers using two additional re-scaling parameters. Specifically, the feature representations are processed by performing a dot product with a scale factor and then adding a shift factor. Both of these parameters are independent of the input features, ensuring that the feature distribution can be effectively modified to align with the downstream dataset. This can be formulated as follows:\nNotably, we initialize $s_1$ and $s_2$ as zero vectors to ensure that the initial state of the pre-trained backbone remains unchanged. This preserves the learned representations of the pre-trained stage and prevents potentially unstable optimization of the model convergence during fine-tuning. Since it is entirely linear, it displays an advantage of re-parameterization characteristics. This means that the channel-wise scaling and shifting operations can be effectively integrated into the preceding linear layers of the pre-trained network, thereby maintaining the simplicity and efficiency of the model architecture."}, {"title": "B. Multi-Kernel Kronecker Adaptation", "content": "The parameters of the pre-trained ViT and final model are defined as Wo and Wt. Here AW = Wt - Wo denotes the updated weight during the fine-tuning process. Existing works attempt to decompose AW into different forms to reduce the number of trainable parameters. For example, LoRA decompose the \u2206W into two matrics A \u2208 RDinor and B\u2208 RrxDout with the rank of r, while the Kronecker product utilizes Eq. (1) as AW to further enhance the adaptation capability. However, it increases the number of learnable parameters and creates only a single subspace with poor capabilities for diverse and complex data distributions. To address this, we propose multi-kernel Kronecker products in Fig. 1 with multiple complementary subspaces, which explore the limit of single mapping space and implicitly foster more adaptive representations across the subspaces.\nWe first decompose AW into N kernels of Kronecker space, which are defined as follows:\nNotably, we use random Gaussian initialization for the matrics Ai, Ci and zero initialization for the matrix B\u2081. This ensures that AW is zero at the beginning of training stage, so the initial state of the transformer block remains unchanged. As a result, our module can be inserted into any part of a pre-trained backbone without disrupting its initial status."}, {"title": "C. Re-Scaling Transmission", "content": "Existing PETL strategies typically freeze pre-trained weights during fine-tuning to avoid overfitting and preserve the generalized patterns learned during the pre-training phase. However, the updated weights alert the value distribution of intermediate features compared to the pre-training phase. The misalignment between pre-trained layers and their corresponding input space limits the model's ability to adapt effectively to specific downstream tasks. Therefore, it is crucial to adjust the merging features adaptively to better align with the pre-trained mapping subspaces.\nTo address this challenge, we introduce a simple yet effective strategy to adjust the merged features from the pre-trained and Kronecker layers using two additional re-scaling parameters. Specifically, the feature representations are processed by performing a dot product with a scale factor and then adding a shift factor. Both of these parameters are independent of the input features, ensuring that the feature distribution can be effectively modified to align with the downstream dataset. This can be formulated as follows:\nNotably, we initialize $s_1$ and $s_2$ as zero vectors to ensure that the initial state of the pre-trained backbone remains unchanged. This preserves the learned representations of the pre-trained stage and prevents potentially unstable optimization of the model convergence during fine-tuning. Since it is entirely linear, it displays an advantage of re-parameterization characteristics. This means that the channel-wise scaling and shifting operations can be effectively integrated into the preceding linear layers of the pre-trained network, thereby maintaining the simplicity and efficiency of the model architecture."}, {"title": "III. EXPERIMENTS", "content": "We conduct comprehensive experiments to prove KARST's effectiveness on VTAB-1K and few-shot datasets:\nVTAB-1k Benchmark. It consists of 19 visual classification datasets. (1) Natural group with classical vision problems contains natural images captured with standard cameras, including Caltech101, CIFAR100, DTD, Flowers102, Pets, Sun397, and SVHN; (2) Specialized group contains images captured from remote-sensing and medical device, including Resisc45, EuroSAT, Patch Camelyon, and Diabetic Retinopathy; (3) Structured group reports structured comprehension of a scene, e.g. object counting or 3D depth prediction, including Clevr, dSprites, SmallNORB, DMLab, and KITTI.\nFew-shot Benchmark. We further conduct experiments on five standard fine-grained datasets under few-shot settings, including FGVC-Aircraft, Oxford-Pets, Food-101, Stanford Cars, and Oxford-Flowers102. These datasets encompass fine-grained classes from five categories in real-world scenarios: aircrafts, pets, food, cars, and flowers. Following previous work, we evaluate various methods in 1, 2, 4, 8, 16-shot settings for comparison."}, {"title": "A. Dataset Descriptions", "content": "In this section, we select the vision transformer (ViT- B/16) pre-trained on the ImageNet-21K dataset as the baseline unless otherwise specified. Besides, we also evaluate KARST on the Swin Transformer (Swin-B) to validate its application capability across diverse architectures."}, {"title": "B. Experiments on VTAB-1K Benchmark", "content": "Baselines. Using ViT-B/16 and Swin-B models, we compare our KARST with fully fine-tuning (Full), partially-tuning with specific task head (Linear), and several competitive strategies, including BitFit, VPT, LST, LoRA, AdaptFormer, NOAH, FacT, SSF, DTL, HEAT, GLORA, and Sparse-Tuning. Note that we set r < 32 for FacT, r = 0.9 for Sparse-Tuning, and r \u2264 8 for LoRA and KARST. The prompt length I for VPT is used from the original paper. Besides, the stacking dimension m and kernel number N are 8 and 2, respectively.\nResults. The results using ViT-B and Swin-B are summarized in TABLE I and TABLE II, respectively. We can discover that KARST produces the best performance with similar trainable parameter usage compared to existing state-of-the-art PETL methods. Notably, KARST can achieve 78.1% and 78.6% across different network architectures, surpassing the best competitors Sparse-Tuning and DTL on the 19 datasets."}, {"title": "C. Experiments on Few-shot Benchmark", "content": "Baselines. For few-shot learning, we employ the ViT-B/16 model pre-trained on ImageNet-21K as the baseline. We compare our KARST with LoRA, FacT, NOAH, Adapter, Adapter-Former, VPT and DTL across five fine-grained datasets. We maintain their official hyper-parameter configurations and report the averaging results over three runs with different random seeds. Note that we set the rank r < 8, the stacking dimension m = 8, and kernel number N = 2, respectively.\nResults. From Fig. 2, we can observe that our proposed KARST consistently achieves the best performance across five fine-grained datasets. Notably, KARST significantly outperforms existing PETL strategies by a large margin in few-shot settings, highlighting its superior effectiveness and generalizability, particularly when limited data is available. This verifies the adaptability and capability of KARST to extract meaningful features even with minimal training samples."}, {"title": "D. Ablation Study", "content": "Effectiveness of Re-Scaling Transmission. In TABLE III, we evaluate the role of the re-scaling module. KA and RST denote multi-kernel Kronecker Adaptation and re-scaling transmission. We notice that averaging performance gains are 0.7% and 0.6% when our RST module works with FacT and our KA module. This shows our RST module can effectively mitigate misalignment issues inside PETL methods and transform intermediate features to better align with pre-trained patterns. The valid and consistent improvements validate the robustness and general applicability of our RST module."}, {"title": "Hyper-parameter of the kernel number.", "content": "Hyper-parameter of the kernel number. Fig. 3 demonstrate the influence of different kernel numbers for multi-kernel Kronecker Adaptation. The ViT-B/16 is adopted as the baseline on the VTAB-1K benchmark. We observe that as the number of kernels increases, the average accuracy improves. However, this also results in a corresponding rise in the number of fine-tuned parameters. After careful evaluation of the trade-off between performance and parameter efficiency, we selected a kernel number of 2 for our experiments."}, {"title": "IV. CONCLUSION", "content": "In this paper, we propose a novel approach called Multi-Kernel Kronecker Adaptation with Re-Scaling Transmission (KARST) for recognition tasks. Specifically, KARST leverages multiple kernel functions within Kronecker products to build compact mapping spaces, enhancing the model's representation capacity for robust adaptation. Additionally, a simple yet effective re-scaling strategy is employed to align the resulting features with the patterns and distribution of the pre-trained weights. Extensive experiments across 24 diverse datasets demonstrate the general effectiveness and broad applicability of KARST, which can also cooperate with existing PETL techniques to achieve better performance gains."}]}