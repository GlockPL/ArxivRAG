{"title": "Acting for the Right Reasons: Creating Reason-Sensitive Artificial Moral Agents", "authors": ["Kevin Baum", "Lisa Dargasz", "Felix Jahn", "Timo P. Gros", "Verena Wolf"], "abstract": "We propose an extension of the reinforcement learning architecture that enables moral decision-making of reinforcement learning agents based on normative reasons. Central to this approach is a reason-based shield generator yielding a moral shield that binds the agent to actions that conform with recognized normative reasons so that our overall architecture restricts the agent to actions that are (internally) morally justified. In addition, we describe an algorithm that allows to iteratively improve the reason-based shield generator through case-based feedback from a moral judge.", "sections": [{"title": "Introduction", "content": "The ultimate goal of building autonomous systems is, at least in many cases, to deploy them in real-world environments. Reinforcement learning (RL) has become a prevalent approach for training these systems. The main idea behind reinforcement learning is to reward and punish the agent for a specific behavior. RL has established itself as the tool of choice for agent-based sequential decision-making under uncertainty. In a sense, it is implemented instrumental rationality: RL agents learn the efficient means to achieve ends as encoded in the reward structure. However, concerning the prospect of real-world deployment of RL systems, it is requested that the actions these systems execute are also morally justifiable \u2013 and it remains a research question how this requirement is to be understood as well as whether and how it can be fulfilled within the RL framework. This article introduces the formal part of the authors' approach to this question. The philosophical part has been discussed elsewhere (cf. [2]). The evaluation of the approach is the subject of future work.\nBridge Setting To clarify the desideratum of moral justifiability, consider the following grid world, which we may think of as modeling a real-world setting with vastly reduced complexity (cf. Figure 1): There are two 'coastlines' of solid ground in the north and the south, connected via a narrow bridge. All other areas are water. An agent spawns randomly at the northern shore (on a field called a). Its goal is to deliver a package to some initially randomly picked field on the southern shore (called b) as fast as possible. Further assume that there are 'persons' wandering aimlessly across the map. They can fall off the bridge at random, or, if the agent enters a field on the narrow bridge where a person is standing, this"}, {"title": "", "content": "person is pushed off the bridge into one of the nearest water fields. If a person is in the water, they will drown after a certain time if not helped. The agent can execute the primitive actions north, east, west, south to move to a nearby field; further, it can idle; and, finally, it can pullOut a person in an adjacent water field.\nIf a person is in the water at risk of drowning, this is a morally relevant fact. It seems indisputable that the agent is supposed to save the person \u2013 the agent is expected to stop working towards its instrumental goal in order to follow its moral obligation. Similarly, if the shortest path from a to b would imply pushing a person on the narrow bridge into the water, the agent is expected to wait in front of the bridge until the person has made their way across. Now, consider a third constellation of morally relevant facts, where there is a drowning person as well as a person standing on the bridge, unluckily blocking the path from the agent to the drowning person. In this case, it is less obvious what the morally correct behavior is, i.e., the agent is faced with a moral dilemma.\u00b9 However, the agent is forced to make some decision between saving the drowning person and waiting in front of the bridge. We thus suggest putting the pressing, but unanswered question of what is morally right aside and instead requiring actions to be morally justifiable.\u00b2\nOur Contribution. One philosophy-backed way to ensure that the action an RL agent takes in such a situation is morally justifiable is to ground the agent's decision whether to wait in front of the bridge or save the drowning person on (good) normative reasons [2]. We therefore propose an extension of the RL architecture that integrates a reason-based moral decision-making process. For this, we include a module that derives moral obligations from a reason theory that includes normative reasons as well as an order among them and is based on the well-established formalization of reason-based moral decision-making proposed by John Horty. This module can be used to filter out actions (based on the actor's observations) that are not supported by the best reasons. Consequently, it yields a shield in the safe reinforcement learning (safe RL) sense. We thus call our module a reason-based shield generator. Further, we want to enable the agent to iteratively improve its reason theory. Therefore, we make it sensitive to case-based feedback that corrects the agent in its reasoning, i.e., feedback consisting of the information on what the agent should have done and the reason for that judgement. We call the instance that provides this feedback a moral judge. Because this form of reasoning and criticism comes naturally to humans, humans can easily take this role \u2013 they can directly communicate corrections to the agent in possible future implementations of our approach. However, we also propose a way to automate the process of giving feedback with regard to some first implementations. To this end, a moral judge can be integrated into the pipeline as an additional module. Assuming that the agent receives feedback such that it learns valid reasons, the agent learns to restrict itself by design to actions that are morally justified."}, {"title": "Related Work", "content": "A prevalent research approach focuses on teaching agents moral behavior by adjusting the reward mechanism. This approach builds on Multi-Objective Reinforcement Learning, where agents receive multiple"}, {"title": "Background", "content": ""}, {"title": "Reinforcement Learning, Labeled MDPs, and Shielding", "content": "Reinforcement learning is a machine learning technique used to solve sequential decision problems. Thereby, an agent interacts with the environment to learn which actions lead to the highest rewards. The environment is usually formalized as Markov decision processes (MDPs) [18], which is a tuple $(S,A, P, R, \\gamma)$ with state space S, an action space A, transition probabilities $P : S \\times A \\times S \\rightarrow [0, 1]$, rewards $R:S\\times A\\times S \\rightarrow R$, and a discount factor $\\gamma$. $P(s'|s,a)$ denotes the probability for transitioning to state s' when performing action a in state s, and $R(s,a,s')$ defines the thereby achieved immediate reward. A policy $\\pi: S \\rightarrow A$ does then define the behavior of an agent in the environment. Given $\\pi$, we can generate trajectories $\\tau = S_0A_0R_1 S_1 A_1, R_2 . . .$, a sequence of states $S_{t+1} \\sim P(\\cdot|S_t, A_t)$, actions $A_t \\sim \\pi(S_t)$ chosen according to the policy $\\pi$, and achieved rewards $R(S_t, A_t, S_{t+1})$. The goal of RL is then to find (or, effectively, approximate) an optimal policy $\\pi^*$ that maximizes the sum of expected discounted rewards, i.e., $E_{\\tau \\sim \\pi} [\\sum_{i=1}^\\infty \\gamma^i R_i]$.\nWe will add morally relevant facts to the states of our environment by using so called labeled MDPs [4]. We therefore assume a set of labels $L$, which can be thought of as representing the facts defining this state, and a labeling function $l : S \\rightarrow P(L)$. They (or some of them) might then qualify as possible normative reasons for moral obligations of the agent (cf. [8, 12, 14]).\nTo filter the state space from actions that do not conform with the agent's moral obligations, we build a moral shield. In safe RL (cf. [7]), a shield prevents the agent from entering unsafe states according to some safety specifications [10, 1]. Shields usually operate on linear temporal logic [10]. In contrast to this, the logical operations of our moral shield are based on what John Horty calls a fixed priority default theory."}, {"title": "A Logic for Normative Reasons", "content": "John Horty proposes to model normative reasoning on the basis of a so-called (fixed priority) default theory $\u2206 := (W, D, <)$, where W is a set of ordinary propositions, \u00d8 is a set of default rules of the form X \u2192 Y, where X is the premise and Y is the conclusion, and < is a strict partial ordering relation on default rules, where \u03b4 < \u03b4' means that the default rule \u03b4' has a higher priority than the rule \u03b4. A scenario I is a subset of the set of default rules D. We refer to the premise and conclusion of a rule 8 with Prem(8) and Conc(8) respectively; the notions are lifted to sets of rules in the usual way.\nHorty introduces a formalism to derive moral obligations in a certain scenario. For a default theory (W, D, <) and a scenario I, he defines the set of triggered default rules (i.e., the rules becoming active in S) as\n$Triggered_{W,D}(S) := {d \\in D : W \\cup Conc(S) \\vdash Prem(\\delta)}$\nand the set of conflicted default rules (i.e, the rules in I by which statements can be derived that contradict each other) as\n$Conflicted_{W,D}(S) := {d \\in D : W \\cup Conc(S)\\vdash \\neg Conc(\\delta)}$.\nFurther, he calls a default rule \u03b4' \u2208 Triggered_{W,D}(S) a defeater for another rule 8 \u2208 I, iff \u03b4 < \u03b4' and $W\\cup{Conc(\\delta')}\\vdash \\neg Conc(8)}$ and defines the set of defeated rules as\n$Defeated_{W,D,<}(S) := {\\delta \\in D : there is a defeater for d}$.\nIntuitively, then, a rule 8 is defeated iff there is a prioritized active rule in I whose conclusion, together with the propositions in the background information W, contradicts the conclusion of 8. Finally, he defines the set of binding rules in the scenario I as\n$Binding_{W,D,<}(S) := {\\delta \\in D : \\delta \\in Triggered_{W,D}(S), \\delta \\notin Conflicted_{W,D}(S), \\delta \\notin Defeated_{W,D}(S)}$.\nFurther, a scenario I is called a proper scenario based on a default theory (W,D,<) iff $S = Binding_{W,D,<}(S)$. Intuitively, a proper scenario is a set of defaults that is chosen by an ideal reasoning agent. A scenario I based on the default theory (W, D, <) generates a belief set & (a set of propositions) through the logical closure of $W\\cup Conc(S)$; i.e. $& = Th(W\\cup Conc(I))$, where $Th(W\\cup Conc(S)) := {X : (W\\cup Conc(I)) \\vdash X)}$. A belief set represents the beliefs of an ideal reasoner, i.e., a reasoner that believes the deductive closures over the believer's initial beliefs.\nFinally, one can derive moral assessments in Horty's framework. If a proposition follows from proper scenarios of a default theory, it can be interpreted as an ought statement: For a default theory $\u2206= (W,D,<)$, we say that under a conflict (disjunctive) account, the simple ought statement $O(Y)$ follows from A just in case $Y \\in &$ for each (any) extension & of this theory. For our purposes, we embrace the disjunctive account."}, {"title": "A Reason-Sensitive Moral RL Agent", "content": "To create a reason-sensitive artificial moral agent, we propose to apply Horty's reason framework in the reinforcement learning setting. The core idea is to extend the classic RL pipeline with a reason-based shield generator. This shield generator yields a shield that functions as a deontic filter [3] which restricts the agent's options to morally permissible actions on the grounds of normative reasons. In order to determine which actions qualify as permissible in the current environment of the agent, the shield"}, {"title": "Moral Shielding", "content": "We introduce an algorithm to derive a shield from an agent's reason theory \u3008<,9) in a state s (see Section 5).\nFirst, we extend it to a default theory (W,D,<). In order to do this, we need to add the (background) information W consisting of the morally relevant facts observed by the agent in s and the information which relevant action types are mutually exclusive in s. We identify each action type \u03c6\u2208 \u03a6 with a set of trajectories consisting of primitive actions each realizing \u03c6:\nLet $k_s$ be the number of trajectories that realize the action type o in state s. For $1 < l \\leq k_s$, $N \\in N$, we set $traj^l_\\varphi(s) := (a^\\varphi_t)^N_{t=1}(s)$ where $a^\\varphi_t(s)$ is the primitive action that is executed at step t when starting in state s on trajectory I realizing the action type q. Further, let $TP(s) = {(a^l_t)^o(\\varphi) |1 \\leq I \\leq k_s}$ be the set of all trajectories that realize o in state s. To identify action types with the set of trajectories that realize them, we thus set $\\varphi = T^{o(\\varphi)}(s)$.\nFurther, we set $T^{o(\\varphi)}(s) := {(a_{1,t})^l(s) |1 \\leq I \\leq k_s}$ to be the set of actions that are each the first element of at least one trajectory that realizes q. Then, in order to realize q, the agent 's next step has to be an action a such that a \u2208 $T^{o(\\varphi)}(s)$. We now define a function $Conflict_9 : S \\rightarrow P(D)$ that maps each state to those subsets of the agent's rule set \u00d8 which include rules whose premises are triggered in s and by which moral obligations are derived for which there is no primitive action in the next state which conforms with all of them: $Conflict_9(s) = {D' \\subset D | Prem(D') \\subseteq l(s)\\wedge \\nexists a\\in D! \\nbigcap_{\\varphi\\in Conc(D')} T_1(\\varphi(s)) = \\emptyset}$. We assume $T(\\varphi)(s)$ to be initially given for each action type \u03c6 \u2208 \u03a6 and each state s \u2208 S. Learning this abstraction inside the agent is identified as an interesting and challenging direction for future work.\nThe impossibility of realizing moral oughts derived from rules that conflict is added to the background information together with the morally relevant facts:\n$W =\\bigcup_{D' \\in Conflict_{9}(s)} {\\neg(\\bigwedge_{\\varphi \\in Conc(D')}\\varphi)} \\cup l(s)$.\nWe then move on to construct the shield. To this end, we compute whether $Binding_{W,D,<}(S) = I$ for all scenarios I \u2208 D and thereby derive all proper scenarios in (W, D,<). If there are several proper scenarios $1,...,Ik$, several incompatible sets of moral obligations can be derived from the default theory. In this case, the agent randomly selects one proper scenario I* \u2208 {$1,...,Ik}. Finally, the shield generator sets the shield S to be the set of primitive actions that conforms with all moral obligations in S*:\n$S:=\\bigcap_{\\delta \\in S*} T_1^{Conc(\\delta)} (s)$.\nThe agent then executes a morally shielded policy by choosing an action a \u2208 S, i.e., it chooses an action compatible with a set of moral obligations it derives from its reason theory in s. Note that the agent thereby follows $(\\forall i\\in{I_1,...,I_k} (\\bigwedge_{\\delta \\in Conc(\\delta)}))$ as an all things considered ought according to the disjunctive account, i.e., it realizes all moral obligations in one (randomly chosen) consistent set."}, {"title": "", "content": "Bridge Setting We return to our running example and describe how a moral shield would be gener-ated in the moral dilemma (where both, D and B, are set to true) from incorporated default rules.\nAssume that the agent has already learned the two reasons Prem(81) for waiting in front of the bridge and Prem(82) for rescuing a drowning person as described in the beginning of this section. Additionally, assume that the agent has not yet learned to strictly prioritize saving a drowning person over not pushing a person off the bridge. Consequently, the reason theory of the agent is ({81, 82},0).\nFor its moral decision-making, the agent first computes whether 8\u2081 and 82 lead to contradicting moral obligations in s. Hence, it computes $\\bigcap_{\\varphi\\in {\\delta_1,\\delta_2}} T_1^{Conc(\\delta)} (s)$. The set is empty, because rescuing the drowning person would only be realized by executing down in the next step (in order to move towards it). However, down contradicts waiting in front of the bridge. Consequently, \u00ac(@w > QR) needs to be added to the background information. Accordingly, the agent sets W = {D,B,\u00ab($w ^ OR)}. It then builds the fixed priority default theory for the moral dilemma based on its reason theory and the background information: (W, {81, 82},0). Next, it calculates the proper scenarios, which are {\u03b4\u2081} and {2}. Since two proper scenarios can be derived from the default theory, the agent chooses randomly. Assume that it chooses {01}. It thereby chooses to prioritize the reason for waiting in front of the bridge over the rea-son for rescuing the drowning person. It then calculates S = T\u2081w (s) = {left, right, up, pullOut, idle}.\nConsequently, the action down is filtered from the action space. The shield then forwards the set of permissible actions to the morally shielded policy based on which the agent chooses its action.\nNotice that the action which the agent chooses is not compatible with the moral obligation that would be derived from our exemplary default theory from the beginning of this section because our exemplary reason theory strictly prioritizes 82. Consequently, a moral judge that embraces the exemplary default theory would forward feedback to the agent so that it can improve its reason theory."}, {"title": "The Moral Judge", "content": "The agent enhances its reasoning through case-based feedback. A moral judge detects if the agent has performed a morally impermissible action and reports which moral obligation the agent should have fulfilled along with the reason for it. That is, the moral judge can be represented as a partial func-tion $MoralJudge : S \\times P(L) \\times A \\rightarrow \\Phi \\times L$, whereby $MoralJudge(s_{t-1},l(s_{t-1}),a_t) = (p,X)$ states that performing $a_t$ in state $s_{t-1}$ with labels $l(s_{t-1})$ was morally impermissible as the agent had the moral obligation to o for reason X, and $MoralJudge(s_{t-1},l(s_{t-1}),a_t)$ is undefined if the agent acted in accordance with its moral obligations. Crucially, providing this information \u2013 telling a moral actor what it has done wrong and to back the accusation by a reason comes naturally to humans. Hence, an easily accessible interface is created for directly communicating moral feedback to the RL agent.\nBridge Setting Returning to our running example, assume a situation as described in the example in 4.1. Imagine someone observes the agent's inaction and tells it afterwards that it should have rescued the person in the water because the person was drowning. This feedback can be formally represented as (D, OR) and can then be forwarded to the agent.\nOne potential drawback of grounding the agent's learning process on case-based feedback from hu-mans is the possibility of inconsistency. Future research will address how to manage inconsistent feed-back. For the first implementation, we plan to automatically provide the agent with feedback through an additional module in the pipeline, which ensures that only consistent feedback is given (for instance derived from a hard-coded, hand-crafted reason theory).\nImplemented as a module, the moral judge receives the agent's action a\u2081, the last state st\u22121, as well as the labels l(st-1) representing the morally relevant facts in st\u22121. It first runs an algorithm with pre-defined"}, {"title": "", "content": "rules that returns the set 0 := {1, ..., \u03a6n } of all moral obligations which the agent should have followed in st-1 as well as the reasons X1, ..., Xn that favor the obligations. We assume that no inconsistent moral obligations can be derived by the pre-defined rules. If the derived set of moral obligations is not empty, the judge has to check whether the primitive action that the agent executed conforms with all its moral obligations, i.e., whether $a \\in \\bigcap_{\\varphi_1 \\in o} T_1^{i}(s) \\forall 1 \\leq i \\leq n$. If this is not the case, the agent has violated its moral obligation to ap\u2081. Then, the judge forwards (pi, Xi) to the shield generator. (See Section 5 for the explicit pseudo-code algorithm.)\nBridge Setting Assume again that the agent breaks ties randomly in the moral dilemma and waits in front of the bridge instead of rescuing the drowning person, i.e., it takes the primitive idle action. After execution, the moral judge would receive as input (st\u22121,{B,D}), idle). Further assume that the moral judge is implemented by our exemplary reason theory for the case from the beginning of this section (cf. Figure 2). Accordingly, the moral judge derives that the agent had a moral obligation to rescue the drowning person in the given situation. It then checks whether the agent's action was morally permissible, i.e., whether the primitive action idle is part of a sequence of primitive actions that realizes the action type OR. As this is not the case, the judge forwards the feedback (OR, D) to the shield generator."}, {"title": "Learning Reasons", "content": "If the agent is provided with feedback (p,X) by a moral judge, it uses this feedback to update its current reason theory (cf. [5]). First, it ensures that its reason theory incorporates a rule that captures this relationship, i.e., it ensures that dreas := X \u2192 \u03c6 \u2208 D. Then, it updates the order relation. Assume that I* is the proper scenario which the agent has picked among the set of all proper scenarios derived from (W,D,<) with W constructed by the shield generator for st\u22121. The agent updates its order relation s.t. {dreas > \u03b4 : \u03b4 \u2208 S*}. Thereby, is ensured that in similar future situations, the agent now strictly prioritizes the moral obligation derived by dreas.\nBridge Setting Assume again that the agent breaks ties randomly in the moral dilemma and chooses {01} among the proper scenarios, i.e., it waits in front of the bridge instead of rescuing the drowning person. Afterwards, it is provided with the information that it should have rescued the person in the water because the person was drowning, i.e., the shield generator receives as input (OR, D) from a moral judge. Based on this information, it checks whether its reason theory already includes the default rule dreas = 82. As this is the case, no new rule is added. The agent then updates the order among its default rules, i.e., it sets < to {\u03b4\u2081 < \u03b42}. Thereby, it extends its reason theory to our exemplary reason theory \u2013 it now has learned good reasons with which it aligns its actions in future situations. Finally, its future actions are morally justified."}, {"title": "Conclusion", "content": "We have introduced an approach for grounding the moral decision-making of RL agents on normative reasons by extending RL architecture with a shield generator that yields a shield binding the agent to morally permissible actions according to a reason theory. Further, our approach enables the agent to iteratively improve its reason theory. The learning process works on case-based feedback that is given to the agent by a moral judge. As this form of critique comes naturally to humans, the mechanism makes it possible for humans to directly communicate feedback to the agent. Under the assumption that the agent learns good reasons, the agent ultimately learns a reason theory based on which its actions are not only morally justifiable but even morally justified."}, {"title": "Appendix", "content": "Algorithm 1 Reason-Sensitive Reinforcement Learning Agent\nD := 0, <:= 0 // initialize reason theory\nwhile True do\nget state s \u2208 S and labels l(s) from environment\nW := $UD'Conflict (S){\\neg(\\bigwedge_{\\varphi \\in })}Ul(s)$\n$1,..., Ik := proper scenarios of (W, D, <)$\nJ* := rand{$1,...,Sk} // pick random scenario\nS:=${\\delta \\in S*} I Conc(5) (5)$\na := Execute policy shielded with S\nif MoralJudge(s,l(s),a) = Some(\u03c6,X) then\ndreas := X \u2192 \u03c6\n<:=<U{dreas > \u03b4 : \u03b4 \u2208 S*} // add new rule to rule set\nD:= DU{dreas} // extend rule order\nend if\nend while"}]}