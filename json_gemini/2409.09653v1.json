{"title": "KAN v.s. MLP for Offline Reinforcement Learning", "authors": ["Haihong Guo", "Fengxin Li", "Jiao Li", "Hongyan Liu"], "abstract": "Kolmogorov-Arnold Networks (KAN) is an emerging neural network architecture in machine learning. It has greatly interested the research community about whether KAN can be a promising alternative of the commonly used Multi-Layer Perceptions (MLP). Experiments in various fields demonstrated that KAN-based machine learning can achieve comparable if not better performance than MLP-based methods, but with much smaller parameter scales and are more explainable. In this paper, we explore the incorporation of KAN into the actor and critic networks for offline reinforcement learning (RL). We evaluated the performance, parameter scales, and training efficiency of various KAN and MLP based conservative Q-learning (CQL) on the the classical D4RL benchmark for offline RL. Our study demonstrates that KAN can achieve performance close to the commonly used MLP with significantly fewer parameters. This provides us an option to choose the base networks according to the requirements of the offline RL tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Kolmogorov-Arnold Networks (KAN) [1] is an emerging neural network architecture in machine learning, attracting widespread attention ever since it was hang on arXiv on April 30, 2024. The research community is very concerned about whether KAN can be a promising alternative of the commonly used Multi-Layer Perceptions (MLP).\nKAN and MLP are different in the fundamental theorems and network architectures, and KAN is thought to enjoy super flexibility and interpretability than MLP. Empirical studies demonstrated that KAN excels in data fitting and partial differential equation solving [3], [19], [38], high-dimensional data handling [4], graph-structured data processing and hyper-spectral image classification [7], complex data relationships capturing [9], and online reinforcement learning (RL) [8], etc. showing that even smaller KAN-based models can achieve or surpass the performance of MLP-based models in these tasks. The efficacy of KAN for offline RL has not been studied yet. Therefore, this paper aimed at applying KAN as an alternative to MLP in offline RL and comparing their performance, parameter scales and training efficiency.\nOffline RL leverages previously collected offline datasets without further direct environment interaction to optimize sequential decision policy [22]. Offline RL has the capacity to provide a safer and more economically viable solution for real-world sequential decision-making comparing to online RL [23], [24]. The main challenge of offline RL is to avoid extrapolation errors caused by distribution shift when learning policies with limited offline datasets. Conservative Q-learning (CQL) [25] is a state-of-the-art offline RL method which can alleviate extrapolation error through conservatively estimating the value of out-of-distribution actions. CQL is originally based on MLP.\nNetworks with fewer parameters yet equal or superior approximation capabilities have the potential to significantly enhance performance in various offline RL applications. In this work, we propose the use of KAN as the basic building blocks of the actor and critic networks for CQL. The main contributions of this paper are summarised as following: 1) The first application of KAN in a offline RL algorithm. 2) Comprehensive comparison of performance, parameter scales, and training efficiency between KAN-based, MLP-based, and KAN-MLP-based CQL in various continuous control tasks."}, {"title": "II. RELATED WORKS", "content": "KAN has recently attracted significant attention due to its versatility and enhanced application performance [5], [6], [16], [30], [31], [34], [35], [41], [44], [45]. Experiments in different fields have shown that KAN-based machine learning can achieve comparable, if not better, performance than MLP-based methods while using much smaller parameter scales and offering better explainability [2]. For example, Liu et al. [3], [12] demonstrated KAN's superior performance in symbolic regression, effectively modeling complex data and achieving precise formula discovery. Recent studies integrated path signatures with KAN to better understand and predict complex time series patterns [10], [11].\nGKAN [13] and GraphKAN [42] extends their principles to graph-structured data through node feature aggregation, embedding layer processing, and dynamic adaptability. Wav-KAN [14] introduces wavelet functions as learnable activation functions into the KAN architecture to achieve nonlinear mapping of input spectral features for hyperspectral image classification. SpectralKAN [15] uses B-spline functions as activation functions to represent multivariate continuous functions for feature extraction and classification of hyperspectral images."}, {"title": "III. BACKGROUND", "content": "Offline Reinforcement Learning (RL) RL is developed for sequential decision-making, involving an agent and its environment. The agent follows a policy $\\pi(\\cdot|s)$, making decisions and taking action a at each time step according to its current state s, resulting in a transition to the next state s' and receiving a numerical reward r from the environment [26]. The policy is optimized by selecting the action with the maximum value, i.e. the maximum cumulative return.\nIn the online setting, the agent can learn the optimal policy through trail and error by continuously interaction with the environment. However, in the offline setting, the environment is inaccessible. The agent learns policy with previously collected static dataset $D = \\{(s, a, r, s', done)\\}$. Thus the main challenge is to avoid extrapolation errors caused by distribution shift when learning policies with limited offline datasets.\nMLP-based Conservative Q-learning (CQL) CQL [25] is a state-of-the-art offline RL algorithm developed upon soft actor-critic (SAC) [27]. It uses an actor network $\\pi(\\cdot|s)$ as the policy function to generate action, and two critic networks $Q_1(s, a)$ and $Q_2(s, a)$ to estimate the action value, also referred to as Q-value. CQL seeks to avoid extrapolation error through penalizing the Q-value of OOD actions while rewarding that of in-distribution actions [25]. The loss functions of the actor and critic networks are as following:\n$L_{ci} = \\alpha_1(\\mathbb{E}_{s \\sim D, a \\sim \\pi(a|s)}[Q_i(s, a)] - \\mathbb{E}_{s, a \\sim D}[Q_i(s, a)])$ (1)\n$+ \\frac{1}{2\\tau} \\mathbb{E}_{s, a, s' \\sim D} (Q_i(s, a) - Q(s, a'))^2 + R(\\pi)$\n$L_a = \\mathbb{E}_{s \\in D, a \\sim \\pi(\\cdot|s)}[-\\underset{j=1,2}{\\min} Q_j(s, a) + \\alpha_2 \\cdot \\log \\pi(a|s)]$ (2)"}, {"title": "IV. METHOD", "content": "We use CQL as the core framework of offline RL and incorporate efficient KAN into its actor and critic networks respectively. The structure diagrams with one hidden layer KAN are shown in the right in Fig. 1.\nOverall, the KAN-based actor network and critic networks have the same middle layers, which is defined as $H_i = W_i \\cdot [SiLU(H_{i-1}), B-Spline(H_{i-1})]$, where SiLU is a fixed nonlinear activation function and B-Spline is a learnable nonlinear function, and $H_i, i = 1,2,...$ are hidden or output layers. One of the difference is that the input of the actor network is state, i.e. $H_1 = W_1 \\cdot [SiLU(s), B-Spline(s)]$, while the input the critic networks are (state, action) pairs, i.e. $H_1 = W_1 \\cdot [SiLU(s, a), B-Spline(s, a)]$. Another difference is that the output layer of the critic networks has only one node, representing the Q-value of the (state, action) pairs. The actor network needs to get the get the mean of the TanhGaussianPolicy, then conduct a linear transformation to get the standard deviation. The remaining processes are the same as the MLP-based actor network.\nWe designed compared various KAN and MLP based CQL architectures. The configurations are shown in Table 1. Other parameters are kept the same as the original MLP-based CQL and the efficient KAN."}, {"title": "V. RESULTS", "content": "Table 2 reports the performance of each method, measured by normalized scores with standard deviation. The score is the return gained at the end of each episode and is normalized to D4RL scores that measure how the learned policy compared with an expert policy and a random policy that are pre-installed in D4RL:\nnormalized score = $100 \\times \\frac{(\\text{learned policy score - random policy score)}}{(\\text{expert policy score - random policy score})}$. The results are averaged over 4 random seeds. We have the following observations: 1) The MLP-based CQL with 3 hidden layers both for the actor and the critic networks enjoys the best performance; 2) The KAN-MLP-based CQL with 2 hidden layers in the KAN-based actor and 3 hidden layers in the MLP-based critic performed close to the best MLP-based CQL, but the latter is slightly better in most of tasks (Fig 2), and increase hidden layers in the KAN-based actor can not gain performance improvements. 3) Pure KAN-based CQL is inferior to MLP-based CQL and KAN-MLP-based CQL when the nonlinear transformation layers are the same."}, {"title": "VI. CONCLUSION", "content": "Our study demonstrates that KAN can achieve performance close to the commonly used MLP with significantly fewer parameters in offline RL tasks. This provides us an option to choose the base networks according to the requirements of the tasks, when the performance is dominant, MLP-based methods are still the preferred ones, when the occupy memory and computation amount is the obstacle in deploying in the real world terminal devices, we can choose KAN-based actor without seriously affecting the performance.\nKAN is originally designed for interpretability [33], which is not analyzed in this study. In the future, we can study how to build explainable offline RL methods with KAN, or learn by MLP-based offline RL methods and explain by KAN."}]}