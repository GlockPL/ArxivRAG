{"title": "A DRIVER ADVISORY SYSTEM BASED ON LARGE LANGUAGE MODEL FOR\nHIGH-SPEED TRAIN", "authors": ["Yuchen Luo", "Jing Xun", "Wei Wang", "Ruize Zhang", "Zicong Zhao"], "abstract": "With the rapid development of China high-speed railway, drivers face increasingly signif-\nicant technical challenges during operations, such as fault handling. Currently, drivers depend on\nthe onboard mechanic when facing technical issues, for instance, traction loss or sensor faults. This\ndependency can hinder effective operation, even lead to accidents, while waiting for faults to be ad-\ndressed. To enhance the accuracy and explainability of actions during fault handling, an Intelligent\nDriver Advisory System (IDAS) framework based on a large language model (LLM) named IDAS-\nLLM, is introduced. Initially, domain-fine-tuning of the LLM is performed using a constructed\nrailway knowledge question-and-answer dataset to improve answer accuracy in railway-related\nquestions. Subsequently, integration of the Retrieval-augmented Generation (RAG) architecture\nis pursued for system design to enhance the explainability of generated responses. Comparative\nexperiments are conducted using the constructed railway driving knowledge assessment dataset.\nResults indicate that domain-fine-tuned LLMs show an improvement in answer accuracy by an\naverage of 10%, outperforming some current mainstream LLMs. Additionally, the inclusion of the\nRAG framework increases the average recall rate of question-and-answer sessions by about 4%.\nFinally, the fault handling capability of IDAS-LLM is demonstrated through simulations of real\noperational scenarios, proving that the proposed framework has practical application prospects.", "sections": [{"title": "INTRODUCTION", "content": "High-speed train drivers play a crucial safety role in the railway transport system, and their\ntraining process is both rigorous and complex (1). Initially, candidates must accumulate 200,000\nkilometers of driving experience on conventional trains over three years to qualify for high-speed\ntrain driver training. During the training phase, they are required to study 20 specialized courses,\nincluding \"High-Speed Trainset Technology\", and pass related theoretical exams. Subsequently,\nthey enter a practical training phase at a training base equipped with advanced simulators for high-\nspeed trainsets, such as the CR (China Railway) and CRH (China Railway High-speed) series,\nwhich can simulate various emergency scenarios that may occur in real operations. Only when\ncandidates have completed the required mileage and passed practical assessments are they eligible\nto apply for a high-speed train driving license. The entire training period can span several years.\nWith the rapid development of China's high-speed railways, the demand for train drivers is increas-\ning, and the current system, which often implements a single driver per shift, not only increases the\nworkload (2) but may also lead to driver fatigue (3), posing a potential threat to operational safety.\nCurrently, high-speed trains in China primarily operate under manual control. When issues\nsuch as traction loss or sensor faults occur, drivers immediately notify the onboard mechanic. The\nonboard mechanic is responsible for monitoring the operational status of the trainset and handling\nany emergencies. Once alerted, they promptly address the issue. This arrangement has the advan-\ntage of clear roles, but a drawback is that it can place the driver in a dilemma. In a real scene,\nthe train was accelerating to pass through a neutral zone, and the driver got a warning for traction\nloss. He noticed the mechanic immediately. Then, the situation at that time was, on one hand, the\nmechanic is busy on solving the fault. On the other hand, the driver needs to cooperate with the\nmechanic before deciding on if he could go on accelerating the train to the speed required to pass\nthrough the neutral zone. However, because the mechanic cannot be reached in a timely manner,\nthe driver missed the time window of acceleration. This caused the accident that the train is unable\nto pass through the phase zone and stops in it. In such cases, providing drivers with an assistant\ncapable of helping manage fault scenarios could be a solution.\nThe driver advisory system(DAS) can assist drivers in real-time to provide personalized\ndriving assistance. For instance, it can recommend the best speed based on traffic conditions,\nweather, and other factors (4). In addition, it can help drivers adjust their driving behavior to\noptimize fuel efficiency, prevent accidents, and comply with traffic regulations. A DAS based\non LLM has potential for providing context-specific driving advice to avoid such an accident as\naforementioned. To make it trusted by drivers, it needs to address the challenge of improving the\nexplainability of LLM.\nIn recent years, large language models (LLMs) like ChatGPT (5) have demonstrated ex-\nceptional capabilities in common sense reasoning, inference, and planning, providing insightful\nsuggestions and finding practical applications in fields such as law (6), finance (7), medicine (8)\nand road traffic (9). While one of potential applications of ChatGPT is intelligent driver assistance\n(10), there are several technical questions that need to be addressed. Most LLMs are pretrained on\ngeneral corpora and may not cover or accurately represent domain-specific knowledge in railway.\nApplying LLMs directly to IDAS could result in inaccurate or erroneous outputs. To overcome\npotential hallucination issues with LLMs in railway knowledge, domain-specific fine-tuning is\nnecessary to enhance performance in fault handling scenarios.\nTherefore, the IDAS-LLM framework, a large language model based system designed to\nassist in managing frequent high-speed train faults, is proposed. Specifically, we begin by con-"}, {"title": null, "content": "sulting the Chinese Railway Locomotive and Vehicle Driver's Qualification Examination syllabus\nand employ existing high-performance large language models to automatically generate a domain-\nspecific fine-tuning dataset for the railway sector. This dataset includes 10,100 structured question-\nand-answer pairs, covering legal provisions, railway regulations, and railway expertise. Subse-\nquently, we conduct domain-specific fine-tuning using the assembled Railway Training Dataset\n(RTD) to enhance the LLM's expertise in the railway field. To further address potential halluci-\nnation issues within this sector, we integrate Retrieval-Augmented Generation (RAG) technique\nto improve the accuracy and professionalism of the LLM's responses. We validate the framework\non our custom railway driving knowledge assessment dataset and perform comparative evaluations\nagainst several contemporary mainstream models. Finally, we demonstrate the framework's re-\nsponse effectiveness through examples of faults encountered in actual high-speed train operations."}, {"title": null, "content": "The main contributions of this paper are as follows:"}, {"title": null, "content": "We construct a domain-specific fine-tuning dataset for the railway sector, RTD, consist-\ning of 10,100 structured question-and-answer pairs that cover railway legal provisions,\nregulations, and expertise."}, {"title": null, "content": "We propose a Chinese railway domain-specific driver advisory LLM named IDAS-LLM.\nThe fault handling capabilities of IDAS-LLM are verified under simulated real-world\nscenarios."}, {"title": null, "content": "We introduce RAG technology, which consults a driver knowledge base to mitigate LLM\nhallucinations, enhancing both the accuracy and explainability of LLM responses."}, {"title": null, "content": "The rest of this paper is organized as follows. Section 2 reviews current research on IDAS\nand the application of LLMs in the transportation sector. Section 3 describes the construction pro-\ncess and the technical methods used in IDAS-LLM. Section 4 presents the experimental design and\nresults, including fine-tuning comparison experiments and RAG comparison experiments. Section\n5 demonstrateshow IDAS-LLM handles two types of fault scenarios under simulated real-world\nconditions. Section 6 concludes the paper and proposes future research directions."}, {"title": "LITERATURE REVIEW", "content": "The current intelligent driver advisory systems are primarily developed with a focus on en-\nergy conservation and punctuality, concentrating largely on exploring theories and control methods\nrelated to optimizing train operation curves. Zhu et al.(11) introduce a novel DAS prototype design\nthat utilizes a PC as the central unit and a smartphone as the on-board unit, enabling bi-directional\nmessage exchange between the two units. They thoroughly detail the comprehensive process from\nthe system structure and methodological design to the practical application using a PC with a\nsmartphone, effectively translating research concepts into real-world implementation. Dong et al.\n(12) explore methods to improve train operational performance using DAS, employing enhanced\ntrain dynamics and energy efficiency models. They design a traction-distance-based method for\ntrajectory optimization and construct a hardware-in-the-loop experiment platform to simulate the\napplication of DAS in actual driving scenarios. A prototype system is developed to provide es-\nsential advisory information to train drivers. Xiao et al. (13) propose an onboard energy-efficient\ndriving advisory system for high-speed trains operating on vehicle control units. They introduce\na data interaction framework to monitor the train's real-time state and temporary speed limits.\nAdditionally, a bilevel optimal control method is designed to compute the energy-optimal speed\nprofile in realtime, extracting driving advice from the optimized trajectories. Formato et al. (14)\naddress the energy aspect with considerations for Timetable programming and TMS regulations."}, {"title": null, "content": "Specifically, they focus on defining appropriate speed profiles to optimize TMS time constraints\nand on developing a C-DAS architecture that can be immediately implemented in the current in-\nfrastructure. In addition to designs aimed at energy optimization, Wang et al. (15) propose a novel\nalgorithm architecture for a connected DAS, designed to provide time/speed advice to freight train\ndrivers. This system offers appropriate recommendations based on a predicted feasible merging\nwindow, aiding drivers in achieving smooth merging operations. Guerra et al. (16) explore the\ninteraction between train drivers and an autonomous driver advisory system (ADAS) based on\nhuman state estimation, aimed at avoiding confrontational interactions and promoting integrative\ncooperation. The findings indicate that such interactions enhancedriver acceptance rates for ADAS\nrecommendations and improve overall train control performance, ensuring mission success.\nLarge language models are demonstrating their extensive influence across various fields,\nincluding road transportation. Research involving these models is gradually unfolding within the\nroad traffic sector, showcasing their potential application value and broad research interest. The\ncurrent focus of research primarily centers on constructing frameworks driven by large language\nmodels. Cui et al.(17) present DriveLLM, a decision-making framework that integrates LLMs with\nexisting autonomous driving stacks. This integration enables commonsense reasoning within the\ndecision-making process. DriveLLM also features a unique cyberphysical feedback system, allow-\ning it to learn from its mistakes and make improvements. Zhang et al. (9) introduce the TrafficGPT\nframework, which bridges the critical gap between LLMs and traffic foundation models (TFMs)\nby defining a series of prompts. This framework aims to infuse LLMs with the capability to inter-\nact with traffic data and systems, ensuring reliability, and provides valuable decision support for\nmanaging urban transportation systems. Cui et al. (18) significantly enhance the decision-making\ncapabilities of autonomous vehicles by deploying LLMs. Their developed framework integrates\nLLMs' advanced language comprehension and contextual analysis, coupled with strategic tool\napplications and collaborative operations with various vehicle modules, markedly improving the\nsafety and efficiency of autonomous driving technologies. Additionally, researchers are employing\nfine-tuning techniques to facilitate knowledge injection into these models. Wang et al. (19) intro-\nduce TransGPT, a novel (multi-modal) large language model tailored for the transportation domain,\ncomprising two independent variants: TransGPT-SM for single-modal data and TransGPT-MM for\nmulti-modal data. They also showcase the potential applications of TransGPT in traffic analysis\nand modeling."}, {"title": "METHODS", "content": "This section explores the construction of the IDAS-LLM, which involves three processes:\nthe construction of the railway driving dataset, supervised fine-tuning (SFT) and the design of\nRAG. We will discuss each step in sequence to reflect the workflow of the research."}, {"title": "Construction of Railway Driving Dataset", "content": "To construct a dataset that accurately reflects the real-world driving scenarios faced by train\ndrivers, we referenced the syllabus of the Chinese Railway Locomotive and Rolling Stock Drivers'\nQualification Examination. This ensures that the included knowledge and skills are aligned with\nindustry standards. The dataset is divided into three main categories, each directly corresponding\nto the core content of the examination syllabus. This categorization aims to provide rich examples\nand contexts for the subsequent fine-tuning of the large model. The specific categories are as"}, {"title": null, "content": "follows:"}, {"title": null, "content": "Legal Provisions: This category covers laws and regulations related to railway safety\nand operations, as well as specific ordinances and methods for railway safety manage-\nment. This content helps train drivers understand their legal responsibilities and ensures\ncompliance with relevant laws and regulations during their work."}, {"title": null, "content": "Railway Regulations: This category includes specific railway technical operations and\nmaintenance procedures. These regulations are the technical foundation for the effective\noperation of the railway system, providing standardized operational guidelines to ensure\nthat drivers can correctly perform technical operations and maintenance tasks."}, {"title": null, "content": "Railway Expertise: This category involves the operation and fault handling techniques\nof trainsets, as well as emergency handling manuals for various types of trainset failures\nduring transit. This content aims to enhance train drivers' mastery of high-speed trainset\ntechnical details and their ability to respond in emergency situations."}, {"title": null, "content": "The original text data collected comprises a total of 776,000 Chinese tokens, which are used\nto construct a railway driving text database. Based on this, a railway driving question and answer\ndataset for fine-tuning training, called the Railway Training Dataset (RTD), will be established.\nThe specific composition of this dataset is shown in Table 1."}, {"title": null, "content": "Ensuring that large language models effectively absorb railway domain knowledge and\ndemonstrate conversational abilities is crucial. This process relies on a substantial amount of\ninstructive data, enabling LLM to generate appropriate outputs based on predetermined instruc-\ntions and inputs. However, there is a significant lack of such datasets in the railway domain, and\nmanually formatting these data is not only time-consuming but also labor-intensive. To address\nthis challenge, an automated processing strategy has been adopted, as shown in Figure 1. Ex-\nisting large language models with excellent performance are utilized to automatically generate\nfine-tuning training data. The generated data is primarily stored in a question-and-answer format,\nfacilitating learning and application by the LLM."}, {"title": null, "content": "Initially, a chunking strategy is used to process the text in the database, segmenting the text\ninto themes based on specific formatting elements such as clauses and chapter titles. Subsequently,\nleveraging the advanced language understanding capabilities of large language models, suitable\nprompts are designed and combined with few-shot learning techniques to generate one or more\nrelated questions for each text block. For each question, the corresponding text block content is\nrestructured into a prompt input, which is submitted to a large language model to generate answers\nthat accurately reflect key information. During this process, issues such as question duplication,\nmissing, or invalid answers may arise, necessitating strict data filtering before the final integration.\nAfter completing the filtering, the question-and-answer pairs are integrated and stored in structured\ndata, thus completing the construction of the RTD and providing data support for conversations in"}, {"title": null, "content": "railway driving scenarios."}, {"title": null, "content": "In conclusion, using the Qwen-14B-Chat (20) model on a single A40 GPU, 10,100 struc-\ntured question-and-answer pairs are generated for fine-tuning."}, {"title": "Dataset for Evaluation", "content": "To assess the IDAS-LLM's effectiveness in applying train driver knowledge, an assessment\nset for railway driving knowledge is created by organizing an existing high-speed train driver as-\nsessment question bank. Similarly, by using the Qwen-14B-Chat model, single choice questions,\nmultiple choice questions, and true/false questions from the question bank are transformed into a\nquestion-and-answer format to facilitate the evaluation of the framework's text generation perfor-\nmance. The question in each question-and-answer pair is a direct rephrasing of the question from\nthe question bank, while the corresponding reference answer is a rephrased text that combines the\nquestion and its corresponding reference answer from the question bank."}, {"title": null, "content": "A total of 2462 questions are collected, including 845 single choice questions, 642 multi-\nple choice questions, and 975 true/false questions. The data converted into question-and-answer\nformat is categorized by question type, and 100 question-and-answer pairs are extracted from each\ncategory for the assessment set. Thus, the railway driving knowledge assessment set includes 100\nquestion-and-answer pairs on legal provisions, 100 on railway regulations, and 100 on professional\nrailway knowledge."}, {"title": "Supervised Fine-Tuning", "content": "SFT is a critical technique for imparting domain-specific knowledge to models, and itsef-\nfectiveness heavily depends on the selection of the base model. Given that not all models support\nChinese, compatibility with the Chinese language becomes a primary consideration in choosing the\nbase model. To evaluate model performance, Chinese large language model evaluation datasets,\nCEval (21) and CMMLU (22), will be utilized. These datasets encompass a wide range of aca-\ndemic subjects and are capable of testing the model's comprehension across various domains and\nits ability to generate natural language. Details on the selection of the base model and its per-\nformance scores on the corresponding benchmark datasets are provided in Table 2. In Table 2,\nthe displayed scores represent the optimal values for each model across various evaluation met-\nrics, with the test scores for C-Eval and CMMLU sourced from the official evaluation results or\nleaderboard submissions for each model."}, {"title": null, "content": "Based on the scores, ChatGLM3-6B demonstrates superior performance among base mod-\nels with less than 10 billion parameters, and thus it has been selected as the base model for IDAS-\nLLM. In terms of semantic understanding, due to training on a large-scale corpus, this model ex-\nhibits strong semantic analysis capabilities. It is able to accurately capture user intent and provide\nprecise responses."}, {"title": "Training Detail", "content": "Fine-tuning training is conducted on a single A40 GPU. Utilizing the transformers and peft\nlibraries, parameter-efficient fine-tuning is achieved through the Low-rank Adaptation (LoRA)\nmethod, with settings following the recommendations found in (26). The dataset is divided into\ntraining and validation sets at an 8:2 ratio. The model undergoes fine-tuning for 10 epochs, with a\nlearning rate of 5e-4 and a batch size of 4, employing the Adam optimizer and the cross-entropy\nloss function."}, {"title": "Design of Retrieval-Augmented Generation", "content": "Given that only 0.03% of the total parameters are trained during fine-tuning, with the train-\nable parameters in the LoRA algorithm comprising 2M out of the model's total 6B parameters,\nand considering the influence of dataset quality and quantity, the fine-tuned model is still able\nto effectively address relevant questions within the railway domain. However, it still exhibits\nhallucinations regarding certain detailed information and may even produce incorrect responses.\nTherefore, to further mitigate LLM hallucinations and enhance the accuracy and explainability of\nthe answers, RAG has been introduced. RAG enhances LLMs by retrieving relevant document"}, {"title": null, "content": "blocks from an external knowledge base through semantic similarity calculations. By referencing\nexternal knowledge, RAG effectively reduces the generation of factually incorrect content. The\nimplementation of RAG is illustrated in Figure 2."}, {"title": null, "content": "Specifically, during the indexing phase, railway textual data are first loaded and processed\nin discrete chunks. Each chunk is then annotated with its specific source, which is derived from\nthe textual data extracted during the training set generation. Then, utilizing the Bge-zh-v1.5 (27)\nembedding model, these railway text data are vectorized. Once vectorization is complete, the data\nare stored in the ChromaDB database, thereby constructing a knowledge base of driving data and\ngenerating a corresponding retriever. In the answer generation phase, user questions are processed\nin two ways: one is direct input into the fine-tuned model to generate answers, and the other\nuses the same Bge-zh-v1.5 embedding model as in the indexing phase to generate a vectorized\nquery. For the latter, the system calculates similarity scores between the generated query and text\nvectors in the driving data knowledge base, ranking texts by similarity and retrieving the top five\ntexts with the highest similarity to the query. If the similarity score is below a preset threshold, it is\nassumed that irrelevant texts have been queried, and the answer from the first method of processing\nis directly output. Conversely, if relevant texts are retrieved, these text blocks are used as extended\ncontext in the prompt and re-entered into the fine-tuned model. Combining the answer from the\nfirst method, the LLM rethinks and outputs a revised or refined answer, including information\nabout the documents it referenced."}, {"title": "EXPERIMENTS", "content": "This section conducts experimental validation on the fine-tuning and RAG design within\nIDAS-LLM, discussing the results of the two experiments through text generation-related evalua-\ntion metrics."}, {"title": "Fine-tuning Comparison Experiments", "content": "Experiments are conducted on the previously prepared railway driving knowledge assess-\nment set. To evaluate the LLM's ability to answer questions across three categories before and after\nfine-tuning, BLEU (28) and ROUGE (29) are used as evaluation metrics to measure the differences\nbetween model-generated responses and reference responses. BLEU (B) is a precision-based sim-\nilarity measure that focuses on accuracy, with higher BLEU scores indicating higher accuracy.\nROUGE, on the other hand, is a recall-based similarity measure focusing on recall, where higher\nROUGE scores indicate higher recall. ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L (RL) are\nchosen as specific metrics for ROUGE, and comparisons are made with some of the current main-\nstream Chinese large language models. The evaluation results are shown in Table 3."}, {"title": null, "content": "The results in Table 3 indicate that, compared to the baseline model, the model fine-tuned\nfor the railway domain shows improved performance in answering questions across three cate-\ngories. Particularly in railway expertise questions, the three ROUGE metrics increased by 8%,\n6%, and 3% respectively, and BLEU increased by 7%, indicating that the fine-tuned model has\nsignificantly improved at the word level, capable of understanding and responding with profes-\nsional railway terminology. In legal provisions and railway regulations questions, the fine-tuned\nmodel also shows improvements in various metrics. Additionally, the BLEU scores across the three\ncategories have significantly improved after fine-tuning, suggesting that the LLM's responses have\nbecome more accurate and capable of capturing relevant information. Compared to current main-\nstream large Chinese language models, the fine-tuned model performs modestly in handling legal\nprovisions and railway regulations questions, possibly reflecting that these models have already\nacquired related knowledge during the pre-training phase. Despite this, the fine-tuned model ex-\ncels in railway professional knowledge, demonstrating that targeted fine-tuning can significantly\nenhance the domain capability of large language models."}, {"title": null, "content": "Based on the experimental results, ROUGE and BLEU provide effective benchmarks for\nimprovement. However, it is noteworthy that the evaluation metrics for the selected models gener-\nally appear low during testing, especially the BLEU scores. This may be due to two factors. On"}, {"title": null, "content": "one hand, although these models match the reference answers well in terms of railway or legal\nterminology, the length of the generated or reference answers leads to lower calculated evaluation\nmetrics. On the other hand, since ROUGE and BLEU primarily focus on literal overlap and do not\nconsider the meanings of words, different words with the same meaning or variants of the same\nword might be mistakenly judged as incorrect generations."}, {"title": "RAG comparison Experiments", "content": "To evaluate the improvement effect of RAG on the answer quality of IDAS-LLM, the eval-\nuation metrics used in the fine-tuning comparative experiments are still employed. Focus is pri-\nmarily on the recall-related metric ROUGE. The comparison of metrics before and after adding\nRAG is shown in Table 4."}, {"title": null, "content": "From the experimental results, the introduction of RAG significantly improves the frame-\nwork's performance in answering railway regulation questions, with the three ROUGE metrics,\neach increasing by 11%, 9%, and 9%. This improvement may be related to the cumulative to-\nkens across categories. Since knowledge may overlap among categories, the railway regulations\ncategory contains richer information, providing effective prompts for the LLM, thus enabling the\ngeneration of higher quality responses. Overall, RAG indeed provides effective references for the\nLLM's responses, enhancing the quality of the answers."}, {"title": null, "content": "Additionally, during the indexing phase, railway textual data are processed, chunked, and\ntransformed into embeddings stored in a vector database. The quality of the index built determines\nwhether the correct context can be retrieved during the retrieval phase (33). Therefore, documents\nare split into chunks based on a fixed number of tokens, and evaluation metrics are compared, with\nresults shown in Figure 3."}, {"title": null, "content": "at 500 tokens yields better metrics across the three categories compared to other chunk sizes.\nThis indicates that in the IDAS-LLM framework, a chunk size of 500 tokens is reasonable for\ntextual data in the driving data knowledge base. Moreover, this also reflects that larger chunks can\ncapture more context but may also introduce more noise. Excessively long text prompts hinder the\nLLM's ability to summarize and induce, reducing its capacity to extract key information. While\nsmaller chunks may not fully convey the necessary context, they indeed carry less noise. However,\nchunking can lead to sentence truncation, which is detrimental to generating complete answers."}, {"title": "CASE STUDY", "content": "To demonstrate the fault handling capabilities of the IDAS-LLM framework during high-\nspeed railway operations, two typical case studies are presented. One involves a common traction\nloss fault in the train traction drive system, and the other involves a frequently occurring sensor\nfault in the train braking system."}, {"title": "Scenario One: Simulation of Traction Loss Faul", "content": "Traction loss is one of the main faults that occur in the train traction drive system during\noperation. There are many causes of traction loss, each associated with different diagnostic codes.\nHowever, the plethora of diagnostic codes demands a lot from the driver's memory capacity, as\ndrivers must execute appropriate fault handling based on various diagnostic codes. Therefore,\ntwo simulation methods are designed. One is for the CR400AF series trainsets, where differ-\nent diagnostic codes and descriptions are given to demonstrate the fault handling capabilities of\nIDAS-LLM for various traction loss issues, as shown in Figure 4. The other compares the fault\nhandling recommendations of IDAS-LLM for traction loss across different train classes, as shown\nin Figure 5, with the CR400AF series and CRH3C / CRH380B (L) series trainsets selected for"}, {"title": null, "content": "comparison."}, {"title": null, "content": "From the responses in Figure 4, it is clear that IDAS-LLM thoroughly references the con-\ntent of retrieved documents similar in nature, summarizing and stating the information to provide\nsatisfactory handling suggestions for three different traction loss scenarios under the CR400AF\nseries trainsets."}, {"title": null, "content": "From the responses in Figure 5, it is apparent that IDAS-LLM retrieves the emergency fault\nhandling manuals corresponding to the different train classes, CR400AF and CRH3C / CRH380B(L).\nBased on the retrieved content, IDAS-LLM summarizes and provides effective suggestions."}, {"title": null, "content": "In general, for traction loss fault scenarios, IDAS-LLM is capable of retrieving the relevant\nemergency fault handling manuals based on the provided diagnostic codes or phenomena observed\nin the driver's cab. By combining the retrieved content with the user's actual needs, it provides\nfault handling suggestions that have practical reference value."}, {"title": "Scenario Two: Simulation of Sensor Fault", "content": "Sensor faults are also among the most common issues in various systems during train op-\nerations. Given the high operating speeds of trainsets, it is crucial to promptly inform the driver\nand the on-board mechanic in the event of a fault. Consequently, numerous sensors are installed\non trains to immediately report any malfunctions detected. Similarly, two simulation methods are\ndesigned. One is for the CR400AF series trainsets, where different types of sensor faults and\ncorresponding diagnostic codes are provided to demonstrate IDAS-LLM's retrieval and handling\ncapabilities in the face of various sensor faults, as shown in Figure 6. The other method compares\nthe fault handling suggestions of IDAS-LLM for speed sensors across different train classes, as\nshown in Figure 7, with the CR400AF series and CRH380B(L) series trainsets selected for com-\nparison."}, {"title": null, "content": "From the responses in Figure 6, it is evident that under the CR400AF model, IDAS-LLM"}, {"title": null, "content": "accurately retrieves the corresponding emergency fault handling manuals for speed sensor and axle\ntemperature sensor faults, and provides guidance on fault handling based on the content of these\nmanuals."}, {"title": null, "content": "From the responses in Figure 7, it is manifest that IDAS-LLM retrieves the handling meth-\nods for speed sensor faults under two different train classes, assisting drivers in completing the\noverall fault handling process."}, {"title": null, "content": "Overall, the IDAS-LLM system demonstrates specific adaptability in handling sensor fault"}, {"title": null, "content": "scenarios. It can effectively retrieve the appropriate emergency fault handling manuals based on\nthe specific train class and type of fault sensor. Additionally, the system integrates the search\nresults with the user's specific needs to provide targeted fault handling."}, {"title": "CONCLUSION", "content": "In this work, the IDAS-LLM framework, a large language model based system designed to\nassist in managing frequent high-speed train faults, is proposed. By incorporating fine-tuning and\nRAG, the LLM's knowledge base in the railway domain and the explainability of its responses are\nenhanced, equipping it with fault handling capabilities in real operational scenarios and providing\na valuable tool for train drivers. Despite these achievements, the framework's limitations still\nexist. Although fine-tuning and RAG alleviate LLM hallucinations, they cannot guarantee the\naccuracy of all responses, and driving suggestions provided in text form may lead to issues such as\ndistracting train drivers.\nIn the future, efforts can be directed towards creating more realistic and effective railway\ndatasets and incorporating work related to knowledge graphs to further enhance the system's ac-\ncuracy and explainability. Additionally, exploring how to maintain the system's real-time perfor-\nmance while improving accuracy is a worthwhile direction to investigate."}]}