{"title": "Analyzing Fairness of Classification Machine Learning\nModel with Structured Dataset", "authors": ["Ahmed Rashed", "Abdelkrim Kallich", "Mohamed Eltayeb"], "abstract": "Machine learning (ML) algorithms have become integral to decision-making in various domains,\nincluding healthcare, finance, education, and law enforcement. However, concerns about fairness\nand bias in these systems pose significant ethical and social challenges. This study investigates the\nfairness of ML models applied to structured datasets in classification tasks, highlighting the\npotential for biased predictions to perpetuate systemic inequalities. A publicly available dataset\nfrom Kaggle was selected for analysis, offering a realistic scenario for evaluating fairness in\nmachine learning workflows.\nTo assess and mitigate biases, three prominent fairness libraries\u2014Fairlearn by Microsoft, AlF360 by\nIBM, and the What-If-Tool by Google\u2014were employed. These libraries provide robust frameworks\nfor analyzing fairness, offering tools to evaluate metrics, visualize results, and implement bias\nmitigation strategies. The research aims to assess the extent of bias in the ML models, compare the\neffectiveness of these libraries, and derive actionable insights for practitioners.\nThe findings reveal that each library has unique strengths and limitations in fairness evaluation and\nmitigation. By systematically comparing their capabilities, this study contributes to the growing\nfield of ML fairness by providing practical guidance for integrating fairness tools into real-world\napplications. These insights are intended to support the development of more equitable machine\nlearning systems.", "sections": [{"title": "1 - Introduction", "content": "Machine learning algorithms are widely used in various domains, including entertainment,\nshopping, healthcare, finance, education, law enforcement, and high-stakes areas like loans [1]\nand hiring decisions [2, 3]. They provide advantages such as tireless performance and the ability to\nprocess numerous factors [4, 5]. However, algorithms can also exhibit biases, leading to unfair\noutcomes [6, 7]. Bias in machine learning can lead to discriminatory outcomes, especially when\ndecisions directly affect individuals or communities. Addressing these issues is essential to ensure\nthat machine learning systems operate ethically and equitably. Fairness in decision-making\nrequires the absence of prejudice or favoritism based on inherent or acquired characteristics, and\nbiased algorithms fail this standard by skewing decisions toward certain groups.\nThe concept of \"fairness\" in algorithmic systems is heavily influenced by the sociotechnical\ncontext. Various types of fairness-related harms have been identified:\n1. Allocation Harm: Unfair distribution of opportunities, resources, or information, such as an\nalgorithm selecting men more often than women for job opportunities [8].\n2. Quality-of-Service Harm: Disproportionate failures affecting certain groups, e.g., facial\nrecognition misclassifying Black women more often than White men [9], or speech\nrecognition underperforming for users with speech disabilities [10].\n3. Stereotyping Harm: Reinforcement of societal stereotypes, such as image searches for\n\"CEO\" predominantly showing photos of White men [8].\n4. Denigration Harm: Offensive or derogatory outputs from systems, like misclassifying\npeople as gorillas or chatbots using slurs [8].\n5. Representation Harm: Over- or under-representation of certain groups, e.g., racial bias in\nwelfare fraud investigations or neglect of elderly populations in public-space monitoring [8].\n6. Procedural Harm: Decision-making practices violating social norms, such as penalizing\njob applicants for extensive experience or failing to provide transparency, justification, or\nappeals for algorithmic decisions [11].\nThese harms often overlap and are not exhaustive, emphasizing the need for careful consideration\nof fairness from the development stage of algorithmic systems.\nThis research focuses on the fairness analysis of machine learning models when applied to\nstructured datasets using classification problems. Structured datasets are widely utilized in\nmachine learning applications due to their organized nature, which allows for efficient analysis and\nprocessing. However, biases in structured datasets, often stemming from historical inequalities or\nsystemic discrimination, can propagate into ML models, necessitating robust fairness evaluation\nand mitigation strategies.\nTo conduct this study, a publicly available dataset [12] from Kaggle was selected. Kaggle datasets\nprovide diverse and realistic scenarios for analyzing machine learning models, making them ideal\nfor this type of research. The dataset was preprocessed and used to develop classification models,\na common task in machine learning that involves predicting discrete labels based on input"}, {"title": "3-Dataset and Model Details", "content": "The study utilized the Adult Income dataset from the UCI Machine Learning Repository [12], which\nis publicly available and commonly used for classification tasks. The dataset contains\ndemographic and income-related attributes, aiming to predict whether an individual's income\nexceeds $50,000 annually. Key features include age, education, occupation, work hours, marital\nstatus, race, and gender, alongside a binary target variable indicating income class.\nTo process the dataset, categorical variables were encoded using techniques such as one-hot\nencoding, and continuous variables were normalized to ensure compatibility with the machine\nlearning models. The dataset was split into training and testing subsets using an 80-20 split to\nevaluate model performance.\nFor the model, the pipeline employed several machine learning algorithms, including Logistic\nRegression, Decision Trees, and Gradient Boosting. Gradient Boosting, specifically implemented\nusing XGBoost, emerged as the most effective algorithm for the classification task.\nHyperparameter tuning was performed using grid search to optimize the model's performance.\nThe study evaluated the model using standard classification metrics such as accuracy, precision,\nrecall, and F1-score. In addition, fairness metrics such as demographic parity and disparate impact\nwere calculated to analyze potential biases. Results highlighted disparities in prediction accuracy\nacross demographic groups, with notable differences between male and female individuals and\namong racial groups. These findings underscored the importance of incorporating fairness\nevaluations into traditional performance assessments for machine learning models."}, {"title": "4 - Implementation of Fairness Analyses", "content": "In recent years, the increasing reliance on machine learning (ML) in various sectors has led to\ngrowing concerns over fairness and bias in classification models. As these models can significantly\ninfluence decision-making processes in critical areas such as healthcare, finance, and criminal\njustice [28], ensuring their fairness has become imperative. Bias in ML models can manifest due to\nvarious factors, including skewed training data, model selection, and underlying societal biases,\nultimately leading to discriminatory outcomes against marginalized groups [29].\nTo address these challenges, several libraries and tools have been developed to assist practitioners\nin analyzing and mitigating bias in their models. Among them, Fairlearn, AIF360, and What-If Tool\nstand out as comprehensive resources that offer unique functionalities for fairness evaluation and\nenhancement.\nFairlearn [30]: Developed by Microsoft, this toolkit helps data scientists assess and improve the\nfairness of their Al models by providing a suite of metrics to evaluate fairness and algorithms for\nmitigating unfairness. Fairlearn emphasizes the importance of both social and technical\ndimensions of fairness in Al systems. It facilitates the understanding of how different aspects of a\nmodel contribute to disparities among groups.\nAIF360 [31]: This comprehensive library offers a wide range of metrics for assessing fairness and\ntechniques for mitigating bias across the entire Al application lifecycle. Developed by IBM, AIF360\nincludes methods that can be integrated into different stages of the machine learning pipeline to\nfacilitate fairness-aware modeling. It includes metrics for evaluating fairness across different\nsocietal demographics and offers re-parameterization strategies to improve model robustness.\nWhat-If-Tool [32]: Created by Google, this interactive visualization tool allows users to explore and\nanalyze machine learning models without requiring any coding. It supports performance testing in\nhypothetical scenarios, facilitating the understanding and explanation of model behavior. It\nenables users to observe model performance across different demographics and explore various\n\"what-if\" scenarios. It supports users in understanding how changes in input features affect model\npredictions, thus empowering them to conduct deeper bias analysis.\nTogether, these tools are essential for researchers aiming to understand and mitigate bias in\nmachine learning classification models, equipping them with the methodologies to ensure\nequitable Al systems and informed decision-making processes."}, {"title": "5- Results and Discussions", "content": "In this section, we discuss the results and outcome of each fairness library. The main objective of\nour analysis is to reduce/improve the fairness metric and maintain or improve the performance\nmetric. The codes and details can be found in [33]. The sensitive feature that we measured the bias\nin is gender. We investigate the model if it is biased against any group (male/female) in gender.\nFor Fairlearn, we used accuracy to measure the model's overall performance in predicting income\nclassification and demographic parity difference as a fairness metric to evaluate the bias in the\nmachine learning model. Accuracy provided a clear assessment of the predictive capability, while\nthe demographic parity difference quantified disparities in outcomes across different demographic\ngroups.\nTo address and mitigate the detected biases, we applied mitigation algorithms at three stages of\nthe machine learning pipeline: preprocessing, in-processing, and postprocessing. Preprocessing\ntechniques involved modifying the training data to reduce bias before feeding it into the model. For\nexample, sensitive features in a dataset may be correlated with non-sensitive features. The\nCorrelation Remover addresses this by eliminating these correlations, while preserving as much of\nthe original data as possible, as evaluated by the least-squares error. In-processing methods\nintegrated fairness constraints into the model training process, with approaches such as\nexponentiated gradient ensuring that the model learned fairer decision boundaries. Postprocessing\nfocused on adjusting the predictions after model training, ensuring that the final outputs adhered to\nfairness criteria without retraining the model such as threshold optimizer which is built to satisfy\nthe specified fairness criteria exactly and with no remaining disparity [34, 35, 36].\nBeyond these individual techniques, we also explored the use of combined mitigation approaches,\nwhere two algorithms were applied in series to enhance fairness outcomes. For instance,\npreprocessing adjustments were complemented by postprocessing tweaks, leading to improved\nalignment with both accuracy and fairness objectives. This combined approach aimed to leverage\nthe strengths of each mitigation stage to produce more equitable and reliable model predictions.\nThe evaluation process identified the best algorithm as the one that achieved a dual objective:\nmaximizing performance metrics such as accuracy, precision, and recall while minimizing bias\nmetrics like demographic parity difference. This comprehensive evaluation ensured that the\nselected method not only improved predictive power but also promoted equitable treatment across\ndemographic groups. Our findings highlight the importance of an integrated approach to fairness,\nwhere multiple strategies are utilized in conjunction to address complex biases inherent in\nstructured datasets.\nFor AlF360, we employed two distinct mitigation algorithms within each stage of the machine\nlearning pipeline to comprehensively address bias. In the preprocessing stage, Reweighing was\nidentified as the most effective technique, where it assigned weights to instances based on their\nrepresentation in different demographic groups. This approach ensured a balanced distribution of\ndata, directly addressing biases embedded in the training dataset.\nIn the post-processing stage, Equalized Odds proved to be particularly impactful. This algorithm-\nimposed constraints during model training to ensure that predictive outcomes were not\ndisproportionately distributed across sensitive attributes such as race or gender. By enforcing\nparity in true positive and false positive rates, Equalized Odds enhanced fairness without\nsignificantly compromising model accuracy.\nWe selected the Average Odds Difference (AOD) as the model performance metric, which was\nmeasured at 0.09 for the baseline model, compared to the ideal value of 0.00. For the fairness\nmetric, we used the Statistical Parity Difference (SPD), which had a baseline value of 0.13, whereas\nthe ideal value is also 0.00.\nThe applications of Reweighing and Equalized Odds yielded superior results, as these methods\neffectively reduced both bias and prediction error. By complementing the adjustments made during\ndata preprocessing with fairness constraints integrated into the model training process, this\napproach addressed multiple facets of bias simultaneously. This synergy between algorithms\nensured that the final model exhibited not only improved fairness metrics but also maintained\nrobust performance on traditional classification metrics. For the Reweighing algorithm, AOD is\n0.007 which is 8.3% better than the baseline value and SPD is 0.01 with 12% better than the\nbaseline value. For the Equalized Odds algorithm, AOD is 0.0001 which is 9% better than the\nbaseline value and SPD is 0.0 with 13% better than the baseline value. See Table 2 for the results.\nOur findings underscore the importance of selecting appropriate mitigation strategies tailored to\nspecific stages of the machine learning pipeline. By leveraging the strengths of Reweighing and\nEqualized Odds, we demonstrated that it is possible to achieve a balanced trade-off between\nfairness and accuracy, highlighting the potential of integrated approaches to bias mitigation in\nstructured datasets.\nIn the What-If Tool, adjusting the threshold for the labeled class impacts both the model's\nperformance and its bias metrics. The optimal thresholds identified for this model were 0.2 and 0.4.\nAt these thresholds, the model performance metric (Accuracy) increased from 0.33 to an average of\n0.62, representing a 29% improvement, while the bias metric (Demographic Parity Difference)\ndecreased from 0.19 to 0.01, reflecting an 18% reduction. These results are summarized in Table 3."}, {"title": "6- Conclusion", "content": "This study examined the fairness of machine learning models in classification tasks using\nstructured datasets, focusing on how biased predictions can reinforce systemic inequalities. A\nKaggle dataset was analyzed to provide a realistic scenario, utilizing three fairness libraries\u2014\nFairlearn (Microsoft), AlF360 (IBM), and the What-If Tool (Google)\u2014to evaluate and mitigate bias.\nThe research compared these libraries' effectiveness, highlighting their unique strengths and\nlimitations. The findings offered practical guidance for integrating fairness tools into machine\nlearning workflows, contributing to the development of more equitable Al systems."}]}