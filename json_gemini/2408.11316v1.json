{"title": "Probabilistic Medical Predictions of Large Language Models", "authors": ["Bowen Gu", "Rishi J. Desai", "Kueiyu Joshua Lin", "Jie Yang"], "abstract": "Large Language Models (LLMs) have demonstrated significant potential in clinical applications through prompt engineering, which enables the generation of flexible and diverse clinical predictions. However, they pose challenges in producing prediction probabilities, which are essential for transparency and allowing clinicians to apply flexible probability thresholds in decision-making. While explicit prompt instructions can lead LLMs to provide prediction probability numbers through text generation, LLMs' limitations in numerical reasoning raise concerns about the reliability of these text-generated probabilities. To assess this reliability, we compared explicit probabilities derived from text generation to implicit probabilities calculated based on the likelihood of predicting the correct label token. Experimenting with six advanced open-source LLMs across five medical datasets, we found that the performance of explicit probabilities was consistently lower than implicit probabilities with respect to discrimination, precision, and recall. Moreover, these differences were enlarged on small LLMs and imbalanced datasets, emphasizing the need for cautious interpretation and applications, as well as further research into robust probability estimation methods for LLMs in clinical contexts.", "sections": [{"title": "BACKGROUND", "content": "Generating credible probability of prediction is crucial in clinical practice and medical research when applying artificial intelligence (AI) to healthcare. Reliable probability outputs are critical for informed decision-making, stratifying patients with different risk levels, and enabling clinicians to set appropriate probability thresholds according to their preferred trade-offs in real-world clinical practice 1-4. For example, a lower threshold could be applied in screening applications to minimize missing cases. Accurate probability outputs significantly influence the adoption and effectiveness of AI in healthcare. Discriminative AI models, such as support vector machines and classic deep learning models5,6, predict labels by assigning probabilities to a fixed set of label candidates and selecting the label with the highest probability. This paradigm naturally generates the probability of the predicted label and has been widely adopted in medical AI applications5,7-9.\nGenerative Al models, especially Large Language Models (LLMs), have demonstrated remarkable general-purpose capabilities and the ability to perform few-shot or zero-shot learning, enabling accurate predictions with little or no annotated data10,11. These models are particularly advantageous in clinical applications where tasks are diverse and annotated data is scarce and expensive to generate12,13. Prompt-based LLMs represent the most common usage of LLMs, as they can flexibly instruct the models to perform different tasks by simply defining prompt text without heavy model training 14-16. However, this generative framework does not naturally output probabilities of the predictions, as it converts all the tasks as a text generation process, rather than assigning probabilities to fixed candidate labels. As a result, probabilistic predictions of LLMs in healthcare have been less reported and evaluated17-19, leading to a lack of crucial assessment of AI in clinical practice."}, {"title": "", "content": "Existing literature has utilized the generation of probability values directly through text generation by LLMs, this text-generated probability, or \u201cexplicit probability\u201d is simple and flexible20,21. For example, one can instruct the LLM output the probability of its prediction by adding a sentence such as \u201cPlease provide the probability along with your prediction\u201d to the prompt. The simplicity of explicit probability ensures that it can be applied in any advanced prompt engineering techniques such as Chain-of-Thought (CoT)22, retrieval augmented generation (RAG)23,24, self consistency25, and LLM agents26,27. However, given the challenges LLMs face with numerical reasoning28,29, the credibility of text-generated explicit probability numbers may be questionable and has not yet been thoroughly examined.\nOn the other hand, several works have delved into the parameter calculations of LLMs to extract the predicting probability of generating label tokens21,30, which we define as \"implicit probability\". For example, for the question \u201cGiven the following lab reports: [lab report text]. Does the patient have COVID? Your answer must be either \u2018Yes' or 'No'.\u201d, and if the LLM response is \u201cNo\u201d, one can extract the probability of each generated token of the LLM, identify the position of prediction token (i.e. \u201cNo\u201d in this example), and assign the model's implicit probability of the prediction as the probability that corresponds to the prediction token. However, such extraction of implicit probability is only available in minimal scenarios. As shown in Figure 1a, in information extraction tasks, the LLM predicted label tokens are diverse and with different lengths; when advanced LLM prompts, such as CoT, are applied, the predicted label tokens are not in a fixed position. These factors complicate the extraction of implicit probability, making it challenging to automate the extraction across different tasks and prompts. In addition, many advanced proprietary LLMs, such as Google Geminia and Anthropic Claudeb, do not provide APIs"}, {"title": "", "content": "that return the probability for each output token, which makes it infeasible to get the implicit probability. Consequently, this implicit probability can only be extracted under very simple settings (e.g., for option selection tasks or tasks without a chain of thought) and is only available in open-source LLMs, which largely limits its application.\nIn this study, we systematically examined the credibility of the probabilistic output of LLMs by comparing the explicit probability and the implicit probability. Specifically, we selected 6 advanced open-weighted LLMs from different organizations and evaluated their performance on 5 medical datasets. To utilize the Area Under the Receiver Operating Characteristic (AUROC) and Area Under the Precision Recall Curve (AUPRC) as evaluation metrics, we convert the LLM prediction task into a binary selection task within a question-and-answer setting with two possible options. We found that while explicit probability reflects a certain level of model prediction confidence, its credibility is consistently lower than that of implicit probability across all LLM models and datasets, especially in the case of small LLMs and imbalanced datasets.\nThe contributions of this work are multifaceted. To the best of our knowledge, this is the first study that systematically explore the probabilistic medical predictions of LLMs, a crucial component of applying AI in healthcare. Moreover, we demonstrated that the commonly used explicit probability of LLMs has lower credibility under certain circumstances, which necessitates caution when applying it in the medical domain. In addition, our study provides a framework for evaluating the credibility of probabilistic outputs of LLMs in medical predictions, which can be easily extended to other fields requiring high-quality probabilistic assessments."}, {"title": "METHODS", "content": "Data Source"}, {"title": "", "content": "This study was conducted on five datasets: four open-access datasets Measuring Massive Multitask Language Understanding Clinical Knowledge (MMLU-CK)31,32, Measuring Massive Multitask Language Understanding College Medicine (MMLU-CM)31,32, United States Medical Licensing Examination (USMLE)33, Mainland China Medical Licensing Examination (MCMLE)33 and one internal EHR dataset, Mass General Brigham - Social Determinant of Health (MGB-SDoH)34. The MMLU-CK, MMLU-CM, USMLE, and MCMLE datasets are publicly available through HuggingFace, where the MMLU-CK and the MMLU-CM datasets are the \"clinical_knowledge\" (299 questions) and the \u201ccollege_medicine\u201d (200 questions)d subsets of the MMLU dataset, respectively. For the USMLE and MCMLE datasets, we randomly selected a 1000-question subset due to the computation resource restriction. MGB-SDoH is a private multiple-choice dataset from real-world EHR notes of the MGB healthcare system. It contains the progress notes of 200 patients and is annotated in 9 different SDoH aspects, including marital status, number of children, employment status, educational status, lifestyle factors (use of tobacco, alcohol, illicit drugs, exercise), and cohabitation status34. This work was approved by the Brigham and Women's Hospital Institutional Review Board."}, {"title": "Experiment Settings", "content": "We selected well-performing open-source LLMs released by 6 different organizations on the LLM leaderboard hosted by LMSYS35-43. Within each organization's models, we primarily used the large models (Qwen2-72B-Instruct, Meta-Llama-3.1-70B-Instruct, gemma-2-27b-it, Mistral-Large-Instruct-2407, Yi-1.5-34B-Chat, and Phi-3-medium-128k-instruct) for our study. The corresponding small models (Qwen2-7B-Instruct, Meta-Llama-3.1-8B-Instruct, gemma-2-9b-it, Mistral-7B-Instruct-v0.3, Yi-1.5-9B-Chat, and Phi-3-mini-128k-instruct) are used to compare the differences in terms of model sizes on the USMLE and the MGB-SDoH datasets. All LLMs used in this study are publicly available. The details and sources of the LLMs are listed in Table S2.\nOur experiments are conducted on an MGB server with 8 x NVIDIA H100 GPUs. AutoTokenizer and AutoModelForCausalLM modules from HuggingFacef are used to load the tokenizer and LLM, respectively. During inference time, we set $max\\_new\\_tokens = 64$ to enable fast response generation. We set $return\\_dict\\_in\\_generate=True$ and $output\\_scores=True$ to enable LLM output transition scores, which are a list of float numbers representing the log probability of LLM output tokens. These scores are essential to calculate the implicit probability. To calculate the standard deviation and the statistical significance, we repeated the experiment three times with $temperature = 0, 0.3$ and $0.7$ respectively. Specifically, for the standard deviation, we used the $np.std$ function from the Python numpy package with $ddof= 1$ since we want the sample standard deviation. For the statistical significance, we conducted a one-sided paired t-test (whether the implicit probability is statistically better than the explicit probability) on the explicit and the implicit probability using the $stats.ttest\\_ref$ function from the Python scipy packageh."}, {"title": "Prediction and Probability Extraction", "content": "To utilize the AUROC and AUPRC as the evaluation metrics that are common in the literature for predictive modeling and are intuitively understood by readers, we convert each question of the datasets from the multiple-choice to binary format (i.e., question answer with two possible answers). Specifically, for each question in these datasets, we extracted its correct option and randomly selected one of the other options to build the two candidate choices. To eliminate the impact of the option order to LLMs performance44, we randomly assigned 50% of the correct options as Option A and the remaining 50% of the correct options as Option B. We then designed a prompt (Table S1) to instruct the LLMs to output their decisions, the probabilities of the decision being correct, and the corresponding explanations following a pre-defined format. Based on the pre-defined output format, we used regular expressions on the response text to extract LLMs' prediction and the corresponding explicit probability, as illustrated in Figure 1b. To extract the implicit probability, we firstly applied the exponential function on the log probabilities according to the transition scores to get the probability of each token being selected as the next token, then we extracted the probability of the token that contains the model's prediction token (\u201cA\u201d or \u201cB\u201d). Validation scripts are implemented to ensure that the extracted content does correspond to the model prediction and the two probabilities. We defined a case as invalid if the validation failed and set the corresponding explicit and implicit probabilities as $np.nan$ so that it is not used in the evaluation."}, {"title": "Evaluation", "content": "Accuracy, AUROC, and AUPRC are used as the evaluation metrics using manually labeled correct answers as the reference standards in each of these datasets. The values are averaged by the results of the three experiments, together with the standard deviation. AUROC is the primary metric as it is the most common metric for the probability prediction. AUPRC is used to compare the performance difference on imbalanced data experiments. Distributions from explicit and implicit probabilities are also visualized for better comparison. P-values are reported to indicate the statistically significance of the one-sided paired t-test."}, {"title": "Imbalanced Datasets Analysis", "content": "It is common that labels are imbalanced distributed in clinical practice, and the performance of AI models are highly affected by the imbalanced distribution. To investigate the difference between explicit and implicit probabilities under imbalanced datasets, we reconstructed the most commonly used medical LLM evaluation dataset, USMLE, with different imbalanced distributions. Specifically, we set a variety of ratios (5%, 10%, 30%, 50%, 70%, and 90%) of option A being the correct option. Then we randomly swapped option pairs of the original prompt until the ratio of option A being correct reached the designated values. Since the metric of AUPRC is highly affected by the imbalance label distribution, we calculated the AUPRC of the two probabilities and compared their differences."}, {"title": "Sensitivity Analysis", "content": "To ensure the robustness of our results, we conducted a sensitivity analysis by using two additional prompts (Table S1) on the USMLE dataset and analyzed if the relative difference between the explicit and the implicit probability persists under different prompt settings. Except the prompt, all experiment settings are the same with the main experiment."}, {"title": "RESULTS", "content": "All LLMs follow instructions well, with outputs strictly adhering to the format in the prompt, except for the experiment with the Qwen2-7B model on the USMLE dataset. The details of instruction adherence for each LLM experiment are shown in Table S7."}, {"title": "LLM Performance", "content": "The accuracy, AUROC, and AUPRC of the large LLMs on different datasets are listed in Table 1 to Table 3, respectively. The \u201cExplicit\u201d stands for the AUROC of the explicit probability, \"Implicit\" stands for the AUROC of the implicit probability, and \u201cDifference\u201d stands for the difference between the AUROC of the implicit probability and the AUROC of the explicit probability. For Table 2 and Table 3, the \u201cP-value\u201d indicates the statistical significance of the implicit probability over the explicit probability."}, {"title": "", "content": "According to Table 1, all LLMs achieved at least a 0.8 average accuracy on all datasets, which indicates the LLMs' satisfactory performance on these datasets. Specifically, the Llama-3.1-70B model has the highest average accuracy among all the LLMs tested, which shows its capability as a more advanced model than the other ones. On the other hand, the Qwen2-72B model, which is trained using a rich Chinese corpus, shows a significant advantage over other models on the MCMLE dataset, which consists of Chinese medical examination questions. The Mistral-Large model, which has comparable capability to the Llama-3.1-70B model, achieved the highest accuracy for the MMLU-CM and the MGB-SDoH dataset but falters on the non-English MCMLE dataset."}, {"title": "", "content": "Although the AUROC and AUPRC for both the explicit and implicit probabilities are high, the metrics for implicit probabilities are consistently higher than those for explicit probabilities across all LLMs, and the majority of the results show that such advantage is statistically significant. This finding is valid on both a non-English dataset (MCMLE) and a dataset from private electronic health records (MGB-SDoH). The only exception is the Llama-3.1-70B model on the USMLE dataset, which is mainly due to the high standard deviation of the implicit probability. We believe that this can be resolved by repeating the experiments a few more times. Overall, this result indicates that for LLMs, the implicit probability is a better indicator of the model answer's confidence (Tables 2 and 3).\nThe ROC and PRC curves of the LLMs on each dataset are shown from Figure S1 to Figure S10. We observed that the ROC and PRC curves for the explicit probability have much fewer transition points compared to the ROC and PRC curves of the implicit probability. This is because the LLMs usually give their probabilities to their nearest tenth (e.g. 90%, 80%, etc.), which makes the explicit probability coarse-grained. On the contrary, the implicit probability, which is obtained by calculating the transition scores, has a much finer-grained probability prediction. This further proves that the implicit probability is a better indicator than the explicit probability.\nWe also found that in some cases, while the LLM appropriately outputs its answer and the corresponding explicit probability according to the instructions, the explicit probability is less than 50% (Table S3). This means while the model proposed one option to be the correct answer, its proposed probability favored the other option, which makes the model's proposed answer not credible. On the other hand, the implicit probability does not have such a contradiction since it is always the most probable token among all the tokens within the model's vocabulary. This serves as another proof of the implicit probability's credibility over the explicit probability."}, {"title": "Large LLM VS Small LLM", "content": "The AUROCs of the large LLMs together with their smaller counterparts on the USMLE and the MGB-SDoH datasets are shown in Figure 2. According to Figure 2, except for the case of applying Yi-1.5 LLM on the MGB-SDoH dataset, all other large LLMs outperform their small compartments on both the explicit and the implicit probability AUROC. This is expected since LLMs with more parameters have richer knowledge and stronger reasoning and understanding capabilities. Figure 2 also shows that except for the case of the Yi-1.5 LLM on the MGB-SDoH dataset, all other small LLMs have larger probability AUROC differences than their small compartments on both the explicit and the implicit probability AUROC. This can be explained by the fact that since small LLMs have fewer parameters, they are less sensitive to the probabilities that they respond to, which makes the explicit probability from the small LLMs less credible. However, the implicit probabilities for small LLMs do not show as much decrease as that of the explicit probabilities, which shows that when using small LLMs, the implicit probability is even more credible to represent the model's actual probability to its prediction."}, {"title": "Imbalanced Dataset Analysis", "content": "The AUPRC differences of the large LLMs are shown in Figure 3a, where the percentage of the imbalance in the legend indicates the percentage of option A being the correct label in the dataset, and a 50% imbalance means that there is an equal number of option A and option B being correct in the dataset. The difference in the figure is defined as the difference between the AUPRC of the implicit probability and the explicit probability. Based on Figure 3, for most LLMs the AUPRC difference increases as the dataset got more imbalanced. This can be explained by how the AUPRC is calculated. Since we defined option A as positive when plotting the AUPRC, the scarcity of option A being the correct label played a significant role in determining the precision and the recall, which are the keys to the PRC curve. As the number of options A being correct gets lower, any false classification will be magnified, which enlarges the differences between the two probabilities if one is better than the other. We did not show the results under the 70% and 90% imbalance rates"}, {"title": "", "content": "since they are very similar to the ones under the 50% imbalance ratio. This is also reasonable given that the PRC is not related to the true negatives, and the excessive number of options A being correct will minimize the differences between the two probabilities."}, {"title": "LLM probability distribution", "content": "The probability distribution of each LLM on each dataset is shown from Figure S11 to Figure S15 and an example of the Meta-Llama-3.1-70B-Instruct LLM on the USMLE dataset is shown in Figure 3b. According to Figure 3b, except for the Phi-3-medium model on the MCMLE dataset, all other LLMs had a relatively great performance on each dataset by having low probability when the true label is 0 and high probability when the true label is 1, which is consistent with the consistent performance we found (Table 1). By comparing the explicit and implicit probability distribution, we noticed that the implicit probability has wider distribution than the explicit probability, which is consistent with what we found on the AUROC and AUPRC curves. Also, we noticed that only the explicit probability may give values that are close to 50%, while the implicit probability values rarely fall between 40% to 60%.\nAdditionally, for most LLMs, both their explicit and implicit probability distributions are very polarized, even if their predictions are incorrect. This indicates that the LLMs are overly confident about most of their predictions, regardless of their actual correctness. Since such polarization still persists on the MGB-SDoH dataset, a private dataset that can't be touched during LLM training, we argue that such confidence is not originated from the data leakage45, but is an intrinsic property of the LLMs. Some exceptions to the above observation are the Mistral-Large and the Phi-3-medium model on the MCMLE and the MGB-SDoH dataset, where models show a broad spectrum of implicit probability distribution, but their explicit probability distributions are still highly polarized."}, {"title": "Sensitivity Analysis", "content": "The AUROCs of the LLMs on the USMLE dataset using the three different prompts are shown in Figure S16. We learned that the main prompt and the first auxiliary prompt have similar performance in terms of AUROC for both probabilities, which is reasonable since both prompts used a similar multiple-choice format and contained similar information. The AUROC for both probabilities significantly decreased when using the second auxiliary prompt. We proposed that this was due to the introduction of the setting of a student proposing answers to the LLM, which introduced additional complexity and lowered the model's performance. The phenomenon that the AUROC of the implicit probability is always higher than that of the explicit probability still holds for all three prompts, meaning that such a phenomenon is not due to the uniqueness of a particular prompt and can be used to prove the higher credibility of the implicit probability over the explicit probability."}, {"title": "DISCUSSION", "content": "This study conducted a systematic investigation of probabilistic prediction of LLMs in healthcare and demonstrated that the simple and flexible explicit probability (directly extracted from the generated text following the prompt) provided relatively high AUROC and AUPRC, but was consistently less reliable than the implicit probability (derived from the transition score of the predicted label token) across different languages, datasets, and prompt designs. The performance gap between explicit and implicit probabilities was especially large for small LLMs.\nProviding predictions with reliable probabilities is crucial for the application of AI in healthcare. Instead of forcing LLM to make binary predictions, estimating the probability based on LLM allows clinicians to apply different thresholds to the LLM-estimated probabilities according to different clinical needs. For example, if the question is to identify patients to receive"}, {"title": "", "content": "low-risk intervention with minimal risks (e.g., responding to an online questionnaire), one may favor a threshold of lower predicted probability to identify as positive case. On the other hand, if the downstream intervention to a positive test is an invasive procedure (e.g., biopsy or endoscopy), one may favor a threshold of higher predicted probability. Incorrect probability estimation in medical research or clinical practice led to misinformed clinical decisions, which may compromise patient safety and treatment efficacy46,47. Our study highlights the critical need to use the LLMs' explicit probability with caution, particularly for small LLMs and tasks with imbalanced label distribution, underscoring the importance of validating the credibility of LLM probabilistic predictions before relying on them in clinical settings.\nWe demonstrated that as LLM performance declines, the credibility of explicit probabilities increasingly falls behind that of implicit ones (Figure 3, Table S5). This suggests that reliance on explicit probability can exacerbate disparities or biases inherent in LLMs, particularly when their performance is inadequate. The suboptimal performance of LLMs often stems from a lack of training data for minority languages, insufficient domain knowledge, and dataset imbalances-issues that are particularly prevalent in the clinical domain48. Specifically, existing LLMs are predominantly trained on English-language corpora49, while many countries' medical systems operate in minority languages that LLMs are not familiar with. Furthermore, most advanced publicly available LLMs are primarily trained on general domain datasets and lack exposure to real-world EHR notes due to data inaccessibility and privacy concerns48. This results in a deficiency of medical domain knowledge, thereby limiting their effectiveness in clinical applications24,50. Additionally, it is common that datasets are highly imbalanced in medical phenotypes, such as presence of serious clinical endpoints or rare diseases51 and cohort identification7. When explicit probability is applied in these clinical settings, the poor performance"}, {"title": "", "content": "of LLMs, coupled with the resulting increased disparity and bias, impairs the accurate and meaningful interpretation of these probabilities.\nThe flaw of the explicit probability underscores the need for cautious use in clinical settings and highlights the importance of enhancing LLM explicit probability outputs by integrating implicit probabilities, which is one of the future works of this study. One promising approach is to fine-tune LLMs' explicit probability output with the supervision of implicit probability, thereby guiding models to generate more accurate probabilities, similar to the work of improving the implicit chain of thought capability of LLMs with external CoT supervision52. Additional approaches that improve the LLMs' capability of numerical reasoning may also be utilized to enhance the explicit probability output29.\nRecent work has shown that during LLM inference, there is a correlation between predictive uncertainty and hallucination a behavior where LLMs generate false facts or knowledge not supported by input53. Hallucination from LLMs poses a substantial risk in clinical applications, potentially leading to incorrect diagnoses, inappropriate treatments, or other detrimental outcomes, undermining trust in LLMs within healthcare. If the explicit probability generation can be enhanced to accurately represent LLMs uncertainty of prediction, it may be served as an important indicator for LLM hallucinations. One of the main reasons an LLM generates output with low explicit probability is a lack of sufficient knowledge, which can further drive the model to produce hallucinated information for predictions54,55. Consequently, predictions with low explicit probability may signal a higher likelihood of hallucination in the LLM's output. We leave the detection of LLM hallucinations with enhanced explicit probability as future work, believing this may pave the way for further research on detecting and mitigating LLM hallucinations in healthcare."}, {"title": "", "content": "There are several limitations in our study. Our experiments were simplified to binary classification settings to facilitate the extraction of implicit probabilities and the calculation of AUROC and AUPRC. As a result, the probability differences for other tasks such as multiple-choice questions answering, or information extraction may be different. Besides, we did not examine the probability performance using Chain of Thought (CoT) prompting, as it is challenging to extract implicit probabilities in these cases. Additionally, our study only focuses on the open-sourced LLMs since it is difficult to obtain the implicit probability of the tokens using proprietary LLMs. Finally, our results are primarily based on medical datasets, the generalizability to different domains needs to be confirmed by further validation studies.\nThis study serves as an important reference for the credibility of LLM-generated probabilities in medical applications and beyond. Our observations have important implications for fields sensitive to predicted probabilities, such as medicine, law, and finance. By highlighting the limitations of the explicit probability and proposing potential improvements, our work can facilitate a flexible and trustworthy application of LLM-generated probabilities according to clinical contexts and needs."}, {"title": "CONCLUSION", "content": "This study represents a pioneering effort to systematically explore the probabilistic outputs of LLMs in the medical domain, an essential aspect of LLM application in healthcare. By comparing explicit and implicit probabilities across multiple advanced LLMs and medical datasets, we have uncovered consistent discrepancies in credibility, particularly under conditions of small model size and dataset imbalance. These findings underscore the need for caution when relying on explicit probability in clinical settings, where the stakes are high, and the accuracy of probabilistic"}, {"title": "", "content": "predictions is paramount. This research informs future studies focused on enhancing the trustworthiness of LLM in healthcare by improving the quality and reliability of probabilistic predictions."}, {"title": "AUTHOR CONTRIBUTIONS", "content": "J.Y. designed the study. B.G. conducted the experiments and analysis. B.G. and J.Y. drafted the manuscript. J.Y. and J.K.L supervised the study. All authors revised the manuscript. B.G. takes responsibility for the integrity of the work."}, {"title": "COMPETING INTERESTS", "content": "None."}]}