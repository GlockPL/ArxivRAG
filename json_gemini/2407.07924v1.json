{"title": "Solving General Natural-Language-Description Optimization Problems with Large Language Models", "authors": ["Jihai Zhang", "Wei Wang", "Siyan Guo", "Li Wang", "Fangquan Lin", "Cheng Yang", "Wotao Yin"], "abstract": "Optimization problems seek to find the best solution to an objective under a set of constraints, and have been widely investigated in real-world applications. Modeling and solving optimization problems in a specific domain typically require a combination of domain knowledge, mathematical skills, and programming ability, making it difficult for general users and even domain professionals. In this paper, we propose a novel framework called OptLLM that augments LLMs with external solvers. Specifically, OptLLM accepts user queries in natural language, convert them into mathematical formulations and programming codes, and calls the solvers to calculate the results for decision-making. In addition, OptLLM supports multi-round dialogues to gradually refine the modeling and solving of optimization problems. To illustrate the effectiveness of OptLLM, we provide tutorials on three typical optimization applications and conduct experiments on both prompt-based GPT models and a fine-tuned Qwen model using a large-scale self-developed optimization dataset. Experimental results show that OptLLM works with various LLMs, and the fine-tuned model achieves an accuracy boost compared to the prompt-based models. Some features of OptLLM framework have been available for trial since June 2023 (https://opt.alibabacloud.com/chat or https://opt.aliyun.com/chat).", "sections": [{"title": "1 Introduction", "content": "Optimization problems have been widely investigated in real-world domains including financial investment (Ye et al., 2020), supply chain management (Li et al., 2023), logistics transportation (Xie et al., 2020) and competitive strategy (Silver et al., 2017). Such ubiquitous optimization problems raise critical demands for efficient modeling and solving methods.\nCurrently, modeling and solving optimization problems in a specific domain usually involves three steps (Ramamonjison et al., 2022). First, based on domain knowledge, experts summarize the application scenarios into problem descriptions using natural language or mathematical formulas, with clear indication of variables, objectives, constraints, and parameters. Second, experts extract and encode critical information from the problem descriptions with modeling languages such as Python, R or AMPL. Finally, the optimization process is carried out by experts or solvers to obtain the final decision-making results. Meanwhile, the entire process calls for a combination of domain knowledge, mathematical skills, and programming ability, which is unfriendly to beginners or even professionals in that domain.\nRecently, large language models (LLMs) have demonstrated strong capabilities in natural language understanding and generation (OpenAI, 2023). However, despite LLMs' strong performance across a range of NLP tasks (e.g., content generation and Q&A dialogue) (Brown et al., 2020), their ability in arithmetic and logical reasoning may be insufficient and unfaithful (Imani et al., 2023). On the other hand, data privacy remains one concern for online services like GPT-4 (OpenAI, 2023). That is, inclusion of domain-specific information in prompts may cause data breach at the LLM service provider side or during transmission in public networks, even under the service level agreements for privacy (Li et al., 2023). Hence, deployment of open-resourced LLMs (e.g., Llama (Touvron et al., 2023), PaLM (Anil et al., 2023), and Qwen\u00b9) is preferred for privacy-sensitive applications.\nIn light of these above, we propose OptLLM, a framework unifying either open-sourced LLMs or online LLM services, and external solvers for automated modeling and solving of optimization problems. Specifically, OptLLM consists of three"}, {"title": "2 Related Work", "content": "LLMs, or large language models, are predominantly Transformers (Vaswani et al., 2017) trained on extensive text corpus from various sources (e.g., webs and books (Brown et al., 2020)). They are trained to predict the next token in a given context, and could generate coherent responses after fine-tuning and alignment (OpenAI, 2023). Below we briefly cover applications and techniques related to automated optimization problem modeling and solving using LLMs."}, {"title": "2.1 Applications of LLMs", "content": "With the widespread attention on LLMs, their applications are popping up in varied domains, such as open-domain Q&A (Liu et al., 2023), database management (Zhou et al., 2023), and strategizing agents (Yao et al., 2022). Studies on arithmetic reasoning, or mathematical reasoning (Qiao et al., 2022), investigate the ability of LLMs to solve math word problems (MWP) (Patel et al., 2021). Existing work mainly focus on general math problems including function evaluation, numerical calculation and theorem proving (Imani et al., 2023; Yang et al., 2023). Unfortunately, the reasoning ability of LLMs is still far from being usable (Wang et al., 2022) and even competent models like GPT-4 are inconsistently bad at numeric calculations. In contrast, our work relies on LLMs to model optimization problems, and external solvers for solving them."}, {"title": "2.2 Techniques of LLMs", "content": "To adapt LLMs to downstream tasks, two strategies are commonly used: prompting and supervised fine-tuning (SFT) (Liu et al., 2023). Prompting, also known as in-context learning, leverages additional task information, zero to a few domain-specific examples, and expected answer format to guide LLMs without additional training. Recent works show that specially-designed prompts, such as those via chain-of-thoughts (Wei et al., 2022), iterative refinement (Madaan et al., 2023) and black-box prompt tuning (Sun et al., 2022), can significantly improve the performance of LLMs on downstream tasks. On the other hand, SFT leverages task-specific data and objective functions to train LLMs, which demonstrates a significant enhancement in downstream applications (Baldazzi et al., 2023). SFT is more effective than prompting when such task-specific data are available."}, {"title": "3 Proposed Framework: OptLLM", "content": "We propose OptLLM that unifies LLMs and external solvers for automated modeling and solving of optimization problems. By designing OptLLM to interact with domain users via natural language, we hope to reduce the need for specialized knowledge on optimization or coding, and improve the experience for end-users. OptLLM primarily consists of three modules: interaction refinement module, converter module, and responser module."}, {"title": "3.1 Interaction Refinement Module", "content": "As shown in Figure 1, the interaction refinement module consists of Step 1 to 4 (marked in orange). Step 1, the user queries OptLLM in natural language. Step 2, the queries are pre-processed, including inserting instructions and prompt engineering. The pre-processing is used to clarify the task and output formats for LLMs. For online LLM service like GPT, a typical instruction could be \u201cYou are an operation research expert and your task is to model the optimization problem given its description in natural language.\u201d The queries are then checked in the 'Complete' part. Complete queries should have clear indication of variables, objectives, constraints, and parameters for optimization. If the user's queries are complete, the queries are sent to the next module for modeling. Otherwise, OptLLM detects some information is missing and request user to provide more details. We will provide an example application in Application 2 below. In practice, OptLLM responds to user inputs in various scenarios. If a user's queries are unrelated to optimization problems, OptLLM would prompt and guide the user towards asking optimization"}, {"title": "3.2 Converter Module", "content": "The Converter module contains Step 5 to 9 in Figure 1 (marked in blue). The module is used to convert problem descriptions in natural language to codes and check their grammar. Step 5 receives the output of the interaction refinement module and passes it to the 'Formulator' of OptLLM. The Formulator translates natural language descriptions into the corresponding formulas for objective and constraints. Then in Step 6, the formulas are fed into 'Coder' to generate corresponding code in a preset programming language. In \u2018Grammatical', the code will be checked for grammar mistakes. If the syntax test fails, it enters the diagnostic module and OptLLM reformulates it based on its own feedback. Otherwise, the code will be sent to an external solver.\nWe use MindOpt Algebraic Programming Language, or MAPL\u00b2 as the default programming language. Designed by Alibaba, MAPL is an efficient and versatile modeling language that supports many mainstream solvers, including MindOpt, Gurobi, CPLEX, Ipopt, Cbc. We use MindOpt\u00b3 by default."}, {"title": "3.3 Responser Module", "content": "The Responser Module consists of Step 10 to 15 in Figure 1 (marked in green). In Step 10, the programming code is sent to the solver. In Step 11, the 'Interpreter' block collects the solver's solution"}, {"title": "4 Applications", "content": "Our framework can solve generic optimization problems based on their natural language description. In this part, we introduce three basic applications, including single-round QA with complete description, multi-round conversations with missing information detection, and optimization problem solving with external data."}, {"title": "4.1 Application 1: Single-round QA", "content": "In the single-round QA application, we assume the user has provided a complete natural language description of the optimization problem such that the variables, objective and constraints can be deduced. This application is often used in the education, e.g., when a student enters a complete optimization problem into the system, or when a teacher lectures a student with a complete problem. As show in Figure 2, the \u2018Formulator' block generates the corresponding formulas, with the ability to automatically detect variable names which are not explicitly specified in the problem description; the 'Coder' then generates the corresponding"}, {"title": "4.2 Application 2: Multi-round Conversations", "content": "The Application 1 assumes user has provided a complete problem description. In many scenarios, users may not provide such a description at once, especially if they would like to gradually build up a complex problem. In light of this, it is necessary to guide them step by step through interactions to provide the necessary information for modelling optimization problems. We hope to start with the simplest chat, detect the missing information, and gradually guide users through interaction to provide necessary information indicating the variables, objective and constraints."}, {"title": "4.3 Application 3: External Data Files", "content": "There are scenarios where the data for an optimization problem cannot be concisely tabulated or embedded in the problem description. In addition, LLMs typically have a token limit of a few thousands, which could be easily exceeded by the lengthy descriptions or multiple rounds of interactions, if lots of data are embedded. To address this, we design OptLLM to accept external data files with a predetermined format. Users may query the system with instructions on from which files each part of the data can be acquired. Inspired by LangChain, the data will not be passed to the model to save tokens and further preserve data privacy. Instead, only the external solver will access the data files in order to calculate the final solutions."}, {"title": "5 Fine-tuning Large Language Model", "content": "OptLLM permits both online LLM service or open-sourced LLMs as the base model. In this section, we introduce the fine-tuning process on Alibaba's self-developed Qwen model. Considering the large size of the Qwen model we use (50B parameter version) and a limited budget (eight NVIDIA V100 GPUs), the data scale and existing hardware do not support continuous pre-training or full parameter fine-tuning. Thus, we adopt LoRA (low-rank adaption) (Hu et al., 2021), a parameter-efficient fine-tuning scheme (PEFT) (Houlsby et al., 2019).\nAs shown in Figure 4, at each linear layer of the Qwen model, LoRA inserts two trainable low-rank matrices \\(A \\in \\mathbb{R}^{d \\times r}\\) and \\(B \\in \\mathbb{R}^{r \\times d}\\) to approximately optimize the original parameters: \\(W_{\\text{new}} = W + AB\\), where x is our fine-tuning data, and h is the output of the linear layer, W is the fixed original parameter matrix. Overall, the amount of parameters introduced by LoRA is below 1% of the original model."}, {"title": "6 Experiments", "content": "We focus primarily on linear programming (LP) and mixed integer linear programming (MILP) problems, which may be of strong interests in industrial applications. To the best of our knowledge, there is currently no publicly available datasets on general optimization problems except the data from NL4OPT competition (Ramamonjison et al., 2023). Few additional problems can be crawled from websites, but they may still not be sufficient for fine-tuning the LLMs. Thus, we constructed our own fine-tuning and test datasets. We ensure that Qwen model has not seen the test datasets during the pre-training phase.\nFine-tuning Dataset. Figure 5 shows the data collection process. To build a large-scale dataset, we start with seed optimization problems manually designed by experts, followed by designing prompts and calling LLMs to generate more problems. We then manually label the data and select prompts that perform well. The resultant prompts are used to generate more data, which are again manually labelled. The labeled data can be used as seeds for the next round of data generation. This process is repeated several times. Finally, we collected a high-quality optimization training dataset with total 15k instances in English and Chinese."}, {"title": "6.2 Metrics", "content": "In this study, we focus on evaluating the model performance on single-round QA as in Application"}, {"title": "6.3 Implementation Details", "content": "We compare the finetuned Qwen model against two prompt-based models: GPT-3.5 (gpt-3.5-turbo) (Ouyang et al., 2022) and GPT-4 (OpenAI, 2023) under our OptLLM framework. For GPT-3.5 and 4, we use the standard one-shot prompt: \u201cYou are an expert in mathematical programming. Please refer to Case 1 and provide a JSON expression for Problem 1 with explanations. Case1: {Question_and_Answer_of_Case1}, Problem1: {Question}.\u201d We have also tried prompts with more shots but the performance does not improve significantly, so we stick to one shot. For Qwen model, we fine-tune it using LoRA as describe in previous section. LORA is inserted at every linear layer of the model. The dimension r of all LORA layers is set to 32. The AdamW optimizer is used with an initial learning rate of 0.0002, \\(\u03b2 = [0.9, 0.999]\\), and a linear decay schedule. The number of training epochs is set to 20 with a mini-batch size of 32 due to limited GPU memory. The model is implemented under HuggingFace's Transformers library (Shen et al., 2023) and trained on eight NVIDIA V100 GPUs using DeepSpeed Zero stage 3 (Yao et al., 2023)."}, {"title": "6.4 Results", "content": "Overall performance. As shown in Table 1, the supervised fine-tuning Qwen, or Qwen-SFT, surpassed GPT-3.5 on both datasets. It also achieved comparable performance to GPT-4 on CN100 and exceeded GPT-4 on EN100. We manually identified the specific error causes - GPT-3.5 and GPT-4 made mistakes in identifying strict constraints, e.g., the problem description states \u201cA is more than B\", that is, \"A > B\", but both GPTs inferred \"A > B\". In contrast, Qwen-SFT had more successes in identifying such constraints, owing to the fine-tuning process enabling it to learn sophisticated patterns.\nImpact of fine-tuning epochs. Figure 6 shows that, the model's performance on the test datasets improves with more fine-tuning epochs, and start to plateaus after 10 epochs. Given the prolonged training time, we set the fine-tuning epochs to be 20 by default.\nImpact of data diversity. To investigate the influence of fine-tuning data size on model performance, we vary the number of samples and epochs so that, in each setting, the model is trained on roughly the same number of tokens. We fine-tune and evaluate the model on Chinese data only. As shown in Table 2, the model performance increases along with the data size. This indicates that we should collect as many diverse data as possible to achieve better results."}, {"title": "7 Path to Deployment", "content": "The proposed OptLLM framework can be deployed on the cloud. We take Alibaba Cloud as an example to illustrate the deployment of OptLLM. As shown in Figure 7, the infrastructure includes: i) the OSS provides data storage for user data that may be used in Application 3; ii) Redis is used for recording online conversation context; iii) ODPS is used for logging historical logs.\nDashScope is an inference platform that supports both existing LLM APIs (e.g., GPT-3.5 and GPT-4) or self-built LLMs (e.g., Qwn-SFT and Llama2-SFT). External tools include Solver, such as MindOpt, and Chimp, a testing platform for the entire framework. Once deployed on the cloud, the proposed OptLLM framework has the potential to support applications in various domains, such as educational services, financial investment and supply chain management. In June 2023, we have deployed the first version on Alibaba Cloud, which includes some of the features introduced in this paper, with more features currently under development."}, {"title": "8 Limitation", "content": "Although our system is capable of handling single-round optimization problems, as well as multi-round addition, deletion, and modification operations for some optimization problems, our model's effectiveness will be somewhat affected when dealing with incomplete issues that require additional knowledge for certain parts. This is because this extra knowledge may not be possessed by our large model due to certain reasons, such as our model's knowledge base being up-to-date only until 2023, meaning it wouldn't be aware of knowledge from 2024. There are two ways to address this issue: one is to update the underlying large model in real-time, but this would entail significant financial and material costs. The other option involves using methods related to Retriever-Augmented Generation (RAG). These are aspects we plan to explore in our future work."}, {"title": "9 Conclusion", "content": "In this paper, we propose OptLLM, an effective framework that augments LLMs (such as Qwen model and GPT-4) with external solvers for automated modeling and solving of optimization problems. Specifically, OptLLM comprises three modules: the interaction module for completing the problem description, the converter for translating the description into code, and the responser for calling solvers and interpreting the results, respectively. By iterating the above steps through chatting with users, OptLLM has the potential to assist both beginners and domain professionals to achieve faithful decision-making for optimization problems. We illustrate the effectiveness of OptLLM with three proof-of-concept applications and experiments. In the future, we will focus on promoting the diversity of optimization problems by including more real-world cases from various domains and scenarios. We will also explore methods to enhance arithmetic and logical reasoning, as well as more open-sourced LLMs and evaluation methods."}]}