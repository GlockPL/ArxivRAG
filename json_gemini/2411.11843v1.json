{"title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models", "authors": ["Shengkun Tang", "Liqun Ma", "Haonan Li", "Mingjie Sun", "Zhiqiang Shen"], "abstract": "The typical selective state-space model (SSM) of Mamba addresses several limitations of Transformers, such as quadratic computational complexity with sequence length and significant inference-time memory requirements due to the key-value cache. However, the growing size of Mamba models continues to pose training and deployment challenges and raises environmental concerns due to considerable energy consumption. In this work, we introduce Bi-Mamba, a scalable and powerful 1-bit Mamba architecture designed for more efficient large language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba models are trained from scratch on data volume as regular LLM pertaining using an autoregressive distillation loss. Extensive experimental results on language modeling demonstrate that Bi-Mamba achieves performance comparable to its full-precision counterparts (e.g., FP16 or BF16) and much better accuracy than post-training-binarization (PTB) Mamba baselines, while significantly reducing memory footprint and energy consumption. Our study pioneers a new linear computational complexity LLM framework under low-bit representation and facilitates the future design of specialized hardware tailored for efficient 1-bit Mamba-based LLMs.", "sections": [{"title": "1 Introduction", "content": "The Selective State-Space Model (SSM) (Gu et al., 2021b, 2022) has recently emerged as a powerful alternative to Transformers (Vaswani et al., 2017) in language modeling, demonstrating comparable or superior performance at small to moderate scales. SSMs are characterized by linear scaling in sequence length during training and a constant state size during generation, which significantly reduces computational and memory overhead, making them more efficient in terms of speed and memory usage.\nThe representative SSM model of Mamba (Gu and Dao, 2024; Dao and Gu, 2024) has a significant advantage when handling long context sequences due to its linear complexity. In contrast, conventional transformers suffer from quadratic complexity as the sequence length increases. This means that for tasks involving large input sequences or extended contexts, Mamba is much more efficient, as its memory and computational requirements scale linearly with the sequence length. In practice, this allows Mamba to process long sequences faster and with lower resource consumption, making it ideal for applications such as long document processing, conversational agents, and any scenario where managing large amounts of contextual information is crucial. On the other hand, transformers require exponentially more resources as the context length increases, which can quickly become a bottleneck in long-context tasks.\nPrior works on Transformers have been extensively studied for several years (Devlin et al., 2019; Radford et al., 2019; Touvron et al., 2023; Achiam et al., 2023), with numerous methods proposed to alleviate their high computational and storage costs,"}, {"title": "2 Related Work", "content": "Post-Training Quantization. Due to their large parameter count, large language models (LLMs) (Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023) are resource intensive to run. Therefore they are often quantized to low bits representations during inference. This type of quantization is typically called post-training quantization (PTQ) (Dettmers et al., 2022b; Xiao et al., 2023; Wei et al., 2022; Frantar et al., 2023; Yao et al., 2022; Xiao et al., 2023; Lin et al., 2024), where the quantization operation is applied to the pre-trained models. Post-training quantization methods of LLMs are typically based on the empirical observations that a small set of salient features in pre-trained Transformers have significantly large magnitudes, and quantization of these features needs to be extra careful (Xiao et al., 2023; Lin et al., 2023). PTQ methods have led to near lossless accuracy drop for INT8 weights and activations quantization, and INT4 weight-only quantization. While many works (Huang et al., 2024; Egiazarian et al., 2024; Shao et al., 2023; Chee et al., 2023; Tseng et al., 2024) have been focused on pushing post-training weight quantization below 4 bits, they often leads to drastic drop of model performance.\nQuantization-Aware Training. Different from post-training quantization, quantization-aware training (QAT) aims to learn the quantized models during training. In the LLM era, QAT is less investi-"}, {"title": "3 Approach", "content": "3.1 Preliminary: Mamba Series\nMamba belongs to a class of models known as State Space Models (SSMs), which is able to offer performance and scaling laws comparable to the Transformer while remaining practical at extremely long sequence lengths (e.g., one million tokens). It achieves this extended context by eliminating the \u201cquadratic bottleneck\u201d present in the attention mechanism. The vanilla structured state space sequence models (S4) transform a 1-dimensional sequence  $x \\in \\mathbb{R}^T$ into another sequence $y \\in \\mathbb{R}^T$ by utilizing an latent state $h\\in \\mathbb{R}^{T,N}$ as follows:\n$h_t = A h_{t-1} + B x_t$\n$Y_t = C h_t + D x_t$ (1)\nwhere $A \\in \\mathbb{R}^{N,N}$, $B \\in \\mathbb{R}^{N,1}$, $C \\in \\mathbb{R}^{1,N}$, $D \\in \\mathbb{R}$. $h_{t-1}$ is the hidden state, $x_t$ is the input, the observation that the model gets each time. $h_t$ then represents the derivative of the hidden state, i.e. how the state is evolving.\nSince the parameters in Equation 1 are constant through time, this model is linear time-invariant (LTI). The LTI model gives equal attention to all elements when processing sequences, which prevents the model from effectively understanding natural language. To address this, Mamba improved it to the Selective State Space Model, where B and C are obtained through a linear projection of xt.\n3.2 Bi-Mamba\nBinarization Space in Mamba. To begin with, we need to identify what weight matrices can be binarized in Mamba architecture. We use the latest Mamba-2 (Dao and Gu, 2024) as our base architecture. According to the description above, we can interpret the SSD matrices of A, B, C, D, and Abias more intuitively, as well as other layers including embedding, layer normalization (LN), Conv-1d, inner linear projection, out linear projection in Mamba-2 to better understand the effect after binarization, so that to determine the binarization space. Briefly, A is the transition state matrix, representing how the current state transitions to the next state. It answers the question of how should the model gradually forget the less relevant parts of the state over time? B maps the new input to the state, addressing the point of which parts of the new input should the model retain? C maps the state to the output of the SSM, asking that how can the model utilize the current state to make an accurate next prediction? D shows how the new input directly influences the output, acting as a modified skip connection and asking how can the model incorporate the new input into the prediction?\nConsidering our base architecture Mamba-2, we present the specific parameters proportion for different layers across different sizes as shown in Figure 2 and Table 1. It is observed that, in Mamba-2, the vast majority of the model\u2019s parameters are in the linear modules, excluding the causal head. For instance, in Mamba-2-2.7B, these parameters account for 95.2% of the entire model. Additionally, the embedding module shares parameters with the causal head. Binarizing the embedding significantly diminishes its capability to represent token semantics, thereby reducing model performance. Therefore, in our Bi-Mamba, we only binarize the parameters in the linear modules (excluding the causal head). This strategy aims to maintain a high compression ratio of 90% while still allowing the binarized model to perform effectively.\nSimple Learnable Scaling Factors for Better Capability. To binarize Mamba, we replace the original linear modules with the FBI-Linear module introduced by FBI-LLM (Ma et al., 2024a). Specifically, the FBI-Linear module primarily consists of two parts: a matrix $W_b \\in \\mathbb{R}^{m \\times n}$ made up solely of {1, -1} and high-precision scale factors $a \\in \\mathbb{R}^n$ and $B\\in \\mathbb{R}^n$. The inference process of FBI-Linear is as follows:\n$y = W_b x$ (4)\nwhere $W$ is derived by performing column-wise multiplication with a and addition with \u1e9e respectively:\n$W = a_i W + B_i$ (5)\nwhere ai and B\u2081 are the learnable scaling and shifting factors at i-th layer.\nObjective of Training. Our training objective is a cross-entropy loss between the outputs of the target student model and the pretrained teacher model at each step of the autoregressive scheme for next-token prediction. This can be expressed as:\n$\\mathcal{L}_{Bi-Mamba} = \\frac{1}{n} \\sum_k \\textbf{p}_T (x_{k+1}) \\cdot log \\textbf{p}_s (x_{k+1})$ (6)\nwhere n is the number of input tokens. Here, $\\textbf{p}_T (x_{k+1})$ represents the token distribution over the vocabulary at the k-th step predicted by the teacher model, and $\\textbf{p}_s (x_{k+1})$ is the corresponding predicted distribution by the student model.\nOverall Design of Bi-Mamba. During binarization-aware training of autoregressive distillation (Ma et al., 2024a), we compute the cross-entropy between the output probability distributions of a high-precision pretrained model and our target Bi-Mamba model. In this process, a and \u1e9e are learnable parameters, while Wb is derived using the sign() function from a learnable high-precision matrix $W_f \\in \\mathbb{R}^{m \\times n}$. Since the sign(\u00b7) function is non-differentiable, we use the Straight Through Estimator (STE) (Bengio et al., 2013) as prior studies (Rastegari et al., 2016; Alizadeh et al., 2019) in BNNs to approximate the gradients of the input variables, enabling the continuation of backward propagation."}, {"title": "4 Experiments", "content": "In our experiments, we solely binarize the parameters in the most linear modules, while keeping other model parameters and activation at original precision. Notably, we do not strictly represent the binarized parameters with 1-bit, instead, we use high-precision values to simulate the binarized parameters. We train Bi-Mamba on different scales and evaluate their performance across multiple tasks.\n4.1 Setup\nDataset. Following FBI-LLM, we train Bi-Mamba with the Amber dataset (Liu et al., 2023). This dataset contains a total 1.26 Trillion tokens from RefinedWeb (Penedo et al., 2023), StarCoder (Li et al., 2023a), and RedPajama-v1 (Computer, 2023).\nThe data is segmented into 360 chunks, each comprising approximately 3.5 billion tokens on average.\nTraining details. We train Bi-Mamba on different scales in Mamba-2 architectures. Specifically, we binarize input projection and output projection matrices in 780M, 1.3B and 2.7B Mamba-2 models. We use LLaMA2-7B as the teacher model for all Bi-Mambas to calculate autoregressive distillation loss. Therefore, all Bi-Mambas we trained have the same vocabulary and tokenizer as LLaMA2. We train models with 32 NVIDIA A100 GPUs in total and maintain BF16 precision while training.\nEvaluation Metrics. We evaluate the models based on their zero-shot performance in downstream tasks including BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), HellaSwag (Zellers et al., 2019), Winogrande (Sakaguchi et al., 2021), ARC (Clark et al., 2018), and OpenbookQA (Mihaylov et al., 2018). All downstream evaluations are done with lm-evaluation-harness package (Gao et al., 2024). We also use perplexity on Wikitext2 (Merity et al., 2016), PTB (Marcus et al., 1993), C4 (Raffel et al., 2020) dataset as the evaluation metric. Perplexity measures how well a probability model predicts a token, quantitatively measuring the model\u2019s generation power.\nBaselines. We compare our work with quantization and binarization methods, namely GPTQ (Frantar et al., 2023) and Bi-LLM (Huang et al., 2024). GPTQ is a post-training quantization method while Bi-LLM is a post-training binarization method. We apply the official implementation of GPTQ and quantize the Mamba-2 models into 3 and 2 bits, respectively. For Bi-LLM, we also utilize their official implementation and binarize Mamba-2 models. Furthermore, we include results from open-source full-precision transformer-based models of various sizes, such as OPT (Zhang et al., 2022), and TinyLLaMA (Zhang et al., 2024), as references.\n4.2 Main Results\nTable 2 presents the comparison of our Bi-Mamba model against various baselines on downstream tasks and perplexity on Wiki2, PTB, and C4 datasets. These evaluations provide insight into model generalization capabilities without further task-specific fine-tuning. The visualization of performance comparison is shown in Figure 3.\nFor the 780M Mamba-2 model, Bi-Mamba demonstrates an average downstream performance of 44.5, outperforming GPTQ-3bit and Bi-LLM,"}, {"title": "5 Analysis", "content": "5.1 Training Result Dynamics\nIn this section, we discuss the performance of Bi-Mamba as the training progresses with different training costs/budgets. The main results are shown in Figure 4. We provide the downstream performance and perplexity curve along different training costs on the Mamba-2-780M model. More results on 2.7B and 1.3B Mamba-2 models can be found in the Appendix. From the figure, we can observe that the perplexity decreases quickly at the beginning of training and gradually converges to the full-precision perplexity. The perplexity of Bi-Mamba on the wiki2 and C4 datasets is more stable than the perplexity on the PTB dataset. Notably, on the C4 dataset, the final perplexity of Bi-Mamba is even lower than the full-precision model, highlighting the superior performance of Bi-Mamba. Interestingly, early in training, our Bi-Mamba model surpasses GPTQ-3bit on the perplexity, demonstrating the effectiveness of binarization-aware training. Since the perplexity of GPTQ-2bit and BiLLM is extremely high on all datasets, we ignore them in the figure and refer to Table 2 for detailed results of GPTQ-2bit and BiLLM. Moving to the downstream task evaluation, we first observe the catastrophic performance degradation of the binarized models, whose performance is even lower than the random results on many benchmarks such as the results on ARC-E and ARC-C. This indicates that directly applying the naive binarization method destroys the ability of the full-precision model. However, after binarization-aware training, the model recovers the performance on all benchmarks and finally outperforms all baselines including GPTQ-3bit, GPT-2bit and Bi-LLM. This underscores the importance of binarization-aware training in achieving competitive results.\n5.2 Binarization Space\nIn this section, we explore the binarization space of Mamba models and discuss the effect of binarizing each part. We conduct experiments that binarize the In_Proj and Out_Proj separately and binarize both. The binarized models are trained with the same data. The results are shown in Table 3. Partial and full binarization are compared with the full-"}, {"title": "6 Conclusion", "content": "We introduced Bi-Mamba, a scalable and efficient 1-bit Mamba architecture designed for large language models in multiple sizes: 780M, 1.3B, and 2.7B parameters. We begin by identifying the binarization space within the Mamba architecture. Then, Bi-Mamba models are trained from scratch on large datasets, similar to standard LLM pretraining, using an autoregressive distillation loss. Extensive language modeling experiments show that Bi-Mamba achieves competitive performance that is slightly lower than its full-precision counterparts (e.g., FP16 or BF16), while substantially reduces memory usage and computational cost compared to the original precision Mamba. This study provided a novel, first accessible and low-bit framework with linear computational complexity, laying the foundation for developing specialized hardware optimized for efficient 1-bit Mamba-based LLMs."}, {"title": "7 Limitations and Ethical Statements", "content": "While Bi-Mamba achieves competitive performance to full-precision models, there may still be trade-offs in accuracy, particularly in complex tasks that rely heavily on nuanced language understanding. Also, full deployment may require specialized hardware to maximize efficiency gains, limiting accessibility on standard hardware setups. On ethical part, reducing model precision could risk oversimplifying nuanced patterns in data, potentially amplifying biases present in the training data. Moreover, while Bi-Mamba reduces energy consumption during inference, training binary models from scratch can still be computationally intensive."}, {"title": "Appendix", "content": "A Training and Implementation Details\nFor configuring different sizes of Bi-Mamba, the details can be found at Table 5. We follow the same architectures as the original Mamba-2 models and apply binarization on both input and output projection matrices. The training process uses the Adam optimizer with parameters \u03b2\u2081 = 0.9 and B2 = 0.95. The initial learning rate is set at 2.5e \u2013 4 and follows a cosine schedule, decreasing to 2.5e \u2013 5 over 2,000 warm-up steps. Gradient clipping is set at 1.0.\nDue to the limited computational resources, the training of Bi-Manba is not full. Currently, we train Bi-Mamba 780M, 1.3B, 2.7B with 12, 10 and 11 data chunks respectively. Even so, the results are already competitive for our models. We will continue training the model depending on the resource situation and update the corresponding results accordingly.\nB More Experimental Results\nB.1 Full Results on Downstream Tasks\nWe present the performance dynamics of the 2.7B and 1.3B Bi-Mamba models during training in terms of perplexity and multiple downstream tasks, as shown in Figure 5 and 6. Each configuration demonstrates different trade-offs between model performance and computational efficiency of Bi-Mamba across various datasets.\nWe can observe that both the 2.7B and 1.3B Bi-Mamba models consistently outperform GPTQ-2bit and BiLLM after reaching a certain training stage. Specifically, Bi-Mamba generally exhibits a gradual decrease in perplexity, indicating the effectiveness of the training. In some cases such as Bi-Mamba-2.7B on C4 dataset, the perplexity is even better than in the full-precision Mamba models. In downstream tasks such as ARC-C, ARC-E, HS, and PIQA, Bi-Mamba consistently outperforms Bi-LLM, indicating that it is a more effective low-bit quantization approach for maintaining accuracy across various data sizes.\nAdditionally, the performance on average increases steadily with more training costs. The overall trend demonstrates that Bi-Mamba provides a robust alternative, balancing computational efficiency with competitive performance. This makes Bi-Mamba particularly valuable in resource-constrained environments where some trade-off in precision is acceptable. At the current training stage, the models have not yet fully converged, indicating potential for further performance gains.\nB.2 Weight Distribution\nWe visualized the weight distributions of different modules in Mamba-2 (Orange histograms) and Bi-Mamba (Blue histograms), as shown in Figure 7, 8 and 9. We visualize the weight parameter distributions of different modules in the first (1st), mid (24th) and final (48th) layers of the corresponding 780M models. The input and output projection matrices are the values after re-scaling. Each pair of histograms compares how Bi-Mamba modifies the distribution of weights in different modules, no matter whether the module is binarized or not, illustrating the impact of Bi-Mamba on each module.\nSpecifically, in the first layer, the weight distribution of the original Mamba-2 such as Conv1d.weight, Conv1d.bias and D are tightly concentrated, indicating the strong focus on specific values. In contrast,"}]}