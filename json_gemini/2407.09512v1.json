{"title": "Design and evaluation of AI copilots \u2013 case studies of retail copilot templates", "authors": ["Michal Furmakiewicz", "Chang Liu", "Angus Taylor", "Ilya Venger"], "abstract": "Building a successful AI copilot requires a systematic approach. This paper is divided into two sections, covering the design and evaluation of a copilot respectively. A case study of developing copilot templates for the retail domain by Microsoft is used to illustrate the role and importance of each aspect. The first section explores the key technical components of a copilot's architecture, including the LLM, plugins for knowledge retrieval and actions, orchestration, system prompts, and responsible AI guardrails. The second section discusses testing and evaluation as a principled way to promote desired outcomes and manage unintended consequences when using AI in a business context. We discuss how to measure and improve its quality and safety, through the lens of an end-to-end human-Al decision loop framework. By providing insights into the anatomy of a copilot and the critical aspects of testing and evaluation, this paper provides concrete evidence of how good design and evaluation practices are essential for building effective, human-centered AI assistants.", "sections": [{"title": "1 Introduction", "content": "The advent of Large Language Models (LLMs) has generated a huge amount of interest across industries in developing intelligent applications, with \u201cAI copilots\u201d \u2013 assistants that help humans in the execution of cognitive tasks \u2013 taking front stage [1]. These AI assistants can be employed to infuse existing technology with advanced capabilities, synthesizing understanding and generation of text, image and other modalities (such as audio and software actions) to enable completely new business use cases. However, building a successful copilot, even based mainly on text, comes with many challenges [2] [3]. LLMs often require additional context or examples on how to perform a task. They may lack the domain-specific knowledge to generate an appropriate response, necessitating a mechanism for retrieving relevant information. Furthermore, particularly for complex business scenarios, there may be a need for orchestration between subtasks, knowledge retrieval and actions the copilot needs to perform. Finally, care must be taken to ensure LLMs are used in an ethical and responsible way, particularly in highly regulated industries or business cases that involve high-stakes decisions or the use of sensitive data."}, {"title": "2 Architectural considerations", "content": "Building a copilot is not simply a case of exposing a Large Language Model to end users. Several components need to work together to ensure the copilot is focused on its task and role, has access to relevant knowledge, and behaves in a way that is ethical, safe and responsible. In this section, we explore these components which include the LLM for managing the human-copilot interactions, plugins for knowledge retrieval, orchestration to coordinate plugin execution, and the system prompt for controlling copilot behavior. We will use the copilot template for personalized shopping [6] as an example to illustrate the role of these components."}, {"title": "2.1 Copilot template for personalized shopping", "content": "The copilot template for personalized shopping aims to provide an interactive, conversational experience for online retail shopping. It is a template of an end-to-end Al copilot solution that can be configured and adapted for use by a range of retail companies. It is particularly relevant to retail companies that sell products for which personalized recommendations are valued by customers, for example fashion retail. The user can engage the copilot to search for clothes by expressing their desired product attributes and the copilot will respond with recommendations from the retailer's product catalog. The copilot is able to explain the suitability of recommended products based on the user's query and the product descriptions. Users can interact with the copilot in a natural conversational manner, providing feedback on the products recommended and asking for the copilot to adapt its recommendations based on this feedback.\nThe AI copilot has the flexibility to handle the diverse ways a human user might choose to search for products. In some cases, the user may make a straightforward request with a concrete set of criteria \u2013 for example, \"Show me a magenta dress\" \u2013 for which a simple keyword search of the product catalog is required. In others, the request may require more complex reasoning and judgement, for example, \"I'm going to a birthday party and the dress code is smart. Can you suggest some clothes I could wear?\". In the latter cases, the copilot may need to draw from multiple sources of knowledge to be able to make appropriate recommendations. The ability of the copilot to respond to a range of inputs, makes for a very natural human-copilot interaction and, by extension, an engaging shopping experience.\nThe copilot template for personalized shopping serves as an exemplar for the design of an Al copilot's architecture. The scenario requires the copilot to have mechanisms to perform information retrieval and a means of orchestrating between those mechanisms and generating responses to user requests, all while ensuring the copilot does not deviate from the confines of its role. Most scenarios will have similar requirements and the components that make this possible together form a reference architecture for Al copilots."}, {"title": "2.2 Employing Large Language Models in a copilot", "content": "The LLM forms the core of the AI copilot architecture. LLMs are complex machine learning models that have been pre-trained on a vast amount of text from a diverse range of sources and topics. They leverage deep learning techniques to learn complex patterns and relationships in language. As such, they can both comprehend and generate natural language with almost human level of fidelity. The performance of LLMs continues to improve and state-of-the-art models, such as GPT-4, perform well even on tests designed for humans [7].\nLLMs are known as foundation models as they can immediately be applied to a wide range of tasks involving the comprehension or generation of natural language. However, for all but the most trivial tasks, they must be provided context on how a task should be performed. These instructions are provided via the LLM's system prompt, a piece of text in which the developer can define the LLM's role, context and desired"}, {"title": "2.3 Plugins - knowledge retrieval and actions", "content": "LLMs are trained on huge corpuses of data from across the world wide web such as the 800Gb Pile dataset [9], forming an internal representation of a knowledge base that includes information about the real world. However, for many business applications, additional domain specific and proprietary knowledge is required to respond accurately to a user's request. This information will not have been included in the LLM's training data, and therefore needs to be provided to the model as additional context. Furthermore, LLMs have no knowledge of the world beyond the time period covered by their training datasets. Additional context will need to be provided to the LLM in cases where up-to-date organization- or user-specific information is required [10].\nPlugins have become an essential component of AI copilot architecture as a means of retrieving additional relevant context. Plugins are functions or tools that provide LLMs access to external data sources such as databases, APIs, or document repositories in order to retrieve relevant information for generating responses. This process is known as Retrieval Augmented Generation (RAG). Plugins can be conditionally executed as needed in the course of an interaction with the user. The information returned by the plugin can then be added to the final prompt that is sent to the LLM, providing the additional context or knowledge required to fulfil the user's request [11].\nOften, a plugin will return tabular data sourced from a database or API, as is the case with the product recommendations plugins featured in the copilot template for personalized shopping. The product catalog resides in a database that is accessed via an API using keywords extracted from the user's query. In other cases, where information is stored in large document repositories, more complex RAG solutions are required with the use of specialized vector databases as well as splitting documents into logical elements (chunking), intermediate summarization and more. The copilot template, for example, includes plugins that access document repositories to answer user queries related to store policies or the company brand.\nPlugins are not only useful as a means of implementing RAG. They may also enable Al copilots to perform actions as discussed above. Examples could include a checkout action in the context of the copilot template for personalized shopping, or an action to send an email to summarize a conversation with a customer service copilot. In these cases, the plugins would make a request to the APIs of other services that would perform the required action. Additionally, plugins can be used to enable the copilot to trigger subprocesses such as mathematical operations or data analysis, for which LLMs may not be the most effective tool.\nPlugins can also enable the development of a multi-agent architecture, facilitating smaller end-to-end automation loops consisting of multiple Al copilots that carry out specialized functions."}, {"title": "2.4 Orchestration", "content": "We have seen how LLMs, with access to information retrieved from plugins, are able to understand user intent and generate relevant and informed responses. To enable complex and dynamic business tasks, an Al copilot needs orchestration to enhance the basic capabilities of an LLM, by negotiating between user intent and plugin execution and maintaining a memory of the conversation. These two LLM-powered features of a copilot perform semantic understanding of the current context and history of the task \u2013 not dissimilar to a \"stateful\u201d software application but more powerful \u2013 in order to navigate the ambiguity and complexity of human-AI interactions. For example, a user of the copilot template for personalized shopping may not know the exact product they want at the start of the conversation and may begin the interaction with an ambiguous"}, {"title": "2.5 Controlling copilot behavior", "content": "When developing an AI copilot, it is critical to tightly define its role and desired behavior. This is important both for ensuring the copilot is provided with the instructions necessary to perform its task well, and for mitigating the risk of the copilot behaving in a way that is unintended or causes harm to the user. The component of the copilot architecture that provides such instructions is the system prompt. This is a document of natural language text containing the instructions that govern the copilot's behavior. A good system prompt will only be arrived at through iterative trial and error. Although state-of-the-art models such as GPT-4 excel at following instructions, it often pays to experiment with different prompts in a process known as prompt engineering. Even small changes to the system prompt, in terms of the content of the instructions or even the way instructions are phrased, can have a large impact on copilot behavior and performance. Testing frameworks and evaluation are required to assess the behavior during development, as discussed in Testing and Evaluation.\nA system prompt needs to clearly represent individual aspects of the Al copilot's desired behavior. Based on our experience in developing the copilot template for personalized shopping, here are some recommendations for sections that should be considered for inclusion in a system prompt:\nRole and context \u2013 this section should come first and give the LLM information about the role of the copilot. It should specify the context in which it operates including the industry and business use case it is being applied to. It should inform the LLM about who the user is or is likely to be. It should provide details of exactly what the copilot can do and what user questions or requests it is able to fulfil. Just as importantly, it should explain any limitations the copilot has and what user requests it cannot respond to.\nBusiness logic \u2013 in this section, detailed instructions can be provided on how the copilot can fulfil various user requests. It may provide examples of the types of requests a user is likely to make and give instructions on how to handle them, including plugins that are needed and what information should be included in the response.\nWorking with plugins \u2013 this section provides instructions that govern how the copilot can use plugins available to it. Aspects to consider include whether the copilot can make assumptions or inferences about the values that function parameters can take, or if they should be explicitly clarified with the user. Another consideration is whether the existence and details of functions can be made known to the user.\nLocalization - it is recommended that the system prompt includes instructions on if and how the copilot can operate in different languages. In this section, guidance can be included to indicate what language a user is likely to be using when interacting with the copilot and instructions can be provided as to what language the copilot should respond in.\nTone of voice and verbosity \u2013 copilots in different contexts and industries are likely to need to adopt different behaviors and styles of response. For example, a retail shopping copilot could adopt a casual or friendly tone of voice whereas a copilot in the medical domain may need to adopt a more serious, authoritative, or perhaps even sympathetic tone. Adjectives that describe the desired tone of voice should be used as well as instructions on how verbose or concise the copilot should be in its responses.\nOutput format \u2013 this section should describe how the output of the copilot should be formatted. Perhaps the copilot should respond in plain text or have a bias towards responding in bullet point lists. A common scenario is that the copilot needs to respond in a data format that can be parsed by a UI application (for example, JSON).\nResponsible AI guardrails \u2013 this section is of critical importance as it aims to ensure the copilot does not respond in a way that causes harm to the user. Instructions should be clear and unambiguous, explaining what the copilot must or must not do in cases where the response has the potential for harm. See below for more details on the aspects to consider for responsible AI guardrails."}, {"title": "2.6 Responsible Al guardrails", "content": "It's important to consider how guardrails can be introduced to the copilot's architecture that minimizes risks of causing harm.\nAutomated content filtering is one tool that can be used to minimize this risk. This approach employs natural language processing and machine learning techniques to scan incoming requests for text that may contain harmful content, including hateful, inequitable, sexual, or violent content. This allows for the pre-filtering of content before it even reaches the LLM for processing. Content filters can also be applied to the LLM's outputs to prevent the copilot responding with any potentially harmful content. Content filtering tools or services [12] can be included in the system's architecture to provide an additional layer of protection against the generation of harmful content, supplementing the protections built into LLMs through their training.\nHowever, mitigation tools like content filtering may not be able to cover all possible harms one would want to mitigate. To reinforce these guardrails, it is strongly advised to include further instructions in the system prompt such that the LLM has clear guidelines on how it should behave when the response has the potential for harm. The following list provides recommendations on what content should be included in the RAI guardrails section of the copilot system prompt:\nHarmful content \u2013 instructions should make clear that the copilot should not respond to user input that includes harmful content, nor generate such harmful content itself. This should further reinforce protections provided by content filtering. Additionally, consideration should be given to harms that may be relevant to the specific context, industry or business use case the copilot is being applied to.\nUngroundedness \u2013 LLMs have the potential to fabricate information that is inconsistent with common sense or knowledge, lacking factual accuracy. In some business use cases, the generation of such misinformation could lead to serious harm to the user and reputational damage to the business. To mitigate this, the copilot should be instructed to only use facts and information available to it from within system prompt or the output of plugins. Furthermore, it should be instructed to not respond to requests which fall outside the scope of the copilot's context.\nPersonal data and privacy \u2013 Instructions should be included to guide the copilot in how it handles sensitive or personal data, depending on the business use case. For example, in some cases it may be necessary to instruct the copilot to never reveal certain information to the user, even when that information is available to it when informing its response. In other cases, it may be necessary to instruct the copilot to not use certain personal characteristics in generating its response, where that information could influence the copilot to respond in a way that is biased or inequitable.\nJailbreaking - Jailbreaking refers to an attempt to circumvent the instructions or restrictions placed on the copilot's behavior. Jailbreaking can be used by malevolent actors to force the copilot to generate content or take actions that could cause harm, introduce security vulnerabilities, or reveal sensitive personal data or the intellectual property of the model itself. The sources of the jailbreaking can come from user input (direct attacks) or third-party data sources (indirect attacks) injected into the LLM to generate unsafe content. It is important to include guardrails against jailbreaking by including specific instructions to the copilot that none of the content or rules contained within the system prompt can be revealed, superseded or overridden by a user's instructions.\nNovel risks - this refers to emerging risks arising from a misalignment of AI model behaviors with human values. Examples include human-like behaviors such as manipulation and power-seeking, multi-modal risks such as a combination of benign images and texts that collectively result in inappropriate content, or custom risks that only arise in a context-specific scenario. These risks warrant special consideration in the system prompt unless more effective methods are available for mitigation."}, {"title": "3 Testing and Evaluation", "content": "Once a working version of a copilot has been developed, it is crucial to ask how its effectiveness and safety can be determined, and whether it is ready for deployment. To address this question, this section explores the testing and evaluation of copilots, illustrating the approach with a copilot template designed for store operations [13].\nIn our experience of building copilots for customers and partners, we observe that safety and quality are the top two business requirements. These needs generalize to two essential business goals of testing and evaluation: the first goal is to manage the desired business outcomes of using copilots as an emerging technology, and the second goal is to manage the unintended consequences from its use. The best practices to achieve these goals may be broadly considered as \u201cresponsible AI practices\u201d. Managing desired business outcomes is typically achieved through quality evaluations, which is concerned with measuring and improving the effectiveness of copilots. On the other hand, managing unintended consequences is achieved through safety evaluations, which is concerned with measuring and mitigating harms induced by the use of the technology. Quality and safety consist of their own taxonomies of metrics depending on the copilot's capabilities and potential harms associated with the use case."}, {"title": "3.1 Copilot template for store operations", "content": "To explain our approach to testing and evaluation of AI copilots, we refer to our experience of testing a copilot template for store operations. This template is configurable for different retail companies to build their own store operations copilots. The copilot assists retail store associates in store operations and procedures by allowing them to query their retailers' knowledge base. It achieves this goal with a Question-and-Answering (Q&A) feature which is capable of answering questions, drawing from unstructured data stored in various company documents [13].\nThe Q&A feature aims to address the challenge for store associates to memorize and retrieve procedural knowledge on demand and with precision. This task may be difficult for new associates until they develop procedural memory and become proficient through repeated practice. Moreover, some situations occur rarely making the procedures pertaining to them difficult for associates to memorize. Consulting a handbook or a colleague at peak hours may slow the operation of the stores.\nConsider the scenario of a customer coming to the store in person and asking to return an item. The associates must first determine the relevant information about the item including the type of item, its condition, and whether the item is \u201cre-sellable\" or not. The return procedures will be different for sellable items, damaged items, and special items such as jewelry, furniture, and digital items. In some circumstances, multiple processes may be executed. For example, re-sellable items trigger both a return process and a quality audit process.\nThe Q&A feature of the copilot template effectively offloads the tasks of memorizing and searching for the relevant information or context for a given situation. It synthesizes the context and the question to formulate an answer or guidance on how to proceed. If the quality of the copilot's output is good, the store associates will team up with the copilot and leverage its Al-powered skills to provide answers, and then follow standard operational procedures to resolve the situation."}, {"title": "3.2 Managing intended outcomes: measuring effectiveness of copilot capabilities", "content": "By building an Al copilot, businesses are able to offer value to customers better than their competition. Good use of AI capabilities in a copilot leads to positive business outcomes, building customer trust and loyalty, and improving profitability for the business. Conversely, poor use of the capabilities in a copilot, including poor fit for purpose and poor performance in domain-specific tasks, could lead to low-quality copilot releases that erode customer trust and the competitiveness of the product or service.\nIt is helpful to start with understanding the capabilities that AI offers. For simplicity, the focus here is on the most common scope of language AI (in contrast to image, voice, video, and a mix of these \"modalities\") and use one taxonomy proposed by Slack et al. for discussion [14]."}, {"title": "3.3 Managing unintended consequences \u2013 measuring potential harms in the use of copilots", "content": "Viewed as a systematic process, testing and evaluation is a critical means to manage the unintended consequences of using Al copilots. Mustafa Suleyman, EVP and CEO of Microsoft AI, in his book The Coming Wave [18] compares the importance of containing AI technology to the importance of containing nuclear technology. As one of the steps towards containment, he proposes auditing to enforce accountability with the use of AI. Testing and evaluation for Al is concerned with building an auditing system to enhance the control of human agency in use of the technology. AI is a general-purpose technology with inherent and pervasive risks that we must manage well, lest the unintended consequences wreak havoc on society. For example, Al has the potential to supercharge misinformation campaigns and cyberattacks against institutions and people through exploiting vulnerabilities in digital systems and social engineering. Building an auditing system for Al is a foundation for better management of both the intended and unintended consequences of using this technology.\nThere are heated debates on who should bear the responsibility for auditing Al systems. In one view, responsibilities should be shared among contributors to the stack of the AI system including copilots that are built collectively, from the copilot developer to the AI foundational model developer. From another perspective, customers interacting with AI through copilots as an interface are likely to deem the copilot developer as directly responsible, because the developer has some agency to control the copilot's behavior and how it interacts with customers. In this view, the developer of a copilot as the last layer in the stack plays the role as the final gatekeeper and therefore bears the most, if not all, responsibility. In a way, the copilot is imbued with the brand, image, and values that customers experience directly and attribute to the copilot owner, as opposed to some neutral object no one is responsible for. This responsibility should not be taken lightly.\nMicrosoft promotes a systematic approach that consists of 4 stages to support responsible AI practices: 1) uncover, 2) measure, and 3) mitigate the harms that may arise during the use of the AI copilot; and 4) operationalize a deployment plan based on the previous stages. Technical recommendations for responsible AI practices are published [4] for each of these stages. These stages form a lifecycle that emphasizes iterations and humans in the loop, where teams might iterate some or all stages to mitigate risks thoughtfully and carefully while leveraging AI for scale and comprehensiveness. This systematic approach can be generalized to include quality evaluations, as explained in the Evaluation as a human-AI teaming model section.\nThere are various types of potential harms to consider for a language AI application. Slack et al. proposed to separate user intent and parties impacted [14]: whether the user intent is benign or malicious, the impact could be felt by the user, targeted groups, or public at large. This distinction is a helpful conceptualization, as it decouples the measurement and mitigation of harms detected in user input and AI copilot response separately. Based on this conceptualization, they proposed a taxonomy [14]."}, {"title": "3.5 Other considerations in testing and evaluating copilots", "content": "Testing and evaluation of AI poses unique challenges compared to traditional software development. First, there is a paradigm shift in software engineering that is best captured by the \u201cSchillace Laws\". It is important to recognize that an AI copilot will consist of computer code and models, and that the boundary between them is flexible and may change over time as model capabilities mature. This is the gist of the first Schillace Law that a model improves, but the code doesn't [26]. An implication of this law is that testing code and testing models have important differences. Traditional software testing techniques such as unit tests rely on deterministic output from a system, while the system we are measuring and the tool we use to measure it may be non-deterministic, and thus require appropriate methodologies for proper measurement.\nSecondly, despite the natural language output, many of the AI systems available in the market are complex and, by nature, a black box to users and copilot developers, who do not yet have a feasible way to understand system output from a causal perspective. There is no guarantee for assertions like \u201cIf I input x, system should output y\" to work deterministically. Thirdly, the dynamic nature of interactions between the system and the user makes the possible output space very large. The multi-turn conversations for which copilots are typically designed make collecting diverse, representative data for testing and evaluation a difficult task. Lastly, there is a set of biases associated with the use of AI evaluators such as self-bias (preferring response from the same model versus other models or humans), and position bias (preferring certain arbitrary ranking over multiple choices). All these challenges demand that care be taken when using Al evaluators for testing and evaluation of copilots.\nA safety and quality trade-off is empirically found [27] to exist in training a foundational model. Such a trade-off implies that increasing the safety performance of a model may decrease the model capabilities, or vice versa. This means that some form of the said trade-off has been made for the user of the model. As previously mentioned, the Al copilot speaks for the brand and the value to customers. As a result, before rolling out the copilot, it is important to re-evaluate this trade-off and align the copilot according to the value preference, perhaps using the lifecycle of responsible AI practices mentioned above [4]. In our experience, we recognize that safety and quality evaluations need to be considered jointly for product decisions, along with other factors. When the effectiveness of the AI capabilities is at odds with the set of human values associated with safety, it is sometimes helpful to quantify this trade-off with measurements and plot a graph to find an \"optimal\" trade-off that aligns with the intended values. For example, if there are multiple versions of a copilot that have been developed, they can be represented by different trade-off points on a Pareto frontier, a graph that measures the best trade-offs among several factors, such as quality, safety, and cost. Quantifying these tradeoffs helps make an informed choice for the best version of the copilot to release in alignment with value preferences. For example, given a certain threshold in quality, the best copilot might be the version with the highest \u201csafety\u201d scores according to the predetermined definitions. Sometimes, after better mitigations are in place, an even better frontier of the quality-safety trade-off may be obtained, showing improved"}, {"title": "4 Conclusion", "content": "In this paper, we have presented some business and technical considerations for the design and evaluation of two Al copilots in the retail domain. Our systematic approaches to the discussion have been motivated by the business goals of managing desired outcomes and unintended consequences. Drawing on our experience building the retail copilot templates, we have presented our approaches to designing for human-Al collaboration and ensuring the quality and safety of the AI-generated outputs, with the aim of aligning copilot responses with the product values. We have also shared some of the responsible AI practices that we used to test and mitigate the risks of deploying Al-powered copilots in enterprise settings. We hope that our experiences and insights can inspire and inform business leaders and practitioners who are interested in building enterprise-grade copilots.\nWe believe that the two copilot templates are just two examples of many promising examples of how AI can augment human productivity in various tasks and scenarios. However, we also acknowledge that there are still many open questions and challenges that need to be addressed in order to ensure the ethical and trustworthy use of Al copilots in real-world contexts. We hope that by sharing our work and learnings, we can motivate and advocate for the continued investment in systematic, human-centric approaches to copilot design and evaluation."}, {"title": "Al disclosure note", "content": "In the process of preparing this paper the authors used the following AI models and tools for literature research, structuring of the content and editing: copilot for Microsoft 365 (https://copilot.microsoft.com/), ChatGPT 3.5 and ChatGPT 4 (via https://chat.openai.com/), Claude.ai (via https://claude.ai/chats), Claude 3 models (https://console.anthropic.com/workbench/ API \u2013 with the following models claude-3-haiku-20240307, claude-3-sonnet-20240229, claude-3-opus-20240229), Gemini (https://gemini.google.com/app), Perplexity.ai (https://www.perplexity.ai/). The authors attest that they hold full responsibility for the text and factual correctness in the paper above, and have fully reviewed references and edited text outputs from Al systems."}]}