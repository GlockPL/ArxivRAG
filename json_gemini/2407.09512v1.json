{"title": "Design and evaluation of AI copilots \u2013 case studies of retail copilot templates", "authors": ["Michal Furmakiewicz", "Chang Liu", "Angus Taylor", "Ilya Venger"], "abstract": "Building a successful AI copilot requires a systematic approach. This paper is divided into two sections, covering the design and evaluation of a copilot respectively. A case study of developing copilot templates for the retail domain by Microsoft is used to illustrate the role and importance of each aspect. The first section explores the key technical components of a copilot's architecture, including the LLM, plugins for knowledge retrieval and actions, orchestration, system prompts, and responsible AI guardrails. The second section discusses testing and evaluation as a principled way to promote desired outcomes and manage unintended consequences when using AI in a business context. We discuss how to measure and improve its quality and safety, through the lens of an end-to-end human-Al decision loop framework. By providing insights into the anatomy of a copilot and the critical aspects of testing and evaluation, this paper provides concrete evidence of how good design and evaluation practices are essential for building effective, human-centered AI assistants.", "sections": [{"title": "1 Introduction", "content": "The advent of Large Language Models (LLMs) has generated a huge amount of interest across industries in developing intelligent applications, with \u201cAI copilots\u201d \u2013 assistants that help humans in the execution of cognitive tasks \u2013 taking front stage [1]. These AI assistants can be employed to infuse existing technology with advanced capabilities, synthesizing understanding and generation of text, image and other modalities (such as audio and software actions) to enable completely new business use cases. However, building a successful copilot, even based mainly on text, comes with many challenges [2] [3]. LLMs often require additional context or examples on how to perform a task. They may lack the domain-specific knowledge to generate an appropriate response, necessitating a mechanism for retrieving relevant information. Furthermore, particularly for complex business scenarios, there may be a need for orchestration between subtasks, knowledge retrieval and actions the copilot needs to perform. Finally, care must be taken to ensure LLMs are used in an ethical and responsible way, particularly in highly regulated industries or business cases that involve high-stakes decisions or the use of sensitive data."}, {"title": "2 Architectural considerations", "content": "Building a copilot is not simply a case of exposing a Large Language Model to end users. Several components need to work together to ensure the copilot is focused on its task and role, has access to relevant knowledge, and behaves in a way that is ethical, safe and responsible. In this section, we explore these components which include the LLM for managing the human-copilot interactions, plugins for knowledge retrieval, orchestration to coordinate plugin execution, and the system prompt for controlling copilot behavior. We will use the copilot template for personalized shopping [6] as an example to illustrate the role of these components."}, {"title": "2.1 Copilot template for personalized shopping", "content": "The copilot template for personalized shopping aims to provide an interactive, conversational experience for online retail shopping. It is a template of an end-to-end Al copilot solution that can be configured and adapted for use by a range of retail companies. It is particularly relevant to retail companies that sell products for which personalized recommendations are valued by customers, for example fashion retail. The user can engage the copilot to search for clothes by expressing their desired product attributes and the copilot will respond with recommendations from the retailer's product catalog. The copilot is able to explain the suitability of recommended products based on the user's query and the product descriptions. Users can interact with the copilot in a natural conversational manner, providing feedback on the products recommended and asking for the copilot to adapt its recommendations based on this feedback.\nThe AI copilot has the flexibility to handle the diverse ways a human user might choose to search for products. In some cases, the user may make a straightforward request with a concrete set of criteria \u2013 for example, \"Show me a magenta dress\" \u2013 for which a simple keyword search of the product catalog is required. In others, the request may require more complex reasoning and judgement, for example, \"I'm going to a birthday party and the dress code is smart. Can you suggest some clothes I could wear?\". In the latter cases, the copilot may need to draw from multiple sources of knowledge to be able to make appropriate recommendations. The ability of the copilot to respond to a range of inputs, makes for a very natural human-copilot interaction and, by extension, an engaging shopping experience.\nThe copilot template for personalized shopping serves as an exemplar for the design of an Al copilot's architecture. The scenario requires the copilot to have mechanisms to perform information retrieval and a means of orchestrating between those mechanisms and generating responses to user requests, all while ensuring the copilot does not deviate from the confines of its role. Most scenarios will have similar requirements and the components that make this possible together form a reference architecture for Al copilots."}, {"title": "2.2 Employing Large Language Models in a copilot", "content": "The LLM forms the core of the AI copilot architecture. LLMs are complex machine learning models that have been pre-trained on a vast amount of text from a diverse range of sources and topics. They leverage deep learning techniques to learn complex patterns and relationships in language. As such, they can both comprehend and generate natural language with almost human level of fidelity. The performance of LLMs continues to improve and state-of-the-art models, such as GPT-4, perform well even on tests designed for humans [7].\nLLMs are known as foundation models as they can immediately be applied to a wide range of tasks involving the comprehension or generation of natural language. However, for all but the most trivial tasks, they must be provided context on how a task should be performed. These instructions are provided via the LLM's system prompt, a piece of text in which the developer can define the LLM's role, context and desired"}, {"title": "2.3 Plugins - knowledge retrieval and actions", "content": "LLMs are trained on huge corpuses of data from across the world wide web such as the 800Gb Pile dataset [9], forming an internal representation of a knowledge base that includes information about the real world. However, for many business applications, additional domain specific and proprietary knowledge is required to respond accurately to a user's request. This information will not have been included in the LLM's training"}, {"title": "2.4 Orchestration", "content": "We have seen how LLMs, with access to information retrieved from plugins, are able to understand user intent and generate relevant and informed responses. To enable complex and dynamic business tasks, an Al copilot needs orchestration to enhance the basic capabilities of an LLM, by negotiating between user intent and plugin execution and maintaining a memory of the conversation. These two LLM-powered features of a copilot perform semantic understanding of the current context and history of the task not dissimilar to a \"stateful\u201d software application but more powerful \u2013 in order to navigate the ambiguity and complexity of human-AI interactions. For example, a user of the copilot template for personalized shopping may not know the exact product they want at the start of the conversation and may begin the interaction with an ambiguous"}, {"title": "Orchestrating between plugins.", "content": "Complex business use cases are likely to require the Al copilot to have access to multiple plugins. For example, the copilot template for personalized shopping has several plugins that search product catalogue databases to provide product recommendations, as well as plugins that access a document corpus to provide information on the brand, policies and terms of sale. The orchestrator needs to decide when to call each of these plugins depending on the user's intent. It must also determine how to call the plugin by supplying any parameter values required by the plugin function. In some cases, multiple plugin calls may be needed to fulfil a user's request and these calls may need to be executed in a particular order with the output of one plugin providing input to the next.\nIt's clear that the task of orchestration is non-trivial and requires the orchestrator to reason over the user's intent and the plugins available to call. Thankfully, we now have a powerful tool that can perform this task: the LLM itself. Recent GPT models include a function calling feature, which has proved to be a critical component of the AI copilots we have developed at Microsoft: starting with GPT-3.5 Turbo and GPT-4 models, developers can provide descriptions of their functions (plugins) and parameters to the model. It is important to note that when using GPT function calling, the LLM itself is not able to execute functions. Instead, when GPT determines that a function is needed, it will output the function name along with any parameter values it deems appropriate. Executing functions is the responsibility of the orchestrator, giving it complete control over when and how functions are called.\nLet's see from an example how GPT function calling can provide an effective execution flow for orchestration of the copilot template for personalized shopping. A user could send a request to the Al copilot such as \u201cgive me recommendations for a red dress\". When the LLM receives this request, it should determine that it needs to call the product search function and will return the function name back to the orchestrator, along with the parameter values \"red\" for color and \"dress\" for apparel type. The orchestrator will now execute the relevant function to search the database using these keywords and the database will return data containing appropriate product"}, {"title": "Maintaining a memory of conversations.", "content": "Aside from coordinating plugin execution, another task for the orchestrator is to manage the conversation history between the AI copilot and user. Often, the user's exact intent may only be apparent after several conversation turns, and information from previous conversation turns will be highly relevant to how the copilot should respond next. For this reason, it is important that the history of the interaction between copilot and user is retained by the orchestrator and included in any subsequent calls to the LLM, such that it can refer to it when generating further responses or invoking plugins. It may also be worthwhile to include the conversation history from previous sessions so that the copilot can maintain a memory of all its interactions with the user.\nOne challenge with maintaining a conversation history is that eventually it may become too long to fit within a single LLM prompt. Moreover, it's likely that only a subset of the information contained in the history will be relevant to the current interaction and including the full history in the LLM prompt may cause confusion and a degradation in performance. There are several strategies for resolving this problem. A simple method is to truncate the history, discarding any further interactions beyond the previous n conversation turns. This method presumes that more recent interactions will be the most pertinent to the current conversation. However, this approach severely limits the memory of the AI copilot and may negatively impact the user experience. A more elegant solution is to apply an LLM to the task of producing a shortened summary of the conversation history, paying attention to information that could be relevant to future interactions. This summarization task could be invoked conditionally after a set number of conversation turns or when the token count of the history exceeds a certain length."}, {"title": "2.5 Controlling copilot behavior", "content": "When developing an AI copilot, it is critical to tightly define its role and desired behavior. This is important both for ensuring the copilot is provided with the instructions necessary to perform its task well, and for mitigating the risk of the copilot behaving in a way that is unintended or causes harm to the user. The component of the copilot architecture that provides such instructions is the system prompt. This is a document of natural language text containing the instructions that govern the copilot's behavior. A good system prompt will only be arrived at through iterative trial and error. Although state-of-the-art models such as GPT-4 excel at following instructions, it often pays to experiment with different prompts in a process known as prompt engineering. Even small changes to the system prompt, in terms of the content of the instructions or even the way instructions are phrased, can have a large impact on copilot behavior and performance. Testing frameworks and evaluation are required to assess the behavior during development, as discussed in Testing and Evaluation."}, {"title": "2.6 Responsible Al guardrails", "content": "It's important to consider how guardrails can be introduced to the copilot's architecture that minimizes risks of causing harm."}, {"title": "3 Testing and Evaluation", "content": "Once a working version of a copilot has been developed, it is crucial to ask how its effectiveness and safety can be determined, and whether it is ready for deployment. To address this question, this section explores the testing and evaluation of copilots, illustrating the approach with a copilot template designed for store operations [13].\nIn our experience of building copilots for customers and partners, we observe that safety and quality are the top two business requirements. These needs generalize to two essential business goals of testing and evaluation: the first goal is to manage the desired business outcomes of using copilots as an emerging technology, and the second goal is to manage the unintended consequences from its use. The best practices to achieve these goals may be broadly considered as \u201cresponsible AI practices\u201d. Managing desired business outcomes is typically achieved through quality evaluations, which is concerned with measuring and improving the effectiveness of copilots. On the other hand, managing unintended consequences is achieved through safety evaluations, which is concerned with measuring and mitigating harms induced by the use of the technology. Quality and safety consist of their own taxonomies of metrics depending on the copilot's capabilities and potential harms associated with the use case. These ideas are summarized in Table 1:"}, {"title": "3.1 Copilot template for store operations", "content": "To explain our approach to testing and evaluation of AI copilots, we refer to our experience of testing a copilot template for store operations. This template is configurable for different retail companies to build their own store operations copilots. The copilot assists retail store associates in store operations and procedures by allowing them to query their retailers' knowledge base. It achieves this goal with a Question-and-Answering (Q&A) feature which is capable of answering questions, drawing from unstructured data stored in various company documents [13].\nThe Q&A feature aims to address the challenge for store associates to memorize and retrieve procedural knowledge on demand and with precision. This task may be difficult for new associates until they develop procedural memory and become proficient through repeated practice. Moreover, some situations occur rarely making the procedures pertaining to them difficult for associates to memorize. Consulting a handbook or a colleague at peak hours may slow the operation of the stores.\nConsider the scenario of a customer coming to the store in person and asking to return an item. The associates must first determine the relevant information about the item including the type of item, its condition, and whether the item is \u201cre-sellable\" or not. The return procedures will be different for sellable items, damaged items, and special items such as jewelry, furniture, and digital items. In some circumstances, multiple processes may be executed. For example, re-sellable items trigger both a return process and a quality audit process.\nThe Q&A feature of the copilot template effectively offloads the tasks of memorizing and searching for the relevant information or context for a given situation. It synthesizes the context and the question to formulate an answer or guidance on how to proceed. If the quality of the copilot's output is good, the store associates will team up with the copilot and leverage its Al-powered skills to provide answers, and then follow standard operational procedures to resolve the situation."}, {"title": "3.2 Managing intended outcomes: measuring effectiveness of copilot capabilities", "content": "By building an Al copilot, businesses are able to offer value to customers better than their competition. Good use of AI capabilities in a copilot leads to positive business outcomes, building customer trust and loyalty, and improving profitability for the business. Conversely, poor use of the capabilities in a copilot, including poor fit for purpose and poor performance in domain-specific tasks, could lead to low-quality copilot releases that erode customer trust and the competitiveness of the product or service.\nIt is helpful to start with understanding the capabilities that AI offers. For simplicity, the focus here is on the most common scope of language AI (in contrast to image, voice, video, and a mix of these \"modalities\") and use one taxonomy proposed by Slack et al. for discussion [14] in Table 2."}, {"title": "3.3 Managing unintended consequences \u2013 measuring potential harms in the use of copilots", "content": "Viewed as a systematic process, testing and evaluation is a critical means to manage the unintended consequences of using Al copilots. Mustafa Suleyman, EVP and CEO of Microsoft AI, in his book The Coming Wave [18] compares the importance of containing AI technology to the importance of containing nuclear technology. As one of the steps towards containment, he proposes auditing to enforce accountability with the use of AI. Testing and evaluation for Al is concerned with building an auditing system to enhance the control of human agency in use of the technology. AI is a general-purpose technology with inherent and pervasive risks that we must manage well, lest the unintended consequences wreak havoc on society. For example, Al has the potential to supercharge misinformation campaigns and cyberattacks against institutions and people through exploiting vulnerabilities in digital systems and social engineering. Building an auditing system for Al is a foundation for better management of both the intended and unintended consequences of using this technology.\nThere are heated debates on who should bear the responsibility for auditing Al systems. In one view, responsibilities should be shared among contributors to the stack of the AI system including copilots that are built collectively, from the copilot developer to the AI foundational model developer. From another perspective, customers interacting with AI through copilots as an interface are likely to deem the copilot developer as directly responsible, because the developer has some agency to control the copilot's behavior and how it interacts with customers. In this view, the developer of a copilot as the last layer in the stack plays the role as the final gatekeeper and therefore bears the most, if not all, responsibility. In a way, the copilot is imbued with the brand, image, and values that customers experience directly and attribute to the copilot owner, as opposed to some neutral object no one is responsible for. This responsibility should not be taken lightly.\nMicrosoft promotes a systematic approach that consists of 4 stages to support responsible AI practices: 1) uncover, 2) measure, and 3) mitigate the harms that may arise during the use of the AI copilot; and 4) operationalize a deployment plan based on the previous stages. Technical recommendations for responsible AI practices are published [4] for each of these stages. These stages form a lifecycle that emphasizes iterations and humans in the loop, where teams might iterate some or all stages to mitigate risks thoughtfully and carefully while leveraging AI for scale and comprehensiveness. This systematic approach can be generalized to include quality evaluations, as explained in the Evaluation as a human-AI teaming model section.\nThere are various types of potential harms to consider for a language AI application. Slack et al. proposed to separate user intent and parties impacted [14]: whether the user intent is benign or malicious, the impact could be felt by the user, targeted groups, or public at large. This distinction is a helpful conceptualization, as it decouples the measurement and mitigation of harms detected in user input and AI copilot response separately. Based on this conceptualization, they proposed a taxonomy [14] as shown in Table 3 (edited for brevity):"}, {"title": "3.4 Evaluation as a human-Al teaming model", "content": "The scope and scale required for rigorous, comprehensive safety and quality evaluation often requires some degree of automation. The use of \u201cAI-assisted\" evaluation \u2013 typically using a high-performing LLM in language tasks \u2013 is one type of automatic evaluation approach. Use of AI evaluators has quickly become popular in testing and evaluation of copilots, supported by the classification capability of language Al models introduced in Table 2. To motivate the use of Al-assisted evaluation, it is helpful to contrast it with manual evaluation. In manual evaluation, a team of human annotators are given instructions and criteria for certain safety or quality metrics, and then serve as judges to label whether a copilot response is deemed safe or high-quality, usually on a binary or Likert scale. Even though manual evaluation is still the gold standard in many domains, there are two problems with a fully manual approach:\n1. Exposing human annotators to potential harms induced by unsafe content."}, {"title": "3.5 Other considerations in testing and evaluating copilots", "content": "Testing and evaluation of AI poses unique challenges compared to traditional software development. First, there is a paradigm shift in software engineering that is best captured by the \u201cSchillace Laws\". It is important to recognize that an AI copilot will consist of computer code and models, and that the boundary between them is flexible and may change over time as model capabilities mature. This is the gist of the first Schillace Law that a model improves, but the code doesn't [26]. An implication of this law is that testing code and testing models have important differences. Traditional software testing techniques such as unit tests rely on deterministic output from a system, while the system we are measuring and the tool we use to measure it may be non-deterministic, and thus require appropriate methodologies for proper measurement. Secondly, despite the natural language output, many of the AI systems available in the market are complex and, by nature, a black box to users and copilot developers, who do not yet have a feasible way to understand system output from a causal perspective. There is no guarantee for assertions like \u201cIf I input x, system should output y\" to work deterministically. Thirdly, the dynamic nature of interactions between the system and the user makes the possible output space very large. The multi-turn conversations for which copilots are typically designed make collecting diverse, representative data for testing and evaluation a difficult task. Lastly, there is a set of biases associated with the use of AI evaluators such as self-bias (preferring response from the same model versus other models or humans), and position bias (preferring certain arbitrary ranking over multiple choices). All these challenges demand that care be taken when using Al evaluators for testing and evaluation of copilots.\nA safety and quality trade-off is empirically found [27] to exist in training a foundational model. Such a trade-off implies that increasing the safety performance of a model may decrease the model capabilities, or vice versa. This means that some form of the said trade-off has been made for the user of the model. As previously mentioned, the Al copilot speaks for the brand and the value to customers. As a result, before rolling out the copilot, it is important to re-evaluate this trade-off and align the copilot according to the value preference, perhaps using the lifecycle of responsible AI practices mentioned above [4]. In our experience, we recognize that safety and quality evaluations need to be considered jointly for product decisions, along with other factors. When the effectiveness of the AI capabilities is at odds with the set of human values associated with safety, it is sometimes helpful to quantify this trade-off with measurements and plot a graph to find an \"optimal\" trade-off that aligns with the intended values. For example, if there are multiple versions of a copilot that have been developed, they can be represented by different trade-off points on a Pareto frontier, a graph that measures the best trade-offs among several factors, such as quality, safety, and cost. Quantifying these tradeoffs helps make an informed choice for the best version of the copilot to release in alignment with value preferences. For example, given a certain threshold in quality, the best copilot might be the version with the highest \u201csafety\u201d scores according to the predetermined definitions. Sometimes, after better mitigations are in place, an even better frontier of the quality-safety trade-off may be obtained, showing improved"}, {"title": "4 Conclusion", "content": "In this paper, we have presented some business and technical considerations for the design and evaluation of two Al copilots in the retail domain. Our systematic approaches to the discussion have been motivated by the business goals of managing desired outcomes and unintended consequences. Drawing on our experience building the retail copilot templates, we have presented our approaches to designing for human-Al collaboration and ensuring the quality and safety of the AI-generated outputs, with the aim of aligning copilot responses with the product values. We have also shared some of the responsible AI practices that we used to test and mitigate the risks of deploying Al-powered copilots in enterprise settings. We hope that our experiences and insights can inspire and inform business leaders and practitioners who are interested in building enterprise-grade copilots.\nWe believe that the two copilot templates are just two examples of many promising examples of how AI can augment human productivity in various tasks and scenarios. However, we also acknowledge that there are still many open questions and challenges that need to be addressed in order to ensure the ethical and trustworthy use of Al copilots in real-world contexts. We hope that by sharing our work and learnings, we can motivate and advocate for the continued investment in systematic, human-centric approaches to copilot design and evaluation."}, {"title": "Al disclosure note", "content": "In the process of preparing this paper the authors used the following AI models and tools for literature research, structuring of the content and editing: copilot for Microsoft 365 (https://copilot.microsoft.com/), ChatGPT 3.5 and ChatGPT 4 (via https://chat.openai.com/), Claude.ai (via https://claude.ai/chats), Claude 3 models (https://console.anthropic.com/workbench/ API \u2013 with the following models claude-3-haiku-20240307, claude-3-sonnet-20240229, claude-3-opus-20240229), Gemini (https://gemini.google.com/app), Perplexity.ai (https://www.perplexity.ai/). The authors attest that they hold full responsibility for the text and factual correctness in the paper above, and have fully reviewed references and edited text outputs from Al systems."}]}