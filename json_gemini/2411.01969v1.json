{"title": "ACTIVE GAZE BEHAVIOR BOOSTS SELF-SUPERVISED OBJECT LEARNING", "authors": ["Zhengyang Yu", "Arthur Aubret", "Marcel C. Raabe", "Jane Yang", "Chen Yu", "Jochen Triesch"], "abstract": "Due to significant variations in the projection of the same object from different viewpoints, machine learning algorithms struggle to recognize the same object across various perspectives. In contrast, toddlers quickly learn to recognize objects from different viewpoints with almost no supervision. Recent works argue that toddlers develop this ability by mapping close-in-time visual inputs to similar representations while interacting with objects. High acuity vision is only available in the central visual field, which may explain why toddlers (much like adults) constantly move their gaze around during such interactions. It is unclear whether/how much toddlers curate their visual experience through these eye movements to support learning object representations. In this work, we explore whether a bio-inspired visual learning model can harness toddlers' gaze behavior during a play session to develop view-invariant object recognition. Exploiting head-mounted eye tracking during dyadic play, we simulate toddlers' central visual field experience by cropping image regions centered on the gaze location. This visual stream feeds a time-based self-supervised learning algorithm. Our experiments demonstrate that toddlers' gaze strategy supports the learning of invariant object representations. Our analysis also reveals that the limited size of the central visual field where acuity is high is crucial for this. We further find that toddlers' visual experience elicits more robust representations compared to adults' mostly because toddlers look at objects they hold themselves for longer bouts. Overall, our work reveals how toddlers' gaze behavior supports self-supervised learning of view-invariant object recognition.", "sections": [{"title": "1 INTRODUCTION", "content": "Toddlers learn visual representations that support the recognition of object instances observed from different viewpoints within their first year of life (Kraebel & Gerhardstein, 2006; Ayzenberg & Behrmann, 2024). This early emergence of view-invariant recognition and the ease with which adults perform this skill hide the complexities of learning it. Images reaching the retina vary drastically when objects are turned in depth. Even state-of-the-art machine learning methods still make absurd recognition mistakes when faced with unusual viewpoints of objects (Dong et al., 2022; Abbas & Deny, 2023; Ruan et al., 2023). It raises the question of what learning mechanisms support such a view-invariant recognition in humans.\nOne of the main theories of object recognition posits that the development of view-invariant object recognition rests on the brain's ability to construct visual representations that slowly change over time (F\u00f6ldi\u00e1k, 1991; Li & DiCarlo, 2008; Miyashita, 1988). The main idea is that learners abundantly manipulate (or walk around) objects while watching them, giving access to different views of a single object over a short period of time. By learning slowly changing representations, a brain discards rapidly changing information from an image (here, information about the view) and naturally builds view-invariant representations. Following this idea, recent computational studies proposed to simulate humans' visual experience by generating or curating large-scale temporal sequences of rotating objects (Aubret et al., 2022; Schneider et al., 2021; Yu et al., 2023); they confirm that learning"}, {"title": "2 RELATED WORK", "content": "Computational studies of visual learning with temporal slowness. Early computational studies found that slowness-based learning can extract representations of simple patterns that are invariant to position, size and rotation (F\u00f6ldi\u00e1k, 1991; Wiskott & Sejnowski, 2002). Other works applied this principle to learn view-invariant object recognition (Wallis & Baddeley, 1997; Franzius et al., 2011; Einh\u00e4user et al., 2005; Stringer et al., 2006). Recent advances in SSL allowed to scale the principle of temporal slowness to large sets of uncurated images of objects (Parthasarathy et al., 2022; Aubret et al., 2022; Schneider et al., 2021). This method was called SSL through time (SSLTT) (Aubret et al., 2022). On the machine learning side, SSLTT can boost category recognition (Aubret et al., 2024b; 2022; Sanyal et al., 2023), view-invariant object instance recognition (Schneider et al., 2021) and the alignment with human representations (Parthasarathy et al., 2023). On the bio-modeling side, SSLTT can shape human-like inter-object semantic similarities (Aubret et al., 2024a) and combines well with visuo-language SSL to model object learning during dyadic play (Schauml\u00f6ffel et al., 2023). However, all these approaches use curated, synthetic, or third-person data, leaving unclear whether the statistical structure of toddlers' visual experience, combined with temporal slowness, can indeed support object recognition. Another notable work studied the learning of view-invariant object representations in impoverished visual environments through the eyes of reared chicken (Pandey et al., 2024). In contrast, we apply SSLTT on complex visual inputs extracted from head cameras carried by toddlers and/or adults during play sessions.\nLearning from egocentric videos. There is a recent surge in trained machine learning models on egocentric video datasets, including models of temporal slowness. For instance, the large-scale Ego4d dataset (Grauman et al., 2022) has been used for training vision models (Nair et al., 2022; Ma et al.; Anderson et al., 2022). However, egocentric videos for toddlers were missing (Anderson et al., 2022); this is a problem since existing research has found that the specific statistical structure of toddlers' visual experience especially supports visual learning (Sheybani et al., 2024; Bambach et al., 2017; Sheybani et al., 2023). The SAYcam dataset presents longitudinal recordings of 150 hours (on average) from each of the three children participants (Sullivan et al., 2021). With SAYcam, computational studies have shown that SSL methods can learn category recognition, with/without temporal"}, {"title": "3 METHOD", "content": "Our objective is to explore whether a bio-inspired model of visual learning can utilize the actual eye-tracking derived visual experience of toddlers to develop robust object representations. To mimic toddlers' central visual experience, we use an eye-tracking dataset recorded during toddlers' play sessions and extract parts of frames centered on the gaze location. For comparison, we also simulate different visual experiences following alternative gaze strategies. Then, we train a bio-inspired SSL model based on temporal slowness."}, {"title": "3.1 TODDLER FIXATION DATASET", "content": "The (Bambach et al., 2018) dataset contains head-camera videos recorded at 30 FPS and eye-tracking data for 38 dyads of toddlers/caregivers. All dyads play with the same set of 24 toys for 12 minutes on average. The children's ages range from 12.3 to 24.3 months. For 30 dyads, a head-camera resolution of 480 \u00d7 640 pixels was used, while four dyads were recorded at 480 \u00d7 720 pixels and the remaining four at 240 \u00d7 320 pixels. The horizontal field of view covers 72 degrees. In the following, we explain how we simulate different gaze strategies by deriving several datasets from these play sessions. Additionally, we include the anonymized information of all toddlers who participated in the study in Appendix C.\nToddler fixation dataset. This dataset aims to simulate the central visual experience of toddlers. We cut out an image patch centered on the gaze point and keep the cropped images as a conforming dataset for each video frame. For the cut out's size, we choose 128 \u00d7 128 pixel, which is 14\u00b0 \u00d7 14\u00b0 in visual angle. A typical temporal sequence of this dataset is illustrated in Figure 1B. If the gaze fixation point is too close to the image border, the crop boundaries may extend beyond the image, making it impossible to extract a patch of the desired size. In this case, we shift the gaze fixation point from the problematic border orthogonally by the minimum number of pixels. This ensures that the cropping operation outputs an image with the correct size. Note that the cropped area always contains the gaze fixation point. This dataset contains 559,522 training images, and this number is consistent across all fixation datasets (see below).\nAdult fixation dataset. We want to investigate the differences between gaze fixation in adults and toddlers and the consequences of these differences on learned representations. Thus, we also extract image patches around adults' gaze fixation points following the procedure of the Toddler fixation dataset. Appendix A illustrates the gaze distribution of toddlers and adults.\nRandom fixation dataset. As a simple comparison dataset, we propose to simulate a completely random gaze strategy. We crop each frame at a uniform, random, and independently sampled location. Unlike Toddler/Adult fixation datasets, this dataset shows little spatio-temporal structure, and the cropped images are unlikely to contain well-centered objects. Figure 1D provides examples of random fixations.\nCentroid fixation dataset. We also propose a stronger comparison dataset that simulates a static gaze within a moving head. Compared to the Random fixation dataset, this contains image patch"}, {"title": "3.2 SELF-SUPERVISED LEARNING THROUGH TIME", "content": "To model the learning process of humans, we learn visual representations with a self-supervised model of temporal slowness, namely SimCLR-TT (Schneider et al., 2021). This algorithm is based on the state-of-the-art SimCLR method (Chen et al., 2020). SimCLR-TT samples an image $X_t$ at time $t$ and a temporally adjacent image $x_{t+\\triangle T}$ at time $t + \\triangle T$ and computes their respective embeddings $z_t, z_{t+\\triangle T}$ with a deep neural network (e.g. ResNet). $\\triangle T$ represents the time interval between two selected frames in SimCLR-TT; unless states otherwise, we set $\\triangle T = \\frac{1}{30}$ seconds. Then, SimCLR-TT minimizes\n$\\mathcal{L}(z_t, z_{t+\\triangle T}) = - \\log \\frac{\\exp (\\text{sim} (z_t, z_{t+\\triangle T}) /\\tau)}{\\Sigma_{z_k\\in B, k\\neq t} [\\exp (\\text{sim} (z_t, z_k) /\\tau)] }$,\nwhere $B$ is a minibatch, sim(\u00b7) is the cosine similarity and $\\tau$ is the temperature hyper-parameter. Thus, SimCLR-TT maximizes the similarity between temporally adjacent representations (numerator) while keeping all representations dissimilar from each other (denominator)."}, {"title": "3.3 TRAINING AND EVALUATION", "content": "We run three random seeds for all experiments. For each random seed, we split the 38 available dyads into 30 train dyads and 8 test dyads. We train the models on train dyads for 100 epochs with a ResNet18, the AdamW optimizer, and set the initial learning rate and weight decay to 10-2 and 10-4, respectively. We set the SimCLR temperature to 0.08 and the batch size to 256. Appendix B.1 presents the results under various settings of hyper-parameters. We conduct all experiments on an Nvidia GeForce RTX 3090 GPU with 24 GB memory.\nWe assess the quality of the learned representations by training a linear classifier on top of the learned representation (right after the average pooling layer) in a supervised fashion (Chen et al., 2020). Since our pre-training datasets do not have labeled images, we always train the linear classifier on the train split of the Objects fixation dataset (same dyad's train split as for pre-training) and evaluate the object recognition accuracy on the test split of the Objects fixation dataset."}, {"title": "4 RESULTS", "content": "This section first investigates whether toddlers' gaze behavior during a play session supports learning view-invariant object recognition. We then analyze the factors contributing to the performance of the toddlers' models."}, {"title": "4.1 TODDLER'S CENTRAL VISUAL EXPERIENCE IS CURATED TO DEVELOP ROBUST OBJECT REPRESENTATIONS", "content": "Here, we evaluate if a toddler's gaze behavior supports strong object representations. In Figure 4, we compare the representations learned by SimCLR-TT when trained on the different datasets introduced in Section 3.1. We find that models trained with the Toddler fixation dataset outperform"}, {"title": "4.2 CONSTRAINED SIZE OF THE CENTRAL VISUAL FIELD IS CRUCIAL FOR LEARNING OBJECT REPRESENTATIONS", "content": "Previous computational studies neglect the importance of the constrained size of the central visual field for learning visual representations (Orhan et al., 2020; Sheybani et al., 2024). Here, we assess whether our simulated central visual experience leads to better/worse object representations than a wide field of view. We vary the crop size applied to the datasets reported in Section 3.1. In Table 1, we observe for both toddlers and adults that an image size of 128 \u00d7 128 produces the best recognition"}, {"title": "4.3 TODDLERS' GAZE BEHAVIOR BOOSTS VERY SLOWLY CHANGING REPRESENTATIONS", "content": "Previous work suggests that semantic aspects of the visual experience vary more slowly for toddlers than for adults (Sheybani et al., 2023) and that extending the gap of time between two positive pairs can improve the quality of object representations if visual inputs are sufficiently stable over time (Aubret et al., 2022; Schneider et al., 2021). Thus, we investigate whether amplifying the temporal slowness of our representation intensifies the difference between toddlers' and adults' representations. To amplify temporal slowness, we increase the temporal gap $\\triangle T$ between representations that are made similar. As shown in Figure 5A, $\\triangle T$ ranges from $\\frac{1}{30}$ to 3.0 seconds, increasing continuously by 0.5 seconds at each step. The models trained with the Toddler fixation dataset achieve the highest recognition accuracy when \u2206T = 1.5s. Conversely, Figure 5B shows that, for models trained with the Adult fixation dataset, increasing the interval between positive pairs decreases the quality of object representations. The results are consistent for both human fixations and centroid fixations (\"Fixation\u201d and \u201cCentroid\u201d). We conclude that even younger toddlers are expected to show even slower behavior compared to adults. This leads to a better visual representation for toddlers."}, {"title": "4.4 TODDLERS' ADVANTAGE HINGES ON LONGER INSPECTIONS OF OBJECTS WHEN HOLDING THEM", "content": "So far, the temporal structure of toddlers' egocentric experience clearly supports object learning better than adults. However, the temporal properties responsible for this effect remain unclear. Here, we further analyze the visual statistics of central visual experiences. We focus on four metrics that characterize the temporal sequence of images: the average fixation duration before making a saccade, the average duration of object looking bouts, the average duration of object looking when the camera-wearer holds the object, and the cumulative duration of object looking in a recording. We explain how we detect saccade and compute the average fixation duration in Appendix A. For other metrics, we leverage manually labeled timestamps (by (Bambach et al., 2018)) about when toddlers and adults look at/hold an object. In the following, we label \"Object looking\" when the gaze fixation points are located on an object while the camera-wearer is not holding the object. We successfully extracted the data from 28 out of 38 toddlers and conducted all subsequent experiments using these 28 toddlers. The remaining participants are excluded from this section due to the lack of data on fixation duration. Table 2 in Appendix C presents the details of these specific 28 toddlers.\nIn Figure 6, we observe that object recognition accuracy is highly linearly correlated with the three average durations but only weakly correlated with the cumulative duration of object looking. This indicates that long fixation bouts are important in explaining the quality of visual representations trained on the Toddler and Adult fixation datasets."}, {"title": "5 CONCLUSION", "content": "Current SSL approaches still struggle to learn robust human-like object representations and the reasons for this remain unclear. Here, we investigated whether a biologically inspired visual learning model can take advantage of toddlers' gaze behavior to develop robust object representations. We cropped the toddlers' gaze location from egocentric and eye-tracking recordings during play sessions. Then, we trained a bio-inspired unsupervised model that drives visual representations to slowly change. Our findings indicate that toddlers' gaze strategies boost the learning of representations that support view-invariant object instance recognition within a single play session of 12 minutes. In particular, the effect was weaker with adults' gaze behavior. Our analysis shows that our approximated central visual experience is crucial for learning object-oriented representations and that toddlers' gaze behavior makes better use of slowness-based learning compared to adults. The latter point is supported by looking longer at objects while holding them. During their relatively long holding periods, we speculate that toddlers may actually be turning and moving the object, giving access to high-quality sequences that contain different object views over a short period of time.\nFrom a developmental perspective, our work provides strong evidence that the development of view-invariant representations can originate in slowly changing representations, a mechanism previously found in the brain (Li & DiCarlo, 2008; Miyashita, 1988). We further demonstrate that toddlers may curate their gaze behavior to enhance the quality of their visual representations. However, it remains unclear if toddlers intentionally optimize their learning progress in visual representations. From a machine learning perspective, we show that combining eye-tracking video data and SSL supports unsupervised view-invariant recognition. This work marks a significant step towards learning strong representations without hand-crafted image datasets (e.g., (Aubret et al., 2022)).\nWe analyzed gaze behavior in toddlers with a minimum age of 12.3 months, meaning they had substantial visual learning experience before the experiment, while our models learned from scratch. Expanding to a wider variety of objects and participants, particularly younger toddlers with distinct visual exploration patterns, could offer deeper insights into early visual representation development. Studying how babies under one year engage with objects may reveal new aspects of gaze behavior that contribute to visual learning (Maurer, 2017; Sheybani et al., 2024). Moreover, refining our approach to incorporate both central and peripheral vision could provide a more accurate simulation of human perception (Wang et al., 2021)."}, {"title": "A ADDITIONAL DETAILS", "content": "Gaze location distribution. In section 3.1, we explain that the center of the frames is misaligned with respect to the stationary position of the eyes. To support this statement, Figure 8 and Figure 9 display the distribution of gaze locations for each toddler and adult, respectively. Brighter areas indicate higher frequencies of gaze fixation at those locations. The results indicate their average gaze location is not centered with respect to the camera.\nExtraction of saccade and fixations. In the study in section 4.4, we extracted fixation bouts. This requires to detect saccades, as they bound the fixations bouts. To detect saccades in gaze movement, we apply a velocity threshold-based method similar to (Raabe et al., 2023). Consecutive gaze points that exceed a threshold $T_1$ are identified as a single saccade. To account for artifacts caused by low frame rates, a second threshold $T_2$, along with an angular criterion $\\theta$, allows the inclusion of the two data points adjacent to the saccade initially detected. Any data points not classified as saccades are considered fixations. For this study, we choose $T_1 = 25^{\\circ} s^{-1}$, $T_2 = 10^{\\circ} s^{-1}$ and $\\theta = 45^{\\circ}$."}, {"title": "B COMPLEMENTARY ANALYSIS", "content": "The learning rate (lr), weight decay (wd), and temperature (tp) used in our main content were selected as the best settings after fine-tuning. To assess the robustness of our method, we conducted additional experiments where we fixed the lr = 10-2 and wd = 10-4 and tp = 0.08 to and varying another hyper-parameter individually. As shown in Figure 10, changes in these hyper-parameters do not affect the conclusions presented in Section 4.1."}, {"title": "B.2 STUDY OF SACCADE DURATION AND AGE", "content": "Here, we complement section 4.4 and study two additional metrics that may impact the performance of individual adults and toddlers, namely the average saccade duration and toddlers' age. According to Figure 11, we observe no significant correlation between the recognition accuracy and both the average saccade duration or toddlers' age. However, the youngest toddlers in the study were older than one year and we can not rule out that babies may induce different results."}]}