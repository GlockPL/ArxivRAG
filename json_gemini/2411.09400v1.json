{"title": "Imagined Speech and Visual Imagery as Intuitive Paradigms for Brain-Computer Interfaces", "authors": ["Seo-Hyun Lee", "Ji-Ha Park", "Deok-Seon Kim"], "abstract": "Recent advancements in brain-computer interface (BCI) technology have emphasized the promise of imagined speech and visual imagery as effective paradigms for intuitive communication. This study investigates the classification performance and brain connectivity patterns associated with these paradigms, focusing on decoding accuracy across selected word classes. Sixteen participants engaged in tasks involving thirteen imagined speech and visual imagery classes, revealing above-chance classification accuracy for both paradigms. Variability in classification accuracy across individual classes highlights the influence of sensory and motor associations in imagined speech and vivid visual associations in visual imagery. Connectivity analysis further demonstrated increased functional connectivity in language-related and sensory regions for imagined speech, whereas visual imagery activated spatial and visual processing networks. These findings suggest the potential of imagined speech and visual imagery as an intuitive and scalable paradigm for BCI communication when selecting optimal word classes. Further exploration of the decoding outcomes for these two paradigms could provide insights for practical BCI communication.", "sections": [{"title": "I. INTRODUCTION", "content": "The human brain signals contain various information about actions and internal imageries, making it crucial for interpreting intentions and facilitating neural communication. Brain-computer interfaces (BCIs) leverage this potential by translating neural activity into computer commands, allowing individuals to control external devices directly through brain signals. Particularly, BCIs have shown significant promise in assisting those with severe motor impairments, such as individuals with paralysis or locked-in syndrome, enabling them to interact with their environment using only their neural signals [1]-[3]. Recent advancement in this field is brain-to-speech (BTS) technology, which aims to reconstruct audible speech directly from neural activity [4], [5].\nWhile conventional BCI research focus on decoding motor-related signals or generating speech from spoken or mimed brain signals through invasive methods [6]-[9], the challenge of decoding endogeneous and communication-related paradigms, such as imagined speech or visual imagery using non-invasive techniques remains significant [10], [11]. Imagined speech, which allows individuals to mentally simulate speech without physical articulation, presents a more practical communication mode for those unable to speak [6], [12]. Visual imagery can also be an effective and intuitive BCI paradigm for communication, as it is not limited by the number of classes and is easy for users to imagine.\nRecently, the paradigms of imagined speech and visual imagery have garnered interest in the field of intuitive BCIs, as they directly engage user intention [13]. Ongoing efforts focus on robustly decoding these two paradigms while investigating their intrinsic features. However, the underlying features and cortical networks associated with imagined speech and visual imagery remain largely unexplored. A comprehensive understanding of these intrinsic cortical networks may significantly enhance the decoding performance of BCI paradigms [2], [14], [15]. Previous studies have demonstrated the feasibility of decoding imagined speech at various levels, including phonemes and simple sentences [16], [17], yet multiclass decoding accuracy remains modest, however, visual imagery performance is typically lower [18].\nStudies have indicated similarities in neural patterns between imagined and spoken speech, particularly in the ventral sensorimotor cortex (vSMC), suggesting that it may be feasible to robustly decode speech from imagined neural signals. By linking these neural features to corresponding speech audio and phonemes, robust speech generation could be achieved, even with limited training data. Training models on spoken speech electroencephalography (EEG) and mapping them to imagined speech could facilitate the reconstruction of previously unseen words by utilizing shared phoneme-level information.\nThis study is an expanded version of previous studies [7], [19], which investigate the brain dynamics of the imagined speech and visual imagery paradigms by comparing the decoding performance among thirteen distinct classes, with brain connectivity analysis during imagery tasks and resting states. We analyze functional connectivity through phase-locking values (PLV) across specific frequency ranges and"}, {"title": "II. METHODS", "content": "The dataset from the previous work was analyzed in the study [7]. This research was conducted in accordance with the Declaration of Helsinki, and informed consent was obtained from Sixteen healthy participants. EEG data were collected using a 64-channel cap with active Ag/AgCl electrodes. The FCz channel was used as the reference, and the FPz channel served as the ground. Data acquisition was performed with a Brain Vision Recorder system (BrainProducts GmbH, Germany) and was managed using MATLAB 2018a.\nThe experimental design consisted of two separate sessions. In session 1, participants engaged in imagined speech, while session 2 involved visual imagery. Each block began with either an auditory cue (for imagined speech) or a visual cue (for visual imagery) presented for 2 seconds, followed by a cross mark displayed for a random interval between 0.8 and 1.2 seconds. After the cross mark disappeared, a black screen appeared for another 2 seconds. Participants were instructed to initiate the task as soon as the cross mark was removed. Following 2 seconds of task performance, the cross mark reappeared for 0.8 to 1.2 seconds, succeeded by an additional 2-second black screen. Each block comprised four trials for the given cue, followed by a 3-second relaxation period to mentally reset before the next cue (Fig. 1).\nIn session 1, participants were asked to imagine saying the given word as though they were speaking, without any physical articulation or sound production. In session 2, they were instructed to visualize the scene corresponding to the presented class. For the rest condition in both sessions, participants remained calm and avoided any intentional brain activity. Clear instructions were provided to ensure that participants did not engage in unrelated cognitive activities during each session, including minimizing body movements and eye blinks while imagining or receiving cues. The cross mark duration was randomized between 0.8 and 1.2 seconds to reduce anticipation regarding task onset."}, {"title": "B. Brain Decoding and Connectivity Analysis", "content": "We decoded each imagined speech and visual imagery class using a basic machine learning classifier, following methodologies from prior research [7], to explore the primary patterns associated with each word. To further understand the brain's functional changes during imagined speech and visual imagery paradigms, we analyzed brain connectivity through the PLV measure. This measure quantifies the synchronization of brain electrical activity, serving as an indicator of functional connectivity. We computed the PLV for both imagery and resting states across four frequency bands and seven cortical regions (including six specific cortical groups as well as an overall group of 64 channels) using the following equations:\n$PLV_{t,i,k} = \\frac{1}{N} \\sum_{n=1}^{N} |exp(j\\theta_{i,k}(t, n))|$\n$\\theta_{i,k}(t, n) = \\phi_{i}(t, n) \u2013 \\phi_{k}(t, n);$\nIn these equations, $\\theta_{i,k}(t, n)$ represents the phase difference between channels i and k during trial n, and N stands for the total trial count. Given that PLV captures connectivity between channel pairs, we computed the grand average across all possible channel pairs within each cortical region. Furthermore, inter-cortical connectivity was assessed by averaging the PLV across combinations of the different cortical groups."}, {"title": "III. RESULTS AND DISCUSSION", "content": "The classification results presented in Table I and Table II reveal the performance of each class versus rest across both the imagined speech and visual imagery paradigms. The overall classification accuracy indicates that both paradigms achieve above chance levels, demonstrating the distinguishable brain activity patterns associated with each class. On average, classification accuracy in the imagined speech paradigm reached 81.5%, with individual classes showing variation from 75.8% to 82.7%. In the visual imagery paradigm, average classification accuracy was slightly lower at 80.5%, with individual class accuracies ranging between 75.8% and 82.2%. These findings suggest that imagined speech yields slightly higher accuracy than visual imagery for the given classes, potentially due to more distinct neural patterns in motor-related brain regions engaged during the mental simulation of speech production, as noted in similar studies [7], [8], [16]."}, {"title": "B. Inter-Class Variability", "content": "The results highlight notable variability across individual classes. In the imagined speech paradigm, words such as 'Hello' and 'Water' consistently showed high classification accuracy across most participants (average accuracy of 81.8% and 82.7%, respectively). In contrast, words like 'Light' and 'Pain' demonstrated relatively lower accuracies (75.8% and 80.8%), which may be attributed to lesser motor or sensory association, resulting in less distinctive neural activation patterns. In the visual imagery paradigm, classes such as \u2018Ambulance' and 'Thank you' showed high classification accuracy, potentially due to their unique or vivid visual associations, which might engage distinct cortical networks involved in visual processing."}, {"title": "C. Brain Connectivity", "content": "Table III shows brain connectivity patterns across cortical regions for imagined speech and visual imagery paradigms. Compared to the resting state, both paradigms displayed notable connectivity increases, particularly in Broca and Wernicke's areas, the auditory cortex, and the prefrontal cortex. For imagined speech, there was significant connectivity between Broca's area and both the auditory and prefrontal cortices, indicating strong engagement of language-related and cognitive processing networks, aligning with previous findings on imagined verbal communication [7]. In visual imagery, increased connectivity was observed between the visual cortex and both the prefrontal cortex and motor cortex, reflecting the activation of spatial and visual processing networks. While the auditory and motor cortices showed connectivity with other regions in both paradigms, the effect was more pronounced in imagined speech, possibly due to cross-modal activation in language tasks."}, {"title": "D. Implications for BCI Applications", "content": "The findings suggest that imagined speech could serve as a robust paradigm for BCI-based communication, especially in scenarios requiring specific word-based commands. Visual imagery, while slightly lower in accuracy, remains an effective paradigm and may serve as a complementary approach, particularly for users with varying cognitive preferences or impairments in speech-motor areas. Furthermore, the high standard deviation observed in some classes (e.g., 'Clock' and 'Yes') across participants indicates that individual differences play a significant role in classification accuracy."}, {"title": "IV. CONCLUSION", "content": "This study highlights the potential of imagined speech and visual imagery as effective BCI paradigms for communication, with imagined speech generally achieving higher classification accuracy due to its engagement of motor and language networks, while visual imagery activates visual and spatial processing regions. Connectivity analysis supports these differences, showing distinct neural patterns in each paradigm. Overall, these results underscore the potential of both imagined speech and visual imagery as intuitive BCI paradigms, providing a foundation for developing more personalized and adaptive BCI communication systems. Future work should expand on this research by including more diverse word classes and advanced machine learning models to enhance decoding accuracy, paving the way for personalized and adaptable BCI communication systems [21], [22]."}]}