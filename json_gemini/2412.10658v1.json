{"title": "Combining Priors with Experience: Confidence Calibration Based on Binomial Process Modeling", "authors": ["Jinzong Dong", "Zhaohui Jiang", "Dong Pan", "Haoyang Yu"], "abstract": "Confidence calibration of classification models is a technique to estimate the true posterior probability of the predicted class, which is critical for ensuring reliable decision-making in practical applications. Existing confidence calibration methods mostly use statistical techniques to estimate the calibration curve from data or fit a user-defined calibration function, but often overlook fully mining and utilizing the prior distribution behind the calibration curve. However, a well-informed prior distribution can provide valuable insights beyond the empirical data under the limited data or low-density regions of confidence scores. To fill this gap, this paper proposes a new method that integrates the prior distribution behind the calibration curve with empirical data to estimate a continuous calibration curve, which is realized by modeling the sampling process of calibration data as a binomial process and maximizing the likelihood function of the binomial process. We prove that the calibration curve estimating method is Lipschitz continuous with respect to data distribution and requires a sample size of 3/B of that required for histogram binning, where B represents the number of bins. Also, a new calibration metric (TCEbpm), which leverages the estimated calibration curve to estimate the true calibration error (TCE), is designed. TCEbpm is proven to be a consistent calibration measure. Furthermore, realistic calibration datasets can be generated by the binomial process modeling from a preset true calibration curve and confidence score distribution, which can serve as a benchmark to measure and compare the discrepancy between existing calibration metrics and the true calibration error. The effectiveness of our calibration method and metric are verified in real-world and simulated data. We believe our exploration of integrating prior distributions with empirical data will guide the development of better-calibrated models, contributing to trustworthy \u0391\u0399.", "sections": [{"title": "1 Introduction", "content": "The prediction accuracy of modern machine learning classification methods such as deep neural networks is steadily increasing, leading to adoption in many safety-critical fields such as intelligent transportation (Lu, Lin, and Hu 2024), industrial automation (Jiang et al. 2023), and medical diagnosis (Luo et al. 2024). However, decision-making systems in these fields not only require high accuracy but also require signaling when they might be wrong (Munir et al. 2024). For example, in an automatic disease diagnosis system, when the confidence of the diagnostic model is relatively low, the decision-making should be passed to the doctor (Jiang et al. 2012). Specifically, along with its prediction, a classification model should offer accurate confidence (matching the true probability of event occurrence). In addition, accurate confidence also provides more detailed information than the no-confidence or class label (Huang et al. 2020). For example, doctors can gather more information to make more reliable decisions in \u201cthere is a 70% probability that the patient has cancer\" than just a class label of \"cancer\". Furthermore, accurate confidence facilitates the incorporation of classification models into other probabilistic models. For instance, accurate confidence allows active learning to select more representative samples (Han et al. 2024) and improves the generalization performance of knowledge distillation (Li and Caragea 2023). Therefore, pursuing more accurate confidence for classification models is a significant work (Penso, Frenkel, and Goldberger 2024; Wang et al. 2024).\nHowever, modern classification neural networks often suffer from inaccurate confidence (Guo et al. 2017), which means that their confidence does not match the true probabilities of predicted class. For example, if a deep neural network classifies a medical image as \u201cbenign\" with a confidence score of 0.99, the true probability of the medical image being \"benign\" could be significantly lower than 0.99, and even its true class may be \"malignant\". Therefore, in recent years, this problem has been attracting increasing attention (Dong et al. 2024; Geng et al. 2024), and many confidence calibration methods, which aim to obtain more accurate confidence through additional processing, have been proposed (Silva Filho et al. 2023; Zhang et al. 2023).\nPrevious works calibrate confidence mostly from three directions: 1) Performing calibration during the classifier's training (train-time calibration), usually modifying the classifier's objective function (Liu et al. 2023; M\u00fcller, Kornblith, and Hinton 2019; Fernando and Tsokos 2021); 2) Binning confidence scores and estimating the calibrated confidence using the average accuracy inside the bins (binning-based calibration) (Zadrozny and Elkan 2001; Naeini, Cooper, and Hauskrecht 2015; Patel et al. 2020); 3) Fitting a function on"}, {"title": "2 Background and related work", "content": "For a K-class classification problem, let (X, Y) \u2208 X \u00d7 Y be jointly distributed random variables, where X C Rd denotes the feature space and y = {1,2, ..., K} is the label space. The classification model can be expressed as f(X) : X \u2192 S, where S = (S1, S2, ..., SK) \u2208 S C \u2206K\u22121 and \u0394\u039a\u22121 represents a simplex with free-degree K \u2013 1. The predicted class Y = argmax {Sk}1<k<K, and the confidence score of predicted class is \u015c = max {Sk}1<k<K\u2022\nTypically, we just care about the confidence of the predicted class. In this case, a multi-classification problem can be formally unified into a binary classification problem. Let the \u201chit\u201d variable H = I[Y = Y], where I is the indicative function, that is, when Y = Y, I[Y = Y] = 1, otherwise I[Y = Y] = 0. Therefore, the data samples become observations of (\u015c, H)."}, {"title": "2.1 Confidence calibration", "content": "The purpose of confidence calibration is to make the confidence of predicted class match the true posterior probability of the predicted class. Formally, we state:\nDefinition 1. (Perfect calibration) A classification model is perfectly calibrated if the following equation is satisfied:\n$$P(Y = \\hat{Y}| \\hat{S} = s) = s.$$"}, {"title": "2.2 Estimates of calibration curve", "content": "Currently, confidence calibration methods can be mainly divided into two groups: train-time calibration (Liu et al. 2023; M\u00fcller, Kornblith, and Hinton 2019; Fernando and Tsokos 2021) and post-hoc calibration (Guo et al. 2017; Kull et al. 2019a; Zhang, Kailkhura, and Han 2020; Rahimi et al. 2020). Train-time calibration usually performs calibration during the training of the classifier by modifying the objective function, which may increase the computational cost of the classification task (Naeini, Cooper, and Hauskrecht 2015) and affect the classification effect (Joy et al. 2023). Post-hoc calibration learns a transformation (referred to as a calibration map) of the trained classifier's predictions on a calibration dataset in a post-hoc manner (Zhang, Kailkhura, and Han 2020), which does not change the weights of the classifier and usually performs simple operations.\nPioneering work along the post-hoc calibration direction can be divided into two subgroups: binning-based calibration and fitting-based calibration. Binning-based calibration methods divide the confidence scores into multiple bins and estimate the calibrated value using the average accuracy inside the bins. The classic methods include"}, {"title": "2.3 Estimates of calibration error", "content": "True calibration error (TCE) The true calibration error is described as the lp norm difference between the confidence score of the predicted class and the true likelihood of being correct (Kumar, Liang, and Ma 2019):\n$$TCE = (E_\\hat{S}[|\\hat{S} \u2013 P(H = 1|\\hat{S})|^p]).$$\nThe true calibration curve P(H = 1|S) and the distribution of confidence scores \u015c S determine the value of TCE. TCE is not computable since the ground truth of P(H = 1|\u015c) and the true distribution of \u015c cannot be obtained in practice. Therefore, statistical methods are needed to estimate P(H = 1|\u015c) and the distribution of \u015c, and then estimate the true calibration error.\nBinning-based calibration metrics Binning-based calibration metrics use the average accuracy of each bin to approximate P(H = 1|\u015c) and use the sample size proportion of the bin to approximate the S. Formally, assume that all confidence scores are partitioned into M equally-spaced non-overlapping bins, and the i-th bin is represented by Bi, then the binning-based expected calibration error (ECEbin) is calculated as follows:\n$$ECE_{bin} = \\sum_{i=1}^M \\frac{|B_i|}{N}|acc(B_i) - conf(B_i)|,$$Binning-free calibration metrics In recent years, confidence calibration evaluation methods that are not based on binning have also been proposed. Gupta et al. (Gupta et al. 2020) proposed KS error, which uses the Kolmogorov-Smirnov statistical test to evaluate the calibration error. Zhang et al. (Zhang, Kailkhura, and Han 2020) and Blasiok et al. (Blasiok and Nakkiran 2023) propose smoothed Kernel Density Estimation (KDE) methods for evaluating calibration error. Chidambaram et al. (Chidambaram et al. 2024) smooth the logit and then use the smoothed logit to build calibration metric."}, {"title": "2.4 Combining prior with experience", "content": "In statistics, integrating prior distribution with experience data to estimate the distribution behind the data is a classic and practical tradition (Zellner 1996; Lavine 1991). Priors refer to initial inference on the form or value of model parameters before observing data. Experience refers to the knowledge a model learns from data. When there is enough data, the model can learn well from experience, and the role of the prior may not be reflected. However, when data size is insufficient, a well-informed prior is often more effective than experience data.\nIn confidence calibration, most existing calibration methods predominantly focus on how to estimate calibrated confidence from data, fit a user-defined calibration function on logit or confidence score, or use naive fitting (e.g., least square method, minimizes cross-entropy loss) to combine priors (e.g., beta prior (Kull, Silva Filho, and Flach 2017b), dirichlet prior (Kull et al. 2019b)) with experience. However, although fitting a user-defined calibration function may also imply some user-observed priors, such as the choice of function shape, these priors are too empirical, and their universality needs to be considered. In addition, naive fitting is prone to be overly affected by data with larger statistical biases (e.g., sparse data). Therefore, it is necessary to study a principled method that better integrates a well-informed prior distribution with empirical data to estimate the calibration curve."}, {"title": "3 Method", "content": "In this section, the following questions are studied: 1) How to introduce priors to estimate calibration curve P(H = 1|S) better? 2) How to choose an appropriate prior? 3) How to build a calibration metric using the estimated calibration curve? 4) How about the theoretical properties of the proposed method?\nIn Section 3.1, to solve the first problem, the sampling process of the calibration data is modeled as a binomial process, and then the calibration curve can be estimated by maximizing the likelihood function of this binomial process. In Section 3.2, a general and effective prior function family is suggested. In Section 3.3, a new calibration metric TCEbpm is proposed. Section 3.4 analyzes the theoretical guarantee of the proposed calibration method and metric."}, {"title": "3.1 Estimating calibration curve", "content": "Binomial process For any fixed \u015c, the repeated sampling process of H is a binomial distribution on Nos, where NPOS = \u2211Ni=1 H(S) represents the number of \"hit\", H(S) represents the i-th \u201chit\u201d label at \u015c, N\u015d represents the number of samples at \u015c. Specifically, Nos ~ BI(N\u015d, P(H = 1|\u015c)), where BI(\u00b7) represents binomial distribution. Formally, the following equation is satisfied:\n$$P(N^{pos}_\\hat{S}) = BI(N_\\hat{S}, P(H = 1|\\hat{S}))\\\\\n= C_{N_\\hat{S}}^{N^{pos}_\\hat{S}}P(H = 1|\\hat{S})^{N^{pos}_\\hat{S}} (1 \u2013 P(H = 1|\\hat{S}))^{N_\\hat{S}-N^{pos}_\\hat{S}},$$Furthermore, for all \u015c \u2208 [0,1], the repeated sampling process of H is a binomial process. Binomial process is a random process (Grimmett and Stirzaker 2020) with binomial distribution in a continuous domain. Specifically, in the binomial process, the distribution of random variable Nos under every point \u015c in the continuous domain [0, 1] is a binomial distribution. Since we are interested in P(H = 1|\u015c), P(H = 1|\u015c) is modeled as a prior function family g(\u015c; 0). Formally, the following equation is satisfied:\n$$P(N^{pos}_\\hat{S}) = BP(N_\\hat{S}, g(\\hat{S}; \\theta)) \\\\\n= C_{N_\\hat{S}}^{N^{pos}_\\hat{S}}g(\\hat{S}; \\theta)^{N^{pos}_\\hat{S}}(1 \u2013 g(\\hat{S}; \\theta))^{N_\\hat{S}-N^{pos}_\\hat{S}},$$\nMaximum likelihood estimation Our purpose is to estimate the calibration curve g(\u015c; 0). Usually, solving g(\u015c; 0) requires a combination of prior and experience. Prior defines that g(S;0) follows a function family of fixed parameter form or structure. Experience represents seeking the optimal parameter @ from the data. Maximum likelihood estimation is a classic and effective solution to this problem. Formally, the following equation needs to be solved:\n$$Argmax_\\theta P[D|g(\\hat{S};\\theta)],$$"}, {"title": "3.2 Selecting prior", "content": "The appropriate prior is crucial to solving Eq. 9. If the prior function family is not correctly selected, the true calibration curve will not be in the feasible region (i.e., solution space). If the prior function family is correctly selected, a good calibration curve will be found quickly and efficiently. Next, this paper suggests a general and effective prior.\nIt is well known that the prior distribution of confidence scores can be modeled by beta distribution (Kull,"}, {"title": "3.3 Estimating TCE", "content": "As can be seen from Algorithm 1, the estimated calibration curve is a continuous function. This allows us to estimate the true calibration error (see Eq. 2) instead of just the expected calibration error (see Eq. 3).\nAccording to Eq. 2, the calculation formula of TCE is:\n$$TCE = \\mathbb{E} _{ \\hat{S} } [ | P(H=1|\\hat{S}) - \\hat{S} |^p  ] = \\int_{0}^{1}  | P(H=1|\\hat{S}) - \\hat{S} |^p f(\\hat{S}) d\\hat{S}],$$"}, {"title": "3.4 Theoretical guarantee", "content": "Continuity Continuity with respect to data distribution is an important property for a calibration method and metric. It tells us whether a slight change in data distribution will lead to a drastic jump in the calibration curve and the calibration metric. Before conducting continuity analysis, a distance measure of the data distributions needs to be defined."}, {"title": "Consistency", "content": "To theoretically prove the effectiveness of a calibration metric, Blasiok et al. (B\u0142asiok et al. 2023) have proposed a unified theoretical framework: consistent calibration measure. Consistent calibration measure means two things: 1) When the true distance to calibration is small, the calibration metric should also be small(i.e., Robust completeness); 2) When the true distance to calibration is large, the calibration metric should also be large (i.e., Robust soundness).\nBefore defining the consistent calibration measure, we need to define how far a data distribution D is from its nearest perfect calibration distribution, as shown in the Definition 3. Then, the consistent calibration measure can be defined as shown in Definition 4.\nTheorem 3 proves that TCEbpm is a consistent calibration measure when certain conditions are met. Corollary 1 proves that when P(\u015c|H = 0) and P(\u015c|H = 1) follow beta distribution, TCEbpm calculated using Eq. 12 is a consistent calibration measure. The proof of Theorem 3 is given in Appendix A.4, and the proof of Corollary 1 is given in Appendix A.5.\nDefinition 3. (True distance to calibration) VD over [0, 1] \u00d7 {0, 1}, let P be the family of all perfectly calibrated distributions, the true distance to calibration is:\n$$d_{CE}(D) = inf_{D \\in P} W(D,D)$$Definition 4. (Consistent calibration measure) For q, t, L1, L2 > 0, calibration metric \u00b5, and data distribution D over [0, 1] \u00d7 {0,1}, if:\n$$\\mu(D) \\leq L_1 (d_{CE}(D))^q,$$then u satisfies q-robust completeness. If:\n$$\\mu(D) \\geq L_2 (d_{CE}(D))^t,$$then u satisfies t-robust soundness. If satisfying robust completeness and soundness, \u00b5 is a consistent calibration measure.\nTheorem 3. TCEbpm is a consistent calibration measure if the following two conditions hold:\n\u2022 The hypothesis set G (The set of all possible g(\u015c;0D)) includes the true calibration curve;\n\u2022 \u2200\u015c \u2208 [0,1], if g(\u015c;0p) \u2208 G and \u00a7\u266d(\u015c) are Lipschitz continuous w.r.t. D."}, {"title": "4 Simulating datasets to compare evaluation metrics", "content": "A key challenge in developing calibration metrics is the lack of ground truth for calibration curves and confidence scores, hindering the measurement of discrepancies between metrics and actual calibration errors. Roelofs et al. (Roelofs et al. 2022) use the fitted function on the publicly available logit datasets as the true distribution behind the data, then use the fitted function to calculate TCE and compare TCE with existing calibration metrics. In this paper, an opposite operation is proposed, i.e., first preseting the true calibration distribution and then obtaining realistic calibration data through binomial process sampling.\nSpecifically, in Section 3.1, we model the process of sampling calibration data as a binomial process. Another important role of this modeling is that realistic calibration data sets can be sampled through using known calibration curves and confidence distributions, as shown in Algorithm 3. The confidence score \u015c are first sampled, and then the calibration value P(H = 1|\u015c) is calculated. Then, P(H = 1|\u015c) is used as the probability of a single event success in the binomial distribution, and binomial distribution sampling is performed to sample H. Since the true calibration curve"}, {"title": "5 Results", "content": "The effectiveness of the proposed method is verified from four perspectives: 1) On the real-world datasets, the calibration curve estimated by our method is compared with the results of histogram binning under various binning schemes; 2) On the datasets simulated by Algorithm 3, the discrepancy between the calibration curve estimated by our method and the true calibration curve is compared; 3) On the datasets estimated by Algorithm 3, the discrepancy between TCEbpm and the true calibration error is compared; 4) On the real-world datasets, multiple calibration metrics comparison between our calibraiton method with other calibration methods is performed. Due to space limitations, details of data selection and implementation details are given in Appendix B.1 and Appendix B.2. In Appendix B.1, ten publicly available logit datasets and five true distributions (named D1, D2, \u2026, and D5, respectively) were selected for the experiments."}, {"title": "5.1 Estimated results of calibration curves", "content": "Results in real datasets The estimated results of the calibration curve on the public ResNet110's logit dataset trained on Cifar10 is shown in (a) of Fig. 1. The results on other datasets are shown in Appendix B.3. In order to intuitively show the effect of the calibration curve estimated by our method, the means and ranges of the calibration values estimated by the histogram binning under various binning schemes are simultaneously visualized. The calibration curve estimated by our method is close to the mean result of the histogram binning under various binnings and"}, {"title": "6 Conclusion and discussion", "content": "In this paper, we focus on how to effectively incorporate prior distributions behind calibration curves with empirical data to calibrate confidence better. To address this, we propose a new calibration curve estimation method via binomial process modeling and maximum likelihood estimation and perform theoretical analysis. Furthermore, using the estimated calibration curve, a new calibration metric is proposed, which is proven to be a consistent calibration measure. In addition, this paper proposes a new simulation method for calibration data through binomial process modeling, which can serve as a benchmark to measure and compare the discrepancy between existing calibration metrics and the true calibration error. Extensive empirical studies on real-world and simulated data support our findings and showcase the effectiveness of our method."}]}