{"title": "Reasoning-Oriented and Analogy-Based Methods for Locating and Editing in Zero-Shot Event-Relational Reasoning", "authors": ["Jingyao Tang", "Lishuang Li", "Liteng Mi", "Haiming Wu", "Hongbin Lu"], "abstract": "Zero-shot event-relational reasoning is an important task in natural language processing, and existing methods jointly learn a variety of event-relational prefixes and inference-form prefixes to achieve such tasks. However, training prefixes consumes large computational resources and lacks interpretability. Additionally, learning various relational and inferential knowledge inefficiently exploits the connections between tasks. Therefore, we first propose a method for Reasoning-Oriented Locating and Editing (ROLE), which locates and edits the key modules of the language model for reasoning about event relations, enhancing interpretability and also resource-efficiently optimizing the reasoning ability. Subsequently, we propose a method for Analogy-Based Locating and Editing (ABLE), which efficiently exploits the similarities and differences between tasks to optimize the zero-shot reasoning capability. Experimental results show that ROLE improves interpretability and reasoning performance with reduced computational cost. ABLE achieves SOTA results in zero-shot reasoning.", "sections": [{"title": "1 Introduction", "content": "In the information extraction domain, reasoning about relations (e.g., causal, temporal, sub-events) between events (Man et al., 2024a; Niu et al., 2024; Wang et al., 2022; Lai et al., 2022) is crucial. These relationships have been used to construct event graphs (Frisoni et al., 2022; Chen et al., 2022), event prediction (Shi et al., 2024), common-sense reasoning (Lv et al., 2024), dialog generation (Wang et al., 2024), and question answering (Majumdar et al., 2024).\nDue to the limitations of manual labeling, we turn our attention to zero-shot event-relational reasoning. Existing approaches (Tao et al., 2023) use"}, {"title": "3 Method", "content": "Recently, researchers have started to utilize generative models (e.g., T5) for event-relational reasoning (Man et al., 2022, 2024b; Chen et al., 2024; Yang et al., 2024), as such models utilize prompts more efficiently. Therefore, in this section, we propose ROLE and ABLE using Flan-t5-large as the backbone model, and their overall framework is shown in Figure 2."}, {"title": "3.1 Reasoning-oriented locating and editing", "content": "We propose reasoning-oriented locating and editing, inspired by knowledge editing (Meng et al., 2022b). First, reasoning-oriented locating identifies the key modules $H_{(T,L)}$ of the language model in the reasoning task. Second, reasoning-oriented editing computes the change magnitude $\\Delta W_{H(T,L)}$ of the key module to optimize the reasoning performance."}, {"title": "3.1.1 Reasoning-oriented locating", "content": "This subsection aims to identify key modules. We iterate over each module $h_{(t,l)}$ of the language model and compute the effect of these modules on the reasoning task using the average indirect effect (Pearl, 2022). For positive samples, the formula for calculating the effect is:\n$Effect(h_{(t,l)}) = E_{pos}[P(Yes|x_{pos}, h_{(t,l)}) - P(Yes|x^*_{pos}, h^*_{(t,l)})],$ (1)\nFor negative samples, the effect is given by:\n$Effect(h_{(t,l)}) = E_{neg}[P(No|x_{neg}, h_{(t,l)}) - P(No|x^*_{neg}, h^*_{(t,l)})],$ (2)\nwhere, h denotes the module type in the language model, t and l denote the token and the layer number, respectively, so that $h_{(t,l)}$ denotes the module h in the l-th layer associated with token t. In addition, x is the prompt, $x^*$ is the prompt with noise, and $h^*$ denotes the noise-affected module. The reverse order of the subtraction of conditional probabilities in Eq. (2) and Eq. (1) is to ensure that the value"}, {"title": "3.1.2 Reasoning-oriented editing", "content": "This subsection aims to compute the magnitude of editing $\\Delta W_{H(T,L)}$ in the parameters of the localized module H(T,L). We edit the weights $W_{out}$ and $W_{o}$ of the last linear layer in the MLP module and the cross-attention module, respectively (see Figure 2), because they directly affect the output of the modules and modifying them can most effectively influence the model's decisions.\nMoreover, we find that the T5 model tends to answer \"Yes\" when inferring event relations (recall is much higher than precision as observed in Table 3 and Table 5). This tendency aligns with existing"}, {"title": "3.2 Analogy-based locating and editing approach", "content": "This subsection aims to fully utilize the similarities and differences between tasks to enhance zero-shot reasoning. We set four analogizable tasks A, B, C and D, i.e., the relationship between task A and task B can be analogized to the relationship between task C and task D. We first utilize ROLE to obtain the locating and editing information of tasks A, B and C:\n$(T, L)_{Task} = argmax_{(T,L)} [P(No|x^*, H_{(T,L)}) - P(No|x^*, H^*_{(T,L)})],$ (6)\n$H \\in \\{EncMLP, DecCrossAtt\\}, Task \\in \\{A, B, C\\},$\n$\\triangle W_{Task} = RK^{-1}(C_0 + K_1K_1^T)^{-1}, Task \\in \\{A, B, C\\},$ (7)\nwhere, $W_{h(T_1L_1)}$ simplifies to $\\triangle W$. These informations are then migrated analogously to task D (see Figure 2):\n$\\langle T, L\\rangle_D = \\langle T, L\\rangle_C - (\\langle T, L\\rangle_A - \\langle T, L\\rangle_B),$ (8)\n$\\triangle W_D = \\triangle W_C - \\alpha \\cdot (\\triangle W_A - \\triangle W_B),$ (9)\nwhere, $\\alpha$ is a hyperparameter that regulates the degree of being analogized. Finally, we optimize zero-shot learning using the locating and editing information of task D."}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Datasets", "content": "We perform zero-shot event relational reasoning tasks on 10 datasets, covering three types of tasks: causal relation extraction, causal relation classification, and sub-event relation extraction.\nCausal relation extraction: following Gao et al. (Gao et al., 2023), we evaluate intra-sentence pairs of causal events in EventStory-Line v0.9 (ESC-intra) (Caselli and Vossen, 2017), Causal-TimeBank (CTB-intra) (Mirza et al., 2014) and MAVEN-ERE (MAVEN-intra-causal) (Wang et al., 2022). Furthermore, following the work of Tao et al. (Tao et al., 2023) for UniEvent, we evaluate SCITE (SCI-uni), EventStoryLine (ESL-uni) and Causal-TimeBank (CTB-uni)."}, {"title": "4.2 Baselines", "content": "T5 and T5-large (Raffel et al., 2020): is a pre-trained language model based on the Transformer architecture, containing encoders and decoders, for a variety of natural language processing tasks.\nT0-3B (Sanh et al., 2022): a language model optimized for zero-shot learning scenarios based on the T5 architecture.\nUniEvent (Tao et al., 2023): based on the T5 architecture, utilizes prefix-tuning and multi-task learning to achieve zero-shot event-relational reasoning.\nGPT series models (Gao et al., 2023): including text-davinci-002, text-davinci-003, GPT-3.5, and GPT-4, which are progressively optimized based on OpenAI's GPT-3 model, improving the ability to"}, {"title": "4.3 Zero-Shot Results", "content": "Table 2 shows the performance of ABLE and the baseline models on the zero-shot causal relation extraction, causal relation classification, and sub-event relation extraction tasks. For causal relation extraction, ABLE achieves SOTA on the SCI-uni, ESL-uni, CTB-uni, and CTB-intra datasets, and shows competitive performance comparable to LLMs on the ESC-intra and MAVEN-intra datasets. For causal relation classification and sub-event relation extraction, ABLE achieves SOTA on all datasets. These results show that ABLE efficiently learns and transfers reasoning knowledge, which improves the performance of various types of zero-shot event-relational reasoning tasks."}, {"title": "4.4 Ablation study", "content": "To verify the effectiveness of ROLE and ABLE, we conduct ablation experiments. We construct four forms of ABLE, including $ABLE_{Enc}$, $ABLE_{Enc}^2$, $ABLE_{Dec}$, and $ABLE_{Dec}^2$ (see"}, {"title": "4.5 Analysis of the analogicality of location", "content": "This subsection analyzes the analogicality of the location of key modules. First, Table 6 shows the layers of editing for ROLE and ABLE under different tasks. $ROLE_{Enc}$ selects the top 3 module layers in terms of average indirect effects (see Equation 2) in negative samples. $ROLE_{Dec}$ selects the 1st ranked module layer (the sub-event relation extraction task selects the 3rd ranked layer because the 1st and 2nd ranked positions overlap for the positive and negative samples). ABLE determines the module layer analogously (see Equation 8). Table 6 shows that ABLE obtains positions that are close to the top ranked positions obtained by ROLE (see Table 11 in Appendix B), which validates the analogicality of the location to some extent.\nSecond, as seen from Figure 6, the difference line plots of temporal, causal, and sub-event relations show similar trends for either positive or negative samples for most tokens, which further validates the analogous nature of location.\nAdditionally, for the positive samples, in the decoder's \"</s>\" token (first 3 rows of the last column), the line plots of the causal and temporal relations are similar, while the plots of the sub-event relations are different. For the negative samples, in the encoder's \"causal/temporal/sub-event\" token (the last 3 rows of the 3rd column), the line plots of causal and subevent relations are similar, while the plots of temporal relations are different. These"}, {"title": "4.6 Analysis of the analogicality of editing magnitude", "content": "This subsection analyzes the analogicality of editing magnitude. Let A, B, C, D, E, F denote the classification and extraction tasks of temporal, causal, and sub-event relations, respectively. Let $\\Delta W_X$ denote the editing magnitude for a module parameter of task X, and let $\\Delta W_{XY} = \\Delta W_X - \\Delta W_Y$, where X \u2208 {A, C, E} and Y \u2208 {B, D, F}.\nTable 7 shows the similarity between different $\\Delta W_{XY}$. We use the cosine of the main eigenvectors of the matrices to compute the similarity, because it reflects the degree of similarity between the main transformation directions of the matrices. If the similarity is near 1, the directions are similar; if it is near -1, the directions are opposite.\nAs seen in Table 7, similarities between $\\Delta W_{XY}$ of analogous tasks are higher than those between $\\Delta W_{XY}$ of non-analogous tasks, which verifies the analogicality of editing magnitude to some extent."}, {"title": "4.7 Analysis on computational resources and time", "content": "We analyze the computational resources and time required for ROLE, ABLE, the fine-tuning method and UniEvent (prefix fine-tuning), respectively, with respect to the amount of parameters (Params),"}, {"title": "5 Conclusion", "content": "We first propose ROLE to locate and edit key modules of language models in reasoning about event relations. The results show that ROLE improves interpretability and reasoning performance with reduced computational cost. Then, we propose ABLE to analogize the similarities and differences between tasks. The results show that ABLE achieves SOTA results for zero-shot event-relational reasoning on most datasets.\nFurthermore, our experiments provide insights into the mechanisms of reasoning about event relations in language models and verify the feasibility of model editing to optimize reasoning capabilities. Future work could utilize ROLE and ABLE to further explore the reasoning capabilities of large language models."}, {"title": "6 Limitations", "content": "Our methods mainly address binary reasoning tasks. Moreover, our study on the reasoning ability of language models is not comprehensive enough. Therefore, future work can be extended to more complex reasoning scenarios, and also, more experiments can be conducted to explore the reasoning mechanism in depth."}]}