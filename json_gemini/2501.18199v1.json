{"title": "HKAN: Hierarchical Kolmogorov-Arnold Network without Backpropagation", "authors": ["Grzegorz Dudek", "Tomasz Rodak"], "abstract": "This paper introduces the Hierarchical Kolmogorov-Arnold Network (HKAN), a novel network architecture that offers a competitive alternative to the recently proposed Kolmogorov-Arnold Network (KAN). Unlike KAN, which relies on backpropagation, HKAN adopts a randomized learning approach, where the parameters of its basis functions are fixed, and linear aggregations are optimized using least-squares regression. HKAN utilizes a hierarchical multi-stacking framework, with each layer refining the predictions from the previous one by solving a series of linear regression problems. This non-iterative training method simplifies computation and eliminates sensitivity to local minima in the loss function. Empirical results show that HKAN delivers comparable, if not superior, accuracy and stability relative to KAN across various regression tasks, while also providing insights into variable importance. The proposed approach seamlessly integrates theoretical insights with practical applications, presenting a robust and efficient alternative for neural network modeling.", "sections": [{"title": "I. INTRODUCTION", "content": "KOLMOGOROV-ARNOLD Networks (KANs), introduced in [1], represent a paradigm shift in neural network (NN) architecture, offering a promising alternative to traditional Multi-Layer Perceptrons (MLPs). Rooted in the Kolmogorov-Arnold representation theorem, which states that any multivariate continuous function $f : [0,1]^n \\rightarrow R$ can be represented as a superposition of continuous functions of a single variable and the binary operation of addition, KANS fundamentally reimagine the structure of NNs.\nKANs introduce a significant architectural innovation: unlike MLPs with fixed activation functions on nodes, KANS employ learnable activation functions on edges. Notably, KANs eliminate linear weights entirely, replacing each weight parameter with a univariate function parametrized as a spline. This seemingly simple modification yields significant improvements in both accuracy and interpretability. In terms of accuracy, smaller KAN models consistently achieve comparable or superior performance to larger MLPs in data fitting and partial differential equations solving tasks. Both theoretical analysis and empirical evidence suggest that KANs exhibit faster neural scaling laws than MLPs. Regarding interpretability, KANS offer intuitive visualization and facilitate easy interaction with human users, enhancing their potential as collaborative tools for scientific discovery.\nThe unique properties of KANs make them valuable collaborators in helping scientists discover mathematical and physical laws, bridging the gap between machine learning and traditional scientific inquiry. As promising alternatives to MLPS, KANs open new avenues for improving contemporary deep learning models, which heavily rely on MLP architectures."}, {"title": "A. Related Work", "content": "Recent research has highlighted the potential of KANs as efficient and interpretable alternatives to traditional MLPs [2]-[4]. Unlike MLPS, KANs replace linear weights with learnable activation functions, enabling dynamic pattern learning and improved performance with fewer parameters. Studies have shown that KANs can achieve comparable or even superior accuracy to larger MLPs, faster neural scaling laws, and enhanced interpretability [5]. From a theoretical perspective, Wang et al. [6] demonstrated that the approximation and representation capabilities of KANs are at least equivalent to those of MLPs. Furthermore, KAN's multi-level learning approach, particularly its grid extension of splines, enhances the modeling of high-frequency components. While MLPs often suffer from catastrophic forgetting, collaborative filtering-based KANs have been proposed to address this issue [7].\nDespite these advancements, KANs are not without criticism. Some studies argue that KAN outperforms MLPs primarily in symbolic formula representation but falls short in tasks like computer vision, natural language processing, and audio processing [8]. Tran et al. [9] reported that despite their theoretical advantages, KANs do not consistently outperform MLPs in practical classification tasks. Additionally, their hardware implementations tend to be less efficient, with higher resource usage and latency. Sensitivity to noise is another limitation; even minimal noise in the data can significantly degrade performance [10].\nTo enhance the interpretability, interactivity, and versatility of KAN, Liu et al. [11] introduced MultKAN, which incorporates multiplication operations. By integrating multiplication nodes, MultKAN explicitly represents multiplicative structures, allowing for a more transparent mapping of physical laws and improved modeling of complex relationships.\nKANs have also been integrated with other architectures to address diverse challenges. In computer vision, [12] combined convolutional layers, demonstrating that KAN"}, {"title": "B. Motivation and Contributions", "content": "KAN models are traditionally trained using backpropagation algorithms, which rely on gradients of the network's loss function with respect to its parameters. However, gradient-based learning processes are sensitive to issues such as local minima, flat regions, and saddle points in the loss function. Additionally, gradient calculations can be computationally expensive, particularly for deep and wide network architectures, complex target functions, and large training datasets.\nIn this study, we propose a randomized learning approach for training KANs as an alternative to backpropagation. Unlike gradient-based methods, which lead to non-convex optimization problems, the randomized approach transforms the problem into a convex one [42]. This is achieved by fixing the parameters of the activation functions, which are selected either randomly or based on the data, and remain unchanged during training. The only adaptation occurs in the linear functions that aggregate the outputs of the basis functions and activation functions. Since the optimization problem becomes linear, the model's weights can be efficiently learned using a standard least-squares method. This significantly simplifies the training process and accelerates computation compared to gradient-based approaches. Numerous studies in the literature have demonstrated the high performance of randomized neural models compared to fully trainable ones [43]-[50].\nOur approach begins with utilizing fixed parameters for basis functions, determined either by data or randomly. These basis functions are then combined in multiple blocks using linear regression. The resulting block functions (activation functions) are subsequently combined through linear regression, and this iterative process is repeated across subsequent layers to form higher-level representations. Combining diverse blocks corresponds to ensembling, while performing it layer by layer constitutes a multi-stacking approach. This hierarchical modeling of the target function progressively enhances accuracy at each level, eliminating the need for backpropagation.\nOur study makes tree significant contributions to the field of NNs, specifically in the domain of KANs:\n1) Novel Training Method for KAN: We introduce an innovative approach to training KANs that eliminates the need for backpropagation. The parameters of basis functions are fixed, determined either randomly or based on data. The model is trained hierarchically using the standard least-squares method. This approach results in a more efficient and robust training process for KANs, offering improvements in both computational efficiency and model accuracy.\n2) Multi-Stacking Approach for Prediction: Our hierarchical KAN (HKAN) implements hierarchically multi-stacking approach to built predictions. In each layer, meta-learners combine predictions performed by weak learners (univariate models). Subsequent layers, fed by predictions from previous layers, successively refine the results, enhancing overall accuracy layer by layer.\n3) Empirical Results for Regression Problems: We provide comprehensive empirical evidence demonstrating that our HKAN outperforms standard KAN in a range of regression problems.\nThe remainder of this paper is organized as follows: Section II provides an overview of the Kolmogorov-Arnold representation theorem and standard KANs, establishing the foundation for our research. Section III introduces the proposed HK\u0391\u039d model, detailing its architecture, components, features, and learning process. Section IV presents a comparison between HKAN and KAN, while Section V examines HKAN through the lens of multi-stacking models. The experimental framework used to evaluate the proposed model is described in Section VI. Finally, Section VII concludes the paper."}, {"title": "II. PRELIMINARY", "content": "The Kolmogorov-Arnold representation theorem, also known as the superposition theorem, stands as a cornerstone"}, {"title": "A. Kolmogorov\u2013Arnold Representation Theorem", "content": "For any continuous function $f : [0,1]^n \\rightarrow R$, there exist continuous functions $\\phi_{q,p}: [0,1] \\rightarrow R$ and $\\Phi_q : R \\rightarrow R$ such that:\n$$f(x_1,...,x_n) = \\sum_{q=1}^{2n+1} \\Phi_q\\left( \\sum_{p=1}^n \\phi_{q,p}(x_p)\\right)$$\n(1)\nThe theorem carries significant implications for function approximation and theoretical computer science. It suggests a universal approximation capability, implying that any multivariate continuous function can be approximated by a network of simple, single-variable functions. Notably, the outer functions $\\Phi_q$ are independent of the function $f$ being approximated, serving as universal building blocks. This property effectively reduces the problem of approximating n-dimensional functions to that of approximating one-dimensional functions.\nHowever, the theorem's practical application faces certain limitations. The inner functions $\\phi_{q,p}$ can be highly non-smooth, even when $f$ is smooth, potentially complicating computational implementation. Moreover, while theoretically powerful, the representation may not be efficiently computable in practice.\nThe Kolmogorov-Arnold representation theorem stands as a bridge between pure mathematics and applied computational science, highlighting the potential for representing complex functions through simpler components while also illustrating the challenges in translating theoretical results into practical applications."}, {"title": "B. Kolmogorov\u2013Arnold Networks (KANs)", "content": "Paper [1] extends and modifies the Kolmogorov-Arnold representation theorem to create Kolmogorov-Arnold Networks (KANs) in several key ways. While the original theorem uses a 2-layer network with a specific width in the hidden layer, KANs generalize this to allow arbitrary widths and depths, stacking multiple \"KAN layers\".\nA KAN layer is defined as a matrix of activation functions $\\Phi_{q,p}$, where q is not restricted to the theoretical limit of 2n+1:\n$$\\Phi(x) = \\begin{bmatrix}\n \\Phi_{1,1}(x_1) & \\cdots & \\Phi_{1,n_{in}}(x_{n_{in}})\\\\\n \\vdots & & \\vdots \\\\\n \\Phi_{n_{out},1}(x_1) & \\cdots & \\Phi_{n_{out},n_{in}}(x_{n_{in}})\n\\end{bmatrix}$$\n(2)\nwhere $x_p$ denotes the p-th input to the layer, $n_{in}$ denotes the number of inputs, and $n_{out}$ denotes the number of outputs (not restricted to 2n + 1).\nDeeper KANs are created by composing multiple KAN layers. Unlike the original theorem which allows non-smooth or even fractal functions, KANs assume smooth activation functions to facilitate learning. The authors propose activation functions parameterized as B-splines with trainable coefficients combined with the sigmoid linear unit (SiLU). Note the substantial difference between KANs and MLPs: instead of fixed multidimensional activation functions on nodes as"}, {"title": "III. HIERARCHICAL KAN", "content": "Table I provides a summary of the main symbols used throughout this study for clarity and reference. The implementation of the proposed model is available in our GitHub repository [51]."}, {"title": "A. Architecture", "content": "HKAN is an advanced NN architecture inspired by the Kolmogorov-Arnold representation theorem. While it shares similarities with the KAN architecture, HKAN introduces unique components and a distinct training process. The architecture of HKAN is shown in Fig. 1."}, {"title": "Gaussian basis functions", "content": "Let $z^{(l-1)}$ be the input vector for layer l. The first layer of HKAN takes input vector $z^{(0)} = x = [x_1,...,x_n]^T \\in R^n$. Each component of the input vector is transformed by a group of blocks. Within each group, there are $n^{(l)}$ blocks, and each block projects its input nonlinearly using $m^{(l)}$ basis functions (BaFs). Consider two common basis functions: Gaussian\n$$g(z) = exp(-\\left(\\sigma (z - \\mu)\\right)^2)$$\n(3)\nand sigmoid\n$$g(z) = \\frac{1}{1 + exp\\left(-\\sigma (z - \\mu)\\right)}$$\n(4)\nwhere $\\mu$ is the location parameter, and $\\sigma$ is the smoothing parameter corresponding to the slope or bandwidth of the BaF.\nThe configuration of BaFs plays a crucial role in the network's performance. The number of BaFs, together with the smoothing parameter, serve as hyperparameters that define the block's flexibility and balance the trade-off between variance and bias in the output. The locations of the BaFs, denoted as $\\mu^{(l)}_{q,p,r}$ for the r-th function in the block corresponding to $\\phi^{(l)}_{q,p}$, define the position of the maximum for Gaussian functions or the inflection point for sigmoid functions. To distribute BaFs in a block across the input interval (typically a bounded region of [0, 1]), these locations can be selected in two ways:\n\u2022 Random uniform distribution: The locations are drawn from a uniform distribution, $\\mu^{(l)}_{q,p,r} \\sim U(0,1)$, ensuring random spread across the input range.\n\u2022 Data-driven distribution (support point method): In this approach, locations are assigned to randomly selected training points (in the first layer) or their projections (in subsequent layers), called support points: $\\mu^{(l)}_{q,p,r} = z^{(\\xi_p^{l-1})}$, where $\\xi \\sim U{1, .., N}$.\nThe support point method aligns the BaFs with the data distribution, avoiding empty regions in the input space (see Fig. 2)."}, {"title": "Sigmoid basis functions", "content": "The BaFs within a block, $g^{(l)}_{q,p,r}: R\\rightarrow R$, are combined using linear regression:\n$$\\phi^{(l)}_{q,p}(z^{(l-1)}) = \\sum_{r=1}^{m^{(l)}} c^{(l)}_{q,p,r}g^{(l)}_{q,p,r}(z^{(l-1)} )$$\n(5)\nResulting function $\\phi^{(l)}_{q,p}: : Rm \\rightarrow R$ is called a block function (BIF). The weights of each BIF, $c^{(l)}_{q,p,r}$, are determined using least squares by minimizing the sum of squared residuals:\n$$L = \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$\n(6)\nwhere $\\hat{y}_i$ is a prediction performed by the BIF: $\\hat{y}_i = \\phi^{(l)}_{q,p}(z_i^{(l-1)})$.\nThe weights of $c^{(l)}_{q,p}$ can be calculated analitically as\n$$c^{(l)}_{q,p} = G^+_{q,p}y$$\n(7)\nwhere $c^{(l)}_{q,p} = [c^{(l)}_{q,p,1},..., c^{(l)}_{q,p,m^{(l)}}]^T$, $y = [y_1,...,y_N]^T$ and $G^+_{q,p}$ is the Moore-Penrose generalized inverse of the BaF response matrix to the N training data points or their projections:\n$$G_{q,p}^{(l)} = \\begin{bmatrix}\n g_{q,p,1}(z_1^{(l-1)}) & \\cdots & g_{q,p,m^{(l)}}(z_1^{(l-1)}) \\\\\n \\vdots & & \\vdots \\\\\n g_{q,p,1}(z_N^{(l-1)}) & \\cdots & g_{q,p,m^{(l)}}(z_N^{(l-1)})\n\\end{bmatrix}$$\n(8)\nFor the p-th input, each block in a group fits different function (due to random positioning of BaFs) that approximate the target function based on this input. Consequently, there are a total of $n^{(l-1)}n^{(l)}$ BIFs in layer l. In the subsequent step, these BIFs are combined linearly by $n^{(l)}$ h-functions. Each q-th h-function takes as input the q-th BIF from every group. Thus, each h-function approximates the target function based on different projections of all components of the input pattern for layer l:\n$$h_q^{(l)}(z^{(l-1)}) = \\sum_{p=1}^{n^{(l-1)}} w_{q,p}^{(l)} \\phi_{q,p}^{(l)}(z^{(l-1)})$$\n(9)"}, {"title": "B. Hyperparameters", "content": "The weights of this combination are calculated as:\n$$w_q^{(l)} = \\Phi^{(l)+}_q y$$\n(10)\nwhere $w_q^{(l)} = [w_{q,1},..., w_{q, n^{(l)}}]$, and $\\Phi_q^{(l)+}$ is the Moore-Penrose generalized inverse of the BIF response matrix to the projections of N training data points:\n$$\\Phi_q^{(l)} = \\begin{bmatrix}\n \\Phi_{q,1}(z_{1,1}^{(l-1)}) & \\cdots & \\Phi_{q, n^{(l)}}(z_{1,1}^{(l-1)}) \\\\\n \\vdots & & \\vdots \\\\\n \\Phi_{q,1}(z_{q,N}^{(l-1)}) & \\cdots & \\Phi_{q, n^{(l)}}(z_{q,N}^{(l-1)})\n\\end{bmatrix}$$\n(11)\nWeights (10) minimize loss function (6), where $\\hat{y}$ is a prediction performed by the h-function: $\\hat{y}_i = h_q^{(l)}(z_i^{(l-1)})$.\nThe output of layer l is given by $z^{(l)} = [..., h_q^{(l)}(z_i^{(l-1)}), ...] \\in R^{n^{(l)}}$, where each component $h_q^{(l)}(z_i^{(l-1)})$. This output is then fed to the next layer and processed using (5)-(11).\nFollowing the design of the original KAN, the structure of the top layer (layer L) in HKAN differs slightly from the preceding layers, as it includes only one BaF per input. The final output of HKAN is obtained through a linear combination of the $n^{(L)} = n^{(L-1)}$ BIFs:\n$$h^{(L)}(z^{(L-1)}) = \\sum_{p=1}^{n^{(L)}} w_p^{(L)} \\phi_p^{(L)}(z^{(L-1)})$$\n(12)\nIn HKAN, we employ standard linear regression, which is applied multiple times both at the block level (BIFs) and for combining multiple blocks (h-functions). However, to mitigate overfitting, we can alternatively use regularized least squares (ridge regression). In the experimental part of this work, we adopt this variant to calculate the weights of BIFs, $\\phi^{(l)}_{q,p}$. The weights in this case are computed using the following closed-form solution:\n$$c_{q,p}^{(l)} = (G_q^{(l)T}G_q^{(l)} + \\lambda^{(l)}I)^{-1}G_q^{(l)T}y$$\n(13)\nwhere I is an identity matrix and $\\lambda \\geq 0$ is a regularization parameter."}, {"title": "Learning", "content": "The optimization problem in HKAN is decomposed into multiple linear regression subproblems. Each subproblem minimizes objective function (6) using the least squares method. Since these optimization subproblems are convex, the least squares approach guarantees optimal weights (within the context of the randomly selected BaFs).\nThe learning process in HKAN is hierarchical. First, the target function is simultaneously approximated by the blocks of the first layer, with each block modeling the target based on a single input variable. Due to the nonlinear nature of the BaFs, this step introduces nonlinearity into the modeling process. Then, based on these approximations, multiple h-functions are fitted to the target function. Each h-function linearly combines single-variable BIFs, producing a multivariable mapping."}, {"title": "C. Hyperparameters", "content": "The HKAN hyperparameters are as follows:\n\u2022 L - total number of layers,\n\u2022 $n^{(l)}$ \u2013 width of the l-th layer (number of nodes),\n\u2022 $m^{(l)}$ - number of BaFs in blocks of layer l,\n\u2022 BaF type,\n\u2022 Way to generate location parameters of BaFs (\u03bc),\n\u2022 $\\sigma^{(l)}$ \u2013 smoothing parameter in layer l,\n\u2022 $\\lambda^{(l)}_{\u03c6}$, $\\lambda^{(l)}_{h}$ \u2013 regularization parameters for functions \u03c6 and h in layer l (optional).\nIntuitively, a more complex target function needs a deeper and wider network to model it with higher accuracy. The modeling burden can be shifted at the block level. In such a case many BaFs are needed (with \u03c3-parameter adjusted to the approximation problem complexity) and less h-functions. Opposite situation is also possible: a small number of BaFs roughly approximates the target function, and the effort of a more accurate approximation falls on a large number of h-functions.\nIt is important to note that the regression problem solved at each level of HKAN processing can vary. At the initial level, the target function is approximated based directly on input patterns x, whereas at subsequent levels, it is approximated based on the predictions generated by the previous level. Consequently, the complexity of the problems addressed at different levels may differ, necessitating distinct values for layer-specific hyperparameters $n^{(l)}$, $m^{(l)}$, $\\sigma^{(l)}$, and optionally $\\lambda^{(l)}_{\u03c6}$ and $\\lambda^{(l)}_{h}$.\nHKANs with more layers, more nodes, more BaFs and with smaller values of the parameters \u03c3 and \u03bb tend to fit the target function more accurately. However, such configurations are more susceptible to overfitting. Therefore, these parameters must be carefully tuned to strike an optimal balance between the model's bias and variance.\nThe strategy for generating \u03bc values, which determine the positions of BaFs, depends on the anticipated data distribution. When the distribution of new, unseen data points is expected to closely mirror that of the training dataset, positioning BaFs at the training points is often an effective approach. This method ensures that the network's approximation capability is concentrated in regions where data is most likely to occur. Conversely, if the distribution of new data is expected to differ from the training set, or if the goal is to create a more generalized model, a random distribution of BaFs may be"}, {"title": "D. Complexity", "content": "In layer l, each block performs linear regression on $m^{(l)}$ BaFs with a complexity of $O(Nm^{(l)2} + m^{(l)3})$. For layers l = 1, ..., L \u2212 1, each containing $n^{(l-1)}n^{(l)}$ blocks, the total complexity per layer is $O(n^{(l\u22121)}n^{(l)}(Nm^{(l)2} + m^{(l)3}))$. The top layer contains $n^{(L)}$ blocks, resulting in a complexity of $O(n^{(L)}(Nm^{(L)2} + m^{(L)3}))$.\nEach function $h_q^{(l)}$ linearly combines $n^{(l-1)}$ blocks, yielding a complexity of $O(Nn^{(l\u22121)2} + n^{(l-1)3})$. For layers l = 1, ..., L \u2212 1, with $n^{(l)}$ such functions per layer, the total complexity becomes $O(n^{(l)}(Nn^{(l\u22121)2} + n^{(l\u22121)3}))$. The final layer produces a single output, resulting in a complexity of $O(Nn^{(L)2} + n^{(L)3})$."}, {"title": "IV. HKAN VS STANDARD KAN", "content": "This section outlines the key differences between KAN as defined in [1] and our proposed HKAN."}, {"title": "A. Basis Functions", "content": "KAN employs B-splines of order 3, which are bell-shaped and computed recursively using the Cox-de Boor formula. The properties of these BaFs, including their number, location, width, and support, are determined by knots. In KANs, these knots are positioned at equidistant intervals, resulting in an even distribution of the basis functions across the input space. The number of knots, which directly influences the spline's flexibility and the model's capacity to capture complex patterns, is a crucial hyperparameter in the KAN architecture.\nIn contrast, HKANs offer greater flexibility in the selection of BaF types. While in Section III-A we introduced Gaussian and sigmoid functions, the HKAN framework is not limited to these and can accommodate various other functional forms (see experimental part of this work, Section VI). Unlike KANS, the distribution of BaFs in HKANs can be either data-driven or random. In the HKAN framework, the smoothing parameter and the number of BaFs serve as key hyperparameters. The adaptability in both function type and distribution allows HKANs to potentially capture a wider range of functional relationships within the data.\nWhen evaluating computational efficiency, it should be noted that KAN uses B-splines, whose computation involves recursive processes, making it computationally intensive. In contrast, HKAN does not require a recursive process to create BaFs, potentially reducing computational complexity."}, {"title": "B. Block Functions (Activation Functions)", "content": "In KAN, what we refer to as a BIF is termed an activation function. This activation function is a composite structure, consisting of two main components: a weighted sum of a spline (which itself is a linear combination of BaFs with trainable weights c) and a SiLU. The incorporation of SiLU was likely designed to enhance the network's training dynamics. In the KAN architecture, the BIFs are aggregated without additional weighting.\nHKAN employ a distinct methodology. In this framework, BIFs are also constructed by combining BaFs with weights c, similar to KAN. However, unlike in KAN, these weights are not optimized via gradient descent. Instead, they are computed using the least-squares method, with the goal of fitting each BIF to the target function in a one-dimensional space. This method provides a more direct, analytical determination of the weights. The HKAN then linearly combines these BIFs, once again using weights determined through the least-squares method, to approximate the target function in multi-dimensional space."}, {"title": "C. Explainability and Function Representation", "content": "In KANS, BIFs serve as interpretable building blocks, designed with the flexibility to be replaced by specific symbolic forms such as polynomial, sine, or logarithmic functions. This design philosophy enables transparent construction of complex functions from simpler components, iterative refinement of the target function during the training process, and potential for direct translation into human-readable mathematical expressions. This modular approach facilitates a bottom-up understanding of the learned function, allowing researchers to dissect and analyze the contribution of each component to the overall model behavior.\nHKANs, on the other hand, employ BIFs in a fundamentally different manner. Each BIF attempts to approximate the target function within a one-dimensional space. This approach provides a direct measure of individual input variable importance, with the quality of these one-dimensional approximations serving as a metric for assessing the expressive power of each input variable. This characteristic of HKANs allows us to quickly identify key input arguments and gauge their significance in the model, providing a clear path for understanding the contributions of individual variables to the overall function approximation."}, {"title": "D. Learning", "content": "KAN and HKAN employ fundamentally different approaches to training, each with distinct characteristics and implications.\nKAN utilizes gradient descent in backpropagation process to train the parameters including the weights of the BaFs, c, and the weights of the spline and SiLU combinations. This approach allows for fine-tuning of the network but introduces the challenges associated with iterative gradient-based optimization, such as potential convergence to local optima and sensitivity to initial conditions."}, {"title": "V. HKAN AS MULTI-STACKING MODEL", "content": "Stacking has emerged as a highly effective approach for enhancing the predictive power of machine learning models [52]. It employs a meta-learning algorithm to optimally combine predictions generated by different learners. By combining multiple diverse weak learners, an ensemble can reduce the overall error.\nIn the context of HKAN, BIFs serve as the weak learners, while h-functions act as the meta-learners. BIFs typically offer a rough nonlinear approximation of the target function within one-dimensional subspaces, providing distinct perspectives on the input data.\nKey aspects of ensembling involve two main considerations: how to combine learners and how to generate diversity among them. Diverse weak learners capture various patterns and relationships within the data. This broad coverage helps the ensemble generalize better to new, unseen data, reducing overfitting to the training set and improving model robustness.\nIn our case, linear regression addresses the combination of learners, while diversity is achieved through the modeling of the target function in one-dimensional subspaces and the randomized distribution of BaFs. Diversity is further controlled by the number of BaFs and the smoothing parameter.\nHKAN builds upon BIFs through a stacking approach. The BIFs, each constructed on different projections of individual inputs, are linearly combined to approximate the target function. This combination is performed by multiple stacking h-functions. Each stacking function integrates a unique set of BIFs, enabling diverse multivariate representations of the target function.\nThe process extends hierarchically, with the stacking functions from one layer serving as inputs to the next. In each subsequent layer, these stacking functions are nonlinearly transformed by BaFs and then linearly combined to generate new BIFs. These new BIFs are subsequently aggregated to form the stacking functions of the next layer, resulting in a cascade of increasingly complex and abstract representations.\nNotably, each layer consists of multiple stacking functions (h), enabling a parallelized process within each level. This multi-level, parallel stacking architecture allows HKAN to efficiently capture intricate relationships in the data, leveraging the strengths of stacking across multiple scales simultaneously."}, {"title": "VI. EXPERIMENTAL STUDY", "content": "In this section, we compare our proposed HKAN with the standard KAN in regression tasks, evaluating their approximation accuracy. The experimental evaluation was performed on a variety of datasets to validate the model's performance across different regression and function approximation scenarios."}, {"title": "A. Datasets", "content": "The selected datasets include benchmark regression datasets and synthetically generated data designed to emulate complex target functions. They were chosen to evaluate model's ability to generalize across both simple and highly nonlinear relationships.\nThe synthetic target functions were defined as follows:\nTF1: $g(x) = (2x_1 - 1)(2x_2 \u2212 1), x_1, x_2 \\in [0,1]$\nTF2: $g(x) = \\sum_{i=1}^2 sin (20 exp x_i) x, x_i \\in [0, 1]$\nTF3: $g(x) = -\\sum_{i=1}^2 x_i sin(\\sqrt[xi]), x_i \\in [-500, 500]$\nTF4: $g(x) = \\frac{1}{0.1+\\sum_{i=1}^{n}x_i^2}$, n = 10, $x_i \\in [-4, 4]$\nTF5: $g(x) = -\\sum_{i=1}^{n} sin(x_i) sin^2(\\frac{ix^2}{\\pi})$, n = 2,5, $x_i \\in [0, \\pi]$"}, {"title": "B. Optimization", "content": "Table III outlines the search space for HKAN hyperparameters. The tree-structured Parzen estimator algorithm, implemented in the Optuna framework [53], was used to explore this space. A total of 1000 trials were conducted, with early stopping applied by pruning trials where the RMSE exceeded twice the baseline RMSE, calculated as the RMSE of the mean prediction on the training set. The optimal hyperparameters were determined using 5-fold cross-validation. The selected hyperparameter values are summarized in Table IV.\nKAN optimization was performed using Optuna's trial system, integrated with a grid search sampler [53]. The network's architecture was optimized across a predefined set of configurations: W = {[n, 1], [n, 2, 1], [n, n + 1, 1], [n, 2n +\n1, 1], [n, 2, 2, 1], [n, n + 1, 2, 1], [n, 2n + 1, 2, 1], [n, n+1, n +\n1,1], [n, 2n+1, n+1,1], [n, 2n+1,2n+1,1]}. These architectures were chosen based on the original authors' recommendations, emphasizing model accuracy over interpretability.\nInitial experiments highlighted the importance of identifying the optimal number of training steps to maximize the performance of the KAN model, as excessive training can lead to overfitting. To mitigate this risk, a 5-fold cross-validation"}, {"title": "C. Results", "content": "Table V summarizes the performance metrics for KAN and HKAN, including the median and interquartile range (IQR) of RMSE for both training and test data, calculated from 50 independent training sessions per model. These results are further illustrated in Fig. 4 using boxplots.\nThe test errors of both models were compared using the Wilcoxon signed-rank test. As shown in Table V, H\u039a\u0391\u039d achieved significantly lower errors than KAN in 12 out of 24 cases, while KAN outperformed HKAN in 9 cases. Notably, HKAN demonstrated superior accuracy on synthetic functions TF1, TF3, TF5, and TF5-5, where its accuracy exceeded that of KAN by several orders of magnitude. For TF2 (a synthetic function with noise), HKAN's errors were over 19% lower than KAN's. A substantial improvement was also observed for the MachineCPU dataset, where HKAN outperformed K\u0391\u039d with a difference exceeding 51%.\nConversely, the largest differences favoring KAN were observed for the Kinematics8nm dataset (over 118%), Ele2 (over 37%), Pumadyn32nh (over 18%), and Elevators (over"}, {"title": "D. How HKAN Constructs Fitted Function", "content": "This section analyzes how HKAN constructs a fitted function layer by layer. Fig. 5 provides an example for TF2. The two upper panels illustrate the functions fitted at successive levels of HKAN processing (successive linear regressions), specifically the BIFs of the first layer (\u03c6(1)), the h-functions of the first layer (h(1)), the BlFs of the second layer (\u03c6(2)), the h-functions of the second layer (h(2)), the BlFs of the third layer (\u03c6(3)), and the h-function of the third layer (h(3)).\nAt each level, except the final one, multiple functions are fitted in parallel; for clarity, only two representative functions are displayed in the two upper panels. The lower panel presents predicted vs. target plots for five selected fitted functions at each processing level, excluding the final level, where only a single function is created.\nThe following insights can be drawn from Fig. 5:\n1) Shapes and Complexity of the Fitted Functions: The fitted function evolves in shape and complexity as it"}, {"title": "E. Input Argument Importance Estimation by HKAN", "content": "HKAN includes a built-in mechanism for estimating the importance of input arguments. The blocks in the first layer approximate the target function based on individual inputs, and the accuracy of each block's fitting, measured by R2, serves as a proxy for the importance of the corresponding input. However, it should be noted that this importance is estimated based on the coarse approximation performed by the first-layer blocks. The more refined approximations developed in subsequent layers are not considered in this estimation.\nFig. 9 presents boxplots of the $R^2$ values for"}]}