{"title": "Multi-Agent Deep Q-Network with Layer-based Communication Channel for Autonomous Internal Logistics Vehicle Scheduling in Smart Manufacturing", "authors": ["Mohammad Feizabadi", "Arman Hosseini", "Zakaria Yahouni"], "abstract": "In smart manufacturing, scheduling autonomous internal logistic vehicles is crucial for optimizing operational efficiency. This paper proposes a multi-agent deep Q-network (MADQN) with a layer-based communication channel (LBCC) to address this challenge. The main goals are to minimize total job tardiness, reduce the number of tardy jobs, and lower vehicle energy consumption. The method is evaluated against nine well-known scheduling heuristics, demonstrating its effectiveness in handling dynamic job shop behaviors like job arrivals and workstation unavailabilities. The approach also proves scalable, maintaining performance across different layouts and larger problem instances, highlighting the robustness and adaptability of MADQN with LBCC in smart manufacturing.", "sections": [{"title": "1 Introduction", "content": "Internal Logistics Vehicles (ILVs) are crucial in enhancing the performance of manufacturing systems by facilitating the movement of products within manufacturing facilities [1]. With the advent of Industry 4.0 technologies, the automation of these movements has been driven by multiple factors, including the improvement of production capacity and the reduction of injuries among human operators who traditionally moved heavy products. Notable technologies in this domain include Automated Guided Vehicles (AGVs) and Autonomous Intelligent Vehicles (AIVs), which autonomously transport products while considering workshop constraints and layouts.\n\nThe implementation of these systems presents several challenges that require careful consideration. Key among these challenges is the task of determining the priority for transporting products and selecting the appropriate vehicle for each transportation task. Moreover, these scheduling activities must account for various constraints, including delivery times, vehicle capacity and battery charging requirements, handling breakdowns of vehicles and machines, addressing urgent jobs, etc. Effective vehicle management is therefore crucial to align internal logistics with manufacturing objectives, such as minimizing tardiness of orders, achieving a balanced workload among workstations and vehicles, and optimizing the energy consumption of vehicles. To address these challenges, scheduling strategies consist of a set of rules designed to allocate vehicles to transportation requests while accounting for these complex constraints.\n\nThis paper addresses these challenges by introducing a multi-agent deep reinforcement learning approach along with a layer-based communication channel to dynamically allocate vehicles to"}, {"title": "2 Related work", "content": "In this section, the state of the art in shop scheduling with intelligent transporters is first presented. Following this, the evolution and application of artificial intelligence and machine learning methods in these problems are reviewed. Finally, the current state of multi-agent systems applied to intelligent transporter scheduling in manufacturing shops is examined, with emphasis on the importance of communication between different agents and the integration of multi-agent systems with machine learning."}, {"title": "2.1 Integrated shop and transporters scheduling", "content": "Integrated shop and transporter scheduling is a comprehensive approach aimed at optimizing both the production schedule within a manufacturing shop and the logistics of material handling and transport. This approach considers the interdependencies between the resource assignment for production tasks to workstations and the transfer of jobs via vehicles such as AGV or AIV.\n\nIn the study [3], the authors addressed two-machine flow shop and open shop scheduling problems where workpieces are transported via a single vehicle. For each problem, they proposed a heuristic algorithm aimed at minimizing the makespan. Notably, the transporter's capacity is assumed to be sufficient to transport any number of jobs, thereby neglecting the potential capacity constraints of vehicles in real-world scenarios.\n\nMore recent studies have increased the complexity of interstage transportation by considering multiple vehicles available in the field. This added complexity aims to better reflect real-world scenarios where multiple vehicles are used to transport workpieces between machines. The study [4], tackled the multi-AGV flow shop scheduling problem using Q-learning to optimize makespan and minimize average job delays. However, this study does not consider the charging consumption of AGVs, which is a significant limitation in real-world scenarios. In [5], authors explored the impact of AGV charging constraints on the scheduling of flexible manufacturing units with multiple AGVs. A"}, {"title": "2.2 Artificial intelligence application in scheduling", "content": "In the past few decades, there has been substantial growth in artificial intelligence (AI) applications within manufacturing, largely attributed to the accessibility of data facilitated by the Internet of Things (IoT). Supervised learning and reinforcement learning are among the most used methods in scheduling problems [7]. Supervised learning relies on sample data, requiring data collection through real-world cases or simulations. A combination of supervised learning and linear programming was presented by [8]. A mixed-integer linear programming was initially employed to obtain the optimal solution, which served as the foundation for the supervised learning training. Subsequently, a random forest classifier was introduced to predict the priority of jobs. However, the study does not address the adaptability of the algorithm in dynamic environments with factors like machine breakdowns or stochastic processing times. The authors [9] employed multiple linear regression to predict the optimal scheduling rule at each decision step in a dynamic job shop with a single AIV transporter. Their proposed method outperformed heuristic approaches in minimizing the makespan. It is important to note that supervised learning methods require substantial data collection and are not well-suited for real-time decision-making processes.\n\nAmong the array of advancements in machine learning techniques, Deep Q-Networks stand out for their simplicity in implementation and their capacity to tackle complex problems by amalgamating deep learning with Q-learning [10]. Reinforcement learning (RL) is recognized as a broader framework encompassing decision-making tasks [11]. A key distinction between RL and other AI algorithms lies in the learning mechanism, where RL learns through interactions with a dynamic environment, contrasting with supervised and unsupervised learning methods that rely on sample data for their learning process.\n\nThe authors [12], employed a Deep Q Network (DQN) to improve adaptive scheduling in dynamic job shops. The DQN algorithm selects actions from a set of ten heuristic dispatching rules. Their method demonstrated superior performance compared to single dispatching rules and traditional Q-learning. However, it is important to note that this work focused on a single performance indicator-total job tardiness thereby overlooking the complexity of managing multiple objectives, which is more representative of real-world manufacturing environments.\n\nThe study [13], utilized Deep Q-network (DQN) for online scheduling of a job shop, aiming to optimize multiple objectives including makespan, production cost, and machine utilization. Their approach effectively handled dynamic system behaviors such as urgent orders and machine failures and outperformed common scheduling methods such as Shortest Processing Time (SPT) heuristic and genetic algorithm. However, this study overlooked a significant complexity of real-world production systems: the optimization of job transportation. Modern manufacturing sites are equipped with intelligent vehicles, introducing new challenges for the production system. In study [14], the"}, {"title": "2.3 Deep Q-Network", "content": "Deep Q-Network (DQN) agent learns through experimenting within an environment [15]. The base framework for such interaction is the Markov Decision Process which is represented by the tuple (S, A, R) where:\n\nS: Stands for the state space, $s_t \\in S$ represents the state at time t\n\nA: Stands for the action space, $a_t \\in A$ represents action taken at time t considering $s_t$\n\nR: stands for the reward function, $r_t \\in R(s_t, a_t)$ represents the reward received by the agent after taking action $a_t$ in the state $s_t$\n\nAt each time step t, the agent takes the action $a_t$ and transitions from state $s_t$ to state $s_{t+1}$. The agent receives an immediate reward $r_t = R(s_t, a_t, s_{t+1})$ upon reaching state $s_{t+1}$ which indicates the performance of the agent. The objective of an RL algorithm is to maximize the cumulative rewards [16]. To achieve this objective, a DQN algorithm aims to approximate an optimal Q function of a given environment. The optimal Q function $Q^*(s, a)$ represents the maximum expected cumulative reward that agent can obtain. Formally the Q function can be defined as :\n\n$Q(s, a) = E[\\sum_{t=0}^{\\infty} \\gamma^t r_{t+1} | s_0 = s, a_0 = a ]$ (1)\n\nWhere E denotes the expected value, $\\gamma$ is the discount factor which lies in range [0,1], determining the importance of future rewards. And $r_{t+1}$ is the expected reward at the next time step. The optimal Q function $Q^* (s, a)$ is formulated as:\n\n$Q^*(s, a) = \\max_{\\pi} Q^{\\pi} (s, a)$ (2)\n\nTo approximate $Q^* (s, a)$, DQN uses a deep neural network parametrized by $\\theta$, which is denoted by $Q^{\\theta} (s, a)$. The network takes the state $s_t$ as input and outputs the Q values for all possible actions"}, {"title": "2.4 Multi agent systems", "content": "The review study conducted by [18] investigated the use of multi-agent reinforcement learning in smart factories, suggesting that multi-agent systems, when combined with AI techniques are the most appropriate for such environments. Furthermore, [19] provides a general background on single-agent and multi-agent deep reinforcement learning, highlighting the challenges and advantages of multi-agent systems over single-agent ones, particularly in wireless communications.\n\nOne of the prominent challenges of Multi-Agent Deep Reinforcement Learning is the non-stationarity of the environment, which refers to the consistent variations in other agents' parameters. In the study [20], authors targeted communication efficiency in multi-UAV cooperative trajectory planning. They applied a Double Stream Attention Multi-Agent Actor-Critic (DSAAC) algorithm. The study needs to enhance the algorithm's adaptability to a broader range of dynamic scenarios and complex operational conditions.\n\nIn the study [21], a Multi-Agent Reinforcement Learning algorithm, specifically DQN, was employed to optimize operations. The agents within this system leverage Internet of Things (IoT) technology to facilitate inter-agent communication. The primary objectives of the study are to minimize the makespan and balance the total workload across the system. The results indicate that the distributed architecture inherent to the proposed approach effectively manages the high dimensionality characteristic of smart factory environments, outperforming traditional centralized methods.\n\nThe authors [22] introduced a novel decentralized multi-agent system approach incorporating capacity constraints for AGVs. The agents collaborate in a decentralized manner to assign product transport tasks to each other and determine their routes. The results demonstrate that te proposed approach yields competitive outcomes compared to the Mixed-Integer Linear Programming model, highlighting its potential as a promising decentralized approach. However, the study primarily focuses on capacity constraints related to the weight of products, rather than the effects of transporting multiple products with a single AGV. In the study [23], the authors address a job scheduling problem in a resource preemption environment using a policy-based Multi-Agent Reinforcement Learning approach. In this method, each job is considered an intelligent agent, utilizing a separate policy network. Additionally, a shared mixed Q-network is proposed to compute the global loss, fostering cooperation among the agents. Nevertheless, This study need to investigate the integration of diverse multi-agent scheduling algorithms to tackle a wider range of Job Shop Scheduling Problems (JSSPs) and to enhance model generalization through advanced representation learning techniques.\n\nIn summary, the literature review underscores the significant advancements and applications of AI in manufacturing scheduling, with a particular focus on the efficacy of multi-agent techniques"}, {"title": "3 Problem description", "content": "For illustrative purposes, a case study is presented of a manufacturing system consisting of four assembly workstations (WS1, WS2, WS3, and WS4), a storage location, two charging stations, and two AIVs. These vehicles are utilized to transport raw materials from storage to workstations and to facilitate the movement of subassemblies and products between workstations.\n\nFour types of products (P1, P2, P3, and P4) are considered, each requiring processing at specific set of workstations. Each product undergoing processing at a workstation is referred to as an operation (the term job is also used for a product, which consists of a set of operations). The manufacturing shop floor is structured as a flexible job shop, where each product follows its own unique routing through the workstations. Additionally, the flexibility of the job shop allows certain operations to be handled by any one of several possible workstations rather than a single designated workstation."}, {"title": "4 Multi-agent deep Q-Network (MADQN) approach with layar-based communication channel (LBCC)", "content": "In this section, a decentralized multi-agent Deep Q-Network (MADQN) approach is proposed to solve the illustrated problem described in section 3 involving AIV transporters. Furthermore, a layer-based communication channel (LBCC) is introduced and employed to deal with the non-stationarity of the multi-agent system. The primary objectives are to minimize total job tardiness, the number of tardy jobs, and AIVs energy consumption. This approach addresses two types of decisions: workstation-selection and AIV-selection. Each job is considered an agent equipped with two deep Q-networks, one for each decision type.\n\nAll available jobs interact with the workshop environment simultaneously, resulting in the formation of queues for both AIVs and workstations. It is noteworthy that the queues for each AIV and workstation are managed based on a First-In-FirstOut (FIFO) policy, wherein jobs that are requested earlier are given priority.\n\nFor each job agent, two Deep Q-networks (DQN) are developed: one for workstation-selection and the other for AIV-selection. Each DQN is designed based on a Partially Observable Markov Decision Process. Status of each job are illustrated in Fig. 2.\n\nThe observations for each DQN are provided as follows:\n\nWorkstation-selection DQN observation :\n\nQueue length of workstations\nDistance of workstations from the job\nWorkstations' busy time percentage"}, {"title": "4.1 DQN characteristics: Actions, Rewards, Neaural Network features", "content": "The action of the workstation-selection DQN is to select the appropriate workstation for each available job. Similarly, the action of the AIV-selection DQN is to select the appropriate AIV for transferring the job to the selected workstation. It is crucial to manage the variation in the action space dimension for the workstation-selection DQN at each step, which necessitates the use of a mask for the current operation. This mask delineates the feasible actions (workstations) available. For instance, if product/job P1 can only be processed on workstations WS2 or WS4 for its first operation among all m workstations, other workstations are masked out except WP2 and WP4. Consequently, the network's output is adjusted to -\u221e where no viable action exists (masked options). This adjustment to -\u221e is advantageous as it allows the objective function, which employs an argmax, to disregard the masked options.\n\nAt each step, two immediate rewards are allocated to the agent, one to the AIV-selection DQN and another to the workstation-selection DQN. The AIV-selection DQN receives a reward immediately upon completing the transfer of a product to the designated workstation. This reward is computed based on the energy consumption of the AIV, encompassing the period from when the AIV commences the job pick-up process until it delivers the product. This reward is assigned a negative value. The workstation-selection DQN obtains a reward immediately after completing the"}, {"title": "4.2 Layer Base Communication Channel (LBCC)", "content": "A key advantage of multi-agent systems lies in their capacity for inter-agent communication, which substantially enhances system performance. In the proposed approach, the uniform structure of all agents facilitates the creation of a communication channel across corresponding layers. This communication channel is integrated into the networks of the job agents' DQNs (workstation-selection and AIV-selection). Specifically, within each hidden layer of a job agent's network, the input to that hidden layer, originating from the output of the preceding layer, is combined with the output of the same hidden layer from all other job agents. This design ensures that each job agent is aware of the outputs of other agents at the same level of the hidden layer. Such an integrated communication mechanism promotes effective information sharing and coordination among agents, enhancing overall system efficiency. The mathematical representation of the communication channel is provided below:\n\n$h^{out,i}_l = tanh (W^{(l)}.(h^{in,i}_l \\cup H^{(l)}_{i} )+b^{(l)})$   i = 1, 2, ..., n; (7)\n\nWhere:\n\n$h^{out,i}_l$ is the output of l-th layer of the i-th job agent.\n\n$W^{(l)}$ is the weight matrix for the l-th layer.\n\n$h^{in,i}_l$ is the input to the l-th layer of the i-th agent.\n\n$H^{(l)}_{i} = \\cup_{j \\neq i} h^{out,j}_{l}$ represents the union of the outputs from the same hidden layer l of all other job agents j except i.\n\n$b^{(l)}$ is the bias vector for the l-th layer.\n\ntanh is the Tanh activation function applied to the linear combination of the inputs, weights, and biases.\n\nAs an example, Fig. 3 illustrates the communication mechanism within the workstation-selection networks of job agents at the third hidden layer. Specifically, for job $J_k$, the input to its third hidden layer is integrated with the outputs from the third hidden layer of all other jobs, ranging from $J_1$ to $J_n$ and $1 < k < n$."}, {"title": "5 Experimentation and results", "content": "The case study described in Section 3, which involves four products and four workstations, is used to experiment the MADQN approach. In this example, each product can have various numbers of job arrivals (each job represent one product). It is assumed that the number of jobs for all product types is equal. For instance, in the case of a total of 60 job arrivals, each of the four product types has 15 job arrivals. The time between the arrival of jobs is assumed to follow an exponential distribution with a rate parameter of $\\lambda$ = 5 time units.\n\nDealing with tardiness as two of the three objectives requires having due dates for each job. In this case study, a due date generator is used based on the work of [25] for each job. The due date"}, {"title": "5.1 Case study data", "content": "The case study described in Section 3, which involves four products and four workstations, is used to experiment the MADQN approach. In this example, each product can have various numbers of job arrivals (each job represent one product). It is assumed that the number of jobs for all product types is equal. For instance, in the case of a total of 60 job arrivals, each of the four product types has 15 job arrivals. The time between the arrival of jobs is assumed to follow an exponential distribution with a rate parameter of $\\lambda$ = 5 time units.\n\nDealing with tardiness as two of the three objectives requires having due dates for each job. In this case study, a due date generator is used based on the work of [25] for each job. The due date"}, {"title": "5.2 Results", "content": "The proposed case study is unique and novel due to the added complexities, constraints, and dynamic events. Since there is no existing solution in the literature to compare our approach with, nine heuristics are provided to evaluate the performance of the proposed MADQN method. These heuristics are generated by mixing three dispatching rules for workstation-selection and three rules for AIV-selection. The three dispatching heuristics for workstation-selection are as follows:\n\nSPT (Shortest Processing Time): Workstation with the Shortest Processing Time is selected for each product. It is worth noting that the processing time of each operation varies depending on the workstation performing the process.\n\nSQL (Shortest Queue Length) : Workstation with the shortest queue length has priority.\n\nSWLW (Shortest Workload): Workstation with the Shortest Workload or shortest percentage of busy time has priority.\n\nThe three dispatching rules for AIV-selection are:\n\nMC (Most Charge): Select the AIV with the most remaining charge.\n\nSTT (Shortest Transfer Time): Select the AIV with the Shortest Transfer Time (from its current location to the location of the first product to pick up).\n\nSWLA (Shortest Workload): Choose the AIV with the Shortest Workload or shortest percentage of busy time.\n\nThe effectiveness of the proposed approach is evaluated based on jobs scalability. Scalability is typically assessed by evaluating the method across different ranges of arriving products. As the number of products increases, the complexity of the scheduling problem also arises, making it more challenging to optimize the three objectives. A set of simulations with an increasing number of jobs (20, 40, 60, 80, and 100) is conducted. For each job count, a unique layout is considered, which is based on the transfer times between workstations and process times of operations on workstations. This layout remains consistent for each specific job count but varies between different job counts. The simulation is executed 100 times (each time with different values of processing times and time arrivals of products/jobs).\n\nThe objectives of the problems are set to the total tardiness of all products, the number of tardy products, and the total AIVs' charging consumption. For each objective, the mean results of all 100 simulations are collected for each job count. Based on Table 3, 4, and 5, where best results are highlighted in bold, the proposed MADQN method outperforms all nine heuristic combinations across all three objectives. Importantly, the results remain stable as the number of jobs increases, even when the job shop layouts change and different processing times are used. This stability demonstrates the scalability of the proposed approach."}, {"title": "DueDate", "content": "of job i is computed as shown in the following equation:\n\n$DueDate_i = ArrivalTime_i + T\\sum MeanProcessingTime_i, i = 1, 2, ..., n;$ (8)\n\nEssentially, the due date is equal to the arrival time of the job plus the total required processing time for that job. T is a coefficient to take into account the transfer time of jobs between workstations. This coefficient follows a normal distribution with parameters defined as follows :\n\n$\\mu = \\frac{Nb Jobs}{4}, \\sigma = 4$\n\nAs the problem is a flexible job shop, jobs have the flexibility to be processed on different workstations for their various operations. Therefore, the processing time for each operation of the job is determined by the mean of the processing times of that operation on the possible workstations. For example, if one operation can be processed on either workstation WS1 with a processing time of 8 or on workstation WS2 with a processing time of 4 minutes, MeanProcssingTime for that operation will be 6 minutes.\n\nTo enhance the dynamic behavior of the job shop, machine breakdowns or in this case workstation unavailibilities are also considered. An exponential distribution is applied for both the time interval between two unavailabilities (TBI) and the time required for fixing the problem (workstation to be available) (TRF) [26]. The parameters for TBI (time between two inspections) and TRF (time required for fixing) are set to $\\lambda_{TIB} = 200$ and $A_{TRF} = 50$.\n\nAnother important aspect is the layout of the job shop, which is defined by the distances between workstations, the raw material storage location, and the charging stations. To generate different layouts, a uniform random variable between 10 and 50 is used for each transfer time, reflecting various possible distances and assuming that both AIVs have the same speed.\n\nEach of the two AIVs is set with a loading capacity of two, and each job/product occupies one out of the two capacity units of the AIVs. In other words, AIVs are capable of transferring two products simultaneously. The transfer policy is that AIVs fill their capacity if there are enough requests; otherwise, they collect the available requests, carry all picked-up products to their destinations, and then become idle at the last destination, ready for the next transfer.\n\nThere are two charging stations each with a capacity of one AIV at the same time. AIVs are sent to the charging station when their battery level falls below 40%. The location of the charging stations can vary across different layout setups. The charging consumption of the AIVs is assumed to correlate with their status (moving or stopped) and the number of product they are carrying. The energy consumption percentage pattern of AIVs is provided in Table 2."}, {"title": "5.3 Discussion", "content": "The proposed approach effectively addresses several key challenges in real-world production systems, particularly in flexible job shop scheduling where multiple routing options and dynamic events, such as machine breakdowns, stochastic job arrivals, and variable processing times, complicate decision-making. The integration of Autonomous Intelligent Vehicles (AIVs) for material handling, while enhancing efficiency, introduces constraints like energy management and battery recharging. Notably, our study incorporates the often-overlooked capability of AIVs to handle multiple jobs simultaneously, which significantly improves both energy efficiency and job tardiness.\n\nOur application of reinforcement learning, specifically the Multi-Agent Deep Q-Network (MADQN), demonstrates the method's suitability for real-time scheduling, offering a rapid response to dynamic changes. While it may not match the precision of exact optimization methods, it provides a favorable balance between speed and accuracy, particularly in large-scale problems."}, {"title": "6 Conclusion", "content": "This study presents a solution for scheduling autonomous internal logistic vehicles in smart manufacturing environments using multi-agent deep Q-network (MADQN) with a layer-based communication channel (LBCC). The proposed approach effectively addresses three key objectives: minimizing the total tardiness of jobs, reducing the number of tardy jobs, and decreasing the total energy consumption of vehicles. Our comprehensive analysis, comparing the method against nine well-known heuristics, highlights the superior performance and adaptability of the MADQN with LBCC.\n\nThe method's compatibility with flexible job shops exhibiting dynamic behaviors, such as dynamic job arrivals and workstation unavailabilities, underscores its practical applicability. Furthermore, the proposed approach ensures consistent results across various job shop layouts and larger problem instances, making it an adaptable tool for real-world applications. For practical applications, it is recommended to integrate the proposed method into fleet management software. This integration allows the algorithm to offer decision recommendations each time a new order is received. These recommendations can be reviewed by experts to finalize vehicle and machine selection decisions, thereby improving both the efficiency and reliability of the decision-making process.\n\nDespite the promising results of this study, several avenues for future research are evident. Future work should explore a broader range of reinforcement learning methods, including policy-based approaches, to enhance the robustness of the proposed solution. Additionally, investigating alternative communication methods for multi-agent systems, such as attention mechanisms, could provide new insights and improve performance. The study focused on AIVs with a capacity of two jobs, but exploring higher-capacity AIVs could address new challenges in optimizing loading and unloading strategies. Different recharging policeies for AIVs should also be evaluated to assess their impact on production efficiency. Lastly, testing the proposed approach across various shop configurations, defined by different numbers of job types, machines, and AIVs, could validate its generalizability and effectiveness in diverse settings."}]}