{"title": "HashAttention: Semantic Sparsity for Faster Inference", "authors": ["Aditya Desai", "Shuo Yang", "Alejandro Cuadron", "Ana Klimovi\u0107", "Matei Zaharia", "Joseph E. Gonzalez", "Ion Stoica"], "abstract": "Utilizing longer contexts is increasingly essential to power better Al systems. However, the cost of attending to long contexts is high due to the involved softmax computation. While the scaled dot-product attention (SDPA) exhibits token sparsity, with only a few pivotal tokens significantly contributing to attention, leveraging this sparsity effectively remains an open challenge. Previous methods either suffer from model degradation or require considerable additional resources. We propose HashAttention a principled approach casting pivotal token identification as a recommendation problem. Given a query, HashAttention encodes keys and queries in Hamming space capturing the required semantic similarity using learned mapping functions. HashAttention efficiently identifies pivotal tokens for a given query in this Hamming space using bitwise operations, and only these pivotal tokens are used for attention computation, significantly improving overall attention efficiency. HashAttention can reduce the number of tokens used by a factor of 1/32\u00d7 for the Llama-3.1-8B model with LongBench, keeping average quality loss within 0.6 points, while using only 32 bits per token auxiliary memory. At 32\u00d7 sparsity, HashAttention is 3-6\u00d7 faster than LightLLM and 2.5-4.5\u00d7 faster than gpt-fast on Nvidia-L4 GPU.", "sections": [{"title": "1. Introduction", "content": "The ability to refer to long contexts efficiently is crucial for modern AI applications, from processing lengthy documents to engaging in extended conversations (Touvron et al., 2023; Achiam et al., 2023; Liu et al., 2024b). It is commonplace for LLM models to preprocess and store huge amounts of text in the form of KV Cache, which is later used to process various prompts. The Scaled Dot Product Attention (SDPA), fundamental to the transformer architecture that has driven the Generative AI revolution (Vaswani, 2017; Brown et al., 2020), does not scale well with context length. Processing small prompts or generating a single token requires SDPA to access the entire context KV Cache, which can be hundreds of GB in size. For instance, a KV Cache of 256K tokens is 128GB in size (0.5 MB per token). Enabling sparsity in attention, where only a subset of tokens is used in each attention step, can significantly reduce the computational and memory burden in LLMs.\nSparsity naturally arises in SDPA. Due to the softmax kernel, only a few tokens significantly contribute to the final attention computation(Bricken & Pehlevan, 2021; Deng et al., 2024). Efficiently identifying these pivotal tokens provides a pathway to achieving efficient attention. Various approaches to identifying pivotal tokens have been explored in the literature. Heuristic-based methods, such as fixed sparsity patterns(Xiao et al., 2023), ignore the dynamic nature of contextual sparsity in attention, resulting in suboptimal attention quality. KV cache discard strategies, such as those proposed in (Liu et al., 2024c; Zhang et al., 2023; Li et al., 2024a), identify the global importance of tokens per attention head, incorporating some degree of dynamic sparsity. These methods discard tokens based on their observed importance in context prefixes. However, token importance is dynamic, and tokens deemed unimportant in prefixes can become critical for future inference, leading these methods to fail in certain scenarios (Xiao et al., 2024; Tang et al., 2024). Some approaches to truly dynamic sparsity have emerged(Xiao et al., 2024; Yang et al., 2024; Tang et al., 2024). However, since these methods rely on heuristics to reduce the computation of pivotal tokens, they often exhibit low recall.\nIn this paper, we take a principled approach to identifying pivotal tokens. Given a query and a large set of key-value pairs, we frame the task of retrieving important tokens as a recommendation problem(Zhang et al., 2021). This approach allows us to leverage the extensive literature on information retrieval (IR)(Sch\u00fctze et al., 2008) to devise an efficient and GPU-friendly solution. One of the key approaches in information retrieval is to learn an embedding space where the semantic similarity between an item (e.g., an attention key-value pair) and a query (e.g., an attention query) is reflected in the distance between their embeddings. HashAttention embeds the key-value pairs and queries in Hamming space using learned functions. Given a query, the nearest key-value embeddings are used to identify pivotal tokens. The binary embeddings for the key-value cache can be packed into integers and stored efficiently using minimal memory. Furthermore, HashAttention identifies pivotal tokens using bit-wise operations, making the retrieval process fast.\nWe demonstrate that HashAttention outperforms all relevant baselines in terms of quality per unit token budget. HashAttention can significantly reduce the number of tokens required to match the quality of the full model.  HashAttention can almost match the full model quality with 32\u00d7 compression. To achieve similar quality competing methods can only afford a 8x compression. All methods for sparse attention, including HashAttention, differ only in their algorithms for computing pivotal tokens, while sharing the same underlying routine for computing sparse attention with the selected tokens. This routine is the bottleneck for overall latency. As a result, achieving superior recall enables models to run with fewer tokens, significantly improving overall efficiency. At 32\u00d7 compression, HashAttention is upto 6\u00d7 faster than LightLLM(LightLLM, 2024) and upto 4.5\u00d7 faster than Gpt-Fast(GPTFast, 2024).\nFurthermore, the additional costs of HashAttention are much lower than those of its competing approaches. First, memory utilization during LLM inference is a significant bottleneck, particularly due to the large sizes of KV caches,"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Long Context and Retrieval-Augmented Generation", "content": "Recently, there has been rising interest in developing models capable of processing long contexts(Beltagy et al., 2020; Guo et al., 2022; Han et al., 2023; An et al., 2024; Ma et al., 2024), driven by emerging applications such as multi-document question answering, multi-turn chatbots, and tutor bots(Bai et al., 2024a; Feng et al., 2022; Davies & Murff, 2024). Two main approaches to addressing this challenge are: (1) designing models that natively support long contexts using methods like ROPE scaling(Su et al., 2023), and (2) employing a retriever to extract relevant portions of text to augment the prompt, a technique known as Retrieval-Augmented Generation (RAG) (Lewis et al., 2020).\nA recent study highlights the advantages of building long-context LLMs over RAG approaches(Li et al., 2024b). Notable progress in this area includes the emergence of long-context models, such as the open-source Llama 3.1(Dubey et al., 2024) series, which supports a context length of 128K tokens, and proprietary models like GPT-4 Turbo(Achiam et al., 2023), Claude-2, and Gemini 1.5(Team et al., 2023), offering context lengths of up to 128K, 200K, and 1M tokens, respectively."}, {"title": "2.2. Post-training Sparse Attention", "content": "Various approaches exist to sparsify the attention computation of pretrained models. These can be classified as follows:\nFixed Sparsity: Approaches such as StreamingLLM (Xiao et al., 2023) adopt a fixed sparsity pattern to approximate attention based on the observed importance of attention sinks (e.g., the first few tokens) and local window tokens. However, subsequent studies (Zhang et al., 2023; Xiao et al., 2024) have convincingly demonstrated the dynamic nature of sparsity and the resulting quality degradation when using fixed sparsity.\nKV Cache Discarding: To manage long contexts, methods such as H2O(Zhang et al., 2023), ScissorHands(Liu et al., 2024c), FastGen(Ge et al., 2023), and SnapKV(Li et al., 2024a) discard tokens based on specific heuristics. However, once discarded, these tokens are no longer available for future generations. This limitation is particularly problematic in scenarios such as multi-turn chat or multiple QA sessions on a fixed document, where it is essential to access different parts of the document in each turn or question.\nEstimating top-k attention scores via partial computation Attention scores are one of the critical components that determine the contribution of a token to the output of the attention mechanism. Identifying the top-k tokens with the highest attention scores requires O(nd) computation, as it involves the dot product of the query with each token in the KV cache, where n n is the number of tokens in the KV cache and d is the dimensionality.\nDouble Sparsity(Yang et al., 2024) reduces this computational cost by selecting fewer channels to estimate dot products, which are then used to identify the top-k tokens. The channels are chosen based on offline calibration of channel norms. InfLLM(Xiao et al., 2024) and Quest(Tang et al., 2024) reduce computation along the n-dimension by creating page-level representations. These methods include or exclude all tokens on a page at once. While these approaches are effective, they often fail to provide high recall for the top-k tokens with respect to attention scores. Additionally, it is important to note that the absolute importance of a token depends not only on its attention score but also on the norm of its value vector, a factor completely ignored by these methods. (see lemma 4.1).\nRetrieval Algorithms for Top-k Recently, RetrievalAttention(Liu et al., 2024a) proposed using a graph-based nearest neighbor algorithm to identify the top-k tokens with the maximum inner product. RetrievalAttention offloads the top-k computation to the CPU due to the sparse computations involved with graphs, leading to additional latency. SqueezeAttention(Hooper et al., 2024), a concurrent work to ours, proposed solving the top-k problem efficiently by clustering keys. Both the methods ignore the contribution of value vectors in determining top-k and it is non-trivial to include this information."}, {"title": "2.3. Efficient Attention from scratch", "content": "Another line of work aimed at improving the efficiency of attention focuses on designing mechanisms to avoid quadratic computation. Notable efforts include linear attention methods based on Random Fourier features, such as Performerc(Choromanski et al., 2020), RFA(Peng et al., 2021), and LARA(Zheng et al., 2022). These approaches approximate the softmax, in expectation, using random features to compute attention weights. Linformer(Wang et al., 2020) reduces attention complexity by using low-rank approximations, and sketching long-context token embeddings along the sequence dimension. This method is inspired by the Johnson-Lindenstrauss Lemma(Johnson et al., 1986) for dimensionality reduction. Reformer(Kitaev et al., 2020) combines locality-sensitive hashing (LSH) tables with attention to reduce complexity through space partitioning. While these approaches have garnered significant interest in the research community, empirical evaluations have shown that Scaled Dot Product Attention (SDPA) demonstrates the most favorable scaling performance \u2013 i.e., as model sizes and training compute increase, the capabilities increase commensurately."}, {"title": "3. Background", "content": ""}, {"title": "3.1. Scaled Dot Product Attention (SDPA)", "content": "The Scaled Dot Product Attention has remained the most promising attention and has stood the test of time. The computation of SDPA for key and value embeddings, K, V : n\u2081 \u00d7 d, and Q:n2 \u00d7 d can be written as,\nSDPA (K, V, Q) = softmax (QK/\u221ad )V\nIf n2 = 1, then it can be simplified into,\nSDPA (K, V, q) = \u2211(a[i])\nwhere ai = are called attention scores."}, {"title": "3.2. Recommendation", "content": "The problem in recommendation can be abstracted as follows. Given a set of items I and a user u \u2208 U, the recommendation aims to select a small subset of I that is relevant to u. The historical relevance of items for users is captured in an interaction matrix, which is |U|\u00d7 |I| matrix. Traditionally, the learning recommendation model has been cast as a matrix factorization of the interaction matrix. The two matrices, thus obtained, provide us with embeddings of items and users that can be used for inference. Subsequently, auxiliary information such as description, profile, interaction history, etc., and deep learning models are used to obtain richer user and item representations. In this embedding space, given a user embedding, items that have the highest inner product are the ones most relevant to items. Alternatively, one can learn the embeddings such that relevant items lie close to user embeddings in terms of lp distance. To improve the efficiency of relevant item retrieval, we often use approximate near-neighbor algorithms and data structures built on the top of the embedding space.\nSparsifying attention is a recommendation problem with key-value pairs as items and queries as a user. We want to select key-value pairs that minimize the error of output embedding w.r.t full attention for a particular query."}, {"title": "4. HashAttention", "content": ""}, {"title": "4.1. General Setting of Sparse Attention", "content": "The general recipe of sparse attention can be viewed as a combination of two subroutines (1) SPARSIFY (2) SPARSE-ATT\nSPARSIFY (K, V, q, k): Given a query q and a set of tokens K, V, SPARSIFY returns k tokens with highest estimated importance from K.\nSPARSE-ATT (K, V, q, I): Given a set of keys K, associated values V, a query q and set of indices I identifying tokens to be used, SPARSE-ATT computes the attention only using tokens indicated by I as follows,\nSPARSE-ATT (K, V, q, I) = \u22111(i\u2208Z) exp ((q,ki))vi\nwhere vi, ki refer to ith elements of V and K respectively, and 1 is the indicator function.\nThe SPARSIFY function can be further decomposed into two steps SCORE and TOPK . SCORE (K, V, q) assigns a query-aware score to each token in K. TOPK picks the top tokens with the highest scores assigned by SCORE. In the text, we will overload the function SCORE (k, v, q) to denote the score assigned to a single key for methods that work on each key individually.\nHashAttention along with various other previous methods fit this structure. The ingenuity of different methods lies in the SCORE function. The ordering established by SCORE needs to represent the importance of keys accurately. The quality of SCORE can be measured in terms of its recall(n, k) defined as follows. Let In be the set of top n indices chosen by SCORE function and Itrue,k be the set of k tokens with the highest true attention scores. Then the recall is\nrecall(In, Itrue,k) = ."}, {"title": "4.2. SCORE function for HashAttention", "content": "Identifying the top tokens for a query is analogous to user-item recommendation problem \u2013 which has been extensively studied in Information Retrieval. HashAttention uses two learnable mapping functions, \u03a6\u03ba\u03c5, \u03c6q : Rd \u2192 [0,1] to lift key-value and queries from Rd to hamming space of dimension b. In this space, we can efficiently compare a given query to the keys using hamming distance H.The SCORE function is defined by\nSCORE (k, v, q) = \u2212H(\u03c6\u03ba\u03c5(k, v), $q(q))\nFor \u03c6\u03ba\u03c5 and \u03c6q, we use independent feed-forward network F, followed by a sign function to extract bits. The general function & can be written as,\n(x) = relu(sign(F(x)))\nThe bits are packed into an integer. We denote complete mapping function by int\nImplementation: The key signatures are computed and cached along with KV Cache. We store them in integer format. During the decoder hot path, we compute the query signature and perform bit operations to compute the hamming distance between queries and keys.\nH(\u03c6\u03ba\u03c5 (k, v), \u03a6q(q))\n= bitcount(bitwise_xor(int \u201akv (k, v), $int,q(q)))\nIt should be noted that we can construct index on the obtained bit signatures for faster computation of SCORE + TOPK. We leave this for future exploration.\nIn our experiments, we focus on ku that acts only on the key vector k and ignore the v vector or its features such as norm. As we will see in the next subsection, the importance of the token depends on the norm of its value vector v. Incorporating v in ku is easy and is left for future exploration."}, {"title": "4.3. HashAttention module for adapting to pre-trained LLMS", "content": "To use HashAttention for existing SDPA attention in pretrained models, SCORE function of HashAttention should align with the best selection of tokens w.r.t the final attention computation. The best ordering for an SDPA attention is presented in the lemma below.\nLemma 4.1. Consider n KV embeddings K, V and query q. Let a be the attention scores of tokens w.r.t query q. Then the contribution of a token i towards the final output is proportional to\nAiVi"}, {"title": "5. Experiments", "content": "In this section, we evaluate HashAttention when adapted to existing LLM models. The section is organized as follows: first, we demonstrate the superiority of HashAttention in a head-to-head comparison against popular baselines. Next, we select the strongest baselines for a Pareto comparison (quality vs. token budget) of HashAttention and micro-benchmarking. Finally, we evaluate the efficiency of the HashAttention attention kernel.\nBaselines: We use the following baselines: StreamingLLM (Xiao et al., 2023), H2O (Zhang et al., 2023), InfLLM (Xiao et al., 2024), DoubleSparsity"}, {"title": "5.1. A head-to-head comparison of HashAttention against sparse-attention baselines", "content": "Models: We use the latest Llama-3.1-8B model, which is trained for a 128K context length. It consists of 32 decoder layers, each containing 32 attention heads. All 1024 attention heads are replaced with sparsifying modules."}, {"title": "5.2. Comparison of HashAttention across different token budgets", "content": "Settings: We use the Llama-3.1-8B model for Pareto curve computation and select a range of datasets from Long-Bench. From Table 1, it is evident that Quest and DS are two strong baselines. In this Pareto curve analysis, we compare HashAttention against these baselines. For all methods, the auxiliary budget is set to 32 bits per token.\nWe make the following observations:\n\u2022 HashAttention outperforms other methods across various token budgets.\n\u2022 The quality gap at lower token budgets is particularly pronounced, with HashAttention demonstrating a significant advantage.\n\u2022 The quality achieved by the baselines at a given token budget can be matched by HashAttention at a much smaller token budget. This implies that the complexity of final sparse attention can be significantly reduced for HashAttention compared to other methods. As we will see, sparse attention is the major bottleneck in terms of latency, and reducing its complexity will result in substantial latency gains."}, {"title": "5.3. Recall of HashAttention", "content": "From the pareto curves, it is clear that Double Sparsity is the strongest baseline. We micro-benchmark Double Sparsity and HashAttention in their quality of retrieving top tokens at different context length.  We note that the superior quality of HashAttention on benchmarks is a directly related to its superior retrieval quality."}, {"title": "5.4. Efficiency of HashAttention", "content": "As mentioned in the section 4, the ingenuity of different sparse attention methods lies in devising the SCORE function that lets us choose the sparsity pattern and perform reduced attention. The rest of implementation can be ported across different methods. In this section to evaluate latency of HashAttention, we use the sparse-fwd kernel from Double Sparsity and use pytorch based scoring function for HashAttention which is compiled using torch.compile.  shows the relative latency of HashAttention compared to LightLLM(LightLLM, 2024) and FlashAttention from torch.scaled_dot_product_attention used in gpt-fast(GPTFast, 2024).  shows the split of time in different components of a sparse attention. We make the following observations,\n\u2022 The overall time of HashAttention is quite similar to that of Double Sparsity at the same sparsity. We observe 3 6\u00d7 gains over LightLLM attention and 2.5-4.5x over flash attention from gpt-fast at a sparsity of 32x.\n\u2022 The time overhead of computing SCORE is minimal as compared to the time spent in computing the actual sparse attention and performing TOPK .\n\u2022 As the number of tokens in sparse-fwd decreases (sparsity increases) the attention latency almost decreases linearly. This implies that there are direct gains from improving the recall of SCORE which lets us work with lesser number of tokens."}, {"title": "5.5. Discussion on latency", "content": "The sparse-fwd kernel from Double Sparsity does not implement sequence parallelism, as is the case with LightLLM attention and FlashAttention in gpt-fast. We are working on an implementation with sequence parallelism. With sequence parallelism, the sparse-fwd times may reduce making SCORE + TOPK the bottleneck."}, {"title": "6. Discussion", "content": "Our exposition of HashAttention is limited in several ways and requires further exploration for complete insights. Firstly, in the current experimental setup, while training the HashAttention modules, we targeted top-k tokens defined purely by attention scores and used only key vectors as input to the learned mapping functions. This was primarily done to compare all baselines with respect to their scoring recipes. We plan to continue exploring the inclusion of value vectors both as inputs to the mapping functions and as part of the target, which is easily achievable in HashAttention.\nAdditionally, further ablation studies are needed to explore various aspects of training HashAttention, including, but not limited to, training loss functions, targets for training, and so on. Currently, we retrieve a fixed number of tokens for all attention heads across all runs of the model. However, it may be valuable to retrieve different counts of tokens depending on the query. We plan to explore advanced retrieval mechanisms with HashAttention.\nThis paper focuses on adapting HashAttention to existing pretrained attention modules. However, if the existing pre-trained full attention is inherently sparse and well approximated by HashAttention, it stands to reason that we can build an HashAttention that can be trained end-to-end to organically develop dynamic sparse attention as an integral part of the model, rather than as an afterthought. We plan to invest in developing end-to-end HashAttention next."}, {"title": "7. Conclusion", "content": "HashAttention proposes a principled approach to sparsify attention based on hashing key-value pairs and queries to a compressed semantic space. Near neighbour comparisons can be efficiently performed in this semantic space using bit operations. The quality of tokens retreived by HashAttention, under similar resources, is significantly superior to other sparsity baselines, leading to upto 4\u00d7 further reduction of tokens as compared to leading sparse attention baselines. This leads to 4\u00d7 gains in efficiency over other baselines."}, {"title": "A. Design Choices", "content": "In this section, we discuss some of the design choices for our method."}, {"title": "A.1. Retrieval Algorithm", "content": "One way to look at HashAttention is that, given a query it divides the set of keys into subsets according to levels of quantized similarities. For instance, if we use 32 bit signatures, then we only have access to 32 levels of similarity. Under such a similarity assignment, what is the best way to choose retrieved set of keys? There are two natural approaches\n1. depth-based-retrieval(d): We find the maximum bit match score, say dmax, and retrieve all the keys with scores > Ad = dmax - d\ndmax = max{bit-match(q, k)}\nkEK\nAd = dmax - d\n2. num-based-retrieval(n): We find the bit match score of the nth (max) key, say An. Then all the keys with scores > \u03bb\u03b7 are chosen.\n3. combined(n,d): we use the maximum of the two scores to avoid retrieving unnecessary excess tokens.\nAd,n = max(Ad, \u03bb\u03b7)"}, {"title": "A.2. Note on #retrieved tokens, scaling, variability and such", "content": "An important question in designing the retrieval is whether, we should always output a fixed number of tokens or have some variability of tokens. Semantics dictate that depending on the query, the number of tokens important for attention mechanism can be different, and thus, we allow the design to retrieve variable number of tokens for every query.\nWe observe that using only depth-based-retrieval leads to linear scaling of count of retrieved tokens. (i.e. number of retrieved tokens increases linearly with context size). Using only number based retrieval causes unnecessary retrieval in some cases when only a few tokens have high bit match score \u2013 especially true in small context lengths. Thus we use combination of both as our retrieval algorithm. We see a sublinear scale up with the combined recipe."}, {"title": "B. Evaluation Philosophy", "content": "How do we evaluate sparse attention? Two schools of thought exist depending on what we are trying to achieve. For end to end trained models, ofcourse sparse attention needs to be applied at all times as defacto thing to do. On the other hand, when trying to speed up decoding for existing LLM models, it maybe considered that prefilling is done offline and thus sparse attention only needs to be applied to decoding phase. However, most baselines which follow the second line of thought for evaluation often disregard that we might need to interact with this long term context differently \u2013 i.e. we might want to ask different questions on the same document. In this case, it would be ideal to apply full attention on long term context (e.g. document) but apply sparse attention to local prompt (e.g. question) and following decoding. This is especially important in evaluation using datasets. For instance, consider passage retrieval task. In this case, you only need to predict the paragraph number. So if you apply full attention to the entire prompt, the comparison of question and retrieval from context has already happened with full attention and thus it does not capture the quality of attention.\nTo remedy the situation agnostically, we offset the sparse attention application \u2013 meaning we start applying sparse attention to the end of the prompt before entering decode phase. In our experiments we use a offset of 128 tokens."}, {"title": "C. Baselines (Adapted-HashAttention)", "content": "The primary purpose of HashAttention is to sparsify attention to improve the memory loading and computation of attention computation. Thus we consider approaches in literature with similar goals."}, {"title": "C.1. StreamingLLM", "content": "This attention was developed for processing streams of text. There are two different challenges when it comes to ingesting stream of text with existing LLM models. Firstly, most LLM models are trained on restricted context length and thus do not support longer texts. Secondly, the computational cost of decoding grows linearly with context length leading to prohibitive computation. StreamingLLM proposes to keep two portions of the context \u2013 the first (also called attention sink and the local context. Thus it sparsifies the attention computation.\nSince StreamingLLM categorically ignores a major chunk of context. It does not perform well on long context benchmarks which needs the model to consider all the context.\nHyperparameters: sink size and local size."}, {"title": "C.2. ScissorHands / H2O", "content": "This attention was developed primarily to reduce the memory storage of KV Cache \u2013 goal very well aligned with sparsification of attention and reducing memory loading during decoding. However, the setting used in ScissorHands / H2O is more restrictive since, the decisions are made in a streaming fashion and tokens evicted can never be retrieved unless recomputed. The idea in ScissorHands / H2O is that if some tokens in context have not been useful in recent predictions then they are not going to be useful in the future generations and can be dropped. In our experiments we use H2O since they have easier codebase.\nScissorhands and H2O both heuristically drop the tokens. The tokens dropped at one point are not available subsequently. This is clearly an issue in different settings such multi-turn chat etc. It should be noted that the proposal of ScissorHands and H2O are for reducing decoding time monologue of LLM. In that particular setting the proposals are useful. But their effectiveness is also restricted to that setting.\nHyperparameters: sink size, local size and token budget."}, {"title": "C.3. Retrieval Attention", "content": "In Retrieval Attention, the attention computation is preceded by top-k computation using approximate near neighbor search algorithms and full attention is computed on estimated top-k. Most graph based algorithms (including the one proposed in Retrieval Attention) need to be run on CPUs due to their irregular computation pattern. Thus, Retrieval Attention by default always stores the KVCache on CPU.\nThis is a close cousin of HashAttention. The motivation of both methods is identical in the sense that attention can be replaced by approximate near neighbour search. RetrievalAttention uses traditional graph based search algorithm to find near nieghbours, whereas HashAttention uses learning to hash to create a quantized similarity space for retrieval. A major drawback of RetrievalAttention is that it is not GPU friendly which causes indexing and querying to be slower for large contexts.\nHyperparameters: sink size, local size and ROAR graph hyper parameters."}, {"title": "C.4. InfLLM", "content": "InfLLM maintains the attention sink and local context as with streaming LLM. Additionally, it also maintains the tokens in between and retrieve chunks from them. It divides the KVCache into contiguous chunks and from each chunk a few representative keys. These keys are used to compute the importance of the chunks w.r.t a given query. Top few chunks are chosen for final attention computation via full computation. In order to choose top scoring chunks, the paper proposes performing full dot product computation can be performed with representative vectors which can also be replaced by off-the-shelf near neighbour computation on CPUs.\nThis again is similar to the setup of HashAttention. The chunk based treatment for retrieval reduces the computational cost of computing the relevant chunks. However, the heuristic way of computing the representative keys can lead to issues of missing key chunks while retrieving. Apart from that, the method is identical to RetrievalAttention.\nHyperparameters: sink size, local size, token budget, page size, number of representative vectors"}, {"title": "C.5. Quest", "content": "Quest is similar to InfLLM with the difference being the computation of the importance of a chunk. Quest maintains a max and min vectors for each chunk which maintains co-ordinate wise extremes of the chunk. These are used to estimate the maximum possible inner product within a chunk given a query.\nIt is clear to see that if we restrict the token budget, we might end up retrieving falsely important chunks at the cost of discarding important ones. We see this in our experiments as well.\nHyperparameters: sink size, local size, token budget, page size"}, {"title": "C.6. Double Sparsity", "content": "Double sparsity chooses 16 coordinates from the 128 dimensions of key embeddings via offline calibration and use them for computing top-k. Then full attention is computed on these top-k. This 16 dimensional slice of K Cache is called as label cache and can be further quantized to 4 bits without much impact on accuracy.\nThis again is similar to the setup of HashAttention and RetrievalAttention \u2013 the difference being how to compute the top-k. Surprisingly the 16 channels (16x16 = 256 bits) identified by Double Sparsity are good top k indicators. In comparison HashAttention uses 32 bit signatures and uses bit operations for manipulating signatures.\nHyperparameters: sink size, local size, token budget, label size.\nQuality and computation of sparsity with HashAttention vs. baselines\nThe choice of completely ignoring parts of context in streamingLLM, heuristic based permanent eviction of Scissorhands/H2O, and heuristic based representative key selection of InfLLM causes these approaches to be inferior to HashAttention. Retrieval Attention, Double Sparsity and HashAttention all are based on determining the top-k and using it for attention computation. Thus, the quality depends on ANN algorithm used. In terms of computational complexity, HashAttention and Double sparsity can be run on GPU and thus are faster for reasonably sized context lengths as compared to Retrieval Attention. Additionally, HashAttention only uses an integer signature for computation of top-k which is memory and compute effective as compared to Double sparsity"}, {"title": "D. Best Sparse solution for adaptation", "content": "Using the following notations, V : n \u00d7 d value matrix, a : n \u00d7 1 attention scores. S:n \u00d7 n diagonal sampling matrix. Then under the sampling the final embedding is,\na SV\nThe residual is . Then the best sampling is the one that minimizes the\nS = argmin\n\n= \n= a (I \u2013 S)VVT (I \u2013 S)a\n= \u2211(1 \u2013 1si)a||Vi,:||2\nTo minimize the residual, we have to choose, 1 si to be 1 which have higher ai||Vi"}]}