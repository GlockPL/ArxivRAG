{"title": "ME-CPT: Multi-Task Enhanced Cross-Temporal Point Transformer for Urban 3D Change Detection", "authors": ["Luqi Zhang", "Haiping Wang", "Chong Liu", "Zhen Dong", "Bisheng Yang"], "abstract": "The point clouds collected by the Airborne Laser Scanning (ALS) system provide accurate 3D information of urban land covers. By utilizing multi-temporal ALS point clouds, semantic changes in urban area can be captured, demonstrating significant potential in urban planning, emergency management, and infrastructure maintenance. Existing 3D change detection methods struggle to efficiently extract multi-class semantic information and change features, still facing the following challenges: (1) the difficulty of accurately modeling cross-temporal point clouds spatial relationships for effective change feature extraction; (2) class imbalance of change samples which hinders distinguishability of semantic features; (3) the lack of real-world datasets for 3D semantic change detection. To resolve these challenges, we propose the Multi-task Enhanced Cross-temporal Point Transformer (ME-CPT) network. ME-CPT establishes spatiotemporal correspondences between point cloud across different epochs and employs attention mechanisms to jointly extract semantic change features, facilitating information exchange and change comparison. Additionally, we incorporate a semantic segmentation task and through the multi-task training strategy, further enhance the distinguishability of semantic features, reducing the impact of class imbalance in change types. Moreover, we release a 22.5 km\u00b2 3D semantic change detection dataset, offering diverse scenes for comprehensive evaluation. Experiments on multiple datasets show that the proposed MT-CPT achieves superior performance compared to existing state-of-the-art methods. The source code and dataset will be released upon acceptance at https://github.com/zhangluqi0209/ME-CPT.", "sections": [{"title": "I. INTRODUCTION", "content": "Remote sensing data has been widely used to observe changes occurring on the Earth's surface over time, with applications spanning environmental monitoring, agricultural surveys, disaster assessment, and map updates [1]. With rapid urbanization, traditional 2D Change Detection (CD) methods are no longer sufficient to meet growing demands. There is an urgent need for automatic and accurate 3D CD methods to support urban planning, emergency response, and infrastructure development. With the development and increasing diversity of 3D data acquisition technologies, such as laser scanning and oblique photogrammetry, point clouds have become a crucial data source [2]. Compared to traditional remote sensing images, the 3D point cloud offers more intricate geometric details and precise spatial structures, unaffected by challenges such as varying lighting conditions and occlusions [3]. Using 3D point clouds enables more comprehensive outcomes, supporting high resolution geometric and attribute for CD in 3D space [4].\nIn recent years, 3D point cloud-based methods for CD have been widely studied and applied in various domains [5]. Point cloud data excels at capturing subtle changes in urban terrain, volume, and structures across various types of land cover, enabling applications such as digital twins and urban renewal. Existing 3D CD methods can be broadly classified into two categories: hand-crafted-based methods and deep learning-based methods.\nHand-crafted-based methods are traditional approaches that utilize manually designed features and predefined rules to detect changes [6]. These methods extract features such as geometric properties, texture characteristics, or statistical attributes from 3D data and compare them to identify areas or objects where changes have occurred. However, the diverse types of land cover and complex dynamic changes in urban environments pose significant challenges to features design. The reliance on empirical thresholds makes them susceptible to false positives or misses. Furthermore, these approaches typically focus on change types of specific objects, limiting their ability to detect diverse semantic changes [7].\nDeep learning-based methods utilize deep neural networks to automatically extract features and patterns from 3D point clouds to detect changes between different epochs. These approaches leverage the capabilities of the deep learning network to extract hierarchical representations directly from raw 3D data, eliminating the reliance on manual feature engineering. Despite their advantages, these methods face three significant challenges:\n1) The limited change features are attributed to the challenge of comprehensively modeling spatial correspondences between multi-temporal point clouds. The extraction of changes in existing work [8, 9] is typically performed by matching points with their nearest or k-nearest neighbors between different epochs and calculating the change feature by subtraction or concatenation. However, the complexity of the spatial distribution and correspondences of cross-temporal point clouds are often overlooked, limiting the model's ability to extract more comprehensive change features.\n2) The imbalance of change samples leads to insufficient semantic features representation for multi-temporal points. The imbalance between changed and unchanged samples, as well as between different categories of changes (newly built, demolition, and new clutter) introduces significant training difficulties [10]. Change regions typically account for a small proportion of training data, causing unchanged regions to dominate the network feature learning and detection results [11]. This imbalance affects feature learning and decision boundaries [12], reducing the accuracy and reliability of detection in multiple types of land cover.\n3) The lack of available and diverse datasets hinders the development of 3D change detection algorithms[13]. Without sufficient datasets covering various change categories, 3D change detection models struggle to learn robust and transferable features, limiting their generalization ability for multiple urban scenarios and making them difficult to apply in real-world applications.\nTo address the key gaps in existing methods and meet the growing demand for urban 3D semantic change detection, we introduce Multi-Task Enhanced Cross-Temporal Point Transformer (ME-CPT), an innovative approach for urban 3D semantic change detection. By integrating cross-temporal attention mechanism and multi-task training strategies. The main contributions of the proposed method are summarized as follows:\n1) A novel cross-temporal point transformer is proposed to enhance change feature extraction. By employing a cross-temporal point cloud serialization process, accurate spatiotemporal distribution correspondences are obtained. Additionally, a cross-temporal attention mechanism is utilized to facilitate feature interaction between multi-temporal points, effectively extracting change features.\n2) A multi-task enhancement strategy is incorporated to enhance semantic feature discriminability. This strategy addresses the challenge of insufficient feature extraction caused by the imbalance of change samples. By adopting a multi-task learning strategy, the model enhances feature representation, enabling robust semantic feature extraction.\n3) To overcome the lack of large-scale urban scene 3D change detection datasets, we released a novel 3D semantic change detection dataset, specifically designed to tackle the challenges posed by complex urban environments. Spanning a total area of 22.5 km\u00b2, the dataset includes diverse scenes and a wide variety of change samples, providing a solid benchmark to evaluate algorithm performance.\nThe remainder of the paper is organized as follows. Section II reviews the related works on 3D CD methods. Section III elaborates on the proposed network. Section IV introduces the new public 3D semantic change detection dataset. Section V presents the datasets and quantitative evaluation. Section VI discusses the effectiveness and generalization tests. The conclusion is outlined in Section VII."}, {"title": "II. RELATED WORKS", "content": "3D CD methods have been extensively studied and applied in various fields such as environmental monitoring, cultural heritage preservation, and urban management. Traditional 3D CD methods rely on hand-crafted features or rules, which can be categorized into geometric differencing-based methods, rule-based methods, and traditional machine learning-based algorithms.\nGeometric differencing based 3D CD methods: These methods compare geometric properties like point coordinates, surface normals, object shapes, or volumes between multi-temporal 3D data. For example, the simplest approach calculates point-to-point distances [14], but it is sensitive to noise and varying point densities. To address these issues, Lague et al. [15] uses local point cloud normals for surface distance calculation, while Liu et al. [6] adapts thresholds using k-nearest neighbors distance and local density. These methods are often used in multi-temporal surface analysis, such as depth of erosion of landslides [16], changes in rockfall volume [17], and tunnel deformation [18]. However, they are vulnerable to misalignment and noise, leading to inaccurate results, and do not capture semantic changes.\nRule-based 3D CD methods: These methods use geometric, textural, or semantic criteria to detect changes between multi-temporal 3D data, typically through rule-based thresholds. Volume, height, and spectral information from different epochs can be used as rules [19, 20]. For example, Dong et al. [21] uses ALS point clouds to extract building footprints and features, classifying buildings based on volume change thresholds into newly constructed, height increased or demolished categories. Similarly, Tamort et al. [7] proposes a semi-automatic method based on ALS point clouds, using rules to identify additions or removals. Although rule-based methods are simple and interpretable, these methods struggle with complex changes and manually defining rules for large datasets can be time-consuming.\nMachine-learning based 3D CD methods: These algorithms commonly detect changes using different classifiers. For instance, Tran et al. [22] designs features based on point distributions within multi-temporal point cloud neighborhoods, then uses random forests to classify change categories. Pushkar et al. [23] applies Support Vector Machine(SVM) to classify erroneous and building points in construction monitoring data and compare them with existing models to detect changes. Similarly, Peng and Zhang [24] combines point cloud data with orthophotos, extracting the Digital Surface Model(DSM) and geometric features, and then uses a decision tree to classify building changes. These methods offer a data-driven approach but require extensive feature engineering. This can lead to errors when handling complex scenes or large changes, increasing the risk of inaccuracies.\nIn summary, although these methods have been widely used, they are often labor intensive and may struggle to capture complex patterns or variations in the data. They often only detect changes for specific objects, limiting their ability to capture the full range of variations in complex scenes."}, {"title": "B. Deep-learning based 3D CD methods", "content": "Deep learning techniques have made significant advances in processing 3D point cloud data, owing to their ability to extract high-level features and handle unstructured data [25]. Deep learning techniques have shown success in automatically detecting changes in satellite images [26]. Inspired by 2D image CD methods, some researchers have transformed 3D point clouds into 2D images for change detection. For instance, Zhang et al. [27] converts multi-modal point clouds into images and inputs them into a feed-forward Convolutional Neural Network (CNN) alongside orthophotos, followed by connectivity analysis to obtain the final change detection results.\nWith the advancements in deep learning techniques for handling irregular 3D point cloud, point-based deep learning change detection methods have gained increasing attention. These methods offer a more direct approach to processing raw 3D data. For example, Krawciw et al. [28] implements binary change detection on a mobile robot platform by generating depth images from both the original point cloud map and the newly scanned point cloud as input, ultimately detecting changes and occlusions in the scene. Stathoulopoulos et al. [29] uses a deep learning framework to extract features from 3D scan points, then performs voxel-to-point comparisons to detect dynamic changes and identify potential obstacles.\nIn addition to predicting binary changes, urban environments require change detection results with multi-class semantic information, which is essential for understanding various types of change in complex urban scenes. Fang et al. [30] proposes a semantic support-based change monitoring method that first introduces a semantic segmentation network to extract semantic information from point clouds at different epochs, followed by obtaining results through the comparison of geometric and semantic information. However, this method does not constitute an end-to-end change detection framework. Consequently, many researchers have begun to explore end-to-end change detection frameworks that do not require post-processing. For example, De G\u00e9lis et al. [8] introduces a Siamese Kernel Point Convolution (KPConv) network for multi-class semantic change detection, utilizing the feature difference obtained from nearest neighboring points. Also, Wang et al. [9] achieves point-level change detection in street scenes by employing a local feature aggregation module to extract features, similarly obtaining the nearest feature difference. In addition, Zhan et al. [31] implements a prior-knowledge-guided 3D change detection network by generating prior knowledge for 3D change detection. In addition to supervised methods, some researchers have explored the application of unsupervised methods for 3D change detection. De G\u00e9lis et al. [32] proposes an unsupervised method that facilitates point-level multi-class change detection by achieving k-class pseudo-clustering, where users select the corresponding true labels for each predicted cluster. Although the unsupervised method reduces the need for labeled samples, the accuracy in change detection often fails to meet the requirements.\nIn summary, deep learning-based methods can improve the accuracy and efficiency of change detection tasks in complex 3D urban environments. However, these methods still face challenges in real-world ALS point cloud data, including low accuracy in multi-class semantic change detection, difficulties in data annotation, and imbalanced change samples."}, {"title": "C. Multi-tasks learning mechanism", "content": "The multi-task mechanism is a powerful approach in deep learning that enables models to efficiently learn from related tasks, improving overall performance and generalization while reducing the need for extensive training data for each individual task.\nThe multi-task mechanism is widely used in the field of remote sensing image CD. The multi-task approach for semantic change detection employs a unified framework that enables the simultaneous training of change detection and related tasks, such as segmentation and classification in each epoch. By leveraging multi-task learning, the model can share representations across tasks, allowing it to learn high-level features from diverse supervised labels concurrently. Typically, many researchers divide semantic change detection into two modules: semantic segmentation and change localization. Many researchers use multi-temporal remote sensing images as input to separately predict semantic maps and change maps for the two epochs [33, 34, 35]. For example, Zheng et al. [36] employs a deep multi-task encoder-transformer-decoder framework, which takes into account semantic change relationships and temporal consistency. The network ultimately predicts semantic changes by inheriting the tasks of semantic segmentation and binary change detection. The multi-task mechanism enhance the network's ability to learn and represent features by leveraging the auxiliary task. Shen et al. [37] uses building segmentation as a semantic constraint branch to improve the accuracy and completeness of the change detection task. In addition to the semantic segmentation task, specific feature enhancement related tasks can also improve the effectiveness of feature learning in the network. Li et al. [38] employs building boundary segmentation as an auxiliary task to improve the geometric boundary accuracy of building change detection.\nAdditionally, the multi-task mechanism has also been applied in deep network frameworks for 3D data processing. For instance, Liu et al. [39] achieves cross-modal change detection by leveraging consistency constraints between the height prediction and semantic segmentation tasks. Beyond change detection, Milli et al. [40] utilizes a multi-task structure with RGB and LiDAR data to improve road segmentation results. Liebel et al. [41] employs DSM data to separately predict roof classification and DSM values, thus refining the DSM data.\nIn summary, the multi-task mechanism in change detection aims to leverage the synergy between different related tasks to enhance the model's performance, efficiency, and generalization ability."}, {"title": "III. MULTI-TASK ENHANCED CROSS-TEMPORAL POINT TRANSFORMER", "content": "The workflow of the proposed ME-CPT network is illustrated in Fig. 1. ME-CPT first performs cross-temporal serialization on the point clouds from two epochs, and dividing them into cross-temporal patches. Next, temporal indicators are introduced in the point embedding layer, and the embedded features are then fed into a multi-stage encoder-decoder, where each stage includes grid pooling and multi-depth attention block (Section. III-B). The multi-task strategy is utilized to enhance feature discriminability (Section. III-C). The final output of ME-CPT includes point-level change prediction as well as semantic prediction for both epochs. The preliminaries of the basic point transformer are first introduced in Section III-A."}, {"title": "A. Preliminary of Point Transformer", "content": "Unlike 2D image change detection, 3D point cloud change detection faces challenges due to irregularity and intensive volume, making it hard to build a feature extraction network that captures long-range contextual relationships. The Point Transformer (PT-V1) [42] processes point clouds to capture local and global geometric features, using self-attention mechanism to model spatial relationships and handle irregularity and scale variations. To further expand the receptive field and improve computational efficiency, Point Transformer V3 (PT-V3) [43] constructs a patch attention process by serializing point clouds.\nThe self-attention mechanism(Section III-A1) and point serialization (Section III-A2) are the foundations of our method and are briefly reviewed below.\n1) Self-Attention Mechanism: To adapt the transformer-based network [44] for processing point clouds, PT-V1 applies the self-attention mechanism to individual points, allowing the model to learn spatial relationships within the point cloud. The self-attention mechanism calculates the attention weights for each point based on the relationships between its neighboring points, allowing the network to capture the spatial relationships and features. The point-based scaled dot-product attention is defined as shown in Eq. (1):\n$\\begin{equation} yout = \\sum_{f_j\\in M(i)}Softmax\\left(\\frac{\\varphi(f_i)^T \\psi(f_j)}{\\sqrt{c_h}}\\right) a(f_j), \\quad (1) \\end{equation}$\nwhere $yout$ is the output feature of the i-th point. The subset $f_j \\in M (i)$ is a set of points features in a local neighborhood of the i-th point. $\\varphi$, $\\psi$, and $a$ represent pointwize feature transformations, such as linear projections. $\\frac{1}{\\sqrt{c_h}}$ is the scaling factor.\n2) Point Serialization: Due to the irregularity, discreteness, and large scale of point clouds, along with significant variations in point cloud density and object sizes, selecting appropriate neighborhoods is a significant challenge. Common neighborhood selection methods include k-nearest neighbors and fixed-radius spherical neighborhoods, but they struggle to adapt to variations in point cloud density. To efficiently structure irregular point clouds, PT-V3 introduces point cloud serialization and a serialized attention mechanism, expanding each point's receptive field to 1024 points.\nPT-V3 utilizes space-filling curves [45], such as Z-order curves [46] and Hilbert curves [47], to serialize points in specific spatial distributions. Mathematically, a bijective function $ : Z \\rightarrow Z^n$ is defined, where n is the dimensionality of the space. Then, using a serialized encoding method, the points' coordinates are converted into integers to reflect their order along the space-filling curve. Using the inverse mapping function $-1 : Z^n \\rightarrow Z$, the point cloud is serialized. PT-V3 assigns a 64-bit integer to each point to record the serialized code, allocating the trailing k bits to the position encoded by 4-1 and the leading bits to the batch index b. The formula for the serialization encoding is given by Eq. (2):\n$\\begin{equation} Encode (p, b, g) = (b << k) | -\u00b9 ([p/g]), \\quad (2) \\end{equation}$\nwhere p represents the point cloud coordinates, g represents the grid size, and [p/g] represents the coordinates of the points converted to the grid."}, {"title": "B. Cross-Temporal Point Transformer", "content": "The transformer-based structure and point cloud serialization offer an efficient approach for point cloud-based feature extraction. However, the challenge of urban multi-class semantic change detection lies in establishing comprehensive correspondence between multi-temporal points and extracting features containing semantic and change information.\nRemark 1. Most existing works [8, 9, 31] adopt Siamese structures, where two branches with shared weights independently compute semantic features for point clouds in each epoch. Change feature extraction in most of these works is achieved by matching points to their nearest or k-nearest neighbors between different epochs and then calculating change features by subtraction or concatenation. However, the extraction of change features remains limited because the complexity of multi-temporal distributional differences and semantic changes have not been fully considered. Therefore, the lack of comprehensive multi-temporal spatial correspondences and the representation of change features will lead to difficulties in effectively extracting the changes that have occurred.\nTo improve change feature extraction, ME-CPT directly establishes spatiotemporal correspondences between multi-temporal point clouds and jointly extracts change features and semantic features, rather than adopting a Siamese structure. Firstly, ME-CPT leverages cross-temporal serialization to construct the spatial distribution relationships of multi-temporal points and divides them into cross-temporal patches (Section. III-B1). Then, for each patch, the cross-temporal attention mechanism (Section. III-B2) is applied to extract semantic and change features. Finally, the encoder and decoder in the network framework are implemented by multi-stage, varying-depth cross-temporal attention block. The proposed method enhances the spatial correspondence and change feature extraction of multi-temporal point clouds. The differences between ME-CPT and the Siamese-based method are illustrated in Fig.2.\n1) Cross-Temporal Point Serialization: In this paper, for the change detection task, we innovatively align point clouds of two epochs and perform cross-temporal serialization within the same coordinate frame, along with implementing cross-temporal neighborhood partitioning, as shown in Fig.3. Pto and Pt1 are point clouds of two epochs, aligned within the same coordinate frame:\n$\\begin{equation} Pto = {pton pton \\in R\u00b3, n = 1, 2, . . ., N}, \\quad (3a) \\end{equation}$\n$\\begin{equation} Pt1 = {p\u00b9m p\u00b9m \\in R\u00b3, m = 1, 2, . . ., M M }. \\quad (3b) \\end{equation}$\nThen, the aligned point clouds are serialized with the serialized positional encoding shown in Eq.(2), where p \u2208 PtoUPt1. Cross-temporal point clouds serialization uses space-filling curves to link the point clouds, constructing the spatiotemporal neighborhood relationships between all points. Specifically, cross-temporal serialization captures the spatial distribution of each point's neighborhood, including points both within and between different epochs.\nTo preserve sufficient spatial geometric relationships while minimizing computational resource requirements, the point cloud can be divided into multiple patches. By focusing on each patch individually, the network can capture the unique geometric and semantic features of the local space. Accordingly, ME-CPT adopts a cross-temporal patch partition approach, and the attention mechanism is implemented within each independent cross-temporal patch. This allows the model to capture both spatial and temporal dependencies within each cross-temporal patch, improving the ability to extract semantic and change features across different epochs.\nAs shown in Fig.3, the cross-temporal serialization process uses different space-filling curve encoding to establish spatial relationships between point clouds from different epochs, linking cross-temporal point clouds based on their spatial relationships. Then, given the number of points k in each patch, the serialized spatial connections are partitioned into non-overlapping cross-temporal patches. Ultimately, each independent cross-temporal patch contains points from different epochs, where the different colors represent the independent cross-temporal patches.\n2) Cross-Temporal Patch Attention: To distinguish points from different epochs within the same cross-temporal patch and enable the network to learn both semantic and change features for both the same and different epochs, ME-CPT introduces a temporal indicator as an additional input dimension. As shown in Fig.1, in the point embedding layer, in addition to the point coordinates x, y, z, a specific value is added as a temporal indicator for the points corresponding to different epochs. The temporal indicators allow the network to distinguish points from the two epochs and to learn both the temporal and spatial relationships effectively. The input of feature embedding layer is written as:\n$\\begin{equation} \\begin{bmatrix} xtoto\\\\ toto\\\\ x,y,zoo\\\\ x1t1\\\\ titi\\\\ Yo, zo\\\\ xt1\\\\ ti\\\\ zt1\\\\ .\\\\ .\\\\ xNN ttooto\\\\\\\\\\ YN, ZN ,NNN \\\\ XM, \u0423M, ZM , \u041c \\end{bmatrix} \\quad , \\quad (4) \\end{equation}$\nwhere xto, yto, zto represent the coordinates of the points in the first epoch, and xt1, yt\u0131, zt\u00b9 represent the coordinates of the points in the second epoch. TIo and TI\u2081 denote the temporal indicators of the points respectively. In this paper, the temporal indicator value is defined as: {TIt = t, t \u2208 {0,1}}.\nBased on the reorganization of point clouds from two epochs into independent cross-temporal patches with spatiotemporal relationships, ME-CPT jointly extracts the change and semantic features of points through a cross-temporal attention mechanism. Accordingly, the encoder and decoder proposed in this method consist of multi-stage, varying-depth cross-temporal attention block, with each block containing a cross-temporal patch attention layer. As shown in Fig. 4, the cross-temporal attention block used in this paper is illustrated.\nIn cross-temporal attention layer, the attention mechanism is applied to each independent cross-temporal patch. The attention mechanism within each patch is implemented using the scaled dot-product attention defined in Eq.(1). Q, K and V refer to the query, key, and value feature vectors obtained by applying linear projections to the point features. pi represents a point from any specific epoch, and M (i) is the set of all points from different epochs within the corresponding cross-temporal patch. Furthermore, before the attention layer, an enhanced conditional position encoding (xCPE) is introduced, which is implemented through a sparse convolution layer with skip connections. Additionally,in both the encoder and the decoder, before each stage of the varying-depth cross-temporal attention blocks, Grid Pooling and Grid UpPooling proposed by Wu et al. [48] are integrated to perform downsampling and upsampling of the point cloud."}, {"title": "C. Multi-task Enhanced Feature Extraction", "content": "In the 3D semantic change detection, the sample distribution in the training data exhibits a significant imbalance, making it difficult for the network to learn sufficient discriminative features. To overcome the samples imbalance and effectively extract features, the proposed ME-CPT introduces a multi-task learning strategy, using semantic segmentation predictions for each epoch as an auxiliary task.\nMore specifically, the proposed multi-task training strategy enhances the discriminability of features by separately feeding the features obtained from the decoder into the change detection branch and the semantic prediction branch. The change detection branch predicts the change categories for points in T\u2081 epoch, while the semantic segmentation branch predicts the semantic categories for points in both To and T\u2081 epochs.\nIn change detection tasks, the unchanged category dominates the majority of the samples and contains multiple types of land cover, while the spatial distribution of urban scenes and semantic changes are complex. By sharing the encoder and decoder and jointly training on different tasks, the model can learn more generalizable semantic features of different land covers through the auxiliary semantic segmentation task, thereby improving the understanding of semantic change information. As shown in Fig. 5, adding the auxiliary semantic segmentation task significantly improves the model's ability to distinguish features of different land covers in unchanged regions. The multi-task mechanism enhances the ability to extract multi-class land cover features, thereby mitigating the negative impact of sample imbalance on the representation of the unchanged regions."}, {"title": "D. Training Loss", "content": "To train the proposed ME-CPT network, the loss functions of the change detection branch and the semantic segmentation branch are jointly used, including the change detection loss Lcd and the semantic segmentation loss Lss. The overall loss function Ltotal composing semantic segmentation loss and the change detection loss is written as:\n$\\begin{equation} Ltotal = \\alpha Lcd + \\beta Lss, \\quad (5) \\end{equation}$\nwhere \u03b1 and \u03b2 are the weights for the semantic segmentation branch and the change detection branch, respectively. In the experiment, both \u03b1 and \u03b2 are set to 0.5. More specifically, the change detection loss Lcd and the semantic segmentation loss Lss are calculated using Eq.(6).\n$\\begin{equation} L = -\\frac{1}{C} \\sum_{i=1}^{C} li log (predi), \\quad (6) \\end{equation}$\nwhere C is the number of semantic classes or change detection classes. li and predi represent the ground truth and the predicted labels, respectively."}, {"title": "IV. NEW YORK CITY SEMANTIC CHANGE DETECTION DATASET (NYC-SCD)", "content": "To the best of our knowledge, currently there is no publicly available urban 3D change detection dataset based on ALS data. To address this gap, we propose a more challenging multi-class 3D semantic change detection dataset."}, {"title": "A. Data acquisition", "content": "The new republic 3D semantic change detection dataset is sourced from high-quality ALS data collected in 2014 and 2017, released by the New York City agency. The two epochs of ALS point cloud data cover a wide range of scenes, and the variety of change samples is sufficient to support research in 3D change detection. The 2014 ALS data were acquired using the Leica ALS70, achieving a root mean square error (RMSE) accuracy of 5.3 cm and a point density of 5.9 points/m\u00b2 based on ground control measurements. The 2017 ALS data were captured using the Leica ALS80, with an improved RMSE of 3.5 cm and a point density of 8 points/m\u00b2. Taking into account the diversity of the change samples and urban scenes, we selected 10 scenes, each covering an area of 1.5 \u00d7 1.5km\u00b2, distributed across the five boroughs of New York City. The total area covered by these scenes is 22.5 km\u00b2, as shown in Fig. 6, where the blue areas represent the selected data coverage locations."}, {"title": "B. Data annotation", "content": "This paper annotates the semantic change detection dataset for the selected multi-temporal point cloud data described in Section IV-A. In NYC-SCD, the ALS point clouds from both epochs are annotated with semantic labels, and the T\u2081 epoch point clouds are additionally annotated with semantic change labels.\nThe semantic labels of the two epoch point clouds are annotated into four categories, with the following classification for each category:\n\u2022 Ground: road surfaces, pavement, flat terrain, etc.\n\u2022 Building: all man-made structures.\n\u2022 Vegetation: trees and other low-growing plants.\n\u2022 Clutter: ground objects that are not vegetation.\nBased on the semantic labels and rules for change detection, the T\u2081 epoch point cloud is annotated with four change categories. The categories are as follows:\n\u2022 Unchanged: all semantic categories that have not undergone any changes.\n\u2022 Newly built: newly constructed buildings and added roof attachments.\n\u2022 Demolition: areas where buildings have been demolished.\n\u2022 New clutter: newly appearing objects that include semantic categories such as vegetation and clutter.\nAdditionally, points that represent rare categories in the dataset, which are not relevant to the urban semantic change categories, are manually checked and labeled as \"Ignored\" categories. These include features such as railways, shoreline shrubs, and surface water points."}, {"title": "C. Statistics of NYC-SCD", "content": "The 10 selected scenes exhibit significant diversity, representing typical urban forms from various cities. Based on the distribution of building heights and density, these scenes can be broadly categorized into three types: high-density, medium-density, and low-density areas. High-density areas feature numerous high-rise buildings with dense distributions. Medium-density areas consist of buildings with moderate heights (within 20 floors) and relatively even distribution. Low-density areas are residential zones located away from commercial or office areas, characterized by lower buildings and abundant open spaces and green areas. Examples illustrating the semantic and change labels for three different types of scenes are presented in Fig.7. To account for diversity between scenes, each scene was divided into train, validation, and test sets in a 6:1:2 ratio, as illustrated in Fig.8. This division ensures a balanced distribution, providing an appropriate proportion for model training, validation, and evaluation.\nThe point counts of semantic labels and change labels for the NYC-CD dataset are summarized in Tab.I."}, {"title": "V. EXPERIMENT", "content": "The network proposed in this paper adopts a four-stage encoder-decoder architecture, with the block depths [2,2,6,2] and [2, 2, 2, 2] respectively. The patch size is set to 1024 for datasets URB3DCD-V2, AHN-CD, and NYC-SCD, while for the street-level dataset SLPCCD, it is set to 512.\nTraning parameters: In the training process, the point clouds of both epochs are first voxelized using a voxel size v. Then, the cylinder-based sample sampling method is used, where each input sample to the network consists of points from a cylindrical region with a given radius r. The training parameters and optimization settings for each dataset are listed in Tab.II."}, {"title": "B. Evaluation Metric", "content": "This paper uses widely adopted evaluation metrics, including overall accuracy (OA), intersection over union (IoU), and mean IoU (mIoU), for point-wise accuracy assessment."}, {"title": "C. 3D Chaneg Detection Datasets", "content": "To validate the effectiveness of the proposed method, experiments are conducted not only on the proposed NYC-SCD dataset, but also on two publicly available datasets with change annotations (Ur3DCD-V2 and SLPCCD), as well as a multi-temporal ALS point cloud dataset with semantic annotations (AHN-CD [Actueel Hoogtebestand Nederland Change Detection]). The descriptions of the datasets are detailed as follows.\n1) Simulated urban change detection dataset (URB3DCD-V2): de G\u00e9lis et al. [49] employees a simulator based on the LoD2 model of Lyon, France, to generate multi-temporal airborne point cloud data by simulating the addition and removal of buildings and assigning change labels. Based on this dataset, De G\u00e9lis et al. [8] improves it by randomly incorporating vegetation, vehicles, and other moving objects, resulting in a multi-class change detection dataset. This dataset includes semantic labels for point clouds of two epochs To and T1, as well as change labels for T\u2081. In the proposed paper, experiments are conducted on the first subset of the dataset, with a point density of approximately 0.5 points/m\u00b2. The semantic labels of point clouds are ground, building, vegetation and mobile objects, while the change labels include unchanged, newly built, demolition, vegetation growth, new vegetation, missing vegetation, and mobile objects.\n2) Street-Level Point Clouds Change Detection Dataset (SLPCCD): SLPCCD is a street-level change detection dataset proposed by Wang et al. [9]. The data consists of colored point clouds collected using mobile LiDAR in 2016 and 2020. In the SLPCCD dataset, the data are divided into cylinders with a radius of 3 meters, centered on street-level objects, and labeled as either change points or background points. For the To epoch, the change point class is labeled as add, while for the T\u2081 epoch, the class is labeled as removed. The objects involved in the changes in this dataset include streetlights, signposts, benches, pedestrians, and other street-level objects.\n3) Netherlands Change Detection (AHN-CD): The AHN-CD dataset is sourced from multiple national LiDAR acquisitions released by the Netherlands. This study uses data from the third and fourth acquisitions, AHN3 and AHN4. The data coverage area is consistent with [8], with the train, validation, and test sets covering approximately 3.75 km\u00b2, 1.25 km\u00b2, and 2.5 km\u00b2, respectively. The point density for AHN3 ranges between 10 and 14 points/m\u00b2, while for AHN4, it ranges from 20 to 24 points/m\u00b2. In this work, the semantic labels for AHN3 and AHN4 are relabeled as ground, building, vegetation, and clutter. Following the change labeling process proposed by De G\u00e9lis et al. [8] the points in the T\u2081 epoch were annotated as unchanged, newly built, demolition, and new clutter."}, {"title": "D. Experimental Results", "content": "1) Simulated urban change detection dataset (Urb3DCD-V2): The quantitative evaluation and qualitative results of the proposed method, along with the comparison methods, are presented in Fig.9 and Tab.III. The proposed method achieves the best mIoU in the simulated dataset. Among the comparison methods, RF [22", "8": "and PGN3DCD [31"}]}