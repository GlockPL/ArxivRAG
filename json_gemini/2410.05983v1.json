{"title": "Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG", "authors": ["Bowen Jin", "Jinsung Yoon", "Jiawei Han", "Sercan \u00d6. Ar\u0131k"], "abstract": "Retrieval-augmented generation (RAG) empowers large language models (LLMs) to utilize external knowledge sources. The increasing capacity of LLMs to process longer input sequences opens up avenues for providing more retrieved information, to potentially enhance the quality of generated outputs. It is plausible to assume that a larger retrieval set would contain more relevant information (higher recall), that might result in improved performance. However, our empirical findings demonstrate that for many long-context LLMs, the quality of generated output initially improves first, but then subsequently declines as the number of retrieved passages increases. This paper investigates this phenomenon, identifying the detrimental impact of retrieved \"hard negatives\" as a key contributor. To mitigate this and enhance the robustness of long-context LLM-based RAG, we propose both training-free and training-based approaches. We first showcase the effectiveness of retrieval reordering as a simple yet powerful training-free optimization. Furthermore, we explore training-based methods, specifically RAG-specific implicit LLM fine-tuning and RAG-oriented fine-tuning with intermediate reasoning, demonstrating their capacity for substantial performance gains. Finally, we conduct a systematic analysis of design choices for these training-based methods, including data distribution, retriever selection, and training context length.", "sections": [{"title": "1. Introduction", "content": "Retrieval-augmented generation (RAG) (Gao et al., 2023) empowers large language models (LLMs) to utilize external information sources by selecting the most relevant pieces from a large corpus (Zhao et al., 2023), thereby enhancing their effectiveness, customizability and efficiency in complex problem-solving. RAG can also mitigate issues such as factual inaccuracies (Augenstein et al., 2023) and hallucinations (Huang et al., 2023), which LLMs often exhibit when confronted with knowledge-intensive tasks. RAG systems typically employ a retriever to identify relevant information from a corpus, which is then presented in the context of an LLM as the generator.\nRecent advances in computational resources and methodological innovations have enabled the development of LLMs that support increasingly longer context (Dubey et al., 2024; Reid et al., 2024). This has even opened up new avenues for directly inputting entire corpora or knowledge bases into the LLMs. Yet, it would still not be feasible for large corpora (e.g., Wikipedia) and can incur higher computational costs. Despite extensive research on RAG (Lee et al., 2024; Li et al., 2024; Xu et al., 2023), the interplay with long-context LLMs, particularly how to optimally design RAG systems using them effectively, remains under-explored. Existing works (Asai et al., 2024; Lin et al., 2024; Yoran et al., 2024) propose tuning LLMs for RAG, but predominantly focus on a limited number of retrieved passages (fewer than 10). Intuitively, longer context would allow for the inclusion of more retrieved passages, leading to higher recall and potentially improved performance. However, our findings reveal that this does not always hold true and highlight the need for a careful re-evaluation of standard RAG designs when utilizing long-context LLMs. We demonstrate that achieving optimal performance in such systems and to fully utilize the opportunities provided by the LLMs require a holistic rethinking and effective novel approaches to the unique challenges."}, {"title": "2. Related Work", "content": "Large language models (LLMs) can be prone to hallucinations especially at knowledge-intensive tasks (Augenstein et al., 2023; Huang et al., 2023; Zhao et al., 2023). Retrieval-augmented generation (RAG) addresses this by incorporating external knowledge sources to provide accurate and relevant information (Gao et al., 2023). Traditional RAG systems comprise a retriever to identify relevant information and a generator to synthesize the answer (Zhao et al., 2024; Zhu et al., 2021). While previous research focused on improving either the retriever (Izacard et al., 2021; Karpukhin et al.,"}, {"title": "3. Challenges of Long context LLMs in RAG", "content": "We present a systematic investigation into the challenges of utilizing long-context LLMs in RAG. Each subsection focuses on a specific research question, outlining corresponding experiments and analyzing the results on the key challenges. These insights inform the development of targeted solutions for improving RAG performance with long-context LLMs, which are presented in subsequent sections."}, {"title": "3.1. The Effect of retrieved context size on RAG performance", "content": "This subsection investigates the relationship between the number of retrieved passages and the performance of long-context LLMs in RAG systems.\nResearch question. Long-context LLMs offer the potential to incorporate more retrieved passages into RAG systems. This raises a crucial question: Does a larger volume of retrieved context consistently translate to better performance when using long-context LLMs in RAG?\nExperimental setting. We evaluate the performance of RAG systems on the Natural Questions (NQ) (Kwiatkowski et al., 2019) dataset using two different retrievers (BM25 (Robertson et al., 2009) and e5 (Wang et al., 2022), where e5 exhibits higher performance on NQ (Recall@40 is 0.90 with e5 and 0.73 with BM25)) and four long-context LLMs (Gemma-7B-Chat (Team et al., 2024a), Gemma-2-9B-Chat (Team et al., 2024b), Mistral-Nemo-12B-Instruct (Jiang et al., 2023) and Gemini-1.5-pro (Reid et al., 2024)). We systematically vary the number of passages retrieved by each retriever."}, {"title": "3.2. The interplay of retrieval quality and LLM capabilities", "content": "This subsection delves into the factors hindering the performance of long-context LLMs in RAG, aiming to discern whether limitations arise from retrieval quality or the LLM's ability to process the retrieved information.\nResearch question. Do the observed performance bottlenecks originate from limitations in the retriever's"}, {"title": "3.3. The importance of hard negatives for long-context LLM evaluation", "content": "This subsection investigates the impact of \"hard negatives\" on the performance of long-context LLMs in RAG, highlighting the need for more robust evaluation methodologies.\nResearch question. In long-context RAG scenarios, where a vast knowledge source necessitates retrieving numerous passages, the likelihood of including relevant information (i.e. obtaining high recall) increases. However, this also elevates the risk of introducing hard negatives. This raises two critical questions: (1) How robust are current long-context LLMs to these hard negatives? and (2) Does the impact of hard negatives vary with the retriever used?\nExperimental setting. This study investigates the effect of hard negative passages on long-context LLM performance in a controlled setting. We tasked three LLMs (Gemma2-7B-Chat, Mistral-Nemo-12B-Instruct, and Gemini-1.5-Pro) with answering queries based on a context comprising a single golden passage and a varying number of hard negative passages retrieved using different methods (e5, Contriever, BM25, and random sampling). This synthetic experiment, detailed in Figure 3, isolates the impact of hard negatives by holding the golden passage constant and intentionally excluding scenarios with multiple golden passages, which are common in real-world RAG systems. See Appendix C for a complete illustration of the experimental setup."}, {"title": "4. Simple and effective training-free RAG improvement", "content": "Building upon the analyses in Section 3 on the detrimental impact of hard negatives on long-context LLMs in RAG, we focus on the training-free solution, retrieval reordering. This method leverages the inherent \"lost-in-the-middle\" phenomenon observed in LLMs to mitigate the negative effects of hard negatives. As highlighted by Liu et al. (2024), LLMs exhibit a tendency to prioritize information presented at the beginning and end of an input sequence, while paying less attention to the middle.\nExploiting this \"lost-in-the-middle\" behavior, we consider a simple and effective strategy: reordering the retrieved passages based on their relevance scores calculated by the retriever. Given a query q and a set of retrieved passages $d_1, d_2, ..., d_k$ with decreasing relevance scores, the standard input sequence construction for an LLM with instruction I would be: [I, $d_1, d_2, ..., d_{k-1}, d_k$, q]. Retrieval reordering modifies this to prioritize passages with higher scores at the beginning and end: [I, $d_1, d_3, ..., d_4, d_2$, q] where the position of passage $d_i$ is determined by\n$Order(d_i) = \\begin{cases}\\frac{i+1}{2} & \\text{if mod(i, 2) = 1} \\\\ (k + 1) - \\frac{i}{2} & \\text{if mod(i, 2) = 0} \\end{cases}$        (1)\nThis reordering strategy aims to guide the LLM's attention towards the most relevant passages, thereby reducing the influence of hard negatives positioned in the middle of the sequence. The pseudo-code for retrieval reordering can be found in Appendix E."}, {"title": "5. Improving Robustness for RAG via Data-Augmented Fine-Tuning", "content": "While the retrieval reordering strategy presented in Section 4 mitigates the detrimental impact of hard negatives, it does not inherently enhance the LLM's ability to handle such irrelevant information within the context. To address this, we conduct a systematic investigation into RAG-specific tuning as a means of improving long-context LLMs for RAG applications.\nOur tuning paradigm involves training LLM to generate the correct answer (a) given a comprehensive input comprising an instruction (I), a query (q), and a set of retrieved passages (d1, d2, ..., dk):\nInput: [I, d1, d2, ..., dk\u22121, dk, q] \u2192 Output: a.        (2)\nThis approach aims to implicitly enhance the LLM's robustness to hard negatives by exposing it to a diverse range of retrieved contexts during fine-tuning, thus enabling it to learn to effectively identify and utilize relevant information even in the presence of noise."}, {"title": "5.1. Implicitly improving LLM robustness through fine-tuning", "content": "While the retrieval reordering strategy presented in Section 4 mitigates the detrimental impact of hard negatives, it does not inherently enhance the LLM's ability to handle such irrelevant information within the context. To address this, we conduct a systematic investigation into RAG-specific tuning as a means of improving long-context LLMs for RAG applications.\nOur tuning paradigm involves training LLM to generate the correct answer (a) given a comprehensive input comprising an instruction (I), a query (q), and a set of retrieved passages (d1, d2, ..., dk):\nInput: [I, d1, d2, ..., dk\u22121, dk, q] \u2192 Output: a.        (2)\nThis approach aims to implicitly enhance the LLM's robustness to hard negatives by exposing it to a diverse range of retrieved contexts during fine-tuning, thus enabling it to learn to effectively identify and utilize relevant information even in the presence of noise."}, {"title": "5.2. Enhancing relevance identification through reasoning augmentation", "content": "While the fine-tuning approach described in Section 5.1 implicitly enhances the LLM's robustness to hard negatives, it does not explicitly train the model to differentiate between relevant and irrelevant passages within the retrieved context. To address this, we investigate the effectiveness of incorporating an intermediate reasoning step into the fine-tuning process.\nThis modified paradigm involves training the LLM to generate both a reasoning paragraph (r) that explicitly identifies the relevant passages for the given query (q) and the final answer (a):\nInput: [I, d1, d2, ..., dk\u22121, dk, q] \u2192 Output: [r, a],        (3)\nDuring training, the LLMs are provided with labeled reasoning paragraphs to guide its learning process. During inference, the LLMs are instructed to first generate the reasoning paragraph and then"}, {"title": "6. Data-Centric Perspectives on Fine-tuning LLMs for RAG", "content": "Impact of training data distribution on generalization. We first examine how the distribution of training data affects the generalization of the fine-tuned LLM. We train LLMs on five different data distributions, each with 50k samples: (1) a mixed dataset comprising NQ, WoW, Fever, and MMLU (12.5k samples from each); (2) NQ only; (3) WoW only; (4) Fever only; and (5) MMLU only."}, {"title": "7. Conclusions", "content": "This paper investigates the impact of increasing the number of retrieved passages on the performance of long-context LLMs in retrieval-augmented generation (RAG) systems. Contrary to expectations, we observe that performance initially improve but then degrade as more passages are included. This phenomenon is attributed to the detrimental influence of retrieved \"hard negatives\". To mitigate this issue, we propose and evaluate three solutions: training-free retrieval reordering, RAG-specific implicit LLM fine-tuning, and RAG-oriented LLM fine-tuning with intermediate reasoning. A systematic analysis of the training-based methods explores the effects of data distribution, retriever for training, and training context length. Interesting future directions include exploring (automated) position optimization with more advanced retrieval ordering methods, and fine-tuning the LLMs for RAG with more fine-grained and multi-step reasoning chains."}, {"title": "Appendix", "content": null}, {"title": "A. Retriever performance and similarity", "content": "We analyze the performance and similarity of four retrievers (BM25, contriever, e5 and bge) on the NQ dataset shown in Figure 8. Each data point corresponds to a retrieval (recall, precision) pair for a specific number of retrieved passages. The overall retrieval performances on NQ are observed as e5 > bge > contriever > bm25, with contriever having a similar performance with BM25 and bge having a similar performance with e5 (as their curves are closer)."}, {"title": "B. Long context LLMs in RAG analysis on other datasets", "content": "In addition to the analysis presented on the NQ dataset in Section 3, we conduct further studies on the PopQA dataset to underscore the generality of our findings."}, {"title": "B.1. The Effect of retrieved context size on RAG performance", "content": "Observations. Figure 9 presents the following key observations similar to that in Section 3.1: 1) Strong Retriever (e5): Across all LLMs, increasing the number of retrieved passages initially enhances performance, but subsequently results in either a sharp decline or a plateau. 2) Weak Retriever (BM25): Performance generally shows a continuous improvement or a slighter decrease as the number of retrieved passages increases. While these observations may appear counter-intuitive - given that one might expect monotonic improvements due to higher recall (i.e., a greater chance of retrieving relevant information) - the inclusion of additional documents can reduce precision, with irrelevant or misleading passages detracting LLMs from overall performance."}, {"title": "B.2. The importance of hard negatives for long-context LLM evaluation", "content": "Observations. Figure 10 shows the following observations similar to that in Section 3.3: (1) Sensitivity to hard negatives: Across all LLMs, increasing the number of hard negative passages generally results"}, {"title": "C. Illustration of Section 3.3: Hard negative study", "content": "Algorithm 1 Data Construction for Hard Negative Study\nRequire: Query q, instruction I, golden passage dgold, golden answer a, retrieved passages D =\n[d1, d2, ..., dn] with decreasing retriever relevance scores, desired number of passages K (K \u00ab N).\nEnsure: Input sequence S.\n1: Initialize list S \u2190 [dgold]\n2: for each passage di in D do\n3: if di \u2260 dgold and a not in di then\n4: Append di to S\n5: end if\n6: if |S| = K then\n7: break\n8: end if\n9: end for\n10: Randomly shuffle S.\n11: Construct input sequence [I, S[1], S[2],...,S[K],q].\n12: return The input sequence [I, S[1], S[2],...,S[K], q]."}, {"title": "D. Hard negatives case study", "content": "In this section, we provide a case study to compare the hard negatives returned by different retrievers. We classify the negative passages into two types: (1) Related but Irrelevant: passages related to some entities mentioned in the question but not containing the ground truth answer; (2) Not Related: passages not related to the question at all. Note that Related but Irrelevant passages are harder and more misleading to the LLMs compared with Not Related passages. We show the top-5 negatives from each retriever for a random sampled question as below. From the case study, we can find that the negatives retrieved by e5 contain more Related but Irrelevant passages compared with those retrieved by bm25, while those retrieved by bm25 still have more Related but Irrelevant passages than random sampling. This qualitatively demonstrates that the hardness of the negatives from different retrievers as e5 > bm25 > random.\nQuestion The south west wind blows across Nigeria between?\nGround Till September\nTruth"}, {"title": "E. Retrieval reordering", "content": "Algorithm 2 Retrieval Reordering Algorithm\nRequire: Query q, instruction I, retrieved passages D = [d1,d2, ..., dk] with decreasing retriever relevance scores.\nEnsure: Reordered sequence S.\n1: Initialize an empty list S of length k.\n2: for i = 1 to k do\n3: if mod(i, 2) = 1 then\n4: Order(di) \u2190 $\\frac{i+1}{2}$ {i is odd}\n5: else\n6: Order(di) \u2190 k + 1 \u2212 $\\frac{i}{2}$  {i is even}\n7: end if\n8: Place di at position Order(di) in S.\n9: end for\n10: Construct input sequence [I, S[1], S[2], ..., S[k], q].\n11: return The reordered sequence [I, S[1], S[2], ..., S[k], q]."}, {"title": "F. Datasets", "content": "In this section, we discuss the datasets for RAG-specific LLM training and evaluation."}, {"title": "F.1. Training datasets", "content": "We select a series of fine-tuning data designed to enhance the model's robustness to hard negatives in the retrieval context and improve its contextual awareness in generating predictions. The training data are from four sources with different answer types: Natural Question (short-form), Wizard of Wikipedia (long-form), FEVER (true/false), and MMLU (close-set). The statistics of the training data mix can be found in Table 1."}, {"title": "F.2. Testing datasets", "content": "To comprehensively evaluate our methods, we select testing datasets across different tasks including:\n(1) Question-answering: TriviaQA, PopQA, WebQuestions; (2) Multi-hop tasks: HotpotQA, 2WikiMultiHopQA, Bamboogle; (3) Long-form tasks: ASQA; (4) Slot filling: T-REx, Zero-shot RE. The statistics of all the datasets can be found in Table 2."}, {"title": "F.3. Retrieval corpus", "content": "Following Karpukhin et al. (2020), we use the text chunks from 2018 Wikipedia dump as the retrieval corpus. The articles are split by section, where long sections are further split into text chunks of equal sizes and contain less than 100 words, leading to a total of 21M text chunks."}, {"title": "G. Implicit RAG Fine-Tuning Experimental Setting", "content": null}, {"title": "G.1. Training settings", "content": "Hyperparameters. We use the top-40 retrieved text chunks for a given example to generate the fine-tuning samples and use e5 as the retriever for the main results. We fine-tune both Gemma-2-9B-Base and Mistral-Nemo-12B-Base using 8 H100 GPUs. For both models, we use the chat template corresponding to Gemma-2-9B-Chat and Mistral-Nemo-12B-Instruct respectively when tuning the models. We use the axolotl\u00b9 codebase for their tuning. For Gemini-1.0-Pro tuning, we use the Google Cloud Tuning API\u00b2 with the default settings. The hyperparameters can be found in Table 3."}, {"title": "Training RAG instruction templates.", "content": "The RAG instruction templates for different training datasets can be found in Table 4."}, {"title": "H. RAG Finetuning with Intermediate Reasoning Experimental Setting", "content": null}, {"title": "H.1. Training settings", "content": "Hyperparameters. The hyperparameter setting is the same to that in Appendix G.1.\nTraining RAG instruction templates. The RAG instruction templates with intermediate reasoning for different training datasets can be found in Table 7."}, {"title": "I. Data-Augmented RAG Case Studies", "content": "Question Which film features the Dawes Tomes Mousley Grubbs Fidelity Fiduciary Bank?\nGround Mary Poppins\nTruth"}, {"title": "J. Data-Augmented RAG Finetuning on Gemma-2-9B", "content": "In Figure 5, we illustrate the performance of implicit RAG finetuning on eight datasets with three different base models due to the space limitation. The whole results with Gemma-2-9B models can be found in Figure 11."}, {"title": "K. Data-Augmented RAG Finetuning on Mistral-Nemo-12B", "content": "In addition to the comprehensive data-augmented RAG fine-tuning results with three different base LLMs reported in Section 5, we also would like to show the results specifically with the Mistral-Nemo-12B models in Figure 13."}, {"title": "L. Data-Augmented RAG Finetuning on Gemini-1.0-Pro", "content": "In addition to the comprehensive data-augmented RAG fine-tuning results with three different base LLMs reported in Section 5, we also would like to show the results specifically with the Gemini-1.0-Pro models in Figure 14. Due to the Gemini-1.0-Pro API call credit limitation, we random sample 1000 queries for each dataset."}, {"title": "M. Training data scaling and RAG performance.", "content": "To investigate the influence of the size of the training data on the effectiveness of RAG-specific tuning, we fine-tune the Gemma-2-9B-Base model using varying amounts (5k to 200k samples) of mixed training data from NQ, WoW, Fever, and MMLU. Table 11 presents the evaluation results on the NQ dataset, demonstrating a clear positive correlation between the scale of training data and the performance of the resulting LLM in RAG. Increasing the amount of training data consistently leads to improved accuracy, highlighting the benefits of leveraging larger datasets for fine-tuning LLMs in RAG applications."}, {"title": "N. RAG-specific tuning data inside SFT mixtures", "content": "Having established the effectiveness of RAG-specific fine-tuning for improving LLM performance in RAG tasks, we now investigate whether combining RAG-specific data with general SFT data can further enhance performance while preserving the LLM\u2019s general capabilities (e.g., reasoning and long-form generation), as a way to assess the potential of the proposed tuning methods to be useful for construction of foundation models. We train the Gemma-2-9B model using two different strategies:\n(1) SFT data only: The LLM is trained solely on general SFT data (Ultrachat 200k). (2) SFT data + RAG-specific data: The LLM is trained on a combination of Ultrachat 200k and 50k RAG-specific data (the same data used in Figure 5). We evaluate the resulting models on MT-Bench to assess their general language capabilities and on NQ and TriviaQA to measure their RAG performance."}]}