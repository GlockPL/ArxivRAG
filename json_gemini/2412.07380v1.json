{"title": "SPECFUSE: ENSEMBLING LARGE LANGUAGE MODELS VIA NEXT-SEGMENT PREDICTION", "authors": ["Bo Lv", "Chen Tang", "Yanan Zhang", "Xin Liu", "Yue Yu", "Ping Luo"], "abstract": "Ensembles of generative large language models (LLMs) can integrate the strengths of different LLMs to compensate for the limitations of individual models. However, recent work has focused on training an additional fusion model to combine complete responses from multiple LLMs, failing to tap into their collaborative potential to generate higher-quality responses. Moreover, as the additional fusion model is trained on a specialized dataset, these methods struggle with generalizing to open-domain queries from online users. In this paper, we propose SpecFuse, a novel ensemble framework that outputs the fused result by iteratively producing the next segment through collaboration among LLMs. This is achieved through cyclic execution of its inference and verification components. In each round, the inference component invokes each base LLM to generate candidate segments in parallel, and the verify component calls these LLMs again to predict the ranking of the segments. The top-ranked segment is then broadcast to all LLMs, encouraging them to generate higher-quality segments in the next round. This approach also allows the base LLMs to be plug-and-play, without any training or adaptation, avoiding generalization limitations. Furthermore, to conserve computational resources, we propose a model exit mechanism that dynamically excludes models exhibiting poor performance in previous rounds during each query response. In this way, it effectively reduces the number of model calls while maintaining overall performance. We conduct extensive experiments using ensembles of five LLMs with different architectures across six benchmarks, covering instruction-response, reasoning, commonsense, and instruction-following tasks. The experimental results demonstrate that SpecFuse consistently enhances performance across all benchmarks, with RougeL scores improving by +3.1 on the Chinese and +3.0 on the English human-computer interaction benchmarks. Furthermore, the model exit mechanism reduces the average models invoked per round from 5 to 2.4, with only a slight reduction in performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Generative large language models (LLMs) (Brown et al., 2020; Yang et al., 2024) have been widely applied attributed to their impressive performance across various domains, providing efficient support for a broad range of user needs. These off-the-shelf generative LLMs specialize in different areas due to differences in training data and model architecture. Therefore, by combining their strengths, an ensemble of LLMs (Yang et al., 2023) can alleviate the biases and errors of individual models, delivering a better user experience. Unfortunately, vocabulary discrepancies across different LLMs limit the application of traditional logits-based fusion methods (Schapire & Freund, 2013; Sagi & Rokach, 2018) in the integration of generative LLMs.\nRecent research on ensembling generative LLMs can be divided into two categories: post-hoc ensemble methods and pre-selection ensemble methods. The post-hoc ensemble method (Jiang et al.,"}, {"title": "2 METHODOLOGY", "content": "In the following sections, we first introduce the overall framework of SpecFuse, followed by a detailed explanation of its three parts: the Inference component, the Verify component, and the Model Exit mechanism."}, {"title": "2.1 OVERVIEW", "content": "Figure 1 shows an overview of SpecFuse. Given K base LLMs $M = \\{m_i\\}_{i=1}^{K}$ and an input I, for each round in the generation process, SpecFuse first invokes the Inference component (\u00a7 2.2), where the base LLMs in M generate candidate segments in parallel. Then, it calls the Verify component (\u00a7 2.3) to score each candidate segment, selecting the highest-scoring one as the current round"}, {"title": "2.2 INFERENCE COMPONENT", "content": "Given a maximum length L for the candidate segments generated at each round, the Inference component parallelly invokes each model in M to generate candidate segments $\\{C_i\\}$, extending from input I, and with a length not exceeding L, where $|M|$ denotes the number of models in M. The probabilities corresponding to each token in the candidate $C_i$, are averaged to produce the self-score $S_i$ of the model $m_i$ :\n$S_i = \\frac{\\sum_{n=1}^{L_i} x_n}{L_i}$\nwhere $L_i$ represents the actual length of candidate $C_i$, as the generated candidates may be shorter than L in the final round. x is the probability obtained by applying Softmax normalization to the logits output by model $m_i$ when generating the n-th token in $C_i$. Finally, each model's generated candidate segment, together with its corresponding self-score, is input into the Verify component.\nWe include the model's self-score, as the do-sample method used by generative LLMs involves high randomness and may not yield the model's best segment. If the model scores other candidates higher than its own output, it indicates that its generated text is of lower quality."}, {"title": "2.3 VERIFY COMPONENT", "content": "The Verify component first concatenates each candidate segment $C_i$ with the Input I, forming the concatenated text $\\bar{C_i}$. These concatenated texts $\\{\\bar{C_i}\\}_{i=1}^{|M|}$ are then grouped into a batch, with each model's own generated candidate being removed from the batch to reduce computational load. Next, the Verify component enables all models to compute the probability of each token in the input text in parallel. Similar to obtaining $S_i$, the probabilities of each token in segment $C_i$ predicted by model $m_j$ are averaged to compute the sequence score $S_i^{mj}$, representing the evaluation of $C_i$ by model $m_j$. For each candidate $C_i$, its self-score and the scores from other models are averaged to obtain its quality score, denoted as $S_i$:\n$S_i = \\frac{\\sum_{j=1}^{|M|} S_i^{mj}}{|M|}$\nFinally, the candidate segment with the highest quality score is selected as the output presented to users for the current round.\nIn the implementation process, we use key-values cache to reduce redundant computations of previous text in both the verification and inference stages, improving the inference speed in each round.\nCollaborating between the inference and verification components to generate the next segment not only alleviates low-quality responses caused by a single model's unfamiliarity with the user's question but also reduces instability from sampling during generation. Furthermore, incorporating the best candidate segment as input for the next round can stimulate other models, and throughout the multi-round generation process, models can continuously inspire one another, ultimately leading to higher-quality responses."}, {"title": "2.4 MODEL EXIT MECHANISM", "content": "While model ensembles can provide users with more stable and higher-quality responses, they also come with increased computational resource demands and costs. To reduce the computational overhead without compromising performance, we propose a Model Exit mechanism. The motivation for this approach stems from our observation that, when responding to a query, some models' output segments rarely rank first. This indicates that these models are not well-suited for responding to the given query, making further computational investment in them inefficient. We use the cumulative scores from previous rounds of each model as prior estimates of quality in subsequent rounds to determine whether a model should be exited. Since the number of rounds varies for different queries, a fixed threshold cannot be used for exit decisions. Therefore, we apply the softmax function to normalize the scores and set a temperature coefficient of $\\sqrt{VT}$ (T being the current round). We choose $\\sqrt{VT}$ as the temperature coefficient because the number of output rounds rarely exceeds 100. By using $\\sqrt{VT}$, we effectively limit the cumulative scores to under 10, preventing extreme values from dominating. Additionally, we analyze the distribution of the best candidate segments. When these segments belong to only a few models, other models can be exited. By combining the temperature coefficient with the best segment distribution, the softmax scores more accurately reflect model performance, allowing underperforming models to exit promptly.\nSpecifically, we use $Q_i = \\sum_{t=1}^{T}S_i^{t}$ to represent the cumulative quality score of the candidate segments generated by model mi from the first step to the current T-th step. Next, we count the number of times each model ranked first in previous steps and weighted them based on the step intervals from the current step: within 4 steps by 1, 4 to 8 steps by 3/4, 8 to 12 steps by 1/2, and beyond 12 steps by 1/4, resulting in a weighted count $r_i$ for each model $m_i$. We introduced positional weights when calculating $r_i$ to prioritize recent steps, since models that perform well only in distant steps are less relevant to future responses. The $r_i$ is then normalized to create a distribution $P_r = \\{r_i\\}_{i=1}^{M}$, where $\\bar{r_i}$ is calculated as follows:\n$\\bar{r_i} = \\frac{r_i}{\\sum_{j=1}^{M} r_j}$\nNext, we use entropy to measure the uncertainty of the distribution, where for an n-model distribution, the entropy ranges from [0, log n]. To facilitate further processing, we normalize the entropy"}, {"title": "3 EXPERIMENTS", "content": "3.1 SETUPS\nEvaluation datasets. We evaluate all the models on six datasets that represent different core capa- bilities of LLMs, open-domain instruction-response (IR), commonsense, reasoning and instruction following."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduce SpecFuse, a novel ensemble framework that generates fused outputs by iteratively producing the next segment through collaboration among LLMs, allowing base LLMs to be seamlessly integrated without any training or adaptation. Additionally, SpecFuse employs a model exit mechanism that dynamically excludes underperforming models in previous rounds during query responses, reducing computational costs. Experimental results across six benchmarks demonstrate that SpecFuse consistently delivers more stable performance compared to single LLMs and previous ensemble methods. We hope our work inspires further research on online model ensemble, improving the quality of responses delivered to users based on existing LLMs."}, {"title": "A DATASET DETAILS", "content": "The following provides a detailed description of the evaluation of instruction-response capability using the datasets.\nFor the English dataset, we choose the Alpaca-gpt4 (Peng et al., 2023) and Dolly-15k (Conover et al., 2023) datasets for evaluation, both of which have inputs that consist of human instructions. We select these two datasets because their response sources differ: the Dolly-15k dataset features human-provided responses, while the Alpaca-GPT-4 dataset contains responses generated by the state-of-the-art GPT-4 (OpenAI et al., 2024) model, which provides neutral answers to each question and can refuse to answer inappropriate or harmful ones. Using both types of responses for scoring allows us to more thoroughly compare the advantages of our ensemble system. Additionally, due to the large size of these datasets, we randomly sample portions from each to create a new test set. From the Dolly-15k dataset, we randomly select 1,500 open-QA samples for testing, with 500 reserved for the development set. In the Alpaca-GPT-4 dataset, after shuffling the data, we manually verify the correctness of GPT-4's responses and select 2,000 validated samples, with 1,500 used for testing and 500 for validation.\nFor the Chinese dataset, we utilize the Human-Value and Ruozb datasets from the COIG-CQIA (Bai et al., 2024) benchmark for testing. The instructions in these two datasets consist of human-posed questions, with answers provided either by humans or generated by GPT-4. The COIA authors manually review and filter the responses, retaining only the correct answers generated by GPT-4."}, {"title": "B EVALUATION METHODS", "content": "To evaluate the quality of our framework's responses to human questions in the dataset, a range of metrics assessing model generation capabilities are selected for the following experiments.\n\u2022 BLEU (B-n) (Papineni et al., 2002) and ROUGE (R-n) (Lin, 2004) compare a generated response with a reference by calculating n-gram overlap. For the Chinese results, we use Jieba3 to split the text into words before calculating these two scores.\n\u2022 BERTScore Zhang et al. (2019) (comprising Precision, Recall, and F1-score) measures the similarity between two texts based on the contextualized embedding from BERT (Devlin et al., 2019). In this paper, we report the F1 score of BERTScore.\n\u2022 BARTScore (Yuan et al., 2021) is a unified evaluator which evaluates with the average likelihood of the pretrained encoder-decoder model, BART (Lewis et al., 2019). It can predict different scores depending on the formats of the source and target.\n\u2022 The GPT4-Rank evaluation utilizes the GPT-44 model to compare two different responses against a ground-truth response. The model will select the better of the two responses. For each test sample, we pair the responses generated by different models and have GPT-4 determine which one is superior. Since the MBR and PairRank methods do not generate new responses, we do not re-rank the responses they select from the base LLMs. Instead, we use the average rankings of the responses they select from the base LLMs to represent their GPT4-Rank. Once all comparisons are complete, we count the number of wins for each model. Based on these win counts, we rank the responses from the different models. The average ranking of each model across all data in the dataset is the value reported in our table. The evaluation instructions for GPT-4 are shown in Table 6."}, {"title": "C CASE STUDY", "content": "Table 5 presents a case from the SpecFuse workflow where the user's request is \u201cWrite a simile to describe a person who is hard-working.\u201d The reasoning process goes through four iterations, and the Verify model's selection of the best candidate is not always from the same model. In the first round, the best candidate is generated by Qwen2. In the second round, Mistral, after receiving Qwen2's output from the previous round, is inspired and generates a response that better meets the user's"}]}