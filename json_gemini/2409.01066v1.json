{"title": "Learning in Hybrid Active Inference Models", "authors": ["Poppy Collis", "Ryan Singh", "Paul F Kinghorn", "Christopher L Buckley"], "abstract": "An open problem in artificial intelligence is how systems can flexibly learn discrete abstractions that are useful for solving inherently continuous problems. Previous work in computational neuroscience has considered this functional integration of discrete and continuous variables during decision-making under the formalism of active inference [13,29]. However, their focus is on the expressive physical implementation of categorical decisions and the hierarchical mixed generative model is assumed to be known. As a consequence, it is unclear how this framework might be extended to the learning of appropriate coarse-grained variables for a given task. In light of this, we present a novel hierarchical hybrid active inference agent in which a high-level discrete active inference planner sits above a low-level continuous active inference controller. We make use of recent work in recurrent switching linear dynamical systems (rSLDS) which learn meaningful discrete representations of complex continuous dynamics via piecewise linear decomposition [22]. The representations learnt by the rSLDS inform the structure of the hybrid decision-making agent and allow us to (1) lift decision-making into the discrete domain enabling us to exploit information-theoretic exploration bonuses (2) specify temporally-abstracted sub-goals in a method reminiscent of the options framework [34] and (3) 'cache' the approximate solutions to low-level problems in the discrete planner. We apply our model to the sparse Continuous Mountain Car task, demonstrating fast system identification via enhanced exploration and successful planning through the delineation of abstract sub-goals.", "sections": [{"title": "1 Introduction", "content": "In a world that is inherently high-dimensional and continuous, the brain's capacity to distil and reason about discrete concepts represents a highly desirable feature in the design of autonomous systems. Humans are able to flexibly specify abstract sub-goals during planning, thereby reducing complex problems into manageable chunks [26,16]. Indeed, translating problems into discrete space offers distinct advantages in decision-making systems. For one, discrete states admit the direct implementation of classical techniques from decision theory such as dynamic programming [21]. Furthermore, we also find the computationally feasible application of information-theoretic measures (e.g. information-gain) in discrete spaces. Such measures (generally) require approximations in continuous settings but these have closed-form solutions in the discrete case [12]. While the prevailing method for translating continuous variables into discrete representations involves the simple grid-based discretisation of the state-space, this becomes extremely costly as the dimensionality increases [7,24]. We therefore seek to develop a framework which is able to smoothly handle the presence of continuous variables whilst maintaining the benefits of decision-making in the discrete domain."}, {"title": "1.1 Hybrid Active Inference", "content": "Here, we draw on recent work in active inference (AIF) which has foregrounded the utility of decision-making in discrete state-spaces [8,12]. Additionally, discrete AIF has been successfully combined with low-level continuous representations and used to model a range of complex behaviour including speech production, oculomotion and tool use [13,29,14,30,31]. As detailed in [13], such mixed generative models focus on the physical implementation of categorical decisions. This treatment begins with the premise that the world can be described by a set of discrete states evolving autonomously and driving the low-level"}, {"title": "1.2 Recurrent Switching Systems", "content": "Previous work has demonstrated that models involving autonomous switching systems are often not sufficiently expressive to approximate realistic generative processes [22]. They study this problem in the context of a class of hybrid state-space model known as switching linear dynamical systems (SLDS). These models have been shown to discover meaningful behavioural modes and their causal states via the piecewise linear decomposition of complex continuous dynamics [15,11]. The authors of [22] remedy the problem associated with limited expressivity by introducing recurrent switching linear dynamical systems (rSLDS) (see Fig. 2). These models importantly include a dependency from the underlying continuous variables in the high-level discrete transition probabilities. By providing an understanding of the continuous latent causes of switches between the discrete states via this additional dependency, the authors demonstrate improved generative capacity and predictive performance. We propose this richer"}, {"title": "1.3 Emergent descriptions for planning", "content": "Unfortunately, the inclusion of recurrent dependencies also destroys the neat separation of discrete planning from continuous control, creating unique challenges in performing roll-outs. Our central insight is to re-instate the separation by lifting the dynamical system into the discrete domain only during planning. We do this by approximately integrating out the continuous variables, naturally leading to spatio-temporally abstracted actions and sub-goals. Our discrete planner therefore operates purely at the level of a re-description of the discrete latents, modelling nothing of the autonomous transition probabilities but rather reflecting transitions that are possible given the discretisation of the continuous state-space. In short, we describe a novel hybrid hierarchical active inference agent [28] in which a discrete Markov decision process (MDP), informed by the representations of an rSLDS, interfaces with a continuous active inference controller implementing closed-loop control. We demonstrate the efficacy of this algorithm by applying it to the classic control task of Continuous Mountain Car [27]. We show that the exploratory bonuses afforded by the emergent discrete piecewise description of the task-space facilitates fast system identification. Moreover, the learnt representations enable the agent to successfully solve this non-trivial planning problem by specifying a series of abstract subgoals."}, {"title": "2 Related work", "content": "Such temporal abstractions are the focus of Hierarchical reinforcement learning (HRL), where high-level controllers provide the means for reasoning beyond the clock-rate of the low-level controllers primitive actions. [10,34,9,18]. The majority of HRL methods, however, depend on domain expertise to construct tasks, often through manually predefined subgoals as seen in [35]. Further, efforts to learn hierarchies directly in a sparse environment have typically been unsuccessful [36]. In contrast, our abstractions are a natural consequence of lifting the problem into the discrete domain and can be learnt independently of reward. In the context of control, hybrid models in the form of piecewise affine (PWA) systems have been rigorously examined and are widely applied in real-world scenarios [33,3,6]. Previous work has applied a variant on rSLDS (recurrent autoregressive hidden Markov models) to the optimal control of general nonlinear systems [2,1]. The authors use these models to the approximate expert controllers in a closed-loop behavioural cloning context. While their algorithm focuses on value function approximation, in contrast, we learn online without expert data and focus on flexible discrete planning."}, {"title": "3 Framework", "content": "The following sections detail the components of our Hierachical Hybrid Agent (HHA). For additional information, please refer to Appendix. A."}, {"title": "3.1 Generative Model: rSLDS(ro)", "content": "In the recurrent-only (ro) formulation of the rSLDS (see Fig. 2), the discrete latent states $z_t \\in \\{1,2,...,K\\}$ are generated as a function of the continuous latents $x_t \\in \\mathbb{R}^M$ and the control input $u_t \\in \\mathbb{R}^N$ (specified by some controller) via a softmax regression model,\n$P(Z_{t+1}|X_t,u_t)=softmax(W_x x_t + W_u u_t + r)$                                                                  (1)\nwhereby $W_x \\in \\mathbb{R}^{K \\times M}$ and $W_u \\in \\mathbb{R}^{K \\times N}$ are weight matrices and $r$ is a bias of size $\\mathbb{R}^K$. The continuous latent states $x_t$ evolve according to a linear dynamical system indexed by the current discrete state $z_t$.\n$X_{t+1}|X_t, u_t,z_t = A_{z_t}X_t + B_{z_t}u_t + b_{z_t}+v_t,$\n$v_t \\sim \\mathcal{N}(0,Q_{z_t})$                                                                                    (2)\n$Y_t|x_t=C_{z_t}X_t+w_t, \\space w_t \\sim \\mathcal{N}(0,S_{z_t})$                                                                                   (3)\n$A_{z_t}$ is the state transition matrix, which defines how the state $x_t$ evolves in the absence of input. $B_{z_t}$ is the control matrix which defines how external inputs influence the state of the system while $b_{z_t}$ is an offset vector. At each time-step $t$, we observe an observation $y_t \\in \\mathbb{R}^M$ produced by a simple linear-Gaussian emission model with an identity matrix $C_{z_t}$. Both the dynamics of the continuous latents and the observations are perturbed by zero-mean Gaussian noise with covariance matrices of $Q_{z_t}$ and $S_{z_t}$ respectively.\nInference requires approximate methods given that the recurrent connections break conjugacy render-ing the conditional likelihoods non-Gaussian. Therefore, a Laplace Variational Expectation Maximisation (EM) algorithm is used to approximate the posterior distribution over the latent variables by a mean-field factorisation into separate distributions for the discrete states $q(z)$ and the continuous states $q(x)$. The discrete state is updated via a coordinate ascent variational inference (CAVI) approach by leveraging the forward-backward algorithm. The continuous state distribution is updated using a Laplace approximation around the mode of the expected log joint probability. This involves finding the most likely continuous latent states by maximizing the expected log joint probability and computing the Hessian to approximate the posterior. Full details of the Laplace Variational EM used for learning are given in [37].\nThe rSLDS is initialised according to the procedure outlined in [22]. In order to learn the rSLDS parameters using Bayesian updates, conjugate matrix normal inverse Wishart (MNIW) priors are placed on the parameters of the dynamical system and recurrence weights. We learn the parameters online via observing the behavioural trajectories of the agent and updating the parameters in batches (every 1000 timesteps of the environment)."}, {"title": "3.2 Active Inference", "content": "Equipped with a generative model, active inference specifies how an agent can solve decision making tasks [28]. Policy selection is formulated as a search procedure in which a free energy functional of predicted states is evaluated for each possible policy. Formally, we use an upper bound on the expected free energy (G) given by:\n$G_{1:T}(\\pi) \\leq -E_{Q(o|\\pi)}[D_{KL}[Q(S|o,\\pi) || Q(s|\\pi)]]$ State Information Gain\n$-E_{Q(o|\\pi)}[D_{KL}[Q(o|o,\\pi) || Q(o|\\pi)]]$Parameter Information Gain\n$-E_{Q(o|\\pi)}[ln p(o)].$ Utility                                                                 (4)"}, {"title": "3.3 Discrete Planner", "content": "In order to create approximate plans at the discrete level, we derive a high-level planner based on a re-description of the discrete latent states found by the rSLDS by approximately \u2018integrating out' the continuous variables and the continuous prior. This process involves calculating the expected free energy (G) for a continuous controller to drive the system from one mode to another. Importantly, the structure of the lifted discrete state transition model has been constrained by the polyhedral partition of the continuous state space extracted from the parameters of the rSLDS 3: invalid transitions are assigned zero probability while valid transitions are assigned a high probability. In order to generate the possible transitions from the rSLDS, we calculate the set of active constraints for each region from the softmax representation, $p(z|x)=\\sigma(Wx+b)$. Specifically, to check that the region i is adjacent to region j, we verify the solution using a linear program,\n$-b_j = min (W_i - W_j)x$                                                                                         (5)\ns.t. $(W_i - W_k)x \\leq (b_i-b_k) \\forall k \\in [K]$\ns.t. $x \\in (x_{lb},x_{ub})$                                                                                           (6)\nwhere $(x_{lb}, x_{ub})$ are bounds chosen to reflect realistic values for the problem. This ensures we only lift transitions to the discrete model if they are possible. After integration, we are left with a discrete MDP which contains averaged information about all of the underlying continuous quantities. This includes information about the transitions that the structure of the task space allows, and their corresponding approximate control costs (see A.2). Note that after each batch update of the rSLDS parameters, this discrete planner must be refitted accordingly.\nThe lifted discrete generative model has all the components of a standard POMDP in the active inference framework:\n$P(o_{1:T},s_{1:T},A,B,\\pi)=P(\\pi)P(A)P(B)P(s_0)\\Pi P(s_t|s_{t-1},B,\\pi)P(o_t|s_t,A)$                                   (8)\nalong with prior over policies $P(\\pi)=Cat(E)$, and preference distribution $P(o_t)=Cat(C)$. Specifically our lifted $P(\\pi)$ reflects the approximate control costs of each continuous transition and $P(o_t)$ reflects the reward available in each mode. We assume an identity mapping between states and observation meaning the state information gain term in Eq. 4 collapses into a maximum entropy regulariser, while we maintain Dirichlet priors over the transition parameters B, facilitating directed exploration. Due to conjugate structure Bayesian updates amount to a simple count-based update of the Dirichlet parameters [25]. At each time step, the discrete planner selects a policy by sampling from the following distribution:\n$Q(\\pi)=softmax(-G(\\pi)+lnP(\\pi)).$                                                                                                                             (9)"}, {"title": "3.4 Continuous controller", "content": "Continuous closed-loop control is handled by a set of continuous active inference controllers. For controlling the transition from mode i to mode j ($x_i$ to $x_j$), the objective of the controller is to minimise the following (discrete-time) expected free energy functional 5:\n$G_{ij}(\\pi)=E_{q(\\cdot|x_0 = x_i,\\pi)} \\Big[(x_s - x_j)^T Q_f (x_s-x_j) + \\Sigma_{t=0}^S u_t^T (R^{-1} -I)u_t \\Big] + Indet II$            (11)"}, {"title": "4 Results", "content": "To evaluate the performance of our (HHA) model, we applied it to the classic control problem of Continuous Mountain Car. This problem is particularly relevant for our purposes due to the sparse nature of the rewards, necessitating effective exploration strategies to achieve good performance. We find that the HHA finds piecewise affine approximations of the task-space and uses these discrete modes effectively to solve the task. Fig. 4 shows that while the rSLDS has divided up the space according to position, velocity and"}, {"title": "5 Discussion", "content": "The emergence of non-grid discretisations of the state-space allows us to perform fast systems identifi-cation via enhanced exploration, and successful non-trivial planning through the delineation of abstract sub-goals. Hence, the time spent exploring each region is not based on euclidean volume which helps mitigate the curse of dimensionality that other grid-based methods suffer from. Interestingly, even without information-gain, the area covered by our hybrid hierarchical agent is still notably better than that of the random continuous action control (see Fig. 5c). This is because the agent is still operating at the level of the non-grid discretisation of the state-space which acts to significantly reduce the dimensionality of the search space in a behaviourally relevant way.\nSuch a piecewise affine approximation of the space will incur some loss of optimality in the long run when pitted against black-box approximators. This is due to the nature of caching only approximate closed-loop solutions to control within each piecewise region, whilst the discrete planner implements open-loop control. However, this approach eases the online computational burden for flexible re-planning. Hence, in the presence of noise or perturbations within a region, the controller may adapt without any new computation. This is in contrast to other nonlinear model-based algorithms like model-predictive control where reacting to disturbances requires expensive trajectory optimisation at every step [32]. By using the piecewise affine framework, we maintain functional simplicity and interpretability through structured representation. We therefore suggest that this method is amenable to future alignment with a control-theoretic approach to safety guarantees for ensuring robust system performance and reliability. Indeed, such use of discrete approx-imations to continuous trajectories has been shown to improve the ability to handle uncertainty. Evidence of the efficacy of this kind of approach in machine learning applications has been exhibited in recent work by [5], which examined the problem of compounding error in imitation learning from expert demonstration. The authors demonstrated that applying a set of primitive controllers to discrete approximations of the ex-pert trajectory effectively mitigated the accumulation of error by ensuring local stability within each chunk.\nWe acknowledge there may be better solutions to dealing with control input constraints than the one given in Sec. 3.4. Different approaches have been taken to the problem of implementing constrained-LQR control, such as further piecewise approximation based on defining reachability regions for the controller [4]."}, {"title": "6 Conclusion", "content": "In summary, the successful application of our hybrid hierarchical active inference agent in the Continuous Mountain Car problem showcases the potential of recurrent switching linear dynamical systems (rSLDS) for enhancing decision-making and control in complex environments. By leveraging rSLDS to discover meaning-ful coarse-grained representations of continuous dynamics, our approach facilitates efficient system identifi-cation and the formulation of abstract sub-goals that drive effective planning. This method reveals a promis-ing pathway for the end-to-end learning of hierarchical mixed generative models for active inference, provid-ing a framework for tackling a broad range of decision-making tasks that require the integration of discrete and continuous variables. The success of our agent in this control task demonstrates the value of such hybrid models in achieving both computational efficiency and flexibility in dynamic, high-dimensional settings."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Framework", "content": "Optimal Control To motivate our approximate hierarchical decomposition, we adopt the optimal control framework, specifically we consider discrete time state space dynamics of the form:\n$X_{t+1}=f(x_t,u_t,n_t)$                                                                                                                                                                                                                                           (12)\nwith known initial condition $x_o$, and noise $n_t$ drawn from some time invariant distribution $n_t \\sim D$, where we assume $f$ to be $p(x_{t+1}|X_t,u_t)$ and is a valid probability density throughout. We use $c_t : X \\times U \\rightarrow R$ for the control cost function at time $t$ and let $U$ be the set of admissible (non-anticipative, continuous) feedback control laws, possibly restricted by affine constraints. The optimal control law for the finite horizon problem is given as:\n$J(\\pi) = E_{x_o,\\pi}[\\Sigma_{t=0}^T c_t(x_t,u_t)]$                                                                                       (13)\n$\\pi^* = argmin_{ \\pi \\epsilon U}J(\\pi)$                                                                                                                                                                                                (14)\nPWA Optimal Control The fact we do not have access to the true dynamical system $f$ motivates the use of a piecewise affine (PWA) approximation. Also known as hybrid systems:\n$X_{t+1}=A_ix_t + B_iU_t + E_i$                                                                                                             (15)\nwhen $(x_t,u_t) \\in H_i$                                                                                                                (16)\nWhere $H = \\{H_i :i\\in [K]\\}$ is a polyhedral partition of the space $X \\times U$. In the case of a quadratic cost function, it can be shown the optimal control law for such a system is peicewise linear. Further there exist many completeness (universal approximation) type theorems for peicewise linear approximations implying if the original system is controllable, there will exist a peicewise affine approximation through which the system is still controllable [3,6].\nRelationship to rSLDS We perform a canonical decomposition of the control objective $J$ in terms of the components or modes of the system. By slight abuse of notation $[x_t=i]:= [(x_t,u_t) \\in H_i]$ represent the Iverson bracket.\n$J(\\pi)=\\Sigma \\int P_\\pi(x_t |x_{t-1},u_t)c_1(x_t,u_t) dx_t dx_{t-1}$                                                                                                                     (17)\n$=\\Sigma \\int \\Sigma[x_{t-1} = i] P_\\pi(x_t |x_{t-1},u_t) c_1(x_t,u_t) dx_t dx_{t-1}$                                                                                                    (18)\nNow let $z_t$ be the random variable on $K$ induced by $Z_t = i$ if $[x_t = i]$ we can rewrite the above more concisely as,\n$J(\\pi) = \\Sigma \\int \\Sigma P_\\pi(x_t |z_{t-1} = i \\,x_{t-1},u_t) c_1(x_t,u_t) dx_t dx_{t-1}$                                                                                                    (19)\n$=\\Sigma \\Sigma \\int P_\\pi(x_t | z_{t-1} = i \\,x_{t-1},u_t) c_1(x_t,u_t) dx_t dx_{t-1}$                                                                                                (20)\n$=\\Sigma \\Sigma E_\\pi [c_t(x_t,u_t)]$                                                                                                                                                   (21)"}, {"title": "A.2 Hierarchical Decomposition", "content": "Our aim was to decouple the discrete planning problem from the fast low-level controller. In order to break down the control objective in this manner, we first create a new discrete variable which simply tracks the transitions of z, this allows the discrete planner to function in a temporally abstracted manner. Decoupling from clock time Let the random variable $(\\tau_s)_{s>o}$ record the transitions of $(z_t)_{t>0}$ i.e. let\n$\\tau_s(\\tau_{s-1})=min \\{t:z_{t+1} \\neq z_t , t>\\tau_{s-1}\\}, \\tau_0=0$                                                                                                                                                  (22)\nbe the sequence of first exit times, then $\\zeta$ is given by $\\zeta_s=Z_{\\tau_s}$. With these variables in hand, we frame a small section of the global problem as a first exit problem.\nLow level problems Consider the first exit problem for exiting region i and entering j defined by:\n$\\tau_{ij} (x_o) = argmin_{ \\pi , S}J_{ij}(\\pi,x_o,S)$                                                                                                                                                    (23)\n$J_{ij}(\\pi,x_o,S) = E_{\\pi,x_o}[ \\Sigma_{t=0}^S c(x_t,u_t)]$                                                                                                                                             (24)\ns.t. $(x_t,u_t) \\in H_i$                                                                                                                                                                                              (25)\ns.t. $c(x,u)=0 \\space when (x,u)\\epsilon \\partial H_{ij}$                                                                                                              (26)\nwhere $\\partial H_{ij}$ is the boundary $H_i \\cap H_j$. Due, to convexity of the polyhedral partition, the full objective admits the decomposition in terms of these subproblems,\n$J(\\pi) = \\Sigma J_{\\zeta_{(s+1)},\\zeta_{(s)}} (\\pi, x_{\\tau_s}, \\tau_{s+1}-\\tau_s)$                                                                                                              (27)\nIdeally, we would like to simply solve all possible subproblems $\\{ J_{ij}(x): i,j\\epsilon [K] \\times [K]\\}$ and then find a sequence of discrete states, $(\\zeta_1,...,\\zeta_S)$, which minimises the sum of the sub-costs, however notice each sub-cost depends on the starting state, and further this is determined by the final state of the previous problem. A pure separation into discrete and continuous problems is not possible without a simplifying assumption.\nSlow and fast mode assumption The goal is to tackle the decomposed objectives individually, however the hidden constraint that the trajectories line up presents a computational challenge. Here we make the assumption that the difference in cost induced by different starting positions within a region is much less than expected difference in cost of starting in a different region. This assumption justifies using an average cost for the low-level problems to create the high-level problem.\nHigh level problem we let $J_{ij}=min_{\\pi} \\int J_{ij} (\\pi,x_o) p(x_o) dx_o$ be the average cost of each low-level problem. We form a Markov decision process by introducing abstract actions $a\\epsilon [K]$:\n$P_{ik}(a)=P(\\zeta_{s+1}=k | \\zeta_s=i, a=j, \\pi)$                                                                                                                                      (28)"}, {"title": "A.3 Offline Low Level Problems: Linear Quadratic Regulator (LQR)", "content": "Rather than solve the first-exit problem directly, we formulate an approximate problem by finding trajectories that end at specific \u2018control priors' (see A.6). Recall the low level problem given by:\n$\\tau_{ij} (x_o) = argmin_{ \\pi , S}J_{ij}(\\pi,x_o,S)$                                                                                                                                                    (31)\n$J_{ij}(\\pi,x_o,S) = E_{\\pi,x_o}[ \\Sigma_{t=0}^S c(x_t,u_t)]$                                                                                                                                             (32)\ns.t. $(x_t,u_t) \\in H_i$                                                                                                                                                                                              (33)\ns.t. $c(x,u)=0 \\space when (x,u)\\epsilon \\partial H_{ij}$                                                                                                              (34)\nIn order to approximate this problem with one solvable by a finite horizon LQR controller, we adopt a fixed goal state, $x^* \\epsilon H_j$. Imposing costs $c_t(x_t,u_t)=u_t^T R u_t$ and $c_S(x_S,u_S)=(x-x^*)^T Q_f (x-x^*)$. Formally we solve,\n$\\tau_{ij} (x_o) = argmin_{ \\pi , S}J_{ij}(\\pi,x_o,S)$                                                                                                                                                    (35)\n$J_{ij}(\\pi,x_o,S) = E_{\\pi,x_o}[(x_S - x^*)^T Q_f (x_S - x^*) + \\Sigma_{t=0}^{S-1} u_t^T R u_t ]$                                                                                                            (36)\n(37)\nby integrating the discrete Ricatti equation backwards. Numerically, we found optimising over different time horizons made little difference to the solution, so we opted to instead specify a fixed horizon (hyperparameter). These solutions are recomputed offline every time the linear system matrices change.\nDesigning the cost matrices Instead of imposing the state constraints explicitly, we record a high cost which informs the discrete controller to avoid them. In order to approximate the constrained input we choose a suitably large control cost R=rI. We adopted this approach for the sake of simplicity, potentially accepting a good deal of sub-optimality. However, we believe more involved methods for solving input constrained LQR could be used in future, e.g. [3], especially because we compute these solutions offline."}, {"title": "A.4 Active Inference Interpretation", "content": "Expected Free Energy Here we express the fully-observed continuous (discrete time) active inference controller, without mean-field assumptions, and show it reduces to a continuous quadratic regulator. Suppose we have a linear state space model:\n$X_{t+1}=Ax_t + Bu_t + E_t$                                                                                                                                                    (38)\nand a prior preference over trajectories $p(x_{1:T}) \\sim N(x_T;x_f, Q_F^{-1})$, active inference specifies the agent minimises\n$G(\\pi)=E_{q(x_{1:T},u_{1:T}; \\pi)}[-ln p(x_{1:T},u_{1:T})+ln q(x_{1:T},u_{1:T}; \\pi)]$                                                                                                        (39)\nNote, since all states are fully observed we have no ambiguity term. Where $p(x_{1:T},u_{1:T}) \\propto p(x_{1:T})P(x_{1:T}| u_{1:T})p(u_{1:T})$, the central term is the dynamics model and the prior over controls is also gaussian, $p(u_{1:T}) = \\Pi_t N(u_t;0,R^{-1})$. Finally, we adopt $q(x_{1:T},u_{1:T}; \\pi) = p(x_{1:T}|u_{1:T}) \\Pi_t \\pi_t(u_t | x_t)$, where we parametrise the variational distributions as $\\pi_t \\sim N(u_t; K_tx, \\Sigma)$ (where $K_t,\\Sigma$ are parameters to be optimised). The expected free energy thus simplifies to:\n$G(\\pi)=E_{q(x_{1:T},u_{1:T}; \\pi)} [(x_T-x_F)^T Q_f (x_T-x_F) -u_t^T (R+I)u_t ] + Indet II$                                                                                                                  (40)\nDynamic Programming (HJB) We proceed by dynamic programming, let the 'value' function be\n$V(x_k)=min_{ \\pi }E_{q(x_{k+1:T},u_{k:T}|x_k, \\pi)} [ \\Sigma_{t=k}^{k+1} (x_t - x_f)^T Q_f (x_t-x_f) + u_t^T (R+I)u_t] + Indet II$                                                                                                         (41)\nAs usual the value function satisfies a recursive property:\n$V(x_k)=min_{ \\pi }E_{q(x_{k+1},u_k|x_k, \\pi)} [u_k^T (R+I)u_k +V(x_{k+1}) ] + Indet II$                                                                                                                 (42)\nWe introduce the ansatz $V(x_k)=x_k^T S_k x_k$ leading to,\n$x_k^T S_k x_k = min_{ \\pi }E_{q(x_{k+1},u_k|x_k, \\pi)} [u_k^T (R+I)u_k + x_{k+1}^T S_{k+1}x_{k+1} ] + Indet II$                                                                                                 (43)\nFinally we take expectations, which are available in closed form, and solve for $\\Sigma_k$ and $K_k$:\n$x_k^T S_k x_k min \\{ x_k^T K^T R K_k x_k +tr(\\Sigma (R+\\Pi)) \\}$                                                                                                                                       (44)\n$+x_k^T (A+B K_k)^T S_{k+1} (A+B K_k) x_k+ tr(\\Sigma S_{k+1})+ Indet II$                                                                                                (45)\nSolving for $\\Sigma_k$ and substituting,\n$\\Sigma = (R+ \\Pi)^{-1}$                                                                                                                                                                                                      (46)\n$S_k = min_K \\{ K^T R K +(A+B K)^T S_{k+1} (A+B K) \\}$                                                                                                                                                           (47)\n$K_k = -(R+B^T S_{k+1} B)^{-1} B^T S_{k+1} A$                                                                                                                                                      (48)\nWhere $S_k$ follows the discrete algebraic Riccati equation (DARE).\nThus we recover $\\pi_k(u | x) \\sim N(K_k x, \\Sigma_k)$ where $K_t$ is the traditional LQR gain, and $E_t$ solves $\\Sigma_k = (R+ \\Pi_k)^{-1}$. Here we use the deterministic maximum-a-posterori \u2018MAP' controller Ktx. However the collection of posterior variance estimates adds a different total cost depending on the variance inherent in the dynamics which can be lifted to the discrete controller."}, {"title": "A.5 Online high level problem", "content": "The high level problem is a discrete MDP with a \u2018known' model", "23": "and optimise over open loop policies $\\pi=(a_o", "a_T)$.\n$G(a_{1": "T"}, "s_0)=E[\\Sigma_{t=0}^T R(s_t,a_t) + IG_p + IG_s |s_0,a_{1:T}"], "p_t(\\theta)=p(\\theta|s_{0": "t})$. In other words, we add a bonus when we expect the posterior to diverge from the prior, which is exactly the transitions we have observed least [19].\nWe also have a state information-gain term, $IG_s = D_{KL}[P_{t+1}(S_{t+1}) || P_{t}(S_{t+1})]$. In this case (fully observed), $P_{t+1}(s_{t+1})=\\delta_s$ is a one-hot vector. Leaving the term $E_t[-ln p_t(s_{t+1})]$ leading to a maximum entropy term [19].\nWe calculate the above with Monte Carlo"}