{"title": "HiDP: Hierarchical DNN Partitioning for Distributed Inference on Heterogeneous Edge Platforms", "authors": ["Zain Taufique", "Aman Vyas", "Antonio Miele", "Pasi Liljeberg", "Anil Kanduri"], "abstract": "Edge inference techniques partition and distribute Deep Neural Network (DNN) inference tasks among multiple edge nodes for low latency inference, without considering the core-level heterogeneity of edge nodes. Further, default DNN inference frameworks also do not fully utilize the resources of heterogeneous edge nodes, resulting in higher inference latency. In this work, we propose a hierarchical DNN partitioning strategy (HiDP) for distributed inference on heterogeneous edge nodes. Our strategy hierarchically partitions DNN workloads at both global and local levels by considering the core-level heterogeneity of edge nodes. We evaluated our proposed HiDP strategy against relevant distributed inference techniques over widely used DNN models on commercial edge devices. On average our strategy achieved 38% lower latency, 46% lower energy, and 56% higher throughput in comparison with other relevant approaches.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep Neural Networks (DNNs) enable a wide range of applications such as augmented reality, smart glasses, and live video analytics, etc [1]. Such applications demand real-time low latency DNN inference over continuous streaming input data [2]. Offloading DNN inference to remote cloud servers can lead to unpredictable latency with communication penalty, while local edge devices have limited compute capabilities to provide low latency inference [3]. Edge inference techniques distribute the inference workload among a cluster of collocated edge nodes to improve DNN inference latency [3]\u2013[5].\nExisting distributed edge inference strategies partition the inference workload into blocks by splitting the layers of a DNN model [6]\u2013[12] and/or the input data of the DNN in-ference request [3], [4], [13]\u2013[16]. Subsequently, these blocks are distributed to different edge nodes within the edge cluster, based on the compute capacity of an edge node [3], [4], [17] and the computation-communication ratio of a block [2], [5], [7]. It should be noted that edge nodes are composed of heterogeneous processing units including CPU, GPU, and Neural Processing Units (NPUs), exhibiting a high degree of compute diversity within the single node and across the edge cluster. However, existing distributed inference techniques make global decisions on DNN partitioning and distribution, without considering the core-level heterogeneity of individual edge nodes.\nAfter the creation and distribution of blocks, aforementioned distributed inference techniques rely on deep learning frameworks to schedule the inference service on a local edge device. Deep learning frameworks do not fully utilize the resources of a heterogeneous multi-core edge node, leading to sub-optimal inference latency. For example, TensorFlow [18] schedules inference on GPU by default, unless explicitly specified by the application to use other CPUs/NPUs [19], [20]. Running inference on a single processing unit (e.g. GPU) misrepresents the compute capacity of the local heterogeneous edge node, resulting in sub-optimal workload partitioning and distribution decisions made on a global level. This problem is emphasized on edge platforms with CPUs performing better than GPUs [21] [10], and while running CPU-friendly layers of DNN models [22]. Advanced inference strategies that consider core-level heterogeneity are tailored for inference on a single edge node [10] [22] [1]. Adapting heterogeneity-aware DNN scheduling techniques for distributed inference requires intelligent orchestration to jointly optimize DNN partitioning and workload distribution, along with infrastructural support for inter-node data and decision control transfer. However, existing distributed inference strategies lack such intelligent orchestration.\nWe demonstrate the limitations of existing distributed inference techniques over four DNN models run on the Jetson TX2 platform [23]. State-of-the-art distributed"}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "For distributed inference, DNNs can be partitioned model-wise [4] and data-wise [16]. In model partitioning, DNN layers are dynamically grouped into executable blocks; these blocks are offloaded to multiple devices for distributed execution in a pipelined fashion. This strategy is inherently temporal since layers across different DNN blocks are executed sequentially. Minimizing inference latency requires creation of variable-sized DNN blocks dynamically by considering resources across the edge cluster. Model partitioning is feasible for dense DNN models that enable coarse creation of compute-intensive blocks. Alternatively, data partitioning splits the input data and creates smaller-sized sub-models of the original model for parallel execution. Data partitioned inference is spatio-temporal, since parallelly executed sub-models exchange intermediate data to maintain accuracy. Workloads with larger input sizes are suitable for data partitioning, considering the computation-communication ratio of intermediate data sharing. Minimizing inference latency of a DNN model on edge clusters requires joint selection of feasible partitioning strategy, optimal partitioning points, and workload distribution and scheduling."}, {"title": "A. DNN partitioning", "content": "For distributed inference, DNNs can be partitioned model-wise [4] and data-wise [16]. In model partitioning, DNN layers are dynamically grouped into executable blocks; these blocks are offloaded to multiple devices for distributed execution in a pipelined fashion. This strategy is inherently temporal since layers across different DNN blocks are executed sequentially. Minimizing inference latency requires creation of variable-sized DNN blocks dynamically by considering resources across the edge cluster. Model partitioning is feasible for dense DNN models that enable coarse creation of compute-intensive blocks. Alternatively, data partitioning splits the input data and creates smaller-sized sub-models of the original model for parallel execution. Data partitioned inference is spatio-temporal, since parallelly executed sub-models exchange intermediate data to maintain accuracy. Workloads with larger input sizes are suitable for data partitioning, considering the computation-communication ratio of intermediate data sharing. Minimizing inference latency of a DNN model on edge clusters requires joint selection of feasible partitioning strategy, optimal partitioning points, and workload distribution and scheduling."}, {"title": "B. Motivational Example", "content": "We demonstrate the benefits of hierarchical DNN partitioning in comparison with global partitioning strategy over a motivational scenario."}, {"title": "C. Related Work", "content": "Existing distributed inference techniques use DNN model partitioning to schedule inference workload over multiple edge nodes in a pipelined fashion [1], [10]\u2013[12] or offload the inference service to resourceful cloud [6]\u2013[9]. Pipelining the model partitioned DNN inference is sequential and is beneficial for dense DNN models with a continuous stream of inference requests. Other techniques focus on data partitioning by splitting the input data into batches [17], sub-images [4], [15], [16], or intermediate layers through Fused Tile Partitioning (FTP) [3], [14]. Data partitioning allows parallel and distributed inference. However, the communication overhead of intermediate data exchange becomes significantly high for DNNs with a smaller input size. Recently, DisNet [5] proposed hybrid partitioning for DNN inference to minimize the overheads of both partitioning techniques without considering granular control over local device resources. HiDP performs hybrid partitioning using model and data partitioning while having fine-grained control over local edge node's resources to minimize inference latency."}, {"title": "III. HIDP FRAMEWORK", "content": "HiDP is a 2-step workload splitting framework for distributed DNN inference across heterogeneous edge clusters. The nodes are connected via wireless networks and can collaborate at run-time to perform inference tasks. The DNN inference requests arrive randomly at a local node and the HiDP framework performs hierarchical partitioning to achieve low inference latency. As shown in the left side of Figure 3, the framework includes Application Module, Communication Module, Global partitioner, Local partitioner, and a Run-time scheduler. The right side of the figure shows the scenario of distributed DNN inference using HiDP framework across a heterogeneous cluster of four edge nodes. The HiDP framework receives a DNN inference request in the Application module of Device-1 and sends the DNN to Run-time scheduler. We designed a scheduling policy in the Run-time scheduler module that monitors and controls the workload splitting and distribution across the edge cluster. The Run-time scheduler gets the status of the cluster-wide node availability and invokes the Global partitioner to find the optimal partitioning point. The Global partitioner consults a Design Space Exploration (DSE) agent to find the optimal partitioning mode and the feasible partitions. The Global partitioner selects model partitioning; then it distributes the workload across the edge cluster via Communication Module. The Communication Module pro-vides access to send and receive data across the edge cluster. The Global partitioner sends the local partition to the Run-time scheduler for local execution. The Run-time scheduler invokes the Local partitioner to find the optimal partitioning point and mode using a DSE agent. The Local partitioner selects data partitioning and splits the workload following heterogeneity of two CPUs and one GPU. The Run-time scheduler gets the local and global results via Communication Module and merges all the results.\nPlatform. The edge nodes are supported by an Operating System (OS) with relevant libraries and programming framework to run DNN inference, allowing inter-node communication, workload scheduling, and application-to-core mapping. The OS provides interfaces between software-software modules to exchange data, and software-hardware modules to bind applications to selected processors. Unlike SoA strategies, HiDP overtakes the control from default OS governors and allocates the workload to the desired processing units. The OS allows run-time monitoring of the board's power consumption through onboard sensors or external power monitoring equip-ment to measure the energy consumption of a DNN inference.\nWorkloads. We have designed HiDP to accept unknown DNN inference workload without prior knowledge of the workload arrival time. The target applications are streaming applications that generate video and image data for live processing. These applications generate scenario-based input data with variable input sizes and batches. We consider the modern example scenario of a person bearing different smart gadgets and wearables including a smartwatch, smartphone, smart ring, and augmented reality gear. Manufacturers like Apple and Samsung have produced a series of smart devices that can communicate with each other at run-time sharing notifications and live data for a single user. These devices have diverse DNN applications that perform cognitive vision tasks of variable input sizes and data volume using similar DNN models depending on the requirements of the application tasks.\nSystem Model. We consider DNN model as Directed Acyclic Graph (DAG) since the data flow is sequential without loops and each partition is executed only once. The DAG nodes represent the DNN layers and the edges represent the tensors. The DNN is denoted as $D(L_i) = \\{L_1, L_2, ..., L_i\\}$, where $L$ represents set of layers that can be partitioned following the model and data partitioning. The layer types include convolution, pooling, flatten, or dense, where each layer can be represented as a vector of kernel size, stride, padding, number of input channels, number of output channels, and height of the input dimensions. We denote edge cluster as $N(\\Phi) = \\{\\phi_1, \\phi_2, ..., \\phi_j\\}$, where $\\phi_j$ represents the edge node. For each node, there exist k processors such that $\\phi = \\{P_1, P_2, ..., P_k\\}$ where $P_k$ represents CPU, GPU, or NPU. We denote the computation frequency of a processor as $f_k$ as computation cycles per second. We define the compute intensity of a DNN as $\\delta$, representing the average number of compute cycles of a processor to execute 1 flop [cycles/flops]. We define the computation rate [flops/sec] of each processor as the ratio of the computation frequency of the processor to the compute intensity of the DNN, such as $\\lambda = \\frac{f_k}{\\delta}$ [24]. We denote the communication rate of each processor as a scalar $\\mu_k$, representing the DNN transmission overhead between two processors for a given time duration t. We calculate the local computation-to-communication ratio of a node as:"}, {"title": "III. HIDP FRAMEWORK (cont.)", "content": "$\\psi\\{\\lambda, \\mu\\} = \\{\\frac{\\lambda_1}{\\mu_1}, \\frac{\\lambda_2}{\\mu_2}, ..., \\frac{\\lambda_k}{\\mu_k}\\}$\nFinally, we calculate the computation rate $A_j$ of a node $\\phi_j$ as the sum of computation rates of the available processors:\n$A_j(k) = \\sum_{i=1}^k [\\lambda_i](k)$\nWe denote the communication rate of each node as a scalar $B_4$; representing the DNN data transmission overhead between two nodes for a given time duration t via a wireless network. HiDP calculates the communication rate of each node by sending a set of pseudo packets to each node and recording the time taken to get the response. We form a global resource vector $\\Psi$ including the global computation-to-communication ratio of all nodes such that:\n$\\Psi\\{\\Lambda, \\beta\\} = \\{\\frac{\\lambda_1(\\kappa)}{ \\beta_1(\\kappa)}, \\frac{\\lambda_2(\\kappa)}{\\beta_2(\\kappa)}, ..., \\frac{\\lambda_j(\\kappa)}{\\beta_j(\\kappa)}\\}(3)$\nHiDP formulates an availability vector $A(N_j)$ based on the communication rate $B_j^{(t)}$ such that:\n$A(N_j) = \\{\\alpha_1, \\alpha_2, ..., \\alpha_j\\}$\n$\\alpha_j = \\{\\begin{array}{cc} 1 & \\text{available}, \\\\ 0 & \\text{unavailable} \\end{array}$$\nFor workload partitioning, HiDP decides between the partitioning modes and distributes the workload to the resources. For model partitioning, we denote the width of a layer block as $w_i$ and calculate the total computation time as:\n$\\Theta_w = \\{\\begin{array}{cc} \\gamma \\cdot w & \\text{for global partitioning} \\\\ \\gamma & \\text{for local partitioning} \\end{array}$\nSimilarly, HiDP explores the number of parallel submodels $\\sigma$ for data partitioning to calculate the total computation time as:\n$\\Theta_\\sigma = \\{\\gamma \\cdot \\sigma\\}$\n$\\gamma = \\{\\begin{array}{cc} \\gamma & \\text{for global partitioning} \\\\ \\gamma & \\text{for local partitioning} \\end{array}$\nHiDP calculates the total computation time $\\Theta$ of both model and data partitioning modes and selects the faster strategy. Scheduling Algorithm. Algorithm 1 shows the high-level workflow of HiDP framework to perform distributed inference. HiDP assigns leader status (1) to the node that receives new inference request $D_c$ (Lines 1\u20132). The leader node checks the availability status ($A(N)$) of the cluster (Line-3) and finds the optimal partitioning mode using Dynamic Programming (DP) algorithm (Lines 4\u20136). We used a standard subset sum algorithm for an efficient recursive search with time complexity O(n * m), where n represents the number of DNN blocks and m represents the number of available nodes. The algorithm starts with the largest possible block sizes following the resource heterogeneity to calculate the inference latency and back-propagates block by block to converge to minimum latency. We used the same algorithm to explore global and local partitioning points because the function arguments are essentially the same in either case including the DNN and the computation-communication ratio. The leader node partitions the workload and distributes the partitions to the global nodes (Line 7). The leader node finds out the optimal partitioning mode and the partitioning point for the local partition (Lines 8-10). The leader node executes the local workload on its local processors and gathers the global results (Lines 11\u201312). The"}, {"title": "Algorithm 1 HiDP framework on leader node", "content": "$\\begin{array}{l} 1: D_c \\text{ arrives on } \\phi_1 \\text{ //Get the input DAG} \\\\ 2: l \\leftarrow 1 \\text{ //Assign leader status} \\\\ 3: \\phi_1 \\leftarrow A(N) \\text{ // Get availability status} \\\\ 4: \\Theta_w \\leftarrow DPalg(w, \\Psi(\\phi_i, \\beta)) \\text{ //latency of model partitioning} \\\\ 5: \\Theta_\\sigma \\leftarrow DPalg(\\sigma, \\Psi(\\phi_i, \\beta)) \\text{ //latency of data partitioning} \\\\ 6: \\Theta \\leftarrow \\text{min}(\\Theta_w, \\Theta_\\sigma) \\text{ //decide partitioning mode} \\\\ 7: \\phi_1 \\text{ partitions } D_c \\text{ and allocates to nodes} \\\\ 8: \\theta_w \\leftarrow DPalg(w, \\psi(\\rho_\\kappa, \\mu_\\kappa)) \\text{ //Matency of model partitioning at local} \\\\ 9: \\theta_\\sigma \\leftarrow DPalg(\\sigma, \\psi(\\rho_\\kappa, \\mu_\\kappa)) \\text{ //latency of data partitioning at local} \\\\ 10: \\theta \\leftarrow \\text{min}(\\theta_w, \\theta_\\sigma) \\text{ //select partitioning mode at local} \\\\ 11: \\phi_1 \\text{ executes local workload} \\\\ 12: \\phi_1 \\leftarrow \\text{local and global results} \\\\ 13: \\phi_1 \\text{ merges the results and reports prediction} \\end{array}$"}, {"title": "Run-time Scheduler", "content": "We have designed the scheduling policy of the Run-time Scheduler as a Finite State Machine (FSM) including Analyze, Explore, Offload, Map, and Execute states as shown in Figure 4. The scheduling policy is the implementation on the method explained in Algorithm 1. The scheduling policy is different for the leader and follower nodes."}, {"title": "Leader Node", "content": "In the Analyze state, the controller waits for an inference request from a DNN application in the Application module. When a request is triggered, the controller checks the availability status of the cluster nodes by sending and receiving a status packet to the nodes via Communication module and transitions to the Explore state. In the Explore state, the controller refers to the global DSE agent to find the optimal partitioning point of the given DNN workload for global distribution. After finding the optimal partitioning point, the controller transitions to the Global: offload state. The controller offloads the workload to the available nodes using the Communication module and transitions to the Local: Map state for local execution of the allocated workload. Here, the controller refers to the local DSE agent to figure out the local partitioning of the workload following the available processing units. After converging to the optimal partitioning point, the controller transitions to the Execute state. In this state, the controller executes the workload while sharing intermediate data with the available nodes for parallel or sequential execution, depending on the partitioning mode. After successful execution, the controller gathers the results and transitions back to the Global: offload state for final merging and reporting of the results. After merging the results from local execution and cluster nodes, the controller transitions to the Analyze state and waits for the next inference request."}, {"title": "Follower Nodes", "content": "For the follower nodes, the state machine is much simpler, such that (i) the node receives the distributed workload from the leader node in the Analyze state, (ii) executes it after local partitioning in the Local: Map and the Execute state, and (iii) report back the results to the leader."}, {"title": "IV. EXPERIMENTAL EVALUATION", "content": "Experimental Platform. We design a collaborative heterogeneous edge cluster, comprising commercial edge platforms (shown in Table II), and deploy the HiDP strategy on the defined platform. We monitor the run-time power consumption of the Jetson boards using the on-board power sensors. We use the external shunt resistor to monitor the power consumption of the Raspberry Pi boards.\nWorkloads. We evaluated our framework over ResNet152, Ef-ficientNetB0, VGG-19, and InceptionNetV3, which are widely used DNN models in mobile vision applications. We imple-mented these models using the TensorFlow library [18], with input image sizes of 224x224, and 299x299. We enhanced the top layer of these models to accept variable input sizes to enable data partitioning.\nMiddleware. We implemented HiDP as a middleware in Python with a source code of 400 lines. Each device hosts Linux 18.04 OS to provide software and hardware interfaces. We used CGroup libraries to bind the workload to the desired number of CPU cores. For GPU implementation, the Ten-sorFlow backend used CUDA libraries for NVIDIA boards and OpenGL for Raspberry Pi boards. All the devices are connected over 80 MBps wireless control through POSIX-based client-server architecture. We used multi-threaded server operations for the leader nodes to communicate with the available nodes dynamically. The overhead of using DP algorithm-based exploration including both global and local partitioning is 15ms on average for our experimentation.\nComparison w.r.t. state-of-the-art approaches. For evaluation, we considered three different state-of-the-art partitioning strategies - data [4], model [7], and hybrid [5]. MoDNN [4] partitions and distributes the input data proportionally among the available edge nodes. We implemented MoDNN using the data partitioning module of HiDP framework. OmniBoost [7] determines the optimal partitioning point using the Monte-Carlo search tree and pipelines the DNN inference over both CPU and GPU. We implemented the throughput estimator of Omniboost using Gymnasium library [25] and trained it on our target workloads. DisNet [5] uses heuristic-based DNN"}, {"title": "A. Experimental Setup", "content": "Experimental Platform. We design a collaborative heterogeneous edge cluster, comprising commercial edge platforms (shown in Table II), and deploy the HiDP strategy on the defined platform. We monitor the run-time power consumption of the Jetson boards using the on-board power sensors. We use the external shunt resistor to monitor the power consumption of the Raspberry Pi boards.\nWorkloads. We evaluated our framework over ResNet152, Ef-ficientNetB0, VGG-19, and InceptionNetV3, which are widely used DNN models in mobile vision applications. We imple-mented these models using the TensorFlow library [18], with input image sizes of 224x224, and 299x299. We enhanced the top layer of these models to accept variable input sizes to enable data partitioning.\nMiddleware. We implemented HiDP as a middleware in Python with a source code of 400 lines. Each device hosts Linux 18.04 OS to provide software and hardware interfaces. We used CGroup libraries to bind the workload to the desired number of CPU cores. For GPU implementation, the Ten-sorFlow backend used CUDA libraries for NVIDIA boards and OpenGL for Raspberry Pi boards. All the devices are connected over 80 MBps wireless control through POSIX-based client-server architecture. We used multi-threaded server operations for the leader nodes to communicate with the available nodes dynamically. The overhead of using DP algorithm-based exploration including both global and local partitioning is 15ms on average for our experimentation.\nComparison w.r.t. state-of-the-art approaches. For evaluation, we considered three different state-of-the-art partitioning strategies - data [4], model [7], and hybrid [5]. MoDNN [4] partitions and distributes the input data proportionally among the available edge nodes. We implemented MoDNN using the data partitioning module of HiDP framework. OmniBoost [7] determines the optimal partitioning point using the Monte-Carlo search tree and pipelines the DNN inference over both CPU and GPU. We implemented the throughput estimator of OmniBoost using Gymnasium library [25] and trained it on our target workloads. DisNet [5] uses heuristic-based DNN"}, {"title": "B. Experimental Results", "content": "For our experiments, we consider inference latency, throughput, energy consumption (based on run-time power monitoring), and accuracy as evaluation metrics. Our proposed HiDP strategy has the lowest inference latency for all the workloads, in comparison with other relevant strategies, achieving upto 61%, 61%, 59%, and 49% lower latency for EfficientNet, InceptionNet, ResNet, and VGG, respectively. The hierarchical partitioning strategy of HiDP jointly optimizes both global-level DNN block creation and workload distribution, followed by local-level fine-grained partitioning and workload scheduling. This results in significantly lower latency compared to other distributed inference strategies that are exclusively confined to global partitioning. On average, HiDP has 37%, 44%, and 56% lower latency than DisNet, OmniBoost, and MoDNN strategies, respectively.\nWe evaluate the adaptability of different strategies under varying workload dynamics. We created a dynamic workload with successive run-time inference requests for every 0.5s, in the order of EfficientNetB0, InceptionNetV3, ResNet152, and VGG-19. This creates a progressively increasing workload such that at t=1.5s, all four DNNs are running concurrently on the edge cluster. HiDP delivers the highest performance throughout the execution cycle. Lower latency achieved by HiDP frees up the resources of edge nodes, which can be efficiently used to service subsequent inference requests. HiDP completes the inference of all the models within 5s in total, achieving 39%, 54%, and 56% higher performance than DisNet, OmniBoost, and MoDNN, respectively.\nWe evaluate the adaptability of different strategies with varying numbers of edge devices within the cluster."}, {"title": "V. CONCLUSION", "content": "We present HiDP framework for low latency DNN inference on distributed edge platforms. Our approach hierarchically partitions the DNN inference workload at a global level, followed by optimized partitioning and scheduling on a local heterogeneous edge node. We evaluated HiDP on four Jetson platforms and two Raspberry Pi platforms achieving latency and energy improvements of 38%, and 46% against SoA strategies, respectively. We consider energy-efficient distributed inference for future work."}]}