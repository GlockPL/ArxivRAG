{"title": "Heterogeneous Graph Auto-Encoder for Credit Card Fraud Detection", "authors": ["Moirangthem Tiken Singh", "Rabinder Kumar Prasad", "Gurumayum Robert Michael", "NK Kaphungkui", "N.Hemarjit Singh"], "abstract": "The digital revolution has significantly impacted financial transactions, leading to a notable increase in credit card usage. However, this convenience comes with a trade-off: a substantial rise in fraudulent activities. Traditional machine learning methods for fraud detection often struggle to capture the inherent interconnectedness within financial data. This paper proposes a novel approach for credit card fraud detection that leverages Graph Neural Networks (GNNs) with attention mechanisms applied to heterogeneous graph representations of financial data. Unlike homogeneous graphs, heterogeneous graphs capture intricate relationships between various entities in the financial ecosystem, such as cardholders, merchants, and transactions, providing a richer and more comprehensive data representation for fraud analysis. To address the inherent class imbalance in fraud data, where genuine transactions significantly outnumber fraudulent ones, the proposed approach integrates an autoencoder. This autoencoder, trained on genuine transactions, learns a latent representation and flags deviations during reconstruction as potential fraud. This research investigates two key questions: (1) How effectively can a GNN with an attention mechanism detect and prevent credit card fraud when applied to a heterogeneous graph? (2) How does the efficacy of the autoencoder with attention approach compare to traditional methods? The results are promising, demonstrating that the proposed model outperforms benchmark algorithms such as Graph Sage and FI-GRL, achieving a superior AUC-PR of 0.89 and an Fl-score of 0.81. This research significantly advances fraud detection systems and the overall security of financial transactions by leveraging GNNs with attention mechanisms and addressing class imbalance through an autoencoder.", "sections": [{"title": "1 Introduction", "content": "Financial transactions, especially credit card usage, have experienced a surge due to the digital revolution. This has resulted in a vast amount of financial data, empowering companies to comprehend customer behavior and utilize data for decision-making. On the other hand, the convenience that comes with this has a downside there is a noticeable rise in fraudulent activities. Traditional methods of fraud detection often struggle to keep pace with the evolving nature of these schemes. In order to tackle this challenge, the field of machine learning (ML) has surfaced as a potent tool that can effectively identify and prevent fraudulent transactions [1]. By leveraging ML algorithms, it becomes possible to analyze massive amounts of financial data, identify recurring patterns, and pinpoint potential fraud through anomaly detection. They enable financial institutions to automate the fraud detection process, facilitating real-time monitoring of transactions and activities. To detect fraud effectively, many professionals rely on techniques such as decision trees, random forests, and support vector machines [2, 3].\nThe conventional approaches to detecting fraud often face difficulties in capturing the intrinsic interrelationships that exist within financial data. Transactions typically involve multiple parties, including cardholders, merchants, banks, and various other entities. The representation of financial transactions as a graph enables us to take advantage of the connections among them, thereby enhancing the effectiveness of fraud detection measures. Despite their widespread use, it is important to acknowledge that traditional methods may face difficulties in accurately differentiating between relevant and irrelevant relationships within the graph, thus impacting their ability to effectively detect fraudulent activity.\nGraph Neural Networks (GNNs) excel at processing graph data and utilizing attention mechanisms to focus on the most relevant entities and relationships within the network structure [4]. This makes them well-suited for tasks like fraud detection, where identifying the most critical factors contributing to a transaction's legitimacy is crucial. By applying attention, the GNN can prioritize information from neighboring nodes (e.g., cardholder's spending habits, merchant's location) that are most relevant to understanding the transaction's nature. This refined focus on critical relationships improves the model's ability to distinguish between normal transactions and those exhibiting suspicious patterns, potentially indicative of fraud."}, {"title": "2 Literature Review", "content": "In this section, we introduce a range of notable works that cover various topics such as probabilistic graphical models, machine learning algorithms (including deep learning models), and advanced graph neural networks and their various variants. \nPapers such as [9] and [10] aim to address the problem of fraud detection in credit card transactions by modeling these transactions using a Hidden Markov Model (HMM), a probabilistic graphical model. The primary difference between them lies in their approach: in the first paper, a card-centric HMM is employed to detect abnormalities in transactions, while the latter paper opts for a merchant-centric HMM model. Both methods have the capability to identify fraud in real-time for merchants, operating in conjunction with modern transaction processing systems that handle card transactions.\nAdditionally, [11] models credit card transaction sequences using the HMM approach, considering three distinct perspectives:\n(i) Determining whether fraud is present or absent in the sequence.\n(ii) Crafting sequences by fixing either the cardholder or the payment terminal.\n(iii) Constructing sequences based on the spent amounts or the elapsed time between consecutive transactions. The combination of these three binary perspectives results in eight distinct sets of sequences derived from the training dataset of transactions. Each of these sequences is then represented using a Hidden Markov Model (HMM). Subsequently, each HMM assigns a likelihood to a transaction based on its sequence of preceding transactions. These likelihood values serve as additional features for the Random Forest classifier to detect fraud. In brief, this model provides a concept of sequential information flow during credit card transactions as part of a feature for a machine learning model.\nThe paper [12] explores the issue of credit card fraud detection and conducts a comparative analysis of three machine learning algorithms: logistic regression, Na\u00efve Bayes, and K-nearest neighbor. To address the class imbalance, the authors utilize different proportions of the dataset and employ a random undersampling technique. They evaluate the algorithms based on various metrics. According to the results, the logistic regression-based model outperforms the prediction models derived from Na\u00efve Bayes and K-nearest neighbor. The paper also suggests that applying undersampling techniques to the data before model development can lead to improved results. In addition, several machine learning algorithms, such as support vector machine (SVM) [13], random forest (RF) [13, 14], AdaBoost, and Majority Voting [15], as well as artificial neural network (ANN) [16, 17], are being explored as models for controlling fraudulent transactions in credit cards.\nTo enhance the performance of the above-mentioned models, [18] defines a model in an ML-driven credit card fraud detection system that uses the genetic algorithm (GA) for feature selection. After identifying optimal features, this detection system utilizes a range of ML classifiers, including Decision Tree (DT), Random Forest (RF), Logistic Regression (LR), Artificial Neural Network (ANN), and Naive Bayes (NB).\nWhile the aforementioned models perform well, a significant class imbalance exists in the credit card fraud dataset, with non-fraudulent transactions vastly outnumbering fraudulent ones. As a result, these models tend to prioritize high precision by predominantly predicting the majority class. To address this issue, several machine learning models (referenced as [19]) employ one or a combination of oversampling and undersampling techniques (as mentioned in [20]).\nThe study cited as [21] conducts a comparative investigation of various approaches to address class imbalance. The findings indicate that a combination of oversampling and undersampling methods performs well when applied to ensemble classification models, including AdaBoost, XGBoost, and Random Forest. Deep learning algorithms such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), combined with a multilayer perceptron, are employed in the studies referenced as [19] and [22]. In [22], the authors use the Hybrid Synthetic Minority Oversampling Technique and Edited Nearest Neighbor (SMOTE-ENN) to balance the distribution of positive (fraud) and negative (non-fraud) instances in the dataset. However, the effectiveness of the SMOTE-ENN technique is crucial, as poor performance in resampling can significantly degrade the model's overall performance.\nWhile oversampling and undersampling techniques can address class imbalance, they come with drawbacks like increased computational cost, potential for overfitting, and information loss (as discussed in [8, 23]). Additionally, they can be sensitive to noise [24] and have limited effectiveness for highly imbalanced datasets [25]. Therefore, [25] propose an approach for Chronic Kidney Disease (CKD) prediction using imbalanced data. Their method leverages information gain-based feature selection and a cost-sensitive AdaBoost classifier. However, this approach focuses on spatial data and might not be suitable for graph data due to potential loss of structural information and inadequate feature representation during feature selection. So, such models will often struggle to capture the full picture of fraudulent activity. As noted in [26], many methods focus solely on spatial data points representing financial transactions, neglecting the valuable insights from temporal relationships. This limitation hinders the ability of these models to identify evolving fraud patterns. Furthermore, many existing models rely solely on labeled data for training, restricting their ability to leverage the vast amount of unlabeled data available in real-world credit card transactions [27].\nTo address these issues, an increasing number of researchers are exploring graph-based techniques for fraud detection, as discussed in [26] and [28]. In this approach, datasets are transformed into graphs, providing a better understanding of the relationships among financial transactions. Graph Neural Network (GNN) algorithms, as detailed in [29], are applied to these graph datasets, allowing for efficient data aggregation from neighboring nodes and the extraction of node representations within the graph datasets. Among the popular GNN variants, GraphSAGE [30] and GAT [31] stand out, utilizing sampling methods and attention mechanisms to gather neighbor information. These techniques have shown promising results in the field of fraud detection. Furthermore, the paper [32] introduces an algorithm designed to tackle the class imbalance problem in graph-based fraud detection. It employs an algorithm known as Pick and Choose Graph Neural Network (PC-GNN) to perform imbalanced supervised learning on graphs. The PC-GNN algorithm selects neighbor candidates for each node within the sub-graph using a neighborhood sampler. Ultimately, it aggregates information from the chosen neighbors and different relations to derive the final representation of a target node. The paper reports that PC-GNN surpasses state-of-the-art baselines in both benchmark and real-world graph-based fraud detection tasks.\nHowever, inconsistency issues arise in the aggregation process of GNN models when applied to fraud detection tasks [33]. The aggregation mechanism relies on the assumption that neighbors share similar features and labels. When this assumption breaks down, the aggregation of neighborhood information becomes ineffective in learning node embeddings.\nTo address these challenges, researchers in [33] and [34] have employed a multi-relational graph, known as a heterogeneous graph, for the classification of financial fraud. In [33], context inconsistency, feature inconsistency, and relation inconsistency in GNN are introduced. To tackle these inconsistencies, the authors propose a new GNN framework called GraphConsis. GraphConsis addresses these issues by combining context embeddings with node features to handle context inconsistency, designing a consistency score to filter inconsistent neighbors and generate corresponding sampling probabilities to address feature inconsistency, and learning relation attention weights associated with the sampled nodes to tackle relation inconsistency.\nIn [34], the authors propose semi-supervised methods that operate with heterogeneous graph datasets to address class imbalance issues in online credit loans. This paper utilizes a Graph-Oriented Snorkel approach to incorporate external expert knowledge, ultimately improving the performance of the learning algorithm when dealing with imbalanced datasets.\nAnother noteworthy work, [35], introduces a heterogeneous graph-based approach for detecting malicious accounts in financial transactions. The authors present an algorithm called GEM, which adapts to learn discriminative embeddings for various node types. GEM employs an aggregator to capture node patterns within each type and utilizes an attention mechanism to enhance algorithm efficiency.\nIn [36], the authors endeavor to design heterogeneous graph embeddings. Their approach incorporates heterogeneous mutual attention and heterogeneous message passing, incorporating key, value, and query vector operations (self-attention mechanism). This work features both a detector and an explainer, capable of predicting the validity of incoming transactions and providing insightful, understandable explanations generated from graphs to aid in subsequent business unit procedures.\nThe framework employed in [37] utilizes an algorithm for graph representation learning to create concise numerical vectors that capture the underlying network structure. The authors in this work assess the predictive capabilities of inductive graph representation learning with GraphSage and Fast Inductive Graph Representation Learning algorithms on credit card datasets characterized by significant data imbalance."}, {"title": "3 Problem Statement", "content": "A heterogeneous graph is a specialized graph data structure that comprises multiple types of nodes and edges, wherein each node or edge is uniquely associated with a distinct type. In essence, it represents a graph in which diverse node and edge types are interconnected. To provide a formal definition, the characteristics of a heterogeneous graph are delineated as follows:\nDefinition 3.1. A heterogeneous graph, also known as a heterogeneous information network or heterogeneous network, is mathematically defined as  G = (V,E,T, R, X), where:\n\u2022 V represents the set of nodes in the graph, and each node vt \u2208 V is associated with a specific type t \u2208 T, where T represents the set of node types.\n\u2022 E represents the set of edges in the graph, and each edge er \u2208 E connects two nodes (vt1, vt2), where t\u2081 and t2 are node types, and r \u2208 R, where R represents the set of edge types or relationships.\n\u2022 X = {Xv, Xe} represents attributes of nodes and edges, respectively, where Xv represents the set of node attributes, and each node vt \u2208 V can have a vector of attributes Xut, and Xe represents the set of edge attributes, where each edge e \u2208 E can have a vector of attributes xer.\nBy adhering to its definition, the financial fraud dataset can be depicted as a heterogeneous graph. These datasets encompass various entities, including customer or credit card numbers, merchants' names, and transaction numbers. These entities are"}, {"title": "4 Methodology", "content": "The primary objective of this paper is to develop an encoder capable of learning graph embeddings for a given graph  G = (V,E,T, R, X). This encoder will be specifically designed to effectively capture the complex information present in a heterogeneous graph, including both its structure and its attributes. Subsequently, a decoder function fdec will be introduced to reconstruct the graph. \nProblem 1. For the given graph G = (V, E,T, R, X), the task is to determine whether it can be classified as fraudulent, considering that the transaction associated with the graph represents a fraudulent class.\nIn this model, there are l encoder units. The first encoder unit takes (D(dt), $, dt) as input, where D(dt) represents the source nodes of dt \u2208 V, and $ represents edges er for each source node to dt. Each encoder unit processes these inputs to produce intermediate representations. The final output of Encoder, is fed into a decoder unit, which is implemented as a deep neural network. This decoder unit utilizes the encoded information to generate d't.\nFinally, the model will calculate the reconstruction error by comparing the reconstructed graph and the original graph. This error serves as a measure of the dissimilarity between the original input and the reconstructed output. Using this error, a threshold for the reconstruction error is established to identify data points that deviate significantly from the normal patterns. Any data point with a reconstruction error that exceeds the threshold is classified as an anomaly, indicating a deviation from expected normal behavior."}, {"title": "4.1 Encoder for Heterogeneous Graph", "content": "Based on the study by ([41]), a heterogeneous graph encoder for the auto-encoder has been designed . For each destination node dt \u2208 V and D(dt) \u2208 V, which represents a list of source nodes for dt, the encoding process fenc is applied as follows:\n\\( \\hat{h}_{dt} = f_{enc}(v_t \\in D(d_t)) \\\\\n    f_{reparam}(Linear_d(f_o(\\hat{At}^{(l)}(h_{vt}, e_r, h_{dt} ) \\\\  hat{h}_{d_t} , mean(h_{vt}), log(\\eta) ) \\)  (1)\nHere, l = 1,2,..., EL represents the encoder layer with a maximum of EL layers, and the initial values are set as (h\u00bavt, er, h\u00badt) = (vt, e\", dt). Additionally, Lineardt : Rdim \u2192 Rdim denotes the linear projection.\nThe encoding process fenc can be broken down as:\n\\( f_{enc}(D(d_t)) = \\sum_{v_t \\in D(d_t)} \\delta_k( f_{Attention}( v_t, e_r,d_t)) \\odot f_{message} ( v_t, e_r,d_t)  ) \\) (2)\nIn Equation (2), the graph attention mechanism is used and the graph message to embed the node feature of the descriptor node dt on edges e\". The attention mechanism is defined as follows:\n\\( f_{Attention}( v_t, e_r,d_t) = Softmax (||A_t^k(v_t, e_r,d_t)) \\) (3)\nInspired by [42], the attention for each edge er is calculated using k-heads, based on the dot product of the linear projection of v \u2208 D(dt) and dt, with a matrix WAtt that depends on the edge, as shown in Equation (4):\n\\( A_t^k(v_t, e_r,d_t) = Linear_s^k(\\vec{h_v}^{(l-1)})  \\cdot W_{Att} \\cdot (Linear_D^k( \\vec{h_d}^{(l-1)})  \\) (4)\nFor each attention head k, LinearSk and LinearDtk perform linear projections of the source node vt and dt, respectively, as well as their subsequent embedded forms. These projections map from Rdim to Rim, where dim represents the vector dimension for each head. Finally, WAtt \u2208 Rim \u00d7 dim is a learnable edge matrix for the edge typer connecting between vt and dt.\nThe message passing function for each attention head k is obtained by performing the dot product between the linear projection of the source node vt and a matrix WMssge Rdim based on the edge type joining vt and dt. The linear projection is carried out by LinearM for each type of node, and thus the final projection is mapped from Rdim to Rim, as shown in Equation (5):\n\\( f_{message} ( v_t, e_r,d_t) =  || LinearM(1) \\cdot W_{Msg} \\) (5)\nFinally, a function called freparam creates a probabilistic model by remodeling the latent variable hat using probabilistic distributions, facilitating gradient-based optimization. This approach captures uncertainty and generates diverse samples. Following [43], the function is written as follows:\n\\( \\hat{h}_{dt} = f_{reparam}(mean(\\hat{h}), log(\\hat{h})) \\\\\n        = mean(\\hat{h}) +\u20ac e exp( \\frac{1}{2}log(\\hat{h})  ) \\)\nwhere \u20ac = N(0, 1) (sampled random noise)."}, {"title": "4.2 Decoder For Heterogeneous Graph", "content": "The graph decoder should account for the heterogeneous nature of the graph G when reconstructing the original structure. It needs to reconstruct the specific types of nodes and edges, ensuring that the reconstructed graph maintains the semantic relationships and attributes associated with each type. This requires incorporating type-specific reconstruction mechanisms into the decoder.\nFor each node dt, a node decoder fdec is applied to reconstruct the attributes d't based on the corresponding node embedding hat:\n\\( d'_t = f_{dec}(\\hat{h_{d_t}}) \\) (6)\nHere, d't represents the reconstructed attribute of node dt with the original attribute hat.\nThe primary objective of the decoder is to reconstruct the original graph data, which is based on the graph embeddings generated by the encoder. The overall loss function for the autoencoder will be expressed as follows:\n\\( L = \\sum \\sum LOSS(d_t, d'_t) \\) (7)\nThe loss function L represents the sum of losses calculated by the LOSS function between the original graph data and the reconstructed data."}, {"title": "4.3 Algorithm", "content": "Algorithm 1 Fraud Detection on a Heterogeneous Graph\nRequire: Heterogeneous Graph G\nEnsure: 'Fraud' or 'Not Fraud'\n1: for dt \u2208 G do\n2: (hot, er, hat) \u2190 (vt,er,dt) Initialization\n3: for l\u2190 1 to EL do Message Passing Layers\n4: for vt \u2208 D(dt) do Neighborhood of dt\n5: hat = Linear dt (fenc (h, er, hat)) hat\n6: end for\n7: hat = freparam (mean(ha), log(hat)) Reparameterization\n8: end for\n9: d't = fdec(hat) Output Layer\n10: end for\n11: L = LOSS(dt, d't) Loss Calculation\n12: if L < Threshold() then\n13: return 'Non-Fraud'\n14: else\n15: return 'Fraud'\n16: end if\nThe algorithm depicted in Algorithm 1, outlines the method for detecting fraud in a heterogeneous graph structure. Here's a detailed breakdown of each step:\n1. Input (Heterogeneous Graph G): This represents the financial transaction network, containing nodes (customers, merchants, transactions) and edges (interactions) with their respective types.\n2. Output: \"Fraud\" or \"Not Fraud\": The algorithm classifies the transaction associated with the input graph as either fraudulent or legitimate.\n3. Algorithm Steps:\n\u2022 For each node dt in the graph G, node dt is initialized with (hot, er, hat). It includes the features of the node itself hat, the connecting edge type er, and the initial representation of the source node hot.\n\u2022 Message Passing Layers (L Layers):\nThis loop iterates through a predefined number of layers (EL) in the GNN architecture.\nWithin each layer 1:\n* For each node vt in the neighborhood of the current node dt:\nA message function fenc (Equation 2) aggregates information from the source node's hidden representation h1, the edge type e\", and the previous hidden representation of the destination node ha\u00b9. The message undergoes a linear transformation with Lineardt as per equations (3-5).\nBy utilizing the attention mechanism, the messages undergo transformation and are subsequently combined with the initial hidden representation of the destination node hoat through element-wise addition ().\n* The message passing happens iteratively for all neighbors of dt.\n* The updated hidden representation hat is subjected to freparam (Equation 1) after message aggregation. Mean and logarithm are utilized in hidden representation to ensure greater stability during training.\n4. Output Layer: The final hidden representation hat is passed through the decoder function fdec (Equation 6) to produce the prediction vector d't.\n5. Loss Calculation: The difference between the predicted output d't and the original node feature dt is evaluated using a loss function LOSS. The LOSS function can use a metric such as mean squared error or any other appropriate loss function.\n6. Fraud Classification: A threshold function Threshold() is used to determine the classification based on the calculated loss. If the loss is lower than the threshold (indicating a good fit), the algorithm outputs \"Non-Fraud\". Conversely, if the loss is higher than the threshold (indicating a poor fit), it outputs \"Fraud\".\nAlgorithm 1 explains the entire framework of the model, which is designed to identify if a specific data point is linked to fraudulent behavior, resulting in one of two possible outcomes: 'Fraud' or 'Not Fraud.' The algorithm calculates a loss value to measure the difference between the original transaction node and its decoded version. The computation of this loss relies on a loss function that has been predetermined. The next step in the process is for the algorithm to compare the resulting loss with a predetermined threshold, once all the calculations have been completed. In the case where the loss falls below the designated threshold, the data point is classified as 'Not Fraud'. The overall time complexity of the algorithm can be approximated as O(nE)"}, {"title": "5 Experiment", "content": "This paper assesses the effectiveness of the proposed model through a series of experiments on credit card fraud datasets and a comparison with other existing machine learning and deep learning models."}, {"title": "5.1 Performance Metrics", "content": "In order to evaluate the performance of various models, this article employed evaluation metrics that include the precision rate (PR), the recall rate (RR), the ROC curve, and the F1 score. These metrics are defined as follows:\n\\( PR = \\frac{TP}{TP + FP} \\)\n\\( RR = \\frac{TP}{TP + FN} \\)\nIn this context, true positive (TP) and false positive (FP) indicate the number of correctly and incorrectly predicted instances of fraud, respectively. Conversely, true negative (TN) and false negative (FN) correspond to the count of transactions accurately and inaccurately predicted as non-fraudulent.\nMeanwhile, the ROC curve illustrates the classifier's ability to differentiate between fraud and non-fraud categories. This curve is created by plotting the true positive rate against the false positive rate at different threshold levels. The AUC, which ranges from 0 to 1, encapsulates the information from the ROC curve. A value of 0 signifies that all classifier predictions are erroneous, while a value of 1 indicates a perfect classifier.\nThe F1 score represents the harmonic mean of precision and recall. Precision is the ratio of true positive predictions to the total predicted positives and recall is the ratio of true positive predictions to the total actual positives. It provides a single value that harmonizes precision and recall, facilitating a balanced evaluation of classifier performance.\n\\(F1 = 2 *  \\frac{(PR * RR)}{(PR + RR)} \\)\nGiven that the dataset is imbalanced, the F1 score is particularly valuable because it considers both precision and recall. This score provides a straightforward way to assess a classifier's overall effectiveness in accurately identifying positive instances while minimizing false positives and false negatives.\nAnother parameter used to gain insight into the model's performance is the Precision-Recall curve (AUC-PR) [44]. This metric offers valuable insights, particularly in situations where class distribution is imbalanced [45]."}, {"title": "6 Conclusion", "content": "In this paper, a novel approach is introduced that incorporates a heterogeneous graph autoencoder with an attention mechanism, designed to extract valuable information from the intricate graph structure. The encoded node information, produced by the encoder, is harnessed to create a probabilistic distribution using a variational autoencoder model, allowing for the capture of uncertainty and the generation of diverse samples of the embedded nodes. This model effectively addresses the first research question. Subsequently, the output of the encoder undergoes further processing by a deep learning neural network, leading to the regeneration of the original node embeddings. This process significantly enhances the embedded representations of the nodes within the heterogeneous graph. The errors generated by the decoder are carefully observed and recorded, playing a crucial role in classifying fraudulent from non-fraudulent transactions. To facilitate this, a straightforward search algorithm is employed to determine an efficient threshold, effectively addressing the second research question. The work is rigorously benchmarked against a selection of state-of-the-art machine learning algorithms and compared with established methods, such as Graph-Sage and FI-GRL, serving as baselines. Remarkably, the approach consistently demonstrates superior performance, outperforming these baseline methods and thus effectively addressing the third research question. However, the model currently lacks the capability to effectively handle temporal data relationships, which is essential for addressing the dynamic nature of datasets, particularly in the context of fraudulent transactions. This issue will be a focal point for future research and development."}, {"title": "Statements and Declarations", "content": "Competing Interests\nThe authors declare that there are no competing interests associated with this research work.\nFunding\nThis research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors."}, {"title": "Informed Consent", "content": "Informed consent was obtained from all individual participants included in the study."}, {"title": "Data Availability", "content": "The datasets generated and/or analyzed during the current study are available in upon reasonable request from the corresponding author."}]}