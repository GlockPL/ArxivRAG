{"title": "The Ingredients for Robotic Diffusion Transformers", "authors": ["Sudeep Dasari", "Oier Mees", "Sebastian Zhao", "Mohan Kumar Srirama", "Sergey Levine"], "abstract": "In recent years roboticists have achieved remarkable progress in solving increasingly general tasks on dexterous robotic hardware by leveraging high capacity Transformer network architectures and generative diffusion models. Unfortunately, combining these two orthogonal improvements has proven surprisingly difficult, since there is no clear and well-understood process for making important design choices. In this paper, we identify, study and improve key architectural design decisions for high-capacity diffusion transformer policies. The resulting models can efficiently solve diverse tasks on multiple robot embodiments, without the excruciating pain of per-setup hyper-parameter tuning. By combining the results of our investigation with our improved model components, we are able to present a novel architecture, named DiT-Block Policy, that significantly outperforms the state of the art in solving long-horizon (1500+ time-steps) dexterous tasks on a bi-manual ALOHA robot. In addition, we find that our policies show improved scaling performance when trained on 10 hours of highly multi-modal, language annotated ALOHA demonstration data. We hope this work will open the door for future robot learning techniques that leverage the efficiency of generative diffusion modeling with the scalability of large scale transformer architectures. Code, robot dataset, and videos are available at: https://dit-policy.github.io", "sections": [{"title": "I. INTRODUCTION", "content": "Modern machine learning has achieved remarkable success by leveraging highly expressive deep neural networks to generate and model samples from extensive offline imitation datasets [1], [2], [3]. Inspired by these advances, the field of robotics is adopting similar techniques to develop general policies and controllers for manipulation [4], [5] and locomotion tasks [6], [7]. However, robotics tasks present multiple challenges that hinder the straightforward application of these methods. First, the policy must learn to process high-dimensional observation streams from multiple cameras, without overfitting to spurious correlations in the data. For example, the policy may learn to regress actions directly from proprioceptive signals or a specific camera view. Thus, during test time it would entirely ignore signals from other modalities (e.g., wrist cameras) that are critical for solving highly dexterous tasks with potential occlusions. This often results in catastrophic failure during deployment. Second, the robot must make extremely precise action predictions, due to the low error tolerance in object manipulation. This is especially important when solving long horizon tasks, where the robot may need to achieve multiple sub-goals in sequence before the trajectory ends. For example, a robot tasked with preparing a sushi dish would need to reach multiple \"cutting\" sub-goals, which each have millimeter level error thresholds, as showcased in Fig. 1. Finally, policy learning needs to contend with multi-modal action distributions - i.e., different ways of solving the same task. Simply learning the average action from this distribution will often result in an indecisive and error-prone behaviors. Handling action multi-modality becomes particularly important as the dataset size increases, since different experts will naturally demonstrate different behaviors. Failing to address these challenges will result in an unreliable and unsafe policy during deployment.\nRecent advancements have begun to address these issues, by developing higher-capacity network architectures for dexterous task [8], and leveraging improved generative modeling frameworks like diffusion [9] for effective multi-modal action learning [10]. Combining these two orthogonal improvements could yield highly capable policies, but has proven surprisingly challenging so far. For example, the original diffusion policy paper [10] proposed a na\u00efve cross-attention Transformer [11] implementation for the policy network that was (according to their own analysis) extremely difficult to train. As a result, most follow-up works [12] build upon their U-Net architecture [13], which is easier to tune but imposes strict requirements on the task setup (e.g., action signals must be sufficiently smooth [10]). As a result, high-capacity diffusion modeling remains inaccessible for a wide range of robotics applications.\nThis work's key insight is that unstable transformer diffusion policy training is not a fundamental problem, and can be largely resolved with a novel policy architecture. Our contributions are: (1) Scalable Attention Blocks: we propose a key improvement (inspired by Peebles et. al. [14]) to stabilize training by adding adaptive Layer Norm (adaLN) blocks to the diffusion transformer policy layers. This simple trick improves performance by 30%+ on long horizon, dexterous, real-world manipulation tasks containing over 1000 decisions! (2) Efficient Observation Tokenization: we compare several methods to tokenize multiple camera observations, such as Vision Transformers [15] and ResNet [16] encoders."}, {"title": "II. RELATED WORK", "content": "a) Encoding high dimensional observations: In order to perceive their environment, robots typically make use of multiple sensory observations. Therefore, how to best combine information from multiple sensors is a age-old question in robotics and computer vision [18], [19], [20], [21], [22]. For example, bi-manual robots like ALOHA [8] must combine information from global cameras that view the whole scene and wrist cameras that get a close-up view of the manipulation itself. The most straight-forward way to handle this problem is to learn a single shared network/encoder that operates across all the input modalities simultaneously [4], [23]. However, these systems often learn brittle features that overfit to specific inputs, e.g., proprioceptive data and global cameras, while ignoring others entirely. Possible solutions from prior work include using separate high-capacity image encoders for each visual stream [8], [10], injecting 3D aware spatial biases into the representation network [24], [25], and properly regularizing the features using observation dropout [26], [27]. Our findings reveal that a combination of these tricks provide a roughly 40% boost on long-horizon, bi-manual tasks, and that these seemingly small details are crucial for successful visuo-motor control.\nb) Predicting multi-modal action distributions: Modeling multi-modal action distributions \u2013 i.e., scenarios where the robot could take multiple entirely different actions from the same observation/goal is a well known challenge for BC methods [28]. This challenge often intensifies as the amount of expert data increases, since different demonstrations may showcase different solutions for the same task. Potential solutions include action space discretization [29], [30], [31], [32], [33], modifying \\( \\pi \\) to predict higher capacity action distributions [23], [34], [35], implicitly modeling the action distribution [36], [37], [38], [39], [40], and using a generative modeling objective like diffusion [41], [42], [10], [43], [44], [45]. Diffusion in particular has shown state-of-the-art results in robotics [10]: it can learn complex 3D-aware policies [46], [47], and concurrent work even showed state-of-the-art manipulation results on bi-manual robotic arms [48]. However, the model architectures/hyper-parameters are very sensitive and difficult to tune [10], [12]. This is a major barrier to scaling, since higher-capacity network architectures, such as Transformers [11], are crucial to fitting large and more diverse datasets. In contrast, our approach alleviates these issues by replacing the standard cross/joint attention conditioning blocks in a transformer decoder, with one better suited for diffusion [14]."}, {"title": "III. PROBLEM SETTING", "content": "We consider the problem of acquiring a robotic controller via imitation learning [51], [52], [53], [54], [55], [56] that can perform challenging, dexterous manipulation behaviors when prompted to via language instructions. Specifically, the robot must learn a goal-conditioned policy \\( \\pi_{\\theta} (a_t | o_t, g) \\) that predicts an action distribution \\( a_t \\sim \\pi(.o_t, g) \\), given a new input observation (\\( o_t \\)) and a desired language goal (g), under environment dynamics \\( T : S \\times A \\rightarrow S \\), with \\( o_t \\in S \\) and \\( a_t \\in A \\). The policy \\( \\pi \\) is optimized via behavioral cloning [57], [28], [58] (BC) to match the optimal action distribution given a demonstration dataset \\( D = {T_1,..., T_n} \\), where each trajectory \\( T_i = {g, o_0, a_0, o_1,...} \\) was collected from an expert agent (e.g., human tele-op data). During test time, actions are sampled from \\( \\pi \\) and executed on the robot. We choose: \\( o_t \\) to be a set of image observations from the robot; \\( a_t \\) to be a chunk of H joint/Cartesian state actions, and g to be a text description of the task. This setting allows us maximum flexibility and generality for a wide range of robotics tasks, where precise states are difficult to infer and goals are free-form natural language instructions.\n A. Training Objective\nOur policy \\( \\pi \\) is formulated as a conditional Denoising Diffusion Probabilistic Model [41] (DDPM), a type of generative model where the output is sampled using a denoising process [59]. Given the initial Gaussian noise \\( x_k \\) and a noise prediction network \\( \\epsilon_{\\theta}(x_k, k, o_t, g) \\) the DDPM process produces \\( x_{k-1} = \\alpha (x_k \u2013 \\gamma \\epsilon_{\\theta}(x_k, k, o_t, g) + N(0, \\sigma^2I)) \\), where k is the diffusion time index and \\( \\alpha, \\gamma, \\sigma \\) are parameters associated with the diffusion noise schedule [41]. When \\( \\epsilon \\) is properly trained, this process will yield a sequence terminating in the optimal action: \\( x_k, x_{k\u22121},...,x^\u00b0 \\sim a_t \\). Thus, our goal is to learn \\( \\epsilon_{\\theta} \\) via gradient descent [60], [61] using the following MSE objective: \\( L = ||\\epsilon_k \u2013 \\epsilon_{\\theta}(a_t + \\epsilon_k, k, o_t, g)||_2 \\). Note that we use k = 100 diffusion steps during training, a cosine noise schedule [62], and a standard deterministic sampling process to reduce the number of samples needed (to k = 10) during test time [63]."}, {"title": "IV. INTRODUCING THE DIT-BLOCK POLICY", "content": "Our method - DiT-Block Policy- is a Transformer neural network architecture designed specifically to be a highly performant conditional noise network (\\( \\epsilon_{\\epsilon} \\) from above) for robotic diffusion policies. The DiT-Block Policy architecture is visualized in Fig. 2. First, the text goal and robot proprioception inputs are encoded into observation vectors. Similarly, the time-step k is turned into an embedding vector using sinusoidal Fourier features [11], [64] and a small MLP network. Then, all these embedding vectors are combined with the input noise vector (\\( x_k \\)) using an encoder-decoder Transformer architecture to produce the denoising output \\( \\epsilon_k \\). We now describe a few ingredients that are key to enable stable training and improved action prediction performance.\na) Processing diverse multi-camera observations: Before passing through the transformer backbone, the input images, text goal, and joint angle observations need to be tokenized. The input images from each camera are processed separately, using Convolutional Neural Network (CNN) backbones [65]. While other vision transformers [15], [66] may skip this stage entirely, the intensive spatial reasoning and limited data in many robotics tasks can benefit from the spatial priors in higher-capacity CNNs. Thus, we used ResNet-26 [16] as the encoder. The text goals are incorporated into the vision encoder via FiLM layers [50]. This enables the text goals to influence the network's visual attention at all layers of the network. Finally, the proprioceptive inputs are regularized with a per-dimension observation dropout [26], [27], before tokenization. After the initial tokenization, learned positional encodings [11] are added to the input tokens, and processed together using the Block Attention transformer encoder implementation from Octo [4]. These results in a series of transformer joint embedding tokens \\( e(1), . . ., e(L) \\), where L is number of layers.\nb) Leveraging adaLN-Zero attention blocks for policy learning.: In parallel, a transformer decoder (with L layers) processes the current (noised) input (\\( x_k \\)), time-step (k), and encoder embeddings. We note that each decoder block i processes its corresponding embedding from the encoder \\( e(i) \\). Typically, this processing occurs via a standard cross attention mechanism, enabling the decoder to index into \\( e(i) \\) using its input tokens. Our key insight is, that this default attention implementation explains the poor training dynamics of prior diffusion policy transformer implementations [10], [12]. Thus, we propose replacing standard cross-attention blocks with an adaptive Layer-Norm (adaLN) mechanism that plays a key role in stabilizing diffusion transformers in image generation tasks [14]. These blocks work by injecting the conditioning vector into the Transformer's LayerNorm blocks, by shifting and scaling the input vectors: \\( x = a(e(i), k) * x + b(e(i), k) \\). We choose a and b to be simple dense layers that operate on the mean encoder embedding and the time vector: \\( a(e(i),t) = token_{mean}(e(i)) + t \\). In addition, the output scales projection layers, before residual layer, are initialized to 0 (hence adaLN-Zero). This essentially initializes the noise network with identity skip connections, and thus further improves its learning dynamics [67]."}, {"title": "V. DIT-BLOCK POLICIES FOR BI-MANUAL TASKS", "content": "Inspired by prior work on data scaling [68], [69], [17], [5], [29], [70], we seek to understand how DiT-Block Policies will behave as they are trained on increasingly diverse demonstrations data. However, the few (open-source) bi-manual datasets that do exist [8], [71] only consist of a handful of tasks, collected using the same controlled scenes/objects. As a result, they are not useful for testing generalization in our bi-manual setting. To address this shortcoming, we collected and annotated BiPlay, a more diverse bi-manual manipulation dataset with randomized objects and background settings as shown in Fig. 3. We collected BiPlay as a series of 3.5 minute long episodes. For each episode, we constructed a random scene with various objects, and solved a sequence of tasks within that scene. After collection, the episodes were broken into clips that were in turn annotated with appropriate language task descriptions. The final dataset contains 7023 clips spanning 10 Hrs of robot data collection.\nA. Training Protocol\nTo train our models, we collected a fixed set of demonstrations (100+ demos) for each of our evaluation tasks (see Sec. VI-A). In addition, we compiled open sourced data from prior work (ALOHA [8] dataset and optimal policy roll-outs from YaY [71]), and added it to the training mix for regularization. The full mix of data is presented in Table I. All DiT-Block Policies were trained on this data-mix for 250K iterations, using the AdamW [61] optimizer and a cosine learning schedule [72]. Finally, instead of predicting a single action at each step, we trained DiT-Block Policy models to predict a chunk of H = 100 actions. This acted as regularization during training, and allowed us to employ temporal ensembling [8], to improve stability at runtime."}, {"title": "VI. EXPERIMENTAL SETUP", "content": "Our experiments are designed to understand DiT-Block Policy's limits and capabilities. First, we defined a series of manipulation tasks using two different robot morphologies in Sec. VI-A. Then, we trained the policies on separate mixes of task demonstration data, grouped by morphology.\nA. Task Setups\nOur first task set considers a bi-manual, low-cost ALOHA robot [8], which enables us to investigate challenging scenarios with highly dexterous, precise behaviors. We now describe these tasks and their success criteria in detail: (1) Pick Place: Given a text instruction (like g =\"pick up the corn and place it in the bowl\") the robot must find the target object, grasp it, and then drop it into the target plate/bowl. There are always two objects and two possible targets in the scene, so the robot must properly ground its behavior in the text instruction. A trial is marked successful if the object ends in the correct receptacle. (2) Pen Uncap: This task evaluates the robot's precision grasping capabilities and its ability to control both arms simultaneously. The robot must pick up the sharpie with one arm, bring it to the other arm, and then remove the cap from the pen. A trial is marked successful if it ends with the pen uncapped. (3) Sushi Cut: This task evaluated the robot's ability to chain precise, dexterous manipulation tasks over a long horizon. It is the most challenging task, since even a small error over a 2 min episode could derail the policy. The robot must place the sushi on the cutting board, pick up a knife from the cup, re-grasp it with the other hand, and then cut the sushi into four pieces. The task is marked successful if it ends with the sushi split into four, but partial credit is given for the fraction of successful cuts (e.g., one cut gets 1/3).\nOur next task set uses a single-arm Franka FR3 robot. While the dexterity is more limited, the Franka allows us to test generalization to an entirely new morphology and control space (Cartesian velocity). We consider the following tasks: (1) Toasting: In this long-horizon task, the robot must pick up the target object, place it in the toaster, and then shut the toaster. A trial is marked as successful if the toaster if the full sequence is completed, and is marked as half successful if the object is only placed in the toaster. (2) Wiping: The robot must localize the sponge, grasp it, and then push the debris into the dustpan. The trial is marked successful if all debris is wiped at the end of the run."}, {"title": "VII. RESULTS", "content": "This section evaluates DiT-Block Policy on our task suite in order to contextualize its performance and analyze the source of its improvements. First, we compare DiT-Block Policy against the strongest baselines from the field in Sec. VII-A, and find an average improvement of 20%. Next, the ablation studies (see Sec. VII-B) reveal that the diffusion head implementation is critical for stable training, and observation tokenizer architecture provides a significant performance boost. Finally, we show that these findings generalize to different robot hardware in Sec. VII-C, and provide a standardized sim evaluation (see Sec. VII-D).\nA. Comparison to Prior SOTA Architectures\nOur first experiments compare DiT-Block Policy against SOTA baselines from the field in order to contextualize its performance. These baselines include: (1) Action Chunking Transformers [8] (ACT): ACT is built with a standard encoder-decoder transformer architecture, concretely DeTR [73]). The encoder processes input observation tokens, which include camera observations (encoded via ResNet-18 [16]), goal conditioning vectors, and (optionally) a latent plan vector computed from ground truth actions during training (randomly sampled during inference). The network is optimized via BC, using a L1-regression loss on expert actions. We implemented this baseline using the recommended hyper-parameters, and omitted the latent plan vector based on advice from the authors. In many respects, this model is analogous to DiT-Block Policy, but with a standard attention block and no diffusion loss. (2) Diffusion Policy w/ U-Net [10] (D.P. U-Net): This is the original Diffusion Policy implementation from Chi et. al. [10]. The camera observations are first processed into representation maps (via separate ResNets [16]), and then the dimensionality is reduced into a vector using spatial softmax [74]. This observation vector is then fed into a conditional U-Net network [13] that functions as the noise network. The policy is trained using the DDPM diffusion training objective. (3) Diffusion Policy w/ Transformer [10] (D.P. Transformer): This is the same setup as described previously, but with the U-Net noise network replaced with a Transformer, which uses a standard causal cross attention block. While higher capacity, this model is notoriously hard to train [10], [12].\nAll three baselines were compared against DiT-Block Policy on our bi-manual evaluation tasks. Each method was trained twice, once with just the demonstration data and once with BiPlay, in order to understand their data scaling properties. Full results are presented in Table II. We find that DiT-Block Policy is able to outperform the strongest by roughly 20% when trained with BiPlay, and by 10% when trained on task data alone. This indicates that DiT-Block Policy delivers SOTA performance, while also scaling better than the baselines. In addition, our method is able to deliver solid performance on all three tasks. In contrast, each of the other baselines has a task where it falls flat - e.g., ACT struggles with pen uncap, and D.P. U-Net struggles with sushi cutting. Finally, note that the D.P. Transformer baseline is unable to solve any of our tasks, because unstable training caused noisy/unsafe action prediction. Thus, we conclude that DiT-Block Policy learns diffusion policy transformers more stably than the baseline does.\nB. Ablation Studies\na) Ablating the attention mechanism: A key finding from the prior section is that DiT-Block Policy's transformer implementation enables more stable training and policy inference. But is this inherent to the transformer architecture, or a factor of some other hyper-parameter? Thus, we conduct an apples-to-apples comparison in order to answer this question. We compare DiT-Block Policy against 3 ablations that use the same exact setup, but with a different attention block: (1) Cross Attention: The diffusion decoder uses a standard per-layer cross attention block [11] to condition on memory embeddings from the encoder stack (i.e., ACT [8] + diffusion). Concurrent work [48] demonstrated SOTA results with this architecture, though with a much larger dataset (26K episodes) and extensive tuning. (2) In-Context: The memory embeddings from the encoder are added to the decoder in context, and all further processing happens with standard causal self-attention. (3) Non-Zero Initialization: This is an adaLN block, but without zero-initializing the final layers.\nWe compare these ablations against a DiT-Block Policy on the pick place and uncapping tasks. Results are presented in Table III. We find that the cross attention and in-context attention blocks are far less stable during training. It is still possible to generate stable actions during evaluation, by significantly increasing the number of diffusion steps during inference. However, the performance is still significantly reduced v.s. our DiT-Block Policy, and the slow inference speed results in jerky trajectories when deployed on the robot. In contrast, we find that the zero initialization ablation is able to effectively train and predict actions with fewer inference steps. But it still under-performs the DiT-Block Policy by 16%. Altogether, we conclude that the DiT-Block Policy's architecture offers a critical boost for diffusion transformer policy performance, and that the initialization scheme provides an additional boost on top.\nb) Ablating the image tokenization scheme: We evaluate our method's observation tokenizer by testing against ablations that move these parameters into the transformer encoder itself. Specifically, the ResNets are replaced with three small convolutional stem layers [4], [65] that produce an equivalent number of tokens (49 per image). Then, we train using these ablated observation tokens, and scale up the parameters to compensate. Results comparing these ablations against the full DiT-Block Policy are presented in Table IV. We find that DiT-Block Policy significantly outperforms the ablation with an even parameter count, and that even the significantly scaling up ablation is unable to compensate. This suggests that CNNs should still be considered as encoders for robotics tasks, particularly for low-data regimes.\nC. Generalization to Other Robot Morphologies\nThe final experiments test if our findings still generalize to a new robot embodiment. Specifically, we test generalization to a single-arm Franka robot. While this setup is morphologically simpler, there are a few important differences that could prove challenging in practice. First, we evaluate the policies with a single external camera so they will need to gracefully handle occlusion during manipulation. Second, these robots use a velocity action space, which may prove more difficult to learn. We evaluate the two strongest baselines against DiT-Block Policy on the toasting and wiping tasks. Results are presented in Fig. 5. Note that DiT-Block Policy again provides SOTA performance on these tasks: it outperforms ACT by 20% on average and D.P. U-Net by 35%. This suggests that DiT-Block Policy can generalize to new robots and is not overly sensitive to the particular choice of action and observation space, unlike the Diffusion Policy U-Net [10].\nD. Standardized Evaluation in Simulation\nWhile real-hardware evaluations are the ultimate test, it is often still useful to compare methods on reproducible, simulated task settings. Thus, we evaluate our DiT-Block Policy against the reference Diffusion Policy Transformer (D.P. Transformer) baseline from Chi et. al. [10] on the robomimic simulated task suite [23]. The results are reported in Table. V. We find that DiT-Block Policy almost completely matches D.P. Transformer on the simulated tasks, despite doing almost no task specific tuning (unlike D.P. Transformer). In addition, our method heavily out-performs the baseline on the real world experiments, which should carry more weight given the sim-to-real evaluation gap [27]."}, {"title": "VIII. CONCLUSION", "content": "This paper presents DiT-Block Policy, an improved transformer architecture that enables stable diffusion policy learning and efficient inference. Our experiments show that DiT-Block Policies provide SOTA performance across 5 tasks and 2 different robots, which have radically different observation spaces, action spaces, and morpohologies. We find that DiT-Block Policy outperform the strongest baselines by 20%, and are able to scale better with diverse play data. Our ablation study reveals that the exact configuration of DiT-Block Policy's transformer block is responsible for this increase. Standard joint-attention mechanisms are simply not able to learn policies as stably as DiT-Block Policy can. In addition, an ablation of our observation tokenizer reveals that using separate ResNet CNNs for image encoding provides stronger performance than using transformers alone. Even scaling the transformers is not enough to make up for this difference. Finally, we open-source the BiPlay dataset used in our experiments. This is the first language annotated, bi-manual dataset with diverse scenes, tasks, and objects."}]}