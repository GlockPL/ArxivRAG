{"title": "The FIX Benchmark:\nExtracting Features Interpretable to eXperts", "authors": ["Helen Jin", "Shreya Havaldar", "Chaehyeon Kim", "Anton Xue", "Weiqiu You", "Helen Qu", "Marco Gatti", "Daniel A Hashimoto", "Bhuvnesh Jain", "Amin Madani", "Masao Sako", "Lyle Ungar", "Eric Wong"], "abstract": "Feature-based methods are commonly used to explain model predictions, but these\nmethods often implicitly assume that interpretable features are readily available.\nHowever, this is often not the case for high-dimensional data, and it can be hard\neven for domain experts to mathematically specify which features are important.\nCan we instead automatically extract collections or groups of features that are\naligned with expert knowledge? To address this gap, we present FIX (Features\nInterpretable to eXperts), a benchmark for measuring how well a collection of\nfeatures aligns with expert knowledge. In collaboration with domain experts, we\nhave developed feature interpretability objectives across diverse real-world settings\nand unified them into a single framework that is the FIX benchmark. We find\nthat popular feature-based explanation methods have poor alignment with expert-\nspecified knowledge, highlighting the need for new methods that can better identify\nfeatures interpretable to experts.", "sections": [{"title": "1 Introduction", "content": "Machine learning is increasingly used in domains like healthcare [67, 75, 84], law [9], gover-\nance [56], automation [15], education [36] and finance [59]. However, modern models are often\nblack-box, which makes it hard for practitioners to understand their decision-making and safely use\nmodel outputs [66]. For example, surgeons are concerned that blind trust in model predictions will\nlead to poorer patient outcomes [29]; in law, there are known instances of wrongful incarcerations\ndue to over-reliance on faulty model predictions [92, 95]. Although such models have promising\napplications, their opaque nature is a liability in domains where transparency is crucial [37, 39].\nTo address the pertinent need for transparency and explainability of their decision-making, the\ninterpretability of machine learning models has emerged as a central focus of recent research [8, 72,\n73]. A popular and well-studied class of interpretability methods is known as feature attributions [50,\n68, 82]. Given a model and an input, a feature attribution method assigns scores to input features that\nreflect their respective importance toward the model's prediction. A key limitation, however, is that\nthe attribution scores are only as interpretable as the underlying features themselves [100].\nFeature-based explanation methods commonly assume that the given features are already interpretable\nto the user, but this typically only holds for low-dimensional data. With high-dimensional data like"}, {"title": "2 Related Work", "content": "Interpretability. Interpretability in machine learning is often viewed as a multifaceted concept\nthat involves algorithmic transparency [28, 65, 76], explanation methods [33, 54], and visualization\ntechniques [12, 78, 88], among others. In this work, we focus on feature-level interpretability, a\ncentral topic in interpretability research [37, 62]. Feature-based methods are popular because they are\nbelieved to offer simple, adaptable, and intuitive settings in which to analyze and develop interpretable"}, {"title": "3 Expert Feature Extraction", "content": "Feature-based explanation methods require interpretable features to be effective. For example,\nsurgeons communicate safety in surgery with respect to key anatomical structures and organs, which\nare interpretable features for surgeons [30, 81]. These interpretable features are a key bridge that can\nhelp surgical AI assistants communicate effectively with surgeons."}, {"title": "3.1 Measuring Alignment of Extracted Features with Expert Features", "content": "At the core of the FIX benchmark is a general framework for measuring the overall alignment of\nextracted features with expert features in diverse settings. Let G be a proposed set of expert features for\nan input x \u2208 Rd. We assume that we have a setting-specific function EXPERTALIGN(\u011d, x, 0) \u2192 [0,1]\nthat captures how interpretable a single extracted feature \u011d \u2208 \u011cis for the input x with (optional)\nfunction parameters 0, where a score of 1 corresponds to an expert feature and 0 corresponds to an\nuninterpretable feature.\nFor convenience, let \u011c[i] = {\u011d \u2208 \u011c : i \u2208 \u011d} \u2286 \u011c be the set of all extracted features that contain the\ni-th feature for i = 1, . . ., d. To measure the overall quality of a set of groups G for an example x,\nwe define the following FIXSCORE:\nFIXSCORE(\u011c, x) = \\frac{1}{d} \\sum_{i=1}^{d} \\frac{1}{\\mid G[i] \\mid} \\sum_{\\hat{g} \\in G[i]} EXPERTALIGN(\\hat{g}, x, \\theta).\nAt a high level, the FIXSCORE computes an average expert alignment for each raw feature based on\nthe extracted features and returns the average result over all raw features. The FIXSCORE has two\nadvantageous properties:\n1. Duplication invariance at optimality. If one extracts perfect expert features (i.e., with an\nalignment score of 1), the FIXSCORE cannot be increased further by duplicating expert\nfeatures. This property ensures that the score cannot be trivially inflated with repeats.\n2. Encourages diversity of expert features. Since the score aggregates a value for each\nfeature from i = 1, . . ., d, adding a new expert feature that does not yet overlap with already\nextracted features is always beneficial.\nThe use of a generic expert alignment function enables the FIXSCORE to accommodate a diverse set\nof applications. There are two main ways one can specify the EXPERTALIGN function: implicitly\nwith a score specified by an expert or explicitly with annotations from an expert.\nCase 1: Implicit Expert Alignment. Suppose we do not have explicit annotations of expert features\nfor ground truth groups. In this case, we use implicit expert features defined indirectly via a scoring\nfunction that measures the quality of an extracted feature. The exact formula of the score is specified\nby an expert and will depend on the domain and task. Implicit expert features have the advantage of\npotentially being easier and more scalable than manual expert annotation. The Mass Maps dataset,\nSupernova dataset, Multilingual Politeness dataset, and Emotion dataset are examples of the implicit\nexpert features case.\nCase 2: Explicit Expert Alignment. In the case where we do have annotations for expert features,\nwe can use a standardized expression for the FIXSCORE that measures the best possible intersection\nwith the annotated expert features. Then, the expert alignment score of a feature group \u011d is\nEXPERTALIGN(\\hat{g}, x, G^*) = \\max_{g^*\\in G^*} MATCH(\\hat{g}, g^*), where MATCH(\\hat{g}, g^*) = \\frac{\\mid \\hat{g} \\cap g^* \\mid}{\\mid \\hat{g} \\cup g^*\\mid}."}, {"title": "4 FIX Datasets", "content": "In this section, we briefly describe each FIX dataset. For each dataset, we provide an overview of\nthe domain task and the problem setup. We then introduce the key expert alignment function that\nmeasures the quality of an expert feature, and explain why certain properties incorporated in the\nexpert alignment function are desirable to experts."}, {"title": "4.1 Mass Maps Dataset", "content": "Motivation. A major focus of cosmology is on the initial state of the universe, which can be\ncharacterized by various cosmological parameters such as \u03a9m, which relates to energy density, and\n08, which pertains to matter fluctuations [1]. These parameters influence what is observable by mass\nmaps, also known as weak lensing maps, which capture the spatial distribution of matter density in the\nuniverse. Although mass maps can be obtained through the precise measurement of galaxies [25, 40],\nit is not known how to directly measure Om and 08 for different galaxies. This has inspired machine\nlearning efforts to predict \u03a9m and 08 from artificial simulations [24, 55, 69] to predict \u03a9m and \u03c3\u03b5\nfrom artificial simulations, but the black-box nature of such models makes it difficult for cosmologists\nto gain insights into reliably predicting real cosmological parameters.\nProblem Setup. Our dataset contains clean simulations from CosmoGridV1 [41]. Each input is a\none-channel image of size (66, 66), where the regression task is to predict Om and 08. \u03a9m captures\nthe average energy density of all matter relative to the total energy density, including radiation and\ndark energy, while 08 describes the fluctuation in the distribution of matter [1].\nExpert Features. When inferring Om and 68 from the mass maps, we aim to discover which\ncosmological structures most influence these parameters. Two types of cosmological structures in\nmass maps known to cosmologists are voids and clusters [55]. An example is illustrated in Figure 3,\nwhere voids are large regions that are under-dense relative to the mean density and appear as dark,\nwhile clusters are over-dense and appear as bright dots."}, {"title": "4.2 Supernova Dataset", "content": "Motivation. The astronomical time-series classification, as mentioned in [83], involves categorizing\nastronomical sources that change over time. This task analyzes simulation datasets that will be\ndiscovered through the Legacy Survey of Space and Time (LSST) [101]. Given the vastness of\nthe universe, it is essential to identify the time periods that have the most significant impact on\nclassification to optimize telescope observations. Time periods with no observed data are less useful.\nTo avoid costly searching over all timestamps for high-influence time periods, we aim to identify\nsignificant timestamps that are linearly consistent in specific wavelengths.\nProblem Setup. We take parts of the dataset from the original PLAsTiCC challenge [83]. The input\ndata comprises four columns: observation times (modified Julian days), wavelength (filter), flux\nvalues, and flux error. The dataset encompasses 7 distinct wavelengths that work as filters, and the\nflux values and errors are recorded at specific time intervals for each wavelength. The classification\ntask is to predict these simulated LSST observations.\nExpert Features. A feature with linearly consistent flux for each wavelength is considered more\ninterpretable in astrophysics. An illustration of expert features used for supernova classification is\npresented in Figure 4. This example showcases the flux value and error for various wavelengths, each\nrepresented by a different color. We colored the timestamp of expert features with the wavelength\ncolor with the highest linear consistency score. For the timestamp where there is no data point, we\ndo not recognize it as an expert feature. We create a linear consistency metric to assess the expert\nalignment score of a proposed feature in the context of a supernova. Our linear consistency metric\nuses p, the percentage of data points that display linear consistency, penalized by d, the percentage of\ntime stamps containing data points:\nEXPERTALIGN(\\hat{g}, x, \\theta) = \\max_{W \\in W} p(\\hat{g}, x_w) \\cdot d(\\hat{g}, x_{\\omega}, \\theta)."}, {"title": "4.3 Multilingual Politeness Dataset", "content": "Motivation. Different cultures express politeness differently [45, 63]. For instance, politeness in\nJapan often involves acknowledging the place of others [77], whereas politeness in Spanish-speaking\ncountries focuses on establishing mutual respect [64]. Therefore, grounding interpretable features\nthat indicate politeness is language-dependent. Previous work from Danescu-Niculescu-Mizil et al.\n[14] and Li et al. [47] use past politeness research to create lexica that indicate politeness/rudeness, in\nEnglish and Chinese respectively. A lexicon is a set of categories, containing a curated list of words"}, {"title": "4.4 Emotion Dataset", "content": "Motivation. Emotion classification involves inferring the emotion (e.g., Joy, Anger, etc.) reflected in\na piece of text. Researchers study emotion to build systems that can understand emotion and thus\nadapt accordingly when interacting with human users. For extracted features to be useful, they must\nbe relevant to emotion. For example, a word like \"puppy\" may be used more frequently in comments\nlabeled with Joy vs. other emotions; therefore, it may be extracted as a relevant feature for the Joy\nclass. However, this is a spurious correlation \u2013 emotional expression is not necessarily tied to a\nsubject, and comments containing the word \"puppy\" could just as easily be angry or sad.\nProblem Setup. The GoEmotions dataset from Demszky et al. [16] contains 58,000 English Reddit\ncomments labeled for 27 emotion categories, or \u201cneutral\u201d if no emotion is applicable. The input is a\ntext utterance of 1-2 sentences extracted from Reddit comments, and the output is a binary label for\neach of the 27 emotion categories.\nExpert Features. Example expert features for are shown in Table 1. To measure how emotion-related\na feature is, we use the circumplex model of affect [70]. The circumplex model assumes that all\nemotions can be projected onto the 2D unit circle with respect to two independent dimensions\n\u2013 arousal (the magnitude of intensity or activation) and valence (how negative or positive). By\nprojecting features onto the unit circle, we can quantify emotional relations. In particular, we\ncalculate the following two attributes of the features with a group: (1) Their emotional signal, i.e.,\nmean distance to the circumplex and (2) their emotional relatedness, i.e., mean pairwise distance\nwithin the circumplex. We then calculate the following: Signal(\u011d, x), which measures the average\nEuclidean distance to the circumplex for every projected feature in \u011d, and Relatedness(\u011d, x), which"}, {"title": "4.5 Chest X-Ray Dataset", "content": "Motivation. Chest X-ray imaging is a common procedure for diagnosing conditions such as atelecta-\nsis, cardiomegaly, and effusion, among others. Although radiologists are skilled at analyzing such\nimages, modern machine learning models are increasingly competitive in diagnostic performance [4].\nThis suggests the possibility of using machine learning models to assist radiologists in making\ndiagnoses. However, in the absence of an explanation, radiologists may only trust the model output if\nit matches their own predictions. Moreover, inaccurate AI assistants are shown to negatively affect\ndiagnostic performance [94], suggesting the use of explainability as a safeguard to let radiologists\ndecide whether or not to trust the model. As such, it is important for machine learning models to\nprovide explanations of their diagnoses.\nProblem Setup. We use the NIH-Google dataset [52] available from the TorchXRayVision li-\nbrary [13]. This is a relabeling of the NIH ChestX-ray14 dataset [89] to improve the quality of\nthe original labels, which contains 28,868 chest X-ray images labeled for 14 common pathology\ncategories: atelectasis, calcification, cardiomegaly, etc. We randomly split the dataset into train/test\nsplits of 23,094 and 5,774, respectively. The task is a multi-label classification problem for identifying\nthe presence of each pathology.\nExpert Features. Radiology reports commonly refer to anatomical structures (e.g., spine, lungs),\nwhich allows radiologists to perform and communicate accurate diagnoses to patients. We provide\nthese expert-interpretable features in the form of anatomical structure segmentations. However,\nbecause we could not find datasets with pathology labels and anatomical segmentations, we used a\npre-trained model from TorchXRayVision to generate the structure labelings for each image. The 14\nstructures, including the left clavicle, heart, etc., correspond to the explicit expert-labeled case."}, {"title": "4.6 Laparoscopic Cholecystectomy Surgery Dataset", "content": "Motivation. Laparoscopic cholecystectomy (gallbladder removal) is one of the most common elective\nabdominal surgeries performed in the US, with over 750,000 operations annually [80]. A common\ncomplication of laparoscopic surgery is bile duct injury, which is associated with an 8-fold increase in\nmortality [57] and accounts for more than $1B in US healthcare annual spending [10]. Notably, 97%\nof such complications are due to human visualization errors [90], primarily due to the complicated\nsurgery site that commonly contains obstructing tissues, inflammation, and other patient-specific\nartifacts - all of which may prevent the surgeon from getting a perfect view. Consequently, there is\ngrowing interest in harnessing advanced vision models to help surgeons identify safe and risky areas\nfor operation. However, experienced surgeons rarely trust model outputs due to their opaque nature,\nwhile inexperienced surgeons risk over-relying on model predictions. Therefore, any safely deployed\nand useful machine learning model must be able to provide explanations that align with surgeons'\nexpectations.\nProblem Setup. The task is to identify the safe and unsafe areas of surgery. We use the open-source\nsubset of the data from [51], wherein the authors enlist surgeons to annotate surgery video data from\nthe M2CAI16 workflow challenge [79] and Cholec80 [86] datasets. This dataset consists of 1015\nannotated images with a random train/test split of 812 and 203, respectively.\nExpert Features. In cholecystectomy, it is a common practice for surgeons to identify the critical\nview of safety before performing any irreversible operations [30, 81]. This view identifies the location\nof vital organs and structures that inform the safe region of operation and is incidentally what surgeons\noften expect as part of an explanation. We provide these expert-interpretable labels in the form of\norgan segmentations (liver, gallbladder, hepatocystic triangle). These surgeon-annotated organ labels\nare taken from Madani et al. [51] and correspond to the explicit expert-labeled case."}, {"title": "5 Baseline Algorithms & Discussion", "content": "We evaluate standard techniques widely used within the vision, text, and time series domains to create\nhigher-level features. We provide a brief summary below, with additional details in Appendix C."}, {"title": "5.1 Domain-Centric Baselines", "content": "Image Baselines. For image data, we consider three segmentation methods [43]. Patches [19]\ndivides the image into grids where each cell is the same size. Quickshift [27] connects similar\nneighboring pixels into a common superpixel. Watershed [46] simulates flooding on a topographic\nsurface. CRAFT [23] generates concept attribution maps.\nTime Series Baselines. For time series data, we can take equal size slices of the data across time as\npatches [74]. We use different slice sizes to see how they impact multiple baselines. We take various\nslice sizes, such as 5, 10, and 15, separately to evaluate the results of multiple baselines.\nText Baselines. For text data, we present three baselines for extracting features [71]. At the finest\ngranularity, we treat each word as a feature. The second baseline considers each phrase as a feature.\nPhrases are comprised of groups of words that are separated by some punctuation in the original text.\nAt the coarsest granularity, we treat each sentence as a feature."}, {"title": "5.2 Domain-Agnostic Baselines", "content": "For all data modalities and domains, we consider domain-agnostic baselines to extract more meaning-\nful features.\nIdentity. We combine all elements into one single group.\nRandom. We select features at random, up to the maximum baseline results for the group. The group\nmaximum is calculated as follows:\ngroup maximum = scaling factor \u00d7 size of distinct expert feature"}, {"title": "5.3 Results and Discussions", "content": "We show results on the baselines in Table 2. For image datasets, Quickshift has the best performance\ncompared to Patch and Watershed on both the Cholecystectomy dataset and the Chest X-ray dataset,\nsince they have natural images. All baselines perform similarly for the Mass Maps dataset. This\nis potentially because mass maps are not natural images, but the map intensities are similar to a"}, {"title": "6 Conclusion", "content": "We propose FIX, a curated benchmark of datasets with evaluation metrics for extracting expert\nfeatures in diverse real-world settings. Our benchmark addresses a gap in the literature by providing\nresearchers with an environment to study and automatically extract interpretable features for experts.\nLimitations and Future Work. The FIX benchmark is not an exhaustive specification of all expert\nfeatures, and may fail to capture others. The ones we included are generally non-controversial and\nwell-accepted by the domain's expert community, but we can foresee that there are cases where\nthis may not be true. Dealing with potential conflicting expert opinions may need a more nuanced\napproach, which is left for future work. Furthermore, although we cover cosmology, psychology, and\nmedicine domains in this work, the metrics for these domains may not be appropriate for all\nsettings. We encourage prospective users to consider and implement metrics most appropriate to their\nparticular settings. We leave as future work the development of new, general purpose techniques that\ncan extract expert features from data and models without supervision.\nBroader Impacts. This work seeks to make explainable machine learning more accessible to experts.\nHowever, misleading explanations have the potential to cause harm if users blindly trust machine\nlearning even when it is wrong."}, {"title": "A Dataset Details", "content": "All datasets and their respective Croissant metadata records and licenses are available on HuggingFace\nat the following links.\n\u2022 Mass Maps:\n\u2022 Supernova:\n\u2022 Multilingual Politeness:\n\u2022 Emotion:\n\u2022 Chest X-Ray:\n\u2022 Laparoscopic Cholecystectomy Surgery:"}, {"title": "A.1 Mass Maps Dataset", "content": "Problem Setup. We randomly split the data to consist of 90,000 train and 10,000 validation maps\nand maintain the original 10,000 test maps. We follow the post-processing procedure in Jeffrey\net al. [40], You et al. [93] for low-noise maps. Following previous works [24, 55, 69, 93], we use a\nCNN-based model for predicting Om and 08.\nMetric. Let x \u2208 Rd be the input mass map with d = H \u00d7 W pixels, and g \u2208 {0,1}d be a boolean\nmask g that describes which pixels belong to the group, where gi = 1 if the ith pixel belongs to the\ngroup, and 0 otherwise.\nWe can compute the purity score of each group to void and cluster. We say a pixel is a void\n(underdensed) pixel if its intensity is below 0, and a cluster (overdensed) pixel if its intensity is above\n30(x), following previous works [55, 93]. We first compute the proportion of void pixels and cluster\npixels in feature g\nPv (g,x) = \\frac{\\sum_{i=1}^{d}1[g_ix_i <0]}{g^T1},   Pc(g, x) = \\frac{\\sum_{i=1}^{d}1[g_ix_i > 3\\sigma(x)]}{g^T1}\nwhere 1 \u2208 1d is the identity matrix, the numerators count the number of underdensed or overdensed\npixels, and g1 is the number of pixels in the feature. In practice, we add a small \u20ac = 10-6 to Pu\nand Pe and renormalize them, to avoid taking the log of 0 later. Next, we compute the proportion of\npixels that are void or cluster, only among the void/cluster pixels:\nP_(v)(g, x) = \\frac{P_v(g, x)}{P_v(g, x) + P_c(g, x)},    P_(c)(g, x) = \\frac{P_c(g, x)}{P_v(g, x) + P_c(g, x)}\nThen, we compute the EXPERTALIGN score for the predicted feature \u011d by computing the void/cluster-\nonly entropy reversed and scaled to [0, 1], weighted by the percentage of void/cluster pixels among\nall pixels.\nPurity_{vc}(\\hat{g}, x) = (1 + P(\\hat{g}, x) \\log_2 P_(\\hat{g}, x) + P(\\hat{g}, x) \\log_2 P_(\\hat{g}, x))\nwhere -(P(\u011d, x) log2 P\u02b9(\u011d, x) + P(\u011d, x) log2 Pl(\u011d, x)) is the entropy computed only on void and\ncluster pixels, a close to 0 score indicating that the interpretable portion of the feature is mostly void\nor cluster. Purityve(\u011d, x) is 0 if among the pixels in the proposed feature that are either void or\ncluster pixels, half are void and half are cluster pixels, and 1 if all are void or all are cluster pixels,\nregardless of how many other pixels there are in the proposed feature.\nWe also have the ratio\nRatio_{vc}(\\hat{g}, x) = (P_v(\\hat{g}, x) + P_c(\\hat{g}, x))\nwhich is the total proportion of the feature that is any interpretable feature type at all."}, {"title": "A.2 Supernova Dataset", "content": "Problem Setup. We extracted data from the PLASTiCC Astronomical Classification challenge [83].\n3 PLASTICC dataset was designed to replicate a selection of observed objects with type information\ntypically used to train a machine learning classifier. The challenge aims to categorize a realistic\nsimulation of all LSST observations that are dimmer and more distorted than those in the training set.\nThe dataset contains 15 classes, with 14 of them present in the training sample. The remaining class\nis intended to encompass intriguing objects that are theorized to exist but have not yet been observed.\nIn our dataset, we split the original training set into 90/10 training/validation, and the original test set\nwas uploaded unchanged. We made these sets balanced for each class. The class includes objects\nsuch as tidal disruption event (TDE), peculiar type Ia supernova (SNIax), type Ibc supernova (SNIbc),\nand kilonova (KN). The dataset contains four columns: observation times (modified Julian days,\nMJD), wavelength (filter), flux values, and flux error. Spectroscopy measures the flux with respect to\nwavelength, similar to using a prism to split light into different colors.\nDue to the expected high volume of data from upcoming sky surveys, it is not possible to obtain\nspectroscopic observations for every object. However, these observations are crucial for us. Therefore,\nwe use an approach to capture images of objects through different filters, where each filter selects light\nwithin a specific broad wavelength range. The supernova dataset includes 7 different wavelengths\nthat are used. The flux values and errors are recorded at specific time intervals for each wavelength.\nThese values are utilized to predict the class that this data should be classified into.\nMetric. We use the following expert alignment metric to measure if a group of features is inter-\npretable:\nEXPERTALIGN(\\hat{g}, x, \\theta) = \\max_{W\\in W} LinearConsistency(\\hat{g}, x_w, \\theta)\nwhere W is the set of unique wavelength, \u011d is the feature group, and xw is the subset of x within\nwavelength w. In the supernova setting, @ includes three parameters: \u20ac, the parameter for how much\nstandard deviation o is allowed, window size A and the step size T. Therefore, we formulate the\nLinear Consistency function as follows:\nLinear Consistency(\\hat{g}, x_w, \\theta) = p(\\hat{g}, x_w, \\theta) d(g, x_w, \\theta)\np(\\hat{g}, x_w, \\theta) is the percentage of data points that display linear consistency, penalized by d(\\hat{g}, x_w, \\theta),\nwhich is the percentage of time steps containing data points.\nLet B(x, y) = arg minB (XTB \u2013 y)2, where X = [x_1] and B = [B1 Bo]. Here, B1 is the slope\nand Bo is the intercept. M is the number of data points in xw, and \u0177w,i = Xw,i \u00b7 B. Then, we have\np(\\hat{g}, x_w, \\theta) = \\frac{1}{M} \\sum_{i=1}^{M}1[Y_{\\omega,i} \\in [Y_{\\omega,i} \u2013 \\epsilon \\cdot W_{\\omega,i}, Y_{\\omega,i} + \\epsilon \\cdot W_{\\omega,i}]]\nLet t1, ..., to be time steps at step size intervals. Then t\u2081 = tstart + i * T, and N is the number of\ntime steps. We also have\nd(g, x_w, \\theta) = \\frac{1}{N} \\sum_{i=1}^{N}1[\\exists_1: x_{w,i} \\in [t_i, t_i + \\lambda]]\nA higher EXPERTALIGN(\u011d, x, \u03b8) \u2208 [0,1] value means the flux slope at each wavelength is consis-\ntently linear and there are not many time intervals without data."}, {"title": "A.3 Multilingual Politeness Dataset", "content": "Problem Setup. This politeness dataset from Havaldar et al. [32] is intended for politeness clas-\nsification, and would likely be solved via a fine-tuned multilingual LLM. Namely, this would be a\nregression task, using a trained LLM to output the politeness level of a given conversation snippet as\na real number ranging from -2 to 2.\nThe dataset is accompanied by a theory-grounded politeness lexica. Such lexica built with domain\nexpert input have been promising for explaining style [14], culture [34], and other such complex\nmultilingual constructs.\nMetric. Assume a theory-grounded Lexica L with k categories: L = 11, 12, ...lk, where each\nset li W, where W is the set of all words. For each category, we use an LLM to embed all\nthe contained words and then average the resulting embeddings, to get a set C of k centroids:\nC = C1, C2, ...Ck. We define this formally as:\nC:\\frac{1}{\\mid l_i \\mid} \\sum_{s \\in l_i} embedding(w) for all s \\in [1,4]\\nFor a group \u011d containing words w1, W2, ..., the group-level expert alignment score can be computed\nas follows:\nEXPERTALIGN(\\hat{g}, x, \\theta) = \\max_{i=1} \\frac{1}{\\mid g \\mid} \\sum_{weg} cos(embedding(w), c)\nNote that each language has a different theory-grounded lexicon, so we calculate a unique domain\nalignment score for each language."}, {"title": "A.4 Emotion Dataset", "content": "Problem Setup. This dataset is intended for emotion classification and is currently solved with a\nfine-tuned LLM [16]. Namely, this is a classification task where an LLM is trained to select some\nsubset of 28 emotions (including neutrality) given a 1-2 sentence Reddit comment."}, {"title": "A.5 Chest X-Ray Dataset", "content": "We used datasets and pretrained models from TorchXRayVision [13].4 In particular, we use the\nNIH-Google dataset [52], which is a relabeling of the NIH ChestX-ray14 dataset [89]. This dataset\ncontains 28,868 chest X-ray images labeled for 14 common pathology categories, with a train/test\nsplit of 23,094 and 5,774. We additionally used a pre-trained structure segmentation model to produce\n14 segmentations. The task is a multi-label classification problem for identifying the presence of each\npathology. The 14 pathologies are:\nAtelectasis, Cardiomegaly, Consolidation, Edema, Effusion, Emphysema, Fibrosis,\nHernia, Infiltration, Mass, Nodule, Pleural Thickening, Pneumonia, Pneumothorax\nThe 14 anatomical structures are:\nLeft Clavicle, Right Clavicle, Left Scapula, Right Scapula, Left Lung, Right Lung,\nLeft Hilus Pulmonis, Right Hilus Pulmonis, Heart, Aorta, Facies Diaphragmatica,\nMediastinum, Weasand, Spine"}, {"title": "A.6 Laparoscopic Cholecystectomy Surgery Dataset", "content": "We use the open-source subset of the data from [51], which consists of surgeon-annotated video data\ntaken from the M2CAI16 workflow challenge [79] and Cholec80 [86] datasets. The task is to identify\nthe safe/unsafe regions of where to operate. Specifically, each pixel of the image has one of three\nlabels: background, safe, or unsafe. The expert labels provide each pixel with one of four labels:\nbackground, liver, gallbladder, and hepatocystic triangle."}, {"title": "B Interpretable Feature Extraction Details", "content": "Figure 8 illustrates a graphical model representing the Interpretable Feature Extraction pipeline for a\ngiven FIX dataset."}, {"title": "C Baselines Details", "content": "The FIX benchmark is publicly available at:\nBootstrapping. For each setting's baselines experiments, we use a bootstrapping method (with\nreplacement) to estimate the standard deviation of the sample means of FIXSCORE.\nGroup Maximum. For the number of groups, we take the scaling factor multiplied by the size of\nthe distinct expert feature, which differs for each setting. The scaling factor we choose across all\nsetting is 1.5 (and round up to the next nice whole number).\nIn the case of a supernova setting, we consider a distinct expert feature size of 6. This is because the\nmaximum number of distinct expert features we can obtain is 6, given that there are a maximum of\n3 humps in the time series dataset. For each hump, there are both peaks and troughs, leading to a\npotential maximum of 6 distinct expert features.\nFor the multilingual politeness setting, the group maximum would be 40, which is the total number\nof lexical categories, 26, with the scaling factor multiplied in to give some flexibility.\nFor the emotion setting, the group maximum would be, which is the total"}]}