{"title": "RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs", "authors": ["John Dang", "Arash Ahmadian", "Kelly Marchisio", "Julia Kreutzer", "Ahmet \u00dcst\u00fcn", "Sara Hooker"], "abstract": "Preference optimization techniques have become a standard final stage for training state-of-art large language models (LLMs). However, despite widespread adoption, the vast majority of work to-date has focused on first-class citizen languages like English and Chinese. This captures a small fraction of the languages in the world, but also makes it unclear which aspects of current state-of-the-art research transfer to a multilingual setting. In this work, we perform an exhaustive study to achieve a new state-of-the-art in aligning multilingual LLMs. We introduce a novel, scalable method for generating high-quality multilingual feedback data to balance data coverage. We establish the benefits of cross-lingual transfer and increased dataset size in preference training. Our preference-trained model achieves a 54.4% win-rate against Aya 23 8B, the current state-of-the-art multilingual LLM in its parameter class, and a 69.5% win-rate or higher against widely used models like Gemma-1.1-7B-it, Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.3. As a result of our study, we expand the frontier of alignment techniques to 23 languages covering half of the world's population.", "sections": [{"title": "Introduction", "content": "What languages are favored in technological progress is often deeply intertwined with historical patterns of technology access and resources (\u2200 et al., 2020; Bird, 2022; Singh et al., 2024). Preference optimization is a valuable and widely adopted post-training technique to align large language models (LLMs) with human preferences (Christiano et al., 2017b; Stiennon et al., 2022; Ouyang et al., 2022a; Bai et al., 2022) which has been shown to lead to large gains in performance across a wide variety of NLP tasks (Wang et al., 2024; Ivison et al., 2023; Xu et al., 2024; Lightman et al., 2024). To-date, the majority of progress in preference optimization has over-fit to a small handful of languages, resulting in large gaps in performance outside of English (Schwartz et al., 2022; Kotek et al., 2023; Khandelwal et al., 2023; Vashishtha et al., 2023; Khondaker et al., 2023), and also risks introducing"}, {"title": "Methodology", "content": null}, {"title": "Addressing Data Scarcity", "content": "Prior works on multilingual preference training such as Okapi (Lai et al., 2023), typically involved using preference data translated from English, however, the Okapi models has since been outper- formed by non-preference aligned models including the base Aya 23 8B model we experiment with here (Aryabumi et al., 2024). While language coverage may be improved by translation (Ranaldi & Pucci, 2023; \u00dcst\u00fcn et al., 2024), the introduction of translation artifacts known as translationese (Bizzoni et al., 2020; Vanmassenhove et al., 2021) can hurt performance. Furthermore,"}, {"title": "Offline vs Online Preference Training", "content": "Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017b; Stiennon et al., 2022; Ouyang et al., 2022a; Bai et al., 2022), which was proposed as the first framework for aligning language models to human preferences, has become a key for training state-of-the-art LLMs (OpenAI et al., 2023; Touvron et al., 2023; Anthropic, 2024; Reid et al., 2024). Canonically, PPO (Schulman et al., 2017b) has been used in RLHF as the online RL algorithm (Stiennon et al., 2022; Ouyang et al., 2022b; Nakano et al., 2021). However, recent offline methods such as Direct Preference Optimization (DPO) (Rafailov et al., 2023) and subsequent works in the same direction (Azar et al., 2024; Ethayarajh et al., 2024; Choi et al., 2024), have proven increasingly popular due to reduced computational complexity. Traditional online methods such as PPO and REINFORCE (Williams, 1992) require an additional network in addition to the policy, maintaining a reward model in memory, and also using the policy to generate doing training, all of which DPO does not require as it is fully offline."}, {"title": "Experimental Set-up", "content": null}, {"title": "Multilingual Base Model", "content": "We perform all experiments with Aya 23 8B (Aryabumi et al., 2024) which is chosen because it (1) is massively multilingual, pre-trained and supervised fine-tuned for 23 languages, and (2) it achieves state-of-the-art multilingual performance in 23 languages compared to other commonly used LLMs in its class, outperforming Mistral-7B-Instruct-v0.3 (Jiang et al., 2023a), Gemma-1.1-7B-it (Gemma-Team, 2024), and Aya-101-13B (\u00dcst\u00fcn et al., 2024). On multilingual benchmarks and open-ended generations, Aya 23 achieves a 65% win-rate or higher in head-to-head comparisons with popular open sourcee models(Aryabumi et al., 2024). Furthermore, Aya 23 is an open weights model that is not preference-trained, allowing us to isolate the impact of multilingual preference optimization in our experiments."}, {"title": "Reward Model", "content": "We use the Cohere reward model (Cohere May 2024) which is competitive with top-scoring state-of-the-art RMS on the RewardBench Leaderboard (Lambert et al., 2024), scoring 88.2, which currently is ranked 4th. This reward model achieves high LLM response ranking agreement with GPT- 4-Turbo on English and non-English languages as shown in Table 1. We intentionally use a separate reward model from the model we use for llm-as-a-judge evaluation (GPT-4-Turbo7) given the known biases incurred by using the same model for both (Verga et al., 2024; Bansal et al., 2024)."}, {"title": "Preference Optimization Training", "content": "We train Aya 23 8B model for 2 epochs in both DPO and RLOO experiments. DPO runs are trained with KL-penalty \u03b2 = 0.5, learning rate 5e-7, and AdamW optimizer (Kingma & Ba, 2014). RLOO runs are trained with RLOO k = 2, KL-penalty B = 0.01, learning rate 5e-6, AdamW optimizer, and online generation sampling tem- perature of 0.75. All runs are performed on a single node with either 8 x Nvidia A100 80GB or 8 x Nvidia H100 80GB GPUs with DeepSpeed ZeRO Stage 3 (Rajbhandari et al., 2020) and full fine-tuning of all 8 billion model parameters. We performed hyperparameter sweeps for learning rate Ir \u2208 {5e-8,5e-7,5e-6,5e-5} and for KL-penalty \u03b2\u2208 {0.05,0.1,0.5} for both DPO and RLOO to the best of our ability. For a fair comparison with DPO, we utilize the same reward model for RLOO which is used to generate our synthetic multilingual preference dataset (for ranking the generations)."}, {"title": "Model Comparisons", "content": "We evaluate against multiple state-of-the-art open-source models to ensure a comprehensive evalu- ation. Details of each model are below:"}, {"title": "Evaluation", "content": "We assess the preference-optimized models on the multilingual open-ended generation and summa- rization tasks using LLM-simulated evaluation:"}, {"title": "Open-ended generations", "content": "For open-ended generations, we use dolly-machine-translated test set from the Aya evaluation suite (Singh et al., 2024) which is a held-out test set of 200 instances from the Dolly-15k dataset (Conover et al., 2023) translated into 101 languages. This test set was curated by avoiding instructions that include culturally-specific or geographic references. For languages that are available (Arabic, French, Hindi, Russian, and Spanish),"}, {"title": "Results and Discussion", "content": "Win-rates Against Open-Weights Models Figure 1 and Table 2 show the win-rates of our preference-trained models against state-of-the-art open-source models. Importantly, the base Aya 23 8B already achieves high win-rates against Gemma-1.1 (62.1%), Llama-3 (66.6%), and Mistral- v0.3 (69%) averaged across all 23 languages on Dolly open-ended generations. Preference-optimized Aya models extend this lead further. Concretely, our best variant of RLOO leads to 69.5% (+7.4), 72.4% (+5.8), and 77.5 (+8.5) win-rates against Gemma, Llama-3, and Mistral respectively."}, {"title": "Conclusion", "content": "Our work presents a comprehensive study on multilingual preference optimization. We show that the inclusion of multilingual data in preference optimization leads to significant improvements in multilingual performance over English-only preference optimization. This improvement scales both with the number of languages and the total number of examples included in the training data. Additionally, we show that preference training exhibits cross-lingual transfer, leading to significant gains in languages not present in the training data. We also find that using online preference opti- mization outperforms offline preference optimization, highlighting the importance of online samples during training. As a result of our study, we expand the frontier of alignment techniques to 23 languages which cover half the world's population, by successfully preference-training an 8-billion parameter Aya 23 model that outperforms both the original Aya 23 8B base and widely used models including Gemma, Mistral, and Llama 3."}, {"title": "Limitations", "content": "A potential risk of relying on synthetic and translated datasets is the presence of particular cultural biases in model behavior. The prompts used in ShareGPT to seed the creation of the synthetic data over-index on contributions from the Global North or Western regions (Longpre et al., 2023). This could introduce a skew towards a narrow selection of cultural viewpoints. Our preference-trained model covers 23 languages and improves performance relative to the closest open-source model. However, this is still only a tiny fraction of the world's linguistic diversity which encompasses 7000 languages. Furthermore, in this research we don't distinguish between dialects within the languages we cover, which are an important part of how language is used in practice (Zampieri et al., 2020; Wolfram, 1997; Brown et al., 2020; Lent et al., 2022; Blaschke et al., 2023; Falck et al., 2012). Future work, should aim to include more of the world's population and therefore languages. Due to compute constraints, we are limited in our ability to run preference optimization experiments for larger models. Many of the runs we describe in this work for a single run can take 5 days to complete on a single 8 x H100 80GB GPU instance. Future work should explore the impact of scaling model size and further tune other hyperparameters for multilingunal preference optimization."}, {"title": "Background on DPO and RLOO", "content": "(1) Instruction fine-tuning (SFT) stage: A pre-trained LM is instruction-tuned using a dataset consisting of a given instruction prompt, and (typically) a human-written completion. The LM/policy is trained with a cross-entropy loss over the completion only. Often, the SFT model, denoted as sft is used to initialize both the reward model (for online RL optimization) and the RLHF policy model. (2) Preference optimization stage: In this stage, the preference data such as rankings of model responses, are collected through humans or AI feedback. This data is then used to further fine-tune the SFT model (policy) to align the model with human feedback via the collected preferences data. Since collecting human feedback is often very expensive, many preference optimization methods train a separate reward model, on collected preference data to act as a proxy for human preferences, enabling for online preference feedback on LLM responses without requiring human intervention. Preference optimization can be performed in a number of ways: Online Preference Optimization includes training a reward model, typically through binary classification, which is then used to provide online feedback in the optimization of the policy with the following objective:\nmax Ex~D,y~\u03c0\u03bf(.\\x) [r\u00a2(x, y) \u2013 \u03b2PKL],\n\u03c0\u03c1\nwith PKL = DKL\u3160\u04e9(.|x)||Tref(.|x)\nwhere \u1e9e is meant to control the distance from the initial policy ref during the optimization of reward re(x, y) as proposed in (Stiennon et al., 2022). The KL-penalty PKL is crucial as penalty- free optimization of the reward model leads to degradation in the coherence of the model. Direct Preference Optimization (DPO; Rafailov et al., 2023) collects pairwise preferences (often over LLM responses) and fine-tunes the policy to beimplicitly consistent with the collected preference pairs by using the following loss:\n\u2013 log \u03c3(\u03b2log\n\u03c0\u03bf(y+x)\nTref(y+x)\nBlog\n\u03c0\u03bf(y\u2212x)\nTref(y-x)\nDifferent from the online RL methods, DPO skips the reward modeling and enables preference opti- mization in an offline manner without requiring online samples from the policy during the training. At its core, DPO uses the analytical formulation of the canonical KL-controlled RLHF objective detailed in equation A and the assumption that preferences can be modeled by the Bradelly-Terry model (Bradley & Terry, 1952), to for-go reward modeling, simplifying the problem into a supervised style classification task. While reinforcement learning approaches share the components above, techniques differ in formu- lating the reward and the sample-based loss. In this work, we use REINFORCE-Leave-one-out (RLOO; Kool et al., 2019; Ahmadian et al., 2024) estimator as the online preference op- timization method as it is effective and more efficient than Proximal Policy Optimization (PPO) (Schulman et al., 2017b). RLOO is a multi-sample extension of REINFORCE (Williams, 1992),"}, {"title": "Additional Win-Rate Results", "content": "To provide a head-to-head comparison between DPO and RLOO, Table 5 shows the win-rate eval- uation in open-ended generations between models trained with RLOO method with the models trained with DPO."}, {"title": "XLSum Summarization", "content": "Table 6 shows the win-rate scores of preference-trained models on 15 languages that are covered by our 23 language list. Win-rates are measured against the original Aya-23-8B model (Aryabumi et al., 2024). The average generation length for the base model, the best DPO, and the best RLOO models are 138, 234, and 171 tokens respectively. Length bias is a known property of DPO (Park et al., 2024) and can bias GPT-4 as an evaluator (Saito et al., 2023) accordingly. Because the base model and the RLOO model generation are similar in length, it is unlikely that the large gains in win-rate for the RLOO model against the base model are caused by GPT-4 as a judge preferring longer responses."}, {"title": "NLP Benchmark Results", "content": "We report benchmark results for the Base Aya 23 8B Model and preference-trained Aya 23 8B Models (DPO and RLOO) in Tables 8, 9, 7 for mMMLU, MGSM, and discriminative tasks respectively."}, {"title": "Judge Prompt", "content": "System preamble:\nYou are a helpful following assistant whose goal is to select the preferred (least wrong) output for a given instruction in [LANGUAGE_NAME].\nPrompt Template:\nWhich of the following answers is the best one for given instruction in [LANGUAGE_NAME]. A good answer should follow these rules:\n1) It should be in [LANGUAGE_NAME]\n2) It should answer the request in the instruction\n3) It should be factually and semantically comprehensible\n4) It should be grammatically correct and fluent."}, {"title": "Full Language Set Win-Rates", "content": "We provide full win-rate results broken down for all 23 languages against the Base Aya 23 8B Model for the ML-23-230K DPO run in Table 13 and the the ML-23-230K RLOO run in Table 14. Additionally we provide win-rate results for the ML-23-230K RLOO run against Gemma-1.1-7B-it, Llama-3-8B-Instruct, and Mistral-7B-Instruct-v0.3 in Tables 10, 11, and 12 respectively."}, {"title": "Language List", "content": "We provide a list and description of all languages supported by Aya 23 8B which we use to perform multilingual evaluations in Table 15."}, {"title": "Additional Win-Rate Results", "content": "where multiple online generations are sampled from the policy per prompt which enables to reduce variance without requiring an additional network as opposed to PPO:\n \\frac{1}{k}\\sum_{i=1}^{k}\\left[R(y_{(i)}, x)-\\frac{1}{k-1} \\sum_{j \\neq i} R(y_{(j)}, x)\\right] \\\n\\nabla log \\pi(y_{(i)}|x) for y_{(1)}, \\dots, y_{(k)} \\sim  \\pi_{\\theta}(.|x)\nRLOOk considers each y(i) individually and uses the remaining k \u2212 1 samples to create an unbi- ased estimate of the expected return for the prompt, akin to a parameter-free value-function, but estimated at each training step."}]}