{"title": "RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs", "authors": ["John Dang", "Arash Ahmadian", "Kelly Marchisio", "Julia Kreutzer", "Ahmet \u00dcst\u00fcn", "Sara Hooker"], "abstract": "Preference optimization techniques have become a standard final stage for training state-of-art large language models (LLMs). However, despite widespread adoption, the vast majority of work to-date has focused on first-class citizen languages like English and Chinese. This captures a small fraction of the languages in the world, but also makes it unclear which aspects of current state-of-the-art research transfer to a multilingual setting. In this work, we perform an exhaustive study to achieve a new state-of-the-art in aligning multilingual LLMs. We introduce a novel, scalable method for generating high-quality multilingual feedback data to balance data coverage. We establish the benefits of cross-lingual transfer and increased dataset size in preference training. Our preference-trained model achieves a 54.4% win-rate against Aya 23 8B, the current state-of-the-art multilingual LLM in its parameter class, and a 69.5% win-rate or higher against widely used models like Gemma-1.1-7B-it, Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.3. As a result of our study, we expand the frontier of alignment techniques to 23 languages covering half of the world's population.", "sections": [{"title": "Introduction", "content": "What languages are favored in technological progress is often deeply intertwined with historical patterns of technology access and resources (\u2200 et al., 2020; Bird, 2022; Singh et al., 2024). Preference optimization is a valuable and widely adopted post-training technique to align large language models (LLMs) with human preferences (Christiano et al., 2017b; Stiennon et al., 2022; Ouyang et al., 2022a; Bai et al., 2022) which has been shown to lead to large gains in performance across a wide variety of NLP tasks (Wang et al., 2024; Ivison et al., 2023; Xu et al., 2024; Lightman et al., 2024). To-date, the majority of progress in preference optimization has over-fit to a small handful of languages, resulting in large gaps in performance outside of English (Schwartz et al., 2022; Kotek et al., 2023; Khandelwal et al., 2023; Vashishtha et al., 2023; Khondaker et al., 2023), and also risks introducing"}, {"title": "Methodology", "content": ""}, {"title": "Addressing Data Scarcity", "content": "Prior works on multilingual preference training such as Okapi (Lai et al., 2023), typically involved using preference data translated from English, however, the Okapi models has since been outper- formed by non-preference aligned models including the base Aya 23 8B model we experiment with here (Aryabumi et al., 2024). While language coverage may be improved by translation (Ranaldi & Pucci, 2023; \u00dcst\u00fcn et al., 2024), the introduction of translation artifacts known as translationese (Bizzoni et al., 2020; Vanmassenhove et al., 2021) can hurt performance. Furthermore,"}, {"title": "Offline vs Online Preference Training", "content": "Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017b; Stiennon et al., 2022; Ouyang et al., 2022a; Bai et al., 2022), which was proposed as the first framework for aligning language models to human preferences, has become a key for training state-of-the-art LLMs (OpenAI et al., 2023; Touvron et al., 2023; Anthropic, 2024; Reid et al., 2024). Canonically, PPO (Schulman et al., 2017b) has been used in RLHF as the online RL algorithm (Stiennon et al., 2022; Ouyang et al., 2022b; Nakano et al., 2021). However, recent offline methods such as Direct Preference Optimization (DPO) (Rafailov et al., 2023) and subsequent works in the same direction (Azar et al., 2024; Ethayarajh et al., 2024; Choi et al., 2024), have proven increasingly popular due to reduced computational complexity. Traditional online methods such as PPO and REINFORCE (Williams, 1992) require an additional network in addition to the policy, maintaining a reward model in memory, and also using the policy to generate doing training, all of which DPO does not require as it is fully offline."}, {"title": "Experimental Set-up", "content": ""}, {"title": "Multilingual Base Model", "content": "We perform all experiments with Aya 23 8B (Aryabumi et al., 2024) which is chosen because it (1) is massively multilingual, pre-trained and supervised fine-tuned for 23 languages, and (2) it achieves state-of-the-art multilingual performance in 23 languages compared to other commonly used LLMs in its class, outperforming Mistral-7B-Instruct-v0.3 (Jiang et al., 2023a), Gemma-1.1-7B-it (Gemma-Team, 2024), and Aya-101-13B (\u00dcst\u00fcn et al., 2024). On multilingual benchmarks and open-ended generations, Aya 23 achieves a 65% win-rate or higher in head-to-head comparisons with popular open sourcee models(Aryabumi et al., 2024). Furthermore, Aya 23 is an open weights model that is not preference-trained, allowing us to isolate the impact of multilingual preference optimization in our experiments."}, {"title": "Reward Model", "content": "We use the Cohere reward model (Cohere May 2024) which is competitive with top-scoring state-of-the-art RMS on the RewardBench Leaderboard (Lambert et al., 2024), scoring 88.2, which currently is ranked 4th.6 This reward model achieves high LLM response ranking agreement with GPT- 4-Turbo on English and non-English languages as shown in Table 1. We intentionally use a separate reward model from the model we use for llm-as-a-judge evaluation (GPT-4-Turbo7) given the known biases incurred by using the same model for both (Verga et al., 2024; Bansal et al., 2024)."}, {"title": "Preference Optimization Training", "content": "We train Aya 23 8B model for 2 epochs in both DPO and RLOO experiments. DPO runs are trained with KL-penalty \u03b2 = 0.5, learning rate 5e-7, and AdamW optimizer (Kingma & Ba, 2014). RLOO runs are trained with RLOO k = 2, KL-penalty B = 0.01, learning rate 5e-6, AdamW optimizer, and online generation sampling tem- perature of 0.75. All runs are performed on a single node with either 8 x Nvidia A100 80GB or 8 x Nvidia H100 80GB GPUs with DeepSpeed ZeRO Stage 3 (Rajbhandari et al., 2020) and full fine-tuning of all 8 billion model parameters. We performed hyperparameter sweeps for learning rate Ir \u2208 {5e-8,5e-7,5e-6,5e-5} and for KL-penalty \u03b2\u2208 {0.05,0.1,0.5} for both DPO and RLOO to the best of our ability. For a fair comparison with DPO, we utilize the same reward model for RLOO which is used to generate our synthetic multilingual preference dataset (for ranking the generations)."}, {"title": "Model Comparisons", "content": "We evaluate against multiple state-of-the-art open-source models to ensure a comprehensive evalu- ation. Details of each model are below:"}, {"title": "Evaluation", "content": ""}, {"title": "Open-ended generations", "content": "For open-ended generations, we use dolly-machine-translated test set from the Aya evaluation suite (Singh et al., 2024) which is a held-out test set of 200 instances from the Dolly-15k dataset (Conover et al., 2023) translated into 101 languages. This test set was curated by avoiding instructions that include culturally-specific or geographic references. For languages that are available (Arabic, French, Hindi, Russian, and Spanish),"}, {"title": "Summarization Task", "content": "For summarization, we use XLSum (Hasan et al., 2021), for the subset of 15 languages covered by the benchmark within the Aya 23 language coverage."}, {"title": "Results and Discussion", "content": "Win-rates Against Open-Weights Models Figure 1 and Table 2 show the win-rates of our preference-trained models against state-of-the-art open-source models. Importantly, the base Aya 23 8B already achieves high win-rates against Gemma-1.1 (62.1%), Llama-3 (66.6%), and Mistral- v0.3 (69%) averaged across all 23 languages on Dolly open-ended generations. Preference-optimized Aya models extend this lead further. Concretely, our best variant of RLOO leads to 69.5% (+7.4), 72.4% (+5.8), and 77.5 (+8.5) win-rates against Gemma, Llama-3, and Mistral respectively."}, {"title": "Win Rates Against Aya-23-8B", "content": "Table 3 shows win-rate results for open-ended gener- ations. Win-rates are measured against Aya 23 8B. We report win-rates for both DPO and RLOO trained with different preference data mixtures. XLSum summarization results are shown in Appendix (Table 6). Our best vari- ant across experiments achieves a win-rate of 70.7% over the base Aya model."}, {"title": "Increasing multilinguality in preference data improves winrates", "content": "For a fixed dataset size of 50K pairwise preferences, we find that increasing the number of languages in training data improves overall performance as shown in Figure 2a and Table 3 (right). For DPO, win- rates against the base model on 23 languages increases from 43.3% to 47.0%.8 For RLOO, this gain is most visible when the number of"}, {"title": "Cross-lingual transfer to unseen languages", "content": "Preference training only with English, achieves performance gains for languages not seen in the training data as shown in Figure 2c (and Table 4). This gain (AW-L) is 2.0% for DPO and 7.3% for RLOO. Furthermore, using a 5-language subset (ML-5) significantly increases these gains to 3.8% and 19.4% for DPO and RLOO respectively. These results provide strong evidence of cross-lingual trans- fer in preference optimization, which is signifi- cantly more present after online training, with an increased degree of transfer facilitated by multilingual training data compared with En- glish only."}, {"title": "Online optimization vs offline", "content": "We find that RLOO (online) outperforms DPO (offline) across the board in multilingual performance. As shown in in Table 3, RLOO achieves higher win-rates with all the preference data mixtures compared to DPO. For the 23-language mixture, RLOO achieves 54.0% win-rates whereas DPO reaches 47.0%."}, {"title": "Is there a multilingual alignment tax?", "content": "Post-training stages of LLMs including supervised fine-tuning and preference optimization have increasingly been torn between objectives: improv- ing traditional discriminative benchmarks like HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2021) and training LLMs to follow instructions, acquire conversational abilities, and be helpful and harmless (Askell et al., 2021). Recent work on multilingual instruction finetuning (\u00dcst\u00fcn et al., 2024) has found that improvements in open-ended generative tasks introduce trade-offs with dis- criminative tasks. However, this work only studies the tensions introduced by supervised instruction finetuning. More recent work (Tang et al., 2024) on preference training suggests that offline methods impart improved ability for discriminative tasks, whereas on-policy sampling improves generative quality. Here, we explore whether this holds for multilingual and characterize the trade-off between discriminate and generative performance."}, {"title": "Related Work", "content": ""}, {"title": "Reinforcement Learning from Human Feedback (RLHF)", "content": "RLHF has become the dominant paradigm for aligning LLMs to human preferences. Canonically, this involves training a reward model and using a reinforcement learning algorithm like PPO (Schulman et al., 2017a) or RLOO (Ahmadian et al., 2024) to optimize the LLM policy to maximize reward of online samples generated throughout training. (Ouyang et al., 2022b; Stiennon et al., 2020; Christiano et al., 2017a). There has been a plethora of work attempting to take the online inference aspect, and the optimization difficulties and complexities of RL that come with it, out of RLHF. The most prominent of these, is the family of methods base upon Direct Preference Optimization (DPO) (Rafailov et al., 2023), such as IPO (Azar et al., 2024), \u039a\u03a4\u039f (Ethayarajh et al., 2024), and SRPO (Choi et al., 2024). This family of methods directly fine-tunes an LLM to be impicitly consistent with collected preference data, forgoing the need for training a separate reward model."}, {"title": "Synthetic Data", "content": "Collecting ground truth completions or feedback data from humans is often very expensive. Thus, many recent works have explored the use of training LLMs using data generated by LLMs. Distallation is a common technique which uses a stronger (typically larger model) to generate ground truth completions to SFT a weaker (typically smaller model) (Taori et al., 2023). As many recent LLMs (and Reward Models intialized from LLMs) have been shown to be strong evaluators of LLM completions (Zheng et al., 2023; Dubois et al., 2024), many works use LLMs or Reward Models to generate synthetic preference data rather than collecting preferences from humans, which is later used in RLHF training(Bai et al., 2022; Pace et al., 2024).LLMs can also be used to provide additional rankings, ratings, or natural language feedback, which can be used in subsequent RLHF training. Methods which use AI generated feedback are part of a family of Reinforcement Learning from AI Feedback (RLAIF) methods."}, {"title": "Preference Optimization for Multilingual LLMs", "content": "There have been limited efforts on multi- lingual preference optimization to-date. (Lai et al., 2023) present a multilingual instruction tuning framework, where they preference train multilingual LLMs such as BLOOMZ (Muennighoff et al., 2023b) for 26 non-English languages with RLHF. They synthetically generated a preference dataset by translating an extended version of the Alpaca dataset (Taori et al., 2023), generating responses from their target LLM and ranking back-translated (into English) responses with ChatGPT.9 In contrast to our work, they perform preference optimization for each language separately. How- ever, due to their potentially low-quality dataset which heavily relies on translations, their resulting language-specific models are outperformed by other massively multilingual LLMs without prefer- ence optimization (\u00dcst\u00fcn et al., 2024; Aryabumi et al., 2024). Wu et al. (2024) study cross-lingual transfer in reward model (RM) training where they propose using preference data in one source lan- guage to train an RM for target language alignment. They show that RMs based on amultilingual base model exhibit zero-shot cross-lingual transfer consistently across different languages. However, they do not experiment with using multiple source languages in training, which we show is crucial in the preference optimization both for offline optimization such as DPO and online RL methods such as RLOO. A number of existing works also explore methods for preference optimization in a highly related setting where LLMs are aligned to preferences of diverse groups of people around the world (Zhao et al., 2024; Jiang et al., 2023b; Hwang et al., 2023; Deshpande et al., 2023)."}, {"title": "Conclusion", "content": "Our work presents a comprehensive study on multilingual preference optimization. We show that the inclusion of multilingual data in preference optimization leads to significant improvements in multilingual performance over English-only preference optimization. This improvement scales both with the number of languages and the total number of examples included in the training data. Additionally, we show that preference training exhibits cross-lingual transfer, leading to significant gains in languages not present in the training data. We also find that using online preference opti- mization outperforms offline preference optimization, highlighting the importance of online samples during training.\nAs a result of our study, we expand the frontier of alignment techniques to 23 languages which cover half the world's population, by successfully preference-training an 8-billion parameter Aya 23 model that outperforms both the original Aya 23 8B base and widely used models including Gemma, Mistral, and Llama 3."}, {"title": "Limitations", "content": "A potential risk of relying on synthetic and translated datasets is the presence of particular cultural biases in model behavior. The prompts used in ShareGPT to seed the creation of the synthetic data over-index on contributions from the Global North or Western regions (Longpre et al., 2023). This could introduce a skew towards a narrow selection of cultural viewpoints.\nOur preference-trained model covers 23 languages and improves performance relative to the closest open-source model. However, this is still only a tiny fraction of the world's linguistic diversity which encompasses 7000 languages. Furthermore, in this research we don't distinguish between dialects within the languages we cover, which are an important part of how language is used in practice (Zampieri et al., 2020; Wolfram, 1997; Brown et al., 2020; Lent et al., 2022; Blaschke et al., 2023; Falck et al., 2012). Future work, should aim to include more of the world's population and therefore languages.\nDue to compute constraints, we are limited in our ability to run preference optimization experiments for larger models. Many of the runs we describe in this work for a single run can take 5 days to complete on a single 8 x H100 80GB GPU instance. Future work should explore the impact of scaling model size and further tune other hyperparameters for multilingunal preference optimization."}, {"title": "Background on DPO and RLOO", "content": " \n(1) Instruction fine-tuning (SFT) stage: A pre-trained LM is instruction-tuned using a dataset consisting of a given instruction prompt, and (typically) a human-written completion. The LM/policy is trained with a cross-entropy loss over the completion only. Often, the SFT model, denoted as sft is used to initialize both the reward model (for online RL optimization) and the RLHF policy model.\n(2) Preference optimization stage: In this stage, the preference data such as rankings of model responses, are collected through humans or AI feedback. This data is then used to further fine-tune the SFT model (policy) to align the model with human feedback via the collected preferences data. Since collecting human feedback is often very expensive, many preference optimization methods train a separate reward model, on collected preference data to act as a proxy for human preferences, enabling for online preference feedback on LLM responses without requiring human intervention. Preference optimization can be performed in a number of ways:\nOnline Preference Optimization includes training a reward model, typically through binary classification, which is then used to provide online feedback in the optimization of the policy with the following objective:\nmax Ex~D,y~\u03c0\u03b8(.\\x) [r\u03b8(x, y) \u2013 \u03b2PKL],\n\u03c0\u03c1\nwith PKL = DKL[\u03c0\u03b8(.|x)||\u03c0ref(.|x)]\nwhere \u03b2 is meant to control the distance from the initial policy \u03c0ref during the optimization of reward r\u03b8(x, y) as proposed in (Stiennon et al., 2022). The KL-penalty PKL is crucial as penalty-free optimization of the reward model leads to degradation in the coherence of the model.\nDirect Preference Optimization (DPO; Rafailov et al., 2023) collects pairwise preferences (often over LLM responses) and fine-tunes the policy to beimplicitly consistent with the collected preference pairs by using the following loss:\n\u2013 log \u03c3(\u03b2log \u03c0\u03bf(y+x) \u2013 \u03b2log \u03c0\u03bf(y\u2212x) )\nTref(y+x) Tref(y-x)\nDifferent from the online RL methods, DPO skips the reward modeling and enables preference opti- mization in an offline manner without requiring online samples from the policy during the training. At its core, DPO uses the analytical formulation of the canonical KL-controlled RLHF objective detailed in equation A and the assumption that preferences can be modeled by the Bradelly-Terry model (Bradley & Terry, 1952), to for-go reward modeling, simplifying the problem into a supervised style classification task.\nWhile reinforcement learning approaches share the components above, techniques differ in formu- lating the reward and the sample-based loss. In this work, we use REINFORCE-Leave-one-out (RLOO; Kool et al., 2019; Ahmadian et al., 2024) estimator as the online preference op- timization method as it is effective and more efficient than Proximal Policy Optimization (PPO) (Schulman et al., 2017b). RLOO is a multi-sample extension of REINFORCE (Williams, 1992),"}, {"title": "Additional Win-Rate Results", "content": "where multiple online generations are sampled from the policy per prompt which enables to reduce variance without requiring an additional network as opposed to PPO:\n1 1 [R(y(i), x) \u2212 R(y(j), x)]\nk i=1 k \u2212 1 j\u2260i\nVlog \u03c0(y(i)|x) for y(1), . . . , y(k) ~ \u03c0\u03b8(.|x)\nRLOOk considers each y(i) individually and uses the remaining k \u2212 1 samples to create an unbi- ased estimate of the expected return for the prompt, akin to a parameter-free value-function, but estimated at each training step."}, {"title": "RLOO vs DPO", "content": "To provide a head-to-head comparison between DPO and RLOO, Table 5 shows the win-rate eval- uation in open-ended generations between models trained with RLOO method with the models trained with DPO."}, {"title": "XLSum Summarization", "content": "Table 6 shows the win-rate scores of preference-trained models on 15 languages that are covered by our 23 language list. Win-rates are measured against the original Aya-23-8B model (Aryabumi et al., 2024). The average generation length for the base model, the best DPO, and the best RLOO models are 138, 234, and 171 tokens respectively. Length bias is a known property of DPO (Park et al., 2024) and can bias GPT-4 as an evaluator (Saito et al., 2023) accordingly. Because the base model and the RLOO model generation are similar in length, it is unlikely that the large gains in win-rate for the RLOO model against the base model are caused by GPT-4 as a judge preferring longer responses."}, {"title": "NLP Benchmark Results", "content": "We report benchmark results for the Base Aya 23 8B Model and preference-trained Aya 23 8B Models (DPO and RLOO) in Tables 8, 9, 7 for mMMLU, MGSM, and discriminative tasks respectively."}, {"title": "Judge Prompt", "content": "System preamble:\nYou are a helpful following assistant whose goal is to select the preferred (least wrong) output for a given instruction in [LANGUAGE_NAME].\nPrompt Template:\nWhich of the following answers is the best one for given instruction in [LANGUAGE_NAME]. A good answer should follow these rules:\n1) It should be in [LANGUAGE_NAME]\n2) It should answer the request in the instruction\n3) It should be factually and semantically comprehensible\n4) It should be grammatically correct and fluent."}, {"title": "Full Language Set Win-Rates", "content": "We provide full win-rate results broken down for all 23 languages against the Base Aya 23 8B Model for the ML-23-230K DPO run in Table 13 and the the ML-23-230K RLOO run in Table 14. Additionally we provide win-rate results for the ML-23-230K RLOO run against Gemma-1.1-7B-it, Llama-3-8B-Instruct, and Mistral-7B-Instruct-v0.3 in Tables 10, 11, and 12 respectively."}, {"title": "Language List", "content": "We provide a list and description of all languages supported by Aya 23 8B which we use to perform multilingual evaluations in Table 15."}]}