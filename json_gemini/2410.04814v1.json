{"title": "LEARNING INTERPRETABLE HIERARCHICAL DYNAMICAL SYSTEMS MODELS FROM TIME SERIES DATA", "authors": ["Manuel Brenner", "Elias Weber", "Georgia Koppe", "Daniel Durstewitz"], "abstract": "In science, we are often interested in obtaining a generative model of the underlying system dynamics from observed time series. While powerful methods for dynamical systems reconstruction (DSR) exist when data come from a single domain, how to best integrate data from multiple dynamical regimes and leverage it for generalization is still an open question. This becomes particularly important when individual time series are short, and group-level information may help to fill in for gaps in single-domain data. At the same time, averaging is not an option in DSR, as it will wipe out crucial dynamical properties (e.g., limit cycles in one domain vs. chaos in another). Hence, a framework is needed that enables to efficiently harvest group-level (multi-domain) information while retaining all single-domain dynamical characteristics. Here we provide such a hierarchical approach and showcase it on popular DSR benchmarks, as well as on neuroscientific and medical time series. In addition to faithful reconstruction of all individual dynamical regimes, our unsupervised methodology discovers common low-dimensional feature spaces in which datasets with similar dynamics cluster. The features spanning these spaces were further dynamically highly interpretable, surprisingly in often linear relation to control parameters that govern the dynamics of the underlying system. Finally, we illustrate transfer learning and generalization to new parameter regimes.", "sections": [{"title": "INTRODUCTION", "content": "Foundation models for time series recently blossomed, based on the hope that generalizable temporal features could be extracted from multiple, diverse datasets (Yeh et al., 2023; Miller et al., 2024; Das et al., 2024; Ansari et al., 2024; Rasul et al., 2024). In scientific and medical applications, however, beyond mere time series forecasting, we are often interested in interpretable and mathematically tractable models that can be used to obtain mechanistic insights into the underlying system dynamics. Such models need to be generative in a dynamical systems sense (Durstewitz et al., 2023), i.e., when simulated, should produce dynamical behavior with the same long-term statistics as the observed system, a property that conventional time series models often lack (Gilpin, 2024). Moreover, accounting for the fact that different time series may be sampled from fundamentally different dynamical regimes, i.e. with different state space topology and attractor structure, may also help to improve generalization and transfer learning (G\u00f6ring et al., 2024).\nThis is particularly important in areas like neuroscience (Zeidman et al., 2019) or mental health (Huys et al., 2016; Valton et al., 2020), where the data is not only expensive and difficult to collect (Koppe et al., 2021; Wen et al., 2017), hence comparatively sparse, but also subject to significant variability between individuals or systems (Meyer-Lindenberg, 2023; Fechtelpeter et al., 2024). The challenge in these settings is to develop models that can still identify and exploit commonalities across different datasets while being flexible enough to account for individual differences in dynamical regimes."}, {"title": "RELATED WORK", "content": "Dynamical Systems Reconstruction (DSR) DSR is a rapidly growing field in scientific machine learning. DSR models may be considered a special class of time series (TS) models that, beyond mere TS prediction, aim to learn surrogate models of the data-generating process from TS observations. A proper DSR model needs to preserve the long-term temporal and geometrical properties of the original dynamical system (DS), i.e. its vector field topology and attractor structure, which then enables further scientific analysis (Brunton and Kutz, 2019; Durstewitz et al., 2023; Platt et al., 2023; Gilpin, 2024). A variety of DSR methods have been advanced in recent years, either based on predefined function libraries and symbolic regression, such as Sparse Identification of Nonlinear Dynamics (SINDy; Brunton et al. (2016); Kaiser et al. (2018)), based on Koopman operator theory (Azencot et al., 2020; Brunton et al., 2021; Naiman and Azencot, 2021), on universal approximators such as recurrent neural networks (RNNs; Gajamannage et al. (2023); Brenner et al. (2022); Hess et al. (2023)), reservoir computing (RC; Pathak et al. (2017); Platt et al. (2022; 2023)), or neural ordinary differential equations (Neural ODEs; Chen et al. (2018); Ko et al. (2023)), or on methods that sit somewhere in between universal and domain-specific, like physics-informed neural networks (PINNs; Raissi et al. (2019)). PINNs, like library-based methods, require sufficient domain knowledge to work well in practice (Fotiadis et al., 2023; Subramanian and Mahadevan, 2023; Mouli et al., 2023). To achieve proper reconstruction of a system\u2019s long-term statistics and attractor geometry, often special, control-theoretic training techniques such as sparse or generalized teacher forcing (Brenner et al., 2022; Mikhaeil et al., 2022; Hess et al., 2023) or particular regularization terms that enforce invariant measures (Platt et al., 2023; Jiang et al., 2023; Schiff et al., 2024) are used. Models using these training techniques currently represent the state of the art in this field.\nHierarchical Time Series and DSR Modeling Hierarchical models for representing multi-level dependencies or nested groups have a long history in statistics and machine learning (Laird and Ware, 1982; Goldstein, 1987; Gelman and Hill, 2006; McCulloch et al., 2011). The general idea of these approaches is to leverage shared group-level information while accounting for individual"}, {"title": "METHOD", "content": "3.1 HIERARCHIZATION\nAssume we have observed multiple, multivariate time series $x^{(j)}_{1...T_{max}}$ of lengths $T_{max}, j = 1... S$, such as measurements from related physical systems, from multiple subjects in medical studies (Huys et al., 2016; Valton et al., 2020), or repeated measurements from the same subject across trials (Zeidman et al., 2019). While generally the individual multivariate time series may come from any type of system, in the following we will denote these as \u2018subjects' as our main application examples will be human data. Our main goal is to infer subject-specific DSR models, parameterized by $\\theta_{DSR}^{(j)}$, i.e. generative models of the latent dynamics underlying subject-specific observations, approximated by a discrete-time recursive (flow) map of the form\n$z_t = F_{\\theta_{DSR}}(z_{t-1}, s_t)$,\n(1)\nwhere $s_t$ are possible external inputs (like task stimuli). Observations are related to the dynamical process {$z_t$} via some parameterized observation function $x_t = h_{\\theta_{obs}}(z_t)$. The differences between subjects are captured within an ideally low-dimensional parameter space, represented here by learnable subject-specific features $l^{(j)} \\in \\mathbb{R}^{N_{feat}}$ which are mapped onto the parameters $\\theta_{DSR}$ of subject-specific DS models through a function parameterized by group-level parameters $\\Theta_{group}$:\n$\\theta_{DSR}^{(j)} = G_{\\Theta_{group}} (l^{(j)})$.\n(2)\nThe model Eqs. 1-2 is then trained simultaneously on data from all subjects, where features $l^{(j)}$ depend only on time series from subject j. The overall approach is illustrated in Fig. 7. In ablation studies, we explored various ways to parameterize the map $G_{\\Theta_{group}}$, including flexible and expressive functions like multilayer perceptrons (MLPs) and reparameterizations (see Appx. A.1.3, Table 2), but found that simple linear mappings often gave the best results."}, {"title": "TRAINING METHOD", "content": "Generalized Teacher Forcing (GTF) For training, we used generalized teacher forcing (GTF) (Hess et al., 2023), a method specifically designed to address exploding gradients, which are known to occur when training DSR models on chaotic time series (Mikhaeil et al., 2022). GTF stabilizes training by linearly interpolating between RNN-generated and data-inferred control states in an optimal way (see Appx. A.1). This method was selected for its state-of-the-art performance across a range of DSR tasks (Hess et al., 2023). As in Hess et al. (2023), we trained directly on the observations, assuming $x_t = h_{\\theta_{obs}}(z_t) = z_t$ to be the identity mapping. We emphasize that GTF is only used to train the model, while at test time the models always run autonomously (i.e., without any feedback from actual observations).\nBatching During training, we used a batching strategy that ensures each gradient update incorporates data from all subjects. For datasets with a manageable number of subjects, we sampled training sequences $x^{(j)}_{1...T_{seq}}$ from each subject, ensuring that updates reflect the collective dynamics across the entire population. For larger datasets, we sampled a random subset of subjects per batch. We further provided multiple sequences per subject within each minibatch to gather more robust statistics for the subject-specific feature vectors. Losses were then added for each subject-specific minibatch and combined to update both group-level parameters, $\\Theta_{group}$, and subject-specific feature vectors $l^{(j)}$. We found that a batch size between 4 - 16\u00d7 the number of subjects led to the most robust training and performance (see Sect. A.1.2 and Figs. 8 & 9).\nDifferential learning rates For training we used the RAdam (Liu et al., 2020) optimizer. To ensure successful training, we found it crucial to assign a significantly higher learning rate to the subject-specific feature vectors (10-3) than to the group-level matrices (10\u22124). This helped prevent numerical instabilities in the group-level matrices, which occurred with higher learning rates. It also prioritizes the incorporation of subject-specific information through the feature vectors, see Fig. 13.\nInitialization We used Xavier uniform initialization (Glorot and Bengio, 2010) for the group-level matrices, which is designed to keep the variance of the outputs $n_{out}$ of a layer roughly equal to the variance of its inputs $n_{in}$ by drawing weights from a uniform distribution in the interval [-a, a], where $a = \\sqrt{\\frac{6}{N_{in}+N_{out}}}$. We reduced the weights further by a factor of 0.1 to stabilize training. Additionally, we applied L2 regularization to the group-level matrices to further enhance stability."}, {"title": "DSR Model", "content": "For our experiments we chose a mathematically tractable (cf. Eisenmann et al. (2024)) RNN model introduced for DSR in Hess et al. (2023), the shallow PLRNN (shPLRNN):\n$z_t = Az_{t-1} + W_1\\phi(W_2z_{t-1}+h_2) + h_1$,\n(3)\nwith latent states $z_t \\in \\mathbb{R}^M$, diagonal matrix $A \\in \\mathbb{R}^{M\\times M}$, connectivity matrices $W_1 \\in \\mathbb{R}^{M\\times L}$ and $W_2 \\in \\mathbb{R}^{L\\times M}$, and thresholds $h_2 \\in \\mathbb{R}^L$ and $h_1 \\in \\mathbb{R}^M$, where L is the dimension of the hidden layer, and rectified-linear-unit nonlinearity $\\phi(\u00b7) = ReLU(\u00b7) = max(0,\u00b7)$.\nWhile our approach is general and can be run with any DSR or TS model, here we spell it out specifically for a hierarchically trained shPLRNN, Eq. 3 (hier-shPLRNN). For this model, the subject-specific DSR model parameters are given by $\\theta_{DSR} = {W_1^{(j)}, W_2^{(j)}, h_1^{(j)}, h_2^{(j)}, A^{(j)}}$. The subject-level parameters $\\theta_{DSR}^{(j)}$ are obtained from the feature vectors $l^{(j)} \\in \\mathbb{R}^{1\\times N_{feat}}$ by linearly projecting group-level parameters into the subject space according to:\n$A^{(j)} := diag(l^{(j)} \\cdot P_A)$, $W_1^{(j)} := mat(l^{(j)} \\cdot P_{W_1}, M, L)$, $W_2^{(j)} := mat(l^{(j)} \\cdot P_{W_2}, L, M)$,\n$h_1^{(j)} := l^{(j)} \\cdot P_{h_1}$, $h_2^{(j)} := l^{(j)} \\cdot P_{h_2}$,\nwhere mat(v, m, n) denotes the operation of reshaping a vector into an m \u00d7 n matrix, and $P_{W_1} \\in \\mathbb{R}^{N_{feat} \\times (ML)}$, $P_{W_2} \\in \\mathbb{R}^{N_{feat} \\times (LM)}$, $P_A \\in \\mathbb{R}^{N_{feat} \\times M}$, $P_{h_1} \\in \\mathbb{R}^{N_{feat} \\times M}$, and $P_{h_2} \\in \\mathbb{R}^{N_{feat} \\times L}$ are group-level parameter matrices learned from all subjects simultaneously. With this, the dynamics of a subject-level shPLRNN, specified through features $l^{(j)}$, is given by:\n$z_t = A^{(j)} z_{t-1} + W_1^{(j)} \\phi(W_2^{(j)} z_{t-1} + h_2^{(j)}) + h_1^{(j)}$,\n(4)"}, {"title": "Learnable scaling matrices", "content": "Lastly, we introduced a learnable, subject-specific diagonal scaling matrix $\\Sigma$ in order to deal with data that is on different scales across subjects (e.g. for observations of the Lorenz-63 system in the cyclic vs. chaotic regime). This was incorporated by the use of a Gaussian negative log likelihood loss\n$\\mathcal{L}(x_t, \\hat{x}_t) = \\frac{M}{2} log |\\Sigma| - \\frac{1}{2} (x_t - \\hat{x}_t)^T \\Sigma^{-1} (x_t - \\hat{x}_t) + const.$\n(5)\nin place of the MSE loss suggested in Hess et al. (2023) for training with GTF."}, {"title": "RESULTS", "content": "In the following sections, we highlight various use cases of our hierarchical DSR framework."}, {"title": "TRANSFER LEARNING", "content": "To illustrate transfer learning, we used two popular DS benchmarks, where we sampled multivariate time series $X^{(i)}$ with varying ground-truth parameters of the underlying ODE systems.\n1) Lorenz-63 data consisting of 64 'subjects', simulated by 64 different parameter combinations for the Lorenz-63 model (Lorenz, 1963) of atmospheric convection (Eq. 13), with\n$\\rho\\in \\{21, 51, 81, 111\\}, \\sigma\\in \\{8, 9, 10, 11\\}$ and $\\beta \\in \\{1, 2, 3, 4\\}$.\n2) Lorenz-96 data, based on a spatially extended version of the Lorenz model (Lorenz, 1996) with N = 10 dimensions (Eq. 15), consisting of 20 subjects with $F \\in {\\frac{10\\cdot(n-1)+1}{20}}; n\\in 1..20$.\nThe parameter ranges were chosen to cover multiple dynamical regimes, i.e. with fundamentally different attractor topologies (like limit cycles and chaos, Fig. 1). From each parameter setting, we sampled only short time series ($T_{max}$ = 1000), such that training on individual time series often led to suboptimal outcomes. Thus, leveraging group information was crucial for optimal reconstructions.\nAs established in the field of DSR (Wood, 2010; Durstewitz et al., 2023), we evaluated the quality of reconstruction in terms of how well the reconstructed DS captured the invariant long-term temporal and geometric properties of the ground truth DS: We used a state space divergence $D_{stsp}$ to assess geometrical agreement (Koppe et al., 2019; Brenner et al., 2022) and the Hellinger distance $D_H$ on power spectra (Mikhaeil et al., 2022; Hess et al., 2023) to check temporal agreement between generated and ground truth trajectories (see Appx. A.2).\nWe compared the performance of our framework to three other recent methods (see Appx. A.4 for details): First, as a baseline we tested an ensemble of individual shPLRNNs, using an otherwise identical training algorithm (a kind of ablation experiment, removing specifically the 'hierarchical component'). Second, we employed LEarning Across Dynamical Systems (LEADS, Yin et al. (2021)), a framework that trains Neural ODEs for generalizing across DS environments by learning a shared dynamics model jointly with environment-specific models. Third, we trained context-informed dynamics adaptation (CoDA, Kirchmeyer et al. (2022)), an extension of LEADS where parameters of the combined and environment-specific models are drawn from a hypernetwork. As evidenced in Table 1, our hierarchical approach (hier-shPLRNN) considerably outperforms all other setups. In fact, competing methods were often not even able to correctly reproduce the long-term attractor dynamics (Appx. Fig. 12), while our approach successfully recovered different attractor topologies (Figs. 1 & 15)."}, {"title": "INTERPRETABILITY", "content": "Dynamical systems benchmarks We assessed the ability of the hierarchical inference framework to discover interpretable structure from the Lorenz-63 (Eq. 13) and R\u00f6ssler system (Eq. 14). To this end, we again sampled relatively short time series of length T = 1000 for 10 different values $\\rho^{(i)} \\in \\{28...80\\}$ for the Lorenz-63, and $c^{(i)} \\in \\{3.8 ... 4.8\\}$ for the R\u00f6ssler, a range where its dynamics undergoes a bifurcation from limit cycle to chaotic dynamics. To reflect these 1-parameter-variation ground truth settings, we also chose $N_{feat} = 1$ for the length of the subject-specific parameter vectors. After training, we found that the extracted feature values $l^{(i)}$ were highly predictive of the ground truth values for $\\rho^{(i)}$ and $c^{(i)}$, with a clearly linear relation (Fig. 2a, see Fig. 17 for the Lorenz-96). The observation that the model automatically inferred such a linear relationship is particularly noteworthy given that the DSR model's piecewise-linear form (Eq. 4) profoundly differs from the polynomial equations defining the ground truth systems (Eqs. 13 and 14). Surprisingly, even when hier-shPLRNNs were trained with many more features ($N_{feat}$ = 10) than theoretically required, a principal component analysis (PCA) on the feature vectors revealed that a dominant part of the variation was captured by the first PC (> 85% of variance), with negligible contributions beyond the third PC (Fig. 2b, blue curve). In contrast, attempts to extract low-dimensional structure from the parameters of individually trained models were unsuccessful, with variation distributed across many PCs (Fig. 2b, gray curve).\nMultidimensional parameter spaces We next examined a scenario where all three of the ground-truth parameters \u03c3, \u03c1 and \u03b2 of the Lorenz-63 (Eq. 13) were jointly varied across a grid of 4\u00d74\u00d74 = 64 subjects, as in Sect. 4.1, covering both periodic and chaotic regimes. While naturally, in this case, a scalar subject-specific feature did not capture the full variation in the dataset, we found that good reconstructions could already be achieved using Nfeat = 3, while using Nfeat = 6 features led to optimal performance (potentially indicating that for more complex, higher-dimensional scenarios some 'disentangling' may be required in feature space to retain linearity in $G_{\\Theta_{group}}$, Eq. 2, despite the highly nonlinear relation of parameters to dynamics in the ground truth model). The extracted features remained highly interpretable, as illustrated in Fig. 3: \u03c1 strongly aligned with the first and \u03b2 with the second PC of the subject feature space, while variation in \u03c3 appeared nested within steps of \u03b2 across PC2. The eight subjects on the left which do not fall into the 4 \u00d7 4 grid with the others, correspond to parameter combinations that put the system into a non-chaotic, cyclic regime. These subjects do not only form a distinct group in the first two PCs, but also have significant non-zero third and fourth PCs (see Fig. 16). This observation helps explain why the algorithm benefits from additional features (Nfeat > 3), as these allow to represent different dynamical regimes as distinct regions in feature space, thereby further supporting its interpretability.\nSimulated arterial pulse waves We then tested our approach on a much higher-dimensional (N = 52) example system, closer to relevant real-world scenarios, a biophysical model of arterial pulse waves (Charlton et al., 2019). The dataset consists of 4,374 simulated healthy adults aged 25-75 years, with four modalities (pressure, flow velocity, luminal area, and photoplethysmogram) across 13 body sites (see Appx. A.3 for details). Additionally, the dataset includes 32 haemodynamic parameters, such as age, heart rate, arterial size and pulse pressure amplification (Table 4), which specified the biophysical model simulations.\nAfter training, we found that the hier-shPLRNN became an almost perfect emulator of the data, with freely generated trajectories closely matching the ground truth pulse waves (Fig. 4a and Appx. Fig. 18 for further examples). This is particularly impressive given that the number of parameters of the hier-shPLRNN constituted less than 1% of the total amount of training data points, and a comparatively low-dimensional (in relation to the number of biophysical parameters) feature vector (Nfeat = 12) was sufficient to capture individual differences between subjects. The model's ability to achieve these reconstructions with so few parameters indicates that it extracted meaningful structure from the data, rather than merely memorizing it. Accordingly, the extracted feature vectors could predict the haemodynamic parameters via linear regression with high \u2013 and in all cases statistically significant (F-tests, p < 0.05) \u2013 accuracy (mean $R^2$ = 0.83, Fig. 4b). Two parameters with a particularly strong relationship ($R^2$ > 0.97) are in Fig. 4c. Moreover, the explained variances of the"}, {"title": "FEW-SHOT-LEARNING", "content": "To test 'few-shot learning', by which we mean here the ability to infer new models from only small amounts of data, we first trained a hier-shPLRNN on S = 9 subjects simulated using the Lorenz-63 with $\\rho_{train} = \\{28,33.8, 39.6, 45.3, 56.9, 62.7, 68.4, 74.2, 80\\}$. We then generated new, short sequences $x^{(j)}_{test}$ with $T_{max} = 100$, using values of ptest randomly sampled from the same interval [28, 80] that also contained the training data, and fine-tuned only a new feature vector $l^{(j)}$ on this test set. The model closely approximated (interpolated) the true $\\rho_e$ values from these single short sequences (Fig. 5a), and was even able to extrapolate to new $\\rho_{test}$ outside the training range. Fig. 5b exemplifies how estimates become increasingly robust for longer sequences. This result is noteworthy as training an individual shPLRNN for the Lorenz-63 is challenging even with 1000 time steps (see Table 1 & Fig. 10). Even for sequences as short as $T_{max}$ = 100, the loss curves were often uni-modal and smooth around the true parameter values (Fig. 5c), as observed previously for training with sparse or generalized teacher forcing (Hess et al., 2023; Brenner et al., 2024b). This allowed gradient descent to converge rapidly, within 6 seconds on a single 11th Gen 2.30 GHz Intel Core i7-11800H. It also enabled efficient use of 2nd-order, Hessian-based methods (Newton-Raphson), which converged within just 1 second. Hence, even with minimal data fast and reliable estimation of previously unseen dynamics is feasible, extending into regimes beyond the training domain."}, {"title": "EXPERIMENTS WITH HUMAN EMPIRICAL DATA", "content": "Unsupervised extraction of DS feature spaces from clinical electroencephalogram (EEG) data In the previous sections, we directly linked extracted feature vectors to known ground truth parameters."}, {"title": "CONCLUSIONS", "content": "While DSR is currently a burgeoning field, the question of how to best integrate dynamically diverse systems into a common structure, and how to harvest this for transfer learning, generalization, and classification, only recently gained traction (Yin et al., 2021; Kirchmeyer et al., 2022; G\u00f6ring et al., 2024). Unlike TS models, where the interest lies mainly in forecasting or TS classification/ regression, in DSR we aim for generative models that capture invariants of the true, underlying system's state space. This commonly requires special training algorithms or loss functions (Mikhaeil et al., 2022; Platt et al., 2023). Using GTF (Hess et al., 2023) for training, we demonstrated that hierarchically structured models significantly improve DSR quality and temporal agreement compared to 'traditional', non-hierarchical methods. This is a strong indication that our method was able to utilize information from all DS observed, even if located in different dynamical regimes, to boost performance on each individual system. This was confirmed in transfer learning experiments, where models trained on multiple subjects generalized effectively to new conditions with minimal additional data, and did so much more convincingly than various competing methods. Identifying dynamical regimes, instead of just utilizing TS waveform features, is also likely to be a more principled approach to TS foundation models, since it is the underlying dynamics which determine a system's temporal properties across contexts. This idea is supported by the strong classification results on human EEG data, where subject groups were much more clearly segregated in the DS feature space than in spaces constructed by more \u2018conventional' TS models. These DS feature spaces furthermore turned out to be surprisingly interpretable. Extracting these dynamically most crucial features and grouping subjects along these dimensions may be of considerable interest in areas like medicine or neuroscience, and is a new application that has not been considered in the DSR field so far.\nLimitations The highly interpretable nature of the subject-level feature spaces, despite the strongly nonlinear and chaotic systems considered, is surprising. In future work, it needs to be determined how general this property is and whether it also holds for other physical and biological systems. It appears that our method automatically infers the critical control parameters of a system that best differentiate between dynamical regimes (by controlling the underlying bifurcation structure), but it"}]}