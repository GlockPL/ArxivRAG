{"title": "Theia: Distilling Diverse Vision Foundation Models for Robot Learning", "authors": ["Jinghuan Shang", "Karl Schmeckpeper", "Brandon B. May", "Maria Vittoria Minniti", "Tarik Kelestemur", "David Watkins", "Laura Herlant"], "abstract": "Vision-based robot policy learning, which maps visual inputs to actions, necessitates a holistic understanding of diverse visual tasks beyond single-task needs like classification or segmentation. Inspired by this, we introduce Theia, a vision foundation model for robot learning that distills multiple off-the-shelf vision foundation models trained on varied vision tasks. Theia's rich visual representations encode diverse visual knowledge, enhancing downstream robot learning. Extensive experiments demonstrate that Theia outperforms its teacher models and prior robot learning models using less training data and smaller model sizes. Additionally, we quantify the quality of pre-trained visual representations and hypothesize that higher entropy in feature norm distributions leads to improved robot learning performance. Code and models are available here.", "sections": [{"title": "1 Introduction", "content": "Visual understanding, i.e., the process of abstracting high-dimensional visual signals like images and videos, includes many different sub-problems, from depth prediction [1] and vision-language correspondence [2, 3], to tasks ranging from coarse to fine granularity such as classification [4, 5] and object grounding [6, 7, 8], as well as tasks defined along spatial and temporal axes like"}, {"title": "2 Related Work", "content": "segmentation [9, 10] and tracking [11]. Given this diversity, a long-standing effort in the vision community has been to develop models tailored to one or a few specific types of visual understanding tasks. In recent years, several models [4, 5, 10, 2, 3, 12, 13, 6, 14, 15, 10, 7, 16] have achieved remarkable generalizability to unseen domains and new tasks, and are commonly referred to as vision foundation models (VFMs).\nVision-based robot policy learning, which learns action policies from visual inputs, requires strong and diverse visual comprehension. These policies involve many implicit vision tasks such as object recognition and semantic grounding, where off-the-shelf VFMs corresponding to some well-defined tasks can be easily found, but there is no single model for all of vision tasks. Studies have shown that off-the-shelf VFMs such as CLIP [2] usually under-perform relative to visual representation models tailored for specific tasks in robot learning [17, 18, 19, 20, 21]. This fact reveals a gap between the needs of robot learning and the limited visual understanding capabilities of any individual VFM. Prior work on learning foundational visual representation models for robotics has focused primarily on improving training data [17, 21, 20] and designing objective functions [17, 18], rather than enhancing the ability to solve multiple implicit visual understanding tasks.\nIn this work, we propose combining multiple large VFMs into a single, smaller model for robot learning that leverages diverse visual understanding abilities from VFMs. We achieve this via knowledge distillation [22]. Unlike conventional distillation from a larger model to a smaller model on the same task, we distill VFMs tailored for varied vision tasks to improve visual representation for robot learning, which is an unseen task for VFMs.\nWe introduce Theia, a robot vision foundation model that simultaneously distills off-the-shelf VFMs such as CLIP [2], DINOv2 [7], and ViT [5]. Theia generates rich representations for downstream robot learning, maintaining a comprehensive understanding of many visual sub-problems at the spatial level. Compared to off-the-shelf VFMs [2, 5, 7] and prior works [19, 20], Theia offers both better pre-trained visual representations for higher downstream robot learning performance and reduced computational costs. Furthermore, training Theia only requires ImageNet [23] and about 150 GPU hours, in contrast to prior works which necessitate substantially more compute [19, 20, 17, 18]. To understand what makes a good visual representation for robot learning, we observe multiple factors that relate to the performance of downstream robot learning tasks. We hypothesize that higher entropy in representation norms [24] correlates with improved robot learning performance.\nIn summary, our contributions are:\n\u2022 We introduce Theia, a model that combines knowledge from multiple VFMs into a single, smaller model using knowledge distillation with low training cost.\n\u2022 Through extensive simulated and real-world experiments, we confirm that Theia's visual representations lead to better downstream robot learning with improved computational efficiency.\n\u2022 We identify key factors relevant to robot learning performance, such as model size, the use of spatial tokens, and the entropy of representation norms, offering valuable insights for guiding future research on optimizing visual representations for robot learning."}, {"title": "2.1 Visual Representations for Robot Learning", "content": "Visual representations are important for vision-based robot policies to parse high-dimensional visual signals. Visual representation learning can happen at different stages, including pre-training [17, 18, 20, 21], joint-learning with robot tasks [25, 26, 27, 28], or a combination of both using either trainable or frozen visual representations [29, 30]. Off-the-shelf visual encoders [4, 5, 2] can also provide visual representations for robot learning. Additionally, an important factor in training visual representations is the choice of data. ImageNet [23], as suggested by Dasari et al. [21], is a particularly effective pre-training dataset, while video datasets [31, 32] are also widely used. Training objectives and auxiliary tasks for visual representation learning vary, including data augmentation [25, 26], prediction tasks [28], contrastive learning [27, 33, 34], and self-supervised learning [19, 20]. Specifically, to handle invariance and equivariance in visual observations, inductive"}, {"title": "2.2 Vision Foundation Models", "content": "Vision foundation models trained on large-scale data exhibit strong task-specific performance and transferability to unseen domains and new tasks. VFMs can focus on single tasks, multiple tasks, or remain task-agnostic at the pre-training stage. For example, ViT [5], DeiT [39], and ConvNeXt [40] are designed for image classification, SAM [10] for semantic segmentation, and Depth-Anything [1] for depth prediction. When combined with Large Language Models (LLMs) [41, 3, 42, 43], these models can solve multiple visual tasks including referring segmentation, visual question answering (VQA), and image editing. Task-agnostic models like the DINO series [6, 7] are trained through self-distillation, while CLIP [2] is trained by aligning image-text pairs. Given the strong generalization capability of VFMs, robot learning should also benefit from leveraging the latent representations of pre-trained VFMs, which is the primary motivation for our research."}, {"title": "2.3 Knowledge Distillation in Vision Models", "content": "Knowledge distillation [22] compresses the knowledge from one or more larger models into a single smaller model. To leverage VFMs, which are usually computationally intensive, several studies have explored distilling them into more compact models. For example, some works distill a single VFM like SAM [10] into smaller variants [44, 45, 46]. There are also relevant works on combining two models: SAM-CLIP [47] merges SAM [10] and CLIP [2] into one model, while ViLD [48] and GroundingDINO [49] combine vision and language models to perform object detection. RADIO [50] is an agglomerative model that simultaneously distills CLIP, SAM, and DINOv2. These studies show that combined models can improve performance on downstream tasks or enable new applications. Similarly, in this work, we investigate whether combining VFMs will benefit robot learning. Notably, RADIO [50] is the closest approach to ours. The key differences between Theia and RADIO [50] are that (1) We aim to use our representation for robot learning tasks that none of the VFMs have covered in their pretraining tasks, (2) we distill only spatial tokens rather than both spatial and [CLS] tokens, (3) we choose a different set of teacher models, and (4) we show how each teacher model contributes to robot learning performance."}, {"title": "3 Method", "content": "Overview. We introduce Theia, a framework that distills the knowledge of multiple VFMs into a smaller model, producing rich spatial representations for downstream vision-based robot learning. \nFigure 2 shows the overall design of Theia. Our model comprises a visual encoder (backbone) and a set of feature translators for distillation. Note that only the visual encoder is used to produce latent representations for downstream robot learning tasks.\nArchitecture. Given an input image x, the visual encoder f(.) produces a rich representation z = f(x) (called the Theia-representation), which is utilized for downstream robot learning tasks. We focus on backbone models that are smaller than typical VFMs, specifically using ViT-Tiny,"}, {"title": "3.1 Rich Spatial Representation", "content": "The Theia-representation is a set of encoded tokens corresponding to input image patches. We choose spatial tokens because spatially-dense representations are the foundation for diverse visual understanding, as evidenced by the powerful per-patch features in DINOv2 [7]. Therefore, we aim to distill all spatial tokens and leave the [CLS] token untouched.\nFeature Translators. Our goal is to supervise Theia-representations with teacher representations from various VFMs. We extract teacher representations hi (x) of VFMs at the last layer for CLIP [2], ViT [5] and DINOv2 [7], or before the decoders for SAM [10] and Depth-Anything [1]. Since a single representation cannot be learned to match all the teacher representations directly, feature translators gi() are used to map the Theia-representation, z, to each teacher representation. Feature translators are shallow CNNs to ensure that knowledge is distilled primarily into Theia's visual encoder. Details are available in the Appendix."}, {"title": "3.2 Training", "content": "Distillation Objective. Our training objective is matching the outputs of the feature translators with their corresponding teacher VFM representations. To achieve this, we use a combination of cosine and smooth-L1 losses [50] to match each pair of predicted and ground truth representations for the same image, taking their weighted average. Formally, our loss is\n$L(x; \\theta) = \\sum_i a_i (\\beta L_{cos}(g_i(f(x)), h_i(x)) + (1 - \\beta) L_{smooth-L1}(g_i(f(x)), h_i(x)))$, (1)\nwhere x is the input image, M is the number of teacher VFMs, a_i is the loss weight for each teacher, and \u03b2 is the weight for balancing cosine loss and smooth-L1 loss respectively. In general, we set ai = 1/M such that the loss weights each teacher equally. We empirically set \u03b2 = 0.9 [50].\nFeature Normalization. To properly accommodate the different scales of teacher representations, we first perform a normalization step. The teacher representations are normalized over each latent dimension, where mean and variance are calculated from all ImageNet training samples.\nDataset. We train our model on the ImageNet [23] training set for 50 epochs. We opt to use ImageNet because of its greater diversity compared to human videos [31, 51, 52, 53] and robot datasets [54, 55, 56, 57, 58] within the same number of images. This diversity has been experimentally shown to improve visual representation learning [21]."}, {"title": "4 Experiments", "content": "Benchmark and Settings\nTo evaluate pre-trained visual representations, we use simulation tasks in CortexBench [20], which combines MuJoCo tasks (Adroit [59, 60], DeepMind Control Suite (DMC) [61], and Meta-World [62]), Habitat [63, 64] tasks (ImageNav [65], ObjectNav [66], and MobilePick [67].), and Trifinger [20] tasks. ImageNav and MobilePick are reinforcement learning (RL) tasks and others are imitation learning (IL) tasks. We follow the experiment settings of Majumdar et al. [20] and re-port aggregated scores for rewards (DMC tasks) and success rates (all other tasks). For DMC tasks, raw rewards are divided by 10 to be in a scale consistent with the success rate. We also conduct real robot experiments, introduced in Section 4.3. We use the same policy heads for the each type of representations (vector or spatial tokens). Full experimental details are available in the Appendix."}, {"title": "4.2 Simulation Results", "content": "We comprehensively evaluate Theia and baseline pre-trained models on the MuJoCo subset of Cor-texBench [20] to provide an overall assessment of pre-trained visual representations. We consider prior works on visual representations for robot learning, including R3M [17], VIP [18], MVP [19], and VC-1 [20], as well as agglomerative models for vision tasks RADIO and E-RADIO [50], and off-the-shelf vision foundation models ViT [5], DINOv2 [7], and CLIP [2]. All pre-trained represen-tations are frozen in this experiment. Throughout this section, we answer the following questions:\n\u2022 How does Theia perform compared to baselines?\n\u2022 Which is more effective for visual representations: [CLS] or spatial tokens?\n\u2022 How does robot learning performance scale with the size of the visual encoder?\nTheia Performance. As shown in Figure 4, Theia outperforms all evaluated models, surpassing the per-formance of the best prior models, R3M and MVP, as well as agglomerative models for vision tasks RA-DIO and E-RADIO. We also tested a naive approach of using multiple VFMs (CLIP, DINOv2, and ViT) simultaneously by concatenating their spatial tokens channel-wise (CDV in Figure 4), but this performed much worse than using just the individual VFMs. Theia models scale effectively from tiny to base sizes, with Theia-S and Theia-B being the only models to break scores of 80 on this subset of CortexBench, even though they use only a small fraction of the inference computation required by comparable models. Theia's training is very efficient, using only the 1.2M images in ImageNet with a training time of about 150 GPU hours on NVIDIA H100 GPUs, compared to approxi-mately 5M images used in prior works [17, 18, 19, 20] and 1B images used by RADIO [50].\nSpatial Tokens vs. [CLS] Token. We evaluate Transformer-based models (all models in Figure 4 except R3M and VIP) using either their [CLS] token or spatial tokens for downstream robot learning. To accommodate spatial tokens in the robot policy, we introduce extra shallow CNN layers at the input of the policy network, known as \u201ccompression layer\" [68, 20]. Figure 4 shows the results of all models we evaluated, clearly showing that for Transformer-based models, providing spatial tokens is consistently better than using the [CLS] token for robot learning. This finding applies to both off-the-shelf VFMs and prior pre-trained representations, including MVP and VC-1."}, {"title": "4.3 Real World Robot Learning", "content": "Based on simulation performance, we test Theia-B and the best-performing baseline models: MVP-L [19], R3M [17], VC-1-L [20], DINOv2-L [7], ViT-H [5], and E-RADIO-L [50] for evaluation on real-world tasks. We employ four tasks (Figure 3): Door Opening, Pick-and-Place, and Toy-Microwave Cooking with a WidowX 250s arm, and Drawer Opening with a Boston Dynamics Spot. We train behavioral cloning policies on top of visual representations using conventional policy net-works composed of CNNs and MLPs in the WidowX setup and diffusion policy [69] in the Spot setup. During testing, we vary the robot position for Door Opening and Drawer Opening, random-ize the object position for Pick-and-Place, and randomize both the object positions and object types in Toy-Microwave Cooking. Full experimental settings are available in the Appendix.\nTable 2 shows the success rates on these real-world tasks. Theia-B achieves the highest success rate across all tasks except Toy-Microwave Cooking. The results also highlight that the Theia-representation is useful for both conventional and diffusion-based policy heads, and for either freez-ing or fine-tuning the visual representation. E-RADIO is the most competitive model in this setting amongst all models compared, likely due to its similar distillation of VFMs and much larger training dataset. VC-1 has high task variance, performing poorly on Door Opening but adequately on Pick-and-Place. ViT-H works much better when being fine-tuned but DINOv2 does not, which could be caused by some fundamental differences in VFMs."}, {"title": "4.4 Ablation Studies", "content": ""}, {"title": "4.5 Qualitative Visualizations", "content": "We present qualitative visualizations to demon-strate how Theia-representations can be trans-formed into teacher representations through fea-ture translators. Using Theia trained with all teachers (CDeDiSV, All), we applied PCA for visualizing predicted DINOv2 [7] features, used the SAM [10] decoder to produce segmentation results, and used the Depth-Anything [1] head to produce estimated depth. Results are shown in Figure 6 with more examples in the Appendix. The visualizations indicate that our predicted rep-"}, {"title": "5 What Makes Visual Representations Good for Robot Learning?", "content": "Traditionally, the quality of the pre-trained visual representations is evaluated through downstream robot learning like IL or RL. However, it is unclear why different visual representations lead to varying robot learning performance outcomes. In this section, we quantify the quality of visual representations and analyze how they correlate with downstream robot learning performance.\nFeature Norm Distributions and Entropy. Darcet et al. [24] analyzed the norm of spatial tokens in vision Transformers and found that high-norm outlier tokens are detrimental to vision task perfor-mance. Following this, we investigate whether a similar phenomenon arises in visual representations for robot learning. We inspect the feature norms of Theia with different teacher combinations and baseline models evaluated in Section 4.2, and their corresponding performance on the MuJoCo sub-set tasks. We sample 1% of the visual observations from the MuJoCo task training set and calculate the L2-norm of each spatial token after encoding. We measure the entropy of the feature norm distribution across all samples and patches per model and use it as a quantitative metric.\nWe confirm that similar outlier tokens also appear in VC-1 corresponding to the image patches that are not task-relevant, shown in the visualizations of feature norms on the right of Figure 7. In contrast, Theia has very few or no outlier tokens, and the tokens with higher norms are more task-relevant even though Theia-representations are not trained on these robot images. In our quantitative analysis (Figure 7, left), we divide the models into distilled and regular based on the observation that distilled models generally have higher entropy (fewer outliers, Figure 4(c) in Darcet et al. [24]). We find that there is a strong correlation (R=0.943) between entropy and robot learning performance among regular models, and a high correlation (R=0.638) among distilled models. We hypothesize that spatial token representations with high entropy (better feature diversity) encode more informa-tion that aids policy learning, while less diverse representations (low entropy) may hinder it. In the Appendix, we discuss the results of other quantitative measurements, including feature similarity and PCA-explained variance ratios, where no strong correlations are found."}, {"title": "6 Conclusion", "content": "In this work, we introduced Theia, a novel robot vision foundation model specifically distilled from multiple VFMs to enhance robot learning tasks. Theia builds a rich visual representation from diverse VFM teachers, preserving spatial features to ensure detailed visual comprehension. Through extensive evaluations on CortexBench and in the real world, Theia consistently outperforms state-of-the-art models, including all prior models for robot learning, off-the-shelf VFMs, and similarly distilled models for vision tasks. Our results highlight the effectiveness of distilling multiple VFMs into a compact model for superior performance in a variety of robot learning scenarios. Furthermore, we answer a key question about what kinds of visual representations lead to better robot learning by finding a strong correlation between the entropy of feature norms and enhanced downstream performance, offering insights for future research on optimizing visual representations for robotics."}, {"title": "A Theia Model Architecture", "content": "Backbone. We use the DeiT-Tiny, DeiT-Small, and DeiT-Base models [39] as our backbone archi-tectures. We keep the [CLS] token in the model and in the forward pass, but there is no supervisory signal provided for it. As a result, the [CLS] token serves as a \"register token\" [24], which provides some benefits for learning high quality representations. We train Theia from scratch (no pre-trained DeiT [39] weights are applied).\nFeature Translators. The feature translators are composed primarily of CNNs, with a linear layer appended at the end to match the teacher's representation dimension. Pure linear transforms might not be able to map Theia-representations to all three teacher representations well, resulting in a failure of learning (See Table 6a). Thus, we use three CNN layers to account for the fact that each teacher model's representations are very different from one another. Details are listed in Table 4, where we show the architectural details of the translators used for our (student, teacher)-feature pairs."}, {"title": "3.1 Rich Spatial Representation", "content": "The Theia-representation is a set of encoded tokens corresponding to input image patches. We choose spatial tokens because spatially-dense representations are the foundation for diverse visual understanding, as evidenced by the powerful per-patch features in DINOv2 [7]. Therefore, we aim to distill all spatial tokens and leave the [CLS] token untouched.\nFeature Translators. Our goal is to supervise Theia-representations with teacher representations from various VFMs. We extract teacher representations $h_i (x)$ of VFMs at the last layer for CLIP [2], ViT [5] and DINOv2 [7], or before the decoders for SAM [10] and Depth-Anything [1]. Since a single representation cannot be learned to match all the teacher representations directly, feature translators $g_i ()$ are used to map the Theia-representation, z, to each teacher representation. Feature translators are shallow CNNs to ensure that knowledge is distilled primarily into Theia's visual encoder. Details are available in the Appendix."}, {"title": "B Training", "content": "We train Theia on 8 NVIDIA H100 GPUs. The main bottlenecks in training are the data transfer speed between devices and the GPU memory bandwidth to load large spatial feature tensors, for example, of size 1280\u00d716\u00d716 for ViT-H and 256\u00d764\u00d764 for SAM. We pre-compute the features from all teacher models instead of doing inference on the fly. This approach requires extra storage space to save all the features extracted from the VFMs, but significantly saves on training time and avoids loading models with high GPU memory usage during training, such as Depth-Anything or SAM (a batch size of 16 cannot fit into 80GB of GPU memory). All training configurations are listed in Table 5.\nTeacher VFM Features. We use the output representations at the last layer of ViT [5], CLIP [2], and DINOv2 [7]. For SAM [10], we use its encoder output. For Depth-Anything [1], since it is initialized from DINOv2, we use the latent representation before the final convolution layer. When decoding SAM and Depth-Anything results from Theia-predicted representations, we send the pre-dicted representations through the remaining layers of original models and obtain the output."}, {"title": "C Additional Ablation Studies", "content": "We conduct two additional ablation studies to verify design choices in the Theia model. The first is a comparison between the current CNN-based feature translator and a linear feature translator. \nIn Table 6a, we find that using a Linear feature translator leads to a significant performance drop. The second ablation studies whether Theia should be trained from scratch or be initialized using the pre-trained DeiT [39] weights. In Table 6b, we find that using pre-trained weights improves the downstream performance. This could be interpreted as the positive effect of incorporating knowl-edge from an additional useful model into the distillation process. We would expect to see similar performance improvements as more informative models are included during training."}, {"title": "D Full Experimental Settings", "content": ""}, {"title": "D.1 Baseline Models", "content": "Theia and baseline models are trained on different sizes of datasets using different objectives. We organize these details in Table 7 to provide a comprehensive comparison between them."}, {"title": "D.2 CortexBench", "content": "For all of our CortexBench experiments, we use the original setup [20], except for a few modifica-tions to produce more reliable results. The modifications include:\n\u2022 We increase the number of evaluation roll-outs from 10 (original) to 25 (ours) in DMC tasks. The mean scores reported are from a total of 75 runs (25 per seed x 3).\n\u2022 We remove the noise added to the policy network output in the CortexBench code base. The noise causes minor performance degradation (about 1.0 on overall mean score for MuJoCo tasks) compared to the version without noise.\n\u2022 We modify the policy networks to take spatial feature inputs for MuJoCo and Trifinger tasks (details follow and are presented in Table 8).\nNote that prior models including R3M, VIP, MVP, and VC-1 are all re-run using the same settings in MuJoCo tasks for the purposes of making a fair comparison when evaluating against Theia.\nPolicy Networks. For MuJoCo and Trifinger tasks we utilize a three-layer MLP for vector-based representations, including ResNet models and Transformer models that use the [CLS] token. For models that generate spatial feature maps, such as Transformers using spatial tokens, we introduce a three-layer CNN before the MLP. For Habitat tasks, we exclusively benchmark models that produce spatial feature maps and adopt the same policy network as used by Majumdar et al. [20]. Details can be found in Table 8."}, {"title": "Spatial Representation dimension d \u00d7 H \u00d7 W", "content": "Conv2d(d, 256, kernel_size=4, stride=2, padding=1)\nReLU\nConv2d(256, 256, kernel_size=3, stride=2)\nReLU\nConv2d(256, 256, kernel_size=3, stride=1)\nLinear(256, 256)\nLinear(256, 256)\nLinear(256, action dimension)\nVector Representation dimension d\nLinear(d, 256)\nLinear(256, 256)\nLinear(256, action dimension)"}, {"title": "D.3 Real World Robot Learning", "content": "D.3.1 WidowX Arm Experiments\nWidowX Arm Setup. The robot used for these experiments is a 6-degree-of-freedom (DOF) Wid-owX 250s arm. The data collection and evaluation framework is based on [56].\nWe train a behavior-cloning policy for each of the four evaluated setups and for each evaluated baseline (see Table 2 and Figure 3); the training hyperparameters are shown in Table 9. The policy's observations are RGB images and robot joint states. Images are encoded by a pre-trained visual encoder and a randomly initialized, unfrozen feature neck (\u201ccompression layer\u201d [68]). We use the same feature neck as we did for the previously discussed MuJoCo tasks in Section 4.2. The encoded vector is concatenated with the robot's joint states, which is fed into a 3-layer MLP with a hidden dimension of size 256. The policy outputs end-effector commands, consisting of the end-effector's delta positions (Cartesian coordinates), delta rotations (Euler angles), and the gripper's opening/closing command; such commands are tracked by the robot at a frequency of 5 Hz. In addition to the hyperparameters listed in Table 9, we vary the policy action prediction"}, {"title": "E More Visualizations of Translating to Teacher Model Features", "content": ""}, {"title": "F Per-Task CortexBench Results", "content": "In Table 11 we report per-task scores of the models evaluated in Figure 4, over the MuJoCo subset of tasks. In Table 12, we report per-task scores of all 14 tasks we evaluated in Cortexbench, corre-sponding to Table 1. Note that we perform the evaluation following the original Cortexbench [20] protocol, where there are a total of 75 runs per MuJoCo task (we increased it from 30 to 75 for DMC tasks), 100 runs in Reach Cube, and 4200 runs in ImageNav."}, {"title": "G Analysis of Visual Representations", "content": "Entropy of the Representation Norm Distribution. Given N representations produced by en-coding N images per model, where each representation contains P spatial tokens, we discretize the distribution of token norms over all NXP tokens by using a histogram. We normalize the count"}, {"title": "H Linear Probing on ImageNet", "content": "In addition to robot learning, we evaluate the Theia-representation on vision tasks to show to how well such abilities are maintained after the distillation process. For example, to evaluate image classification performance we apply linear probing on the Theia-representation to classify images from ImageNet-1k [23]. We use mean pooling of the Theia-representation (i.e. spatial tokens) and the same training schedule as MAE [14]. Results are shown in Table 13, where we find that Theia outperforms MAE [14] at the same model size, but is not comparable to SOTA results from models like DINOv2 [7]."}]}