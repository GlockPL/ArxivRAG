{"title": "Hybrid Transformer for Early Alzheimer's Detection: Integration of Handwriting-Based 2D Images and 1D Signal Features", "authors": ["Changqing GONG", "Huafeng Qin", "Moun\u00eem A. El-Yacoubi"], "abstract": "Alzheimer's Disease (AD) is a prevalent neurodegenerative condition where early detection is vital. Handwriting, often affected early in AD, offers a non-invasive and cost-effective way to capture subtle motor changes. State-of-the-art research on handwriting, mostly online, based AD detection has predominantly relied on manually extracted features, fed as input to shallow machine learning models. Some recent works have proposed deep learning (DL)-based models, either 1D-CNN or 2D-CNN architectures, with performance comparing favorably to handcrafted schemes. These approaches, however, overlook the intrinsic relationship between the 2D spatial patterns of handwriting strokes and their 1D dynamic characteristics, thus limiting their capacity to capture the multimodal nature of handwriting data. Moreover, the application of Transformer models remains basically unexplored. To address these limitations, we propose a novel approach for AD detection, consisting of a learnable multimodal hybrid attention model that integrates simultaneously 2D handwriting images with 1D dynamic handwriting signals. Our model leverages a gated mechanism to combine similarity and difference attention, blending the two modalities and learning robust features by incorporating information at different scales. Our model achieved state-of-the-art performance on the DARWIN dataset, with an F1-score of 90.32% and accuracy of 90.91% in Task 8 ('L' writing), surpassing the previous best by 4.61% and 6.06% respectively.", "sections": [{"title": "I. INTRODUCTION", "content": "Alzheimer's disease (AD), the most common cause of dementia, is a progressive neurodegenerative disorder (ND) characterized by gradual nerve cell degeneration, leading to cognitive decline in memory, reasoning, and daily functioning [1, 2, 3, 4, 5]. Similar conditions, including Lewy body disease, frontotemporal degeneration, Parkinson's disease, and stroke, also impair cognitive functions. The incidence of these diseases increases with age [6, 7, 8, 9, 10]. Though incurable, current treatments aim to manage progression, emphasizing the need for improved early diagnostic methods.\nThe current medical consensus is that dementia is irreversible once clinical symptoms appear, but early detection and intervention can slow its progression [11]. However, expensive and invasive diagnostics (e.g., A-PET, cerebrospinal fluid testing) [12] and subjective neuropsychological tests (e.g., MMSE, MOCA) hinder early diagnosis and widespread screening of Alzheimer's disease [13]. Researchers have explored biomarkers sensitive to cognitive decline, using machine learning (ML) to analyze signals like eye movement [14], speech [15, 16], galvanic skin response [17], and Gait disturbances and frailty [18, 19, 20, 21]. Handwriting changes caused by AD have also been studied recently [1, 22, 23, 24, 25]. Handwriting, which involves cognitive and motor functions, offers a non-invasive, cost-effective way to track disease progression [25, 26, 27]. ML applied to motor function can reduce clinical assessment time [28], and graphic tablets enable easy online handwriting tasks while capturing kinematic and dynamic data [29].\nState-of-the-art research on handwriting-based AD detection has predominantly relied on manually extracted features, fed as input to shallow ML models[1, 30, 31, 32]. Recently, deep learning (DL) has shown strong feature representation capabilities, yielding promising results in tasks like image segmentation [33], video processing [34], object tracking [35], and biometric recognition [36]. Few works [37, 38, 39] have proposed, for AD detection, deep learning (DL)-based models, either 1D-CNN modeling 1D feature signals or 2D-CNN modeling 2D handwriting images, outperforming handcrafted schemes. These approaches, however, overlook the relationship between the 2D spatial patterns of handwriting strokes and their 1D dynamic characteristics, thus limiting their capacity to capture the multimodal nature of handwriting data. Moreover, the application of Transformer models remains basically unexplored. Inspired by the success of Transformers in image recognition and natural language processing, we propose a novel hybrid Transformer model for early AD that addresses these limitations. Our Transformer is multimodal as it integrates 2D handwriting images with 1D feature signals, by encoding both modalities and incorporating a learnable similarity and difference attention mechanism. Our model leverages a hybrid attention mechanism and introduces a new loss function combining template contrastive loss with cross-entropy loss to improve classification performance. Designed to be lightweight due to the small dataset size, the model uses a shallower architecture with shorter encodings. We benchmarked our model against state-of-the-art classifiers, achieving superior performance. Our main contributions are as follows:"}, {"title": "II. RELATED WORK", "content": "Deterioration in writing ability is a known diagnostic indicator of Alzheimer's Disease (AD) [40], and kinematic handwriting analysis has revealed pathological features in the handwriting process [1]. Handwriting-based AD detection methods can be broadly categorized into two categories: traditional machine learning (ML) and deep learning (DL).\nMany studies have applied traditional ML techniques for AD detection. Qi et al. [31] used logistic regression on kinematic features, such as writing speed and pen pressure, achieving an accuracy range from 71.5% to 96.55%. Chai et al. [32] employed SVMs leveraging handwriting dynamics based on writing speed, time, and pressure, with an accuracy of 89% in distinguishing mild cognitive impairment (MCI) from AD. Meng et al. [41] applied a 2D discrete Fourier transform, corner detection, and gray-level co-occurrence matrix analysis on Archimedes spiral and labyrinth lattice handwriting images, and achieved a mean AUC of 0.94 with a Decision Tree classifier. Cilia et al. [42] employed Random Forest on a novel large dataset for AD detection, achieving an accuracy of 85.29%. These methods show promise in identifying temporal dynamics, kinematics, and spatial characteristics associated with Alzheimer's, such as writing speed and letter size.\nDeep learning (DL) has proven to be a powerful tool for handwriting-based neurodegenerative disease detection, including AD and Parkinson's Disease (PD). DL methods use either 2D image data or 1D feature signals. Given the shortage of papers leveraging DL for assessing AD from handwriting, we report also papers on PD. Pereira et al. [43] transformed 1D signals from a smart pen into 2D images for PD classification using CNNs, achieving 93.5% accuracy. Taleb et al. [44] transformed 1D time series into 2D images, fed to CNN and CNN-BLSTM models, for PD detection. The accuracy improved from 83.33% to 97.62% with data augmentation. Diaz et al. [45] combined 1D convolutional layers with Bi-GRU layers for PD recognition, achieving 94.44% accuracy. For AD detection, Cilia et al. [42] introduced the DARWIN (Diagnosis Alzheimer With Handwriting) dataset, with 174 participants, comprising AD patients and healthy controls. In a related study, Cilia et al. [46] classified AD using handcrafted and CNN-extracted features from color and binary images. They employed CNN models such as VGG19, ResNet50, InceptionV3, and InceptionResNetV2 to extract features from RGB and binary images, fed to ML algorithms, like k-Nearest Neighbors (kNN), MLP, Random Forest, and SVM, for classification, with CNN-extracted features outperforming handcrafted features. Subsequently, Cilia et al. [47] converted handwriting into color images encoding dynamic information to enhance feature representation. Erdogmus et al. [39] transformed manually extracted 1D features into 2D features fed to CNN, achieving an accuracy of 90.4%. Dao et al. [37] developed a 1D-CNN to detect early-stage AD from online handwriting loops. To tackle the limited training data, they employed various data augmentation techniques, including a GAN variant (DoppelGANger) to generate realistic handwriting sequences, achieving an accuracy of 89% accuracy. It is worth noting that the accuracies reported above are essentially not comparable as most were obtained on different datasets, under different experimental protocols. In our experiments, we implement several state-of-the-art models in order to soundly benchmark our approach on the same dataset.\nThe literature on handwriting-based AD detection highlights a range of approaches. Traditional ML techniques have been widely adopted by extracting key handwriting features. They often require, however, extensive manual feature engineering, which limits their ability to fully capture the complexity of handwriting variations in AD. DL methods have shown superior performance by learning intricate spatial and dynamic patterns directly from raw handwriting samples. Some studies, nevertheless, still depend on manual feature extraction, converting features into 2D images for CNNs. While a few studies have explored one-dimensional (1D) time series feature signals, none have examined the correlation between 2D handwriting images and 1D signals, and the impact of combining these modalities on AD. Furthermore, the application of Transformers to handwriting recognition for AD remains unexplored, leaving a gap in current research. To address these challenges, we propose a multimodal Transformer model that integrates 2D handwriting images with 1D feature signals, offering a promising approach for more accurate AD detection."}, {"title": "III. MATERIAL AND METHOD", "content": "In this section, we introduce the dataset, describe our preprocessing of raw signal data, the extraction of 1D signal features and the reconstruction of handwriting images."}, {"title": "A. Dataset", "content": "We used the DARWIN-RAW dataset [42], a gold-standard resource for AD diagnosis, with data from 174 participants (89 AD patients and 85 healthy controls). This dataset includes 25 handwriting tasks designed for early AD detection [48], categorized into four types: (1) graphic tasks, (2) copy tasks, (3) memory tasks, and (4) dictation tasks. The raw handwriting data (xi, Yi, Pi) were preprocessed to generate 2D images and 1D feature signals. This process was motivated by the effectiveness of kinematic features in detecting early AD."}, {"title": "B. Pre-processing", "content": "In the preprocessing phase, we examined the dataset for missing values based on the timestamps of each handwriting task. Missing values were estimated using interpolation and imputed accordingly, and outliers were removed. If a participant had any unrecorded task, their data for that task were discarded. The data for each task were standardized to have a mean of 0 and a standard deviation of 1."}, {"title": "C. 1D Signal Feature Extraction", "content": "Six key features, speed, acceleration, pressure rate of change, curvature, and angular speed, were extracted from raw handwriting signals for analysis and model training. The final dataset combines these computed features with the original data. An example of 1D signal is shown in Figure 2."}, {"title": "D. Generation of 2D Images from online handwriting", "content": "Building on prior work [47], we used the original (xi, Yi) coordinates to generate images for network training. In contrast to related studies, the RGB components, ri, gi, and bi, were derived from pressure rate of change, acceleration, and angular velocity, respectively. The generated images were normalized and smoothed using interpolation, with examples shown in Figure 3."}, {"title": "E. Data Augmentation", "content": "To enhance data diversity and improve model generalization, we have applied data augmentation techniques, including rotation, noise addition, scaling, window warping, and window slicing, to simulate real-world disturbances. Some augmented images are shown in Figure 3."}, {"title": "IV. PROPOSED WORK", "content": "Handwriting recognition faces challenges in capturing the relationships between handwriting images and signals such as pen pressure, acceleration, and angular velocity, due to the differences between sequence-based 1D signal tasks and vision-based 2D image tasks. To address these challenges, we propose HSDA-MS Transformer, a Multi-Scale Transformer based on Hybrid Similarity and Difference Attention for early Alzheimer's detection. The Hybrid Similarity and Difference Attention (HSDA) scheme employs a gating mechanism to combine similarity and difference weights, capturing dependencies between both 2D images and 1D signals. Convolutions are integrated into the Transformer to capture features at different scales, enhancing robustness. Additionally, we introduce a plug-and-play template contrastive loss function, which updates positive and negative templates during training to learn more discriminant features."}, {"title": "A. HSDA-MS Transformer", "content": "1) Image embedding: In ViT [49], an image is split into non-overlapping 2D patches, transformed into 1D embeddings using a multi-layer perceptron (MLP). To preserve spatial information lost in this process, we introduce a stem module. As shown in Figure 5(a), the stem block consists of three convolutional layers and an MLP. Three 3 \u00d7 3 convolutions with a stride of 2 reduce the input size, while one 3\u00d73 convolution with a stride of 1 extracts local spatial features. The MLP then converts the feature map into a fixed-size embedding, capturing global context and abstract features. Given an input image $X^{2d} \\in R^{H \\times W \\times 3}$, the stem block generates a feature map $X'_{2d} \\in R^{\\frac{H}{4}\\times \\frac{W}{4}\\times C}$, where C = 128. The MLP then transforms it into $X' \\in R^{1 \\times d}$, where d = 128, as shown in Eq. (1) and Eq. (2):\n$X'_{2d} = ImageEmbedding(X^{2d})$\n$X' = MLP(X'_{2d})$\n2) Signal embedding: In this network, the 1D feature signal is processed to extract robust features. To prevent loss of critical information, we introduce an embedding module using adaptive average pooling, fully connected layers, and normalization. As shown in Figure 5(a), the embedding block consists of an adaptive average pooling layer, two fully connected layers, and an MLP. The pooling layer reduces the input signal's dimensionality, while the fully connected layers, with normalization and activation, extract meaningful features. The final MLP converts these features into a fixed-size embedding, capturing global context. Given an input signal $X^{1d} \\in R^{N\\times D}$, where N is the number of signals and D is the dimensionality, the embedding block produces $X'_{1d} \\in R^{N\\times D'}$ with D' = 2048. The MLP then transforms this into $X'' \\in R^{N\\times d}$, where d = 128, as shown in Eq. (3) and Eq. (4).\n$X'_{1d} = SignalEmbedding(X^{1d})$\n$X'' = MLP(X'_{1d})$\n3) Hybrid Attention block: The hybrid attention module combines similarity and difference attention, as shown in Figure 4(c), with features normalized using a normalization layer [50], followed by a sequential Feed-Forward Network (FFN) to enhance representation, as shown in Figure 4(d). The Multi-scale hybrid module mixes cross-level learning relationships. The 2D features, obtained from upper-layer image features, are processed through three layers of 2D convolution and downsampling to capture multi-scale information. Similarly, 1D features are processed via three layers of 1D convolution and downsampling to obtain high-dimensional signal features. Both 2D and 1D features are concatenated with the output of the hybrid attention module to produce the final feature representation.\nThis design offers two advantages: it combines feature differences and similarities for multi-level multimodal feature extraction, and integrates cross-level convolutions to capture both structural and spatial information. This mitigates Transformer's limitations in capturing local relationships and patch-level structural information, promoting comprehensive feature representation learning. Next, we detail the gating mechanism to combine similarity attention and difference attention.\nHybrid Attention Module: As shown in Figure 4(c), the proposed hybrid attention model integrates two types of attention: similarity and difference attention, enhancing thereby multimodal feature representation. Similarity attention captures global patterns by focusing on the similarity between queries and keys, providing contextual information. Difference attention, by contrast, learns subtle variations between queries and keys, focusing on local changes. By combining the two, the model captures both global similarities and local differences, allowing for more precise attention distribution. To grant multimodal hybrid attention, feature maps X' and X'' are considered as non-overlapping patches and concatenated into X. Each patch is transformed into an embedded feature vector, as shown in Eq. (5):\n$X = Concat(X', X'')$\nthe feature map X is converted into a token sequence $\\hat{X} \\in R^{\\tilde{N} \\times d}$, where $\\tilde{N} = N + 1$ represents the number of patches. Subsequently, X is transformed through three linear layers, resulting in three matrices: Q, K, and V. The matrices Q, K, and V are the query, key, and value matrices, calculated as $Q = XW_Q, K = XW_K, and V = XW_V$.\nSimilarity attention weights: To perform similarity attention among N tokens, we use the dot product between the Q and K tokens to calculate the similarity attention weights as follows Eq. (6):\n$SAW(Q, K) = Softmax(\\frac{Q K^T}{\\sqrt{d}} + B)$\nwhere $B \\in R^{\\tilde{N}\\times\\tilde{N}}$ indicates the relative position bias, $Softmax(\\cdot)$ is applied to the rows of the similarity matrix $A = Q K^T$ with d providing normalization.\nDifference attention weights: Inspired by graph convolutional networks[51, 52, 53], where relationships between nodes are learned by calculating differences between input nodes, we propose, in this work, a feature difference attention"}, {"title": "Gating Mix", "content": "To aggregate the value matrix V using the attention weights for the updated feature representation, we combine the similarity attention weights and the discrepancy attention weights, as shown in Eq. (9):\n$HA = Mix(SAW(Q, K), DAW(Q, K))V$\nwhere $Mix(\\cdot)$ is a gating mixing operation. The gating mechanism learns gating weights, allowing the model to flexibly adjust the proportion of the two attention weights based on different input features. This dynamic adjustment helps capture the diversity and complexity of the input data. The $Mix(\\cdot)$ function, based on the inputs Q and K, is formulated by Eq. (10) and Eq. (11):\n$Mix(SAW, DAW) = G \\cdot SAW + (1 - G) \\cdot DAW$\n$G = \\sigma(W_g[SAW; DAW])$ \nwhere $\\sigma$ is the Sigmoid function, $W_g$ is a learnable weight matrix, $[SAW; DAW]$ denotes the concatenation of SAW and DAW, respectively. To capture enriched information, we concatenate the L individual attention heads to construct a multi-head attention, as shown in Eq. (12):\n$X' = Concat(HA_1, HA_2, ..., HA_L)W$\nwhere $HA_h = Mix(SAW_h, DAW_h)V_h$, and h indicates the head number.\nTo facilitate description, we pack all equations in the mix attention process into Eq. (13):\n$X' = HSDT(X)$\nFFN: The FFN is a two-layer feed-forward neural network applying non-linear transformations to enhance feature extraction, as shown in Eq. (14):\n$X'' = MLP(MLP(X'))$\nBased on Eq. (13) and Eq. (14), (as shown in Figure 4(b)), we restate them as Eq. (15) and Eq. (16) respectively:\n$Y = X + HSDT(LN(X))$,\n$X_{l, i+1} = Y + FFN(LN(Y)).$\nwhere l represents the number of stages, as shown in Figure 4(a), with l \u2208 (1,2,3,4), and i denotes the number of blocks.\n4) Mlti-scale hybrid block: Multi-scale feature fusion leverages information from different scales to extract richer features. Methods such as Feature Pyramid Networks (FPN) [54], BiFPN [55], YoloV3 [56], Inception [57], and PSPnet [58] achieve feature fusion by introducing hierarchical structures and fusion techniques. Transformer-based models, such as HVT [59], PVT [60], and MViT [61], have incorporated pyramid structures into ViT to improve performance. Recently, Qin et al. [62] proposed a Multi-Scale Vein Transformer (MSVT) to learn dependencies between patches at different scales, while also integrating convolutions to enhance robustness.\nHandwriting patterns in Alzheimer's patients often exhibit irregularities and fine-grained tremors, as shown in Figure 2. To model these tremors, we propose a multi-scale module that extracts features at different scales for each layer of the hybrid attention module. Capturing detailed features at various scales enhances the model's robustness and generalization."}, {"title": "2D Residual Feedforward Module", "content": "RFM2D is a residual block [63] where the traditional convolution learns a feature representation over a localized receptive field by the convolution kernels, with weights shared over the whole feature map. The intrinsic characteristics of a locality mechanism allows information exchange within a local region. Specifically, we first pool the feature map $X^{2d}_{la}$ obtained from the previous layer. Within the first multi-scale feature map fusion, $X^{2d}_{la}$ refers to the feature map obtained from Eq. (1). Fine-grained features are then extracted as shown in Eq. (17) and Eq. (18):\n$Y^{2d}_{la} = P(X^{2d}_{la})$\n$X^{2d}_{la+1} = Y^{2d}_{la} + Conv1\\times1(DWConv(Conv(Y^{2d}_{la})))$\nwhere $P(\\cdot)$ is a 2D convolution with a kernel size of 3 and a stride of 2. The separable convolution DWConv(\u00b7) extracts local information with minimal additional computational cost. Similar to classical residual networks, the residual connection enhances the gradient propagation capability across layers. Then, we flatten the feature map $X^{2d}_{la+1}$ obtained from Eq. (18), and use an MLP to extract high-dimensional features, as shown in Eq. (19):\n$Z' = MLP(X^{2d}_{la+1})$"}, {"title": "1D Residual Feedforward Module", "content": "We first pool the feature map $X^{1d}_{la}$ obtained from the previous layer. During the first multi-scale feature map fusion, $X^{1d}_{la}$ refers to the feature map obtained from Eq. (3) and then extract fine-grained features as shown in Eq. (20) and Eq. (21):\n$Y^{1d}_{la} = P(X^{1d}_{la})$\n$X^{1d}_{la+1} = Y^{1d}_{la} + Conv1d(DWConv1d(Conv1d(Y^{1d}_{la})))$\nwhere P(\u00b7) is an adaptive 1D max pooling layer. Then, we flatten the feature map $X^{1d}_{la+1}$ obtained from Eq. (21), and use an MLP to extract high-dimensional features (Eq. (22)).\n$Z'' = MLP(X^{1d}_{la+1})$\nBased on Equations (19) and (22), we concatenate Z' and Z'', and then concatenate the result with the output of the hybrid attention module. This concatenated result is used as input for the next stage of the hybrid attention module. To facilitate the description, we refer to the output of the hybrid attention module as Z''', as shown in Eq. (23):\n$\\tilde{Z} = Concat(Z',Z'', Z''')$\nwhere $\\tilde{Z} \\in R^{\\tilde{N}\\times(d+d')}$, $\\tilde{N} = N+1$, and d' is the vector length of Z' and Z''."}, {"title": "B. Template contrastive loss", "content": "The motivation for introducing the template contrastive loss is to enhance the model's ability to distinguish between positive and negative samples by explicitly learning from their differences. By incorporating both cross-entropy and contrastive losses, we aim to leverage the benefits of"}, {"title": "A. Experimental Setup", "content": "To assess our approach, we conducted extensive experiments on the DARWIN-RAW publicly available gold-standard dataset, collected using Wacom's Bamboo tablet from 174 participants. The x-y coordinate sequences of pen-tip movements were recorded at a frequency of 200 Hz. The dataset consists of x-y coordinates (174 subjects \u00d7 25 tasks \u00d7 1 x-y coordinate sequence, with some missing data). The x-y coordinates were then processed and augmented following the procedures described in Chapter 3. We compared our model's classification performance against various state-of-the-art classifiers, including CNN-2D(AD)[39], CNN-1D(AD)[37], VGG[64], ResNet[63], DenseNet[65], Inception-ResNetV2[66], Xception[67], and MobileNetV2[68]. For a fair comparison, we used pretrained models from the TIMM library. During training, we set the learning rate to 0.01 and the batch size to 16. The optimizer used was Stochastic Gradient Descent (SGD) with a momentum parameter of 0. 9 and a weight decay parameter of 0.05. Additionally, we employed cosine annealing as the learning rate scheduler and set the maximum number of training epochs to 100, with early stopping, halting the training when the accuracy did not improve for 10 consecutive epochs. All experiments were conducted using the PyTorch framework on a computer equipped with NVIDIATMGPUs."}, {"title": "B. Evaluation Metrics", "content": "We employed standard evaluation metrics, namely Accuracy, Precision, Recall (also known as Sensitivity), and F1-score, to assess our model classification performance. Let P denote the positive samples, the samples labeled with the target class (AD), and N denote the negative samples, labeled as HC. Accuracy is the most widely-used evaluation metric, representing the ratio of correctly predicted samples to the total number of samples. Precision is the ratio of correctly predicted positive samples to the total samples predicted as positive. Recall is the ratio of correctly predicted positive samples to all actual positive samples. The F1-score, the harmonic mean of Precision and Recall, is particularly useful for evaluating performance on imbalanced datasets."}, {"title": "C. Recognition Performance for HSDT", "content": "We evaluated the performance of existing methods across six subtask datasets, encompassing four task categories: memory and dictation (M), graphic (G), and copy (C). Due to the similarity among several tasks within the 25 subtasks, we selected a representative subset of these subtasks. As described in Section 5, 20% of the entire dataset was set aside as the test set. The remaining data were used for training and validation purposes, according to the stratified k-fold cross-validation technique, that maintains the percentage of samples for each class. Based on the experimental results of the hyperparameter optimization, k was set to 4, meaning the training set was divided into 4 parts: the first part used as the validation set, and the remaining 3 parts used as the training set. This process was repeated 4 times, utilizing the entire dataset for both training"}, {"title": "D. Ablation Study Results", "content": "We conducted two ablation studies on the six sub-datasets, one without the Multi-scale Hybrid Block (MSH), and the other without using the Template Contrastive Loss (CL). The experimental results are shown in Table III.\nThe results demonstrate the significant impact of both components on model's performance. Specifically, the removal of the Multi-scale Hybrid Block led to a notable decline in both accuracy and F1-score across all tasks. For instance, on Task 1, the F1-score dropped from 81.08% to 76.47%, and similar trends were observed in other tasks. This confirms that the Multi-scale Hybrid Block plays a crucial role in capturing multi-level features, which are essential for distinguishing subtle patterns in the data.\nThe Multi-scale Hybrid Block integrates multi-scale features from both 1D signal data and 2D images, enabling the model to capture local fine-grained details as well as global contextual patterns. This is particularly important for tasks involving complex data, such as handwriting signals, where both small variations in stroke patterns and overarching movement trends need to be considered. The block's ability to fuse features across different scales allows the model to better generalize across tasks, contributing to the model's robustness and enhanced classification performance.\nMoreover, by utilizing cross-level feature fusion, the Multi-scale Hybrid Block enables the model to learn more comprehensive representations, which enhances its ability to detect subtle distinctions between healthy controls and patients with AD. Without this component, the model's capacity to process both local and global information simultaneously is weakened, leading to lower accuracy and F1-scores, as observed in the ablation results.\nIn the second ablation study, the removal of the Template Contrastive Loss also resulted in a significant decrease in performance, particularly in precision and F1-score. For example, in Task 5, the precision dropped from 93.33% to 71.43%, and the F1-score decreased from 87.50% to 78. 95%. This highlights the critical role of the Template Contrastive Loss in enhancing feature discrimination.\nThe Template Contrastive Loss boosts the model's ability to learn more robust and discriminative representations by explicitly modeling the similarity relationships between samples in high-dimensional space. By enforcing a separation between positive and negative samples, it ensures that the learned features are more distinct, leading to better classification outcomes. This is particularly important in datasets with overlapping or ambiguous class boundaries, where the contrastive loss helps the model to better differentiate between the subtle patterns associated with AD and normal aging. Additionally, the dynamic template update mechanism within the Template Contrastive Loss allows the model to continuously refine its understanding of the feature space throughout the training process, improving adaptability and generalization. The ablation study clearly demonstrates that removing this component diminishes the model's ability to accurately classify challenging cases, as evidenced by the drop in precision and overall performance.\nIn conclusion, the ablation study results underscore the importance of both the Multi-scale Hybrid Block and Template Contrastive Loss. Together, these components enhance the model's ability to capture complex, multi-scale features and improve feature discrimination, leading to more accurate and robust classification across a range of tasks."}, {"title": "VI. CONCLUSION", "content": "In this study, we propose a novel HSDA-MS Transformer model for early detection of Alzheimer's Disease (AD). The model integrates both 2D handwriting images and 1D dynamic signal data, effectively capturing global and local feature variations. It demonstrates strong performance across multiple handwriting tasks by introducing a hybrid similarity and difference attention mechanism, a multi-scale hybrid block, and a template contrastive loss function, all validated through rigorous data processing and experimental evaluation.\nThe hybrid similarity and difference attention mechanism allows the model to capture both global patterns, such as stroke structure, and subtle local variations, crucial for detecting AD-related motor impairments. The similarity attention mechanism focuses on global handwriting patterns, while the difference attention mechanism refines the detection of fine-grained changes, improving the model's ability to process complex multimodal data.\nThe multi-scale hybrid block further enhances feature representation by incorporating information from multiple scales. By fusing features from different levels of both 2D and 1D modalities, the model captures fine local details and broad global patterns, resulting in improved classification performance. This multi-scale approach strengthens the model's ability to handle the complexities of handwriting tasks and adapt to varied input conditions.\nThe template contrastive loss function enhances the model's ability to discriminate between AD patients and healthy controls. By comparing positive and negative samples and learning their relationships in high-dimensional space, the loss function improves class separation, leading to more accurate classifications and better generalization to new data. This ensures the model can effectively distinguish early-stage AD from normal aging patterns.\nIn conclusion, the HSDA-MS Transformer model successfully integrates the hybrid similarity and difference attention mechanism, multi-scale hybrid block, and template contrastive loss function to achieve superior performance in early AD detection. Future work could explore applying this model to other neurodegenerative diseases and extending its use within multimodal deep learning frameworks, potentially integrating additional data types, such as EEG or speech analysis, for broader clinical applications. We will also investigate sound explainability techniques to uncover which patterns in the handwriting inputs are most predictive of AD [69]"}]}