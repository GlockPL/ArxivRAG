{"title": "Semantic Compositions Enhance Vision-Language Contrastive Learning", "authors": ["Maxwell Mbabilla Aladago", "Lorenzo Torresani", "Soroush Vosoughi"], "abstract": "In the field of vision-language contrastive learning, models such as CLIP capitalize on matched image-caption pairs as positive examples and leverage within-batch non-matching pairs as negatives. This approach has led to remarkable outcomes in zero-shot image classification, cross-modal retrieval, and linear evaluation tasks. We show that the zero-shot classification and retrieval capabilities of CLIP-like models can be improved significantly through the introduction of semantically composite examples during pretraining. Inspired by CutMix in vision categorization, we create semantically composite image-caption pairs by merging elements from two distinct instances in the dataset via a novel procedure. Our method fuses the captions and blends 50% of each image to form a new composite sample. This simple technique (termed CLIP-C for CLIP Compositions), devoid of any additional computational overhead or increase in model parameters, significantly improves zero-shot image classification and cross-modal retrieval. The benefits of CLIP-C are particularly pronounced in settings with relatively limited pretraining data.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in vision-language pretraining have propelled a multitude of tasks, including zero-shot image classification [22, 32, 41], video understanding [52, 59], and various multi-modal applications [27, 36, 54]. These successes echo the transformative trajectory initiated by large-scale pretraining efforts in Computer Vision (CV) [17, 26] and later in Natural Language Processing (NLP) [1, 11, 42, 43]. A prominent recent example in the vision-language interplay is the Contrastive Language-Image Pre-training (CLIP) model [41], which has become a benchmark for language-supervised training.\nThe objective of contrastive language-supervised pretraining is straightforward yet powerful: to align embeddings of corresponding image-text pairs in a shared embedding space while distancing the non-matching pairs [4, 5, 16]. CLIP has pioneered this direction with a dual-encoder framework, training on an expansive dataset of image-caption pairs sourced from the internet, using a bidirectional contrastive loss [37]."}, {"title": "2 Related Works", "content": "The use of language as an effective supervisory signal for learning visual representations has a rich history in machine learning [15, 24, 30, 40, 41]. Early influential works such as DeViSE [15] first learned semantic relations using unannotated textual data before mapping the images into that semantic space using class labels. More recently, models like CLIP [41], ALIGN [22], and others [20, 21, 32, 35] further improved the capabilities of joint vision-language embedding models by training on massive image-text paired datasets contrastively using the InfoNCE loss [37]. Our work aligns with these prior arts but focuses on incorporating semantic compositions during pretraining to improve data efficiency and enhance performance.\nBoth CLIP [41] and ALIGN [22] use huge datasets -400 million and 1B image-text pairs for CLIP and ALIGN, respectively. DeCLIP [32] improves the data efficiency of CLIP by incorporating several training objectives, including self-supervision within each modality [5, 11], nearest-neighbor supervision, and multi-view supervision [2]. SLIP [35], on the other hand, adds image self-supervision, SimCLR [4], to the language supervision. In [50], Wu et al. show good zero-shot results in the low data regime through soft image-text matches via optimal transport distillation. These methods, however, require multiple passes through the image encoder [32, 35] for each update or a first-in-first-out feature queue [32] to generate the representations for the extra objectives. Our method is free of these additional complexities.\nMost similar to our work are data augmentation methods such as CutMix [56] and MixUP [58] which have been very effective in training categorization models in computer vision. Our work brings the benefits of these established augmentation techniques in image understanding to the vision-language joint-embedding space. In addition to the incorporation of language, our method differs from CutMix by concatenating the image crops instead of pasting one crop on the other. Additionally, we train our models using contrastive loss. As our method is a pre-training mechanism, we do not discuss works [19, 23, 31, 55, 57] that use open-source CLIP checkpoints transfer learning scenarios."}, {"title": "3 Method", "content": "This section covers a background of the baseline method as well as the core components of CLIP-C's framework.\n3.1 Background\nContrastive Language-Image Pre-training (CLIP) from Radford et al. [41] has emerged as a highly successful approach for training vision-language models. CLIP is a dual encoder model with separate encoders $f_I$ and $f_T$ for extracting visual and textual features respectively. It also has two dedicated projection functions $g_I$ and $g_T$ that map the outputs of the encoders to a shared embedding space. Given a batch of B images and text pairs $\\{x_I^{(i)}, x_T^{(i)}\\}_{i=1}^B$ in each training step, CLIP computes the embeddings $z_I^{(i)} = g_I(f_I(x_I^{(i)}))$ and $z_T^{(i)} = g_T(f_T(x_T^{(i)}))$ where $z_I^{(i)} \\in \\mathbb{R}^d$ represents the normalized features of image $x_I^{(i)}$. $z_T^{(i)} \\in \\mathbb{R}^d$ denotes the normalized features of the corresponding caption $x_T^{(i)}$. The loss is evaluated using InfoNCE [37] whereby matching image-text pairs $\\{x_I^{(i)}, x_T^{(i)}\\}$ constitute the positive samples and non-matching pairs $\\{x_I^{(j)}, x_T^{(i)}\\} \\forall j \\neq i$ form the negative examples. A bidirectional loss is computed as\n$$L_{I2T} = - \\frac{1}{B} \\sum_{i=1}^B \\log \\frac{\\exp \\left(\\text{sim}\\left(z_I^{(i)}, z_T^{(i)}\\right)\\right)}{\\sum_{j=1}^B \\exp\\left(\\text{sim}\\left(z_I^{(i)}, z_T^{(j)}\\right)\\right)},$$ \n$$L_{T2I} = - \\frac{1}{B} \\sum_{i=1}^B \\log \\frac{\\exp \\left(\\text{sim}\\left(z_T^{(i)}, z_I^{(i)}\\right)\\right)}{\\sum_{k=1}^B \\exp\\left(\\text{sim}\\left(z_T^{(i)}, z_I^{(k)}\\right)\\right)},$$ \nwhere temperature $\\tau$ is typically a learnable parameter used to scale the logits. $\\tau$ is fixed in all of our ablation experiments as it has a noticeable impact on the model [29] which makes comparisons across different experiments difficult. $\\text{sim}(\\cdot,\\cdot)$ is a similarity function measuring the distance between the features. In CLIP [41] and our experiments, $\\text{sim}(\\cdot,\\cdot)$ is set as the dot product function. The total loss is an average of the two losses in Eq. (1) and Eq. (2):\n$$L = (L_{I2T} + L_{T2I})/2 .\n$$\n3.2 CLIP-C\nIn each training step, CLIP-C samples a batch of examples of of size B, $\\{x_I^{(i)}, x_T^{(i)}\\}_{i=1}^B$. Any given paired instance $(x_I^{(i)}, x_T^{(i)})$ is either the original example $(x_I^{(i)}, x_T^{(i)})$ or a composition of that example and another example $(x_I^{(i')}, x_T^{(i')}), i \\neq i'$, drawn from the dataset. Note that index i' is taken with respect to the dataset size and not the batch size B, i.e., sample i' may not be present in the current mini-batch. The proportion of composed samples in any mini-batch is controlled by a sampling rate hyper-parameter p. The impact of this parameter is discussed in Sec. 6.2.\nIn the case whereby $(x_I^{(i)}, x_T^{(i)})$ is a composite sample, the new caption $x_T^{(i')}$ is a concatenation of the two original captions involved: $x_T^{(i)} = [x_T^{(i)}, x_T^{(i')}]$ where [,] is a string concatenation function with the word \"and\" as a conjunction. The positions of the captions on either side of this conjunction change, with $x_T^{(i)}$ appearing first fifty percent of the time.\nThe new image is composed of the center half crops spanning either the height or the width of each image. For example, if the images have resolution (S\u00d7S), either $(\\frac{S}{2}\u00d7S)$ or $(S\u00d7\\frac{S}{2})$ center crops are taken from both images and concatenated as illustrated in Fig. 1. We experiment with other forms of image augmentation methods such as MixUP [58] and CutMix [56] in Tab. 8.\nAfter assembling the mini-batch as described above, CLIP-C proceeds to extract the image and text features as in CLIP: $z_I^{(i)} = g_I(f_I(x_I^{(i)}))$ and $z_T^{(i)} = g_T(f_T(x_T^{(i)}))$. With $z_I^{(i)}$ and $z_T^{(i)}$ computed, Eq. (1), Eq. (2), and Eq. (3) are used to compute the InfoNCE loss.\nThe sampling strategy CLIP-C employs exposes the model to a much higher diversity of images and their corresponding captions compared to the vanilla pre-training pipeline. As a result, we observe much more significant improvements in downstream transfer when the pretraining dataset is small. It is reasonably"}, {"title": "4 Experimental Setup", "content": "All our experiments use the CLIP framework due to its demonstrated effectiveness, simplicity, and widespread usage. We emphasize that we do not use pretrained CLIP checkpoints from prior works as our method is a pretraining mechanism. Thus, we retrain CLIP on our pretraining datasets and compare it to our approach. Finally, due to resource constraints, we conduct our experiments in the low data and small model regimes. Consequently, we are unable to compare with prior large-scale training systems.\nPretraining Datasets. We use three widely adopted web-crawled datasets of varying sizes and distributions for our experiments: Conceptual Captions [44], Conceptual 12M [3], and RedCaps [10]. These three datasets together enable us to assess the effectiveness of our method across pretraining datasets of different sizes and qualities.\nModels. We use Vision Transformer [12] models of various sizes as in [35]. The vision encoder is set to ViT-S/16 [45] in all our ablation experiments unless explicitly specified otherwise. We use ViT-B/16 [12, 45] as the image encoder to demonstrate the efficacy of our method at scale as we are unable to run much bigger models such as ViT-L/16 because of resource constraints. The text encoder in all our experiments is set to the 38M parameter text Transformer model from [41]. Following previous methods, Byte-Pair encoding is used for tokenization with a context length of 77 and a vocabulary size of 49k. Finally, we fixed the temperature parameter at 0.01, the maximum value used in CLIP [41].\nHyper-parameters. We train all our models using PyTorch [39] with a global batch size of 2,048 split across 8 GPUS in a single machine. AdamW [34] is the optimizer during pretraining. All models are pretrained for 40 epochs using a cosine decay learning rate schedule with a base rate of 0.003, a warm-up period of 5 epochs, and a final learning rate of 1e-5. The weight decay parameter is always set to 0.1. Random cropping is the only augmentation applied to the images during pretraining. We refer the reader to the Supplemental for more detailed information about these and other hyper-parameters.\nEvaluation. We perform zero-shot evaluation on several classification benchmarks using class names and prompts provided by [35, 41]. First, the embeddings for all classes in a given benchmark are computed with each class embedding being an ensemble of multiple prompt templates. The highest cosine similarity between the image embedding and the class embeddings is then used as the zero-shot prediction.\nWe test our model on eleven downstream datasets including ImageNet [9], CIFAR-10 [25], CIFAR-100 [25], Caltech-101 [14], Oxford Pets [38], Country211 [41], DTD [7], Sun397 [51], STL-10 [8], RESISC-45 [6], and EuroSAT [18]. Following previous works [13, 35], we use \u201cmean per class accuracy\u201d as the metric for Oxford"}, {"title": "5 Results", "content": "This section outlines our key comparisons between CLIP and CLIP-C (our method) on zero-shot image classification, cross-modal retrieval, and linear probing. However, we explain first why our method works.\n5.1 Why is CLIP-C an Effective Method?\nWhy will combining multiple different image-caption pairs into single instances during pretraining lead to improvements in downstream evaluations? In other words, why will CLIP-C work? To investigate this salient question, we examined the pretraining losses and cosine similarities of both the composite examples and plain examples as the model evolves. This fine-grained tracking of training mechanics provides insights into how the model handles plain simple examples versus composite examples, and whether there are any differences between the two groups.\nContrary to expectation that compound examples will be the more challenging to the model (since they are multiple examples condensed into single instances), we observed precisely the opposite: as shown in Fig. 2, the loss on the composite examples is lower than the loss on plain examples especially in the early stages. Our hypothesis for this empirical observation is that the model\nmore easily recognizes compound image-caption pairs because they tend to be structurally different from plain examples. The more interesting development arising from this phenomenon, however, is that the model is encouraged to dedicate more effort into learning the plain examples in CLIP-C compared to CLIP as seen in Fig. 3. We believe this elevated learning of plain examples together with the use of dynamic semantic compositions (See Sec. 6) all contribute to the superior capabilities of our method as discussed in the next sections.\n5.2 Zero-shot Image Classification\nWe conduct a thorough study of the transfer learning capabilities of our model in zero-shot image classification on many downstream benchmarks, including ImageNet [9] in Tab. 1. Across different pretraining datasets, our method substantially improves over CLIP. For ViT-S/16, CLIP-C achieves a 2% top-1 improvement over the baseline CLIP model on ImageNet while outperforming CLIP on 12 out of 12 downstream datasets when pretraining on CC3M. Furthermore, these enhancements are maintained when we scale the vision encoder from ViT-S/16 to ViT-B/16 showing the continued effectiveness of our method over CLIP in a bigger model. When pretraining on RedCaps and CC12M, the gains of CLIP-C over CLIP on ImageNet are respectively are 0.9% and 0.4%. These results are remarkable, considering that our approach and CLIP both use the same number of parameters, memory, and computational resources during pretraining. Even in the relatively data-rich settings of CC12M and RedCaps, CLIP-C\nstill improves over CLIP on 11 out of 12 benchmarks for RedCaps and 9 of the 12 benchmarks for CC12M.\n5.3 Zero-shot Cross-Modal Retrieval\nIn addition to the zero-shot transfer results as detailed in Sec. 5.2, we also provide analysis of the performance of CLIP-C versus CLIP on zero-shot cross-modal retrieval in Tab. 2. For these evaluations, we use MS-COCO [33] and Flickr30k [53] as the downstream benchmarks. As in the zero-shot transfer setting, CLIP-C yields significant improvements over the baseline model on both MS-COCO and Flickr30k across different pretraining datasets and model sizes. For example, when using CC3M as the pretraining dataset, our method outperforms CLIP by over 5% absolute top-1 retrieval accuracy in both image-to-text and text-to-image retrieval. The enhancement on MS-COCO is 4% on image-to-text and 3% on text-to-image retrievals. For both CLIP and our method, we noticed low retrieval results when pretraining on RedCaps, which we believe is related to the data distribution. We leave that analysis out for later works.\n5.4 Linear Probe Evaluations\nHaving verified the efficacy of our method using joint-embedding features in the zero-shot settings, we conduct several linear-probing evaluations in Tab. 3 to test the quality of the learned image features. In these experiments, a randomly initialized linear layer is added on top of the pretrained image encoder $f_I$ which is frozen. The text encoder $f_T$, along with linear projections $g_I$ and $g_T$ are"}, {"title": "6 Ablations", "content": "We ablate the various components of our framework including (1) providing more training resources to the CLIP model, (2) the sampling probability p, (3) semantic versus stylistic compositions, (4) the impact of stochasticity in drawing the second example, and (5) the composition function used for the images. We"}, {"title": "7 Conclusion", "content": "We have demonstrated in this study that fast and simple compositions of different image-caption pairs can significantly enhance the effectiveness of language-supervised visual representation learning models. This approach proves particularly beneficial when pretraining on smaller datasets. Our comprehensive analysis shows that CLIP-C, our proposed model, delivers substantial improvements in zero-shot learning tasks over the baseline CLIP model and performs robustly in linear evaluation settings. Our ablation studies provide crucial insights, emphasizing that the observed performance improvements stem not from a mere increase in data augmentation but from the strategic use of semantically distinct examples in compositions. We anticipate that these findings will encourage further exploration into novel and efficient uses of small-scale datasets for vision-language pretraining, especially in settings where it is difficult to curate massive amounts of paired data (e.g., medical and satellite images)."}]}