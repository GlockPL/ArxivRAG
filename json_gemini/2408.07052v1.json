{"title": "The News Comment Gap and Algorithmic Agenda Setting in Online Forums", "authors": ["Flora B\u00f6wing", "Patrick Gildersleve"], "abstract": "The disparity between news stories valued by journalists and those preferred by readers, known as the \"News Gap\", is well-documented. However, the difference in expectations regarding news related user-generated content is less studied. Comment sections, hosted by news websites, are popular venues for reader engagement, yet still subject to editorial decisions. It is thus important to understand journalist vs reader comment preferences and how these are served by various comment ranking algorithms that represent discussions differently. We analyse 1.2 million comments from Austrian newspaper Der Standard to understand the \"News Comment Gap\" and the effects of different ranking algorithms. We find that journalists prefer positive, timely, complex, direct responses, while readers favour comments similar to article content from elite authors. We introduce the versatile Feature-Oriented Ranking Utility Metric (FORUM) to assess the impact of different ranking algorithms and find dramatic differences in how they prioritise the display of comments by sentiment, topical relevance, lexical diversity, and readability. Journalists can exert substantial influence over the discourse through both curatorial and algorithmic means. Understanding these choices' implications is vital in fostering engaging and civil discussions while aligning with journalistic objectives, especially given the increasing legal scrutiny and societal importance of online discourse.", "sections": [{"title": "1. Introduction", "content": "Active public debate is vital for a stable democratic society as it fosters informed citizenry and accountability for those in power. In the digital age, much of this discourse occurs online. Online platforms allow broad participation but also face challenges like uncivil postings and algorithms that may reinforce existing beliefs. Hosting high-quality public debate online requires content moderation and well-designed algorithms to keep users engaged whilst avoiding strong filter bubble or echo chamber effects. Among such online platforms, news media user forums are a promising venue for public debate, acting as modern letters to the editor. They enable engagement between journalists and readers, providing diverse perspectives. However, news organisations have their interests that may differ from readers', influenced by competition and (declining) profit margins, affecting the curation and presentation of both news content and user-generated content (UGC). The difference between what journalists\ndeem worth publishing and what readers consider worth consuming, the so-called news\ngap, has been thoroughly researched. However, there is only limited work on the dif-\nference between what journalists expect from UGC, and what users expect from it. It\nis the intention of this paper to analyse this news comment gap. Firstly, we ask:\nRQ1 How do the preferences of readers and journalists differ in news comment forums?\n1.1 What are the characteristics of comments that are popular / unpopular\nwith readers?\n1.2 What are the characteristics of comments that are selected as Editors'\nPicks?\n1.3 How large is the gap between readers' and journalists' preferences for com-\nments?\nConsidering the size of the discussions and users' limited time and attention, com-\nment ranking policies are a crucial part of platform design. Comments can be ranked\nbased on metrics reflecting both journalists' and readers' preferences, influencing which\ncomments are prioritised. This control over comment display impacts the representa-\ntion of news stories, extending agenda-setting power. We must then ask:\nRQ2 What are the consequences of different comment ranking algorithms on perceived\ndiscussion quality?\n2.1 What is the effect of user-based ranking on perceived discussion quality?\n2.2 What is the effect of editors' preferences via pinned comments on perceived\ndiscussion quality?\n2.3 What is the effect of discussion structure, i.e. reply trees, on perceived\ndiscussion quality?\nThe dataset underpinning this paper has been gathered from one of the largest\nGerman-speaking news media online forums, hosted by Austrian newspaper Der Stan-\ndard. Textual and contextual features of the 1m+ comments are used to model pref-\nerences through \"Editors' Picks\" and user votes through regression and classification\nmodels. Furthermore, we introduce a novel approach to measure the performance of\ndifferent comment ranking algorithms, thereby showing which policies favour what\nkind of content."}, {"title": "2. Related work", "content": ""}, {"title": "2.1. Significance of online debate for democracy", "content": "Political discourse is a key pillar of civic engagement in a democracy. Journalism as the\n'fourth estate' has traditionally played a large role by informing and interacting with\nthe public (Tichenor, Donohue, & Olien, 1980). More recently, audience participation\nhas emerged in journalism, opening up the traditional 'one-to-many' approach to new\navenues for political discourse, also enabled by the internet (R. Rogers, 2009). Notably,\ncomment sections underneath articles on newspaper websites as the successors to the\n\"letters to the editor\" have gained popularity (Borton, 2013). Such a mindset of citizen\nparticipation may help counter recent tendencies of democratic backsliding (I. Mano-\nsevitch & Tenenboim, 2017) by fostering productive debate, but also has the potential\nto facilitate polarisation if platform design and moderation are poorly implemented.\nComment sections underneath online news articles first emerged in the early 2000s,"}, {"title": "2.2. Motivations behind news engagement", "content": "Why people read the news and which stories are interesting to them has been thor-\noughly researched. News values are the criteria that journalists and news organisations\nuse to determine the newsworthiness of an event or story, such as timeliness, proxim-\nity, impact, prominence, conflict, and shareability on social media (Harcup & O'Neill,\n2017). Despite the effort to understand readers' preferences, a gap remains between\nthe news supplied by journalists and what readers demand. Journalists often priori-\ntise public-affairs stories, while readers prefer sports, entertainment, and crime news\n(Boczkowski & Mitchelstein, 2013). According to Boczkowski and Mitchelstein, this\nso-called \"news gap\" is considerable in size: on average, stories considered by journal-\nists as most newsworthy contained 19 percentage points more public-affairs articles\nthan those news stories that received most reader views. Gina M. Masullo and Stroud\n(2022) found a similar gap in COVID-19 local news coverage, with readers favouring\ninformation on local businesses' pandemic responses and fact-checking over economic\nnews. Bright (2016) explored the \u201csocial news gap\u201d, noting that social sharing be-\nhaviour is driven more by social status than emotion, with different topics being more\nor less popular to share. Online news consumption and rising competition make it\neasier for readers to selectively engage with stories, exacerbating the news gap.\nMotivations for engaging with news through commenting differ compared to read-\ning and sharing, affecting the nature of any news gap. While many read news and\ncomment sections online, active commenters are a specific subset. For example, der-\nstandard.at online readers are roughly gender-balanced (45% female, 55% male), but\nonly 20% of active commenters are female (Krenn & Petrak, 2021). Emotional re-\nactions, especially negative ones like anger and anxiety, strongly predict commenting\n(Diakopoulos & Naaman, 2011; Naab, 2022; Ziegele, Weber, Quiring, & Breiner, 2018).\nPositive emotions have a more ambiguous effect, sometimes increasing willingness to"}, {"title": "2.3. Quality of comments", "content": "Within journalism research, \"quality\" can be viewed through various lenses, such as\nfrom a professional perspective (Diakopoulos, 2015) or a reader-centred perspective\n(Meijer & Bijleveld, 2020). For user comments, social signals like up- or down-voting\n(Cheng, Danescu-Niculescu-Mizil, & Leskovec, 2014) and commenting frequency (Di-\nakopoulos & Naaman, 2011) play an important role when assessing quality. Other di-\nmensions used for evaluating comments include readability, coherence, argument qual-\nity, criticality, emotionality, personal experience, relevance, and novelty (Diakopoulos,\n2015; Y. Wang & Diakopoulos, 2022).\nMany studies use \u201cwisdom of the crowds\" as ground truth, like user votes and\nauthor reputation for quality (Momeni, Cardie, & Diakopoulos, 2015; Tan, Niculae,\nDanescu-Niculescu-Mizil, & Lee, 2016; Wei, Liu, & Li, 2016). Some approaches use\nexternal assessors to judge quality independently (Momeni et al., 2015), though this\ncan introduce new biases. Regular text-based features alone are found to be ineffective\nfor distinguishing quality; social interaction and argumentation features show more\npromise (Momeni et al., 2015). Based on R. Y. Wang and Strong's (1996) data quality\nframework for data consumers and its application to news media (Tang, Ng, Strza-\nlkowski, & Kantor, 2003), we categorise the most common measurements of comment\nquality in Table 1.\nIntrinsic quality refers to features of a comment assessed independently of con-\ntext (i.e., other comments). The category mostly consists of textual features, such as\nsentiment scores and lexical diversity of the comment. Contextual quality considers\na comment's relation to the article and discussion, such as topical relevance to the\narticle or the discussion reply structure. Representational quality involves social sta-\ntus markers, like author reputation, and metrics like how often a comment directly\nreferences other users.\nThere is no universal metric for comment quality, but researchers have explored\nvarious aspects using qualitative and quantitative features. For news organisations,\nhosting UGC is risky, making it crucial to measure comment quality to manage their\ncontent effectively."}, {"title": "2.4. Institutional aspects of user generated content in news organisations", "content": "Over the past decades, news media platforms have invested in interactive elements\nto engage users. Qualitative research on UGC suggests it can enhance democratic\nparticipation and promote political efficacy (I. Manosevitch & Tenenboim, 2017)."}, {"title": "2.5. Information overload and ranking policies", "content": "The scale of online discussions encountered by users is considerably larger that that\noffline, leading to information overload. Users often lack the time to scan hundreds\nof comments, missing potentially interesting ones and sometimes ending their partic-\nipation altogether (Hogan, 2009; Lampe, Zube, Lee, Park, & Johnston, 2014; Levy,\n2008). On some platforms, users view only 3% of comments (Althuniya, Liu, Sirri-\nanni, & Adams, 2020). Automated assessment and ranking are increasingly important\n(Momeni et al., 2015). Ideally, users see a summary of the discussion through selected\ncomments, though this is challenging without biasing the results. Many news organisa-\ntions rank comments in reverse-chronological order by default, or allow sorting by user\nup/downvotes. Some organisations run an Editors' Picks section, where comments that\nhave been selected by professionals at the newspaper are made prominently visible to\nreaders.\nSince most users don't change the default sorting (Lampe et al., 2014), the default\npolicy choice has a significant impact. Comment sorting policies can improve com-\nment quality, boost user participation, and foster constructive discussion (Diakopou-\nlos & Naaman, 2011). For instance, Y. Wang and Diakopoulos (2022) found that New\nYork Times users whose comments were selected as Editors' Picks subsequently wrote\nhigher-quality comments. Exposure to thoughtful comments encourages users to write\nmore thoughtfully (Sukumaran, Vezich, McHugh, & Nass, 2011) and increases their\nwillingness to participate in discussions (Han & Brazeal, 2015).\nRanking policies based on the \"wisdom of the crowds\" (e.g., user votes) can cre-\nate social pressure, leading users to comment for upvotes rather than content quality\n(Heiss, Schmuck, & Matthes, 2019; Marcus et al., 2000). Disapproving responses can\ndiscourage even civil commenters, prompting them to withdraw (Naab, 2022). Naab\nsuggests that social sanctioning can be used to suppress certain voices, requiring atten-"}, {"title": "3. Data and Methodology", "content": ""}, {"title": "3.1. Data", "content": "Der Standard is one of the two most-read \"quality-newspapers\u201d (\u201cqualit\u00e4tszeitung\",\nroughly equivalent to a \u201cbroadsheet\") in Austria, which hosts one of the largest\nGerman-speaking online forums globally with more than 20 million comments written\nby 73,000 active users per year (Burger, 2022). The online version of the newspaper\ndoes not have a paywall. Readers can freely access all articles, read all user comments,\nand sort displayed comments according to time published and user votes. In order\nto write comments or interact through voting on comments, readers need to register\n(pseudonymously, if desired). The interface offers features to change the ranking policy\n(newest, oldest, most positive, and most negative) and to hide or show replies of each\nlevel individually. Pinned posts (or \u201cEditors' Picks\") are comments deemed especially\nvaluable by the respective editors, community management, or community guides\u00b9. They are shown at the top of the discussion and marked as pinned (\u201cAngeheftet\").\nThe dataset used in this paper was scraped using RSelenium (Harrison, Kim, &\nV\u00f6lkle, 2024) from derstandard.at. The dataset contains 5,874 articles published in\nSeptember and October 2022 and their comments (1.2 million). Summary statistics are\npresented in Table 2. There are on average 204.8 comments in each article discussion.\nThe discussion sizes is a long tailed distribution with a lot of articles having relatively\nfew comments (median = 79), and a small number of discussions being very large. The\naverage length of a comment in the dataset is 27.6 words or 184.3 characters. 31.6%\""}, {"title": "3.2. Feature Selection and Models", "content": ""}, {"title": "3.2.1. Features", "content": "We selected 19 comment-level features of interest based on metrics used in previous\nresearch (see Table 1 in the Related Work section). These are summarised in Table\n3. We selected these relatively off the shelf features for consistency and comparabil-\nity with prior work and to illustrate fundamental differences between journalists and\nreaders, as well as taking into account limited computational resources. We also add\narticle-level controls for the level of engagement with the article (number of comments,\nmean up/downvotes in discussion) and article genre variables to account for differing\nnatures of discussion by genre. Furthermore, in Section 3.3.2, a subset of these features\nare used to highlight how even simple ranking algorithms can significantly alter the\nkinds of comments prioritised for display to users.\nSome of the comments only contain a few words or a weblink, which leads to missing\nvalues in the cosine similarity score, the lexical diversity score and the readability score.\nIn the former two cases, we set the missing values to zero, as we assume that such\nshort comments are rather irrelevant to the topic and too short to be lexically diverse.\nFor the readability score, we also set the missing values to the minimum, as these\nare the shortest and simplest comments. To assist with model convergence / fit, and\nfor appropriate comparison between features, we log transform (x' = log(1 + x)) the\nlong tailed explanatory variables (Author follower count, Level in tree, Size of tree,\nHeight of tree, Hours since article, Num punctuation, Num replies, Num Upvotes, Num"}, {"title": "3.2.2. Regression Analysis", "content": "To address RQ1, we use a subset of the features above that only factor in informa-\ntion available at the time of comment posting. We first fit a number of regression\nmodels to discover associations between comment features and user votes (RQ1.1) /\nEditors' Picks (RQ1.2). We use negative binomial regressions for Upvotes and Down-\nvotes and a binary logistic regression model for Editors' Pick status (in this case,\nonly root comments are eligible to be selected, so we drop the 'is root comment' and\n\u2018comment level in tree' variables). We fit the models on 80% of the data (962,716\nrandomised comments) and test model fit using a test set of the remaining 20%. We\ncompare coefficients from the models to understand journalist and readers preferences.\nAfter answering RQ1, we build predictive models for upvotes and Editors' Picks using\nall of the features that we use as mock ranking algorithms for RQ2 (further details\nin Appendix A). These models take all available features, and produce a ranking of\ncomments based on predicted upvotes / Editors' Picks. We do this with negative bi-\nnomial and logistic regressions, as well as applying a more complex statistical learning\napproach with XGBoost to predict user votes / Editors' Picks."}, {"title": "3.2.3. XGBoost", "content": "We use the XGBoost library (eXtreme Gradient Boosting) (Chen & Guestrin, 2016)\nfor regression on upvotes and classification for Editors' Picks. We used successive\nrandom search and narrower grid search to tune hyperparameters for both models.\nFor regression, we chose root mean squared logarithmic error loss (for appropriate\ncomparison to the negative binomial regressions), and for classification we chose cross-\nentropy loss."}, {"title": "3.2.4. Measuring the comment gap", "content": "The size of the news comment gap between user votes and Editors' Pick preferences\n(RQ1.3) can be calculated through the Jaccard coefficient. This is done in line with\nthe approach of Juarez Miro (2022). We consider only the largest discussions where\ncomments are pinned (N = 1,511). For each discussion, we calculate the overlap of p\npinned Editors' Pick comments with the p most-liked comments in terms of upvotes\nand relative votes."}, {"title": "3.3. Evaluating Comment Ranking Algorithms", "content": ""}, {"title": "3.3.1. Constructing Ranking Algorithms", "content": "In order to answer RQ2.1-3, we first define different possible policies by which to\nrank comments. Each comment ranking algorithm, or \"sorting policy\", is made up of\nthree policy elements. The primary ordering, the Editors' Pick status, and the reply\nstructure. Primary ordering is the main way in which comments are ranked relative\nto each other (e.g. in order of upvotes, time posted, etc.). We implement 11 different\nprimary orderings. Editors' Pick status determines whether the comments selected as\nEditors' Picks are pinned to the top of the discussion or treated as regular comments to\nbe ranked. There are three options for reply structure: hiding replies, showing full reply\ntrees attached to their root comment, and showing replies \u201cloose\u201d-unattached to their\nroot comment. When full reply trees are shown, the root comments are sorted by the\nprimary ordering, but the reply trees retain sorting by comment time and discussion\nstructure. In total, there are 11 \u00d7 2 \u00d7 3 = 66 different combinations of policy elements\nthat may make up a ranking algorithm, summarised in Table 4."}, {"title": "3.3.2. Comparing Ranking Algorithms with FORUM", "content": "The subsequent analysis of different ranking algorithms is partly inspired by Young\net al. (2021), who compared the performance of sorting policies on a debating website\nwith regard to winning arguments in argumentation theory. For each discussion in\nthe dataset, we order the comments according to each ranking policy. We devise the\n\"Feature-Oriented Ranking Utility Metric\u201d (FORUM) that measures how well a\nranking policy p does at surfacing the best comments in a discussion according to\na feature f. The metric compares how the ordering of comments $o_p$ under policy p"}, {"title": "3.3.3. Modelling FORUM", "content": "Each of the 66 possible ranking policies is applied to every article comment section,\nwith FORUM score recorded for each feature after 10 comments ($\u03a6_{10}$) and over the\nfull discussion (\u03a6N ). The full discussion is used to assess overall performance of each\npolicy. The first 10 comments are used since regular users are more likely to only\nread the first comments displayed, and policy performance over these two different\nranges is not necessarily correlated. In practice, this measure over the top ranked\ncomments is likely a better indicator of the impact of different sorting policies on user\nexperience. We seek to evaluate how well different comment ranking policies return\ncomments by each feature over all of the article comment sections. For each feature\nin question, we perform a beta regression to evaluate the ranking performance of each\npolicy, measured by FORUM score, and how it depends on their constituent policy\nelements. We perform the necessary transformation on the outcome variable (\u03a6n ) from\nrange (-1,1) to (0,1) by taking \u03a6n' = (\u03a6n + 1)/2.\nFor the explanatory variables in each regression we create 10 dummies for each user-\nbased policy element (with random ordering as a baseline), 1 dummy for the editor-\nbased policy element to pin Editors' Picks (with no pinned comments as baseline), and\n2 dummies for structure based policy elements (with replies shown loose as baseline).\nWith these selections, expected FORUM score on any given feature for a random\nordering, no pinned comments, replies loose policy is 0, and the alternative policies\nare accordingly compared against this. Each coefficient then effectively measures the\nexpected change in ranking performance from baseline associated with employing each\npolicy element. In total, this results in 8 regressions, one for each of the 4 target\nfeatures and 2 discussion lengths to evaluate, all with 13 main explanatory variables.\nWe perform model selection (Appendix C) to decide which terms and interactions to\nuse. In the final model, these explanatory variables are evaluated as main effects, with\ninteraction terms between primary ordering and reply structure, and primary ordering\nand Editors' Pick status also evaluated. The final model terms with interactions are\nsummarised in Table 5."}, {"title": "3.3.4. Code, Data, and Ethics", "content": "The repository for this project is hosted at github.com/dornleiten/commentgap. Com-\nment data is not published to preserve user privacy. This work was approved by the\nuniversity ethical review procedures (ID 179954). The dataset is based exclusively on\npublic data and was scraped according to the terms & conditions of derstandard.at\n(Der Standard, 2024). The authors report there are no competing interests to declare."}, {"title": "4. Results", "content": ""}, {"title": "4.1. The comment gap (RQ1)", "content": "We use regression models to estimate the relationship between comment features and\nuser votes, as well as comment features and Editors' Picks. First, we examine the re-\nsults from logistic and negative binomial regression models for Editors' Picks, Upvotes,\nand Downvotes. All continuous variables are standardised, and log-transformed where\nnecessary, prior to model fitting as detailed in Section 3.2. Model summaries with their\ncoefficients are provided in Table 6. Since users are able to both Upvote and Down-\nvote comments, and that one would expect both of these factors to be associated with\nsimply the level of exposure/attention a comment receives, we also compute a score to\nassess the relative preferences of readers by the coefficients for upvoting/downvoting\na comment. For each feature, the Relative Voting Preference (RVP) is defined by\nRelative Voting Preference = $\u03b2_{Upvote} - \u03b2_{Downvote}$.\nTaking $e^{RVP}$ then describes the factor by which the count of Upvotes on a comment\nincreases relative to Downvotes when there is a one unit increase in the explanatory\nvariable. Similarly, for each feature we compute the Comment Gap (CG) to assess the\ndifference between journalist and reader preferences\nComment Gap = $\u03b2_{Editors' Pick} - (\u03b2_{Upvote} - \u03b2_{Downvote})$.\nTaking $e^{CG}$ then describes the factor by which the odds of a comment being selected as\nan Editors' Pick increase compared to the relative Upvotes vs Downvotes when there\nis a one unit increase in the explanatory variable. These are presented in Table 7. In\naddition to the model summary tables, we visualise the comment-level and article-level\n(standardised) coefficients for each model in Figures 4 and 5."}, {"title": "4.1.1. User votes", "content": "The following findings give an indication about which characteristics of comments\nare associated with user upvotes and downvotes (RQ1.1). We may take the negative\nbinomial regression coefficients from Table 6 and exponentiate them to get the rate\nratios that describe the factor by which Upvotes/Downvotes increase with a one unit\nincrease in the explanatory variable.\nMore instructive, however, are Relative Voting Preferences in Table 7. These offer\na better assessment for the relative preferences of readers. Exponentiating the RVPs\ndescribes the factor by which the count of Upvotes on a comment increases relative\nto Downvotes when there is a one unit increase in the explanatory variable. Here, we\nfind that readers prefer comments with higher Positive Sentiment and lower Negative\nSentiment. Increasing Lexical Diversity, Readability score, Number of Sentences de-\ncreases the Relative Voting Preference, indicating these factors more strongly increase\nthe number of Downvotes a comment receives. Readers thus prefer simpler, shorter\ncomments. Readers prefer comments with greater Topical Similarity to the Article\nand with more Punctuation Marks. Readers also prefer comments from those with\nhigher Author Follower Count, indicating a quality or reputational effect. Increased\nTime Since Article Publication decreases the rate ratio of a comment being upvoted\nless than it being downvoted, so readers relatively prefer later comments. On struc-\ntural factors, a comment being a Root Comment increases the rate ratio of it receiving"}, {"title": "4.1.2. Editors' Picks", "content": "We next turn to how different characteristics of comments are associated with being\nbeing pinned as \"Editors' Picks\" (RQ1.2). For journalists' comment preferences, we\nconsider the logistic regression coefficients in Table 6. An increase in a comment's\nPositive Sentiment is associated with increased odds of it being selected as an Editors'\nPick, whereas an increase in Negative Sentiment is associated with (weakly significant)\ndecreased odds of being selected. Journalists prefer more positive comments. Journal-\nists also prefer more Lexically Diverse, higher Readability score comments with higher\nNumber of Sentences, that use 2nd Person Pronouns from commenters with high Au-\nthor Follower Count. Comments with high Number of Punctuation Marks are less\nlikely to be selected as Editors' Picks (weakly significant), and there is no signifi-\ncant association between a comment's Topical Similarity to the article and whether\nit is selected as an Editors' Pick. For the article-level effects, firstly, the Number of\nComments in Discussion is negatively associated with a comment being selected as\nan Editors' Pick-the number of picks tends to be limited, no matter how long the\ndiscussion. Both Mean Upvotes and Downvotes in Discussion are associated with in-\ncreased odds of comments being selected as Editors' Picks discussions with higher\nlevels of engagement have more comments as Editors' Picks. On article genre, com-\npared to the baseline category (Domestic) with the exception of Law (no significant\ndifference), comments in articles of other genres have increased odds of being selected\nas an Editors' Pick. The strongest effects here being for the Science, Lifestyle, and\nWomen's Issues genres."}, {"title": "4.1.3. The comment gap", "content": "Here we address RQ1.3, on the gap between journalist and reader preferences. As\nindicated previously, journalists and readers share several preferences in comments.\nNamely, for Positive Sentiment comments (and against Negative Sentiment comments)\nfrom commenters from high Author Follower Count, with several shared genre pref-\nerences. We also seek to assess the relative preferences of journalists vs readers, using\nthe Comment Gap coefficients in Table 7 and Figure 5. This compares the Editors'\nPicks odds against the Relative Voting Preferences previously calculated. Relative to\nreaders, journalists have greater preference for Positive Comments and no significant\ndifference in preference for Negative Comments. Editors have stronger preference for\ncomments with higher Lexical Diversity, Readability score, Number of Sentences that\nuse 2nd Person Pronouns. In contrast, relative to journalists, readers prefer comments\nwith higher Topical Similarity to the Article, with more Punctuation Marks, from\ncommenters with Higher Author Follower Count, made a longer Time Since Article\nPublication. For the article-level features, the negative score for Number of Com-\nments in Discussion indicates that the number of Upvotes in a discussion scales more\nclosely with discussion length compared to Downvotes and Editors' Picks. The nega-\ntive(/positive) scores for Mean Upvotes(/Downvotes) trivially indicate that comments\nin discussions featuring more reader Upvotes(/Downvotes) are relatively preferred\nby readers(/journalists). Regarding article genre, compared to the baseline Domes-\ntic genre, the odds of a comment being selected as Editors' Pick increase compared\nto the Relative Voting Preference for most genres, with the exception of Opinion and\nLaw. Journalists strongest relatively preferred comments are on Science, Lifestyle,\nand Video articles, whereas Readers strongest relatively preferred comments are on\nOpinion, Domestic, and International articles.\nAdditionally, in line with Juarez Miro (2022), we estimate the size of the news\ncomment gap using the Jaccard similarity between top editor preferred comments\nand top reader preferred comments. The average Jaccard similarity between pinned\ncomments and the top user-voted comments across all article discussions is 59.5% for\nupvotes and 54.8% for relative votes (upvotes - downvotes). This indicates a news\ncomment gap of 40.5% (or 45.2%), in comparison to Juarez Miro's finding of 82.8%.\nIn fact, since we have not accounted for the likely positive causal relationship between\nupvotes and Editors' Picks (pinned comments are more likely to be seen by readers\nand upvoted, and/or highly upvoted comments are more likely to be seen and picked\nby editors), this indicates the comment gap for Der Standard is at least 40.5%."}, {"title": "4.2. Ranking policies (RQ2)", "content": "We plot the distributions of FORUM score for for all articles for the best, default,\nand worst ranking algorithms across the features of interest in Figure 6. Clearly, there\ncan be large differences in the kinds of comments prioritised by different ranking\nalgorithms. To understand how the elements of a ranking algorithm contribute to\nhow it (de-)prioritises certain kinds of comments, we interpret the coefficients from\nbeta regressions presented in figures 7, 8, and 9 to understand the consequences of\n(RQ2.1) Primary ordering or \"user-based\" ranking policy categories, (RQ2.2) editor-\nbased ranking policy categories, and (RQ2.3) structure-based ranking policy cate-\ngories. One can calculate the expected change on FORUM score from a coefficient\nwith $\u0394\u03a6 = \\frac{e^\u03b2 - 1}{e^\u03b2 + 1}$.  Full regression tables are shown in Appendix D."}, {"title": "4.2.1. Primary ordering", "content": "The 11 primary ordering categories are listed in Table 4. Considering lexical diversity,\nall the tested policies return more lexically diverse comments earlier than expected\nwith random ordering, with the exception of Chronological over both the first 10\ncomments and the full discussion and Reverse Downvotes over the full discussion.\nEarlier comments are less lexically diverse than comments posted more recently. The\nleast downvoted comments are more lexically diverse than other comments, but overall\ncomments with more downvotes are more lexically diverse. The strongest of all effects\nis shown by implementing ordering by Predicted Upvotes (NBR) ($\u03b2_{10}$ = 0.890) and\nPredicted Editors' Picks (XGB) (\u03b2N = 0.773), which increase \u03a6 from the baseline by\n0.418, 0.368 respectively.\nThe picture for sentiment is slightly more mixed, with weaker effects in different\ndirections. Most primary orderings across the first 10 comments and full discussion\ntend to slightly prioritise more negative sentiment comments than expected at random.\nHowever, Reverse Downvotes and Chronological prioritise more positive sentiment\ncomments, and Relative Votes very slightly prioritises more positive comments over\nthe full discussion. The strongest of all effects is shown by implementing ordering\nby Predicted Upvotes (NBR) (\u03b210 = -0.250) and Predicted Editors' Picks (XGB)\n(\u03b2N = \u22120.107), which decreases \u03a6 from the baseline by 0.124, 0.053 respectively.\nFor topical similarity, all policies return more relevant comments earlier than ex-\npected with random ordering, with the exception of Reverse Chronological over both\nthe first 10 comments and the full discussion and Reverse Downvotes (very weakly)\nover the full discussion. More recent comments are less topically similar to the article\nthan comments posted earlier. The least downvoted comments are more topically sim-\nilar to the article than other comments, but overall comments with fewer downvotes\nare less similar to the article. The strongest of all effects is shown by implementing\nordering by Predicted Upvotes (NBR) (\u03b210 = 0.617, \u03b2N = 0.719), which increases \u03a6\nfrom the baseline by 0.299, 0.345 respectively."}, {"title": "4.2.2. Editor-based ranking", "content": "The editor-based sorting policy element controls whether comments selected by jour-\nnalists as Editors' Picks are pinned to the top of the conversation, i.e. ranked ahead\nof all other comments. Pinning Editors' Picks to the top of a comment discussion has\nan insignificant to very weak effect on the sorted comments returned over the full\ndiscussion. This is unsurprising, given typically only 1-5 comments are pinned out of a\ndiscussion of several hundred comments. The effect is more notable, however, for lexi-\ncal diversity, readability score, and topical similarity when only"}]}