{"title": "The News Comment Gap and Algorithmic Agenda Setting in Online Forums", "authors": ["Flora B\u00f6wing", "Patrick Gildersleve"], "abstract": "The disparity between news stories valued by journalists and those preferred by readers, known as the \"News Gap\", is well-documented. However, the difference in expectations regarding news related user-generated content is less studied. Comment sections, hosted by news websites, are popular venues for reader engagement, yet still subject to editorial decisions. It is thus important to understand journalist vs reader comment preferences and how these are served by various comment ranking algorithms that represent discussions differently. We analyse 1.2 million comments from Austrian newspaper Der Standard to understand the \"News Comment Gap\" and the effects of different ranking algorithms. We find that journalists prefer positive, timely, complex, direct responses, while readers favour comments similar to article content from elite authors. We introduce the versatile Feature-Oriented Ranking Utility Metric (FORUM) to assess the impact of different ranking algorithms and find dramatic differences in how they prioritise the display of comments by sentiment, topical relevance, lexical diversity, and readability. Journalists can exert substantial influence over the discourse through both curatorial and algorithmic means. Understanding these choices' implications is vital in fostering engaging and civil discussions while aligning with journalistic objectives, especially given the increasing legal scrutiny and societal importance of online discourse.", "sections": [{"title": "1. Introduction", "content": "Active public debate is vital for a stable democratic society as it fosters informed citizenry and accountability for those in power. In the digital age, much of this discourse occurs online. Online platforms allow broad participation but also face challenges like uncivil postings and algorithms that may reinforce existing beliefs. Hosting high-quality public debate online requires content moderation and well-designed algorithms to keep users engaged whilst avoiding strong filter bubble or echo chamber effects. Among such online platforms, news media user forums are a promising venue for public debate, acting as modern letters to the editor. They enable engagement between journalists and readers, providing diverse perspectives. However, news organisations have their interests that may differ from readers', influenced by competition and (declining) profit margins, affecting the curation and presentation of both news content and user-generated content (UGC). The difference between what journalists deem worth publishing and what readers consider worth consuming, the so-called news gap, has been thoroughly researched. However, there is only limited work on the difference between what journalists expect from UGC, and what users expect from it. It is the intention of this paper to analyse this news comment gap. Firstly, we ask:\nRQ1 How do the preferences of readers and journalists differ in news comment forums?\n1.1 What are the characteristics of comments that are popular / unpopular with readers?\n1.2 What are the characteristics of comments that are selected as Editors' Picks?\n1.3 How large is the gap between readers' and journalists' preferences for comments?\nConsidering the size of the discussions and users' limited time and attention, comment ranking policies are a crucial part of platform design. Comments can be ranked based on metrics reflecting both journalists' and readers' preferences, influencing which comments are prioritised. This control over comment display impacts the representation of news stories, extending agenda-setting power. We must then ask:\nRQ2 What are the consequences of different comment ranking algorithms on perceived discussion quality?\n2.1 What is the effect of user-based ranking on perceived discussion quality?\n2.2 What is the effect of editors' preferences via pinned comments on perceived discussion quality?\n2.3 What is the effect of discussion structure, i.e. reply trees, on perceived discussion quality?\nThe dataset underpinning this paper has been gathered from one of the largest German-speaking news media online forums, hosted by Austrian newspaper Der Standard. Textual and contextual features of the 1m+ comments are used to model preferences through \"Editors' Picks\" and user votes through regression and classification models. Furthermore, we introduce a novel approach to measure the performance of different comment ranking algorithms, thereby showing which policies favour what kind of content."}, {"title": "2. Related work", "content": ""}, {"title": "2.1. Significance of online debate for democracy", "content": "Political discourse is a key pillar of civic engagement in a democracy. Journalism as the 'fourth estate' has traditionally played a large role by informing and interacting with the public (Tichenor, Donohue, & Olien, 1980). More recently, audience participation has emerged in journalism, opening up the traditional 'one-to-many' approach to new avenues for political discourse, also enabled by the internet (R. Rogers, 2009). Notably, comment sections underneath articles on newspaper websites as the successors to the \"letters to the editor\" have gained popularity (Borton, 2013). Such a mindset of citizen participation may help counter recent tendencies of democratic backsliding (I. Manosevitch & Tenenboim, 2017) by fostering productive debate, but also has the potential to facilitate polarisation if platform design and moderation are poorly implemented. Comment sections underneath online news articles first emerged in the early 2000s,"}, {"title": "2.2. Motivations behind news engagement", "content": "Why people read the news and which stories are interesting to them has been thoroughly researched. News values are the criteria that journalists and news organisations use to determine the newsworthiness of an event or story, such as timeliness, proximity, impact, prominence, conflict, and shareability on social media (Harcup & O'Neill, 2017). Despite the effort to understand readers' preferences, a gap remains between the news supplied by journalists and what readers demand. Journalists often prioritise public-affairs stories, while readers prefer sports, entertainment, and crime news (Boczkowski & Mitchelstein, 2013). According to Boczkowski and Mitchelstein, this so-called \"news gap\" is considerable in size: on average, stories considered by journalists as most newsworthy contained 19 percentage points more public-affairs articles than those news stories that received most reader views. Gina M. Masullo and Stroud (2022) found a similar gap in COVID-19 local news coverage, with readers favouring information on local businesses' pandemic responses and fact-checking over economic news. Bright (2016) explored the \u201csocial news gap\u201d, noting that social sharing behaviour is driven more by social status than emotion, with different topics being more or less popular to share. Online news consumption and rising competition make it easier for readers to selectively engage with stories, exacerbating the news gap.\nMotivations for engaging with news through commenting differ compared to reading and sharing, affecting the nature of any news gap. While many read news and comment sections online, active commenters are a specific subset. For example, der-standard.at online readers are roughly gender-balanced (45% female, 55% male), but only 20% of active commenters are female (Krenn & Petrak, 2021). Emotional reactions, especially negative ones like anger and anxiety, strongly predict commenting (Diakopoulos & Naaman, 2011; Naab, 2022; Ziegele, Weber, Quiring, & Breiner, 2018). Positive emotions have a more ambiguous effect, sometimes increasing willingness to comment and sometimes decreasing it (Berger & Milkman, 2012; Marcus, Neuman, & MacKuen, 2000; Orehek, Bessarabova, Chen, & Kruglanski, 2011). The news factors of 'controversy' and 'damage' in articles also increase the likelihood of discussing and commenting on news (Boczkowski & Mitchelstein, 2012; Harber & Cohen, 2005; E. M. Rogers, 2000; Weber, 2014; Ziegele & Quiring, 2013). News values and discussion factors both influence the quantity and quality of comments (Ziegele et al., 2018).\nControversial political and social issues generate more comments due to their emotional impact, making comment sections engaging but prone to incivility and low-quality arguments. Enhancing debate quality remains challenging due to diverse stakeholder interests. Comment ranking policies can be a crucial lever for news organisations aiming to improve the nature of discussions in their comment sections."}, {"title": "2.3. Quality of comments", "content": "Within journalism research, \"quality\" can be viewed through various lenses, such as from a professional perspective (Diakopoulos, 2015) or a reader-centred perspective (Meijer & Bijleveld, 2020). For user comments, social signals like up- or down-voting (Cheng, Danescu-Niculescu-Mizil, & Leskovec, 2014) and commenting frequency (Diakopoulos & Naaman, 2011) play an important role when assessing quality. Other dimensions used for evaluating comments include readability, coherence, argument quality, criticality, emotionality, personal experience, relevance, and novelty (Diakopoulos, 2015; Y. Wang & Diakopoulos, 2022).\nMany studies use \u201cwisdom of the crowds\" as ground truth, like user votes and author reputation for quality (Momeni, Cardie, & Diakopoulos, 2015; Tan, Niculae, Danescu-Niculescu-Mizil, & Lee, 2016; Wei, Liu, & Li, 2016). Some approaches use external assessors to judge quality independently (Momeni et al., 2015), though this can introduce new biases. Regular text-based features alone are found to be ineffective for distinguishing quality; social interaction and argumentation features show more promise (Momeni et al., 2015). Based on R. Y. Wang and Strong's (1996) data quality framework for data consumers and its application to news media (Tang, Ng, Strzalkowski, & Kantor, 2003), we categorise the most common measurements of comment quality in Table 1.\nIntrinsic quality refers to features of a comment assessed independently of context (i.e., other comments). The category mostly consists of textual features, such as sentiment scores and lexical diversity of the comment. Contextual quality considers a comment's relation to the article and discussion, such as topical relevance to the article or the discussion reply structure. Representational quality involves social status markers, like author reputation, and metrics like how often a comment directly references other users.\nThere is no universal metric for comment quality, but researchers have explored various aspects using qualitative and quantitative features. For news organisations, hosting UGC is risky, making it crucial to measure comment quality to manage their content effectively."}, {"title": "2.4. Institutional aspects of user generated content in news organisations", "content": "Over the past decades, news media platforms have invested in interactive elements to engage users. Qualitative research on UGC suggests it can enhance democratic participation and promote political efficacy (I. Manosevitch & Tenenboim, 2017)."}, {"title": "2.5. Information overload and ranking policies", "content": "The scale of online discussions encountered by users is considerably larger that that offline, leading to information overload. Users often lack the time to scan hundreds of comments, missing potentially interesting ones and sometimes ending their participation altogether (Hogan, 2009; Lampe, Zube, Lee, Park, & Johnston, 2014; Levy, 2008). On some platforms, users view only 3% of comments (Althuniya, Liu, Sirrianni, & Adams, 2020). Automated assessment and ranking are increasingly important (Momeni et al., 2015). Ideally, users see a summary of the discussion through selected comments, though this is challenging without biasing the results. Many news organisations rank comments in reverse-chronological order by default, or allow sorting by user up/downvotes. Some organisations run an Editors' Picks section, where comments that have been selected by professionals at the newspaper are made prominently visible to readers.\nSince most users don't change the default sorting (Lampe et al., 2014), the default policy choice has a significant impact. Comment sorting policies can improve comment quality, boost user participation, and foster constructive discussion (Diakopoulos & Naaman, 2011). For instance, Y. Wang and Diakopoulos (2022) found that New York Times users whose comments were selected as Editors' Picks subsequently wrote higher-quality comments. Exposure to thoughtful comments encourages users to write more thoughtfully (Sukumaran, Vezich, McHugh, & Nass, 2011) and increases their willingness to participate in discussions (Han & Brazeal, 2015).\nRanking policies based on the \"wisdom of the crowds\" (e.g., user votes) can create social pressure, leading users to comment for upvotes rather than content quality (Heiss, Schmuck, & Matthes, 2019; Marcus et al., 2000). Disapproving responses can discourage even civil commenters, prompting them to withdraw (Naab, 2022). Naab suggests that social sanctioning can be used to suppress certain voices, requiring attention to prevent this (Ziegele, Naab, & Jost, 2020). Social feedback can heavily influence discussions, potentially sanctioning opinions that diverge from the majority, and political leanings of news organisations may amplify this pressure.\nIn online forums, comment ranking influences which comments users read as well as write. Editor-picked comments provide a good debate overview but may reflect undisclosed editorial policies. Community-based ranking can sanction incivility but may discourage minority opinions. This creates tension between audience participation and journalistic gatekeeping (Diakopoulos, 2015; Masullo Chen & Pain, 2017). Users' and editors' judgements often differ; only 17.2% of New York Times comments made both the readers' and Editors' Picks lists. Readers favoured direct, confrontational, and aligned comments, while editors chose more articulate, conciliatory, and diverse ones (Juarez Miro, 2022).\nBuilding on Juarez Miro's (2022) work, we examine the \u201cnews comment gap\" by using computational methods to analyse the characteristics that correlate with reader votes and Editors' Picks. We compare the quality of comments and discussions under different ranking policies, and analyse the impact of editorial features like Editors' Picks and structural features like reply-trees. Existing research about news media user comments has predominantly focused on English-speaking platforms, as well as specifically on hate-speech (Reimer, H\u00e4ring, Loosen, Maalej, & Merten, 2023). This study explores comment quality and ranking algorithms in the largest German-speaking news forum, derstandard.at."}, {"title": "3. Data and Methodology", "content": ""}, {"title": "3.1. Data", "content": "Der Standard is one of the two most-read \"quality-newspapers\u201d (\u201cqualit\u00e4tszeitung\", roughly equivalent to a \u201cbroadsheet\") in Austria, which hosts one of the largest German-speaking online forums globally with more than 20 million comments written by 73,000 active users per year (Burger, 2022). The online version of the newspaper does not have a paywall. Readers can freely access all articles, read all user comments, and sort displayed comments according to time published and user votes. In order to write comments or interact through voting on comments, readers need to register (pseudonymously, if desired). The interface offers features to change the ranking policy (newest, oldest, most positive, and most negative) and to hide or show replies of each level individually. Pinned posts (or \u201cEditors' Picks\") are comments deemed especially valuable by the respective editors, community management, or community guides\u00b9. They are shown at the top of the discussion and marked as pinned (\u201cAngeheftet\").\nThe dataset used in this paper was scraped using RSelenium (Harrison, Kim, & V\u00f6lkle, 2024) from derstandard.at. The dataset contains 5,874 articles published in September and October 2022 and their comments (1.2 million). Summary statistics are presented in Table 2. There are on average 204.8 comments in each article discussion. The discussion sizes is a long tailed distribution with a lot of articles having relatively few comments (median = 79), and a small number of discussions being very large. The average length of a comment in the dataset is 27.6 words or 184.3 characters. 31.6%"}, {"title": "3.2. Feature Selection and Models", "content": ""}, {"title": "3.2.1. Features", "content": "We selected 19 comment-level features of interest based on metrics used in previous research (see Table 1 in the Related Work section). These are summarised in Table 3. We selected these relatively off the shelf features for consistency and comparability with prior work and to illustrate fundamental differences between journalists and readers, as well as taking into account limited computational resources. We also add article-level controls for the level of engagement with the article (number of comments, mean up/downvotes in discussion) and article genre variables to account for differing natures of discussion by genre. Furthermore, in Section 3.3.2, a subset of these features are used to highlight how even simple ranking algorithms can significantly alter the kinds of comments prioritised for display to users.\nSome of the comments only contain a few words or a weblink, which leads to missing values in the cosine similarity score, the lexical diversity score and the readability score. In the former two cases, we set the missing values to zero, as we assume that such short comments are rather irrelevant to the topic and too short to be lexically diverse. For the readability score, we also set the missing values to the minimum, as these are the shortest and simplest comments. To assist with model convergence / fit, and for appropriate comparison between features, we log transform ($x' = log(1 + x)$) the long tailed explanatory variables (Author follower count, Level in tree, Size of tree, Height of tree, Hours since article, Num punctuation, Num replies, Num Upvotes, Num"}, {"title": "3.2.2. Regression Analysis", "content": "To address RQ1, we use a subset of the features above that only factor in information available at the time of comment posting. We first fit a number of regression models to discover associations between comment features and user votes (RQ1.1) / Editors' Picks (RQ1.2). We use negative binomial regressions for Upvotes and Downvotes and a binary logistic regression model for Editors' Pick status (in this case, only root comments are eligible to be selected, so we drop the 'is root comment' and \u2018comment level in tree' variables). We fit the models on 80% of the data (962,716 randomised comments) and test model fit using a test set of the remaining 20%. We compare coefficients from the models to understand journalist and readers preferences. After answering RQ1, we build predictive models for upvotes and Editors' Picks using all of the features that we use as mock ranking algorithms for RQ2 (further details in Appendix A). These models take all available features, and produce a ranking of comments based on predicted upvotes / Editors' Picks. We do this with negative binomial and logistic regressions, as well as applying a more complex statistical learning approach with XGBoost to predict user votes / Editors' Picks."}, {"title": "3.2.3. XGBoost", "content": "We use the XGBoost library (eXtreme Gradient Boosting) (Chen & Guestrin, 2016) for regression on upvotes and classification for Editors' Picks. We used successive random search and narrower grid search to tune hyperparameters for both models. For regression, we chose root mean squared logarithmic error loss (for appropriate comparison to the negative binomial regressions), and for classification we chose cross-entropy loss."}, {"title": "3.2.4. Measuring the comment gap", "content": "The size of the news comment gap between user votes and Editors' Pick preferences (RQ1.3) can be calculated through the Jaccard coefficient. This is done in line with the approach of Juarez Miro (2022). We consider only the largest discussions where comments are pinned (N = 1,511). For each discussion, we calculate the overlap of p pinned Editors' Pick comments with the p most-liked comments in terms of upvotes and relative votes."}, {"title": "3.3. Evaluating Comment Ranking Algorithms", "content": ""}, {"title": "3.3.1. Constructing Ranking Algorithms", "content": "In order to answer RQ2.1-3, we first define different possible policies by which to rank comments. Each comment ranking algorithm, or \"sorting policy\", is made up of three policy elements. The primary ordering, the Editors' Pick status, and the reply structure. Primary ordering is the main way in which comments are ranked relative to each other (e.g. in order of upvotes, time posted, etc.). We implement 11 different primary orderings. Editors' Pick status determines whether the comments selected as Editors' Picks are pinned to the top of the discussion or treated as regular comments to be ranked. There are three options for reply structure: hiding replies, showing full reply trees attached to their root comment, and showing replies \u201cloose\u201d-unattached to their root comment. When full reply trees are shown, the root comments are sorted by the primary ordering, but the reply trees retain sorting by comment time and discussion structure. In total, there are 11 \u00d7 2 \u00d7 3 = 66 different combinations of policy elements that may make up a ranking algorithm, summarised in Table 4."}, {"title": "3.3.2. Comparing Ranking Algorithms with FORUM", "content": "The subsequent analysis of different ranking algorithms is partly inspired by Young et al. (2021), who compared the performance of sorting policies on a debating website with regard to winning arguments in argumentation theory. For each discussion in the dataset, we order the comments according to each ranking policy. We devise the \"Feature-Oriented Ranking Utility Metric\u201d (FORUM) that measures how well a ranking policy p does at surfacing the best comments in a discussion according to a feature f. The metric compares how the ordering of comments $o_p$ under policy p compares to the best-possible ordering ($o_b$ - descending order of score $f$), worst-possible ordering ($o_w$ - ascending order of score $f$), and the expected random baseline. We can use FORUM to evaluate how well a full discussion of N comments is ordered, or the first $n < N$ comments returned. Here we describe how FORUM is calculated for a ranking policy p with feature f on a particular discussion on a news article.\nIf a user is shown i comments from a discussion, they will be exposed to $t_i = \\Sigma_{j=1}^{i} s_f(o_j)$ cumulative feature score, where $o_j$ is the comment at position j in ordering o, and $s_f(o_j)$ is the feature score of said comment. The expected random baseline cumulative score is defined by $t_i^r = iT_f/N$, where N is the total number of comments and $T_f$ is the total score of the N comments. We measure the difference in cumulative scores to the random baseline for the best-possible ordering, worst-possible ordering, and policy p: $\\Delta_i^b = t_i^b - t_i^r$, $\\Delta_i^w = t_i^w - t_i^r$, and $\\Delta_i^p = t_i^p - t_i^r$ (note that $\\Delta_i^b, \\Delta_i^w > 0$). A graphical interpretation of this is shown in Figure 2. If a policy tends to return higher scored comments earlier than expected at random, then the line tracking cumulative score will fall above the diagonal baseline. If the policy tends to return higher scoring comments later than expected at random, then it will fall below the baseline. Then, to compare performance relative to the best/worst possible ordering in showing i comments, the \"normalised policy delta\" is:\n$\\Gamma_i^p = \\frac{\\Delta_i^p}{(\\Delta_i^b-\\Delta_i^w) \\cdot (1-H(\\Delta_i^w))}$  (1)\nwhere $H(\\Delta_i^w)$ is the Heaviside step function (= 1 for $\\Delta_i^w > 0$, = 0 for $\\Delta_i^w < 0$). This allows for differential normalisation when $\\Delta_i^w$ is positive or negative. Policies that return many of the highest scoring comments in the first i comments achieve $\\Gamma_i^p \u2248 1$, those that return many of the lowest scoring comments in the first i comments achieve $\\Gamma_i^p \u2248 -1$, and those no better/worse than random achieve $\\Gamma_i^p \u2248 0$.\nThe normalised policy delta measure, $\\Gamma_i^p$, can be used to measure the total quality of the first i comments returned, as compared to best/worst/random. However, it does not measure how well the i comments themselves returned are ranked. To measure how well a policy p ranks up to n comments, we must take the average normalised policy delta for all i up to n:\n$\\Phi^p_n = \\frac{1}{n} \\sum_{i=1}^n \\Gamma_i^p  =  \\frac{1}{n} \\sum_{i=1}^n  \\frac{\\Delta_i^p}{(\\Delta_i^b-\\Delta_i^w) \\cdot (1-H(\\Delta_i^w))} $ (2)\n$\\Phi^p_n$ is then our Feature-Oriented Ranking Utility Metric (FORUM). Here, the ordering of the n comments returned is considered, since we average normalised policy delta for the first 1, 2, 3, ..., n comments. Even if two policies return the same n comments (same $\\Gamma_i^p$), the one that returns higher scoring comments earlier will amass cumulative comment score, $t_i^p$, earlier, and will have higher $\\Delta_i^p$ for $i < n$. Thus, the FORUM score for n comments, $\\Phi^p_n$, will be larger. The full process for three example policies is shown in Figure 3.\nNote that $\\Phi_N^p$ is undefined due to division by zero. Intuitively, there is only one way to return a set of N out of N total comments (i.e., all of them), so a relative performance score does not make sense. Relatedly, this means to score a ranking policy over a full discussion of N comments we take $\\Phi_{N-1}^p$ - scoring a policy for how it ranks a full discussion is equivalent to scoring it for N \u2013 1 comments, since there is only one position for the final comment to go.\nIn cases where a ranking policy removes comments from immediate view, such as the \"hide replies\" functionality on Der Standard, we choose to interpolate a straight line from the cumulative feature score of the final visible comment up to the total cumulative feature score $T_f$ in the process of calculating $\\Gamma_i^p$. This is done to enable comparison between ranking policies over a different number and set of comments. Practically, this is akin to a situation where any one of the hidden comments is equally likely to be viewed next by a reader (if they so choose to click one or more to expand).\nThe FORUM score is bounded -1 << 1 and readily interpretable. Policies close to best-possible ordering achieve \u2248 1, those close to worst-possible ordering achieve \u2248 -1, and those no better/worse than random achieve \u2248 0. The measure allows one to compare policies to one another for fixed n (e.g. how well do policies pa, p\u00df do at returning the first 10 comments in a discussion) and differing n (e.g. how well do policies pa, p\u00df do at ranking the first 10, 20, N comments). The measure is also normalised such that one can compare policy performance over different comment sections, even if average feature scores between them differ say the topic of one comment section lends itself to comments with higher readability score, compared to another (e.g. how well does policy pa do at ranking the first n comments in discussions 1 and 2, or how well does it do at ranking the full discussions of length N1, N2).\nDespite these normalisations, the scores/weights of each comment in a discussion are still factored in when evaluating policy performance, beyond just considering the"}, {"title": "3.3.3. Modelling FORUM", "content": "Each of the 66 possible ranking policies is applied to every article comment section, with FORUM score recorded for each feature after 10 comments ($\u03a6_{10}$) and over the full discussion ($\u03a6_N$). The full discussion is used to assess overall performance of each policy. The first 10 comments are used since regular users are more likely to only read the first comments displayed, and policy performance over these two different ranges is not necessarily correlated. In practice, this measure over the top ranked comments is likely a better indicator of the impact of different sorting policies on user experience. We seek to evaluate how well different comment ranking policies return comments by each feature over all of the article comment sections. For each feature in question, we perform a beta regression to evaluate the ranking performance of each policy, measured by FORUM score, and how it depends on their constituent policy elements. We perform the necessary transformation on the outcome variable ($\u03a6_n$) from range (-1,1) to (0,1) by taking $\u03a6_n' = (\u03a6_n + 1)/2$.\nFor the explanatory variables in each regression we create 10 dummies for each user-based policy element (with random ordering as a baseline), 1 dummy for the editor-based policy element to pin Editors' Picks (with no pinned comments as baseline), and 2 dummies for structure based policy elements (with replies shown loose as baseline). With these selections, expected FORUM score on any given feature for a random ordering, no pinned comments, replies loose policy is 0, and the alternative policies are accordingly compared against this. Each coefficient then effectively measures the expected change in ranking performance from baseline associated with employing each policy element. In total, this results in 8 regressions, one for each of the 4 target features and 2 discussion lengths to evaluate, all with 13 main explanatory variables. We perform model selection (Appendix C) to decide which terms and interactions to use. In the final model, these explanatory variables are evaluated as main effects, with interaction terms between primary ordering and reply structure, and primary ordering and Editors' Pick status also evaluated. The final model terms with interactions are summarised in Table 5."}, {"title": "3.3.4. Code, Data, and Ethics", "content": "The repository for this project is hosted at github.com/dornleiten/commentgap. Comment data is not published to preserve user privacy. This work was approved by the university ethical review procedures (ID 179954). The dataset is based exclusively on public data and was scraped according to the terms & conditions of derstandard.at (Der Standard, 2024). The authors report there are no competing interests to declare."}, {"title": "4. Results", "content": ""}, {"title": "4.1. The comment gap (RQ1)", "content": "We use regression models to estimate the relationship between comment features and user votes, as well as comment features and Editors' Picks. First, we examine the results from logistic and negative binomial regression models for Editors' Picks, Upvotes, and Downvotes. All continuous variables are standardised, and log-transformed where necessary, prior to model fitting as detailed in Section 3.2. Model summaries with their coefficients are provided in Table 6. Since users are able to both Upvote and Downvote comments, and that one would expect both of these factors to be associated with simply the level of exposure/attention a comment receives, we also compute a score to assess the relative preferences of readers by the coefficients for upvoting/downvoting a comment. For each feature, the Relative Voting Preference (RVP) is defined by\nRelative Voting Preference = $\\beta_{Upvote} - \\beta_{Downvote}$. (3)\nTaking $e^{RVP}$ then describes the factor by which the count of Upvotes on a comment increases relative to Downvotes when there is a one unit increase in the explanatory variable. Similarly, for each feature we compute the Comment Gap (CG) to assess the difference between journalist and reader preferences\nComment Gap = $\\beta_{Editors' Pick} - (\\beta_{Upvote} - \\beta_{Downvote})$. (4)\nTaking $e^{CG}$ then describes the factor by which the odds of a comment being selected as an Editors' Pick increase compared to the relative Upvotes vs Downvotes when there is a one unit increase in the explanatory variable. These are presented in Table 7. In addition to the model summary tables, we visualise the comment-level and article-level (standardised) coefficients for each model in Figures 4 and 5."}, {"title": "4.1.1. User votes", "content": "The following findings give an indication about which characteristics of comments are associated with user upvotes and downvotes (RQ1.1). We may take the negative binomial regression coefficients from Table 6 and exponentiate them to get the rate ratios that describe the factor by which Upvotes/Downvotes increase with a one unit increase in the explanatory variable.\nMore instructive, however, are Relative Voting Preferences in Table 7. These offer a better assessment for the relative preferences of readers. Exponentiating the RVPs describes the factor by which the count of Upvotes on a comment increases relative to Downvotes when there is a one unit increase in the explanatory variable. Here, we find that readers prefer comments with higher Positive Sentiment and lower Negative Sentiment. Increasing Lexical Diversity, Readability score, Number of Sentences decreases the Relative Voting Preference, indicating these factors more strongly increase the number of Downvotes a comment receives. Readers thus prefer simpler, shorter comments. Readers prefer comments with greater Topical Similarity to the Article and with more Punctuation Marks. Readers also prefer comments from those with higher Author Follower Count, indicating a quality or reputational effect. Increased Time Since Article Publication decreases the rate ratio of a comment being upvoted less than it being downvoted, so readers relatively prefer later comments. On structural factors, a comment being a Root Comment increases the rate ratio of it receiving Downvotes more than Upvotes. In addition, as the Comment Level in Tree increases, the rate ratio of Downvotes to Upvotes increase further. This is a slightly odd result, but can be explained by the frequency of comments at each level. There are fewer comments at each increasing level, but they are relatively more likely to attract Downvotes compared to Upvotes (likely due to long disputes or \"reply-wars\"). The majority of non-root comments (direct and secondary replies) have greater rate ratios of receiving Upvotes compared to Downvotes. At the article level, trivially, the Mean Upvotes and Mean Downvotes in Discussion are associated with increased rate ratios of upvoting and downvoting respectively. Readers prefer comments in discussions where the Number of Comments is higher-popular articles attract more positive engagement. On genre, readers are relatively more inclined to Upvote than Downvote articles in most genres, compared to the baseline Domestic category, the strongest of which being the Opinion genre. The exceptions being Women's Issues (negative Relative Voting Preference, weakly significant), Law and Economy (no significant difference compared to Domestic)."}, {"title": "4.1.2. Editors' Picks", "content": "We next turn to how different characteristics of comments are associated with being being pinned as \"Editors' Picks\" (RQ1.2). For journalists' comment preferences, we consider the logistic regression coefficients in Table 6. An increase in a comment's Positive Sentiment is associated with increased odds of it being selected as an Editors' Pick, whereas an increase in Negative Sentiment is associated with (weakly significant) decreased odds of being selected. Journalists prefer more positive comments. Journalists also prefer more Lexically Diverse, higher Readability score comments with higher Number of Sentences, that use 2nd Person Pronouns from commenters with high Author Follower Count. Comments with high Number of Punctuation Marks are less likely to be selected as Editors' Picks (weakly significant), and there is no significant association between a comment's Topical Similarity to the article and whether it is selected as an Editors' Pick. For the"}]}