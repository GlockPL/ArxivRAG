{"title": "BOOSTING Camera Motion CONTROL FOR VIDEO DIFFUSION TRANSFORMERS", "authors": ["Soon Yau Cheong", "Duygu Ceylan", "Armin Mustafa", "Andrew Gilbert", "Chun-Hao Paul Huang"], "abstract": "Recent advancements in diffusion models have significantly enhanced the qual-ity of video generation. However, fine-grained control over camera pose remainsa challenge. While U-Net-based models have shown promising results for cam-era control, transformer-based diffusion models (DiT)\u2014the preferred architecturefor large-scale video generation\u2014suffer from severe degradation in camera mo-tion accuracy. In this paper, we investigate the underlying causes of this issueand propose solutions tailored to DiT architectures. Our study reveals that cam-era control performance depends heavily on the choice of conditioning methodsrather than camera pose representations that is commonly believed. To address thepersistent motion degradation in DiT, we introduce Camera Motion Guidance(CMG), based on classifier-free guidance, which boosts camera control by over400%. Additionally, we present a sparse camera control pipeline, significantlysimplifying the process of specifying camera poses for long videos. Our methoduniversally applies to both U-Net and DiT models, offering improved camera con-trol for video generation tasks. Code and models will be available from the projectpage on https://soon-yau.github.io/CameraMotionGuidance/.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advances in text-to-video (T2V) generation have significantly improved video quality, withdiffusion models playing a key role in producing coherent and visually appealing outputs. Whilethese models effectively translate text prompts into dynamic videos, they often lack fine-grainedcontrol over aspects like camera pose. To address this, methods such as Wang et al. (2024); He et al.(2024); Xu et al. (2024) introduced camera pose conditioning into U-Net-based (Ronneberger et al.(2015)) diffusion models pretrained on 2D images (Rombach et al. (2022); Podell et al. (2023)),"}, {"title": null, "content": "demonstrating promising results in controlling camera trajectories in generated videos. Recently,transformer-based diffusion models (DiT) (Peebles & Xie (2022)) have emerged as the preferredarchitecture for large-scale video generation models (Ma et al. (2024); Liu et al. (2024); Yang et al.(2024b); Zheng et al. (2024)) due to their superior scalability.\nHowever, existing camera control methods may not be effective for DiT due to the architecturaldifferences. Concurrent work (Bahmani et al. (2024)) found that direct porting of these methodsto DiT led to loss of controllability, but no in-depth investigation was conducted to isolate the rootcauses. The introduction of new conditioning methods, alongside different camera pose represen-tations, new DiT architecture, and the shift from space-time (2D+1D) to spatio-temporal(3D) latentencoding, further complicated the issue. To address this, we conducted an extensive study to iden-tify the causes of camera control degradation in DiT architectures and proposed solutions broadlyapplicable across all DiT models and U-Net models.\nOur experiments reveal that the strength of camera conditioning weakens in DiT due to the largerembedding dimensions in transformers compared to U-Net. Specifically, the 12-parameter extrinsiccamera parameters, a common camera pose representation used in MotionCtrl (Wang et al. (2024)),prove to be ineffective in this context. Although Pl\u00fccker coordinates used by (He et al. (2024); Xuet al. (2024); Bahmani et al. (2024)) may mitigate the problem slightly but our study reveals it is thecamera embedding dimension that plays a more significant role. Improvement can be achieved withappropriate method to project the camera parameters into higher dimension.\nDespite these improvements, DiT still experiences significant deterioration in camera motion, evenwith an optimal combination of camera representation and conditioning methods. Implementing twostate-of-the-art U-Net methods (Wang et al. (2024); He et al. (2024)) in DiT resulted in videos withstatic or limited camera motion and less accurate camera orientations. To address this, we proposea novel Camera Motion Guidance (CMG) method based on the widely used classifier-free guidancetechnique Ho & Salimans (2021). CMG improves camera pose accuracy and motion over 400%compared to baseline DiT models. Being architecture-agnostic, it applies generically to both space-time and spatio-temporal architecture, in either DiT or U-Net models to improve camera control invideo generation.\nOn the other hand, existing methods for camera control rely on dense camera input, requiring acamera pose for every frame, which is tedious, especially for long videos. To simplify this process,we propose a novel data augmentation pipeline that introduces sparse camera control, where onlythe camera pose for the final frame or a few key interval frames is required. Our experimentsdemonstrate that sparse camera control shown promising results, simplifying the input process whilemaintaining high-quality results and precise camera motion.\nWe summarize our main contributions as below:"}, {"title": null, "content": "\u2022 We introduce novel Camera Motion Guidance, a classifier-free guidance method, improving camera motion by over 400% in video diffusion transformers.\n\u2022 We identify the root causes of camera control degradation in DiT and successfully devel-oped the first camera control model for space-time video diffusion transformers.\n\u2022 We develop a novel data augmentation pipeline that enables sparse camera control, whichsimplifies the required input control signal. To our knowledge, this is a unique feature notdemonstrated in the existing work."}, {"title": "2 RELATED WORKS", "content": "Video Diffusion Models. Diffusion models (Ho et al., 2020; Rombach et al., 2022; Stability.ai,2023; Podell et al., 2023) have achieved remarkable success in image generation, leading to advance-ments in video diffusion models that build upon this foundation. The video diffusion models (Hoet al., 2022; Blattmann et al., 2023) first process individual frames by applying 2D latent encodingsto each image separately, and then fuse the temporal dimension to generate videos, this is known asspace-time encoding (2D+1D). More recent developments (Liu et al., 2024; Ma et al., 2024; Zhenget al., 2024; Lab & etc., 2024) have introduced spatio-temporal encoding, which encodes multipleframes simultaneously (3D), creating more compact latent code for long video generation."}, {"title": null, "content": "Camera Pose Control has existed since early 2D human image generation methods. By modifyingthe size and vertical rotation of skeleton images, methods such as Ma et al. (2017); Ren et al. (2022);Ju et al. (2023); Cheong et al. (2024) could influence camera pose, though only indirectly in limitedways. As neural network architectures have evolved from convolutional networks (Lecun et al.,1998) to transformers (Vaswani et al., 2017), parameterized poses have been explored as substitutesfor pose images (Cheong et al., 2022). In a study, Cheong et al. (2023) explicitly mapped 3D cameratranslation parameters along with 3D SMPL body pose parameters (Lopper et al., 2015), allowingfor simultaneous control of both body pose and camera pose.\nAs video generation emerged as active research, attention has been focused on controlling cameramotion for video. AnimateDiff (Guo et al., 2024) trains module on specific motion and hence requir-ing new module for a different motion, hindering usability. Hence, parameterized camera controlhas recently become a focal point in video generation to improve its ease-of-use. Direct-a-video(Yang et al., 2024a) uses only translation movement limiting the camera motion to only panningand zooming. Instead, MotionCtrl (Wang et al., 2024) introduced a rotation-and-translation matrixderived from camera extrinsic parameters, allowing more complex motion. CameraCtrl (He et al.,2024), on the other hand, uses Pl\u00fccker coordinates as camera pose representation, enabling geomet-ric interpolation for each pixel. We evaluate both state-of-the-art parameterization methods to assesstheir effectiveness in DiT-based models. In these methods, camera poses are applied to individualimage latents on a frame-by-frame basis, which makes them unsuitable for direct application tospatio-temporal models such as concurrent works (Bahmani et al., 2024; Zhang et al., 2024) wheremultiple frames are jointly encoded into a single latent representation. To the best of our knowledge,we are the first to develop camera control methods specifically for space-time DiT, enabling videomodels to leverage the extensive availability of pretrained 2D diffusion models.\nDiffuser Guidance. Dhariwal & Nichol (2021) demonstrated that applying guidance in diffusionmodels significantly enhances image quality. During inference, in each denoising step, the modeldenoises the latent twice-once without conditioning and once with it. The difference between theconditioned and unconditioned latents defines a direction that can be extrapolated by multiplyingit with a scalar guidance scale. In particular, classifier-free guidance (Ho & Salimans (2021)) hasbecome ubiquitous in modern text prompting diffusion models in which an empty string or negativetext prompt is used as unconditional reference to provide latent extrapolation to improve image orvideo quality. Despite advancements in camera control for video generation (Wang et al., 2024;He et al., 2024; Xu et al., 2024; Bahmani et al., 2024), diffuser guidance has not been explored forcamera motion improvement. In this paper, we introduce camera motion guidance to enhance theaccuracy and quality of camera motion in video generation models.\nSparse Camera Control. Existing methods rely on dense camera poses to achieve effective control.SparseCtrl (Guo et al., 2023) explores applying sparse image-based structural control but does notincorporate camera control, leaving a gap in addressing sparse camera pose scenarios for video\ngeneration tasks."}, {"title": "3 OUR APPROACH", "content": null}, {"title": "3.1 PRELIMINARY", "content": "Camera Representation. Extrinsic camera parameters describe the camera's position and orienta-tion in 3D space represented by a rotation matrix $R \\in \\mathbb{R}^{3\\times3}$ and a translation vector $T \\in \\mathbb{R}^3$ whichform the rotation-and-translation (RT) matrix $[R|T] \\in \\mathbb{R}^{3\\times4}$. Intrinsic parameters, encapsulatedin the camera matrix K, define the camera's internal characteristics, including focal length, principalpoint and pixel size. These are used to map a 2D pixel location in the image to a 3D directionvector in camera's coordinate system. For each pixel (x, y) in image coordinate space, its Pl\u00fcckercoordinate is calculated as $(O \\times d_{x,y}, d_{x,y})$ where $O \\in \\mathbb{R}^3$ is the camera center in world coordinatesderived from $-R^TT$; and the direction vector $d \\in \\mathbb{R}^3$ is obtained by:"}, {"title": null, "content": "$d_{x,y} = RK^{-1}[x, y, 1]^T                                         (1)$\nThis formulation represents the direction and location of 3D lines, allowing for efficient geometricoperations like interpolation and transformation, which are useful for camera trajectory and ray-based rendering. Compared to a flattened $RT \\in \\mathbb{R}^{12}$ in a video frame, Pl\u00fccker coordinates hashigher dimension $\\in \\mathbb{R}^{h,w,6}$ where h and w are height and width of video resolution."}, {"title": "Camera Conditioning for Video Generation", "content": "We examine the state-of-the-art camera conditioningin U-Net models to understand how their network topology differences from DiT models affect theeffectiveness of camera conditioning. In U-Net architecture, the spatial resolution decreases as thefeatures traverses down the network, while the channel C increases from e.g. 320 to a maximumof e.g. 1280 before stepping down again. MotionCtrl(Wang et al. (2024)) employs a RT matrix toas camera pose representation for the entire video frame. To incorporate this into the U-Net, theflattened RT is repeated for every latent pixel and concatenated with the U-Net's features alongchannel dimension, forming a new dimension of C+12 where C is the U-Net embedding's channelnumber. However, increased spatial resolution of U-Net's embedding is accompanied with repetitiveRT in each spatial position and does not carry more camera information. Thus we can ignore thespatial dimension when considering camera conditioning influence and consider only the channeldimension. As illustrated in Figure 2(a), U-Net has the smallest embedding channel at its top andbottom layers, resulting in the highest ratio of camera parameter dimension to embedding dimension,we refer to as condition-to-channel ratio. For simplicity, we display only the channel dimension ofthe full tensor shape [B, N, H, W, C'] (batch size, number of frames, image height, image width,channel) and omit the other dimensions in the figure."}, {"title": null, "content": "In contrast, transformers maintain a consistent channel number e.g. 1024 across all the layers. Whenimplementing MotionCtrl's method to OpenSora (Zheng et al. (2024)) DiT, the increase of minimumchannel number from 320 to 1152 lead to significant drop (3x) of the condition-to-channel ratio of12:C from 1:27 to 1:96. We hypothesize camera conditioning strength is proportionate to this ratio,which leads to a considerably weakened control strength in DiT."}, {"title": null, "content": "Our experiment compares another conditioning scheme from CameraCtrl (He et al. (2024)) to con-firm our hypothesis. As Pl\u00fccker coordinates has dimension of [h, l, 6] per camera pose, a cameraencoder is necessary to produce multi-scale camera embedding that matches the dimensions of theDiT's features in each layer l, represented as [H\u0131, H\u0131, C\u0131] for element-wise addition as shown inFigure 2b. This difference is even more pronounced when considering the spatial dimension, aseach Pl\u00fccker embedding is unique for each spatial location, thereby offering significantly strongercamera conditioning. At a high level, the main differentiators between the methods are the camerarepresentations and the approach to fusing camera embeddings: MotionCtrl concatenates RT, whileCameraCtrl adds Pl\u00fccker embedding to the U-Net features.\nBoth methods share several common practices. First, each fused embedding is projected back tothe original embedding dimension via a linear layer at every U-Net layer. Additionally, cameraconditioning is applied before the temporal transformers. Only the temporal transformers and thenewly added camera control components are updated during training, while all other parametersremain frozen. These practices were similarly adopted in our experiments. Another method, CamCo(Xu et al. (2024)), employs a conditioning scheme similar to CameraCtrl, with the key differencebeing the use of 1\u00d71 convolution layers instead of linear layers for projection. However, due to thelack of open-source implementation, we have not experimented with it in our study."}, {"title": "3.2 DIT BASELINE MODEL IMPLEMENTATION", "content": "We adopts an open source DiT text-to-video (T2V) model OpenSora (Zheng et al. (2024)) as ourbase video generation model. They have newer versions that employ a spatio-temporal 3D encoder,enabling higher resolution and better image quality. However, we use OpenSora 1.0 that uses space-time architecture, allowing for direct comparison with the U-Net models. Then OpenSora 1.0 modelconsists of a cascade of 28 Spatial-Temporal DiT (ST-DiT) blocks inspired by Ma et al. (2024), eachcontaining a spatial transformer followed by a temporal transformer.\nWe implemented two methods as our DiT baselines: DiT-MotionCtrl and DiT-CameraCtrl, namedafter their respective U-Net-based counterparts. The methods were faithfully implemented for like-for-like comparison, with adjustments to the channel dimension of linear layers and the cameraencoder to match the embedding dimension of DiT. Additionally, given DiT's uniform channel di-mension, we have only one block in the camera encoder, in contrast to three blocks in U-Net'sproducing camera embeddings at three different resolutions."}, {"title": "3.3 DATA PROCESSING AND CAMERA AUGMENTATION", "content": "Given that our base video model is limited to 16 frames, translating to about 0.5 seconds of footageat 30 frames-per-second, taking consecutive frames would result in minimal camera motion. There-fore, we extract video frames from the dataset in strides of 4 to 8 frames, sampled uniformly, ensur-ing we capture sufficient temporal information while preserving meaningful motion dynamics. We\nrandomly sample 16 frames from a training video sample, and this can produce arbitrary large start-ing translation vector. Therefore, we represent all RT matrices relative to the first frame by settingthe translation vector $T_1 = 0_3$ and rotation matrix $R_1 = I_{3\\times3}$ and multiplying $[R_1 T_1]^{-1}$ to the restof the RT matrices.\nTo ensure smooth integration of our novel camera control method, we now establish a robust cameraaugmentation pipeline. In training, we randomly generate static video by repeating the first frameand its corresponding camera pose across all subsequent frames. In other words, all camera posesare filled with value of $[R_1|T_1]$ or its corresponding Pl\u00fccker coordinate, a condition we denote asnull camera, \u00d8c. Inspired by Srivastava et al. (2014), we also randomly drop out camera poses ina video by setting them to zeros to prevent overfitting and allow for sparse camera control, we callthis zero camera. In addition to the new proposed augmentation, we also adopted standard videoaugmentation of center image cropping and video temporal reversal which is critical as it balancesthe distribution of two opposite camera motions."}, {"title": "3.4 CAMERA MOTION GUIDANCE (CMG)", "content": "We now introduce our novel method - Camera Motion Guidance- based on classifier-free guidance.Equation 2 shows the generic classifier-free guidance equation for text prompt (Ho & Salimans(2021)).\n$\\theta(z_t, C_T) = \\theta(z_t,\\emptyset_T) + s_T{\\theta(z_t, C_T) - \\theta(z_t,\\emptyset_T)}                                   (2)$\nwhere $\\theta$ is the denoising model (U-Net or DiT), $z_t$ is the noisy latent at time step t, $C_T$ is the textcondition, $\\emptyset_T$ is null text condition and $s_T$ is text prompt guidance scale. In essence, the secondterm in the equation finds the text embedding direction in the latent space and extrapolates it with ascalar $s_T$, using the unconditioned first term as a reference point.\nIn existing camera-controlling literature (Wang et al., 2024; He et al., 2024), classifier-free guidanceis applied solely to the text prompt, with the camera condition $C_C$ present in all guidance terms.This approach is analogous to Equation 3, effectively canceling out the camera condition's influencein the second term.\n$\\theta(z_t, C_T, C_C) = \\theta(z_t,\\emptyset_T, C_C) + s_T{\\theta(z_t,C_T,C_C) - \\theta(z_t, \\emptyset_T,C_C)}                                       (3)$"}, {"title": null, "content": "To address this, we propose a new camera motion guidance term, disentangling it from the originaltext guidance term, as outlined in Equation 4. The null camera \u00d8c is used as a reference, while thecamera motion guidance scale, sc, is applied to guide the camera motion independently.\n$\\theta(z_t, C_T, C_C) = \\theta(z_t, \\emptyset_T, \\emptyset_C) + s_T{\\theta(z_t,C_T,\\emptyset_C) - \\theta(z_t, \\emptyset_T,\\emptyset_C)}                                       (4)$\nWith simple changes in data augmentation, our method CMG can enhance any video generativemodel employing classifier-free guidance, offering improved camera control across a variety of ar-chitectures."}, {"title": "4 EXPERIMENTS", "content": null}, {"title": "4.1 IMPLEMENTATION DETAILS", "content": "We train our models using the RealEstate10k dataset (Zhou et al., 2018), which features indoor andoutdoor real estate videos with corresponding camera poses. The models are trained at a resolutionof 256x256 and 16 frames per video sample, matching the settings of the U-Net models for com-parisons. Since the original dataset lacks captions, we use the text prompts provided by He et al.(2024). The training was conducted on GPUs with 40GB memory, using a batch size of 3 per GPU,and models were trained for 8 epochs with a fixed learning rate of 1 \u00d7 10\u22125.\nWe evaluate two baseline DiT models: DiT-MotionCtrl and DiT-CameraCtrl, which use RT matricesand Pl\u00fccker coordinates as their respective camera representations. Additionally, to enable CMG, wetrain the same models with our augmentation pipeline which saw 5% null camera data augmentation.Both the baseline and CMG versions also applied a 5% camera dropout, which is randomly setbetween 70% and 100% of camera frames in a video to zero.\nDuring inference, the two baseline models use standard guidance (Eq. 3), while CMG models applyEq. 4. A text guidance scale st of 4.0 is used consistently across all the DiT models. We evaluatea range of CMG scale sc between 4.0 and 7.0, eventually selecting 5.0 for comparison with thebaselines. Apart from this, identical configurations and random seeds are applied to all DiT mod-els to ensure fair comparisons. We use U-Net models' default configurations for inferences. Toinvestigate the effect of camera representation further, we also replace the Pl\u00fccker coordinates inDiT-CameraCtrl with RT matrices and re-train a new model."}, {"title": "4.2 METRICS", "content": "We aim to measure two aspects of camera motion: first, the accuracy with which the camera motionadheres to the specified camera conditioning and second, the extent of motion present in the gen-erated videos. For the camera motion accuracy, we adopt approach from He et al. (2024) to utilizeCOLMAP (Sch\u00f6nberger & Frahm, 2016) in extracting rotation matrices $R_{gen} \\in \\mathbb{R}^{N\\times3\\times3}$ and trans-lation vectors $T_{gen} \\in \\mathbb{R}^{N\\times3}$ of generated videos where N is frame length. The rotation error $R_{err}$and translation error $T_{err}$ are calculated by comparing with ground truth $R_{gt}$ and $T_{gt}$ respectivelyusing Eq. 5 and 6.\n$R_{err} = \\sum_{n=2}^{N} cos^{-1}(\\frac{tr(R^T_{ngent}) - 1}{2}) (5)$\n$T_{err} = \\sum_{n=2}^{N} ||T^gt_n - T^gen_n||_2                                     (6)$\nwhere n is n-th video frame and tr is the trace of a matrix. We exclude the calculation of error forthe first frame, as it is always zero by definition due to the camera poses preprocessing. We reportthe rotation error in radian. The translation range in generated videos can vary, but more importantly,COLMAP estimation can yield a wide translation range. Therefore, we normalize both translationvectors Tgt and Tgen to have a unit maximum distance during inference for metric evaluation.\nTo quantitatively assess the level of motion in the generated videos, it is essential to identify anappropriate measurement method. After considering various options, we ultimately select Teed &Deng (2020) to measure the optical flow between two frames, which gives a flow field represented"}, {"title": null, "content": "as two arrays u and v corresponding to each pixel's horizontal and vertical components of motion.The motion magnitude M is then calculated for all adjacent frames using Eq. 7, and the results areaveraged over the entire video.\n$M = \\frac{1}{K}\\sum_{k=1}^{K} \\sqrt{u_k^2 + v_k^2}                                          (7)$\nwhere k is k-th pixel in a video frame and K is total pixel counts in a frame. We also use FID(Heusel et al. (2017)) to measure image quality in the ablation study, ensuring that our methodmaintains high video quality."}, {"title": "4.3 RESULTS", "content": "Motion Degradation with DiT Implementation. Porting the MotionCtrl method into DiT leadsto a total loss of controllability, as also observed by Bahmani et al. (2024). This is evident insharp rise in rotation error as shown in Table 1 (Model 1a\u21921b). Most importantly, the motionmagnitude collapses 80% from 7.780 to 1.485. The lack of motion is difficult to perceive fromstill images, therefore we included \"Supplementary Video 1 - Method Comparison\" to demonstrate the stark contrast in motion. Although applying CMG to DiT-MotionCtrl (Model 1c) brings slightimprovement(compared to Model 1b), the high errors and the lack of motion persist, indicating thecorresponding U-Net method is not effective for DiT."}, {"title": null, "content": "Camera Motion Guidance Restores Controllability and Significantly Boosting Motion. Whilethe baseline DiT-CameraCtrl's rotation error remained at a similar level, meaning it has better con-trollability, it still suffered from severe losses in motion and translation accuracy (Model 2a 2b)as illustrated in Figure 3 and \"Supplementary Video 1 - Method Comparison\". Applying CMG sig-nificantly boosted both metrics, with a notable 412% increase in motion magnitude, from 1.565 to6.450 (Model 2b\u21922c). Our model (Model 2c) significantly outperformed both DiT baseline models"}, {"title": null, "content": "(Model 1b and 2b) and also surpassed both U-Net models in translation error, which is a more criti-cal metric for our study than rotation error. It is worth noting that, motion as measured from opticalflow is sensitive to pixel quality and video content. Therefore, the motion magnitude is not directlycomparable with U-Net models which are based on different pre-trained base models and trainingrecipes."}, {"title": "Comparing Pl\u00fccker Coordinates and Extrinsic Parameters", "content": "Previously in Table 1, we haveshown that DiT-CameraCtrl (Model 2b) is more effective than DiT-MotionCtrl (Model 1b), and theperformance gap becomes even more significant with our CMG (Model 1c\u2192 2c). However, asidefrom their conditioning methods, the two methods also use different camera data representation.To make a fair comparison, we repeated the DiT-CameraCtrl experiment by replacing the Pl\u00fcckercoordinates with RT matrices. This allowed us to isolate and compare the methods with only onedifferentiating factor at a time."}, {"title": "4.4 ABLATIONS", "content": null}, {"title": null, "content": "We conduct an extensive study varying the CMG scale across a range of values to prove its effec-tiveness in inducing and controlling camera motion. As illustrated in Figure 4, increasing the CMGscale results in greater camera motion. The preservation of video content highlights the strong dis-entanglement of our method, allowing independent camera motion control, separated from the textguidance in classifier-free guidance (Eq. 4) that describes the video scene."}, {"title": null, "content": "As DiT-MotionCtrl has proven ineffective, we focus our discussion on DiT-CameraCtrl for the quan-titative result shown in Table 3. Determining the optimal CMG scale is not straightforward, as nosingle value consistently delivers the best results across all metrics. Additionally, each metric has itsown limitations, making the selection of an ideal CMG scale more nuanced. While higher motionmagnitude increases movement, it can also result in blurrier videos, as reflected by the degradationin FID scores compared to those produced by U-Net model using the same text prompt. Amongthe error measurements, translation error plays a more critical role in typical camera motion sce-narios. Therefore, we selected CMG scale of 5.0 which minimizes translation error for optimalperformance, and used it for main comparison in Table 1."}, {"title": "4.5 SPARSE CAMERA CONTROL", "content": "Since we drop certain interval frames by setting the camera poses to zeros during training, ourmethod allows users to provide camera control for only a sparse set of frames at test time, which, toour knowledge, is not supported by existing methods. Translation motion in a single dimension, suchas zooming, can be easily interpolated and does not offer significant value in testing sparse control.Therefore, we excluded simple translation motion from evaluation. Table 4 presents the rotation andtranslation errors at different sparsity ratios, which we define as the ratio of dropped camera posesto N 1 frames, excluding the first frame. While errors do increase with higher sparsity ratios, therate of error increase remains relatively modest compared to the level of sparsity, even up to 87%sparsity, where only the camera poses in the first, middle, and last frames are provided."}, {"title": null, "content": "Figure 5 shows videos generated using sparse camera poses as specified in the leftmost columnwhere only 4 frames (73% sparsity) and 1 frame (93% sparsity) are used respectively. When onlythe last frame was used, the specified camera pose Figure 5a and Figure 5b end in similar position,differing only in the camera rotation. In Figure 5a, the generated camera motion accurately followsthe translation-only trajectory, maintaining a straight-facing camera angle as expected. On the otherhand, the rotated ending camera pose in Figure 5b result in a smooth rotating motion alongsidetranslation, similar to the video above generated using denser camera poses. The videos, which canbe viewed in \"Supplementary Video 3: Sparse Control\" highlights our model's ability to interpolatethe camera poses to fill in the gaps and maintaining smooth, coherent camera motion. Our sparsecamera data augmentation technique is also effective with standard DiT methods without CMG.While this model demonstrates weaker camera controllability and motion without CMG, it stillsuccessfully interpolates camera poses, proving its robustness in sparse control scenarios."}, {"title": null, "content": "(a) Translation only camera motion."}, {"title": null, "content": "(b) Simultaneous translation and rotation camera motion."}, {"title": "5 LIMITATIONS", "content": "Although object motion is excluded from our study, it is not negatively impacted by CMG. In \"Sup-plementary Video 4 - Object Motion\u201d, we showcase object motion from natural landscapes and 3Dcharacter animation alongside camera motion controlled using our method. However, the videosgenerated by our models may show limitations in image quality and content richness compared tomodels pre-trained on larger datasets. The OpenSora 1.0 we use was pre-trained on 400K video\nclips-a much smaller dataset than the 10M videos used by MotionCtrl. This constraint may alsohave led to occasional deformations for objects such as the Eiffel Tower, which are not present inthe RealEstate10k dataset we trained on. Since our CMG method effectively disentangles cameramotion from the text prompt, we believe more visually appealing videos could be generated with ahigher-quality base video model."}, {"title": "6 CONCLUSIONS", "content": "This paper thoroughly examined the impact of various camera representations and conditioningmethods on camera control for video generative diffusion transformers. Our extensive experimentsconfirmed that high-dimensional camera embedding is critical for effective camera control, support-ing our hypothesis. We also found that camera representation alone is not the key factor for success-ful control; instead, it must be paired with effective conditioning methods and guidance techniquesto achieve optimal results. We successfully demonstrated the first camera control model for space-time DiT by combining the CameraCtrl architecture, Pl\u00fccker coordinates for camera representation,and our novel camera motion guidance (CMG).\nWe have proved that CMG is highly effective in inducing motion and enhancing camera control.Due to limited resources and code availability, we could not experiment with CMG on a broaderrange of video models. However, we believe it would be equally effective for U-Net and other DiTmodels with spatio-temporal architectures. Additionally, we introduced novel camera data augmen-tation techniques that enable sparse camera control. These simple yet effective methods are generic,making them applicable and beneficial for a broader range of video model architectures.\nIn future work, we aim to test our CMG method on a wider range of models, including U-Nets andother spatio-temporal DiT. Additionally, we plan to enhance our approach to sparse camera control,ensuring that it can achieve even greater accuracy in interpolating camera poses."}]}