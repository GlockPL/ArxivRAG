{"title": "Towards Collaborative Intelligence: Propagating Intentions and Reasoning for Multi-Agent Coordination with Large Language Models", "authors": ["Xihe Qiu", "Haoyu Wang", "Xiaoyu Tan", "Chao Qu", "Yujie Xiong", "Yuan Cheng", "Yinghui Xu", "Wei Chu", "Yuan Qi"], "abstract": "Effective collaboration in multi-agent systems requires communicating goals and intentions between agents. Current agent frameworks often suffer from dependencies on single-agent execution and lack robust inter-module communication, frequently leading to sub-optimal multi-agent reinforcement learning (MARL) policies and inadequate task coordination. To address these challenges, we present a framework for training large language models (LLMs) as collaborative agents to enable coordinated behaviors in cooperative MARL. Each agent maintains a private intention consisting of its current goal and associated sub-tasks. Agents broadcast their intentions periodically, allowing other agents to infer coordination tasks. A propagation network transforms broadcast intentions into teammate-specific communication messages, sharing relevant goals with designated teammates. The architecture of our framework is structured into planning, grounding, and execution modules. During execution, multiple agents interact in a downstream environment and communicate intentions to enable coordinated behaviors. The grounding module dynamically adapts comprehension strategies based on emerging coordination patterns, while feedback from execution agents influnces the planning module, enabling the dynamic re-planning of sub-tasks. Results in collaborative environment simulation demonstrate intention propagation reduces miscoordination errors by aligning sub-task dependencies between agents. Agents learn when to communicate intentions and which teammates require task details, resulting in emergent coordinated behaviors. This demonstrates the efficacy of intention sharing for cooperative multi-agent RL based on LLMs.", "sections": [{"title": "1 Introduction", "content": "With the recent advancements of large language models (LLMs), developing intelligent agents that can perform complex reasoning and long-horizon planning has attracted increasing research attention. A variety of agent frameworks have been proposed, such as ReAct , LUMOS , Chameleon and BOLT . These frameworks typically consist of modules for high-level planning, grounding plans into executable actions, and interacting with environments or tools to execute actions. \nDespite their initial success, existing agent frameworks may experience some limitations. Firstly, most of them rely on a single agent for execution. However, as tasks become more complex, the action dimension can be increased exponentially, and it poses significant challenges for a single agent to handle all execution functionalities. Secondly, existing frameworks lack inter-module communication mechanisms. Typically, the execution results are directly used as input in the planning module without further analysis or coordination. When execution failures occur, the agent may fail to adjust its strategies accordingly. Thirdly, the grounding module in existing frameworks operates statically, without interactions with downstream modules. It grounds plans independently without considering feedback or states of the execution module. LLMs struggle to handle emergent coordination behaviors and lack common grounding on shared tasks. Moreover, existing multi-agent reinforcement learning (MARL) methods often converge on suboptimal policies that fail to exhibit a certain level of cooperation. \nHow can the agents with LLMs effectively communicate and collaborate with each other? we propose a novel approach, Recursive Multi-Agent"}, {"title": "2 Preliminary", "content": "In this section, we introduce the methods of the proposed REMALIS framework in detail. As illustrated in Figure 1, REMALIS consists of four key components:\nPlanning Module \\(p_{\\theta}\\) predicts the next pending sub-goal \\(s_{t+1}\\), given the current sub-goal \\(s_t\\) and other inputs \\(s_{t+1} = p_{\\theta}(s_t, I_t, e_t, f_t)\\), where \\(I_t\\) is the current intention, \\(e_t\\) is the grounded embedding, and \\(f_t\\) is agent feedback. \\(p_{\\theta}\\) first encodes information through encoding layers \\(h_t = Encoder(s_t, I_t, e_t, f_t)\\) and subsequently predict the sub-goal through \\(S_{t+1} = Softmax(T_{\\theta}(h_t))\\), where \\(T_{\\theta}\\) utilizes the graph neural network (GNN) architecture.\nThe module is trained to maximize the likelihood of all sub-goals along the decision sequences given the current information on time step t. This allows the dynamic re-planning of sub-task dependencies based on agent feedback.\n\\[\\theta^* = arg\\underset{\\theta}{max} \\prod_{t=1}^{T} P_{\\theta} (s_{t+1} | s_t, I_t, e_t, f_t).\\]\nGrounding Module \\(g_{\\phi}\\) contextualizes symbol embeddings \\(e_t = g_{\\phi}(s_t, I_t, f_{1:t})\\), where \\(s_t\\), \\(I_t\\), and \\(f_{1:t}\\) represent the states, intention, and feedback up to time step t, respectively. These embeddings are processed by encoders \\(h_t = Encoder(s_t, I_t, f_{1:t})\\) and then by cross-attention layers and convolutional feature extractors: \\(e_t = Conv(Attn(h_t, V)) + P_t\\) over vocabulary V. Here, \\(P_t\\) includes agent feedback to enhance grounding accuracy based on coordination signals for more accurate contextual understanding. The module maps language symbols to physical environment representations through:\n\\[g(x) = f_{\\theta}\\left(\\sum_{i=1}^{N} w_i g(x_i)\\right),\\]\nwhere \\(g(x)\\) is the grounded embeddings of policy set x and \\(g(x_i)\\) represents its individual action"}, {"title": "3 Approach", "content": "The collaborative MARL of REMALIS focuses on three key points: intention propagation for grounding, bidirectional coordination channels, and integration with recursive reasoning agents. Detailed parameter supplements and pseudocode details can be found in Appendix C and Appendix F."}, {"title": "3.1 Planning with Intention Propagation", "content": "We formulate a decentralized, partially observable Markov game for multi-agent collaboration. Each agent i maintains a private intention \\(Z_i\\) encoded as a tuple \\(I_i = (\\gamma_i, \\Sigma_i, \\pi_i, \\delta_i)\\), where \\(\\gamma_i\\) is the current goal, \\(\\Sigma_i = {\\sigma_{i1}, \\sigma_{i2}, ...}\\) is a set of related sub-goals, \\(\\pi_i(\\sigma)\\) is a probability distribution over possible next sub-goals, and \\(\\delta_i(\\sigma)\\) is the desired teammate assignment for sub-goal \\(\\sigma\\).\nIntentions are propagated through a communication channel \\(f_{\\Lambda}\\) parameterized by \\(\\Lambda\\). For a received message \\(m_{ij}\\) from agent j, agent i infers a belief over teammate j's intention \\(b_i(I_j|m_{ij}) = f_{\\Lambda}(m_{ij})\\), where \\(\\Lambda\\) is a recurrent neural network. The channel \\(f_{\\Theta}\\) is trained in an end-to-end manner to maximize the coordination reward function \\(R_c\\). This propagates relevant sub-task dependencies to enhance common grounding on collaborative goals.\n\\[\\Lambda^* = arg \\underset{\\Lambda}{max} E_{Z,m \\sim f_{\\Lambda}}[R_c(I, m)].\\]\nAt each time-step t, the LLM witll processinputs comprising the agent's state \\(s_t\\), the intention \\(I_t\\), and the feedback \\(f_{1:t}\\)."}, {"title": "3.2 Grounding with Bidirectional Coordination Channels", "content": "The execution agent policies, denoted by \\(\\pi_{\\Theta}(a_i|s_i, Z_i)\\), are parameterized by g and conditioned on the agent's state \\(s_i\\) and intention \\(I_i\\). Emergent coordination patterns are encoded in a summary statistic \\(c_t\\) and passed to upstream modules to guide planning and grounding adjustments. For example, frequent miscoordination on sub-goal \\(\\sigma\\) indicates the necessity to re-plan \\(\\sigma\\) dependencies in I.\nThis bidirectional feedback aligns low-level execution with high-level comprehension strategies. In addition to the downstream propagation of intents, execution layers provide bidirectional feedback signals \\(\\Phi(t)\\) to upstream modules \\(\\Psi(t) = P(h_{exec}):\n\\[h_{exec} = [\\phi_1(o_1),...,\\phi_N(o_N)],\\]\nwhere \\(\\phi(\\cdot)\\) aggregates agent encodings to summarize emergent coordination, and \\(\\phi_i(\\cdot)\\) encodes the observation or for agent i.\nExecution agents generate feedback \\(f_t\\) to guide upstream LLM modules through: \\(f_t = g_{\\Theta}(T_{1:t})\\), where \\(g_{\\Theta}\\) processes the action-observation history"}, {"title": "3.3 Execution: Integration with Reasoning Agents", "content": "We parameterize agent policies \\(\\pi_{\\Theta}(a_t|s_t, I_t, C_{1:t})\\) using an LLM with weights \\(\\Theta\\). At each time step, the LLM takes as input the agent's state \\(s_t\\), intention \\(I_t\\), and coordination feedback \\(C_{1:t}\\). The output is a distribution over the next actions \\(a_t\\):\n\\[\\pi_{\\Theta}(a_t|s_t, I_t, C_{1:t}) = LLM_{\\Theta}(s_t, I_t, C_{1:t}).\\]"}, {"title": "3.3.2 Grounding Strategy Adjustment", "content": "We model action dependencies using a graph neural policy module \\(h_{\\Theta}^a = GNN(s_t, a)\\), where \\(h_{\\Theta}^a\\) models interactions between action a and the state \\(s_t\\). The policy is then given by \\(\\pi_{\\Theta}(a_t|s_t) = \\prod_{i=1}^{N} h_{\\Theta a_i}^i\\). This captures the relational structure in the action space, enabling coordinated action generation conditioned on agent communication.\nThe coordination feedback \\(c_t\\) is used to guide adjustments in the grounding module's strategies. We define a grounding confusion matrix \\(C_t\\), where \\(C_t(i, j)\\) represents grounding errors between concepts i and j. The confusion matrix constrains LLM grounding as:\n\\[f_{\\phi}(s_t, I_t) = LLM_{\\phi}(s_t, I_t) \\odot \\Lambda C_t\\]\nwhere \\(\\odot\\) is element-wise multiplication and \\(\\Lambda\\) controls the influence of \\(C_t\\), reducing uncertainty on error-prone concept pairs.\nWe propose a modular regularization approach, with the grounding module \\(g_{\\Theta}\\) regularized by a coordination confusion estimator:\n\\[L_{confusion} = \\frac{1}{N} \\sum_{i,j} \\lambda_{\\psi}(c_i, c_j) \\cdot Conf(c_i, c_j)\\]\nwhere \\(L_{task}\\) is the task reward, \\(Conf(c_i, c_j)\\) measures confusion between concepts \\(c_i\\) and \\(c_j\\), and \\(\\Lambda_{\\psi}(c_i, c_j)\\) are attention weights assigning importance based on grounding sensitivity.\nAn episodic confusion memory \\(M_t\\) accumulates long-term grounding uncertainty statistics:\n\\[M_t(i, j) = M_{t-1}(i, j) + I(Confuse(c_i, c_j)_t),\\]\nwhere I() are indicator functions tracking confusion events. By regularizing with a coordination-focused confusion estimator and episodic memory, the grounding module adapts to avoid miscoordination."}, {"title": "3.4 Collective Learning and Adaptation", "content": "The coordination feedback signals \\(c_t\\) and interpretability signals \\(E_t\\), \\(U_t\\), \\(R_t\\) play a crucial role in enabling the LLM agents to adapt and learn collectively. By incorporating these signals into the training process, the agents can adjust their strategies and policies to better align with the emerging coordination patterns and requirements of the collaborative tasks.\nThe collective learning process can be formalized as an optimization problem, where the goal is to minimize the following objective function \\(L(\\eta, \\gamma, \\zeta, \\xi) = E_{s_t,I_t,f_{1:t}} [\\alpha U_t + \\beta E_t \u2013 R] + \\Omega(\\eta, \\gamma, \\zeta, \\xi)\\). Here, \\(\\alpha\\) and \\(\\beta\\) are weighting factors that balance the contributions of the grounding uncertainty \\(U_t\\) and coordination errors \\(E_t\\), respectively. The team reward R is maximized to encourage collaborative behavior. The term \\(\\Omega(\\eta, \\gamma, \\zeta, \\xi)\\) represents regularization terms or constraints on the model parameters to ensure stable and robust learning.\nThe objective function L is defined over the current state \\(s_t\\), the interpretability signals \\(I_t = {E_t, U_t, R_t}\\), and the trajectory of feedback signals \\(f_{1:t} = {C_1, Z_1, ..., c_t, I_t}\\) up to the current time step t. The expectation \\(E_{s_t,I_t,f_{1:t}}[]\\) is taken over the distribution of states, interpretability signals, and feedback signal trajectories encountered during training."}, {"title": "4 Experiments", "content": "To assess the performance of our models, we conducted evaluations using two large-scale real-world datasets: the traffic flow prediction (TFP) dataset and the web-based activities dataset.\nTFP dataset comprises 100,000 traffic scenarios, each accompanied by corresponding flow outcomes. Each example is detailed with descriptions of road conditions, vehicle count, weather, and traffic control measures, and is classified as traffic flow: smooth, congested, or jammed. The raw data was sourced from traffic cameras, incident reports, and simulations, and underwent preprocessing to normalize entities and eliminate duplicates.\nWeb activities dataset contains over 500,000 examples of structured web interactions such as booking flights, scheduling appointments, and making reservations. Each activity follows a template with multiple steps like searching, selecting, filling forms, and confirming. User utterances and system responses were extracted to form the input-output pairs across 150 domains, originating from real anonymized interactions with chatbots, virtual assistants, and website frontends."}, {"title": "4.2 Implementation Details", "content": "To handle the computational demands of training our framework with LLMs, we employ 8 Nvidia A800-80G GPUs  under the DeepSpeed training framework, which can effectively accommodate the extensive parameter spaces and activations required by our framework's LLM components and multi-agent architecture.\nFor the TFP dataset, we classified the examples into four difficulty levels: \u201cEasy\u201d, \u201cMedium\", \"Hard\", and \"Hell\u201d. The \u201cEasy\u201d level comprises small grid networks with low, stable vehicle arrival rates. The \"Medium\u201d level includes larger grids with variable arrival rates. \"Hard\" tasks feature large, irregular networks with highly dynamic arrival rates and complex intersection configurations. The \"Hell\" level introduces challenges such as partially observable states, changing road conditions, and fully decentralized environments.\nFor the web activities dataset, we divided the tasks into \"Easy\u201d, \u201cMedium\u201d, \u201cHard\u201d, and \u201cAll\u201d levels. \"Easy\" tasks required basic single-click or short phrase interactions. \u201cMedium\u201d involved complex multi-page sequences like form submissions. \"Hard\u201d tasks demanded significant reasoning through ambiguous, dense websites. The \u201cAll\u201d level combined tasks across the full difficulty spectrum.\nThe dataset was divided into 80% for training, 10% for validation, and 10% for testing, with examples shuffled. These large-scale datasets offer a challenging and naturalistic benchmark to evaluate our multi-agent framework on complex, real-world prediction and interaction tasks."}, {"title": "4.3 Results and Analysis", "content": "Table 1 displays the principal experimental results of our REMALIS framework in comparison with various single-agent baselines and contemporary methods using the web activities dataset. We evaluated the models across four levels of task difficulty: \"Easy\", \"Medium\u201d, \u201cHard\u201d, and \u201cAll\u201d.\nThe results from our comparative analysis indicate that REMALIS (7B), equipped with a 7B parameter LLM backbone, significantly outperforms competing methods. On the comprehensive"}, {"title": "4.4 Ablation Studies", "content": "1)The Impact on Improving Multi-Agent Coordination Accuracy We conduct ablation studies to evaluate the impact of each component within the REMALIS framework. The observations can be found in Table 2. Excluding intention propagation results in a decrease in accuracy by over 6% across both datasets, highlighting difficulties in achieving common grounding among agents without shared local beliefs This highlights the importance of intention sharing for emergent team behaviors.\nThe absence of bidirectional coordination channels leads to a 4.37% decline in performance across various metrics, illustrating the importance of execution-level signals in shaping planning and grounding strategies. Without feedback coordination, agents become less responsive to new scenarios that require re-planning.\nSubstituting recursive reasoning with convolutional and recurrent neural networks reduces contextual inference accuracy by 5.86%. Non-recursive agents display short-sighted behavior compared to the holistic reasoning enabled by recursive transformer modeling. This emphasizes that recursive architectures are vital for complex temporal dependencies.\n2)The Impact on Improving Multi-Agent Coordination Capability As presented in Table 3, on aligned sub-task percentage, the proposed Basic Propagation, Selective Propagation, and Full Intention Sharing methods consistently outperform baseline models like REACT and AgentLM across varying difficulty levels (\u201ceasy\u201d, \u201cmedium\u201d, and \"hard\"). For example, Full Intention Sharing achieves alignment of 91%, 71%, and 62% across these levels, respectively. These results are substantially higher compared to scenarios with no communication (31%, 23%, and 17%).\nSimilarly, coordination time metrics exhibit major efficiency gains from intention propagation. On \"Hard\" tasks, Full Intention Sharing reduces coordination time to 521 ms, 57% faster than the 1198 ms for No Communication. As task complexity increases from easy to hard, the coordination time savings compared to baselines grows from 138 ms to 677 ms. This reveals that intention sharing mitigates growing coordination delays for difficult scenarios.\nThe highlighted propagation mechanisms also demonstrate clear incremental performance improvements over increasingly selective information sharing. As agents propagate more precise intentions to relevant teammates, both sub-task alignment and coordination efficiency improve. Moving from Basic to Selective to Full sharing provides gains on top of gains."}, {"title": "5 Conclusion", "content": "In this paper, we introduce a novel framework, REMALIS, designed to enhance collaborative capabilities within multi-agent systems using LLMs. Our approach incorporates three principal innovations: intention propagation for establishing a shared understanding among agents, bidirectional coordination channels to adapt reasoning processes in response to team dynamics, and recursive reasoning architectures that provide agents with advanced contextual grounding and planning capabilities necessary for complex coordination tasks. Experimental results indicate that REMALIS significantly outperforms several baseline methods, underscoring the efficacy of cooperative multi-agent AI systems. By developing frameworks that enable LLMs to acquire cooperative skills analogous to human team members, we advance the potential for LLM agents to manage flexible coordination in complex collaborative environments effectively."}, {"title": "6 Limitiation", "content": "While REMALIS demonstrates promising results in collaborative multi-agent tasks, our framework relies on a centralized training paradigm, which may hinder scalability in fully decentralized environments. The current implementation does not explicitly handle dynamic agent arrival or departure during execution, which could impact coordination in real-world applications, the recursive reasoning component may struggle with long-term dependencies and planning horizons beyond a certain time frame."}, {"title": "A Related Work", "content": "A.1 Single Agent Frameworks\nEarly agent frameworks such as Progprompt directly prompt large language models (LLMs) to plan, execute actions, and process feedback in a chained manner within one model. Despite its conceptual simplicity, an integrated framework imposes a substantial burden on a single LLM, leading to challenges in managing complex tasks\nTo reduce the reasoning burden, recent works explore modular designs by separating high-level planning and low-level execution into different modules. For example, LUMOS consists of a planning module, a grounding module, and an execution module. The planning and grounding modules break down complex tasks into interpretable sub-goals and executable actions. FiReAct introduces a similar hierarchical structure, with a focus on providing step-by-step explanations. Although partitioning into modules specializing for different skills is reasonable, existing modular frameworks still rely on a single agent for final action execution. Our work pushes this idea further by replacing the single execution agent with a cooperative team of multiple agents.\nA.2 Multi-Agent Reinforcement Learning\nCollaborative multi-agent reinforcement learning has been studied to solve complex control or game-playing tasks. Representative algorithms include COMA, QMIX and ROMA . These methods enable decentralized execution of different agents but allow centralized training by sharing experiences or parameters. Drawing on this concept, our REMALIS framework places greater emphasis on integrating modular LLMs to address complex language tasks. In REMALIS, each execution agent specializes in specific semantic domains such as query, computation, or retrieval, and is coordinated through a communication module.\nThe concept of multi-agent RL has recently influenced the design of conversational agents. EnsembleBot utilizes multiple bots trained on distinct topics, coordinated by a"}, {"title": "B Methodology and Contributions", "content": "Based on the motivations and inspirations above, we propose recursive multi-agent learning with intention sharing framework (REMALIS), an innovative multi-agent framework empowered by integrated learning for communication and collaboration. The main contributions are:\n1. We design a cooperative execution module with multiple agents trained by integrated learning. Different execution agents specialize in different semantic domains while understanding peer abilities, which reduces redundant capacities and improves efficient division of labor.\n2. We propose an attentive communication mod-"}, {"title": "7 Conclusion", "content": "in this paper,we describe a number of related variables for which the the model should have some effect."}]}