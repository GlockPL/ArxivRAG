{"title": "Towards Collaborative Intelligence: Propagating Intentions and Reasoning for Multi-Agent Coordination with Large Language Models", "authors": ["Xihe Qiu", "Haoyu Wang", "Xiaoyu Tan", "Chao Qu", "Yujie Xiong", "Yuan Cheng", "Yinghui Xu", "Wei Chu", "Yuan Qi"], "abstract": "Effective collaboration in multi-agent systems requires communicating goals and intentions between agents. Current agent frameworks often suffer from dependencies on single-agent execution and lack robust inter-module communication, frequently leading to sub-optimal multi-agent reinforcement learning (MARL) policies and inadequate task coordination. To address these challenges, we present a framework for training large language models (LLMs) as collaborative agents to enable coordinated behaviors in cooperative MARL. Each agent maintains a private intention consisting of its current goal and associated sub-tasks. Agents broadcast their intentions periodically, allowing other agents to infer coordination tasks. A propagation network transforms broadcast intentions into teammate-specific communication messages, sharing relevant goals with designated teammates. The architecture of our framework is structured into planning, grounding, and execution modules. During execution, multiple agents interact in a downstream environment and communicate intentions to enable coordinated behaviors. The grounding module dynamically adapts comprehension strategies based on emerging coordination patterns, while feedback from execution agents influnces the planning module, enabling the dynamic re-planning of sub-tasks. Results in collaborative environment simulation demonstrate intention propagation reduces miscoordination errors by aligning sub-task dependencies between agents. Agents learn when to communicate intentions and which teammates require task details, resulting in emergent coordinated behaviors. This demonstrates the efficacy of intention sharing for cooperative multi-agent RL based on LLMs.", "sections": [{"title": "1 Introduction", "content": "With the recent advancements of large language models (LLMs), developing intelligent agents that can perform complex reasoning and long-horizon planning has attracted increasing research attention (Sharan et al., 2023; Huang et al., 2022). A variety of agent frameworks have been proposed, such as ReAct (Yao et al., 2022), LUMOS (Yin et al., 2023), Chameleon (Lu et al., 2023) and BOLT (Chiu et al., 2024). These frameworks typically consist of modules for high-level planning, grounding plans into executable actions, and interacting with environments or tools to execute actions (Rana et al., 2023).\nDespite their initial success, existing agent frameworks may experience some limitations. Firstly, most of them rely on a single agent for execution (Song et al., 2023; Hartmann et al., 2022). However, as tasks become more complex, the action dimension can be increased exponentially, and it poses significant challenges for a single agent to handle all execution functionalities (Chebotar et al., 2023; Wen et al., 2023). Secondly, existing frameworks lack inter-module communication mechanisms. Typically, the execution results are directly used as input in the planning module without further analysis or coordination (Zeng et al., 2023; Wang et al., 2024b). When execution failures occur, the agent may fail to adjust its strategies accordingly (Chaka, 2023). Thirdly, the grounding module in existing frameworks operates statically, without interactions with downstream modules. It grounds plans independently without considering feedback or states of the execution module (Xi et al., 2023). LLMs struggle to handle emergent coordination behaviors and lack common grounding on shared tasks. Moreover, existing multi-agent reinforcement learning (MARL) methods often converge on suboptimal policies that fail to exhibit a certain level of cooperation (Gao et al., 2023; Yu et al., 2023).\nHow can the agents with LLMs effectively communicate and collaborate with each other? we propose a novel approach, Recursive Multi-Agent"}, {"title": "2 Preliminary", "content": "In this section, we introduce the methods of the proposed REMALIS framework in detail. As illustrated in Figure 1, REMALIS consists of four key components:\nPlanning Module \\(p_{\\theta}\\) predicts the next pending sub-goal \\(s_{t+1}\\), given the current sub-goal \\(s_t\\) and other inputs \\(s_{t+1} = p_{\\theta}(s_t, I_t, e_t, f_t)\\), where \\(I_t\\) is the current intention, \\(e_t\\) is the grounded embedding, and \\(f_t\\) is agent feedback. \\(p_{\\theta}\\) first encodes information through encoding layers \\(h_t = Encoder(s_t, I_t, e_t, f_t)\\) and subsequently predict the sub-goal through \\(S_{t+1} = Softmax(T_{\\theta}(h_t))\\), where \\(T_{\\theta}\\) utilizes the graph neural network (GNN) architecture.\nThe module is trained to maximize the likelihood of all sub-goals along the decision sequences given the current information on time step t. This allows the dynamic re-planning of sub-task dependencies based on agent feedback.\n\\[\\underset{\\theta}{arg\\ max} \\prod_{t=1}^{T} P_{\\theta}(S_{t+1} | S_t, I_t, e_t, f_t).\\]\nGrounding Module \\(g_{\\phi}\\) contextualizes symbol embeddings \\(e_t = g_{\\phi}(s_t, I_t, f_{1:t})\\), where \\(s_t\\), \\(I_t\\), and \\(f_{1:t}\\) represent the states, intention, and feedback up to time step t, respectively. These embeddings are processed by encoders \\(h_t = Encoder(s_t, I_t, f_{1:t})\\) and then by cross-attention layers and convolutional feature extractors: \\(e_t = Conv(Attn(h_t, V)) + P_t\\) over vocabulary V. Here, \\(P_t\\) includes agent feedback to enhance grounding accuracy based on coordination signals for more accurate contextual understanding. The module maps language symbols to physical environment representations through:\n\\[g(x) = f_{\\theta} \\sum_{i=1}^{N} w_i g(x_i),\\]\nwhere \\(g(x)\\) is the grounded embeddings of policy set x and \\(g(x_i)\\) represents its individual action"}, {"title": "Cooperative Execution Module", "content": "Cooperative Execution Module comprises N specialized agents \\(\\{A_1, ..., A_N\\}\\). This architecture avoids using a single agent to handle all tasks. Instead, each agent is dedicated to a distinct semantic domain, cultivating expertise specific to that domain. For instance, agents \\(A_1\\), \\(A_2\\), and \\(A_3\\) may be dedicated to query processing, information retrieval, and arithmetic operations, respectively. This specialization promotes an efficient distribution of tasks and reduces overlap in capabilities.\nDecomposing skills into specialized agents risks creating isolated capabilities that lack coordination. To address this, it is essential that agents not only excel individually but also comprehend the capacities and limitations of their peers. We propose an integrated training approach where specialized agents are trained simultaneously to foster collaboration and collective intelligence. We represent the parameters of agent \\(A_i\\) as \\(\\theta_i\\). Each agent's policy, denoted as \\(y_i \\sim \\pi_{\\theta}(\\cdot|s)\\), samples an output \\(y_i\\) from a given input state s. The training objective for our system is defined by the following equation:\n\\[L_{exe} = \\sum_{i=1}^{N} E_{(s, y^*)\\sim D} l(\\pi_{\\theta_i} (y_i|s), y^*),\\]\nwhere l() represents the task-specific loss function, comparing the agent-generated output \\(y_i\\) with the ground-truth label \\(y^*\\). D denotes the distribution of training data. By optimizing this objective collectively across all agents, each agent not only improves its own output accuracy but also enhances the overall team's ability to produce coherent and well-coordinated results.\nDuring training, we adjust the decomposition of grounding tasks to enhance collaboration, which is represented by the soft module weights \\(\\{W_1, ..., w_n\\}\\). These weights indicate how the distribution of grounding commands can be optimized to better utilize the capabilities of different agents. The objective of this training is defined by the following loss function: \\(L_{com} = l(d, w^*)\\), where l represents the loss function, d is expressed as subgoal task instruction data, and \\(w^*\\) signifies the optimal set of weights."}, {"title": "3 Approach", "content": "The collaborative MARL of REMALIS focuses on three key points: intention propagation for grounding, bidirectional coordination channels, and integration with recursive reasoning agents. Detailed parameter supplements and pseudocode details can be found in Appendix C and Appendix F."}, {"title": "3.1 Planning with Intention Propagation", "content": "We formulate a decentralized, partially observable Markov game for multi-agent collaboration. Each agent i maintains a private intention \\(Z_i\\) encoded as a tuple \\(L_i = (\\gamma_i, \\Sigma_i, \\pi_i, \\delta_i)\\), where \\(\\gamma_i\\) is the current goal, \\(\\Sigma_i = \\{\\sigma_{i1}, \\sigma_{i2}, ...\\}\\) is a set of related sub-goals, \\(\\pi_i(\\sigma)\\) is a probability distribution over possible next sub-goals, and \\(\\delta_i(\\sigma)\\) is the desired teammate assignment for sub-goal \\(\\sigma\\).\nIntentions are propagated through a communication channel \\(f_{\\Lambda}\\) parameterized by A. For a received message \\(m_{ij}\\) from agent j, agent i infers a belief over teammate j's intention \\(b_i(I_j|m_{ij}) = f_{\\Lambda}(m_{ij})\\), where A is a recurrent neural network. The channel \\(f_{\\theta}\\) is trained in an end-to-end manner to maximize the coordination reward function \\(R_c\\). This propagates relevant sub-task dependencies to enhance common grounding on collaborative goals.\n\\[\\Lambda^* = arg\\ max E_{z, m \\sim f_{\\Lambda}} [R_c(I, m)].\\]\nAt each time-step t, the LLM witll processinputs comprising the agent's state \\(s_t\\), the intention \\(I_t\\), and the feedback \\(f_{1:t}\\)."}, {"title": "3.2 Grounding with Bidirectional Coordination Channels", "content": "The execution agent policies, denoted by \\(\\pi_{\\xi_i}(a_i|s_i, Z_i)\\), are parameterized by \u0121 and conditioned on the agent's state \\(s_i\\) and intention \\(I_i\\). Emergent coordination patterns are encoded in a summary statistic \\(c_t\\) and passed to upstream modules to guide planning and grounding adjustments. For example, frequent miscoordination on sub-goal \\(\\sigma\\) indicates the necessity to re-plan \\(\\sigma\\) dependencies in I.\nThis bidirectional feedback aligns low-level execution with high-level comprehension strategies. In addition to the downstream propagation of intents, execution layers provide bidirectional feedback signals \\(\\Phi(t)\\) to upstream modules \\(r(t) = P(h_{exec}):\n\\[h^{exec} = [\\Phi_1(o_1),...,\\Phi_N(o_N)],\\]\nwhere \\(\\Phi(. )\\) aggregates agent encodings to summarize emergent coordination, and \\(\\phi_i(\\cdot)\\) encodes the observation o for agent i.\nExecution agents generate feedback \\(f_t\\) to guide upstream LLM modules through: \\(f_t = g_{\\theta}(T_{1:t})\\), where \\(g_{\\theta}\\) processes the action-observation history"}, {"title": "3.3 Execution: Integration with Reasoning Agents", "content": "3.3.1 Agent Policy Generation\nWe parameterize agent policies \\(\\pi_{\\theta}(a_t|s_t, I_t, C_{1:t})\\) using an LLM with weights \\(\\theta\\). At each time step, the LLM takes as input the agent's state \\(s_t\\), intention \\(I_t\\), and coordination feedback \\(C_{1:t}\\). The output is a distribution over the next actions \\(a_t\\):\n\\[\\pi_{\\theta}(a_t|S_t, I_t, C_{1:t}) = LLM_{\\theta}(s_t, I_t, C_{1:t}).\\]"}, {"title": "3.3.2 Grounding Strategy Adjustment", "content": "We model action dependencies using a graph neural policy module \\(h^i = GNN(s_t, a)\\), where h models interactions between action a and the state \\(s_t\\). The policy is then given by \\(\\pi_{\\theta}(a_t|s_t) = \\prod_{i=1}^{|H|} h^i_{a_i}\\). This captures the relational structure in the action space, enabling coordinated action generation conditioned on agent communication.\nThe coordination feedback \\(c_t\\) is used to guide adjustments in the grounding module's strategies. We define a grounding confusion matrix \\(C_t\\), where \\(C_t(i, j)\\) represents grounding errors between concepts i and j. The confusion matrix constrains LLM grounding as:\n\\[f_{\\phi}(s_t, I_t) = LLM_{\\phi}(s_t, I_t) \\odot AC_t\\]\nwhere \\(\\odot\\) is element-wise multiplication and \\(\\Lambda\\) controls the influence of \\(C_t\\), reducing uncertainty on error-prone concept pairs.\nWe propose a modular regularization approach, with the grounding module \\(g_{\\phi}\\) regularized by a coordination confusion estimator:\n\\[L_{confusion} = \\frac{1}{T} \\sum_{i,j} \\Lambda(c_i, c_j) \\cdot Conf(c_i, c_j)\\]\nwhere Ltask is the task reward, \\(Conf(c_i, c_j)\\) measures confusion between concepts \\(c_i\\) and \\(c_j\\), and \\(\\Lambda_y(c_i, c_j)\\) are attention weights assigning importance based on grounding sensitivity.\nAn episodic confusion memory \\(M_t\\) accumulates long-term grounding uncertainty statistics:\n\\[M_t(i, j) = M_{t-1}(i, j) + I(Confuse(c_i, c_j)_t),\\]\nwhere I() are indicator functions tracking confusion events. By regularizing with a coordination-focused confusion estimator and episodic memory, the grounding module adapts to avoid miscoordination."}, {"title": "3.4 Collective Learning and Adaptation", "content": "The coordination feedback signals \\(c_t\\) and interpretability signals \\(E_t, U_t, R_t\\) play a crucial role in enabling the LLM agents to adapt and learn collectively. By incorporating these signals into the training process, the agents can adjust their strategies and policies to better align with the emerging coordination patterns and requirements of the collaborative tasks.\nThe collective learning process can be formalized as an optimization problem, where the goal is to minimize the following objective function \\(L(\\eta, \\gamma,\\zeta, \\xi) = E_{s_t, I_t, f_{1:t}} [\\alpha U_t + \\beta E_t - R] + \\Omega(\\eta, \\gamma,\\zeta, \\xi)\\). Here, \\(\\alpha\\) and \\(\\beta\\) are weighting factors that balance the contributions of the grounding uncertainty \\(U_t\\) and coordination errors \\(E_t\\), respectively. The team reward R is maximized to encourage collaborative behavior. The term \\(\\Omega(\\eta, \\gamma, \\zeta, \\xi)\\) represents regularization terms or constraints on the model parameters to ensure stable and robust learning.\nThe objective function L is defined over the current state \\(s_t\\), the interpretability signals \\(I_t = \\{E_t, U_t, R_t\\}\\), and the trajectory of feedback signals \\(f_{1:t} = \\{C_1, Z_1, ..., c_t, I_t\\}\\) up to the current time step t. The expectation \\(E_{s_t, I_t, f_{1:t}}[]\\) is taken over the distribution of states, interpretability signals, and feedback signal trajectories encountered during training."}, {"title": "4 Experiments", "content": "4.1 Datasets\nTo assess the performance of our models, we conducted evaluations using two large-scale real-world datasets: the traffic flow prediction (TFP) dataset and the web-based activities dataset.\nTFP dataset comprises 100,000 traffic scenarios, each accompanied by corresponding flow outcomes. Each example is detailed with descriptions of road conditions, vehicle count, weather, and traffic control measures, and is classified as traffic flow: smooth, congested, or jammed. The raw data was sourced from traffic cameras, incident reports, and simulations, and underwent preprocessing to normalize entities and eliminate duplicates.\nWeb activities dataset contains over 500,000 examples of structured web interactions such as booking flights, scheduling appointments, and making reservations. Each activity follows a template with multiple steps like searching, selecting, filling forms, and confirming. User utterances and system responses were extracted to form the input-output pairs across 150 domains, originating from real anonymized interactions with chatbots, virtual assistants, and website frontends."}, {"title": "4.2 Implementation Details", "content": "To handle the computational demands of training our framework with LLMs, we employ 8 Nvidia A800-80G GPUs (Chen et al., 2024) under the DeepSpeed (Aminabadi et al., 2022) training framework, which can effectively accommodate the extensive parameter spaces and activations required by our framework's LLM components and multi-agent architecture (Rasley et al., 2020).\nFor the TFP dataset, we classified the examples into four difficulty levels: \u201cEasy\u201d, \u201cMedium\u201d, \"Hard\", and \"Hell\". The \u201cEasy\u201d level comprises small grid networks with low, stable vehicle arrival rates. The \"Medium\u201d level includes larger grids with variable arrival rates. \"Hard\" tasks feature large, irregular networks with highly dynamic arrival rates and complex intersection configurations. The \"Hell\" level introduces challenges such as partially observable states, changing road conditions, and fully decentralized environments.\nFor the web activities dataset, we divided the tasks into \"Easy\u201d, \u201cMedium\u201d, \u201cHard\u201d, and \u201cAll\" levels. \"Easy\" tasks required basic single-click or short phrase interactions. \u201cMedium\u201d involved complex multi-page sequences like form submissions. \"Hard\u201d tasks demanded significant reasoning through ambiguous, dense websites. The \u201cAll\u201d level combined tasks across the full difficulty spectrum.\nThe dataset was divided into 80% for training, 10% for validation, and 10% for testing, with examples shuffled. These large-scale datasets offer a challenging and naturalistic benchmark to evaluate our multi-agent framework on complex, real-world prediction and interaction tasks."}, {"title": "4.3 Results and Analysis", "content": "Table 1 displays the principal experimental results of our REMALIS framework in comparison with various single-agent baselines and contemporary methods using the web activities dataset. We evaluated the models across four levels of task difficulty: \"Easy\u201d, \u201cMedium\u201d, \u201cHard\u201d, and \u201cAll\u201d.\nThe results from our comparative analysis indicate that REMALIS (7B), equipped with a 7B parameter LLM backbone, significantly outperforms competing methods. On the comprehensive"}, {"title": "4.4 Ablation Studies", "content": "1)The Impact on Improving Multi-Agent Coordination Accuracy We conduct ablation studies to evaluate the impact of each component within the REMALIS framework. The observations can be"}, {"title": "5 Conclusion", "content": "In this paper, we introduce a novel framework, REMALIS, designed to enhance collaborative capabilities within multi-agent systems using LLMs. Our approach incorporates three principal innovations: intention propagation for establishing a shared understanding among agents, bidirectional coordination channels to adapt reasoning processes in response to team dynamics, and recursive reasoning architectures that provide agents with advanced contextual grounding and planning capabilities necessary for complex coordination tasks. Experimental results indicate that REMALIS significantly outperforms several baseline methods, underscoring the efficacy of cooperative multi-agent AI systems. By developing frameworks that enable LLMs to acquire cooperative skills analogous to human team members, we advance the potential for LLM agents to manage flexible coordination in complex collaborative environments effectively."}, {"title": "6 Limitiation", "content": "While REMALIS demonstrates promising results in collaborative multi-agent tasks, our framework relies on a centralized training paradigm, which may hinder scalability in fully decentralized environments. The current implementation does not explicitly handle dynamic agent arrival or departure during execution, which could impact coordination in real-world applications, the recursive reasoning component may struggle with long-term dependencies and planning horizons beyond a certain time frame."}, {"title": "H Supplementary application description of the overall framework", "content": "To further illustrate the practical applicability and versatility of our proposed REMALIS framework, we present a supplementary application scenario. Figure 2 depicts a high-level overview of how REMALIS can be employed in a real-world setting to tackle complex, multi-step tasks that require orchestrating multiple agents with diverse capabilities. This exemplary use case demonstrates the framework's ability to decompose intricate problems into manageable sub-tasks, dynamically allocate appropriate agents, and seamlessly coordinate their actions to achieve the overarching goal efficiently and effectively."}, {"title": "Grounding Module (Figure 5):", "content": "1. Contextualize the abstract traffic concepts and symbols into grounded representations.\n2. Map entities like intersections, vehicles, and signal phases to their physical counterparts.\n3. Resolve ambiguities and uncertainties in grounding based on the current traffic context.\n4. Adjust grounding strategies based on feedback from execution agents and emerging coordination patterns.\n5. Provide grounded embeddings to inform the execution agents' decision-making."}, {"title": "Execution Module (Figure 6,7):", "content": "1. Specialized agents monitor their respective domains (vehicle counts, road conditions, signal timings, etc.).\n2. Agents communicate their local intentions and goals to relevant teammates.\n3. Agents align their actions based on shared intentions and the coordinated plans.\n4. Agents execute their assigned subtasks (adjusting signal phases, routing emergency vehicles, etc.).\n5. Agents observe the impact of their actions and provide feedback on emerging coordination patterns.\n6. Agents adapt their strategies dynamically based on the feedback and changing traffic conditions.\n7. Agents continuously monitor and respond to fluctuations in vehicle arrival rates and traffic patterns.\n8. Agents collaborate and coordinate their efforts to collectively alleviate congestion and optimize traffic flow."}, {"title": "Planning Module (Figure 4):", "content": "1. Analyze the current traffic conditions, including vehicle counts, road incidents, and construction zones.\n2. Identify intersections experiencing congestion and potential bottlenecks.\n3. Formulate high-level goals to alleviate congestion and optimize traffic flow.\n4. Break down the goals into a sequence of subgoals and subtasks.\n5. Determine the dependencies and coordination needs between subtasks.\n6. Plan the assignment of subtasks to specialized execution agents based on their expertise."}, {"title": "A Related Work", "content": "A.1 Single Agent Frameworks\nEarly agent frameworks such as Progprompt (Singh et al., 2023) directly prompt large language models (LLMs) to plan, execute actions, and process feedback in a chained manner within one model (Song et al., 2023). Despite its conceptual simplicity (Valmeekam et al., 2022), an integrated framework imposes a substantial burden on a single LLM, leading to challenges in managing complex tasks (Raman et al., 2022; Wang et al., 2024a).\nTo reduce the reasoning burden, recent works explore modular designs by separating high-level planning and low-level execution into different modules. For example, LUMOS (Yin et al., 2023) consists of a planning module, a grounding module, and an execution module. The planning and grounding modules break down complex tasks into interpretable sub-goals and executable actions. FiReAct (Chen et al., 2023) introduces a similar hierarchical structure, with a focus on providing step-by-step explanations (Zhang and Gao, 2023). Although partitioning into modules specializing for different skills is reasonable, existing modular frameworks still rely on a single agent for final action execution (Miao et al., 2023; Qiu et al., 2024). Our work pushes this idea further by replacing the single execution agent with a cooperative team of multiple agents.\nA.2 Multi-Agent Reinforcement Learning\nCollaborative multi-agent reinforcement learning has been studied to solve complex control or game-playing tasks. Representative algorithms include COMA (Foerster et al., 2018), QMIX (Rashid et al., 2020) and ROMA (Wang et al., 2020). These methods enable decentralized execution of different agents but allow centralized training by sharing experiences or parameters (Lyu et al., 2021). Drawing on this concept, our REMALIS framework places greater emphasis on integrating modular LLMs to address complex language tasks. In REMALIS, each execution agent specializes in specific semantic domains such as query, computation, or retrieval, and is coordinated through a communication module (Mao et al., 2022).\nThe concept of multi-agent RL has recently influenced the design of conversational agents (Zimmer et al., 2021a; Schumann et al., 2024). Ensemble-Bot (Schuchard and Crooks, 2021) utilizes multiple bots trained on distinct topics, coordinated by a"}, {"title": "Integrated & Collaborative Learning", "content": "A.3 Integrated & Collaborative Learning\nIntegrated learning techniques originate from transfer learning (Zhuang et al., 2020; Zhu et al., 2023), aiming to improve a target model by incorporating additional signals from other modalities (Lotfollahi et al., 2022; Shanahan et al., 2023). For multi-agent systems, (Li et al., 2022; Zhao et al., 2024) find joint training of multiple agents simultaneously boosts performance over separately trained independent agents (Lee and Perret, 2022). Recently, integrated learning has been used in single agent frameworks like (Shen et al., 2020) and (Loey et al., 2021), where auxiliary losses of interpretable outputs facilitate main model training through multi-tasking (Khamparia et al., 2021; Saber et al., 2021).\nOur work adopts integrated learning to train specialized execution agents that are semantically consistent. At the team level, a communication module learns to attentively aggregate and propagate messages across agents, which indirectly coordinates their strategies and behaviors (Fan et al., 2020). The integrated and collaborative learning synergizes individual skills and leads to emerged collective intelligence, enhancing the overall reasoning and planning capabilities when dealing with complex tasks (He et al., 2021; Li et al., 2020).\nB Methodology and Contributions\nBased on the motivations and inspirations above, we propose recursive multi-agent learning with intention sharing framework (REMALIS), an innovative multi-agent framework empowered by integrated learning for communication and collaboration. The main contributions are:\n1. We design a cooperative execution module with multiple agents trained by integrated learning. Different execution agents specialize in different semantic domains while understanding peer abilities, which reduces redundant capacities and improves efficient division of labor.\n2. We propose an attentive communication mod-"}, {"title": "C Key variables and symbols", "content": "Table 4 summarizes the key variables and symbols used in the proposed recursive multi-agent learning framework called REMALIS. It includes symbols representing various components like the planning module, grounding module, execution policies, intentions, goals, sub-goals, and the intention propagation channel.\nD Tasks Setup\nD.1 Traffic Control\nWe define four levels of difficulty for our traffic control tasks: Easy, Medium, Hard, and Hell in Table 5.\nD.2 Web Tasks\nSimilarly, we categorize the web tasks in our dataset into four levels of difficulty: Easy, Medium, Hard, and All.\nEasy: The easy web tasks involve basic interactions like clicking on a single link or typing a short phrase. They require navigating simple interfaces with clear options to reach the goal.\nMedium: The medium-difficulty tasks demand more complex sequences of actions across multiple pages, such as selecting filters or submitting forms. They test the agent's ability to understand the site structure and flow.\nHard: The hard web tasks feature more open-ended exploration through dense sites with am-"}, {"title": "E Experimental Setups", "content": "E Experimental Setups\nIn this study, we compare the performance of several state-of-the-art language models, including REMALIS, LUMOS, AgentLM, and GPT-3.5. These models vary in size, architecture, and training configurations, reflecting the diversity of approaches in the field of natural language processing in Table 6.\nREMALIS is a 7 billion parameter model trained using the AdamW optimizer with a learning rate of 1e-4, a batch size of 32, and no dropout. It has 12 layers, a model dimension of 768, and 12 attention heads. The model was trained for 15 epochs with a warmup period of 1 epoch and a weight decay of 0.01. REMALIS employs a Graph Neural Network (GNN) architecture, which is particularly suited for modeling complex relationships and structures.\nLUMOS, a larger model with 13 billion parameters, was trained using the Adam optimizer with a learning rate of 2e-5, a batch size of 64, and a dropout rate of 0.1. It has 8 layers, a model dimension of 512, and 8 attention heads. The model was trained for 20 epochs with a warmup period of 2 epochs and a weight decay of 0.001. LUMOS follows a Transformer architecture, which has proven effective in capturing long-range dependencies in sequential data.\nAgentLM, a 6 billion parameter model, was trained using the AdamW optimizer with a learning rate of 1e-4, a batch size of 32, and no dropout. It has 6 layers, a model dimension of 768, and 12 attention heads. The model was trained for 10 epochs with a warmup period of 1 epoch and a weight decay of 0.01. AgentLM also uses a Transformer architecture.\nGPT-3.5, the largest model in this study with 175 billion parameters, was trained using the Adam optimizer with a learning rate of 2e-5, a batch size of 64, and a dropout rate of 0.1. It has 48 layers, a model dimension of 1024, and 16 attention heads. The model was trained for 20 epochs with a warmup period of 2 epochs and a weight decay"}, {"title": "F Pseudo-code", "content": "F Pseudo-code\nThis algorithm 2 presents the hierarchical planning and grounding processes in the proposed recursive multi-agent learning framework. The planning module \\(p_{\\theta}\\) takes the current sub-goal \\(s_t\\), intention \\(I_t\\), grounded embedding \\(e_t\\), and feedback \\(f_t\\) as inputs, and predicts the next sub-goal \\(s_{t+1}\\). It first encodes the inputs using an encoder, and then passes the encoded representation through a graph neural network \\(T_{\\theta}\\) parameterized by 0. The output of \\(T_{\\theta}\\) is passed through a softmax layer to obtain the probability distribution over the next sub-goal.\nThe grounding module \\(g_{\\phi}\\) takes the current state \\(S_t\\), intention \\(I_t\\), and feedback trajectory \\(f_{1:t}\\) as inputs, and produces the grounded embedding \\(e_t\\). It encodes the inputs using an encoder, and then applies cross-attention over the vocabulary V, followed by a convolutional feature extractor. The output is combined with agent feedback \\(P_t\\) to enhance the grounding accuracy. The grounding module is parameterized by \\(\\phi\\)."}, {"title": "4.  ", "content": "This algorithm 3 describes the intention propagation mechanism in the proposed recursive multi-agent learning framework. The goal is for each agent i to infer a belief \\(b_i(I_j|m_{ij})\\) over the intention \\(I_j\\) of a teammate j, given a message \\(m_{ij}\\) received from j."}, {"title": "4.  Algorithm 2 Hierarchical Planning and Grounding", "content": "1: Input: Current sub-goal st, intention It, grounded embedding et, feedback ft\n2: Output: Next sub-goal st+1\n3: ht = Encoder(st, It, et, ft) {Encode inputs}\n4: St+1 = Softmax(To(ht)) {Predict next subgoal}"}, {"title": "5: Te is a graph neural network parameterized by\n0 {Planning module pe }", "content": "5: Te is a graph neural network parameterized by\n0 {Planning module pe }\n6: Input: Current state st, intention It, feedback f1:t\n7: Output: Grounded embedding et\n8: ht = Encoder(st, It, f1:t) {Encode inputs}\n9: et = Conv(Attn(ht, V)) + Pt {Grounded embedding}"}, {"title": "10: Attn(,) is a cross-attention layer over vocabulary V", "content": "10: Attn(,) is a cross-attention layer over vocabulary V\n11: Conv(\u00b7) is a convolutional feature extractor\n12: Pt includes agent feedback to enhance grounding accuracy\n13: go is the grounding module parameterized by \u03a6"}, {"title": "It initializes an intention propagation channel\nf\u0245, parameterized by A, which is implemented as\na recurrent neural network.", "content": "It initializes an intention propagation channel\nf\u0245, parameterized by A, which is implemented as\na recurrent neural network."}, {"title": "The intention inference process works as follows:", "content": "The intention inference process works as follows:\n1. The received message mij is encoded using an encoder to obtain a representation hij.\n2. The encoded message hij is passed through the propagation channel f\u0245 to infer the belief bi(Ij|mij) over teammate j's intention Ij."}, {"title": "The objective is to train the parameters A of the propagation channel f\u2081 to maximize the coordination reward Re over sampled intentions I and messages m from the distribution defined by fa.", "content": "The objective is to train the parameters A of the propagation channel f\u2081 to maximize the coordination reward Re over sampled intentions I and messages m from the distribution defined by fa."}, {"title": "4.  Algorithm 3 Intention Propagation Mechanism", "content": "4.  Algorithm 3 Intention Propagation Mechanism\nRequire: Current intention I\u2081 of agent i, message mij from teammate j\nEnsure: Belief bi(Ij|mij) over teammate j's intention Ij\n1: Initialization:"}, {"title": "6: Intention Inference:", "content": "6: Intention Inference:\n5: Encode message: hij \u2190 Encoder(mij)\n6: Infer intention belief: bi(Ij|mij) \u2190 f\u0245(mij)\n7: Objective:"}, {"title": "This algorithm 4 describes the bidirectional```json\ng and accurate text. All of those sentences are the model", "The full response is": "content\": \"tion mechanism in the proposed recursive"}]}