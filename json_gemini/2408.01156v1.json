{"title": "TCR-GPT: Integrating Autoregressive Model and Reinforcement Learning for T-Cell Receptor Repertoires Generation", "authors": ["Yicheng Lin", "Dandan Zhang", "Yun Liu"], "abstract": "T-cell receptors (TCRs) play a crucial role in the immune system by recognizing and binding to specific antigens presented by infected or cancerous cells. Understanding the sequence patterns of TCRs is essential for developing targeted immune therapies and designing effective vaccines. Language models, such as auto-regressive transformers, offer a powerful solution to this problem by learning the probability distributions of TCR repertoires, enabling the generation of new TCR sequences that inherit the underlying patterns of the repertoire. We introduce TCR-GPT, a probabilistic model built on a decoder-only transformer architecture, designed to uncover and replicate sequence patterns in TCR repertoires. TCR-GPT demonstrates an accuracy of 0.953 in inferring sequence probability distributions measured by Pearson correlation coefficient. Furthermore, by leveraging Reinforcement Learning(RL), we adapted the distribution of TCR sequences to generate TCRs capable of recognizing specific peptides, offering significant potential for advancing targeted immune therapies and vaccine development. With the efficacy of RL, fine-tuned pretrained TCR-GPT models demonstrated the ability to produce TCR repertoires likely to bind specific peptides, illustrating RL's efficiency in enhancing the model's adaptability to the probability distributions of biologically relevant TCR sequences.", "sections": [{"title": "Introduction", "content": "T-cell receptors (TCRs) are integral components of the immune system, playing a crucial role in recognizing and responding to antigens. Each TCR is composed of unique sequences of amino acids, enabling T cells to identify specific pathogens and infected cells. The diversity and specificity of TCR sequences are vital for the adaptive immune response, allowing the immune system to target a vast array of antigens with precision (Singh, N. K., et al. 2017; van der Merwe, P. A., and O. Dushek 2011).Understanding the repertoire of TCR sequences is essential for advancing immunological research and developing targeted therapies.\nText-generation models, inspired by natural language processing techniques (Chung, Junyoung, et al. 2014; Hochreiter, Sepp, and J\u00fcrgen Schmidhuber 1997; Vaswani, Ashish, et al. 2017), can potentially offer a transformative approach to this task. By building a probability distribution over TCR sequences, and even more, tailoring this distribution to generate TCR sequences of specific desired functionality, text-generation models can not only enhance our understanding of TCR diversity but also has practical applications in designing vaccines and immunotherapies.\nIn the realm of auto-regressive text generation, decoder-only transformers have become a cornerstone technology. This architecture processes input sequences to generate text in an auto-regressive manner, predicting the next token in a sequence based on the tokens it has previously generated. At each step, the model takes the current sequence of tokens as input, processes it through multiple layers of self-attention and feed-forward networks, and produces a probability distribution over the next possible tokens. The model then samples from this distribution to select the next token, appends it to the input sequence, and repeats the process until the desired output length is achieved or a stopping condition is met. Decoder-only transformers like GPT (Generative Pre-trained Transformer) (Brown, Tom B., et al. 2020; Radford, Alec, et al. 2018, 2019) have demonstrated remarkable performance in various natural language processing tasks, from completing sentences and paragraphs to generating entire articles or dialogues, thus setting new benchmarks for text generation capabilities.\nReinforcement Learning (RL) involves training agents to maximize long-term rewards by interacting with complex environments. This method has been applied across diverse fields such as game playing (Souchleris, Sidiropoulos, and Papakostas 2023), robotics (Zhang and Mo 2021), drug discovery (Bilodeau et al. 2022; Du et al. 2022; Fromer and Coley 2023; Luukkonen et al. 2023; Tang, Ewalt, and Ng 2021), and more. Reinforcement learning has also emerged as an effective alternative training approach to enhance the performance of generative models, offering adaptable goals via its reward function, unlike the fixed distribution modeling objectives found in both supervised and unsupervised learning (Cao et al. 2023).\nIn this study, we introduce TCR-GPT, a probabilistic model based on a decoder-only transformer architecture designed to capture underlying sequence patterns in TCR repertoires and hence generate TCR sequences based on the learned probability distribution. And through reinforcement learning, we tailored the distribution of TCR sequences to generate TCRs that can recognize specific peptides, which holds great promise for advancing targeted immune therapy and vaccine design. TCR-GPT demonstrates greater ac-"}, {"title": "Related work", "content": "Traditional methods for modeling TCR sequence patterns calculate the probability of a TCR sequence as the product of the selection factor and the generation probability, which are inferred separately from the processes of TCR selection and generation. A notable example of this approach is soNNia (Isacchini, G., et al. 2021).\nTrials have also been conducted using variational autoencoders (VAEs) parameterized by deep neural networks to model TCR repertoires (Davidsen, K., et al. 2019). However, it requires TCR sequences to be padded to a fixed length, which can introduce noise into the original data and obscure valuable information about the diversity of sequence lengths.\nInspired by natural language processing and autoregressive text generation models, TCRpeg (Jiang, Y., and S. C. Li 2023) employs an auto-regressive framework to infer the probability of TCR sequences in an end-to-end manner. TCRpeg utilizes a deep autoregressive architecture with gated recurrent unit (GRU) layers (Chung, Junyoung, et al. 2014)to effectively model the probability of TCR sequences, offering a streamlined alternative to the above trials.\nTransformer models, introduced by Vaswani et al. in 2017, have revolutionized the field of natural language processing (NLP) (Vaswani, Ashish, et al. 2017). Unlike previous sequence-to-sequence models that relied heavily on recurrent neural networks (RNNs) (Chung, Junyoung, et al. 2014; Hochreiter, Sepp, and J\u00fcrgen Schmidhuber 1997; Schulman, John, et al. 2017)and convolutional neural networks (CNNs) (O'Shea and Nash 2015), transformers leverage self-attention mechanisms to process input data in parallel, allowing for greater efficiency and scalability. The core innovation of the transformer architecture is its ability to capture long-range dependencies within the data, which is crucial for understanding and generating coherent text. A notable example that combines transformer and autoregressive models is the GPT (Generative Pre-trained Transformer) series (Brown, Tom B., et al. 2020; Radford, Alec, et al. 2018, 2019). These models generate text by predicting each word or token based on the preceding sequence, utilizing the attention mechanism inherent in transformer decoders to create coherent and contextually relevant text.\nReinforcement learning has demonstrated its efficacy as a robust method for training generative models, especially in NLP tasks (Cao et al. 2023). Reinforcement Learning with Human Feedback (RLHF) (Bai et al. 2022; Shi et al. 2018; Gao, Meyer, and Gurevych 2020; Kreutzer, Riezler, and Lawrence 2020; Ouyang et al. 2022) is a representative technique where reinforcement learning is used to fine-tune language models based on human-provided feedback. In the context of large language models (LLMs) like GPT, RLHF helps improve the model's performance by aligning its outputs with human preferences, leading to more contextually appropriate responses."}, {"title": "Methods", "content": "Given the critical role of the CDR3-\u03b2 chain in TCR antigen recognition (Singh, N. K., et al. 2017; van der Merwe, P. A., and O. Dushek 2011), we utilized the CDR3-\u03b2 sequence as a representation of the entire TCR. We developed an autoregressive model, TCR-GPT, to estimate the probability of a TCR sequence x as p(x|\u03b8), where \u03b8 represents the parameters of a decoder-only transformer model. This probability p(x|\u03b8) is computed by the autoregressive likelihood, which involves calculating the product of conditional probabilities over sequential residues of length L:\n$$p (x|\\theta) = p (x_1|\\theta) \\prod_{i=2}^{L}P (x_i|x_1, x_2, ..., x_{i-1}; \\theta)$$\nOur model employs a decoder-only transformer equipped with attention mechanisms to capture interactive relationships among residues, thereby formulating the autoregressive likelihood efficiently. After embedding layers transform the discrete representation of each amino acid in batches of TCR sequences into continuous vectors of dimensionality 32, a total of 8 attention heads, each with 6 self-attention layers, collectively capture the features. Specifically, let $X \\in [0, 1, 2, ..., 22]^{B \\times L}$ denote a batch of B TCRs of maximum length L. Each element in X is an integer representing an amino acid (a total of 20 different amino acids), as well as special tokens such as < SOS > (Start of Sequence), < EOS > (End of Sequence), and padding token. After embedding, X is transformed to a continuous tensor $E \\in R^{B \\times L \\times d}$ where each vector of dimension d = 32 represents the embedding of a single token. The multi-head attention layers, along with the linear layers after multi-head feature concatenation then transform E to features $Z \\in R^{B \\times L \\times d}$. These features are then processed by a position-wise feedforward network to output the probability distribution over all 20 possible amino acids at each position. We set the negative log likelihood as the loss function as the following:\n$$L = -\\sum_{n=1}^{N}\\sum_{i=1}^{L}logp_i$$\nN is the batch size and $p_i$ is the output probability of the model on the right amino acid in i-th amino acid position."}, {"title": "Jensen-Shannon Divergence", "content": "We use Jensen-Shannon divergence (Djs(r\u00b2, ri)) to measure the ability of the trained model to distinguish two subrepertoires r\u00b2 and ri, which is formulated as:\n$$D_{JS} (r^i, r^j) = \\frac{1}{2}KL(P_{infer}^i||M) + \\frac{1}{2}KL(P_{infer}^j||M)$$\nwhere $P_{infer}^i$ and $P_{infer}^j$ are probabilities computed from TCR-GPT trained on sub-repertoires $r^i$ and $r^j$, $M = \\frac{1}{2}(P_{infer}^i + P_{infer}^j)$, and $KL(P||Q)$ denotes the Kullback-Leibler divergence. A higher $D_{JS} (r^i, r^j)$ indicates greater discrepancy between sub-repertoires.\nFor the scenario where we utilize Jensen-Shannon divergence to assess the difference of the inferred likelihood from the model $P_{infer}(x)$ and the observed frequencies $P_{data}(x)$, we simply calculate $D_{JS}(P_{infer}, P_{data})$ as the following:\n$$D_{JS}(P_{infer}, P_{data}) = \\frac{1}{2}KL(P_{infer}(x)||M) +KL(P_{data}(x)||M)$$\nwhere $M = \\frac{1}{2}(P_{infer}(x) + P_{data}(x))$."}, {"title": "Using TCR-GPT to generate TCR sequences", "content": "With the TCR-GPT model trained, we employed a straightforward sampling method to generate new TCR sequences. Initially, we input the start token (< SOS >) into the model and then sampled the amino acid for the next position from the resulting probability distribution. This sampled amino acid was appended to the previous tokens, and the process was repeated iteratively to generate each subsequent amino acid. This approach can be summarized by the following formula:\n$$A_t = P(A_t|A_{0:t-1}; \\theta)$$\nHere, $A_t$ represents the amino acid token at position t, and \u03b8 denotes the parameters of the TCR-GPT model. The generation process concludes when the < EOS > (End Of Sequence) token is generated. At this point, all the generated amino acids are collected to form the complete TCR sequence."}, {"title": "Using the features from TCR-GPT for classification tasks", "content": "Upon training the TCR-GPT model, the features produced by the multi-head attention module can encode valuable information from the sequence input. We developed an additional classification network that utilizes these features for a downstream classification task. Specifically, let $X \\in [0, 1, 2, ..., 22]^{B \\times L}$ represent a batch of B TCRs with a maximum length of L. The features extracted from the multi-head attention module are denoted as $Z \\in R^{B \\times L \\times d}$, where the feature of the amino acid at each position in the L-length sequence is a d-dimensional tensor. We then flattened the feature Z to $Z_{flatten} \\in R^{B \\times q}$, where each TCR sequence is represented as a q-dimensional tensor. We constructed a fully connected neural network for each classification task, which consists of three linear layers that can be formulated as follows:\n$$Z_0 = Z_{flatten}$$\n$$Z_l = \\sigma(W_lZ_{l-1} + b_{l-1}), l \\in \\{1,2\\}$$\n$$P = softmax(W_3Z_2+b_3)$$\nhere, $W_i$ and $b_i$ represent the parameters of the l-th layer of the fully connected neural network, and o denotes the activation function. The final layer, combined with the softmax operation, produces the predicted probability distribution of the classes."}, {"title": "Fine-tuning TCR-GPT with RL to generate peptide-specific TCR repertoires", "content": "To generate TCR repertoires capable of binding to specific peptides, we employ peptide-specific RL to integrate knowledge from high-performance peptide-TCR binding predictors into our pretrained TCR-GPT model. PanPep (Gao, Yicheng, et al. 2023), a versatile and robust model for evaluating peptide-TCR binding affinity, leverages meta-learning and neural Turing machine concepts to achieve superior performance in both few-shot and zero-shot scenarios. We adopt PanPep as the reward model in our reinforcement learning framework. During the reinforcement learning iteration, PanPep rates the generated TCR sequences from TCR-GPT based on their binding affinity to the given peptide.\nSpecifically, with given peptide denoted as s, let the reward for the generated TCR sequences from the TCR-GPT model be denoted as $F_R(s, X)$. Here, PanPep serves as the reward function $F_R$ for the generated TCR sequences. We constructed another transformer network parameterized by k at the k-th iteration as the critic $V_{\\theta_k}$, which shares the same architecture as the TCR-GPT model (the actor in this scenario). However, the critic's output is a single value representing the predicted return for each TCR sequence entry. Using PPO (Schulman, John, et al. 2017)as the reinforcement learning algorithm, we formulate the objective function as\n$$L (s, X, \\theta_k, \\theta) = min(\\frac{p_{\\theta} (X|s)}{p_{\\theta_k} (X|s)} A^{p_{\\theta_k}}(s, X), g(\\epsilon, A^{p_{\\theta_k}} (s, X)))$$\nwhere\n$$g(\\epsilon, A) = \\begin{cases}(1+\\epsilon)A, A \\geq 0\\\\(1 - \\epsilon)A, A < 0\\end{cases}$$\n$$A^{p_{\\theta_k}} (s, X) = F_R (s, X) \u2013 V_{\\theta_k} (X)$$\nThe advantage $A^{p_{\\theta_k}} (s, X)$ is the difference between reward $F_R (s, X)$ and the value $V_{\\theta_k} (X)$, the predicted reward of TCR sequence X by the critic network parameterized by in the k-th iteration. \u20ac is a hyperparameter that we set at 0.2 as used in the previous paper (Schulman, John, et al. 2017). At k-th iteration, we update the parameters of TCR-GPT, and the parameters of the crtic in the following formula:\n$$\\theta_{k+1} = arg \\max_{\\theta} \\frac{1}{|D_k|}\\sum_{X \\in D_k} L (s, X, \\theta_k, \\theta)$$\n$$\\varphi_{k+1} = arg \\min_{\\phi} \\frac{1}{|D_k|}\\sum_{X \\in D_k} [V_{\\varphi_k} (X) \u2013 F_R(s, X)]^2$$\nWhere $D_k$ is a batch of TCR sequences collected using TCR-GPT model during the k-th iteration. The peptide-specific RL workflow can be visualized in Figure 1B."}, {"title": "Experiments", "content": "To evaluate the performance of TCR-GPT, we first applied it to infer the probability distribution of TCR-8 CDR3 sequences and compared its performance with two other algorithms, soNNia (Isacchini, G., et al. 2021)and TCRpeg (Jiang, Y., and S. C. Li 2023). We constructed a universal TCR repertoire by pooling the CDR3 sequences from a large cohort of 743 individuals (Emerson, R. O., et al. 2019). This comprehensive dataset provided a robust foundation for training and testing our model as well as the comparative algorithms. To assess the accuracy of these three methods, we randomly divided the universal repertoire into training and testing sets using a 50:50 ratio, the same strategy employed by TCRpeg. This approach ensured a fair and consistent comparison across the different models.\nWe measured the concordance between the inferred (Pinfer(x)) and the actual probability distributions (Pdata(x)) by Pearson correlation coefficients. TCR-GPT demonstrated greater concordance, achieving a Pearson correlation coefficient of r = 0.953, compared to soNNia (r = 0.673) and TCRpeg (r = 0.932) (Figure 2A-C). These results indicate that TCR-GPT has the highest accuracy among the three methods, as reflected by its closer alignment with the actual probability distributions.\nIn addition to Pearson correlation coefficients, we utilized the Jessen-Shannon divergency (Djs, Methods) to assess the difference between Pinfer(x) and Pdata(x). The Djs statistics provides a measure of divergence between two probability distributions, with a higher Djs indicating a greater difference, and consequently lower model accuracy. The Djs statistic further confirmed the higher accuracy of TCR-GPT, which had a Djs value of 0.031. In contrast, soNNia and TCRpeg had Djs values of 0.131 and 0.039, respectively. These results underscore the effectiveness of TCR-GPT in accurately inferring the probability distribution of TCR-B CDR3 sequences.\nAccording to the comparisons above, we conclude that TCR-GPT outperformed soNNia and TCRpeg in both measures of accuracy, making it the most reliable method among the three for inferring the probability distribution of TCR-B CDR3 sequences. The superior performance of TCR-GPT, as evidenced by both Pearson correlation coefficients and Jensen-Shannon divergence, highlights its potential as a powerful tool for TCR repertoire analysis."}, {"title": "TCR-GPT captures specific features of TCR repertoires efficiently", "content": "We further investigated TCR-GPT's ability to learn the specific probability distributions of different TCR repertoires to compare their properties from a probabilistic perspective. CD4 and CD8 T cells have distinct roles in adaptive immunity across various tissues, which may be reflected by their unique TCR repertoires. Utilizing a dataset (Jiang, Y., and S. C. Li 2023; Seay, H. R., et al. 2016)comprising three different cell types (CD8 cytotoxic T lymphocytes(CD8+), CD4 T help cells(Tconv) and CD4 Treg cells(Treg)) collected from three tissues (spleen, pancreatic draining lymph nodes [PLN] or inguinal \u2018irrelevant' lymph nodes [iLN]), we compared the discrepancy among these nine sub-repertoires.\nWe used TCR-GPT to construct a probabilistic generative model for each of these nine TCR sub-repertoires. For each sub-repertoire, we inferred its probability using nine models: one trained from its own TCR repertoire (denoted as P) and eight from the remaining TCR repertoires (denoted as Qs). To estimate the distance between each pair of sub-repertoires, we calculated the Jessen-Shannon divergency (Djs) for P and each Q. The Djs matrix revealed an interesting pattern: sub-repertoires of the same cell type were more similar to each other than to those from different cell types, and CD4 T cell subtypes exhibited greater similarity to each other than to CD8 T cells (Figure 3).\nGiven the fundamental role of the CDR3 sequence in determining T-cell function through antigen recognition (PMID: 28923982), these results suggest that the same cell subtypes with similar CDR3 repertoire performed analogous functions across different tissues.\nMoreover, the higher similarity observed among CD4 T cell subtypes compared to CD8 T cells aligns with the distinct functional roles that CD4 and CD8 T cells play in the immune system. CD4 T cells are primarily involved in helper or regulator functions, facilitating the activation or inhibition and coordination of other immune cells, while CD8 T cells are chiefly responsible for cytotoxic activities, directly targeting and eliminating infected or malignant cells. This probabilistic comparison of TCR repertoires across different cell types and tissues enhances our understanding of the adaptive immune system's complexity. It provides insights into how specific TCR repertoires are tailored to meet the unique functional demands of different T cell subtypes in various tissue environments. Such knowledge is invaluable for advancing immunotherapy and vaccine development, where targeted manipulation of TCR repertoires could lead to more effective and precise treatments."}, {"title": "Classification of cancer-associated TCRs and SARS-CoV-2 epitope-specific TCRs using features from TCR-GPT", "content": "During the process of TCR probability inference, TCR-GPT yields features for each TCR sequence. Unlike predefined embeddings, the features provided by TCR-GPT are generated in a learnable manner, allowing the model to adaptively capture the underlying patterns within the TCR sequences.\nThe Uniform Manifold Approximation and Projection (UMAP) dimensionality reduction applied to the feature space revealed explicit clusters in the 2D map (Figure 4A, B). Sequences within the same cluster shared a similar motifs, suggesting that the features generated by TCR-GPT could accurately represent the TCR sequences.\nTo further illustrate the effectiveness behind TCR-GPT's features of TCRs, we performed classification(Methods) of caTCRs and SARS-TCRs from control (negative) TCRs sampled from the universal TCR repertoire mentioned above. We extensively compared TCR-GPT with TCRpeg in both caTCR and SARS-TCR classification tasks. Using five-fold cross-validation, TCR-GPT exhibited more stable and accurate performance than TCRpeg in caTCRs classification task, achieving Aera Under the Curve (AUC) values of 0.895\u00b10.005 for TCR-GPT and 0.863\u00b10.012 for TCRpeg (Figure 4C).\nThe results from the SARS-TCR classification task also support the superior performance of TCR-GPT, with AUCs of 0.862\u00b10.030 compared to 0.847\u00b10.027 for TCRpeg (Figure 4D). The consistent performance of TCR-GPT across different classification tasks indicates its robustness and generalizability in modeling TCR sequences. The higher AUC values for TCR-GPT suggest that it can more effectively capture the relevant features that distinguish specific TCRs from the general repertoire, thereby enhancing its predictive accuracy."}, {"title": "Generating peptide-specific TCRs using TCR-GPT fine-tuned with RL", "content": "Using TCR-GPT trained with universal TCR sequences, we address a more practical scenario. For a specific peptide, we employ RL to fine-tune the learned distribution, adapting it from universal TCR sequences to those that can bind to the specific peptide(Methods). PanPep (Gao, Yicheng, et al. 2023)can output binding score ranges from 0 to 1 that indicate the binding probability of the given peptide and TCR. Based on this, we defined the binding percentage as the proportion of peptides generated by TCR-GPT with RL that have a binding score from PanPep exceeding threshold 0.5. RL demonstrated promising training efficiency, as evidenced by the significant increase in binding percentage of five tested peptides shown in Figure 5.\nRL adjusts the distribution of learned TCR sequences, and it is crucial to ensure that the tuned model can generate TCR sequences similar to actual TCRs that bind to the target peptide. To verify this, we assess how many TCR sequences generated by the model-with and without RL-match real TCR sequences known to bind the target peptide. For each tested peptide, we compiled a set of known TCRs from IEDB (Vita et al. 2018) that can bind to it. We then used both models to generate 10,000 TCR sequences each and calculated the proportion of these sequences that matched the known TCRs. This process was repeated 100 times, with each proportion serving as a sample. we observed that the proportions of generated TCR sequences overlapped with real TCRs capable of binding to the given peptide were significantly higher for the TCR-GPT model fine-tuned with RL compared to the one without RL, as shown in Figure 6. This demonstrates that RL is capable of fine-tuning the TCR-GPT model to match the distribution of real TCRs specifically binding to the given peptide."}, {"title": "Conclusion", "content": "Text-generation models, driven by advanced natural language processing (NLP) techniques, have the potential to transform our understanding and utilization of TCR diversity. In our research, we introduce TCR-GPT, a sophisticated probabilistic model based on a decoder-only transformer architecture. This model is designed to identify sequence patterns within TCR repertoires and generate TCR sequences from the learned probability distribution. By implementing RL, we tailored the TCR sequence distribution to produce repertoires capable of recognizing specific peptides. This advancement holds significant potential for revolutionizing targeted immune therapies and vaccine development, paving the way for practical and effective medical applications.\nThe limitations of this study primarily stem from the fact that TCR-GPT is a probabilistic model focused exclusively on the CDR3 region of the TCR beta chain, thereby excluding the full-length sequences of both the alpha and beta chains of TCR sequences. This narrowed focus restricts the comprehensiveness of the model in representing the complete TCR repertoire. Future research efforts will be directed"}]}