{"title": "Hybrid Primal Sketch: Combining Analogy, Qualitative Representations, and Computer Vision for Scene Understanding", "authors": ["Kenneth D. Forbus", "Kezhen Chen", "Wangcheng Xu", "Madeline Usher"], "abstract": "One of the purposes of perception is to bridge between sensors and conceptual understanding. Marr's Primal Sketch combined initial edge-finding with multiple downstream processes to capture aspects of visual perception such as grouping and stereopsis. Given the progress made in multiple areas of AI since then, we have developed a new framework inspired by Marr's work, the Hybrid Primal Sketch, which combines computer vision components into an ensemble to produce sketch-like entities which are then further processed by CogSketch, our model of high-level human vision, to produce both more detailed shape representations and scene representations which can be used for data-efficient learning via analogical generalization. This paper describes our theoretical framework, summarizes several previous experiments, and outlines a new experiment in progress on diagram understanding.", "sections": [{"title": "1. Introduction", "content": "Perception provides organisms with information about the world around them, organized in ways that are useful given their capabilities. For people, and human-like AI systems, this includes providing a bridge between sensory data and conceptual understanding: What objects are like, how they are arranged into scenes, and how these change over time. One of the early revolutions in computer vision was Marr's (1982) Primal Sketch, which proposed that edges actually formed a kind of hybrid representation, a combination of metric and symbolic information that served as a starting point for many down-stream computations, such as stereopsis and lightness. For example, stereopsis was modeled by aligning edges from pairs of images, since edges provide more stable information about the world than pixels. The primal sketch was used to construct the 2 \u00bd D sketch, which carved images up into regions, so that properties of surfaces could be estimated (e.g. lightness and material composition)."}, {"title": "2. Background and Related Work", "content": "Here we briefly summarize necessary background and closely related work."}, {"title": "2.1 Computer Vision", "content": "A survey of even one area of computer vision is beyond the scope of this paper. Most relevant for this work are algorithms that extract and recognize visual structure. Broadly, these can be divided into edge finders, recognizers, and segmenters. Edge finders like Canny (1986) operate over natural images finding zero crossings in intensity gradients that correspond to edges. The advantage of edge-finders is that they are (ideally) fully general-purpose, placing no limitations on the kinds of objects in the images. The disadvantage is that additional processing is required to use the edges found to propose segmentations of the image into regions and more complex structures. A special case is processing bitmaps that depict sketches (e.g. the MNIST dataset). For such images a common technique is to treat them as binary images, by down-sampling and thresholding, then using a combination of edge thinners (e.g. Zhang & Suen 1984) and tracing algorithms (e.g. Potrace (Krenski & Selinger, 2003)) to produce edges. We use such techniques here to produce vector representations of sketches depicted in bitmaps.\nBy contrast, recognizers, like MASK-RCNN (He et al. 2018), directly provide proposed regions and conceptual labels. However, they must be trained on the kinds of entities to be recognized, and hence are not as broadly applicable as edge finders. Nonetheless, recognizers are an extremely popular choice for building vision systems today. We have used both MASK-\nRCNN and Faster-RCNN (Ren et al. 2015) as object recognizers over natural images (Chen &\nForbus, 2021). Our techniques only assume that some form of boundary (bounding box or mask)\nis provided plus a word or phrase as a conceptual label and should work with other recognizers.\nWe note than an increasingly popular choice are open-vocabulary detectors, where one must\nprovide the set of words corresponding to the kinds of entities sought in the image (Wu et al.\n2024). We also have used OWLv2 (Minderer et al. 2023) as an input component, when top-\ndown constraints suggest what kinds of entities should be sought (e.g. arrows).\nSegmenters carve up an image into regions, roughly corresponding to entities (or pieces of\nthem). For example, Meta's Segment Anything Model (SAM; Kirillov et al. 2023) was trained\non over a billion masks across 11 million images. Downstream processes must still pull together\ninterpretations of regions into objects, but given the smaller number of regions, this is a much\neasier problem than not having the segmentation."}, {"title": "2.2 Qualitative Visual Representations", "content": "Qualitative representations provide symbolic representations of continuous phenomena. In visual processing qualitative representations have been introduced for both shapes (i.e. within-object relationships) and for scenes (i.e. between-object relationships). We discuss each in turn."}, {"title": "2.3 Sketch Understanding versus Scene Understanding", "content": "Work in artificial intelligence on sketch understanding tends to equate understanding with recognition (Forbus, 2019). In general recognition is insufficient for sketch understanding because the mapping between sketches and the objects or systems they can depict is many-to- many. When people sketch they usually talk, providing another channel of information that enables others in the interaction to understand them. In other words, even people do not rely solely on recognition when sketching with each other. Recognition is a catalyst, not a requirement. Sketching also often provides timing information, i.e. watching the ink being laid down. This extra source of information can help improve recognition, e.g. it is well known that in handwriting recognition, timing data is crucial for accuracy. In CogSketch (Forbus et al. 2011), people sketching lay down visual ink, manually segment it into entities, and provide conceptual"}, {"title": "2.4 Analogical Learning", "content": "We use an analogy stack based on Structure-Mapping Theory (Gentner, 1983). Matching is performed via the structure-mapping engine (SME, Forbus et al. 2017b), which takes as input two structured representations and produces one or more mappings between them. Each mapping indicates what statements and entities in one description correspond with the other, what inferences follow from these correspondences, and a numerical estimate of their similarity. Retrieval is performed by MAC/FAC (Forbus et al. 1995), a two-stage map reduce process. The first stage uses sparse vector dot-products over a massive library of cases to find a small number of items for the second stage, which uses SME in parallel with the structured version of the probe and the MAC output cases to select the few most similar as remindings. Generalization is performed by the Sequential Analogical Generalization Engine (SAGE; Kandaswamy & Forbus, 2012). SAGE models concepts via generalization pools which are incrementally constructed by examples. These pools contain both generalizations and outliers. Generalizations are formed when a new example retrieves an outlier, i.e. a prior example that wasn't similar enough to anything else so far. Each new example added to a generalization causes the probability of statements in that generalization to be updated, e.g. statements that don't show up frequently wear away. A generalization pool can have multiple generalizations, providing the ability to represent disjunctive concepts. It is similar to K-means plus outliers, but SAGE figures out the number of generalizations automatically, rather than them having to be pre-specified. There is also a working memory version of SAGE, SageWM, where the generalization pools are in working memory and retrieval is based on SME similarity scores, modulated by recency.\nWe note that this analogy stack supports incremental learning, but today's deep learning systems do not. Thus all the experiments used for testing are batch settings.\nThere have been multiple models of analogy in cognitive science (e.g. Doumas et al. 2008;\nHummel & Holyoak, 1997; Mitchell 1993). Unfortunately, these models do not have the representational capacity to handle the kinds of vision tasks described here."}, {"title": "3. The Hybrid Primal Sketch", "content": "The key idea of Marr's (1982) Primal Sketch was that vision has a symbolic component. Edges were viewed as a simple primitive symbolic representation, also having continuous properties such as location and contrast, being grounded in parallel visual processing elements. These descriptions were used in a variety of downstream processes. For example, visual structure including texture would be found by performing grouping operations based on statistical properties of the edges and continuity of form constraints. Marr's group focused on both trying to understand the purpose of visual computations and how they were implemented biologically. While some of the original hypotheses of the Primal Sketch have not worked out, particularly in terms of the biological picture, the overall flow of operations and the construction of intermediate representations \u2013 something like edges, regions that provide clues to surfaces in the world, abductive interpretation processes to accumulate local pieces into hypotheses about the perceived world \u2013 remain of interest. That said, many efforts in computer vision carve the processing up somewhat differently. For example, recognizers trained using deep learning directly on images provide labels for some region of the image (either a bounding box or a mask approximating the outline of the object being hypothesized). Are such recognizers biologically plausible? We leave that question aside here, since our interest is more practical: How can we use the advances in computer vision to build cognitive systems that can see? Our assumption is that at some point human visual processing does produce symbolic descriptions, but we are agnostic as to where and how. Our concerns lie upstream: The computation of relational representations of shapes and scenes so that analogical learning can be used in visual processing. Even though visual stimuli are plentiful for most organisms, it still behooves them to learn incrementally, in a data-efficient manner. Since mistakes are inevitable in learning, organisms lucky enough to have language can discuss and refine their visual concepts, if they are\ninspectable. The kinds of qualitative visual and spatial representations developed in qualitative\nreasoning research are, we hypothesize, among the contents that can be inspected by people (and\nhence should be inspectable by our machines, if they are to participate in discussions of them). In\nother words, at the level of inspectability, we want human-like representations and processing,\nwhereas for computations below that level, as long as they produce functionally reasonable\nrepresentations, that is all that matters.\nThus the Hybrid Primal Sketch consists of an ensemble of visual components (see Figure 2).\nBut what is the common currency that these components produce, which serves as an input to the\nmore inspectable levels of processing? Our hypothesis is that the notion of glyph in open-domain\nsketch understanding (e.g. CogSketch; Forbus et al. 2011) provides that output format. In sketch\nunderstanding, a glyph is a visual entity consisting of digital ink and a conceptual label, produced\nvia someone drawing. In some datasets, visual stimuli are encoded in the vector graphics format\nSVG, which can be directly imported into CogSketch. For bitmaps of drawings, there are\nalgorithms which can produce such vector representations automatically (i.e. Potrace, Section\n2.1), and hence can be treated as glyphs by CogSketch. For natural images, the bounding boxes\nor masks produced by recognizers (e.g. MASK-RCNN, Section 2.1) plus the label they provide\ncan be treated as if they were glyphs. Thus the elements of the ensemble produce glyphs for\nCogSketch, which then analyzes them in the same ways that it analyzes human-drawn sketches.\nCogSketch was designed as a model of high-level human vision. Prior work has shown that it\ncan be used to model human visual problem solving (Forbus & Lovett, 2022) and as a component\nin multimodal learning by reading (Chang 2016) and modeling conceptual change (Friedman, et\nal. 2018). The use of analogy in Sketch Worksheets, a deployed educational software system\n(Forbus et al. 2017a; Forbus et al. 2018) provides additional evidence that the representations it\nproduces make sense to people and can be used robustly with a broad range of inputs. The results\nin subsequent sections provide more evidence for this claim, in terms of learning visual concepts\nfrom images (and Kinect video).\nGiven an image and a task context, which algorithms should be used? Currently this is a\nprocedural parameter set by the experimenter, who sets up a pipeline custom-built for that\nexperiment, using the capabilities of the Companion architecture to streamline the process (i.e.\nHTN plans provide a form of scripting, and Worker agents enables parallel processing to speed\nup encoding). We are moving to automatic control of processing by the Companion itself, as\ndiscussed in future work."}, {"title": "4. Experiments with the Hybrid Primal Sketch", "content": "We briefly summarize some prior experiments and describe an experiment in progress, to provide\nevidence for the utility of this framework. We start with sketch recognition, then turn to visual\nrelationship detection, to illustrate its capabilities for handling images. We summarize\nexperiments using Kinect video, to show how these ideas can be extended beyond single images.\nFinally, we discuss an experiment in progress on diagram understanding, since it provides a\ntestbed for exploring the roles of conceptual understanding in scene understanding."}, {"title": "4.1 Sketch Recognition", "content": "The classic analogical learning pipeline works as follows: Encode the stimuli to be learned, and add it to a SAGE generalization pool for that concept. At any time, test items can be tried with MAC/FAC against the union of the generalization pools, with the classification of the test item being the label associated with the pool the top retrieval came from. Running this pipeline on the classic MNIST dataset (LeCun et al. 1998) provides an illustration. Encoding was performed by resizing the original image to be below 300 pixels, then blurred and filtered to black and white. Potrace and Zhang-Suen's thinning algorithm were\nused to generate SVG for CogSketch. CogSketch\nbroke the SVG down into edges and edge cycles,\nproducing a geon representation (Biederman,\n1987), and computed a variety of properties for\nsegments and geons (see (Chen et al 2019) for\ndetails. We compared our performance to LeNet-5,\nthe classic baseline. To test data efficiency, we\nstarted with many fewer training samples."}, {"title": "4.2 Visual Relationship Detection", "content": "Sketches are of course simplified compared to natural images. How does the HPS approach fare on natural images? To explore this, we used a standard Visual Relationship Detection dataset (Lu et al. 2016), which consists of 5,000 images involving 100 object categories and 70 predicates. As described in (Chen & Forbus, 2021), given ground truth bounding boxes and categories (the PREDT task), analogical learning over HPS representations had 52.38 for both recall@50 and recall@100, putting it in the middle of the pack compared to prior systems, and hence competitive. We note that, as usual, the analogical learner only examined each training item once, while the deep learning systems required between seven and 30 epochs, thereby demonstrating training efficiency. (As before, the analogy learner only used CPU machines, whereas the deep learning systems use GPUs or TPUs.) While we have not been able to find\nrecall@1 for other systems,\nthe analogy learner had 32.26\nfor recall@1.\nIn addition to being data and training efficient, analogical learning produces inspectable\nmodels. Because the visual relationships computed by CogSketch are inspired by human vision,\nthe generalizations produced tend to be interpretable. Figure 4 shows an example of an image\nillustrating the \"wears\" relation, the bounding boxes computed for it, and one of the\ngeneralizations that SAGE constructs from the training data. (A generalization pool is a\ndisjunctive concept description, consisting of different generalizations and outliers.) For\nexample, in this generalization that which is being worn overlaps the person, is small compared to\nthe size of the person, and is most likely pants, shoes, jeans, or shorts. This illustrates how the\ncombination of the symbolic relational descriptions and probabilities produced by SAGE are\ninspectable."}, {"title": "4.3 Human Action Recognition", "content": "Seeing images in isolation is a simplification of vision, with video being a closer approximation since organisms exist in dynamic environments. Here we examine learning from Kinect video, a special case focused on human actions. Kinect sensors produce skeleton descriptions of human motion, based on data from depth cameras, with 3D coordinates at every joint. We developed a\npipeline that generates qualitative representations of\nsuch video in the form of video sketch graphs, a\nsequence of snapshots, akin to panels in a comic strip.\nEach snapshot describes the motion that is happening\nover some interval of time corresponding to a qualitative\nstate. This temporal decomposition is accomplished\nwith QSRLib (Gatsoulis et al. 2016), a library of\nqualitative spatial relations and calculi that takes streams\nof coordinates in, and segments them based on changes\nin the qualitative spatial relationships being tracked.\nThe original 3D coordinates are projected into a front\nand side view, over which qualitative spatial\nrelationships are computed for temporal segmentation.\nThere are multiple data sets of videos of simple\nhuman actions, like swiping left, clapping, throwing,\njogging, walking, etc. Here we used Florence 3D\nAction (Seidenari et al. 2013) and UTD-MHAD (Chen\net al. 2015), and UTKinect-Action3D (Xia et al. 2012)\ndatasets. As reported in (Chen & Forbus, 2018), this\napproach was competitive with others, but did not perform as well with actions involving\nsubstantial noise. However, unlike the other approaches, ours provides inspectable models that\ncan be used to explain a system's answers"}, {"title": "5. In Progress: Understanding Diagrams", "content": "Learning from instructional media, like text with diagrams, is an important task for bootstrapping\nAI systems. This involves understanding conventions used for depicting things in diagrams and\nsketches. For example, the area inside a circle intended to depict the Earth is solid, while the area\ninside a circle depicting an orbit is not. Lockwood et al. (2008) casts this as the problem of\nconceptual segmentation, i.e. assigning conceptual interpretations to regions and edges within a\ndiagram or sketch. We are currently building on this approach as a further way to extend the HPS\nin the direction of broader conceptual interpretations, beyond just object recognition or simple\nvisual relationship detection. We are using as a testbed the AI2 Diagrams dataset (Kembhavi et\nal. 2016) which focuses on types of diagrams commonly used in science educational materials.\nThe first challenge is to generate an initial visual segmentation as the starting point for\nconceptual segmentation. Here we are relying on Meta's SAM (Kirillov et al. 2023), which is\ncapable of carving up diagrams into useful intermediate regions with reasonable accuracy."}, {"title": "6. Conclusions and Future Work", "content": "Given the complexity of perception, finding the appropriate intermediate representations is\ncrucial. This paper argues that CogSketch's glyph representation provides a notion of visual\nentity that is useful for perception beyond just sketch understanding. The ability to decompose\nshapes within a glyph, and represent relationships between glyphs, enables analogical learning to\nbe used for recognition of entities and properties of scenes.\nWe plan three lines of future work. The first is to expand the exploration of diagram\nunderstanding, since it stress-tests the integration of conceptual and visual knowledge. The\nsecond is to make the ensemble operations more automatic. Like visual routines (Ullman 1984)\nand spatial routines (Lovett, 2012), we believe there is learnable procedural knowledge used to\nguide the application of ensemble operations to new situations and tasks. Uncovering the nature\nof these orienting computations is high on our agenda. Finally, we plan to continue to expand the"}]}