{"title": "Graph-based Complexity for Causal Effect by Empirical Plug-in", "authors": ["Rina Dechter", "Annie Raichev", "Alexander Ihler", "Jin Tian"], "abstract": "This paper focuses on the computational complexity of computing empirical plug-in estimates for causal effect queries. Given a causal graph and observational data, any identifiable causal query can be estimated from an expression over the observed variables, called the estimand. The estimand can then be evaluated by plugging in probabilities computed empirically from data. In contrast to conventional wisdom, which assumes that high dimensional probabilistic functions will lead to exponential evaluation time of the estimand. We show that computation can be done efficiently, potentially in time linear in the data size, depending on the estimand's hypergraph. In particular, we show that both the treewidth and hypertree width of the estimand's structure bound the evaluation complexity of the plug-in estimands, analogous to their role in the complexity of probabilistic inference in graphical models. Often, the hypertree width provides a more effective bound, since the empirical distributions are sparse.", "sections": [{"title": "Introduction", "content": "Given a causal graph and data from the observed distribution of a Structural Causal Model (SCM), a causal effect query can be answered by a two step process: First, determine if the query is identifiable, namely if it can be answered uniquely given the graph and observational data, and if so generate an estimand for the query. The estimand is an algebraic expression over probabilistic functions that involve observed variables only. Second, the estimand is evaluated using data from the observational distribution. A straightforward approach for evaluation is the empirical \"plug-in\" method, which simply replaces the probabilistic functions in the estimand with the empirical probabilities from the data.\nHowever, the estimand expression often involves high dimensional conditional probability functions and marginalization over a large number of variables (e.g., see Eq. (8)). A common assumption is that, for discrete models, the computation required is at least the size of the largest table, and thus exponential in the number of arguments of the largest term in the expression. This assumption suggests that the estimand expression is difficult or impossible to evaluate in high-dimensional settings. However, taking into account the data size, the initial sizes of the empirical probability tables are bounded, regardless of the functions' dimension. So, the estimand evaluation can also be bounded in terms of the data size, in some cases linearly.\nOur paper focuses on the complexity of computing plug-in estimates. We explore the impact of both the functions' dimension and the data size on the complexity of evaluation. We show that well-known graph parameters such as tree-width and hypertree width, which play a central role in the complexity of probabilistic inference, play a similar role in plug-in estimand evaluation. It is well known that probabilistic inference is exponential in the tree-width [Dechter, 2003, 2013]. However, when a graphical model's functions are sparse (e.g., have many zeros), the hypertree width can provide a tighter exponential bound than treewidth. [Gottlob et al., 2000, Kask et al., 2005, Otten and Dechter, 2008]. We build on these results to show that the tree-width and hypertree width play a similar role in the complexity of plug-in estimand evaluation. Since, the empirical probabilities are inherently sparse, the hypertree width is often far more informative than the tree-width for this task. We associate an estimand expression with a subexpression hierarchy and show how the hypertree widths of subexpressions additively determine complexity bounds on estimand evaluation. Our bounds help characterize the computational feasibility of the empirical plug-in scheme, establishing it as a simple and practical baseline for causal effect estimation. Moreover, since a causal query can have many candidate estimands, tree-width and hyperwidth-based bounds can be used as one metric to selecting among different estimands. Finally, we illustrate the effectiveness of the hypertree width in capturing the actual time and memory bounds of empirical plug-in estimation."}, {"title": "Background", "content": "We begin with some useful definitions and notation.\nDefinition 1 (Structural Causal Model) A structural causal model (SCM) [Pearl, 2009] is a 4-tuple M = (U,V, F, P(U)) where: (1) U = {U1, U2, ..., Uk} is a set of exogenous (latent) variables whose values are affected by outside factors; (2) V = {V1, V2, ..., Vn} are endogenous, observable variables; (3) F = {fi : Vi \u2208 V} is a set of functions fi where each fi determines the value vi of Vi as a function of Vi's causal parents $PA_i \\subseteq U \\cup (V \\setminus V_i)$ so that $v_i = f_i(pa_i)$; (4) P(U) is a probability distribution over the latent variables. The latent variables are assumed to be mutually independent. i.e., $P(U) = \\prod_{U_j \\in U} P(U_j)$.\nCausal diagrams. The causal diagram of an SCM M is a directed graph G = \u3008VUU, E), where each node represents a variable, and there is an arc in E from a node representing X to a node representing Y iff X is a parent of Y. We assume semi-Markovian SCMs in which latent variables connect to at most two observable variables [Tian, 2002]. Here it is common to omit latent variables having a single child, and replace any latent variable with a bidirectional dashed arc between the children (see Figure 2a, 2b, 2c).\nAn SCM M induces a Causal Bayesian Network (CBN) B = (G, P) specified by M's causal diagram G = (VUU,E) along with its associated conditional probability distributions P = {P(Vi|PAi), P(Uj)}. The distribution P(V,U) factors according to the causal diagram:\n$P(V,U) = \\prod_{V_i \\in V} P(V_i|PA_i) \\prod_{U_j \\in U} P(U_j).$ (1)\nThe observational distribution, P(V), is given by\n$P(V) = \\sum_{U} P(V,U).$ (2)\nCausal effect and the truncation formula. An external intervention forcing variables X to take on value x, called do(X = x), is modeled by replacing the mechanism for each X \u2208 X with the function X = x. Formally,\n$P(V\\setminus X,U | do(X = x)) = \\prod_{V_i \\notin X} P(V_i|PA_i) \\prod_{U_j} P(U_j)$\n$X=x$ (3)\nNamely, it is obtained from Eq. (1) by truncating the factors corresponding to the variables in X and setting X = x. The effect of do(X) on a variable Y, denoted P(Y | do(X)), is defined by marginalizing all the variables other than Y.\nThe standard formulation of causal inference assumes that we only have access to the causal graph G and the observational distribution P(V) (or a sample from it). The identifiability task is to determine if the query can be uniquely answered from G and P(V). This occurs if the answer is unique for any full model that is consistent with the graph and P(V) [Pearl, 2009]. In such cases an estimand expression in terms of P(V) can be generated and evaluated.\nDefinition 2 (Causal-effect query) Given a causal diagram G = (VUU, E), data samples from the observational distribution P(V), and an identifiable query P(Y | do(X)), the task is to compute the value of P(Y | do(X)).\nEstimand-based approaches. The now-standard methodology for answering causal-effect queries is to break the task into two steps. The first is the identifiability step: given a causal diagram and a query P(Y | do(X)), determine if the query is identifiable and if so, generate an estimand, or algebraic expression in terms of the observational distribution P(V) that answers the query. A complete polynomial algorithm called ID has been developed for this task [Tian, 2002, Shpitser and Pearl, 2006]. The second step is estimation: use samples from the observational distribution P(V) to estimate the value of the estimand expression. A number of approaches have been applied to estimation. A simple approach, called the plug-in estimator, replaces each term in the estimand with its empirical probability value in the observed data. More sophisticated approaches have been developed recently [Jung et al., 2020a,b, Raichev et al., 2024].\nA CBN and SCM belong to the class of probabilistic graphical models. A graphical model is defined by a collection of functions over subsets of variables:\nDefinition 3 A probabilistic graphical model is a triplet M = <X, D, F) where X = {X1, ..., Xn} is a set of variables with finite domains D = {D1, ..., Dn}, and F = {f1,...,fr} is a set of discrete real-valued functions, each defined over a subset of variables Si \u2286"}, {"title": "Graph concepts", "content": "Graphs are an integral components of graphical models, so we next define well-known concepts over graphs and hypergraphs and their relation to graphical models. For more details see Kask et al. [2005], Dechter [2013], Gottlob et al. [2014].\nDefinition 4 (hypergraph) The hypergraph of a graphical model M =< X,D,F > is a pair H = (V, ST) where V = X and is a set of subsets of V, called hyper-edges, The primal graph of a hypergraph H = (V, ST) is an undirected graph G = (V, E) such that there is an edge (u,v) \u2208 E for any two vertices u, v \u2208 V that appear in the same hyperedge. The dual graph of a hypergraph is a graph where each hyperedge is a node, and two nodes are connected by an edge if their nodes have non-empty intersection.\nDefinition 5 (hypertree) A hypergraph is a hypertree, also called acyclic, if its dual graph has an edge subgraph that is a tree (called a join-tree) satisfying that all its nodes that contain a common variable form a connected subgraph. This condition is also known as the \u201crunning intersection property\" or the \"connectedness property\". A join-graph is an edge subgraph of the dual graph that satisfies the conectedness property. If the hypergraph of a graphical model is a hypertree, the graphical model is called acyclic."}, {"title": "Tree and Hypertree Decompositions", "content": "Tree decomposition schemes have been widely used for constraint processing and probabilistic reasoning. The most popular variants are join-tree (also known as junction-tree) algorithms, which include variable elimination schemes [Dechter, 2003, Gottlob et al., 2000, Dechter, 2013]. The methods vary somewhat in their graph definitions as well as the way the tree decomposition is processed. However, all involve a decomposition of a hypergraph of a graphical model into a hypertree:\nDefinition 6 (tree & hypertree decompositions) [Kask et al., 2005, Gottlob et al., 2000] A tree-decomposition of a graphical model M = (X, D, F) is a triplet (T, x,\u03c8), where T = (V, E) is a tree, and X and \u03c8 are labeling functions that associate with each vertex v \u2208 V two sets, x(v) \u2286 X and \u03c8(v) \u2286 F, that satisfy the following conditions:\n1. For each function fi \u2208 F, there is exactly one vertex v \u2208 V such that fi \u2208 \u03c8(\u03c5).\n2. If fi \u2208 \u03c8(v), then scope(fi) \u2286 x(v).\n3. For each variable Xi \u2208 X, the set {v \u2208 VX \u2208 x(v)} induces a connected subtree of T. This is also called the running intersection or the connectedness property.\nThe treewidth w of a tree-decomposition T = (T, x, y) is $max_{v \\in V}|x(v)| - 1$. The treewidth of a graphical model is the minimum treewidth over all its tree-decompositions.\nThe tree-decomopsition is also called a hypertree decomposition of the graphical model if it satisfies the additional condition that the variables in each node, also called a cluster, are covered by the arguments of the functions in the cluster. Formally:\n4. For each v \u2208 V, $x(v) \\subseteq \\cup_{f_j \\in \\psi(v)}scope(f_j)$.\nIn this case the hypertree width of t is $hw = max_v|\u03c8(v)|$. The hypertree-width of a graphical model is the minimum hypertree width over all possible hypertree decompositions of the graphical model."}, {"title": "Complexity of Tree Decomposition", "content": "Once a (hyper) tree-decomposition of a graphical model is generated, any sum-product query (e.g., P(X|Y = y) where X and Y are subsets of variables) can be answered by a message passing algorithm where each vertex of the tree sends a function to each of its neighbors. We will use Cluster-tree Elimination (CTE) [Kask et al., 2005] as a generic name for a message passing algorithm over a tree-decomposition.\nAlgorithm CTE. The following 3 steps define the basics of the CTE algorithm [Kask et al., 2005]. Given a hypertree decomposition, each node u has to send a single message to each neighbor v. We can compute m(u,v) as follows:\n1. Combine all functions \u03c8(u) in node u yielding function $h(u) = \\prod_{f \\in \\psi(u)} f$, (assuming the combination is a product). This step can be done in time and space $O(t^{|v(u)|})$. In particular, the tightness of the product function is $O(t^{|v(u)|})$.\n2. For each neighbor c of u, c\u2260 v iterate the following $h(u) \\leftarrow h(u) \\cdot \\sum_{X(u) \\cap X(c)} M(c,u)$. This step can be accomplished in O(deg\u00b7hw\u00b7log t\u00b7thw) time and O(thw) space, deg can be dropped if messages are sent only from leaves to root.\n3. Make $m(u,v) \\leftarrow h(u)$.\nThe following bounds of algorithm CTE as a function of the treewidth and hyperwidth are restated from the literature, with a slight simplification to account for one way message-passing.\nTheorem 1 (Graph-based complexity of CTE) [Kask et al., 2005] Given a graphical model M = (X,D,F) and a hypertree-decomposition (T, x, \u03c8), let n be the number of variables in X, w its treewidth, hw its hypertree width and t its tightness, and k the maximum domain size of a variable. A sum-product query can be computed by CTE within the two following bounds:\n1. as a function of the treewidth:\nO(n\u00b7kw+1) time and O(nk\u2122) space (4)\n2. as a function of the hyperwidth:\nO(n \u00b7 hw\u00b7 log t\u00b7thw) time and O(thw) space (5)"}, {"title": "Causal Effect Estimand Evaluation", "content": "We use these results to analyze the plug-in evaluation of causal queries' estimands. The notion of empirical Bayesian networks is central to this evaluation.\nEmpirical Bayesian Network. Given a directed graph G whose nodes are discrete variables X = {X1,...,Xn} and given a data set D = {d1,...dt} over the variables, the empirical Bayesian network, empBN(G,D), is the Bayesian network (BN) whose graph is G and its functions are the empirical CPTs extracted from the dataset D. That is, any entry (x,pax) in the empirical CPT PD(X = x|PAx = pax) for a variable X and its parents PAx in the G is obtained by counting the number of appearances of (x,pax) in D, divided by the number of appearances of pax in D. Formally, $P_D(x|pax) = \\frac{#D(x,pax)}{#D(pax)}$ where #D(s) is the number of elements in D that are consistent with s.\nIt is easy to see that the number of non-zero configurations of any CPT table of empBN(G,D) cannot exceed the data size t. Therefore, by Theorem 1, we can immediately conclude the following.\nTheorem 4 (Complexity of empirical BN.) Assume a given DAG G over variables {X1, ..., Xn} and let hw and w be the hypertree width and treewidth of the Bayesian network, respectively. Given also a dataset D, where t = |D|, and domain sizes bounded by k, then computing a sum-product query over empBN(G,D) by CTE as a function of wis O(n\u00b7kw+1) time and O(kw) space. Computing the query as a function of hw is O(n.hw.log t\u00b7thw), time and O(thw) space by CTE.\nCorollary 1 If empBN has hypertree width 1, the sum-product query can be answered in almost linear time, O(ntlog t).\nImplication to estimands evaluation. We show a causal effect estimand can be associated with a hierarchy of empirical Bayesian networks so that evaluating the sum-product inference on each will yield the estimand evaluation. Applying a hypertree decomposition algorithm like CTE to each sub-expression will yield an effective estimand evaluation scheme whose performance would be bounded by the graph parameters of treewidth and hyperwidth. We first illustrate via several examples."}, {"title": "Empirical validation", "content": "For benchmarks we used three semi-markovian examples, and one markovian (without latent confounding). The semi-markovian examples were taken from Raichev et al. [2024], and the markovian example was constructed to have higher hyperwidth. Since, O(thw) is a worst case bound we ran our algorithm on various data sets, and present results that are close to worst case. We sampled data different distributions: uniform, deterministic, Dirichlet, and their mixes. Queries were chosen to correspond to complex estimands that could pose computational challenges."}, {"title": "Performance Measures", "content": "We report the time and max table size during computation of the empirical plug-in on different sample sizes. The max table size reflects the space needed to evaluate the estimand. We also report the tightness t, and the density, which gives us a measure on the largest factor in the estimand, where density =$\\frac{\\text{# entries in table}}{\\text{# of configurations in the joint table}}$."}, {"title": "Results", "content": "The empirical plug-in results with varying sample sizes are presented in Table 1 on the Chain, Cone-cloud, Diamond, and 3-layer models (Figure 2). We show that the complexity for estimand evaluation is reflected in the hyperwidth associated with the estimand, and it provides a much tighter bound than the treewidth.\nThe disparity between bounds is most evident in the Chain model where we have hyperwidth of one, thus computation is linear in the tightness; but the treewidth is 98, with max domain size k = 4. So bounding the space and time complexity in the treewidth we get $O(k^{99}) \\approx 4\\times 10^{59}$ for time and $O(k^{98}) \\approx 1\\times10^{59}$ for space, giving us a loose and irrelevant bound. Even with 10,000 samples and t = 10,000 the bound from the hyperwidth is $O(t^{1}) = O(10,000^{1})$, showing it's a more informative bound.\nWe also show that the performance can be non-linear. For example, in the case of the Cone-cloud and Diamond models, both with hw = 2. Here t vs time and t vs max table size are both O(t\u00b2). For example when t = 1,000 the max table size for the cone-cloud and diamond, respectively, is 882,604 and 928,510, just under 1,000\u00b2. We also see that t vs time grows faster than linearly, and is O(t\u00b2). Again, the treewidth is much higher than the hyperwidth, and the max(k) is 10 and 50, so the treewidth bound is large and uninformative.\nIn the 3-layer model we have hyperwidth 4 and treewidth 15, with a max domain size of 50. The relationship of t vs time and t vs max table size appears roughly cubic. So, while the worst-case is not realized we see significant impact of the hyperwidth. On the other hand, we do not see any values getting as high as the treewidth suggests (O(5015)).\nSince the tightness can increase with number of samples, the bound dictated by the hyperwidth can increase with number of samples. This is something to consider if you have a large number of samples with dense tables."}, {"title": "Conclusion", "content": "The paper provides a new algorithm and analysis for evaluating plug-in estimands for causal effect queries. The approach uses structural parameters, the treewidth and hypertree width, in bounding the complexity of plug-in estimand evaluation, similar to their role in graphical models schemes. We show that the hypertree width is more informative for this task because of its sensitivity to function sparseness, and evaluating plug-in estimands from data yields sparse functions.\nWe introduce a new algorithm Plug-in hypertree evaluation (PI-HTE) which harnesses hypertree decomposition algorithms to efficiently estimate the estimand, and we evaluate its performance over several classes of benchmarks and causal effect queries. Our results confirm the significance of the hypertree parameter in capturing the algorithm's performance and its superiority over the treewidth parameter in the context of causal effect evaluation. In particular, it enables the evaluation of estimands previously thought to be computationally infeasible.\nOur bounds help characterize the computational feasibility of the empirical plug-in scheme, establishing it as a simple and practical baseline for causal effect estimation. In particular, since a causal query can have many candidate estimands, tree-width and hyperwidth-based bounds can be used as one metric to selecting among different estimands."}]}