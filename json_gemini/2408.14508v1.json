{"title": "ARTIFICIAL INTELLIGENCE FOR SCIENCE: THE EASY AND HARD PROBLEMS", "authors": ["Ruairidh M. Battleday", "Samuel J. Gershman"], "abstract": "A suite of impressive scientific discoveries have been driven by recent advances in artificial intelligence (AI). These almost all result from training flexible algorithms to solve difficult optimization problems specified in advance by teams of domain scientists and engineers with access to large amounts of data. Although extremely useful, this kind of problem solving only corresponds to one part of science\u2014the \u201ceasy problem.\u201d The other part of scientific research is coming up with the problem itself-the \u201chard problem.\u201d Solving the hard problem is beyond the capacities of current algorithms for scientific discovery because it requires continual conceptual revision based on poorly defined constraints. We can make progress on understanding how humans solve the hard problem by studying the cognitive science of scientists, and then use the results to design new computational agents that automatically infer and update their scientific paradigms.", "sections": [{"title": "The easy problem", "content": "Most work applying AI to science has focused on what might be called the \"easy problem.\" This is a relative term, since the easy problem is actually quite hard. A scientist specifies a function that they want to optimize (e.g., a function that generates a protein's structure given its amino acid sequence). Included in the specification is the input for the function (e.g., the amino acid sequence), the output (e.g., the 3D structure), and a way to compare the function's output with the ground truth (e.g., the average 3D distance of an amino acid residue from where it should be). The scientist then finds or collects a dataset, usually very large, with examples of the ground truth; or, designs some other way of assessing the model's output (e.g., turbulence parameters in plasma flow). AI optimization tools can then be applied to this problem. So far, this kind of application has been highly successful, with new discoveries of tertiary protein structures, antibiotics, and nuclear fusion reactor designs (see [1] for a recent review).\nWhat makes this problem \"easy\" is not the form of the solution (which may require a great deal of engineering work) but rather the form of the problem. It is clear from the beginning what needs to be optimized, and what kinds of tools can be brought to bear on this problem. The engineering breakthrough comes from building much better versions of these tools. In other words, the problem is relatively easy because it does not require any conceptual breakthroughs of the sort involved in the discovery of relativity theory, genetics, or the periodic table.\nAre these conceptual breakthroughs just patterns that can be discovered with a sufficiently powerful pattern recognition system? In a sense yes, but before that can happen, something has to tell the pattern recognition system what kind of patterns are interesting, important, and useful. What problem is the pattern-recognition system designed to solve, and where does this come from?"}, {"title": "The hard problem", "content": "The fundamental barrier to automating science is conceptual. Great scientists are not simply extraordinary optimizers of ordinary optimization problems. It is not like Einstein had a better function approximator in his brain than his peers did; or Mendeleev's brain had a better version of backprop. More commonly, great scientists are ordinary optimizers of extraordinary optimization problems. It is the formulation of the problem, not its solution, that is the truly hard problem: The hard problem is the \u201cproblem problem.\u201d\nOne might be tempted to relegate the hard problem to the fringes of \u201crevolutionary science,\u201d which rarely erupt into mainstream scientific practice, whereas the easy problem occupies the focus of the \u201cnormal science\" that scientists spend most of their time on [2]. However, normal science is not simply optimization. This is obvious to any first-year graduate student trying to figure out what to work on. Normal science isn't a catalog of optimization problems waiting to be solved by a queue of grad students. Their fundamental barrier is the same one facing AI scientists: It is the conceptual problem of formulating an optimization problem. This encompasses both major conceptual breakthroughs, like relativity theory, and the more modest ones achieved by graduate students on a regular basis, which nonetheless remain out of reach for existing AI systems."}, {"title": "The problem with optimization", "content": "Much of the classic work on AI science (mainly by Simon, Langley, and their collaborators [3, 4, 5], but also more recently by Schmidt & Lipson [6], Udrescu & Tegmark [7], and others [8]) focused on the easy problem. For Simon and Langley, this approach was premised on the psychological thesis that scientific cognition was essentially the same as regular problem solving, only applied to a different (and sometimes more challenging) set of problems. Consequently, they developed algorithms that emulated human problem solving, and applied these to scientific discovery.\nExisting AI scientists have had some success at the easy problem. Simon, Langley, and others were able to solve a range of seminal science problems [9], including the (re-)discovery of oxygen with STAHLp [10]; more modern methods for automated physics have inferred many existing and novel laws, including classical and quantum problems with AI Feynman and non-linear dynamical systems with SINDy [7, 8]; and, discovery algorithms in biology have advanced our ability to solve many difficult problems, including AlphaFold2 for protein folding [11].\nThis success is analogous to the earliest use of computers, in which they were used to complete calculations too laborious for any human (such as in the Enigma cryptography project in World War II; or, to prove all edge cases of a complicated theorem [12]). Algorithms that solve the easy problems of science are useful, even essential, to progress. For example, there is an increasing discrepancy between the number of amino-acid sequences that are discovered in biology and the recovery of the 3D protein structures they correspond to using the experimental method.\nWhile recognizing the importance of such algorithms, we should also recognize their limitations. Several decades ago, Chalmers, French, & Hofstadter (focusing on the models of Simon, Langley, and their collaborators) challenged the idea that this kind of optimization was a complete model of scientific discovery and investigation [13]. Systems like STAHLp are only able to solve scientific problems and make discoveries, they argued, because the modelers have represented the inputs and outputs to the problem in hindsight; only relevant data have been included and those data are already organized such that the proposed heuristics will be able to easily extract the right solution. In other words, they have been provided a representation of the scientific problem that already includes the basic primitives needed for the final theory, but skirt the central problem of representation itself: Where do the primitives come from, and how do we know if we have discovered the right ones?\nSimon insisted (contra Popper [14]) that there was a logic of scientific discovery, but Simon's proposal was really a logic of scientific problem solving-how to sequentially search through hypotheses given a problem statement and primitive representations [3]. This is not discovery in the sense of problem creation. The latter involves representation learning in service of the problem, but also something deeper: Identification of the goal or objective function itself.\nIn machine learning terms, these systems might be extremely good at interpolation, and they may become better at extrapolation to new data, but they will never automatically generate or choose to investigate new scientific problems. This is because neither the inferential nor the learned components of the algorithm contain the knowledge"}, {"title": "Problem representation", "content": "The representation of input data involves two fundamental choices-which are the right primitive variables and which datapoints to include. In trying to emulate the investigative processes of scientists, it is important to consider the primitives they would have begun with. The representation chosen for the inputs cannot be too permissive-it cannot use concepts or data that scientists came up with in the course of solving the problem; nor too restrictive-it cannot exclude concepts or data that the would have originally affected the problem-solving process.\nThe output for a scientific problem comprises the scientific theory and any predictions it generates. This choice of representation determines which theories are considered \u201cwell-formed\" and therefore valid solutions for the problem. The output representation can be defined either explicitly, in the form of a set of symbols and operations, or implicitly, through the space of operations that can be applied to the input variables. Modeling the full scientific process requires specifying a generative system for theories that has sufficient flexibility for conceptual change, which in turn might affect the space of theories that are considered well-formed.\nThe third component of a problem representation is the goal, expressed in the language of optimization as a loss function that assesses the adequacy of a solution compared to the \"ground-truth.\" The choice of loss function corresponds closely to the way the modeler has chosen to represent the structure of the natural domain. For example, when providing deterministic physical models for cosmically short distances, loss functions based on Euclidean distance might be suitable; for classification models, some assessment of decision accuracy; for probabilistic models, a loss based on relative entropy. The choice of loss is also influenced by the cognitive biases of scientists themselves. A classic finding from cognitive science is that for perceptual stimuli with separable feature dimensions participants' generalizations are better captured with a Manhattan loss, in contrast to a Euclidean loss for stimuli with integral dimensions [15]."}, {"title": "Solving the hard problem", "content": "In contemplating how to build AI systems that solve the hard problem, it is instructive to look at how human scientists do it. The high-level objective in science is clear: We would like to account for more data with our theories. At this level, human scientists break the hard problem down into several sub-problems:\n\u2022 Domain specification. What are the relevant phenomena that need to be explained by a theory?\n\u2022 Constraint specification. What kinds of constraints need to be imposed on a theory based on existing knowledge (both domain-specific and domain-general)?\nOnce the domain and constraints have been specified, we can define an optimization problem (theory search); hence, we have converted the hard problem into the easy problem. For most current AI scientists, the modeling team conducts domain specification in advance in the representation and selection of data, and constraint specification in the representational scheme for potential scientific theories (outputs) and the objective function that assesses them. However, it is uncommon for real scientists to do a single pass from hard to easy, because they often realize that the problem they are solving is the wrong one. This may happen for several reasons. One is the realization that a theory is internally inconsistent or paradoxical. Another is the realization that the theory may (with suitable modification) be able to explain a broader range of phenomena, prompting a respecification of the domain. Conversely, phenomena which were previously included in a domain may need to be excluded if no adequate unifying theory is found for all the phenomena. Respecification can also happen when new empirical phenomena are reported. In a related vein, constraint respecification can happen when domains are merged, split, expanded, or shrunk. The key point is that problem creation and problem solving are cyclically coupled in scientific practice.\nIn the following sections, we motivate the distinction between the easy and hard problems with three case studies from the birth of modern chemistry, physics, and molecular biology. For each case study, we summarize the elements of the problem, the historical setting, and modern computational systems that have tried to recapture some aspects of these discoveries. We will argue that none of these modern systems offers a complete solution to the hard problem."}, {"title": "Case study 1: The discovery of oxygen", "content": "In the 18th century, it had been observed that lead increased in weight when it was slowly heated (which today we call \"oxidation,\" but at that time was called \u201ccalcination\"). This was difficult to explain with contemporary chemical theories, because they posited that something left a metal when it was heated (a type of inflammable earth called \"phlogiston\u201d). In 1774, the English chemist Joseph Priestley collected and identified a particularly inflammable and respirable form of air following the thermal reduction of calx-of-mercury (mercury-oxide) [16]. The French chemist Antoine Lavoisier eventually called this air \u201coxygen,\u201d and posited that it went into the metal during calcination instead, causing the weight change [17]. Lavoisier's course of investigations were so successful that he has been credited as having started the Chemical Revolution and introduced the principled application of the conservation of mass into the quantitative sciences.\""}, {"title": "STAHLP", "content": "Rose and Langley proposed a computational model called STAHLp to account for the discovery of the role of oxygen in calcination reactions [10] (see Box 1).\nThe input to STAHLp is a set of interconnected beliefs about 1) which substances are present before and after a particular reaction or 2) the chemical composition of each substance. These inputs are encoded using two types of variable: The functions (or programs) REACTS and COMPOSED OF, which operate on an unbounded space of discrete chemical names.\nSTAHLp's desired output is a coherent and consistent \u201ctheory\" a set of beliefs that entail the inputs and do not contradict each other. STAHLp uses a hard objective function to enforce this: The theory cannot contain any inconsistent equations containing \"nil\" (the empty set).\nSTAHLP solves this problem by applying a set of \u201cproduction rules\" to its beliefs at each step, generating further beliefs. If the system generates an inconsistent belief, STAHLp throws an error. At that point, a second set of \"belief revision\u201d heuristics is applied to try to identify the source of the inconsistency and correct it. After the lowest-cost correction is made, STAHLp applies its production rules to generate the updated theory entailed by"}, {"title": "STAHLp: Analysis", "content": "It is hard to argue with the assumption that reactions and compositions of substances were the central concepts in chemistry indeed, this was how Stahl himself defined its scope [18]. However, leading up to the chemical revolution, chemists had a different way of thinking about the internal structure of substances, in which observable substances arose from mixtures of the latent primitives of earth, water, and fire. A new name could not be added arbitrarily, and had to be placed within the existing ontological structure. Second, they would not have considered air-what we now call gas-to have chemical properties and enter into chemical combinations: The discrete name representation is too permissive, and \"oxygen\" is simply not a valid entry [19].\nIt also excludes relevant data. Most of the scientific work leading up to the discovery of oxygen was concerned with the sensory properties of substances,\u00b9 where they came from,2 and their weight. For example, there was actually a great deal of inconclusive or even negative evidence that metals apart from lead increased weight on calcination, detracting from the general statement that an air entered into metals. Similar arguments can be put forward for using a single function to represent a number of different reactions.\nThere is also the question of data selection. The creators of STAHLp include only two facts from the many heterogenous and often inconsistent observations and beliefs in 18th century chemistry. If they took into account others-for example, that when nitrous acid was poured on mercury colored vapors and fumes were given off-the model's conclusions may well have changed.\nFinally, the objective function for STAHLp is based on the detection of nil statements. Once again, this is a retrospective assumption that relies on the application of the conservation of mass. By contrast, in the 18th century it was widely held that substances could dissipate away to nothing-from diamond [19], to phlogiston itself [20]. Lavoisier had to create the right conceptual framework needed to support the use of equations in his investigations before using them."}, {"title": "Historical perspective: Lavoisier and the discovery of oxygen", "content": "Instead of being bound by a fixed type structure, Lavoisier made a number of conceptual innovations that were closely informed by the ontological structure of chemical knowledge [21]. The first was that \u201cair\u201d (what we would now call gas) could be involved in chemical reactions at all. Robert Boyle and others had offered a physical interpretation of air and derived various laws. But, as surprising as it sounds today, in Continental Europe air was not thought to enter into chemical combinations\u2014it was not a chemical type. By including air in the \u201cdefinitions of chemistry,", "equations\" seldom balanced, and the conservation of mass was used more as a post-hoc and abstract principle rather than a tool for quantitative purposes. Lavoisier developed the conceptual machinery to represent a reaction in terms of the total weight of materials at the start and end, and in doing so established the loss function to be optimized the inference of a consistent and useful set of equations. In other words, he constructed the right representation of the problem. This placed emphasis on the use of a density constant to relate changes in air volume to changes in weight.\nDiscrepancies in subsequent experiments led Lavoisier to the conclusion that there must be different subtypes of air with different densities. This led to the development of new equipment to measure those densities, and ultimately the finding that the air of the atmosphere was in fact a composite of these subtypes, rather than an elemental root. He then showed that the reduction of calx-of-mercury with charcoal produced a different air (carbon monoxide and carbon dioxide) than the reduction of calx-of-mercury without charcoal, eventually calling the latter air \u201coxygen.\u201d Lavoisier explained the differences between these two reactions by positing an underlying, potentially infinite range of chemical primitives that could take the familiar three states of matter depending on how much of the \u201cmatter of fire": "as coupled with them. This was the beginning of the main Chemical Revolution\u2014actually more of an"}, {"title": "Case Study 2: The electromagnetic field", "content": "By the middle of the 19th century, Michael Faraday had published a set of discoveries and observations related to electromagnetic induction: A current could be generated in a conducting wire in the presence of a strong permanent magnet by moving the magnet or the wire. Faraday recorded the intensity of magnetic force surrounding magnets of various shapes, strengths, and number, as well as electrical circuits, arguing that the most useful representation for these data was in terms of lines of magnetic force [23]. He had speculated on what might be the cause of these patterns, but had been largely unsuccessful [24]. The Scottish physicist James Clerk Maxwell derived a brilliant and creative theoretical solution to this problem that provides the foundation of modern physics\u2014the mathematical representation of the electromagnetic field.\nNo computational model has been proposed to emulate Maxwell's discovery. However, several influential models target the general setting of deriving physical laws from datasets of this sort [7, 8, 6]. Here we will focus on AI Feynman [7], an algorithm that uses symbolic regression to recover natural laws from physical data (see Box 2)."}, {"title": "AI Feynman", "content": "The input for AI Feynman is a data table, comprising data samples (rows) of a dependent variable and several independent variables (columns) that the modeler has specified in advance for each problem. Variables take continuous values, correspond to measurements of the physical system, and are augmented with type information representing their fundamental physical units (meter, second, kilogram, kelvin, and volt.).\nAI Feynman outputs predictions that match the dimensionality and type structure of the input, as well as a symbolic formula representing a theory of the observed system. The objective function uses a squared-error loss to assess predictions in the input space and a hard loss on whether its current solution is equivalent to the ground-truth expression.\nAI Feynman cycles through a set of computational strategies premised on commonalities in the functional forms of solutions to known physical problems (Figure 2). The inputs and outputs to physical problems tend to have units, which justifies algebraic manipulations based on their types (dimensional analysis). Solutions, or parts thereof, often contain polynomial expressions, justifying polynomial fitting; they tend to be compositional, justifying search over symbolic expressions; they tend to be smooth, justifying approximation by a neural networks; they tend to exhibit symmetry and separability, allowing a reduction of variables after transformation by the neural network components. If nothing else works, a fixed set of transformations are applied to the variables, including the transcendental functions.\nFor example, the data in \u201cmystery table 5\" comprises samples from one dependent variable, F, and nine independent variables corresponding to the masses and 3D positions of two objects, and Newton's constant G. The algorithm runs through its pre-determined steps: Algebraic manipulations yield a reduced set of dimensionless variables; the application of a neural network component identifies translational symmetry; a good factorization is found; then polynomials are fit to two subsets of transformed variables. The end result of this process is an equation that accounts for the data below some error threshold, \u20ac (see Figure 3).\""}, {"title": "AI Feynman: Analysis", "content": "Although the choice of input variables for AI Feynman might seem logical, they in fact correspond to quite an advanced stage of problem solving-when scientists have already constructed an idealized model for the system at hand. For example, Newton had to posit the idea of a gravitational constant, expressed implicitly in terms of proportionality; and he had to posit that these were the only influential factors when explaining gravity\u2014that action-at-a-distance was the correct framework to use, rather than the transmission of forces through an underlying medium. Similarly, Maxwell invented dimensional analysis to help solve difficult physics problems. But he did not always choose this representation\u2014for electromagnetism, for example, he chose to think about dynamical properties of the aether."}, {"title": "Historical perspective: Maxwell and the electromagnetic field concept", "content": "Nancy Nersessian has given a thorough cognitive-historical analysis of Maxwell and the development of the electromagnetic field concept [27]. In order to make progress given the ill-defined and heterogenous state of electrical science, Maxwell restricted his scope to Faraday's data on electromagnetic induction and lines of force. In 1855, he gave a rigorous and analyzable form to Faraday's observations and theoretical postulations using a descriptive mathematical model based on continuum mechanics of stresses in an underlying medium [28]. From 1861-1864, he tackled the deeper problem of providing a dynamical model that would explain these data [29, 30]. He began with magnetic phenomena, and showed that the constraints provided by his descriptive analysis could be fit by a vortex model. From this model he could calculate the magnetic force at any point in the medium by carrying over the system of equations describing the mechanical force exerted and replacing mechanical variables with magnetic ones [31].\nWhen he generalized this model to a medium composed of these vortices, however, he found the model unsatisfactory because of the friction caused by adjacent vortices. This brought to mind the idle wheels interposed between rotating machine gears, from which he introduced the idea of idle-wheel-particles to communicate between vortices. Idle-wheel particles provided a good way to model electrical current, so his next step was to include electromagnetic phenomena. But this required the relaxation of the model to allow the particles to translate in conductive medium, and to rotate without generating any friction. Using the new model, he could bring in a set of equations to represent electrical current as the flux density of these particles, driven by the circumferential velocity of the vortices [31]. Maxwell continued this process of domain relaxation and model building to include electrostatic phenomena and the polarization of light.\nA striking feature of Maxwell's problem solving is how explicit he was about the scope of his theories and the utility of intermediate models. Selectively restricting the domain allowed him to identify which parameters or features of the intermediate model were essential, and an analysis of those features afforded selective expansion of the domain-a process Nersessian has called \"generic abstraction\u201d [27]. Like Lavoisier, Maxwell was guided in this process of abstraction by ontological knowledge about the structure of different physical and mathematical systems, which also helped him sequentially assemble and modify the mathematical expressions underlying the model. Perhaps these idealized models played a role in Lavoisier's early investigations, albeit in a simpler form involving crude movements of air and changes of weight. This process is not captured by systems like AI Feynman, which are given the problem variables from the mature idealized model, and lack the flexibility to alter their own conceptual systems."}, {"title": "Case Study 3: Protein folding", "content": "Several major conceptual breakthroughs led to the \"protein folding problem.\" The discovery that proteins are linear chains of amino acids goes back to the seminal sequencing of insulin by Frederick Sanger in 1951 [32], based on the isolation and recursive extraction of hydrolyzed protein fragments using various media and electrical currents. Evidence that the overall 3D structure of proteins was important for their function, rather than the identity of individual amino acids,4 came from X-ray crystallography of oxygen-carrying proteins [33, 34], the structural effects of natural and artificial variation of amino acids [35], and catalytic-rate analyses with different cellular conditions, substrates, and inhibitors [36, 37].\nThe third development was the specification of the protein folding problem, primarily by Christian Anfinsen Jr. [38], who found that ribonuclease A would lose its enzymatic activity in artificial conditions and recover it when physiological conditions were re-established. This led to the \u201cthermodynamic hypothesis\" that the correctly folded protein occupied the minimum free-energy state in its natural cellular environment, and provided evidence against the competing hypothesis that proteins folded sequentially as they were synthesized. The remaining step was to characterize the physical process by which the protein folded.\""}, {"title": "AlphaFold2", "content": "One of the most successful recent discovery algorithms is AlphaFold2 [11], which predicts the 3D structure of a protein given its 1D amino-acid sequence (see Box 3). When it was released, AlphaFold2 brought the average molecular deviation for a protein down from 0.3 to 0.1 nanometers, which was precise enough for biologists to make use of.\nAlphaFold2's input is a multiple sequence alignment (MSA), which augments the protein of interest's 1D amino-acid sequence with additional rows containing similar amino-acid sequences from existing databases. If any of the MSA sequences have already had structures derived, 2D distograms of the pairwise distance between residues and a sequence of torsion angles between adjacent amino acid residues are added to the inputs.\nAlphaFold2 outputs a set of atomic co-ordinates, a confidence score in each residue's position, torsion angles between adjacent amino-acid backbones, the 2D distogram between residues, and a prediction of any masked parts of the MSA. The objective function during training contains a loss term for each of these representations, with the most important components penalizing the 3D deviations of heavy atoms in the amino-acid chain. The loss function during \"fine-tuning\u201d contains all of these terms, plus two extra terms that penalize the final structure for violating physical constraints.\nAlphaFold2 uses complex heuristics to solve this optimization problem, based on a great deal of biological and engineering knowledge. At a high level, the Evoformer module learns increasingly rich and abstract representations of the 1D primary structure and 2D distogram that the Structure module uses to build a 3D model of the protein. The network is trained end-to-end, meaning all operations are differentiable and the loss signal from the final 3D positions is back-propagated to inform the update of neural networks weights in all operations after the input.\nThe main biological insight behind the Evoformer module is that information about the 3D protein structure can be derived by comparing its primary sequence with the sequences of similar proteins in different organisms. Some of these sequences might have had their structures discovered experimentally, which can be used directly in the 2D distance representation. But even when no related structure exists, significant covariation of residues in two different positions across multiple organisms is an indication that they are close in 3D space. These 3D dependencies might be quite far away in the 1D representation (the primary sequence), so the inductive bias of attention, which can model longer dependencies [39], is more suitable than other deep learning methods. When a particular structure is available, it biases the attention mechanism to learn similar representations for amino acids that are physically close together, and when it is not, the information flows the other way, with the covariance between amino acid positions used to infer the 2D distances.\nThe Structure module uses the Evoformer's final representations to iteratively move rigid frames representing each amino-acid residue as close as possible to their ground truth cognates. After residues have been aligned in the main"}, {"title": "AlphaFold2: Analysis", "content": "AlphaFold2's success comes in large part from the engineering choice of problem statement. In particular, it does not solve the original \u201cproblem\u201d of protein folding, the time-evolving movement of the polypeptide chain from 1D denatured to 3D functional state. The authors relax the physical requirements to define a new, related problem: The prediction of a final folded state given the 1D sequence.\nThis choice was motivated by an abundance of sequence data, which can be used for the new, but not the old, problem. It was also based on a suite of models with the ability to use that information to solve the relaxed problem formulation-deep neural networks with attention mechanisms. The positive consequence of this choice is that some requirements of a solution to the original problem are met\u2014we can predict the structure of hydrophilic proteins with lots of analogous evolutionary sequences well. The negative consequence is that we don't have a model of the folding dynamics which can make good predictions of the structures of orphan molecules like antibodies, lipophilic molecules with no experimentally derived homologous structures, or the effect of a new mutation or ion on the final folded structure.\nThe choice of inputs and outputs reflect the new problem. Evolutionary correlations have been used for some time to make arguments about folded structures and function [40], but do not obviously inform folding dynamics. And,"}, {"title": "Understanding the hard problem", "content": "The previous sections depict a recurring pattern: Much progress in applications of AI to science has been made, but only with the aid of humans specifying the problem formulation. Thus, these systems are essentially solving the easy problem, not the hard problem. What makes the hard problem so hard?\nAn important and elusive feature of problem specification is that it is not a data modeling problem. The selection of what to model and and what constraints to condition on are antecedent to any data modeling problem. It is also not reducible to a representation learning problem, in the sense of figuring out how raw sensory input maps to abstract representations. Of course, that problem also needs to be solved, but first the scientist needs to know what problems the representations are being used to solve.\nSociological, aesthetic, and utility considerations enter at the problem specification stage. Building an AI scientist is as much about shaping its tastes, style, and preferences as it is about endowing it with powerful problem-solving abilities. Again, a look at how we train human scientists is instructive: A good graduate advisor educates students about what problems matter, what phenomena are interesting, which explanations count, and so on. These considerations can't be brushed aside as subjective factors irrelevant to the purely technical problems facing AI systems; they are in fact constitutive of those technical problems. Without them, the technical problems would not exist.\nA research program for attacking the hard problem should begin with the cognitive science of science [43], focusing on the understudied subjective, creative aspects discussed above and how they interact with the objective aspects of problem solving. However, this presents two immediate challenges. First, how can we gain the conceptual background necessary to understand scientists' innovations in a short enough time to iterate meaningful research? For most graduate students, arriving at the point where they can begin to generate meaningful and achievable problems within their field takes 2-5 years of dedicated higher education. Second, how can we gather enough results to make statistically robust arguments for any individual problem? There was, of course, only one Antoine Lavoisier.\nCognitive-historical analyses are one approach to deriving such insights [27, 42, 44]. In this methodology, modern cognitive theories are used to build hypotheses about how the scientists were thinking. Historical data can add content or temper these theories, and historical analyses and techniques can be used to make the retrospective analyses relatively unbiased, robust to historical contingencies, and generalizable to new contingencies. At the birth of a modern scientific field, the concepts and measurements are relatively undifferentiated, and can be acquired quickly. They are also, necessarily, edge-cases of creativity, where one or a group of scientists broke away from the normal tradition.\nSpending time observing scientists' behaviors in modern operating laboratories is another way to increase the amount of data available for cognitive scientists to build theories about problem specification. For instance, the construction of intermediate in-vitro models as sources of analogy has been hypothesized to explain the success of scientific research practices in biochemistry [45]. Studies of scientific collaborations between people from"}, {"title": "Towards scalable AI scientists that solve the hard problem", "content": "Once we understand what human scientists are doing with enough precision that we can formalize their activities, we can try to leverage these insights to build scalable AI scientists. At least initially, it is unlikely that these will be standalone systems, but rather more like research assistants or first-year grad students: Curious agents with some technical competence but in need of expert guidance. This guidance can come in the form of natural language instruction, reading curricula, and demonstrations. The growth of models beyond this requires the examination and emulation of the communal aspects of science and related cultural institutions. Lab meetings, conferences, and presentations and discussions are ultimately the place where judgements on the quality of a scientific problem are made.\nThe use of natural language processing for scientific discovery is at the heart of the recently proposed \u201cAI Scientist"}]}