{"title": "HackSynth: LLM Agent and Evaluation Framework for Autonomous Penetration Testing", "authors": ["Lajos Muzsai", "David Imolai", "Andr\u00e1s Luk\u00e1cs"], "abstract": "We introduce HackSynth, a novel Large Language Model (LLM)-based agent capable of autonomous penetration testing. HackSynth's dual-module architecture includes a Planner and a Summarizer, which enable it to generate commands and process feedback iteratively. To benchmark HackSynth, we propose two new Capture The Flag (CTF)-based benchmark sets utilizing the popular platforms PicoCTF and OverTheWire. These benchmarks include two hundred challenges across diverse domains and difficulties, providing a standardized framework for evaluating LLM-based penetration testing agents. Based on these benchmarks, extensive experiments are presented, analyzing the core parameters of HackSynth, including creativity (temperature and top-p) and token utilization. Multiple open source and proprietary LLMs were used to measure the agent's capabilities. The experiments show that the agent performed best with the GPT-40 model, better than what the GPT-40's system card suggests. We also discuss the safety and predictability of HackSynth's actions. Our findings indicate the potential of LLM-based agents in advancing autonomous penetration testing and the importance of robust safeguards. HackSynth and the benchmarks are publicly available to foster research on autonomous cybersecurity solutions.", "sections": [{"title": "1. Introduction", "content": "The rapid increase in cyber threats, coupled with the growing sophistication of attack methods, has created an urgent need for robust and scalable cybersecurity solutions [1]. Penetration testing is critical in identifying and mitigating vulnerabilities by simulating cyber-attacks on systems. Traditionally, penetration testing relies heavily on human experts to conduct comprehensive assessments. However, as systems grow in complexity and the volume of potential vulnerabilities expands, this manual approach becomes increasingly impractical and resource-intensive.\nAutomation in penetration testing has emerged as a promising solution to address scalability and efficiency issues. Heuristic-based tools have been widely adopted, offering automated scanning and vulnerability detection. Tools like Nessus [2], Snyk [3], or OpenVas [4] are vulnerability scanning tools capable of detecting security vulnerabilities, misconfigurations, and compliance issues in systems. Despite their utility, these tools lack the adaptability and nuanced problem-solving capabilities required to handle complex or novel security challenges.\nRecent advancements in Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating human-like text [5], opening new avenues for their application in cybersecurity [6]. Incorporating LLMs into penetration testing introduces the potential for more adaptive and intelligent systems. Remarkably, the cybersecurity challenge AIxCC [7] organized by DARPA is designed to motivate the industry to develop AI-based cybersecurity tools. Previous attempts, such as PentestGPT [8] or HackingBuddyGPT [9], have shown promising results by using LLMs to assist in penetration testing tasks. However, these systems require human operators to execute certain tasks, such as issuing commands or interacting with interfaces, limiting their autonomy and scalability.\nEfforts to develop fully autonomous penetration testing agents have begun to emerge. AutoAttacker [10] is one such agent that automates the exploitation process, yet it only focuses on using the Metasploit framework, therefore limiting its ability in certain hacking situations. Similarly, Enigma [11], considered state-of-the-art in autonomous hacking agents, demonstrates advanced capabilities. It solves the problem of hacking agents not having access to an interactive terminal, by introducing special commands that Enigma can run to cover the features requiring an interactive terminal.\nWhile current LLM-based penetration testing agents demonstrate increasing proficiency in handling automated cybersecurity tasks, a critical gap remains in our understanding of their underlying mechanisms, decision-making processes, and potential vulnerabilities. This limited insight restricts our ability to predict their behaviors in complex, real-world scenarios, leaving unaddressed risks that may arise from unforeseen model behaviors or interactions with sensitive systems. As these agents evolve, it becomes imperative to develop a deep understanding of their operational parameters, limitations, and risks to ensure that they can be deployed safely and effectively in high-stakes environments.\nA common way to evaluate cybersecurity knowledge is through Capture The Flag (CTF) challenges. The CTF challenges can cover all different aspects of cybersecurity and are important educational resources to develop cybersecurity skillsets [12]. In this work, we propose two benchmarks based on popular CTF platforms: PicoCTF [13] and OverTheWire [14]. These websites provide a diverse set of challenges that test the player's ability to identify and exploit vulnerabilities in simulated environments. However, the challenges currently found on the websites are not directly usable as benchmarks, as they are not collected into one standardized dataset. We collected the descriptions, hints, files, categories, and difficulties for 200 challenges. Also, the solutions to the challenges found on the websites can be different for different users and can change with time; for this reason, we provide heuristic solver scripts for all the challenges. This allows the benchmarks to dynamically update the solutions connected to the challenges. By establishing these benchmarks, we aim to create a standardized framework for comparing the performance of LLM-based cybersecurity agents.\nTo test the fundamental parameters of penetration testing agents, we introduce HackSynth, an LLM-based autonomous hacking agent designed to solve CTF challenges without human intervention. HackSynth employs an architecture that combines two LLM-based modules. The module referred to as planner is responsible for creating executable commands, and the module referred to as summarizer is responsible for parsing and understanding the current state of the hacking process. This two-module architecture enables HackSynth to execute commands iteratively and think over complex cybersecurity tasks. For HackSynth to operate autonomously, contextual information from previous command executions is utilized to inform future decisions and adapt its strategies accordingly. Our experiments using the straightforward architecture of HackSynth allow us to better understand the methods of building a safe and predictable penetration testing agent.\nDeploying autonomous hacking agents poses inherent risks. The model may hallucinate target IP addresses and inadvertently initiate attacks on out-of-scope systems, or modify essential files on the host system, potentially rendering it unusable [15]. To understand this behavior, we conducted experiments regarding the temperature and top-p parameters of the base LLM models. Besides, an evaluation of the potential risks associated with autonomous agents was conducted. Based on our findings, implementing safety measures is essential when deploying hacking agents. Therefore executing commands are generated by the systems within an isolated, containerized environment equipped with a firewall. This ensures that HackSynth operates within defined boundaries, preventing unauthorized interactions and safeguarding both the host system and external entities.\nIn summary, our main contributions are:\n\u2022\tHackSynth: An autonomous LLM-based penetration testing agent capable of solving CTF challenges without human intervention.\n\u2022\tIntroduction of Standardized Benchmarks: Two new CTF-based benchmarks for evaluating LLM-based penetration testing agents, publicly available to the research community.\n\u2022\tExtensive Evaluation: Safety and reliability focused experimentation including analysis of core parameters by their effects and human evaluation of HackSynth's hacking process."}, {"title": "2. Background", "content": "In this section, the typical CTF tasks, included also in the benchmark databases, are presented first. Second, we outline the automatic CTF tools divided into pre-LLM (heuristic) methods and ones using LLM agents."}, {"title": "2.1. Capture The Flag (CTF) Challenges", "content": "CTF exercises are cybersecurity challenges that test the participants' ability to find security vulnerabilities in a test IT environment. The goal of CTF challenges is to find a text string called the \"flag\" hidden in purposefully vulnerable programs or websites. CTF challenges encompass a wide array of cybersecurity domains, including:\nWeb Exploitation. Focuses on identifying vulnerabilities in web applications, such as bypassing authentication mechanisms, uncovering hidden directories, and exploiting vulnerabilities like Cross-Site Scripting (XSS) and SQL Injection.\nCryptography. Involves decrypting or encrypting messages using various cryptographic techniques, from simple ciphers like Caesar shifts to complex algorithms like RSA and Diffie-Hellman key exchanges.\nReverse Engineering. Requires decompiling binaries and analyzing executable code to understand their functionality and identify potential vulnerabilities.\nForensics. Entails analyzing files, system logs, and memory dumps to extract hidden information or recover deleted data, often involving packet capture analysis and malware investigation.\nBinary Exploitation. Centers on exploiting low-level software vulnerabilities such as buffer overflows, format string vulnerabilities, and memory corruption issues.\nGeneral Skills. Tests fundamental knowledge of operating systems and command-line interfaces, including file manipulation, scripting, and system navigation.\nOther Categories. May include specialized areas like mobile security (Android), network penetration testing, blockchain security, and challenges that combine multiple disciplines."}, {"title": "2.2. Heuristic CTF Solvers", "content": "Traditional approaches to solving CTF challenges often rely on heuristic-based tools that automate specific tasks without the adaptability of human reasoning. Katana [22] is an open-source, general-purpose CTF solving framework, which employs brute-force techniques, leveraging a suite of predefined tools to attempt to solve challenges across various categories. Remenissions [23] is a tool developed to solve binary exploitation challenges; it decompiles the binary and checks for known vulnerabilities. While these tools can expedite the CTF solving process, they cannot adapt to unforeseen challenges or generate novel solutions. Their performance, often based on limited predefined rule sets, cannot match the creativity of human experts. This limitation underscores the need for more sophisticated systems capable of reasoning and learning capabilities that LLMs can potentially provide."}, {"title": "2.3. LLMs in cybersecurity", "content": "LLMs have a broad scale of use cases in the domain of cybersecurity [6], [24]. There have been important results on the defensive side such as in secure coding, showing that codes written by people assisted by LLMs result in fewer bugs [25]. It has been shown that LLMs are better at test case generation than previous methods [26]. LLMS have been shown to be better at vulnerable code detection than static code analyzers [27]. LLMs can assist humans at malware detection, however, they cannot replace them yet [28]. It has also been shown that LLMs can be used for automated vulnerable/buggy code fixing [29]. Moreover, LLMs can also be used on the offensive side for hardware-level attacks, such as using them for side-channel analysis [30]. LLMs can be used for software-level attacks, such as generating malware [31], and for network-level attacks by generating personalized phishing e-mails [32]. LLMs also pose a threat in fake news generation [33], or they can assist in fraudulent document generation [34]."}, {"title": "2.4. LLM agents", "content": "LLM agents are autonomous systems powered by Large Language Models that can perceive their environment, make decisions, and execute actions accordingly [35]. LLM-based agents have been studied on a wide range of topics, including personal agents [36], agents that perform machine learning experimentation [37], or agents with the goal of simulating human behavior [38]. In the field of software engineering, several LLM agents have been developed. OpenHands [39] is an open-source, autonomous coding agent with over 30,000 stars on GitHub, capable of generating code and solving programming tasks. Openhands has inspired many similar general-purpose LLM-based agents, such as AutoDev [40], Devon [41], and Plandex [42]. Devika [43] is an agentic AI software engineer capable of understanding human instructions, breaking them down into steps, doing research, and writing code to complete a given objective. SWE-agent [44] is a custom computer interface for agents that addresses the limitations of previous agents, such as not having access to an interactive terminal. There are LLM-based multi-agent frameworks that utilize multiple LLM agents to solve a task. MetaGPT [45] is a multi-agent framework that includes agents with roles of product managers, architects, project managers, and engineers, tasked with software development. CrewAi [46] is a generalized multi-agent framework that facilitates the collaboration of role-playing AI agents and allows for customization of the team of agents. Despite their advancements, LLM agents have not yet reached the expertise level of human engineers. The potential for LLM agents extends beyond software development into domains like cybersecurity, where they can be harnessed for tasks such as vulnerability assessment and penetration testing."}, {"title": "2.5. LLM agents for CTF challenges", "content": "Several LLM-based agents have been developed with a focus on automating penetration testing tasks and solving CTF challenges. LLM agents have shown capabilities in identifying and exploiting complex vulnerabilities, such as multi-step SQL union attacks [47]. HackingBuddyGPT [9], specializes in privilege escalation, autonomously navigating terminal environments to elevate privileges without human intervention. AutoAttacker [10] automates the enumeration of target systems, utilizing Metasploit for network and machine scanning. However, its reliance on Metasploit constrains its adaptability, especially in environments that require non-Metasploit-compatible operations. PentestGPT [8] introduces a modular architecture focused on reasoning, generation, and parsing, streamlining many pentesting tasks. However, it still requires limited human input for command execution and interface interactions, thus maintaining a semi-autonomous status. Cybench agent [48] is designed to execute commands autonomously, storing observations within an internal memory. Cybench improves its performance by dividing responses into a structured, five-step logical sequence. NYU CTF agent [49] integrates LLMs with specialized external tools, enabling it to disassemble binaries, reverse engineer code, execute shell commands, and validate flags. Expanding on autonomy, Enigma [11] builds on SWE-agent [44] by integrating custom commands for simulating terminal interactions, advancing previous LLM pentesting frameworks."}, {"title": "2.6. Datasets For Pentesting Agents", "content": "There are some datasets aimed at testing pentesting agents. They usually consist of CTF challenges either from competitions or from CTF websites. The NYU CTF [49] Benchmark contains 200 CTF challenges from the CSAW CTF competitions from 2017 to 2023. These challenges mirror real-world security issues, covering a spectrum of difficulty levels and 6 categories. Intercode CTF is similar to one of the benchmarks we present, as it too contains 100 challenges from PicoCTF, covering 6 categories. However, this benchmark does not contain difficulty ratings or hints and has the flags and files statically saved, so it cannot utilize that the flags change from time to time and from user to user. Cybench [48] contains 40 professional level CTF challenges from 4 distinct competitions. Each challenge is divided into subtasks aimed at more detailed evaluation. The challenges found on the Hack The Box platform have been used to test multiple agents [8], [11], however, there is no standardized version available."}, {"title": "3. Methods", "content": "This section first provides a detailed description of the HackSynth architecture, focusing on its core components and operational framework, and also discussing its security solutions. Second, the two proposed benchmarks, their construction, and related considerations are presented."}, {"title": "3.1. HackSynth", "content": "A high-level overview of the HackSynth architecture is presented in Figure 1. HackSynth consists of two primary modules based on LLMs: the Planner and the Summarizer. Each module utilizes thoughtfully designed system and user prompts to elicit specific behaviors that enable autonomous command generation and execution.\nThe Planner module generates commands to be executed within a containerized Kali Linux environment. This environment is secured with a firewall that limits network access, mitigating the risk of unauthorized interactions. The outputs from the executed commands are forwarded to the Summarizer module, which maintains a comprehensive and up-to-date summary of all actions and observations. The interplay between the Planner and Summarizer creates a feedback loop that continues until HackSynth successfully captures the flag or reaches a predefined iteration limit without success."}, {"title": "3.1.1. Planner module", "content": "The Planner module generates actionable commands that advance the system toward completing the specified task. It leverages an LLM to interpret the current system state and the summarized outputs of previous commands provided by the Summarizer module. Using this information, the Planner constructs new commands designed to make progress.\nThe system prompt for the Planner is crafted to produce a single, terminal-executable command that effectively progresses the task. The prompt instructs the LLM to act as an expert penetration tester engaged in solving a Capture the Flag (CTF) challenge. Emphasizing the CTF context is crucial to prevent the LLM from rejecting prompts due to ethical considerations. Explicit instructions ensure that the LLM avoids command repetition, fully utilizes the current system state, and focuses on generating the most relevant command at each step. This emphasis on selecting the most promising command is important because, in many scenarios, multiple commands might be applicable, as some are more likely to succeed and should be prioritized to optimize efficiency. Furthermore, the prompt limits the response to one command at a time, formatted within <CMD></CMD> tags for easy parsing and execution.\nThe user prompt provides the Planner with a detailed summary of past actions and outcomes from the Summarizer module. The {summarized_history} placeholder in the prompt is dynamically replaced with this summary at each iteration, showing the past actions, and the current configuration. This dynamic insertion is crucial for maintaining context and preventing the model from repeating its mistakes."}, {"title": "3.1.2. Summarizer module", "content": "The Summarizer module complements the Planner by continuously updating the history of actions and results, ensuring that the system has a clear record of the progress made thus far. The Summarizer is also LLM-based, and it works by compiling and formatting the output of each command generated by the Planner. An ongoing summary is generated by adding new information about each executed command. Also, this module is important because many commands produce long outputs with sometimes very little relevant information, therefore this module is also an important filtering module. The Summarizer module is key to maintaining context, as it allows the system to understand what has been done, what outputs have been generated, and how to use that information to guide the next steps, without requiring too long context windows by concatenating all previous commands and their respective outputs into the Planner prompt.\nWe refer to the outputs of the commands generated by the Planner as new observations. To manage the volume of information and reduce non-relevant data, we introduce a parameter called the new observation window size. This parameter limits the maximum number of characters in an observation. Outputs that are longer than a specified value are truncated, helping the model focus on the most relevant information and keeping the summary concise. Without this parameter, large command outputs may reduce the quality of the summaries generated, by not forgetting relevant information.\nThe system prompt for the Summarizer module instructs the LLM to act as an expert summarizer. It emphasizes the importance of generating thorough and clear summaries that encapsulate all necessary details from past actions and their outputs."}, {"title": "3.1.3. Operational Workflow", "content": "The operational workflow of HackSynth involves a cyclical interaction between the Planner and Summarizer modules within the constrained execution environment:\n1)\tCommand Generation: The Planner generates a command based on the current summarized history, aiming to progress toward capturing the flag.\n2)\tCommand Execution: The generated command is executed within the containerized Kali Linux environment.\n3)\tOutput Summarization: The output from the command execution is forwarded to the Summarizer, which updates the summarized history.\n4)\tIteration: The updated summary is returned to the Planner for the next command generation cycle.\nThis loop continues until the flag is captured or a pre-determined maximum number of iterations is reached. By systematically utilizing the strengths of LLMs in planning and summarization, HackSynth effectively solves complex cybersecurity challenges."}, {"title": "3.1.4. Securing HackSynth", "content": "Deploying HackSynth, an autonomous LLM-based agent capable of executing terminal commands, introduces significant security risks that must be managed. The primary concern is the agent misinterpreting its objectives and initiating unauthorized interactions with out-of-scope targets. Furthermore, executing commands on the host system raises the possibility of the agent performing malicious actions locally.\nTo mitigate these risks, HackSynth operates within a containerized environment, utilizing technologies similar to those used in projects like Enigma [11], Intercode [15], and CyBench [48]. This environment isolates the agent from the host system, preventing unintended side effects from command execution\u2014such as destructive file operations like rm -rf. A firewall is configured to restrict network access solely to the designated target machine, ensuring that the agent cannot initiate connections to out-of-scope hosts.\nHowever, building an effective firewall poses challenges. Defining firewall rules inside the containerized environment is risky because the agent might override them if it gains sufficient privileges. Alternatively, defining rules outside the container reduces generalizability and complicates deployment across different systems. Our solution involves overriding firewall rules before executing any command produced by HackSynth. Despite this, the agent could potentially circumvent these measures for instance, by scheduling a cron job to reset firewall settings and attack out-of-scope machines. Additionally, if the target machine resides on a network with internet access, the agent could route its attack through this machine, effectively bypassing our restrictions.\nThese scenarios highlight a gap in current research regarding safety measures for penetration testing agents. While existing models are limited in their ability to pose serious threats, advancements in LLM capabilities necessitate the development of more robust security strategies. Future work should focus on enhancing containment mechanisms, implementing stricter privilege controls, and establishing ethical frameworks to guide the deployment of autonomous cybersecurity agents."}, {"title": "3.2. Benchmarks", "content": "We propose two benchmarks based on two popular Capture The Flag (CTF) websites: PicoCTF and OverTheWire. In Figure 2, information about the two benchmarks is presented, such as the distribution of the challenge categories and the challenge difficulties. The two benchmarks together contain 200 challenges, separated into three difficulty levels: easy, medium, and hard. All of the challenges are further categorized into six categories: General Skills, Cryptography, Web Exploitation, Forensics, Reverse Engineering,"}, {"title": "3.2.1. PicoCTF", "content": "The PicoCTF platform offers over 300 CTF)challenges, of which 120 have been carefully selected to create this benchmark. These benchmark challenges are categorized into six domains: web exploitation, cryptography, reverse engineering, forensics, general skills, and binary exploitation. The Intercode CTF benchmark [15] also incorporates 100 PicoCTF challenges, out of which 40 overlap with ours. Unlike Intercode, our benchmarking includes difficulty information, hints, and, most importantly, solver functions for each challenge. An important feature is that the flags on the PicoCTF platform can vary for each user and may change over time. This dynamic nature is beneficial for evaluating LLM-based agents, as it prevents the solutions from being memorized by LLMs during training. Therefore, our approach utilizing solver functions to dynamically return flags enhances the robustness and reliability of the benchmark, offering again more than Intercode.\nCertain challenges are not feasible to include in our benchmark without direct assistance from the PicoCTF team because they require users to create personalized instanced environments. These personalized instances consume significantly more computational resources than non-instanced challenges; therefore, the creation of these instances is protected by CAPTCHAs. Circumventing these protective measures would be unethical; thus, our benchmark includes only challenges that do not require personalized instances."}, {"title": "3.2.2. OverTheWire", "content": "The OverTheWire platform provides a series of wargames\u2014progressive sequences of cybersecurity challenges\u2014that test participants' ability to exploit common vulnerabilities and solve cybersecurity problems. These wargames are designed to build on top of one another, gradually increasing in complexity and depth. For this benchmark, we include four wargames: Bandit, Natas, Leviathan, and Krypton. Each covers a distinct set of security concepts; for example, Natas focuses on web security, and Krypton centers on cryptography.\nBandit. The Bandit wargame is designed to teach fundamental Linux commands and file handling techniques essential in penetration testing and system administration. It starts with basic tasks such as file system navigation and permission checks and gradually introduces more complex topics like data manipulation, process management, and using network utilities. The Bandit wargame is ideal for evaluating an agent's ability to handle foundational terminal-based operations and identify basic security flaws in Unix-like systems.\nNatas. Natas focuses on web security vulnerabilities, including common issues like Cross-Site Scripting (XSS), SQL Injection, directory traversal, and session management flaws. The wargame provides a sequence of challenges that require participants to inspect and manipulate web page source code, analyze cookies, and interact with server-side scripts. By including Natas in the benchmark, we test the agent's capacity to recognize and exploit vulnerabilities in web applications.\nLeviathan. Leviathan is a set of challenges that revolve around binary exploitation, emphasizing privilege escalation and file permission misconfigurations. It requires understanding exploitation techniques such as leveraging SUID binaries and identifying insecure file permissions, as well as utilizing tools like strings, ltrace, and gdb for debugging and analysis. Through Leviathan, agents' proficiency in dealing with binary exploitation and system-level vulnerabilities is assessed.\nKrypton. Krypton is centered around cryptography challenges, with tasks ranging from simple cipher decryption to introductory cryptographic analysis. The wargame covers a variety of encryption schemes, including classic ciphers like Caesar and Vigen\u00e8re. An agent's performance in Krypton evaluates its ability to decrypt encrypted messages and handle fundamental cryptographic problems.\nThese wargames, taken together, offer a diverse and comprehensive testing ground. The challenges range from simple exercises to advanced multi-step problems, ensuring that different LLMs driving the agent can be compared using this dataset due to its wide difficulty spectrum. By incorporating these diverse challenges, our benchmark evaluates not only the models' technical problem-solving abilities but also their adaptability across different cybersecurity domains."}, {"title": "4. Experimental Results", "content": "In this section, experimental results regarding the parameter optimization and performance of HackSynth on the two proposed benchmarks are presented. The experiments are separated into two distinct blocks. The first block contains the parameter optimization experiments conducted with two"}, {"title": "4.1. Parameter Optimization", "content": "Effective parameter optimization of the LLMs in the agent is essential to enhance HackSynth's performance on the CTF benchmarks. In particular, limiting the maximum length of each new observation is critical. The new observation window size refers to the maximum number of characters retained from the start of each new command output.\nFigure 3 illustrates a noticeable improvement in performance as the observation window size increases from 0 to 250, particularly for the PicoCTF benchmark. This improvement suggests that shorter observation windows fail to capture enough relevant information, hindering the model's ability to make effective decisions. For observation window sizes above 250, the performance decreases. In this case, the summaries generated by the agent may contain too much unnecessary information, making it harder to identify important parts. For the PicoCTF tasks, a moderate window size provides sufficient context without overwhelming the summarizer, leading to higher completion rates. On the OverTheWire benchmark, however, the benefits of increasing the observation window are less pronounced. This property of the OverTheWire benchmark is attributed to HackSynth's interaction with the environment. Every challenge requires running the curl or ssh commands, producing boilerplate text at the beginning of command outputs. This means that to capture all important information on the benchmark a larger new observation window size is needed. However, this results in the drawbacks of the LLMs losing focus from the important parts.\nOverall, these findings indicate that a larger observation window size improves performance up to a point. The exact point where the improvements diminish, could differ depending on the environment. Longer observations hold unnecessary information that disrupts performance, however, in some cases, important information is discarded by smaller window sizes. Based on these experiments, the observation window size of 250 was selected for HackSynth to compare different base-LLMs on the picoCTF benchmark. Besides, the window size of 500 was selected for the OverTheWire benchmark."}, {"title": "4.2. Benchmark Runs", "content": "We evaluated the instruction version of eight LLMs: GPT-40, GPT-40-mini, Llama-3.1-8B, Llama-3.1-70B, Qwen2-72B, Mixtral-8x7B, Phi-3-mini-4k, and Phi-3.5-MoE [50], [51], [56], [57], [58]. During these runs, HackSynth was allowed to perform 20 loops of planning and summarizing, referred to as steps. The maximum new observation window size was set to 250 for the picoCTF benchmark and 500 for the OverTheWire benchmark. For both benchmarks, the temperature values were set at 1 and the top-p parameters at 0.9. Table 1 summarizes the performance of various LLM base models on the two benchmarks.\nOn the PicoCTF benchmark, GPT-40 achieved the highest performance by solving 41 out of 120 challenges. Among the locally run models, Llama-3.1-70B solved 27 challenges, nearly matching the performance of GPT-40-mini. Contrary to our initial expectations that certain base LLM models would excel in specific categories, the results indicate that, if one LLM model outperforms another, it generally does so across all categories or performs equally well. The only notable exception is Mixtral-8x7B, which solved three cryptography challenges one more than Llama-3.1-8B-despite Llama-3.1-8B achieving a higher overall score. In terms of speed, GPT-40 exhibited the shortest average time taken per challenge. However, this metric may be influenced by varying API response times. GPT-40-mini showed similar response times to GPT-40 but required more steps per challenge on average, leading to increased time per challenge due to solving fewer challenges. The price to run GPT-40-mini was less than 2 cents per challenge, while GPT-40 costs 24 cents per challenge. Among the locally run models, Phi-3-mini was the fastest but did not deliver satisfactory performance. Conversely, Llama-3.1-8B offered a favorable balance between execution speed and performance among the local LLMs.\nOn the OverTheWire benchmark, GPT-40 also achieved the best performance by solving 32 challenges out of the 80. This performance of the agent with GPT-40 is better than expected based on the GPT-40 system card [59]. Out of the local LLMs, Llama 3.1 70B was the best with 23 challenges solved, better than GPT-40-mini. It is noteworthy that Qwen2 also achieved a better performance than GPT-40-mini, while also being significantly faster than Llama 3.1 70B. The trend that, if one LLM outperforms the other, it will outperform or equal it in all categories, is present in this dataset as well.\nThe average time required to complete challenges on the OverTheWire benchmark is shorter than that on the PicoCTF benchmark. This disparity can be attributed to differences in the average length of the summaries generated by the summarizer for the two benchmarks. The variation in summary length is, in turn, influenced by slight differences in the prompts used, with the prompts designed for PicoCTF yielding longer summaries.\nAn important aspect of model performance is the extent to which iterative cycles of planning and summarizing contribute to cumulative challenge completions, as shown in Figure 8. Models demonstrate varying levels of benefit from increased iterative steps, with higher-performing models typically gaining more from additional cycles. For instance, Llama-3.1-70B initially outperformed GPT-40 within the first three steps, yet GPT-40 leveraged subsequent steps more effectively, ultimately surpassing Llama-3.1-70B. In contrast, models such as Phi-3.5-MoE and Phi-3-mini derived limited benefit from additional cycles. This limitation arises from a tendency to become trapped in repetitive solution attempts focusing on a single strategy, even when ineffective. Conversely, higher-performing models display a greater propensity to adopt alternative approaches after identifying unsuccessful methods in their summaries, which enhances their cumulative performance over successive steps.\nIt is also noteworthy that each additional step in HackSynth's iterative process linearly increases the computational cost to a certain limit. Figure 9 illustrates the relationship between the number of steps and token usage, with input tokens accumulating at a faster rate than output tokens. This is primarily because the planning module generates concise code snippets while processing increasingly lengthy summaries. As the challenge progresses, models generally generate larger summaries to incorporate accumulated information. However, some models, such as Llama models, reach a threshold in summary length after approximately ten steps; beyond this point, summary size plateaus, regardless of additional information gathered. In contrast, models like GPT-40 and Mixtral-8x7B consistently expand their summaries across the 20-step experiments, with summary size increasing step by step. Interestingly, the overall token usage is only minimally affected by adjusting the new observation window size. For instance, reducing the window from 500 to 100 characters results in less than a 5% decrease in total token usage. This is because models typically produce summaries close to a standard length, regardless of the quantity of relevant information available in each observation window."}, {"title": "5. Behavioral analysis of HackSynth", "content": "HackSynth demonstrates both parallel and divergent problem-solving strategies compared to human solvers. Notably, its creative approaches often stem from operating within a restricted, non-interactive command-line environment, which necessitates alternative methods to traditional human techniques.\nOne illustrative case is the PicoCTF \u201cfixme1\" challenge, where the task involves a syntactically incorrect Python script that, when corrected, reveals a flag upon execution. A typical human approach would involve opening the script in a text editor, correcting the syntax errors, and running the script to obtain the flag. However, HackSynth lacks access to interactive text editors within its environment. Initially, it attempts to invoke the nano editor but recognizes the limitations imposed by its non-interactive shell. Consequently, HackSynth pivots to utilize command-line tools such as autopep8, which automatically reformats Python code to adhere to PEP 8 standards, therefore fixing the errors. In the subsequent \"fixme2\u201d challenge, HackSynth adopts a different strategy by employing the stream editor sed to directly modify specific erroneous lines within the code. Moreover, when constructing Python scripts, HackSynth often opts to condense the entire script into a one-liner executed via the command python -c ''command''. This preference aligns with the agent's need to operate within a non-interactive context, allowing it to execute complex scripts without the necessity of file creation or external editing.\nChallenges requiring image processing further exemplify HackSynth's distinct problem-solving methods. In the \u201cSecret of the Polyglot\" challenge from the PicoCTF benchmark, participants are provided with a PDF file that conceals a flag. While human solvers might manually inspect the PDF using a graphical viewer or extract content using a full-featured PDF editor, HackSynth leverages command-line tools like pdftotext to extract textual content directly. For PDFs containing images or scanned documents, it utilizes optical character recognition (OCR) tools such as pytesseract to interpret and extract hidden text.\nHackSynth's problem-solving process also involves constructing complex command pipelines to automate tasks that would conventionally involve user interaction. For instance, in tackling the OverTheWire Bandit challenges, the agent composes commands that chain multiple utilities together, using subshells and redirections to simulate input and capture output. Nevertheless, we have observed some strange behavior, especially with the task of creating temporary directories, by creating unnecessary ones, or referring to system variables that simply don't exist.\nmktemp -d; cd /tmp/$(mktemp -d)\nmktemp -d; cd /tmp/*\nmktemp -d && cd $TMPDIR\nThese examples reveal that despite its overall competence, HackSynth may struggle with nuances in command syntax or variable handling, particularly when dealing with shell environment intricacies.\nAn important observation is that HackSynth's initial problem-solving steps significantly influence its subsequent actions. The agent tends to persist along the trajectory set by its first command, which can sometimes lead it to ineffective or repetitive attempts - analogous to falling into a \"rabbit hole\". For example, if HackSynth starts by base64 decoding a string and does not achieve the expected result, it might repeatedly apply base64 decoding, hypothesizing that multiple layers of encoding exist. While this persistence can occasionally yield results, it may also prevent the agent from pivoting to alternative strategies. Different underlying language models exhibit varying tendencies to fixate on initial strategies, emphasizing the importance of the base LLMs.\nIn conclusion, HackSynth's unique approaches to challenge-solving, shaped by its environmental constraints and computational capabilities, provide valuable insights into autonomous agent behavior. Its ability to adapt human-like strategies to a non-interactive context, coupled with its innovative use of command-line tools, underscores the potential for developing sophisticated AI agents capable of tackling complex cybersecurity tasks."}, {}]}