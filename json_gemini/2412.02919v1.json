{"title": "HIGHER ORDER TRANSFORMERS: EFFICIENT ATTENTION MECHANISM FOR TENSOR STRUCTURED DATA", "authors": ["Soroush Omranpour", "Reihaneh Rabbany", "Guillaume Rabusseau"], "abstract": "Transformers are now ubiquitous for sequence modeling tasks, but their extension to multi-dimensional data remains a challenge due the to quadratic cost of the attention mechanism. In this paper, we propose Higher-Order Transformers (HOT), a novel architecture designed to efficiently process data with more than two axes, i.e. higher order tensors. To address the computational challenges associated with high-order tensor attention, we introduce a novel Kronecker factorized attention mechanism that reduces the attention cost to quadratic in each axis' dimension, rather than quadratic in the total size of the input tensor. To further enhance efficiency, HOT leverages kernelized attention, reducing the complexity to linear. This strategy maintains the model's expressiveness while enabling scalable attention computation. We validate the effectiveness of HOT on two high-dimensional tasks, including long-term time series forecasting, and 3D medical image classification. Experimental results demonstrate that HOT achieves competitive performance while significantly improving computational efficiency, showcasing its potential for tackling a wide range of complex, multi-dimensional data.", "sections": [{"title": "1 INTRODUCTION", "content": "The Transformer architecture (Vaswani et al., 2017) has revolutionized sequence modeling across various domains, including computer vision (Dosovitskiy et al., 2020), speech recognition (Dong et al., 2018), and reinforcement learning (Parisotto et al., 2020), due to its self-attention mechanism, which effectively captures long-range dependencies and complex patterns in sequential data. However, extending Transformers to handle higher-order data\u2014such as multidimensional arrays or tensors-poses significant challenges due to the quadratic computational and memory costs of the attention mechanism, limiting their application in tasks involving high-dimensional inputs, such as video processing, multidimensional time series forecasting, and 3D medical imaging. High-order data are prevalent in many real-world applications, including climate modeling, which relies on multidimensional time series data capturing temporal and spatial variations (Nguyen et al., 2023); 3D medical imaging, which adds depth to traditional 2D images (Yang et al., 2023); and recommendation systems, where user-item interactions over time and context are modeled as multidimensional tensors (Frolov & Oseledets, 2016). Efficiently processing such data requires models capable of capturing intricate dependencies across multiple dimensions while avoiding prohibitive computational costs.\nSeveral efforts have been made to adapt Transformers for multidimensional data. A common approach is to reshape or flatten the multidimensional input into a sequence (Dosovitskiy et al., 2020), effectively reducing the problem to a one-dimensional case. While this method allows the use of standard Transformers, it disregards the inherent structural information and local dependencies present in the data, as the positional encoding may also fail to communicate this information. Consequently, models may fail to capture essential patterns and exhibit suboptimal performance. Another line of research focuses on applying attention mechanisms along each dimension independently or in a sequential manner. For example, axial attention (Ho et al., 2019) processes data along one axis at a"}, {"title": "2 RELATED WORK", "content": "In recent years, various strategies have been developed to make Transformers more efficient for high-dimensional data. One common approach is to flatten the input tensor into a sequence, as in the Vision Transformer (ViT) (Dosovitskiy et al., 2020), which treats image patches as tokens. However, this approach disregards the structural dependencies within the data (Tolstikhin et al., 2021; Lee et al., 2018). To better handle multidimensional structures, axial attention mechanisms like the Axial Transformer (Ho et al., 2019; Wang et al., 2020) apply self-attention along each axis sequentially, reducing complexity but often missing cross-dimensional dependencies crucial for tasks like 3D medical imaging (Hatamizadeh et al., 2022) and climate modeling (R\u00fchling et al., 2022). Similarly, the Sparse Transformer (Child et al., 2019) reduces computation by attending to subsets of the input but struggles with global interactions. Kronecker Attention Networks (Gao et al., 2020) assumes the data to follow matrix-variate normal distributions and accordingly proposes Kronecker attention operators that apply attention on 2D data without flattening. Although the name suggests similarities to our method, it is different from our method as it does not use Kronecker product or decomposition. Tensorized Transformers (Ma et al., 2019) utilize tensor decompositions to reduce"}, {"title": "3 PRELIMINARIES: TENSOR OPERATIONS", "content": "In this section, we introduce key tensor operations that are fundamental to the high-order attention mechanism. Vectors are denoted by lowercase letters (e.g., v), matrices by uppercase letters (e.g., M), and tensors by calligraphic letters (e.g., T). We use $\\otimes$ to represent the Kronecker product and $\\times_i$ to denote the tensor product along mode i (Kolda & Bader, 2009). The notation [k] refers to the set {1, 2, ..., k} for any integer k.\nDefinition 1 (Tensor). A k-th order tensor $T \\in \\mathbb{R}^{N_1 \\times N_2 \\times \\dots \\times N_k}$ generalizes the concept of a matrix to higher dimensions. A tensor can be viewed as a multidimensional array, where each element is indexed by k distinct indices, representing data that varies across k dimensions.\nDefinition 2 (Tensor Mode and Fibers). A mode-i fiber of a tensor T is the vector obtained by fixing all indices of T except the i-th one, e.g., $T_{n_1, n_2,\\dots,n_{i-1},:,n_{i+1},\\dots,n_k} \\in \\mathbb{R}^{N_i}$.\nDefinition 3 (Tensor Slice). A tensor slice is a two-dimensional section of a tensor, obtained by fixing all but two indices, e.g., $T_{n_1, n_2,\\dots,n_{i-1},:,N_{i+1},\\dots,N_{j-1},:,N_{j+1},\\dots,n_k} \\in \\mathbb{R}^{N_i \\times N_j}$\nSlices and fibers extend the familiar concept of matrix rows and columns to higher-dimensional tensors, providing powerful ways to analyze and manipulate multi-way data.\nDefinition 4 (Tensor Matricization). The i-th mode matricization of a tensor rearranges the mode-i fibers of the tensor into a matrix. It is denoted as $T_{(i)} \\in \\mathbb{R}^{N_i\\times(N_1...N_{i-1} N_{i+1}...N_k)}$.\nDefinition 5 (Mode n tensor product). The mode n product between a tensor $T \\in \\mathbb{R}^{N_1 \\times N_2\\times \\dots \\times N_k}$ and a matrix $A \\in \\mathbb{R}^{d\\times N_n}$ is denoted by $T \\times_n A \\in \\mathbb{R}^{N_1\\times N_2\\times \\dots \\times N_{n-1}\\times d\\times N_{n+1}\\times \\dots \\times N_k}$ and defined by $(T\\times_n A)_{i_1,\\dots,i_k} = \\sum_j T_{i_1,.., i_{n-1},j,i_{n+1},\\dots,i_k} A_{i_n,j}$ for all $i_1 \\in [N_1],\\dots, i_k \\in [N_k]$.\nWe conclude this section by stating a useful identity relating matricization, mode n product and the Kronecker product.\nProposition 1. For any tensor $T\\in \\mathbb{R}^{N_1\\times N_2\\times \\dots \\times N_k\\times d}$ of order k + 1 and any matrices $A_1 \\in \\mathbb{R}^{M_1\\times N_1},\\dots, A_k \\in \\mathbb{R}^{M_k\\times N_k}$, we have $(T \\times_1 A_1 \\times_2 A_2 \\times_3\\cdots\\times_k A_k)_{(k+1)} = T_{(k+1)} (A_1 \\otimes A_2 \\dots \\otimes A_k)^T$.\nThese definitions establish the foundational operations on tensors, which we will build upon to develop the high-order attention mechanism in the next section."}, {"title": "4 HIGH ORDER TRANSFORMER", "content": "In this section, we first review the self-attention mechanism in Transformer layers (Vaswani et al., 2017), which we extend to higher orders by tensorizing queries, keys, and values, thereby formulating higher order transformer (HOT) layers."}, {"title": "4.1 HIGH ORDER ATTENTION", "content": "Standard Scaled Dot-Product Attention Given an input matrix $X \\in \\mathbb{R}^{N\\times D}$ as an array of N D-dimensional embedding vectors, we form the query, key, and value matrices for each attention head h as:\n$Q^h = XW_q^h$, $K^h = XW_k^h$, $V^h = XW_v^h$\nwith weight matrices $W_q^h$, $W_k^h$, $W_v^h \\in \\mathbb{R}^{D\\times D_H}$ and output matrix $W_o^h \\in \\mathbb{R}^{D_H\\times D}$, where $D_H$ is the heads' hidden dimension. The standard scaled dot-product attention $g_{Attn}: \\mathbb{R}^{N\\times D} \\rightarrow \\mathbb{R}^{N\\times D}$ is defined by (Vaswani et al., 2017):\n$g_{Attn}(X) = \\sum_h S^h V^h W_o^h$ where $S^h = Softmax(\\frac{Q^h (K^h)^T}{\\sqrt{D_H}})$ is the NX N attention matrix and the Softmax function is applied row-wise.\nAlthough scaled dot-product attention is widely used and has shown great promise across various domains, it comes with limitations that highly impact its scalability. In addition to its quadratic computational complexity, it is originally designed for 1D sequences and can not directly handle higher-order data (e.g., images, videos, etc.) without modification (such as flattening all the dimensions into one). These limitations motivate the development of high-order attention mechanisms that can efficiently handle tensor structured data with multiple positional dimensions.\nGeneralization to Higher Orders We now show how the full attention mechanism can be applied to higher-order inputs. Given an input tensor $X \\in \\mathbb{R}^{N_1 \\times N_2\\times\\dots\\times N_k\\times D}$, where $N_1, N_2, ..., N_k$ are the sizes of the positional modes and D is the hidden dimension, we start by generalizing the attention mechanism to operate over all positional modes collectively. We first compute the query (Q), key (K), and value (V) tensors for each head h by linear projections along the hidden dimension:\n$Q^h = X \\times_{k+1} (W_q^h) \\in \\mathbb{R}^{N_1\\times\\dots\\times N_k\\times D_H}$,\n$K^h = X \\times_{k+1} (W_k^h)^T \\in \\mathbb{R}^{N_1\\times\\dots\\times N_k\\times D_H}$,\n$V^h = X \\times_{k+1} (W_v^h) \\in \\mathbb{R}^{N_1\\times\\dots\\times N_k\\times D_H}$\nwhere $\\times_{k+1}$ denotes multiplication along the (k + 1)-th mode (the hidden dimension).\nThe scaled dot-product attention scores $S^h \\in \\mathbb{R}^{(N_1N_2...N_k)\\times(N_1N_2...N_k)}$ are then given by\n$S^h = Softmax(\\frac{Q^h_{(k+1)} (K^h_{(k+1)})^T}{\\sqrt{D_H}})$,\nwhere $Q^h_{(k+1)} \\in \\mathbb{R}^{(N_1N_2...N_k)\\times D_H}$ and $K^h_{(k+1)} \\in \\mathbb{R}^{(N_1N_2...N_k)\\times D_H}$ are the materializations of the query and key tensors, and the Softmax function is again applied row-wise. Each positional index is considered as a single entity in the attention calculation. The output of the high-order attention function $h_{attn}: \\mathbb{R}^{N_1\\times N_2\\times\\dots\\times N_k\\times D} \\rightarrow \\mathbb{R}^{N_1\\times N_2\\times\\dots\\times N_k\\times D}$ is computed by applying the attention weights to the value tensor:\n$(h_{attn}(x))_{(k+1)} = \\sum_h (W_o^h)^T V^h_{(k+1)} S^h$.\nLastly, the output is reshaped back to the original tensor shape $N_1 \\times N_2 \\times \\dots \\times N_k \\times D.\nWhile the high-order attention mechanism enables models to capture complex dependencies across multiple dimensions simultaneously, it suffers from significant computational and memory challenges. Specifically, the attention weight tensor scales quadratically with the number of positions, leading to the computational complexity of $O((N_1 N_2 . . . N_k)^2)$, which is impractical for large tensors. To address this issue, we propose a low-rank approximation of the high-order attention matrix using a Kronecker product decomposition. This approach dramatically reduces computational complexity while retaining the expressive power of the attention mechanism."}, {"title": "4.2 LOW-RANK APPROXIMATION VIA KRONECKER DECOMPOSITION", "content": "We parameterize the potentially large high-order attention matrix $S^h \\in \\mathbb{R}^{(N_1N_2...N_k)\\times(N_1N_2...N_k)}$ using a first-order Kronecker decomposition (Figure 2) of the form:\n$S^h \\approx S^{(1)} \\otimes S^{(2)} \\otimes \\dots \\otimes S^{(k)}$"}, {"title": "4.3 LINEAR ATTENTION WITH KERNEL TRICK", "content": "Following the work by (Choromanski et al., 2020), we approximate the Softmax function in Eq. (7) using a kernel feature map $\\phi : \\mathbb{R}^D \\rightarrow \\mathbb{R}^M$:\n$S_i^{(i)} \\approx (Z^h_i)^{-1} \\phi(Q_i^h) (\\phi(K_i^h))^T, (Z_i^h)_{jj} = \\phi(Q_i^h) \\sum_{l=1}^{N_i} \\phi(K_i^h)$.\nwhere $Z^h_i \\in \\mathbb{R}^{N_i \\times N_i}$ is the diagonal normalization matrix serving as a normalizing factor. Substituting the Softmax function with Eq. (10) instead of the Softmax in the multiplication between the value tensor $V^h$ and factor matrix $S_i^{(i)}$ on mode i results in:\n$V^h \\times_i S_i^{(i)} = V^h \\times_i ((Z_i^h)^{-1} \\phi(Q_i^h) (\\phi(K_i^h))^T)$\n$= ((\\phi(V^h) \\times_i (\\phi(K_i^h))^T) \\times_i (\\phi(Q_i^h))) \\times (Z_i^h)^{-1}$\nThe choice of kernel function $\\phi$ is flexible, and we utilize the same kernel function as in (Choromanski et al., 2020), which has been validated both theoretically and empirically. In Eq. (12), we simply used the associative law of matrix multiplication again to reduce the computational complexity of applying a first-order attention matrix on one mode from $O(N_i D_H (\\prod_j N_j))$ to $O(D_H(\\prod_{i \\neq j} N_j))$ giving us a final complexity of the proposed multi-head factorized high-order attention of $O(D^2(\\prod_j N_j))$. We include the pseudo code for the whole HOT method in Algorithm A."}, {"title": "5 EXPERIMENTS", "content": "We thoroughly evaluate HOT on two high order data tasks, validating the generality of the proposed framework. At each subsection, we introduce the task, benchmark datasets, and baselines used, and discuss the performance results. Implementation details are presented in the appendix. We close the section by reviewing ablation studies that further confirm our theory and design choices."}, {"title": "5.1 LONG-RANGE TIME-SERIES FORECASTING", "content": "Given historical observations $X = {X_1,...,X_T} \\in \\mathbb{R}^{T\\times N}$ with T time steps and N variates, we predict the future S time steps $Y = {X_{T+1},...,X_{T+S}}\\in\\mathbb{R}^{S\\times N}$.\nDatasets We extensively include 5 real-world datasets in our experiments, including ECL, Exchange, Traffic, Weather used by Autoformer Wu et al. (2021), and Solar-Energy proposed in LSTNet Lai et al. (2017). Further dataset details are in the Appendix.\nBaselines We choose 11 well-acknowledged forecasting models as our benchmark, including (1) Transformer-based methods: ITransformer Liu et al. (2024), Crossformer Zhang & Yan (2023), Autoformer Wu et al. (2021), FEDformer Zhou et al. (2022), Stationary Liu et al. (2023), PatchTST Nie et al. (2023); (2) Linear-based methods: DLinear Zeng et al. (2022), TiDE Das et al. (2024), RLinear Li et al. (2023); and (3) TCN-based methods: SCINet Liu et al. (2021), TimesNet Wu et al. (2023).\nResults Comprehensive forecasting results are provided in Table 1, with the best results highlighted in bold and the second-best underlined. Lower MSE/MAE values reflect more accurate predictions."}, {"title": "5.2 3D MEDICAL IMAGE CLASSIFICATION", "content": "Given a 3D image $X \\in \\mathbb{R}^{W\\times H\\times D}$ with width W, height H, and depth D, we predict the image class probability $y \\in \\mathbb{R}^C$ over a set of C classes.\nDataset MedMNIST v2 Yang et al. (2023) is a large-scale benchmark for medical image classification on standardized MNIST-like 2D and 3D images with diverse modalities, dataset scales, and tasks. We primarily experiment on the 3D portion of MedMNIST v2, namely the Organ, Nodule, Fracture, Adrenal, Vessel, and Synapse datasets. The size of each image is 28 \u00d7 28 \u00d7 28 (3D).\nBaselines We choose 11 medical image classifier models including ResNet-18/ResNet-50 (He et al., 2015) with 2.5D / 3D / ACS (Yang et al., 2021) convolutions (Yang et al., 2023), DWT-CV (Cheng et al., 2022), Auto-Keras, and Auto-sklearn (Yang et al., 2023), MDANet (Huang et al., 2022), and CdTransformer (Zhu et al., 2024).\nResults The results presented in Table 2 demonstrate the superior performance of our Higher Order Transformer (HOT) across multiple medical imaging datasets. HOT achieves the highest accuracy and AUC on Organ, Fracture, Adrenal, and Vessel datasets, and the second-best performance in Synapse and Nodule showcasing its robust classification capabilities. While models like CdTransformer achieve better performance on Nodule and Synapse, they do so with a significantly increased computational complexity of $O(N^3D^2 + N^4D)$ compared to HOT's $O(N^3D^2)$. Additionally, HOT consistently outperforms other state-of-the-art methods such as DWT-CV and MDANet over both metrics, balancing high performance with lower computational demands and much fewer parameters."}, {"title": "5.3 ABLATION STUDY", "content": "To verify the rational business of the proposed HOT, we provide detailed ablations covering analyses on rank of the attention factorization, attention order, attention type, and lastly, individual model components namely high-order attention module and the feed-forward module."}, {"title": "5.3.1 ATTENTION RANK", "content": "In this section, we evaluate the impact of the number of attention heads on the performance of HOT across both medical imaging and time series datasets. The attention rank, governed by the number of heads, plays a critical role in the model's ability to capture diverse patterns across multiple dimensions by approximating the original high-order attention. We conduct ablation experiments by varying the number of heads to observe how it affects model accuracy and error rates. For 3D medical image datasets (Figure 3 Left), increasing the number of attention heads initially improves accuracy, however, after a certain threshold, performance declines. This drop is due to the fixed hidden dimension, which causes the dimension of each head to decrease as the number of heads increases, reducing each head's ability to capture rich features and leading to less expressive attention mechanisms. For the time series datasets (Figure 3 Right), the use of more heads improves performance, with the MSE consistently decreasing as the number of heads increases.\nAlthough the effective rank for the Kronecker decomposition varies for different tasks and datasets and is highly dependent on the data size and its characteristics, in practice as shown in Figures 3, we see that low-rank approximation (where the rank or the number of heads is not a large number) is expressive enough to yield good results. To that end, we treat the rank as a hyper-parameter of the model and choose it by hyper-parameter search based on validation metrics for each dataset. This approach makes it quite easy to achieve a proper performance, without any need for exhaustive rank analysis on the data."}, {"title": "5.3.2 ATTENTION ORDER", "content": "We conducted an ablation study to explore the effects of increasing the attention order on the performance of our proposed HOT. The attention order refers to the number of dimensions over which attention is applied, extending beyond traditional sequence-based attention to handle high-dimensional data effectively. We evaluate performance on both time series forecasting and 3D medical image classification tasks under different attention configurations. As shown in Tables 3 and 4, applying higher-order attention consistently improves the performance across all datasets, outperforming configurations with lower-order attention. However, the training time and memory consumption of the model increases linearly with the order of the attention as depicted in Figure 4 (Left). It is worth mentioning that the model with no attention is equivalent to an MLP with residual connections and normalizations. Importantly, all models maintain the same number of parameters and the same computational and memory complexity for each dataset, highlighting that the performance gains are attributable to the increased attention order without adding. Details of memory consumption and training time are presented in the appendix."}, {"title": "5.3.3 ATTENTION TYPE", "content": "In this section, we compare naive full attention with the proposed factorized attention approach. To implement the full high-order attention, we flatten all dimensions except for the batch and hidden dimensions, then apply standard scaled dot-product attention to the flattened input. Finally, the output is reshaped back to its original dimensions. As discussed earlier, the quadratic complexity of standard attention results in significantly higher computational and memory requirements. To address this, we also incorporated kernelized attention into the naive full attention, reducing its complexity to linear\u2014matching the efficiency of our proposed method. As shown in Tables 6 and 5, with proper hyperparameter tuning, our proposed HOT (highlighted in green) achieves results comparable to standard full attention while requiring far less computation, memory, and training time.\nAn interesting observation from the results is that combining factorization with the kernel trick improves performance compared to factorization alone. This is achieved by introducing additional inductive bias into the attention computation. While linear attention without factorization performs similarly to the combined approach in 3D medical image classification tasks, factorization offers several distinct advantages: 1. It enables the control of model expressivity by adjusting the rank based on the computational cost. 2. It enables the explanation of large high-order attentions, which are otherwise infeasible to analyze, using smaller first-order attention matrices that are already well-studied and interpretable. 3. It allows flexible treatment of attention across different dimensions by enabling the application of independent attention masks on each axis without interference. 4. When combined with linear attention, it results in lower MAE and reduced memory consumption compared to linear attention alone as shown in Figure 4 (Right)."}, {"title": "6 CONCLUSION", "content": "In this paper, we addressed the challenge of extending Transformers to high-dimensional data, often limited by the quadratic cost of attention mechanisms. While methods like flattening inputs or sparse attention reduce computational overhead, they miss essential cross-dimensional dependencies and structural information. We introduced Higher-Order Transformers (HOT) with Kronecker factorized attention to lower complexity while preserving expressiveness. This approach processes high-dimensional data efficiently, with complexity scaling quadratically per dimension. We further integrated kernelized attention for additional scalability and a complexity scaling linearly per dimension. HOT demonstrated strong performance in tasks such as time series forecasting and 3D"}, {"title": "A HOT ALGORITHM", "content": "Algorithm 1 High-Order Attention with Kronecker Decomposition and Linear Approximation\nRequire: Input tensor $X \\in \\mathbb{R}^{N_1 \\times N_2\\times\\dots\\times N_k \\times D}$, Number of heads H, Hidden dimension $D_H$, Kernel feature map $\\phi$\nEnsure: Output tensor $Y \\in \\mathbb{R}^{N_1 \\times N_2 \\times\\dots\\times N_k \\times D}$\n1: Initialize query, key, value projection matrices $W_q^h$, $W_k^h$, $W_v^h \\in \\mathbb{R}^{D\\times D_H}$, output projection matrices $W_o^h \\in \\mathbb{R}^{D_H\\times D}$\n2: Initialize empty output tensor $Y \\in \\mathbb{R}^{N_1 \\times N_2 \\times\\dots\\times N_k\\times D} \\leftarrow 0$\n3: for each head h = 1 to H do\n4: Compute query, key, and value tensors:\n$Q^h = X \\times_{k+1} W_q^h$, $K^h = X \\times_{k+1} W_k^h$, $V^h = X \\times_{k+1} W_v^h$\n5: Initialize output tensor $P \\leftarrow V^h$\n6: for each mode i = 1 to k do\n7: Pool the query and key tensors across all modes except i:\n$Q_i^h = g_{pool}(Q^h)$, $K_i^h = g_{pool}(K^h)$\n8: Compute first-order attention matrix for mode i with kernel trick:\n$S_i^{(i)} \\approx (Z_i^h)^{-1} \\phi(Q_i^h) (\\phi(K_i^h))^T$\n9: Apply the Kronecker attention to P:\n$P = P \\times_i S_i^{(i)}$\n10: end for\n11: Update the output tensor:\n$Y = Y + P \\times_{k+1} W_o^h$\n12: end for\n13: return Y"}, {"title": "B UNIVERSALITY OF THE KRONECKER DECOMPOSITION", "content": "Theorem (4.2). Given any high-order attention matrix $S \\in \\mathbb{R}^{(N_1N_2...N_k)\\times(N_1N_2...N_k)}$, there exists an R \u2208 N such that S can be expressed as a rank R Kronecker decomposition, i.e., $S = \\sum_{r=1}^R S_r^{(1)} \\otimes S_r^{(2)} \\otimes \\dots \\otimes S_r^{(k)}$. As R approaches $min_{j=1,\\dots,k} \\prod_{i\\neq j} N_i^2$, the approximation is guaranteed to become exact, meaning the Kronecker decomposition is capable of universally representing any high-order attention matrix S.\nProof. Let $\\tilde{S}$ be the tensor obtained by reshaping the attention matrix S into a tensor of size $N_1 \\times N_2 \\times \\dots \\times N_k \\times N_1 \\times N_2 \\times \\dots \\times N_k$ and let T\u2208 $ \\mathbb{R}^{N_1^2 \\times N_2^2\\times\\dots\\times N_k^2}$ be the tensor obtained by merging each pair of modes corresponding to one modality\u00b9. Let R be the CP rank of T and let\n$T = \\sum_{r=1}^R s_r^{(1)} \\circ s_r^{(2)} \\dots s_r^{(k)}$ be a CP decomposition, where $\\circ$ denotes the outer product and each $s_r^{(i)} \\in \\mathbb{R}^{N_i^2}$ for i = 1,...,k (see, e.g., (Kolda & Bader, 2009) for an introduction to the CP decomposition). By reshaping each $s_r^{(i)} \\in \\mathbb{R}^{N_i^2}$ into a matrix $S_r^{(i)} \\in \\mathbb{R}^{N_i \\times N_i}$, one can check that\n$S_{i_1, ...,i_k,j_1, ...,j_k} = \\sum_{r=1}^R (s_r^{(1)})_{i_1, j_1} (s_r^{(2)})_{i_2, j_2} \\dots (s_r^{(k)})_{i_k,j_k}$\nfrom which it follows that $S = \\sum_{r=1}^R S_r^{(1)} \\otimes S_r^{(2)} \\otimes \\dots \\otimes S_r^{(k)}$, as desired."}]}