{"title": "Pay Less On Clinical Images: Asymmetric Multi-Modal Fusion Method For Efficient Multi-Label Skin Lesion Classification", "authors": ["Peng TANG", "Tobias Lasser"], "abstract": "Existing multi-modal approaches primarily focus on enhancing multi-label skin lesion classification performance through advanced fusion modules, often neglecting the associated rise in parameters. In clinical settings, both clinical and dermoscopy images are captured for diagnosis; however, dermoscopy images exhibit more crucial visual features for multi-label skin lesion classification. Motivated by this observation, we introduce a novel asymmetric multi-modal fusion method in this paper for efficient multi-label skin lesion classification. Our fusion method incorporates two innovative schemes. Firstly, we validate the effectiveness of our asymmetric fusion structure. It employs a light and simple network for clinical images and a heavier, more complex one for dermoscopy images, resulting in significant parameter savings compared to the symmetric fusion structure using two identical networks for both modalities. Secondly, in contrast to previous approaches using mutual attention modules for interaction between image modalities, we propose an asymmetric attention module. This module solely leverages clinical image information to enhance dermoscopy image features, considering clinical images as supplementary information in our pipeline. We conduct the extensive experiments on the seven-point checklist dataset. Results demonstrate the generality of our proposed method for both networks and Transformer structures, showcasing its superiority over existing methods We will make our code publicly available.", "sections": [{"title": "1. Introduction", "content": "Early detection of melanoma holds crucial significance for patient treatment. Patients with ulcerated melanoma exceeding 4 mm thickness experience a 15% five-year survival rate, whereas those with the melanoma thinner than 1 mm exhibit a 95% 5-year survival rate (Balch et al., 2009; Vestergaard et al., 2008). However, early-stage melanoma prevention faces limitations due to the restricted count of experienced dermatologists. It is anticipated that deep learning-based decision support systems will enhance the diagnostic accuracy of young dermatologists and may even serve as a potential replacement for experienced dermatologists (Zhang et al., 2023).\nMimicking routine dermatologist's examinations the seven-point checklist (SPC) dataset was introduced using clinical screening followed by dermoscopic analysis (Ge et al., 2017), Kawahara et al. (2018). This dataset is comprised of paired clinical-dermoscopy images, facilitating research in multi-modal skin lesion classification (MM-SLC). For the presentation of localized visual features, dermoscopy images (DIs) are captured using a high-resolution magnifying imaging device (Vestergaard et al., 2008) in direct contact with the skin (e.g. dermatoscopy and epiluminescence microscopy). In contrast, clinical images (CIs), taken with a standard digital camera or smartphone, exhibit more variations in terms of perspective and angle (Ge et al., 2017). In contrast to single modality-based SLC, MM-SLC harnesses complementary"}, {"title": null, "content": "information from both modalities and leads to a more accurate and robust diagnosis, driving further exploration on this topic (Zhang et al., 2023).\nCurrent MM-SLC approaches (Ge et al., 2017; Yap et al., 2018; Kawahara et al., 2018; Bi et al., 2020; Tang et al., 2022; Fu et al., 2022; He et al., 2023; Zhang et al., 2023) are predominantly based on deep learning structures, i.e., convolution neural networks (CNN) and transformers (TF). All the aforementioned approaches utilize a symmetric multi-modal framework, as depicted in Figure 1(a), where features from both image modalities are extracted using identical deep learning models and are subsequently fused through a modality interaction operation. Compared to simple concatenation (Yap et al., 2018; Ge et al., 2017; Fu et al., 2022) and summation (Tang et al., 2022), recent works have increasingly utilized mutual modality attention mechanisms (Bi et al., 2020; He et al., 2023; Zhang et al., 2023), achieving higher accuracy. However, emphasizing the supplementary information of Clinical Images (CI) in the mutual-attention mechanism may impact diagnostic results. Furthermore, all these methods require a relatively large number of model parameters with associated high computational costs, limiting their practical use in clinics. For example, in some rural areas, medical resources are scarce. While tele-dermatology can extend access to specialists online (Liu et al., 2020), poor infrastructure may not guarantee reliable network signals for remote diagnosis. In such cases, the local deployment of AI algorithms becomes a better choice. Reducing the parameters of these algorithms, including SkinGPT-4 (Zhou et al., 2023) which has demonstrated potential for 24-hour care of skin diseases, contributes to cheaper cost and thus enables broader usage.\nOur primary objective is to design a MM-SLC framework that achieves a favorable parameter/accuracy trade-off, i.e., substantially reducing the model's parameters while only modestly impacting its accuracy. Our idea is motivated by two observations: (1) Clinical Statistics: According to experienced dermatologists, the diagnostic accuracy of melanoma based on Dermoscopy Imaging (DI) is 25% higher than that of visual inspection with naked eyes. Visual features observed by naked eyes are akin to those captured by standard cameras and smartphones (Togawa et al., 2023). (2) Experimental Results of DL Algorithms: In diagnosis tasks using the SPC dataset, the majority of papers (Kawahara et al., 2018; Bi et al., 2020; Tang et al., 2022; Fu et al., 2022; He et al., 2023; Zhang et al., 2023) reported a 6% higher accuracy in Dermoscopy Imaging (DI)-based diagnosis compared to Clinical Image (CI). Additionally, (Dascalu et al., 2022) demonstrated an increased"}, {"title": null, "content": "diagnostic accuracy with DI (85%) compared to CI (75%). Considering both observations, it is evident that a significant amount of key diagnostic information comes from DI rather than CI. Therefore, employing two identical structures to extract information from DI and CI individually appears to be wasteful.\nInspired by that, in this paper, we propose a novel Asymmetrical Multi-Modal Fusion Method (AMMFM) for efficient multi-label skin lesion classification. Our approach differentiates itself from previous methods in two key aspects, i.e., Asymmetric Fusion Framework (AFF) and Asymmetric Attention Block (AAB). First, differing from the commonly-used Symmetrical Fusion Framework (SFF), our AFF incorporates the prior domain knowledge into the structure design. AFF utilizes an advanced model, e.g., ResNet, ConvNext, and SwinTransformer (He et al., 2016; Liu et al., 2022a,b) for capturing the primary diagnostic information from DI, but a lightweight deep model, i.e., MobilenetV3 (Howard et al., 2019), for the supplementary information from CI. Compared to SFF, AFF significantly reduces the model's parameters with only a subtle decrease in accuracy. Second, in contrast to previous methods utilizing bidirectional attention blocks (BAB) to mutually enhance DI and CI (Fig. 1.a), we believe that enhancing the supplementary information CI may lead to overfitting, affecting the final classification. Therefore, we propose an asymmetric attention block (AAB) to exclusively leverage the features of CI to enhance those of DI (Fig. 1.b), achieving superior performance compared to BAB with fewer model parameters. In total, our contributions can be summarized as follows:\n1. Inspired by prior knowledge, we introduce a novel Asymmetrical Fusion Framework (AFF) that significantly reduces the model's parameters while maintaining unchanged or slightly decreased classification accuracy compared to the currently used Symmetric Fusion Framework (SFF).\n2. We present a new Asymmetrical Attention Block (AAB) that exclusively utilizes features extracted from clinical images (CI) to enhance those of dermoscopy images. This approach addresses potential accuracy impacts associated with focusing on supplementary information from CI. In comparison to the former Bidirectional Attention Block (BAB), our AAB demonstrates improved classification performance with fewer parameters.\n3. Our proposed Asymmetrical Multi-Modal Fusion Method achieves state-"}, {"title": null, "content": "of-the-art performance in both accuracy and model's parameters. The extensive results confirm the effectiveness of our proposed AFF and AAB, demonstrating their applicability to various deep learning algorithms, including both CNN and transformer structures."}, {"title": "2. Related works", "content": "2.1. Single-modality based skin lesion classification\nPrior to the release of the first public multi-modal skin lesion classification dataset, SPC, by Kawahara et al. (2018), most researchers developed their skin lesion classification (SLC) methods exclusively using dermoscopy images (DI). Current SLC methods based on DI (Yu et al., 2017; Tang et al., 2020; Liu et al., 2022c; Sarker et al., 2022; Yang et al., 2023) predominantly rely on CNN or transformer structures, leveraging the success of these algorithms in computer vision tasks. For example, Yu et al. (2017) secured the first place in the ISBI-2016 SLC challenge by proposing a deep residual CNN with the assistance of a segmentation model. Additionally, Yang et al. (2023) introduced a novel vision transformer for SLC, surpassing state-of-the-art methods on the HAM10000 dataset (Tschandl et al., 2018).\n2.2. Multi-modality based skin lesion classification\nDespite their success, these methods tend to deviate from routine dermatologists' examinations Zhang et al. (2023) and overlook the potential to enhance diagnostic accuracy by exploiting complementary information from additional modalities. To fill this gap, several works about MM-SLC were presented (Ge et al., 2017; Yap et al., 2018; Kawahara et al., 2018; Tang et al., 2022; Fu et al., 2022; He et al., 2023; Zhang et al., 2023). Ge et al. (2017) and Yap et al. (2018) extracted the features from clinical images and dermoscopy images using VGG-16 (Simonyan and Zisserman, 2014) and Resnet-50 (He et al., 2016), and then fused the features by a simple concatenation to learn a joint representation for the final prediction. The introduction of the Seven-Point Checklist (SPC) dataset by Kawahara et al. (2018) marked a significant advancement in multi-modal skin lesion classification. They proposed a multi-modal framework based on Inception-V3 (Szegedy et al., 2016) for the simultaneous classification of diagnosis and the seven-point checklist. After the release of the SPC dataset, there has been a growing number of multi-modal approaches proposed for multi-label skin lesion classification. To enhance performance, Tang et al. (2022) and Fu"}, {"title": null, "content": "et al. (2022) introduced weighted-fusion and graph-based fusion schemes, respectively. Both approaches combine predictions from CI and DI in the late stages of the model. More recently, He et al. (2023) and Zhang et al. (2023) recognized the limitations of the simple concatenation operation used in former methods. They introduced multiple bidirectional attention blocks to mutually enhance CI and DI, facilitating efficient interaction between these two modalities across multiple scales. While these methods have achieved state-of-the-art (SOTA) performance for Multi-Modal Skin Lesion Classification (MM-SLC) tasks, the emphasis on Clinical Images (CI) may potentially impact their performance, considering CI is regarded as supplementary information in our opinion. Additionally, the substantial number of parameters and associated high computational cost in these methods may pose limitations in various clinical scenarios.\n2.3. Asymmetrical Fusion Method\nA limited number of works on the Asymmetrical Fusion Model (AFM) (Yang et al., 2019; Zhu et al., 2019; Gao et al., 2020; Yang et al., 2021; Wu et al., 2023) have been proposed for various computer vision tasks. For example, Zhu et al. (2019) introduced an asymmetric non-local network to fuse multi-scale features for semantic segmentation. Gao et al. (2020) presented an asymmetrical model to extract asymmetric relations between humans and objectives for action recognition. Additionally, Wu et al. (2023) proposed an asymmetrical fusion framework with a focus on the gallery side for image retrieval. In the medical domain, Yang et al. (2023) employed an asymmetrical model to address issues in 3D slices for universal lesion detection.\nThese methods are tailored to specific modalities (e.g., 3D slices (Gao et al., 2020)) or tasks (e.g., semantic segmentation (Zhu et al., 2019), action recognition (Gao et al., 2020), and image-text retrieval (Wu et al., 2023)). Their design is not directly applicable to the Multi-Modal Skin Lesion Classification (MM-SLC) task, however, the successes of AFM in different fields encouraged us to explore the potential of asymmetrical fusion methods for the MM-SLC task."}, {"title": "3. Asymmetrical Multi-Modal Fusion Method", "content": "As illustrated in Fig. 2, the proposed Asymmetric Multi-Modal Fusion Method (AMMFM) is constructed with four components: Asymmetric Fusion Framework (AFF), Asymmetric Attention Blocks (AABs), Fully Con-"}, {"title": "3.1. Asymmetric Fusion Framework", "content": "Before introducing the Asymmetric Fusion Framework (AFF), we provide a definition for easier understanding: in this article, the term \u201cframework\" specifically refers to the composition of two feature extractors, excluding the modality interaction modules (see Fig. 2).\nAs discussed in Sec. 1, the majority of current methods are based on a symmetrical fusion structure (SFF), utilizing two identical structures to extract features from clinical and dermoscopy images, respectively. However, relevant research has demonstrated that the accuracy based on dermoscopy"}, {"title": "3.2. Asymmetric Attention Block", "content": "Building on the discussion in Sec. 1, the information from the clinical branch is considered as supplementary in this paper. Therefore, we introduce an asymmetric attention block (AAB) for the modality interactions between clinical and dermoscopy images."}, {"title": null, "content": "In contrast to the state-of-the-art bidirectional attention block (BAB) He et al. (2023) that mutually enhances the features of both modalities (see Fig.3(a)), our AAB only adopts clinical features to generate an attention map for enhancing dermoscopy features. This design allows us to save approximately half of the parameters compared to BAB (See Fig.3(b)). Similar to BAB, AAB is embedded into different stages of deep learning models to facilitate the interaction of multi-scale features from the two modalities.\nIn our AAB, the inputs are the extracted clinical features $C\\in \\mathbb{R}^{H\\times W \\times C}$ and dermoscopy features $D \\in \\mathbb{R}^{H\\times W \\times C}$, both of which have the same size (H, W, and C indicate the height, width, and channel number of the features, respectively). As shown in Fig. 3(a), firstly, two $1\\times1$ convolutions are applied to C to generate $C_k$ and $C_q$, and one $1\\times1$ convolution is employed on D to obtain $D_v$, where $(C_k, C_q, D_v) \\in \\mathbb{R}^{H\\times W \\times C}$. Then, $C_k$ and $C_q$ are reshaped to $\\mathbb{R}^{N\\times C}$, where $N = H \\times W$. Next, a multiplication is conducted between the reshaped $C_k$ and $C_q$, followed by a non-linear activation function $softmax$ to generate the attention map $M_c \\in \\mathbb{R}^{N\\times N}$. Finally, the refined dermscopy features are obtained based on Eq. 1,\n$D_{refined} = D_v . M_c + D, \\qquad (1)$\nwhere . indicates matrix dot product operation, and + indicates matrix summation.\n3.3. Loss Function and Final Prediction\nThe total loss $L_{total}$ used to optimize our model is as follows:\n$L_{total} = L_{derm} + L_{clic} + L_{fusion}, \\qquad (2)$\nwhere $L_{derm}, L_{clic}$ and $L_{fusion}$ are the multi-label classification losses for the dermscopy image branch ($P_D$ in Fig. 2), clinical image branch ($P_C$ in Fig. 2) and fusion image branch ($P_{FU}$ in Fig. 2), respectively. All the losses are computed as\n$L_K = \\sum^{X}_{j} \\sum^{Y}_{i} CE (D_{j}^{i}, C_{j}^{i}, G_{j}^{i}, P_{j}^{i}; \\theta_k), \\qquad (3)$\nwhere X is the batch size in our training, Y=8 indicates the number of the multi-label classification tasks (see Table 1), $C^i$ and $D^i$ represent input pairs of dermoscopy and clinical images, respectively. $G^i$ and $P^i$ are the"}, {"title": null, "content": "corresponding ground truths and predictions, respectively, and $\\theta_k$ is the parameters of our model. CE indicates the cross-entropy loss.\nDuring the testing stage, we use a weighted average scheme to fuse $P_D, P_C$ and $P_{FU}$ into the final prediction $P_{FI}$ for the evaluation as follows:\n$P_{FI} = W_D * P_D + W_C * P_C + W_{FU} * P_{FU}, \\qquad (4)$\nwhere $W_D, W_C$ and $W_{FU}$ are the corresponding weights for $P_D, P_C$ and $P_{FU}$, respectively, which are obtained by the conducting a weight search scheme on the validation dataset (Tang et al., 2022)."}, {"title": "4. Experiments and Discussion", "content": "4.1. Implementation Details\nDuring training, we use Adam (Kingma and Ba, 2014) with a batch size of 24 to optimize our model for 250 epochs. Data augmentations, including flipping, shifting, scaling, rotating, and brightening operations, are randomly conducted to enhance the generalization ability of the model. The Stochastic Weights Averaging (SWA) Izmailov et al. (2018) scheme is used in the last 50 epochs to generate the final weights for evaluation. All images are resized to 224 \u00d7 224 \u00d7 3 for both training and evaluating the model. Following Tang et al. (2022), test time augmentation is also used during the evaluation to improve the classification performance. All of our experiments are conducted on NVIDIA GPUs A100 (40GB). Unless otherwise specified, our AMMFM are based on MobilenetV3 for clinical images and Swin-Transformer for dermoscopy images, as it achieves the best classification performance in our experiments. More details can be found in our released code.\n4.2. Dataset and Metrics\nThe effectiveness of our AMMFM is evaluated on the well-recognized seven-point checklist (SPC) dataset Kawahara et al. (2018), which contains 1011 patient cases. Each case includes a pair of dermoscopy and clinical images, diagnosis (Diag) label, and labels of seven-point checklist (SPC) features. As shown in Table 1, Diag has five categories: BCC, NEV, MEL, MISC, and SK. The SPC labels include Pigment Network (PN), Streaks (STR), Pigmentation (PIG), Regression Structures (RS), Dots and Globules (DaG), Blue Whitish Veil (BWV), and Vascular Structures (VS), which are divided into the following categories: ABS, PRS, TYP, ATP, REG, and IR."}, {"title": "4.3. Comparisons with State-of-the-art Methods", "content": "In Tables 2 and 3, a comparative analysis was conducted to assess the performance of the proposed (AMMFM) against state-of-the-art classification methodologies utilizing clinical and dermoscopy images. The evaluated methods encompass Inception-combined (Kawahara et al., 2018), HcCNN (Bi"}, {"title": null, "content": "ing the highest values in seven categories and the second-highest in eight categories, showcasing its excellence across all eight classification tasks. Notably, AMMFM outperforms FM-FS in the Diag task and most Seven-Point features tasks. In comparison to CAFNet, AMMFM achieves comparable performance in the Diag task (AVG AUC: CAFNet: 93.1%, AMMFM: 92.6%) and clearly better performance in other Seven-Point feature tasks (AVG AUC: CAFNet: 83.0%, AMMFM: 86.4%), establishing its overall superiority over CAFNet.\nSimilarly, in Table 3, AMMFM secures the highest averaged accuracy (AVG Acc) value of 77.2%, outperforming all other methods. It attains the highest values in four classification tasks (PN, BWV, STR, DaG) and the second-highest values in two tasks (PIG, Diag). CAFNet and FM-FS secure the second-best and third-best AVG Acc in Table 3, respectively. These results underscore not only the superior performance of AMMFM, but also the efficacy of the cross-attention modules in CAFNet and the late fusion scheme in FM-FS. For further insights, Table 4 details that AMMFM achieves the"}, {"title": null, "content": "highest AUC and sensitivity values in melanoma-related features, substantiating its proficiency in melanoma detection. However, AMMFM achieves the lowest precision value, which is because of the bad performance in the categories of RS-PRE (51.9 %) and VS-IR (16.7%). Especially, VS-IR is much lower than other methods. We attribute this to the unbalanced categories in RS-PR (ABS: 758 PRE: 253) and VS-IR (ABS: 833, REG: 117 and IR: 91, see Table 1).\nIn Table 5, we compare all the methods mainly based on model parameters, and for convenience, we again present the AVG AUC and AVG ACC in this table. Since only the source codes for FM-FS and TFormer were available, the model parameters for Incep-com, HcCNN, GIIN, and CAFNet were estimated by us, albeit with some approximations. In our estimation, we calculated the parameters based on the employed backbones, specifically, two InceptionV3 (57.4Mb) for Incep-com and two ResNet-50 (51.2Mb) for GIIN, HCCNN, and CAFNet. However, concerning Incep-com and GIIN, the model parameters are marginally higher than those of their utilized backbones, attributed to the absence of multiple blocks employed in constructing the third branch. Concerning HeCNN and CAFnet, which construct a third branch utilizing attention and ResNet blocks, the model parameters are expected to significantly exceed those of their backbones. As shown in Table. 5, our AMMFM achieves the highest values in both AVG AUC and AVG ACC with the least model parameters (33.06Mb), demonstrating the great accuracy/parameter trade-off of our AMMFM."}, {"title": "4.4. Ablation Studies", "content": "In following experiments, all the models are trained and tested ten times to obtain the mean value and standard deviation for a fair comparison.\nIn Table 6, ablation studies are conducted to analyze the two primary components of our AMMFM: the asymmetrical fusion framework (AFF) and the asymmetrical attention block (AAB). For comparative purposes, we establish a baseline utilizing a commonly-used symmetrical fusion framework (SFF) based on two Swin-Transformer (ST) models and a concatenation operation, serving as a reference point in the ablation studies.\nCompared to baseline model (second row), the proposed AFF with concatenation operation (third row) can significantly reduce the parameters from 58.49M to 32.48M without compromising the performance metrics, as evidenced by the maintained the AVG AUC (Baseline: 87.2%, AFF: 87.2%) and AVG ACC (Baseline:76.6%, AFF: 76.5%). These outcomes substantiate our initial hypothesis that substituting an advanced model (ST) with a more lightweight model, MobileNetV3 (MN), for information extraction from clinical images, would have a subtle or negligible impact on overall performance. This observation underscores the supplementary nature of clinical images in the context of multi-label skin lesion classification tasks, where dermoscopy images remain the primary source of information. Subsequently, ABB contributes to a further enhancement in the performance of AFF from. This improvement is observed across both metrics, with AVG AUC increasing from 87.2% for AFF to 87.6% for AFF+AAB, and AVG ACC from 76.5% to 76.7%. Remarkably, this performance boost is achieved with only a marginal increase in model size, rising from 32.48Mb for AFF to 33.06Mb for AFF+AAB, illuminating the effectiveness of our AAB in accuracy/parameters trade-off."}, {"title": "4.5. Comparison between single-modal, baseline multi-modal and our proposed multi-modal methods", "content": "In Table 7, we conduct a comprehensive comparison among single-modal, baseline multi-modal (SFF with concatenation), and our proposed multi-modal approach (AFF with AAB). The results reveal a substantial performance improvement when utilizing dermoscopy images, regardless of whether based on MN or ST, compared to clinical images. Specifically, there is an increase of over 7% in AVG AUC and 5% in AVG ACC values, underscoring the pronounced significance of dermoscopy images in the context of multi-label classification tasks. Furthermore, the baseline multi-modal methods exhibit an additional increase in accuracy compared to their Derm-based counterparts. This emphasizes the complementary nature of clinical images, which provide supplementary information to dermoscopy images. In the single-modal approaches, substituting ST with MN for dermoscopy image processing results in a 3.5% reduction in both metrics. Conversely, replacing ST with MN for clinical image processing shows a more modest 1.1% decrease in Average Accuracy (AVG ACC). Also, compared to another counterpart in AMMFM approaches, employing ST for the dermoscopy branch and MN for the clinical branch enhances both metrics by approximately 3%. These results demonstrate support the effectiveness of our AFF and affirm the suitability of the chosen architecture for efficiently optimizing performance in multi-modal classification tasks.\nIn Table 8, we present detailed information about both ST-based single-"}, {"title": null, "content": "modal and multi-modal methods, facilitating a nuanced analysis of their impact on individual classification tasks. Examining the table reveals that dermoscopy-based ST consistently outperforms clinical image-based ST across all categories (CTs), a result aligned with expectations given that the seven-point checklist criteria are formulated based on observed features under dermoscopy (Kawahara et al., 2018). Moreover, in comparison to dermoscopy-based ST, both the Baseline and our AMMFM demonstrate performance improvements across nearly all CTs. This observation illustrates the complementary role of clinical images in enhancing the overall performance when combined with dermoscopy images."}, {"title": "4.6. Comparison between bidirectional attention block (BAB) and asymmetrical attention block (AAB)", "content": "To delve deeper into the impact of the proposed asymmetrical attention block (AAB), a comparative analysis is conducted with other fusion blocks within different fusion frameworks, namely the symmetrical fusion framework (SFF) and the asymmetrical fusion framework (AFF). As illustrated in Table 9, the block attention block (BAB) demonstrates performance improvement over concatenation (CAT) for both fusion frameworks, highlighting the effectiveness of multi-modal interactions. Notably, our proposed AAB further enhances the accuracy achieved by BAB within both SFF and AFF. This observation lends support to our assumption that over augmenting the importance of clinical supplementary information in the multi-modal pipeline may impact the classification task. At the same time, ABB also show it's superiority in model's parameters compared to BAB.\n4.7. Generalization ability of AMMFM using different backbones\nTo assess the generalization capability of our AMMFM across various backbones, we employ ResNet-50, ConvnextTiny, and Swin-Transformer as backbone architectures for comparative analysis. As depicted in Table 10, our AMMFM consistently outperforms the baseline multi-modal method across all three backbones, while maintaining significantly fewer parameters. This substantiates the robustness of our AMMFM, showcasing its ability to deliver superior performance across diverse deep learning backbones."}, {"title": "5. Conclusion", "content": "In this study, we introduced a novel Asymmetrical Multi-Modal Fusion Method (AMMFM) for efficient multi-label skin lesion classification, driven by the observation that dermoscopy images provide more crucial information than clinical images. Our AMMFM comprises two key components: the asymmetrical fusion framework (AFF) and the asymmetrical attention block (AAB). To optimize efficiency by reducing parameters, AFF integrates one advanced model for feature extraction from dermoscopy images and one lightweight model for clinical images. This design is grounded in the assumption that affecting the ability to capture supplementary information from clinical images will subtly or not impact the overall multi-modal pipeline's performance. In contrast to the bidirectional attention block (BAB), \u0410\u0410\u0412 focuses solely on enhancing dermoscopy features while excluding attention on clinical images, due to our belief that directing attention to supplementary information may adversely impact the final classification performance. Extensive results demonstrate that, in comparison to the previous symmetrical fusion framework, AFF significantly reduces model parameters while maintaining accuracy. Additionally, AAB enhances the performance of BAB"}, {"title": null, "content": "with fewer parameters, showcasing its efficacy in improving the overall classification task. Last but not least, our AMMFM attains state-of-the-art performance with the fewest model parameters."}]}