{"title": "LT3SD: Latent Trees for 3D Scene Diffusion", "authors": ["Quan Meng", "Lei Li", "Matthias Nie\u00dfner", "Angela Dai"], "abstract": "We present LT3SD, a novel latent diffusion model for large-scale 3D scene generation. Recent advances in diffusion models have shown impressive results in 3D object generation, but are limited in spatial extent and quality when extended to 3D scenes. To generate complex and diverse 3D scene structures, we introduce a latent tree representation to effectively encode both lower-frequency geometry and higher-frequency detail in a coarse-to-fine hierarchy. We can then learn a generative diffusion process in this latent 3D scene space, modeling the latent components of a scene at each resolution level. To synthesize large-scale scenes with varying sizes, we train our diffusion model on scene patches and synthesize arbitrary-sized output 3D scenes through shared diffusion generation across multiple scene patches. Through extensive experiments, we demonstrate the efficacy and benefits of LT3SD for large-scale, high-quality unconditional 3D scene generation and for probabilistic completion for partial scene observations.", "sections": [{"title": "1. Introduction", "content": "In recent years, there has been increasing demand for 3D digital content, both for content creation purposes such as films, video games, and mixed reality, as well as for reconstruction to enable machines to perceive real environments from visual inputs. For instance, video games have driven efforts to produce realistic virtual 3D environments, requiring significant costs for highly-trained 3D artists to create a vast array of 3D scene assets. In particular, AAA video game budgets have risen dramatically in the last five years, with games planned to be released this year and next having development budgets of $200 million or more [51].\nRecent advances in diffusion models [21, 33, 40, 42, 44, 48] have achieved remarkable success in generating complex, high resolution, realistic images, and videos. In the 3D domain, however, generative modeling remains a significant challenge due to the data and representation complexity and limited training data availability. In addition, signal in 3D content is highly unevenly distributed, with many regions being predominantly free space, whereas most scene detail is located only around certain surface areas. Existing 3D diffusion models [10, 14, 18, 30, 35, 38, 46, 56] overwhelmingly focus on object-level generation. Following latent diffusion (LDMs) [44] for 2D images, current state-of-the-art methods focus on training diffusion models on learned implicit representations of 3D shapes. However, these methods assume that shapes lie in a canonical and bounded space that is amenable to fixed and compact representation encodings, such as global latent codes [14], hypernetworks [18], and triplanes [20, 46]. While these representations yield impressive results for class-specific distributions, such as chairs or cars, they face challenges when applied to 3D scene environments characterized by highly unstructured geometries, diverse object arrangements, and varying spatial extents. Although early attempts at scene-level synthesis exist, they predominantly focus on single rooms or limited spatial extents [23, 26, 43] with relatively low-resolution [28, 29, 32] 3D scenes.\nTo this end, we propose LT3SD, a new probabilistic model for generating high-quality, large-scale, arbitrary-sized 3D scenes. Key to our approach is a novel latent tree representation (Fig. 2-right) that progressively decomposes a 3D scene into a tree hierarchy from fine to coarse resolutions. Our latent tree representation efficiently factorizes 3D scenes into geometry (lower-frequency) and latent feature (higher-frequency) volumes, enabling a more compact, effective representation for latent scene diffusion. We then synthesize 3D scenes in a coarse-to-fine, patch-by-patch fashion by reversing the latent tree decomposition.\nWe introduce a latent tree-based diffusion model to generate the latent feature volume conditioned on the corresponding geometry volume at the same tree level. This conditional learning of scene details helps to reduce the complexity of modeling 3D scene distributions. As 3D scenes can vary significantly in size, we design our diffusion model to learn at the level of scene patches. This strategy helps to shift the learning focus from complex unaligned 3D scenes to local structures with higher shared similarity.\nAs a result, our approach can generate large-scale, even infinite 3D scenes by coarse-to-fine construction of the latent trees in a patch-wise fashion. Starting from the coarsest level, our method operates patch-wise to generate the overall structure of a 3D scene, which is critical for geometric coherency. Then, in a coarse-to-fine manner, our method conditionally generates the latent feature volume, which represents fine details for each upper level\u2014the final 3D scene results from the inverse process of latent tree decomposition. Experiments show that our method significantly outperforms existing baselines, improving FID scores by 70% on this challenging 3D scene generation task.\nIn summary, we present the following contributions:\n\u2022 We introduce a novel latent 3D scene diffusion approach, leveraging a latent tree representation to compactly encode complex 3D scene geometry. This enables high-fidelity 3D scene generation in a coarse-to-fine fashion along the latent tree resolution levels.\n\u2022 We characterize 3D scene generation in a patch-wise fashion, enabling efficient training and seamless infinite 3D scene generation through shared diffusion generation across multiple scene patches."}, {"title": "2. Related Work", "content": "Neural 3D Scene Representations. Various neural representations have been developed to characterize 3D shapes or scenes, with recent works extensively exploring implicit functions [11, 36, 37, 39], feature volumes [24, 31, 41], triplanes [7, 46], and tensor decompositions [9]. These representations are proposed as alternatives to traditional representations, such as voxels [15-17, 47, 53] or point clouds [1, 35, 55], to encode geometry more compactly and while maintaining high resolutions. Earlier approaches model an implicit function as an MLP, though such a global representation can result in difficulty retaining both local and global structures, with limited editability [36, 37, 39]. Tri-planes and tensor decompositions more effectively balance memory cost and performance, making them popular choices for object-level 3D shape generation [20, 46]; however, scene outpainting is not straightforward as the n-planes are highly correlated, requiring intricate synchronization processing for extrapolation [54]. In this work, we use a feature grid representation for capability to handle arbitrary topologies and its effective feature spatial locality to enable patch-based compositions of large-scale scenes."}, {"title": "3. Method", "content": "3.1. Overview\nOur goal is to develop a probabilistic model for generating 3D scenes encompassing complex geometric structures, diverse object arrangements, and varying spatial extents. We propose LT3SD, a novel coarse-to-fine, patch-by-patch 3D scene generation approach by leveraging diffusion models in the space of a learned latent tree representation for 3D scenes. Our method has two main stages: latent tree encoding and patch-based latent diffusion, as illustrated in Fig. 3.\nIn the first stage (Sec. 3.2), we progressively decompose a 3D scene into a hierarchical latent tree from fine to coarse levels. Each tree level is a factorization of the scene into a geometry volume and a latent feature volume at the corresponding resolution level (Fig. 3-left). The geometry volume, represented as a truncated unsigned distance field (TUDF), captures lower-frequency scene information, while the latent feature volume compactly encodes higher-frequency details. As shown in Fig. 3-top right, we learn this factorization with a 3D CNN-based encoder E. The inverse process is built upon a decoder D that combines the two decomposed volumes to accurately recover the geometry volume at a higher resolution level.\nIn the second stage (Sec. 3.3), we train a 3D UNet-based diffusion network on the latent tree encodings. At each level, we randomly crop fixed-size patches from the scene geometry volumes and predict their corresponding latent feature patches (Fig. 3-bottom right). At inference time, we can effectively synthesize large-scale 3D scenes by progressively generating latent trees in a patch-by-patch fashion from coarse-to-fine levels."}, {"title": "3.2. Latent Tree Representation for 3D Scenes", "content": "To enable high-quality 3D scene generation, we propose to transform a 3D scene into a latent tree representation. Each level of the hierarchy comprises factorized geometry and latent feature encodings of the scene at the corresponding resolution. We represent the geometry encoding as truncated unsigned distance field (TUDF) grids, characterizing lower-frequency scene information, while higher-frequency scene details are encoded into a latent feature grid.\nOur latent tree representation, as depicted in Fig. 2-right, offers the following advantages: (1) It allows for generating complex scene geometry in a coarse-to-fine fashion, in contrast to a single latent (Fig. 2-left), which does not well-represent shared local structures; (2) Compared to a multi-level latent pyramid (Fig. 2-left), our latent tree offers better encoding of 3D scenes with higher reconstruction quality.\nConstruction. To construct the latent tree representations, we first compute UDFs for 3D scenes. UDFs can easily encode arbitrary 3D scene geometry with various topologies. To focus on surface geometry regions and reduce data distribution complexity, we clip distances exceeding a specified threshold T to obtain TUDF voxel grids.\nConcretely, we decompose the TUDF grid of a 3D scene into a latent tree with N levels, where each scene resolution level i \u2208 [1, N \u2013 1] consists of a TUDF grid L and a latent feature grid H. We denote the root of the tree (i.e., the highest resolution) as L. As illustrated in Fig. 3-top right, we learn this decomposition process with fixed-size patches Li+1 randomly cropped from the scene grid Li+1 for computational efficiency. All scene levels can be trained in parallel through patch-based training.\nAs shown in Fig. 3-top right, at resolution level i + 1, we use the encoder Ei+1 to factorize the TUDF patch Li+1 \u2208 R^{1\u00d7Di+1\u00d7Hi+1\u00d7Wi+1} into a lower-resolution TUDF patch Li and a latent feature patch Hi at the next coarser level i:\nE_{i+1}(L_{i+1}) \\rightarrow [L_i, H_i],\nwhere Li \u2208 R^{1\u00d7D_i\u00d7H_i\u00d7W_i} represents the coarser patch geometry, and Hi \u2208 R^{C \u00d7 D_i\u00d7 H_i\u00d7W_i} with a latent feature dimension C encodes higher-frequency details. In practice, we compute Li by downsampling Li+1 with average pooling and predict Hi with a 3D CNN.\nWe then train a decoder network Di+1 to reconstruct the TUDF patch Li+1 by combining the factorized grids Li and Hi as follows:\nD_{i+1}([L_i, H_i]) \\Rightarrow L_{i+1}.\nThe decoder Di+1 is also implemented as a 3D CNN.\nLearning. To train the encoders and decoders, our loss measures the l2 error between the reconstructed TUDFs and the ground truth TUDFs across all the resolution levels:\nL_{latent} = \\sum_i (L_{i+1} - D_{i+1}(E_{i+1} (L_{i+1})))^2\nGiven the trained encoders, we can progressively decompose a high-resolution, arbitrary-sized 3D scene into a compact latent tree {L\u2081, H\u2081,\u00a8\u00a8\u00a8,LN-1, HN-1}"}, {"title": "3.3. Patch-Based Latent Scene Diffusion", "content": "We learn a denoising diffusion probabilistic model Gi [21] for each resolution level i of the latent tree to generate high-fidelity 3D scenes. We leverage the explicit factorization in our latent tree (Sec. 3.2) and train G\u2081 to generate the latent feature grid H, conditioned on geometry grid L. Since 3D scenes typically consist of local structures with shared similarities, our latent diffusion models are trained on scene patches randomly cropped from scene grids.\nWe denote the diffusion model as Gi (zt, t, c), denoising a noisy latent patch z\u0142 at each time step t based on condition c. At each training step, a patch of Gaussian noise \u2208 ~ N(0,1) of the same size and a time step t \u2208 [1,T] are randomly sampled and applied to a latent patch z to obtain a noisy latent patch zt. At level i > 1, we train the diffusion model Gi such that given a coarse geometry patch Li as condition, it predicts its corresponding latent feature patch Hi, i.e., z = Hi and c = Li. At level i = 1, the coarsest level of the latent tree, the diffusion model generates both grids Li and Hi unconditionally, i.e., z = [Li, Hi] and c = \u00d8. The diffusion model Gi learns to denoise zt using the following training loss:\nL_{diff} = E_{z,c,\\epsilon,t} [ ||\\epsilon - G_i (z_t, t, c) ||^2].\nOur latent diffusion models {G} build upon 3D UNet backbones. The patch-wise training strategy also serves as data augmentation, helping our model avoid overfitting and learn shared local structures across scenes. Our conditional diffusion modeling can capture both lower-frequency structures as well as higher-frequency detail, due to our decomposition of 3D scenes into complementary geometry and latent feature grids at each resolution level. Our latent diffusion model can then synthesize arbitrary-sized 3D scene outputs by generating scenes in a patch-by-patch fashion."}, {"title": "3.4. Large-Scale Scene Generation", "content": "At inference time, the generation of large-scale 3D scenes is enabled by our hierarchical latent tree representation (Sec. 3.2) and the latent diffusion models trained on scene patches (Sec. 3.3). The generation process reverses the latent tree decomposition and constructs the hierarchy from coarse to fine levels. At each resolution level, the scene is synthesized in a patch-wise manner, allowing for arbitrary-sized 3D outputs.\nPatch-by-Patch Scene Generation. To construct a latent tree from scratch, we first use the learned diffusion model G\u2081 to create a coarse scene structure at the lowest resolution level i = 1. That is, starting from random Gaussian noise, we apply the diffusion model G\u2081 to unconditionally generate both the geometry L\u2081 and latent feature Hi grids of the scene. We find that using a simple inpainting-based patch-generation algorithm [26, 44], without requiring additional networks, works well to produce 3D scenes without seams. Patches of Li and H\u2081 are synthesized autoregressively on the ground (i.e., xy) plane. We adopt a breadth-first patch ordering scheme. From each known patch 20, we first generate its adjacent patches in the x direction and then the y direction; this is then repeated in a \"dilated\" scene generation process. To ensure a smooth transition between patches, we require that each patch to be generated by the diffusion model G\u2081 has partial overlap with existing patches [34]. We use the same partial overlap size for patches at all resolution levels. Specifically, we follow LDMs [44] and adopt the autoregressive Stable Inpainting scheme:\nz_{t-1}=m z_{t-1}^{known} + (1-m) z_{t-1}^{unknown},\nwhere znown is sampled from the existing patch 20, while zunknown is sampled from the previous denoising iteration Zt. These two are then combined using the inpainting mask m to form the new sample 2t-1, keeping the known region unchanged.\nCoarse-to-Fine Refinement. We then refine the generated coarse scene at higher resolution levels. At each level i > 1, the scene grid L is first reconstructed from the geometry grid L-1 and the latent feature grid H-1 at the lower-resolution level i 1 using the trained decoder Di, as defined in Eq. (2). Next, the diffusion model Gi generates the latent feature grid H conditioned on the geometry grid L at patch-level with overlap.\nHowever, generating patches sequentially like Stable In-painting [44] at higher-resolution levels or infinite scenes with significantly more patches can be extremely time-consuming [54]. To speed up inference, we adapt the denoising fusion scheme from MultiDiffusion [3], which takes each denoising step on all patches simultaneously. First, we divide the generated coarse 3D scene into n patches with the same overlap size as previous levels. Then, the model Gi performs a denoising step in parallel across all patches from the noisy scene grid z\u0142 with geometry patches from L as the condition. Finally, an aggregation step A blends the patches by averaging the predictions from overlapping regions, ensuring smooth transitions across patches and producing seamless 3D scenes. Concretely, each denoising step t at scene resolution level i is formulated as:\nF_j(z_t),t, F_j(L)),\n-1=A({2-1}=1),\nwhere Vj \u2208 [1,n], F; is an operation that crops the j-th patches from the scene grids z\u0142 and L, respectively. The latent feature volume H of the scene is obtained after completing all denoising time steps, i.e., H = z\u0151.\nThe decoders Di and diffusion models G\u2081 are then applied alternately, as illustrated in Fig. 3-bottom right, until the scene geometry L at the highest level N is synthesized as the final output."}, {"title": "4. Experiments", "content": "Experimental Setup. We train LT3SD on house-level scene data from the 3D-FRONT dataset [19], a large-scale indoor scene dataset with diverse structures and layouts. Each house contains various room types (e.g., bedrooms, living rooms, kitchens) with different layouts. After filtering out rooms with incorrect furniture scales or no furniture, we retained 6,479 houses. We used an 80%/5%/15% train/validation/test split for both stages of LT3SD and the baselines. Due to open surfaces in scene meshes, consistent signed distance computation is challenging, so we compute UDFs with a voxel size of 0.022m and a truncation value of r = 0.1m. Instead of preprocessing scenes into fixed patches, we continuously sample chunks from 3D scenes during training to reduce overfitting, applying random flips and rotations for data augmentation.\nImplementation Details. We implemented our method in Pytorch and ran all experiments on NVIDIA RTX A6000. To first construct latent trees for 3D scenes, we train the encoders and decoders with Adam optimizer [27], using a batch size of 4 and a learning rate of 1e-4, which takes from 5 hours to one day for different resolution levels until convergence. We use N = 3 levels in our latent tree. For all levels, the latent feature volumes have 4 channels. We use the Adam optimizer to train the diffusion models, with a batch size of 8 and a learning rate of 1e-4, which takes approximately 6 days until convergence on 2 GPUs. Note that during the second stage, we infer the latent of each TUDF patch on the fly without storing them on the disk in practice.\nBaselines. We compare our method against 3D object diffusion models PVD [58] and NFD [46], as well as the recent 3D scene diffusion approach BlockFusion [54]."}, {"title": "4.1. 3D Scene Generation", "content": "We evaluate our method on 3D scene generation, both quantitatively and qualitatively in comparison with state-of-the-art 3D diffusion alternatives. We generate 3D scenes of a fixed size for each method for evaluation.\nWe show qualitative comparisons in Fig. 4. Our results demonstrate superior surface quality and object detail: PVD generates plausible global structures but produces incomplete geometric information in point clouds; NFD [45] generates plausible scene structures and scales well for complex 3D scenes, but struggles to produce complex arrangements of furniture and smooth surfaces; BlockFusion [54] achieves smooth local surfaces and structural diversity, but struggles in generating meaningful higher-level structures (e.g., Fig. 4-1st row, where walls are misaligned despite improved surface quality). In contrast, our approach generates scenes with both plausible room structures and layouts while maintaining the highest quality in finer-scale details."}, {"title": "4.2. Ablation Study", "content": "Latent Tree Structure. We validate the latent tree structure of our method in Tab. 1 for 3D scene generation. In this ablation, we compare with the one-level latent tree of the same spatial dimension as the coarsest resolution in our latent tree. As shown in Tab. 1-bottom, the multi-level latent tree-based diffusion model outperforms the single-level one. Rather than using a single level of the tree and generating a 3D scene in a single step, our LT3SD adopts a coarse-to-fine strategy for encoding; the diffusion model only generates the higher-frequency component of 3D scenes in each step and, finally, both lower-frequency and higher-frequency components are gracefully combined together and decoded to a high-quality 3D scene. This ablation demonstrates that the hierarchical tree structure is essential for effective generative modeling with diffusion for complex 3D scenes.\nHierarchical Latent Representation. We compare our latent tree with latent Cascaded Models (CMs) (Fig. 2-left), commonly used in 2D image synthesis [22] and 3D generation [32, 43]. Unlike CMs, which independently model redundant information at each level, our latent tree decomposes 3D scenes into coarse geometry and fine latent components, capturing residual high-frequency details. As shown in Tab. 2, with the same autoencoder and parameter count, our model trains more efficiently (requiring less time per level) and achieves higher reconstruction accuracy (lower 12 error) on test data. Additionally, its more compact latent representation reduces storage requirements when converting TUDF grids to latent grids. Training time and storage statistics are reported as ratios w.r.t. latent CM; please see the supplemental for more details."}, {"title": "4.3. Novelty of Generated Scene Patches", "content": "An important consideration in unconditional generation is the ability of trained models to produce novel outputs, including new shapes and scene layouts that differ from the training data. We retrieve the nearest training patch to each generated scene patch based on Chamfer Distance to evaluate this. We perform retrieval from the entire training set, applying all permutations of the training augmentations (flip and rotation). In Fig. 5, we show scene patches generated by our LT3SD, along with the top-4 nearest neighbor training patches. We observe that our generated patches produce new structures different from the training patches. Our method generates strong differences in local object characteristics (top row) as well as differences in layout arrangements (bottom row). We attribute this partly to the random, continuous patch-wise training strategy employed on the fly during the two training stages and the augmentations."}, {"title": "4.4. Probabilistic Scene Completion", "content": "We further demonstrate the generative capability of LT3SD for scene completion from partial observations. Fig. 6 shows that from small portions of a given scene, our diffusion-based approach enables sampling a diverse set of possible completed scenes that explain the partial geometry input. Our approach can preserve the initial scene geometry while generating completed scenes of various sizes. Although LT3SD is trained without semantic annotations, our completion results respect the input object semantic structures (e.g., given a sofa, generating various living rooms)."}, {"title": "5. Conclusion", "content": "We have presented LT3SD, a novel approach for high-quality 3D scene generation through latent scene diffusion. We propose decomposing high-resolution 3D scene voxel grids to compact latent trees for efficient diffusion training in this latent space. This strategy enables seamless infinite 3D scene generation in a patch-wise manner. Experimental validation demonstrates the capacity of LT3SD for large-scale 3D scene generation, significantly outperforming state-of-the-art alternatives for unconditional 3D scene generation. We hope our method provides an avenue for future exploration towards the challenging task of 3D scene generation and automated 3D content creation."}, {"title": "A. Additional Results", "content": "A.1. Intermediate Visualization\nWe visualize the full inference process of generating a 3D scene unconditionally with LT3SD in Fig. 7. Our method operates patch-wise and coarse-to-fine, as introduced in the main paper. First, starting from the random 3D Gaussian noise on the left of the first row, LT3SD gradually denoises it to the mesh at the end of the row. Then, at the second row, our method autoregressively extrapolates the unknown region in a patch-wise manner with an overlap size of one-half of the patch size until the 3D scene of the specified spatial extent is complete. Finally, in the third row, conditioned at the coarse 3D scene, LT3SD generates the fine details also in a patch-wise manner with the same overlap size. This process is implemented in a batch-wise manner"}, {"title": "A.2. Additional Novelty Analysis Results", "content": "In Fig. 8, we provide additional novelty analysis results. The results further show that our method learns to generate novel 3D scenes with different furniture layouts and details. We highlight that our model is only trained on random patches of the 3D scenes with complex furniture layouts but without semantic information."}, {"title": "A.3. Infinite 3D Scenes", "content": "Here, we present more examples of the infinite 3D scene generation beyond Fig. 1 in the main paper. In Fig. 10, we show two more infinite 3D scenes with the resolution of [4096, 2048, 128] and size of 45.1m \u00d7 90.3m \u00d7 2.8m. Although trained on the house-level data of 3D-FRONT with only a few connected rooms in each sample, our method generates 3D scenes with diverse room structures, furniture layouts, and varying sizes. We generate infinite 3D scenes in the same manner, i.e., patch-wise and coarse-to-fine. The supplementary video shows more examples of infinite 3D scenes with zoom-ins and intermediate visualization."}, {"title": "A.4. Additional Comparisons", "content": "We provide additional comparison results in Fig. 9 using the same model as in the main paper's qualitative results shown in Fig. 4."}, {"title": "B. Evaluation Metrics", "content": "The MMD, COV, and 1-NNA metrics are formally defined as:\nMMD(S_g, S_r) = \\frac{1}{|S_g|} \\sum_{X \\in S_g}  \\min_{Y \\in S_r} D(X,Y),\nCOV(S_g, S_r) =  \\frac{|\\{arg \\min_{Y \\in S_r} D(X, Y) | X \\in S_g\\}|}{|S_r|},\n1-NNA(S_g, S_r) =  \\frac{\\sum_{X \\in S_g} 1[N_X \\in S_g] + \\sum_{Y \\in S_r} 1[N_Y \\in S_r]}{|S_g| + |S_r|},\nwhere D(,) represents the CD or EMD distance, Sg and Sr are the sets of generated point clouds and reference point"}, {"title": "C. Ablation Study", "content": "C.1. Hierarchical Latent Representation\nHere, we provide detailed statistics for Tab. 2 in the main paper: We compare the performance of our latent tree representation against the latent cascaded representation for encoding TUDF voxel grids, using a downsampling/upsampling factor of 4. The latent tree representation requires 20 hours of training and consumes 33KB per room sample for storage, while the latent cascaded representation takes 24 hours and 41KB per sample. After training, we evaluate reconstruction performance on the test set by calculating the 12 error relative to the ground-truth TUDF voxel grids."}, {"title": "C.2. Coarse-to-Fine Refinement", "content": "For large-scale 3D generation, generating patches autoregressively can be highly time-consuming. In the 3D scene depicted in Fig. 4, our full model initially generates the scene patch-by-patch, as outlined in Eq. (5), focusing on capturing the coarse structure. We then apply a coarse-to-fine refinement process, adding high-frequency details using the parallel algorithm, as described in Eq. (6). In the ablation study (Tab. 3), we replaced the parallel algorithm with the autoregressive one Eq. (5) to generate the finer levels. The results show that this approach uses 2.5\u00d7 inference time, compared to our full model. This demonstrates that the batch-wise coarse-to-fine approach significantly reduces overall inference time for large-scale 3D scene generation, in contrast to the purely patch-by-patch method. Furthermore, our approach can be accelerated using multi-GPU setups, a capability that previous works [32, 54], which employ naive autoregressive patch-wise outpainting, do not support."}]}