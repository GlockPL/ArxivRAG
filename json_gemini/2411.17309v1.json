{"title": "PIM-AI: A NOVEL ARCHITECTURE FOR HIGH-EFFICIENCY LLM INFERENCE", "authors": ["Cristobal Ortega", "Yann Falevoz", "Renaud Ayrignac"], "abstract": "Large Language Models (LLMs) have become essential in a variety of applications due to their\nadvanced language understanding and generation capabilities. However, their computational and\nmemory requirements pose significant challenges to traditional hardware architectures. Processing-\nin-Memory (PIM), which integrates computational units directly into memory chips, offers several\nadvantages for LLM inference, including reduced data transfer bottlenecks and improved power\nefficiency.\nThis paper introduces PIM-AI, a novel DDR5/LPDDR5 PIM architecture designed for LLM infer-\nence without modifying the memory controller or DDR/LPDDR memory PHY. We have developed\na simulator to evaluate the performance of PIM-AI in various scenarios and demonstrate its sig-\nnificant advantages over conventional architectures. In cloud-based scenarios, PIM-AI reduces the\n3-year TCO per queries-per-second by up to 6.94x compared to state-of-the-art GPUs, depending on\nthe LLM model used. In mobile scenarios, PIM-AI achieves a 10- to 20-fold reduction in energy per\ntoken compared to state-of-the-art mobile SoCs, resulting in 25 to 45% more queries per second and\n6.9x to 13.4x less energy per query, extending battery life and enabling more inferences per charge.\nThese results highlight PIM-AI's potential to revolutionize LLM deployments, making them more\nefficient, scalable, and sustainable.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 LLM Applications and Rapid Evolution", "content": "Large Language Models (LLMs) have become an integral part of many domains [1, 2], including healthcare, education,\nsocial media, business, law, creative industries, and scientific research. They demonstrate exceptional capabilities in\nnatural language processing (NLP) tasks such as language translation, text generation, and question answering [3].\nTheir rapid development has been driven by advances in deep learning, increased computational resources, and large\ntraining datasets. LLMs now play a critical role in achieving human-like literacy and communication in machines,\naddressing a long-standing challenge in artificial intelligence (AI).\nLLMs evolved from statistical approaches and n-gram models [4], which struggled with long-term dependencies [3].\nRNNs improved sequential data modeling [5] but had issues like vanishing gradients [3]. The breakthrough came with\nthe Transformer architecture [6], which used self-attention to handle long-range dependencies effectively. This led\nto models like BERT [7] and GPT [8]. Encoder-decoder models like T5 [9] and BART [10] unified understanding\nand generation tasks. LLMs also developed emergent capabilities, such as in-context learning seen in GPT-3 [11].\nSpecialized models like Codex [12] and WebGPT [13] showed diverse applications. Recent models, including Meta's\nLLaMA [14, 15], Google's PaLM [16, 17] and Mistral 7B [18], exhibit strong performance with fewer parameters.\nGuided by scaling laws, LLMs like GPT-4 [19] and Mixtral-8x22B [20] achieve high coherence and relevance across\ntasks, pushing the boundaries of AI."}, {"title": "1.2 Key Elements and Architectures of LLMs", "content": "Large Language Models (LLMs) perform inference by transforming input words or prompts into tokens, which are\nthe basic units of these models. Tokens can represent words or characters [21]. The tokenized input is then processed\nby the model.\nThe execution of LLM is divided into two phases: encoding and decoding. Tokens are first translated into embeddings, which are numerical representations\nof the tokens [22, 23]. These embeddings pass through layers consisting of an input normalization layer [24, 25],\na multi-head attention (MHA) mechanism, an output linear projection, and a feed-forward layer [6]. After the last\nlayer, the embeddings are translated back into tokens and the next token is selected.\nThe MHA mechanism computes the attention of each token with respect to all previous tokens and includes three\ncomponents for each token: Query (Q), Key (K), and Value (V). To reduce redundant computations, the KV cache\nstores previously computed keys and values, offloading some of the computational load to memory [26].\nThe balance between memory bandwidth and compute performance depends on the size of the input matrix (embed-\ndings) [27]. The balance point varies from a few tens to a few hundred tokens, depending on the hardware.\nDuring the encoding phase, the entire initial prompt is processed. The yellow layers in Figure 1 are general matrix\nmultiplications (GEMM), where the input matrix has as many rows as there are tokens in the prompt. This makes the\nencoding compute-bound, since the full prompt is typically large enough to require significant computational resources.\nThe KV cache is populated based on the parameters of the encoders/decoders and the MHA. Once the initial prompt\nis processed, the model generates a token that marks the transition to the decoding phase.\nIn the decoding phase, the most recently generated token is processed using the KV cache. Since the yellow layers\nin Figure 1 process a single row matrix, i.e. general matrix vector multiplications (GEMV), the decoding is memory-\nbound. New keys and values are appended to the KV cache. This cycle of token generation and decoding continues\nuntil a stop token is generated or the maximum number of tokens is reached.\nTo optimize decoding performance, it is beneficial to batch multiple requests to balance memory bandwidth and\ncompute performance [27]. This technique is common in cloud environments, but is not always feasible for mobile or\nedge devices, which typically run on more constrained systems."}, {"title": "1.3 Challenges of Executing LLMs", "content": "The vast majority of LLMs today run in the cloud [28, 29], which presents significant challenges due to their compute-\nand memory-intensive nature. Scalability and cost issues arise from the need to support large numbers of users,\nrequiring significant compute resources [30] and resulting in high operational costs [31]. Real-time applications, such\nas virtual assistants, suffer from high latency due to data transfer times and processing delays. The memory wall\nproblem [32, 33], caused by the separation of memory and processing units, leads to inefficiencies due to slow data\ntransfers, especially for large Key-Value caches. Running LLMs in the cloud using GPUs consumes significant energy,"}, {"title": "1.4 Rationale for a Novel PIM Architecture", "content": "Researchers and engineers are exploring a variety of solutions to improve the efficiency and scalability of LLMs and\novercome the computational and storage challenges associated with running LLMs."}, {"title": "1.4.1 Model Reduction Techniques:", "content": "Several techniques have been developed to make LLMs less compute- and memory-intensive. Quantization reduces\nmodel weights to smaller bit widths to save memory and energy [42, 43, 44, 45, 46] but can lead to a loss of accuracy\nat very low bit widths. Pruning [47, 48] removes unimportant weights to enhance efficiency, though it may degrade\nperformance by eliminating critical parts of the model. Compression [49] reduces model size but can slow down\ninference due to the need to decompress data on the fly. Knowledge distillation [50, 51] trains smaller models under\nthe guidance of larger ones but often requires extensive fine-tuning and may not match the performance of their larger\ncounterparts."}, {"title": "1.4.2 Traditional Hardware Accelerators:", "content": "An alternative approach to model reduction is the development of specialized hardware accelerators [30]. Field Pro-\ngrammable Gate Arrays (FPGAs) offer flexibility through reprogrammability, allowing customization for specific\nworkloads. However, they are often expensive and require significant expertise to program efficiently. CPUs and\nGPUs excel at parallel processing and are widely used for AI tasks, but they face significant inefficiencies. GPUs in\nparticular have high power consumption and cost. Application-Specific Integrated Circuits (ASICs) offer the high-\nest performance and energy efficiency for fixed tasks, but are extremely expensive to develop and lack flexibility for\ndifferent models.\nDespite their advantages, these traditional accelerators are based on the von Neumann architecture where processing\nand memory are separated. This leads to inefficiencies due to the memory wall problem [32, 33], where the speed\nat which data can be transferred between memory and processing units lags behind processor speeds. This mismatch\ncauses significant delays and energy consumption, especially during attention computations in LLMs.\nWhile high-speed memory technologies such as High Bandwidth Memory (HBM) and Compute Express Link (CXL)\nimprove capacity and bandwidth [52], they do not fully address the fundamental problem of data transfer bottlenecks."}, {"title": "1.4.3 Processing-in-Memory (PIM) Accelerators:", "content": "The PIM architecture integrates computational units into the memory chip itself, enabling data processing directly\nwhere the data is stored [53]. This approach eliminates the need for extensive data transfers between memory and\nCPU/GPU, directly addressing the memory wall issue. By exploiting bank parallelism within the DRAM structure,\nPIM can achieve internal bandwidths many times greater than external bandwidths.\nEmerging memory technologies [54], such as capacitorless gain cell-based eDRAM, ferroelectric memory, spin-\ntransfer torque magnetic random access memory (STT-MRAM), and spin-orbit torque magnetic random access mem-\nory (SOT-MRAM), are being explored but are far from commercialization. More mature technologies, such as UP-\nMEM PIM [55] and Samsung's HBM PIM [56], seem more realistic as a short-term solution:\nUPMEM PIM is the first commercially available PIM architecture. It combines each DRAM bank with a custom 32-\nbit processor capable of running 24 threads independently. While UPMEM PIM shows impressive speedups, as well"}, {"title": "1.5 Research Objectives and Key Contributions", "content": "This research introduces a novel accelerator architecture for LLMs and other memory-intensive workloads, overcom-\ning limitations of traditional hardware accelerators. The core innovation is the PIM-AI architecture, integrating com-\nputational units directly into the memory chip, significantly reducing data transfer bottlenecks and improving overall\nperformance and energy efficiency.\nA simulator was developed to analyze LLM execution on various hardware platforms, providing a comprehensive\nperformance analysis of PIM-AI under different scenarios and highlighting its advantages over reference architectures.\nThe paper is organized as follows: Section 2 introduces the PIM-AI architecture, Section 3 outlines the experimental\nsetup and hardware simulator, Section 4 presents the results, Section 5 discusses the simulation results, limitations,\nand future research directions, and Section 6 concludes the paper."}, {"title": "2 A new PIM architecture for LLMs", "content": "The PIM-AI architecture is a novel PIM design aimed at addressing the computational and energy efficiency challenges\nof the encoding and decoding phases of LLM operations. This architecture integrates seamlessly with existing systems\nwithout requiring modifications to the memory controller or DDR/LPDDR memory PHY."}, {"title": "2.1 PIM-AI Chip", "content": "The PIM-AI chip operates in two modes: non-PIM mode and PIM mode. In non-PIM mode, the operating system (OS)\nuses the full 2GB capacity of the memory chip as standard memory. In PIM mode, the chip acts as an accelerator,\nimproving performance and energy efficiency."}, {"title": "2.2 PIM-AI DIMM", "content": "The PIM-AI architecture can be scaled by integrating multiple PIM-AI chips into a dual-in-line memory mod-\nule (DIMM), increasing both bandwidth and compute capacity. A typical PIM-AI DIMM contains 32GB of memory\nwith a total aggregate bandwidth of 1.6TB/s and a compute capacity of up to 128 TFLOPs FP16/BF16.\nEach PIM-AI DIMM contains an internal chip interconnect system that links all PIM-AI chips within the DIMM.\nThis interconnect facilitates data sharing among the chips, minimizing the data transfer load from the HOST and\nenabling effective parallel processing.\nIn a mode of operation where the same data needs to be sent to all chips, the HOST can partition the input data and\nsend a segment of it to each PIM-AI chip. The chips then internally distribute their segments to all other PIM-AI chips\nwithin the DIMM, ensuring that each chip has access to the full input data set. This mechanism reduces the amount of\ndata that must be written by the HOST.\nIn another mode of operation, where different data sets are processed by different PIM-AI chips, the internal communi-\ncation system allows intermediate results to be exchanged between chips. This allows different stages of a processing\npipeline to be parallelized, improving overall efficiency and performance.\nDuring read operations, the HOST retrieves the results of the partial operations performed by each PIM-AI DIMM.\nThis modular approach allows the PIM-AI architecture to efficiently handle large computations, taking advantage of\nthe high bandwidth and processing power distributed across multiple DIMMs."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Hardware LLM Simulator", "content": "To compare our proposed PIM-AI architecture with state-of-the-art hardware, we developed a simulator for PyTorch\nmodels. This simulator is available at https://github.com/upmem/upmem_llm_framework. It can execute mul-\ntiple layers and functions during model inference without requiring modifications to the original PyTorch model.\nDuring execution, the simulator collects metrics such as total number of TOPs (Tera Operations), execution time, data\ntransfer sizes (H2D: HOST to device, D2H: device to HOST and main memory), energy consumption, and power\nconsumption for a given hardware profile.\nA hardware profile is a set of configurable parameters that represent a hardware accelerator. These parameters include:\nTOPS and energy per TOP, Bandwidth and energy per bit for D2H and H2D data transfers, Bandwidth and energy\nper bit for data transfers with main memory, and Execution cycles for other functions (e.g., activation functions,\nnormalization functions).\nTo model execution, the user provides a mapping scheme that assigns each layer and function to a specific hardware\nprofile. Synchronization points can be defined where data is transferred between devices that are not interconnected.\nAt these points, data is sent from the last executed hardware profile to the HOST and then back to another device if\nneeded.\nThe simulator captures the start, end, type, and inputs of functions during graph execution (Figure 3). Based on the\nfunction type and inputs, the simulator derives all relevant metrics. Inputs are multidimensional arrays or tensors (e.g.,\n[batch_size, num_rows, num_cols] or [batch_size, num_heads, num_rows, num_cols]).\nData transfers are modeled based on the bandwidth and energy per bit parameters of the involved hardware profiles.\nThe total number of bits is calculated from the input data format and the metrics are derived accordingly. While data\nexchanges could be partially hidden by overlapping data exchanges and computations, the current baseline provides a\nworst-case latency scenario."}, {"title": "3.2 Hardware Profiles", "content": "We parameterize the PIM-AI chip and DIMM to create hardware profiles for the simulator based on DDR4 PIM\nproducts such as UPMEM [55]. In addition, we parameterize state-of-the-art NPUs from mobile SoCs: A17 Pro,\nSnapdragon 8 Gen 3, and Dimensity 9300, as well as the NVIDIA H100 for cloud comparisons.\nMemory energy per bit (pJ/bit) was derived from the memory technologies used: LPDDR5 for A17 Pro, LPDDR5x for\nSnapdragon 8 Gen 3, LPDDR5T for Dimensity 9300, and HBM for NVIDIA H100. For main memory access energy,\nwe included both the SoC interface and the corresponding memory-side interface, resulting in a 2x factor that is not\npresent in PIM-AI, where memory and processing units are integrated on the same chip. Table 1 shows the resulting\nhardware profiles used in this study.\nFor NVIDIA H100 and PIM-AI DIMMs, data transfer energy per bit include the energy of broadcasting the data to all\nother devices (e.g. an NVIDIA H100 within a DGX-H100 server shares the output of its sub-operation to other GPUs\nin the DGX-H100). Also, we consider that GPUs within a DGX-H100 server synchronize with other GPUs through\nswitches. Therefore, we account 20 pJ/bit from GPU to switch and another 20pJ/bit for the opposite path."}, {"title": "3.3 Target Scenarios", "content": "We envision two different deployment scenarios for LLMs:"}, {"title": "3.4 Benchmarks", "content": "Open source implementations of various LLMs from Huggingface [60] are used to provide a comprehensive analysis of\nour proposed architecture in comparison to current and widely used architectures. The simulator seamlessly integrates\nthese pre-trained models implemented in PyTorch.\nThe evaluation is based on a standard experimental setup where 1000 tokens are used for the initial query (input) and\n100 tokens are generated as output.\nFor cloud inference, we use the Llama2-70B and Mixtral-22x7B models, inferred using the 16-bit data format.\nWe normalize the number of PIM-AI DIMMs with respect to H100 GPUs in a rack unit (U) form factor. A DGX-\nH100 server has an 8U form factor with 8 GPUs, while a 2U PIM-AI server has 32 DIMM slots (24 PIM-AI DIMMs\nand 8 standard DIMMs), allowing 96 PIM-AI DIMMs in 8U. Each model requires eight PIM-AI DIMMs, enabling par-\nallel batch processing on 12 inference engines. Both models use 8-group GQA to manage memory requirements, and\nour PIM-AI architecture supports MHA because of its superior memory capacity, bandwidth, and energy efficiency.\nFor mobile inference, industry standard on-device inference techniques are applied to the Llama2-7B and Mistral-\n8x7B models. Both models are quantized to 4-bit weights, the KV cache is stored in 16-bit, and activation functions\nare computed with 16-bit."}, {"title": "3.5 Performance Metrics", "content": "We evaluate the PIM-AI architecture using specific metrics to provide a comprehensive understanding of its efficiency\nand performance in handling both the encoding and decoding phases of LLM operations. In addition, we include an\nevaluation of its overall performance to highlight its advantages over alternative architectures.\n\u2022 Encoding Performance\nTime to First Token: Measures the time to produce the first token after receiving the initial prompt.\nShorter times indicate better performance.\nEnergy Consumption: Evaluates the total energy consumed during the encoding phase to produce the\nfirst token. Lower energy consumption indicates higher efficiency.\n\u2022 Decoding Performance\nTokens per second: Measures the rate at which tokens are generated after the first token. Higher rates\nindicate better performance.\nEnergy per token: Evaluates the energy efficiency of generating each token during the decoding phase.\nLower energy per token indicates higher efficiency.\n\u2022 Overall Performance\n4 Results\nQueries per Second (QPS): Combines time to first token and total decoding time to measure the overall\nrate of processing queries. Higher QPS indicates better performance.\nEnergy per Query (EPQ): Evaluates the total energy consumed per query, providing an overall measure\nof energy efficiency. Lower EPQ indicates better energy efficiency."}, {"title": "4.1 Performance in Cloud Scenario", "content": "Figure 4 shows the results of simulating Llama2-70B and Mixtral-8x22B models on a DGX-H100 and four PIM-AI\nservers. For Llama2-70B with GQA=8, batch sizes are 200 on H100 and 80 on PIM-AI; with MHA, batch sizes are\n46 on H100 and 10 on PIM-AI. For Mixtral-8x22B with GQA=8, batch sizes are 200 on H100 and 80 on PIM-\u0391\u0399;\nfor MHA, batch sizes are 88 on H100 and 20 on PIM-AI. When running these models with MHA on H100 GPUs,\npeak performance and energy efficiency is constrained by batch size limits."}, {"title": "4.1.1 Encoding Phase:", "content": "For both the Llama2-70B and Mixtral-8x22B models, with GQA=8, the first token latency for PIM-AI is roughly 3x\nlonger than that of the DGX-H100 server, while the energy consumption is more than halved. With MHA, the first\ntoken latency for PIM-AI is about 75% longer than that of the DGX-H100 server, but the power consumption is about\nfour times lower."}, {"title": "4.1.2 Decoding Phase:", "content": "For both the Llama2-70B and Mixtral-8x22B models, and for both GQA=8 and MHA configurations, PIM-AI achieves\n2.23x to 2.75x more tokens per second and 15% to 40% less energy per token compared to the DGX-H100 server."}, {"title": "4.1.3 Overall Performance:", "content": "On average, the four PIM-AI servers process 55% more queries per second than the DGX-H100 server, while consum-\ning equivalent energy per query."}, {"title": "4.2 Performance in Mobile Scenario", "content": "Figure 5 shows the results of running the Llama2-7B and Mistral-7B models with a batch size of 1, simulating a single\nuser interaction."}, {"title": "4.2.1\nEncoding Phase:", "content": "All hardware profiles achieve similar first token latency due to their comparable TOPS capabilities. However, the\nPIM-AI chip achieves notable energy savings of 28.5%, 16.4%, and 15.3% compared to the A17 Pro, Snapdragon 8\nGen3, and Dimensity 9300, respectively, due to its direct memory access."}, {"title": "4.2.2 Decoding Phase:", "content": "The PIM-AI chip outperforms the A17 Pro, Snapdragon 8 Gen3, and Dimensity 9300 in tokens per second due to its\nhigher bandwidth of 102.4 GB/s compared to 77 GB/s for the other profiles. It achieves tokens per second improve-\nments of 49.6%, 24.5%, and 24.7%, respectively. The PIM-AI chip is also significantly more energy efficient, 20\ntimes more efficient per token than the A17 Pro and 10 times more efficient than the other profiles. As the number of"}, {"title": "4.2.3 Overall Performance:", "content": "PIM-AI processes around 45% more queries per second than the A17 Pro and 25% more than the other SoCs. It con-\nsumes about 13.4 times less energy than the A17 Pro and 6.9 times less energy than the others."}, {"title": "5 Discussion", "content": ""}, {"title": "5.1 Analysis of Simulation Results", "content": "In cloud scenarios, while the initial token latency with PIM-AI is significantly higher than with the DGX-H100, the\nhigher token generation rate compensates for this. PIM-AI achieves more queries per second for equivalent energy\nconsumption. The advantage of PIM-AI becomes more pronounced with longer performance. In an experimental\nsetup with 1,000 input tokens and 1,000 output tokens, PIM-AI delivers 47% more queries per second and consumes\n15% less energy than the DGX-H100. The primary advantage of PIM-AI in this context is a significant reduction in\nthe 3-year total cost of ownership (TCO) per QPS, with PIM-AI showing cost ratios between 6.2x and 6.94x more\nfavorable than the DGX-H100, depending on the LLM model. This estimate is based on a PIM-AI production server\ncost of approximately $15k ($60k for 4 servers) compared to $300k for a DGX-H100 server, and a world wide average\nelectricity price of $0.153/kWh [61].\nFor mobile devices, the overall energy efficiency of PIM-AI translates directly into a significant increase in battery\nlife. The reduction in energy per query means that users can perform 6.9x to 13.4x more inferences before the battery\nruns out. When generating 1,000 tokens per inference, these ratios are even higher at 9.8 to 19.5."}, {"title": "5.2 Limitations", "content": "In this paper, we assume that the entire inference process, including both encoding and decoding phases, is performed\non the PIM-AI. Our current PIM-AI chip significantly improves the decoding phase compared to other hardware\nprofiles. However, the encoding phase could be optimized either by increasing the TOPS or by using a heterogeneous\napproach with a less energy-efficient accelerator [56]."}, {"title": "5.3 Future Perspectives", "content": "Previous research on heterogeneous approaches has shown good performance for both encoding and decoding phases,\nwhile improving overall energy efficiency [56]. Our simulations confirm that combining PIM-AI with other accelera-\ntors could optimize both phases and improve energy efficiency. However, this poses challenges for embedded devices\nwhere factors such as area, energy, power consumption, and CPU orchestration are critical. Future work will analyze\nthis approach.\nIn parallel, we will validate these simulation results on a real PIM-AI chip, aiming for a prototype by the end of\n2025. This validation will confirm the benefits observed in the simulations and ensure the practical applicability of the\nPIM-AI technology."}, {"title": "6 Conclusions", "content": "This research introduces PIM-AI, a novel PIM architecture designed to meet the computational and memory require-\nments of Large Language Models (LLMs). By integrating computational units into DDR5/LPDDR5 memory chips,\nPIM-AI addresses data transfer bottlenecks and improves performance and energy efficiency. Simulations show that\nPIM-AI reduces the 3-year total cost of ownership per QPS by up to 6.94x for cloud scenarios and achieves a 10x to\n20x reduction in energy per token for mobile scenarios. Furthermore, PIM-AI processes 25 to 45% more queries per\nsecond for mobile devices while consuming 6.9x to 13.4x less energy compared to state-of-the-art SoCs. These re-\nsults suggest that PIM-AI can significantly enhance the efficiency, scalability, and sustainability of LLM deployments,\npaving the way for more advanced, energy-efficient, and cost-effective AI solutions."}]}