{"title": "PowerPM: Foundation Model for Power Systems", "authors": ["Shihao Tu", "Yupeng Zhang", "Jing Zhang", "Yang Yang"], "abstract": "The emergence of abundant electricity time series (ETS) data provides ample opportunities for various applications in the power systems, including demand-side management, grid stability, and consumer behavior analysis. Deep learning models have advanced ETS modeling by effectively capturing sequence dependence. Nevertheless, learning a generic representation of ETS data for various applications remains challenging due to the inherently complex hierarchical structure of ETS data. Moreover, ETS data exhibits intricate temporal dependencies and is susceptible to the influence of exogenous variables. Furthermore, different instances exhibit diverse electricity consumption behavior. In this paper, we propose a foundation model PowerPM to model ETS data, providing a large-scale, off-the-shelf model for power systems. PowerPM consists of a temporal encoder and a hierarchical encoder. The temporal encoder captures both temporal dependencies in ETS data, considering exogenous variables. The hierarchical encoder models the correlation between hierarchy. Furthermore, PowerPM leverages a novel self-supervised pre-training framework consisting of masked ETS modeling and dual-view contrastive learning, which enable PowerPM to capture temporal dependency within ETS windows and aware the discrepancy across ETS windows, providing two different perspectives to learn generic representation. Our experiments involve five real-world scenario datasets, comprising private and public data. Through pre-training on massive ETS data, PowerPM achieves SOTA performance on diverse downstream tasks within the private dataset. Impressively, when transferred to the public datasets, PowerPM maintains its superiority, showcasing its remarkable generalization ability across various tasks and domains. Moreover, ablation studies, few-shot experiments provide additional evidence of the effectiveness of our model.", "sections": [{"title": "1 Introduction", "content": "The volume of Electricity Time Series (ETS) data has recently increased rapidly due to the emergence of advanced power systems known as smart grids [10]. This abundance of data has paved the way for diverse applications in power systems, including demand-side management [22], grid stability [2] and consumer behavior analysis [49], etc. Meanwhile, these applications have spawned various tasks, as shown in Fig. 1(d), such as load forecasting [27, 4], clock anomaly detection [46], as well as electricity theft [15] and elderly living alone detection [45]. Recent statistics show that the total\nRecently, numerous research studies on pre-training approaches for ETS data have emerged. These approaches adopt the \"pre-training then fine-tuning\" paradigm, which solves the dilemma of limited annotation data, and the pre-trained model can easily adapt to new tasks. Such as PatchTST [21], TS2Vec [42], CoST [37], etc. However, these pre-training methods only utilize small-scale of data with a small number of instances (e.g. users), resulting in poor performance on downstream tasks.\nAs the same time, many researcher begin to apply Large Language Models (LLMs) to assist time series modeling by using pre-trained LLM to encode time series [51] or incorporating additional descriptions related to the time series [17, 20]. Nevertheless, these models have limited ability in the power system scenario due to insufficient pre-training data of power systems and the lack of sufficient domain-specific knowledge. Additionally, none of these models are tailored for the scenario of power systems, neglecting the unique characteristics of ETS data. Therefore, existing power systems related works still maintain a large research gap in modeling ETS data with a foundation model.\nIn our scenario, the ETS data contains numerous instances and naturally exhibits a complex hierarchy [41, 23]. As depicted in Fig. 1(a), a city ETS can be disaggregated into district ETS through the administrative divisions, which are further disaggregated into user ETS in this district. For the complex hierarchy of ETS data, modeling ETS data entails the consideration of several challenges:\n(1) Hierarchical Dependency Modeling. The hierarchy of ETS data facilitates information interaction across different granularities. Fine-grained ETS provides detailed insights into individual electricity usage, while coarse-grained ETS from districts and cities captures broader factors, indicating overall trends. For example, user-level data reflects user-specific behaviors and city-level data encompasses demographics and policy effects [29, 35]. Integrating these levels of granularity to provide both macro and micro perspectives is a complex task that requires sophisticated modeling.\n(2) Temporal dependencies within ETS window. An ETS window refer to a piece of electricity time data over a period of time. The temporal dependencies within an ETS window refer to the correlations and dependencies between observations at different timestamps. As shown in Fig. 1(b), the city-level ETS exhibits daily and weekly dependency. Moreover, the temporal dependencies are often influenced by exogenous variables, such as weather, temperature, and seasonal effects. Integrating these factors into the model is challenging because their impact may interact with the temporal dynamics in complex ways. Accurately capturing the temporal dependencies with the impact of exogenous variables is a key challenge in modeling ETS data.\n(3) Discrepancy across ETS windows. The patterns observed in ETS windows can vary significantly across different instances and different timestamps. For instance, as shown in Fig. 1(c), residential electricity consumption (User A) reaches its peak in the mornings and evenings, used for lighting, appliances, and heating. However, due to residents typically being away for work or education"}, {"title": "2 Methodology", "content": "Overview. As shown in the middle part of Fig. 3: Firstly, the hierarchical graph G is constructed according to the naturally existing hierarchical relationship of ETS data. The ETS windows in G and its corresponding exogenous variables are denoted as {$\\mathbf{x}_i$}$_{i=1}^{N}$ and {$\\mathbf{o}_i$}$_{i=1}^{N}$, where N is the number of instances, $\\mathbf{x}_i \\in \\mathbb{R}^{T_w}$, $\\mathbf{o}_i \\in \\mathbb{R}^{T_w \\times K}$, each instance ETS window spans $T_w$ time points starting at\n$\\mathbf{x}_i \\in \\mathbb{R}^{N \\times d}$,\n$\\{\\mathbf{x}_i\\}_{i=1}^{N}$,\n$L_{MSE} = \\frac{1}{2N}\\sum_{i=1}^{N}||\\hat{x}_i - x_i||^2_2$\nTa and ending at Tb. Each time point has K kinds of exogenous variables. Our objective is to perform pre-training on an encoder f(\u00b7) to encode each window into a latent representation $\\mathbf{z}_i$, where d indicates the dimension of the latent representation. More specific, PowerPM consists of an exogenous variable enhanced temporal encoder fT(\u00b7) and a hierarchical encoder fH(\u00b7), with the process: $\\mathbf{z}_i = f(\\mathbf{x}_i, \\mathbf{o}_i, G) = f_H(f_T(\\mathbf{x}_i, \\mathbf{o}_i), G)$. In addition, a novel self-supervised strategy, which combines masked ETS modeling and dual-view contrastive learning, is used for pre-training PowerPM. Next, we will detail the techniques in both model architecture and pre-training strategy.\n2.1 Hierarchical Graph Construction.\nThe cities, districts, and users in ETS data naturally form a hierarchical relationship, based on which we can construct a hierarchical graph. However, the imbalance in the number of users and districts means there will be multitude of edges between user nodes and district nodes, which significantly increases the complexity of graph modeling. To address this, we employ a clustering strategy to create intermediary nodes, a common approach to implement graph sparsification [13] and a user group policy in the power systems [36, 44, 12]. As depicted in Fig. 3 (c), we use clustering method to categorize users into several clusters, the detailed process can be found in App. D.1. The cities are bidirectionally connected to districts, and these user clusters are also bidirectionally connected to districts, while users are unidirectionally connected to districts. By sparsifying the edges, we enhance the efficiency of graph modeling. Mathematically, we represent the hierarchy as a directed graph G = (V,E,R), where V is the set of nodes, each node corresponds to an instance, E is the set of directed edges, and R is the set of type of edges (e.g. user cluster \u2192 district, district \u2192 user, etc.).\n2.2 Temporal Encoder with Exogenous Variables.\nPatching. In the G, each node's feature xi is a window of ETS data corresponding to instance i. Due to the semantic sparsity of time series, we patch each window xi into Np segments, each of length P, resulting in $\\mathbf{p}_i \\in \\mathbb{R}^{N_p \\times P}$, where $N_p = [\\frac{T_w}{P}] + 1$, and this method proved its validity in many works [21, 17, 20]. Subsequently, a linear projection is applied to each segment to obtain the window representation hi \u2208 RNp\u00d7d\nExogenous Variables Encoding. To efficiently interact with exogenous variables, we model these variables using learnable embeddings E\u2208R(Mr)\u00d7d, where K indicates the number of exogenous variables (e.g. weather type and temperature), Mk represents the number of value types of the k-th exogenous variable (e.g. sunny and rainy in weather type variable). The exogenous variables $o_i^{(k)} \\in \\mathbb{R}^{N_p \\times P}$ corresponding to pi of the k-th exogenous variable are used to obtain the exogenous variables representations from E, indexing out $(e_i^{(k)}) \\in \\mathbb{R}^{N_p \\times d}$, as illustrated in Fig. 3"}, {"title": "2.3 Hierarchical Encoder", "content": "To model the complex correlation across different hierarchies, we employ Graph Neural Networks (GNNs). GNNs have gained significant popularity recently for modeling relationships among time series, thereby enhancing temporal representation [7, 26, 40]. In addition, considering that the correlation relationships of different edges are distinct, we adopt R-GCN [25] to integrate information across various hierarchies and instances, as depicted in Fig 3 (a). Specifically, we use R-GCN to update the representation 2 by considering its neighboring nodes in G, with the final node representation denoted as zi \u2208 RNp\u00d7d. Moreover, we use zi to perform self-supervised pre-training."}, {"title": "2.4 Self-supervised Pre-training", "content": "2.4.1 Masked ETS Modeling\nTo model temporal dependency within an ETS window, we have adopted the widely utilized masked reconstruction strategy. Nevertheless, existing random masking methods may encounter an issue: they reconstruct the missing part based on the known surrounding part [21, 8], without considering the prediction of future parts relying solely on the past part, which not only diminishes the difficulty of the pre-training stage but also lacks consistency across pre-training task and forecasting task.\nTo address this issue, we propose a novel masking approach that combines random and casual masking as shown in Fig. 3 (d) (left). Specifically, we randomly select one of the masking approaches for a given patched window pi, resulting in masked pi. This approach not only retains the benefits of the random masking strategy but also ensures that the model learns to predict future parts based solely on past information, thereby more comprehensively capturing the temporal dependencies within\nmasked pi=\\begin{cases}\nMask_r(p_i) \\text{ if } \\alpha < 0.5 \\\nMask_c(p_i) \\text{ otherwise}\n\\end{cases},\nwhere Maskr and Maske denote the random and causal masking, respectively, and \u03b1 \u2208 [0, 1] is a uniformly distributed variable. Specifically, after the xi is inputted into PowerPM for masked ETS modeling, we will obtain a reconstructed 2. The corresponding reconstruction loss is:"}, {"title": "2.4.2 Dual-view Contrastive Learning", "content": "The objective of contrastive learning is to learn representations by bringing positive pairs closer and pushing negative pairs farther apart in the latent space [5, 6]. Motivated by this, to make PowerPM aware of the discrepancy across ETS windows, we employ dual-view contrastive learning (DVCL) to discern subtle differences in electricity usage behavior.\nPositive and Negative Sample Pairs. These pairs are determined from two views: one is temporal view, which is based on the time difference between the two windows. Another is the instance view, which depends on whether two windows belong to the same instance. For the same instance, the closer the time difference between two windows, the closer their representations are likely to be. This idea is also presented in [31, 42]. Conversely, windows from different instances or the same instance with a larger time difference are likely to have more distinct representations. Overall, we consider adjacent windows from the same instance as positive samples, while windows from different instances or non-adjacent windows from the same instance are negative samples. As depicted in Fig. 3 (d) (right), for the district node V in G, the original start timestamp about this window is Ta. After shifting several time steps d on, we obtain another window V+ starting at Ta + d, which serves as a positive sample. Meanwhile, we select windows from other nodes in G, such as city P, starting at Ta, as well as windows from the same node V but starting at Tc, where Tc-Ta \u226b \u03b4. These windows serve as instance and temporal negative samples, respectively, and are denoted as P- and V-."}, {"title": "3 Experiments", "content": "3.1 Experiment Setup\nPre-training Dataset PowerPM is pre-trained on 987.42GB ETS data, a private dataset collected by the State Grid Corporation of China in Zhejiang province. This pre-training dataset encompasses ETS data from 11 cities, 90 districts, and 1530826 users, with over 1000 days records. The ETS data is collected at a frequency of one data point every 15 minutes. More details are in App. C\nDownstream Dataset To evaluate the performance of PowerPM, we conduct comprehensive experiments on eleven downstream private and public datasets. Seven private datasets are also collected from the State Grid in Zhejiang, China. These datasets have different labels for different tasks. Among them, the solar generation dataset does not have a hierarchical structure due to its particularity. Four public datasets are obtained from CSISO 2, ISONE\u00b3, NYISO 4, and PJM 5, which all exhibit a hierarchical structure. Further details can be found in Appendix C.\nSettings. For the model configurations, the temporal encoder contains a 26-layer Transformer encoder with model dimension 1024, inner dimension (FFN) 2048 and 16 attention heads, and the hierarchical encoder contains 2-layer R-GCN. PowerPM contains about 250M parameters. During pre-training, the 40% segments in each input window are masked in the form of random mask and casual mask, the user cluster numbers is set to 12. See further details in App. D.1\nBaselines. We compare with 8 state-of-the-art methods: including Large Language Model (LLM) enhanced models: GPT4TS [51], Time-LLM [17], UniTime [20]; pre-train models: PatchTST [21], COST [37], TS2Vec [42]; supervised models: DLinear [43], TimesNet [38]. More implementation details are provided in App. D.2.\nEvaluation Metrics. For forecasting and imputation tasks, we use mean squared error (MSE):\n$\\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$ and mean absolute error (MAE): $\\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y}_i|$ as the evaluation metric. For classification tasks, we use accuracy as the metric. The metric of the anomaly detection task includes precision, recall, F0.5, and F\u2081 scores. The Fmeasure is a metric defined as the weighted harmonic\nF_{\\beta} = \\frac{(1+\\beta^2)\\times\\text{precision}\\times \\text{recall}}{\\beta^2\\times\\text{precision} + \\text{recall}}.\nWe use F0.5 for anomaly detection, as precision is more important than recall in power systems scenario [15]."}, {"title": "3.2 Downstream Tasks", "content": "Demand-side Management. Demand-side management aims to optimize and balance the power system by managing and adjusting the electricity demand of end-users. We develop tasks to predict load at different levels (such as cities and users) and tasks to forecast solar generation. With demand-side management, we can better plan and schedule power resources, improve energy efficiency, promote the development of renewable energy, and achieve sustainable energy management.\nGrid Stability. To ensure the stability of the power grid, we have implemented a series of measures, including electricity theft detection, load imputation, and clock anomaly detection, to address the impact of potential appliance failures within the grid and external electricity theft on the quality of power data and grid operations. Internal appliance malfunctions within the grid, such as clock anomalies or the inability to record electricity usage accurately, decrease the accuracy of power data, making it challenging for power dispatch and management. Additionally, external electricity theft"}, {"title": "3.3 Main Results", "content": "Overview. As a foundation model for power systems, PowerPM achieves SOTA performance on various tasks when compared to other baseline models, highlighting its ability to generalize effectively across a wide range of tasks. We derive more detailed comparisons of each task in the following paragraphs, where in all tables we mark the best results in bold, the second-best in underlined, and the third-best in * asterisk in each column.\nDemand-side Management. The forecasting results for load and solar generation are presented in Tab. 1 (upper part). The results cover various forecast horizons, including 4 (1 hour), 96 (1 day), 288"}, {"title": "3.4 Model Analysis", "content": "Generalization ability analysis. To further verify the generalization ability of PowerPM on more datasets from other domains, we evaluate PowerPM on 4 public datasets mentioned above. The results in Tab. 2 demonstrate that not only PowerPM outperforms nearly all SOTA methods but alsoPowerPM freeze surpasses most SOTA methods, highlighting the superiority of PowerPM in terms of generalization ability."}, {"title": "Ablation Study", "content": "Ablation Study. To assess the effectiveness of each component in our model, we conduct several ablation experiments. Specifically, we remove the following components from our model to examine their effects on performance: the hierarchical encoder (PowerPM-H), the dual-view contrastive learning strategy (PowerPM-C), and the exogenous variables encoding module (PowerPM-E). Furthermore, we replace the masked ETS modeling module with vanilla random masking (PowerPM-M). We categorize the 44 tasks into four traditional time series analysis tasks: forecasting, missing value imputation, anomaly detection, and classification. The evaluation metrics are Mean Squared Error (MSE) for forecasting and missing value imputation, F0.5 score for anomaly detection, and accuracy (Acc.) for classification. The performance is averaged to provide a comprehensive assessment.\nThe results of the ablation study are in Fig. 4 (a). The results indicate that PowerPM outperforms other variants of it, providing evidence for the contribution of each component in our model. Among the different variants, PowerPM-H exhibits the most substantial decrease in performance compared to the full PowerPM, emphasizing the significance of interactions occurring between micro- and macro-levels in modeling hierarchical ETS data. The observed performance degradation in PowerPM-M, particularly in forecasting tasks, provides evidence that causal masking can capture more complex temporal dependency. Moreover, the decline in the performance of PowerPM-C, particularly in anomaly detection and classification tasks, suggests that dual-view contrastive learning is effective in capturing subtle discrepancies between instances. Furthermore, PowerPM-E also exists in performance degradation. This emphasizes the effectiveness of the exogenous variables encoding module in capturing the impact of exogenous factors. For the full results of 44 tasks, please refer to App. 7."}, {"title": "Few-shot Learning", "content": "Few-shot Learning. In power systems, collecting abundant ETS data for downstream tasks is a significant investment. To demonstrate the value of the practical application of our work, we conduct a performance comparison between PowerPM and baseline models on downstream tasks, considering the limited availability of ETS data. Specifically, models are fine-tuned on 10%, 30% and 60% of the downstream dataset, respectively. Similar to an ablation study, we present our results grouped by task type. The result can be seen in Fig. 4 (b), the performance of PowerPM exhibits a slight decrease when there is a significant reduction in the proportion of fine-tuning data. This observation serves as evidence of the effectiveness of our novel pre-training strategy, including masked ETS modeling and dual-view contrastive learning. Additionally, it highlights that the PowerPM adeptly captures temporal dependencies and hierarchical correlations present in the ETS data during pre-training, enabling easier adaptation to downstream tasks. More detailed results can be referred to App. 8."}, {"title": "Model Scale Evaluation", "content": "Model Scale Evaluation. To explore the impact of model size on performance, we design three variants of PowerPM with smaller sizes: PowerPM-Tiny (about 30M), PowerPM-Small (about 70M), PowerPM-Medium (about 120M), PowerPM (about 250M), and pre-train them on the same datasets. For the pre-training details, please refer to App. D.1. After pre-training, we evaluate these variants on all downstream tasks and present the results grouped by task type, similar to the ablation study. As shown in Fig. 4 (c), as the size of the model increases, we observe an overall improvement in the performance of all downstream tasks. Specifically, PowerPM outperforms the other variants in all metrics. In addition, larger models exhibit almost a decrease in standard deviation, indicating a more stable performance. Therefore, the utilization of a larger model with higher capacity and vast amounts of ETS data enables better generalization across a wide range of downstream tasks."}, {"title": "4 Conclusion", "content": "This paper introduces the PowerPM, a foundational model designed to model ETS data within power systems. PowerPM consists of a temporal encoder and a hierarchical encoder. Furthermore, PowerPM leverages a novel self-supervised pre-training framework consisting of masked ETS modeling and dual-view contrastive learning. Our experiments involve two real-world scenario datasets, comprising private and public data. Through pre-training on massive ETS data, PowerPM achieves SOTA performance on diverse downstream tasks within the private dataset. Moreover, when transferred to the public dataset, PowerPM maintains its superiority, showcasing its remarkable generalization ability across various tasks and domains. Further analysis shows the effectiveness of a foundation model in the field of power system. PowerPM is an off-the-shelf model with its code and weights, which significantly alleviates the issue of sample and label efficiency and can directly participate in other power systems."}, {"title": "A Related Work", "content": "Self-supervised Pre-training model. Large-scale model based on self-supervised pre-training has become more and more significant in both industrial and academic domains due to the versatility and impressive performance. It initially developed and matured in the fields of computer vision [14] and natural language processing [8, 11]. Self-supervised pre-training in time series is typically classified into two paradigms: contrastive learning and mask modeling. The objective of contrastive learning is to learn representation by pushing positive pairs closer and negative pairs far from each other in the embedding space [16]. TS2Vec [42] proposes contextual consistency for positive pair selection. Afterward, CoST [37] extracts the trend and seasonal feature representations, and takes advantage of both time and frequency domain contrastive loss to encourage discriminative seasonal representation. And TF-C [47] applies time-frequency consistency for embedding time-based and frequency-based neighbors. In mask modeling, The core idea is to recover the masked content from the unmasked part. To extract the contextual semantic information, PatchTST [21] masks at the series-level.\nSupervised learning model. Since the self-attention mechanism in Transformer [33] showed the great ability to seize the global dependencies between input and output, recently many variants of Transformer have been proposed to tackle power system tasks. LogTrans [19], Informer [48] reduce the complexity by optimizing the vanilla self-attention mechanism. Autoformer [39] leverages auto-correlation mechanism to achieve series-wise representation aggregation. FEDformer [50] incorporates frequency-domain information to enhances prediction performance while reducing complexity to linear levels. Besides, DLinear [43] questions the effectiveness of transformers as it outperforms most Transformer-based SOTAs, which employs a simple linear model. TimesNet [38] has treated time series as a 2D signal and utilized a convolution-based inception net backbone to function as a comprehensive time series analysis model.\nLarge Language models Enhanced Model. Recently, with the development of Large Language Models (LLMs), time series modeling has unveiled new prospects. Many LLMs have demonstrated the capability to capture complex dependencies and understand varied textual data, while producing reasonable generation results, such as llama [32], GPT-3 [11], GPT-4 [1], ChatGLM [9]. Therefore, many reserachers begin to apply LLMs to assist time series modeling. Time-LLM [17] and TEXT [28] employs reprogrammed input time series with text prototype embedding and incorporate textual prompts for time series analysis. GPT4TS [51] and UniTime [20] apply fine-tuning to selected components of LLMs to improve performance in time series analysis tasks. TEMPO [3] incorporates the decomposition of time series and retrieval-based prompt design for non-stationary time series data.\nHowever, despite the existence of numerous methods for self-supervised and supervised of time series, the research on foundation models specifically designed for power systems in time series remains relatively sparse. And LLMs are limited capabilities in power systems scenario, which is lack of enough textual descriptions for domain knowledge."}, {"title": "B Application Scenario", "content": "For practical applications in power systems, PowerPM is integrated into a smart grid framework referred to as the Advanced Power Management System, as shown in Fig. 5. This AI-driven system targets three principal applications: demand-side management, grid stability, and consumer management, and has been developed by Hangzhou Huayun Technology Co., Ltd., China.\nInitially, the system aggregates large amounts of data from various sources, such as electricity meters, ensuring a continuous data currency. This system is easy to use. For events triggering tasks (e.g., forecasting), the model autonomously predicts outcomes using the most recent data. Additionally, for non-trigger tasks, users can select specific time series segments for input into PowerPM, catering to particular requirements. Moreover, we list three basic but important features of the system below:\nOn-demand configuration. This allows users to adapt model parameters based on varying needs. For enhanced precision, a model with more extensive parameters may be chosen, whereas a model with fewer parameters may be preferred to conserve computational resources without significantly compromising performance."}, {"title": "CDataset Description", "content": "We conduct experiments on 5 real-world hierarchical electricity time series datasets, one of which was collected from the State Grid in Zhejiang, China. The other four are collected from CSISO 6, ISONE7, NYISO 8, and PJM 9. Our experiments include four typical time series analysis tasks on these datasets to evaluate the effect of our approach in both in-domain and cross-domain settings: prediction, missing value imputation, anomaly detection, and classification, which include different sampling frequencies (5 minutes, 15 minutes, 1 hour, 1 day). Moreover, it covers a variety of application scenarios in power systems (load forecasting, solar generation forecasting, electricity theft detection and consumer analysis, etc.). Tab. 3 and Tab. 4 summarize the detailed descriptions of these datasets."}, {"title": "D.2 Baselines Implementation", "content": "We compare with 8 state-of-the-art methods: including Large Language Model (LLM) enhanced models: GPT4TS [51], Time-LLM [17], UniTime [20]; pre-train models: PatchTST [21], COST [37], TS2Vec [42]; supervised models: DLinear [43], TimesNet [38]. To make a fair and comprehensive comparison, we reproduce all models with official implementation, and use different output head for different downstream tasks. Due to the large scale of the ETS dataset, we increase the number of training epoch and reduce the learning rate in order to make the parameters of the model fully learned.\nGPT4TS [51] combines the LLM with Transformer, which use frozen pre-trained GPT-2 for general time series analysis. To implement GPT4TS, we utilized their open-source code, available at https://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All. We use the 6 layers of GPT-2, which is proved to have the optimal performance in original paper and the total size of GPT4TS is about 105.15M, and the trainable parameters are 24.04M (GPT-2 is frozen). We set the number of train epochs to 50, the learning rate to 0.0005, and the batch size to 256.\nTime-LLM [17] frezees the LLM as the backbone, and align time series to text with patch reprogram-ming. It also designs Prompt-as-Prefix including dataset context, task instruction and input statistics to enrich the input context to direct the transformation of reprogrammed input. We utilized their open-source code, available at https://github.com/KimMeen/Time-LLM to implement Time-LLM. We set the llama-7b with 32 layers as the backbone, which is the most effective recorded in [17] and the total size of Time-LLM is about 7.28B, and the trainable parameters are 58.55M (llama-7b is frozen). To align the dataset context input to our datasets, we constuct different natural language prompt summarized in App. C for private and public datasets, and we set the number of train epochs to 50, the learning rate to 0.005, and the batch size to 256.\nUniTime [20] leverages LLM to handle time series forecasting across time series domains, which exhibit significant differences in temporal patterns and distribution. The same as dataset context in Time-LLM, UniTime also designs human-crafted instructions to furnish the model with explicit domain identification information. To implement UniTime, we utilized their open-source code, available at https://github.com/liuxu77/UniTime. We implement the backbone LLM with GPT2-small like original paper, and the total size of UniTime is about 108.54M without freeze any parameters. We use the same natural language prompt in Time-LLM as the human-crafted instructions for different datasets, and we set the number of train epochs to 50, the learning rate to 0.0005, the weight decay to 0.0001, and the batch size to 256.\nTS2Vec [42] performs contextual consistency using overlapping subseries and a hierarchical loss function to capture data consistency at the observation and sample levels. We utilize the open-source code available at https://github.com/zhihanyue/ts2vec. Specifically, we set the number of epochs for pre-training to 100, the learning rate to 0.0005, and the batch size to 512. Due to the large scale and complex semantics of the pre-trained ETS data, we adjust the representation dimension to 640, matching the ETS data characteristics. We adopt the default settings provided by the TS2Vec implementation for other settings during pre-training.\nCOST [37] comprises both time domain and frequency domain contrastive losses to learn dis-criminative trend and seasonal representations. We utilize the open-source code available at https://github.com/salesforce/CoST to implement CoST. Specifically, we set the number of epochs for pre-training to 100, the learning rate to 0.0005, representation dimension to 640, and the batch size to 256. We adopt the default settings provided by the CoST implementation for other settings during pre-training.\nPatchTST [21] changes the input sequence as a series of patch windows, focus the subseries-level attention to capture local semantic information while minimizing memory consumption. We utilize the open-source code available at https://github.com/yuqinie98/PatchTST. For hyperparameters of PatchTST, We set the patch len to 32 and stride to 16, the number of epochs for pre-training to 100, the learning rate to 0.0005, and the batch size to 512. We adopt the default settings provided by the PatchTST implementation for other settings during pre-training.\nTimeNet [38] is a CNN based time series model which extends the analysis of temporal variations into the 2D space. It designs TimesBlock with an inception block to extract complex temporal patterns, leading to multiple time series tasks. To implement TimesNet, we utilized their open-source code, available at https://github.com/thuml/Time-Series-Library. Specifically, we set the number of"}, {"title": "D.3 Cluster Method", "content": "We use K-means algorithm to cluster users. Firstly, we get filter out user ETS by labels, and normalize the time series data, represented as an N \u00d7 M matrix, to ensure that differences in scale do not affect the clustering results; Next, we use DTW as the distance metric to cope with time shifts and different rate variations in ETS data and randomly initialize a cluster centers. By calculating the distance from each time series to each cluster center, it is assigned to the nearest cluster center, and the cluster center is recalculated according to the assignment result, and the process is iterated until the cluster center is stable. we attampt 10 times at different initial random cluster numbers, and finally the most frequent occurrence of clustering results is selected as our final clustering number 12."}, {"title": "E Full Results", "content": "Due to the limited length of the text, we summarize all the experiments in the main text into two parts: the main experiment and the analytical experiment. We categorize and index them in Table 6, 7, 8."}, {"title": "F Limitations", "content": "PowerPM is designed for electricity time series modeling, containing over 250M parameters. As a foundation model, although we have provided relatively comprehensive results to verify the model's effectiveness, the model still exsits somelimitation. In fact, there are various kinds of ETS in the power system, which contain not only the electricity consumption data generated by human activities, but also the sequence generated by system operation and sensor detection. In this paper, PowerPM only pre-train on load data. In the future, by increasing model parameters and improving model architecture, we will use more kinds of ETS data for training, so that it can capture more complicated ETS semantic information, understand more complex power system operation rules, and provide more complete help for power system."}, {"title": "G Social Impacts", "content": "This paper presents PowerPM as a foundation model for power systems and has been deployed in Zhejiang power grid to bring considerable benefits. It focus on demand-side management, grid stability and consumer behavior analysis, providing the possibility to understand and analyze electricity time series. There is no potential ethical risk or negative social impact."}]}