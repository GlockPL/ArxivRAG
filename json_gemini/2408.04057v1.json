{"title": "PowerPM: Foundation Model for Power Systems", "authors": ["Shihao Tu", "Yupeng Zhang", "Jing Zhang", "Yang Yang"], "abstract": "The emergence of abundant electricity time series (ETS) data provides ample opportunities for various applications in the power systems, including demand-side management, grid stability, and consumer behavior analysis. Deep learning models have advanced ETS modeling by effectively capturing sequence dependence. Nevertheless, learning a generic representation of ETS data for various applications remains challenging due to the inherently complex hierarchical structure of ETS data. Moreover, ETS data exhibits intricate temporal dependencies and is susceptible to the influence of exogenous variables. Furthermore, different instances exhibit diverse electricity consumption behavior. In this paper, we propose a foundation model PowerPM to model ETS data, providing a large-scale, off-the-shelf model for power systems. PowerPM consists of a temporal encoder and a hierarchical encoder. The temporal encoder captures both temporal dependencies in ETS data, considering exogenous variables. The hierarchical encoder models the correlation between hierarchy. Furthermore, PowerPM leverages a novel self-supervised pre-training framework consisting of masked ETS modeling and dual-view contrastive learning, which enable PowerPM to capture temporal dependency within ETS windows and aware the discrepancy across ETS windows, providing two different perspectives to learn generic representation. Our experiments involve five real-world scenario datasets, comprising private and public data. Through pre-training on massive ETS data, PowerPM achieves SOTA performance on diverse downstream tasks within the private dataset. Impressively, when transferred to the public datasets, PowerPM maintains its superiority, showcasing its remarkable generalization ability across various tasks and domains. Moreover, ablation studies, few-shot experiments provide additional evidence of the effectiveness of our model.", "sections": [{"title": "1 Introduction", "content": "The volume of Electricity Time Series (ETS) data has recently increased rapidly due to the emergence of advanced power systems known as smart grids [10]. This abundance of data has paved the way for diverse applications in power systems, including demand-side management [22], grid stability [2] and consumer behavior analysis [49], etc. Meanwhile, these applications have spawned various tasks, as shown in Fig. 1(d), such as load forecasting [27, 4], clock anomaly detection [46], as well as electricity theft [15] and elderly living alone detection [45]. Recent statistics show that the total\nelectricity consumption in China reached 9.22 trillion kilowatt-hours in 2023\u00b9, which ranks among the top in the world. The substantial economic benefits that accompany this significant electricity usage are also considerable. On the other hand, unreasonable electricity planning can have a detrimental impact on the environment[30]. Thus, given the large amount of data and diverse tasks, there is a pressing need to explore effective modeling methods of ETS data for these tasks, which can lead to enhanced economic efficiency while adhering to low-carbon principles.\nRecently, numerous research studies on pre-training approaches for ETS data have emerged. These approaches adopt the \"pre-training then fine-tuning\" paradigm, which solves the dilemma of limited annotation data, and the pre-trained model can easily adapt to new tasks. Such as PatchTST [21], TS2Vec [42], CoST [37], etc. However, these pre-training methods only utilize small-scale of data with a small number of instances (e.g. users), resulting in poor performance on downstream tasks. As the same time, many researcher begin to apply Large Language Models (LLMs) to assist time series modeling by using pre-trained LLM to encode time series [51] or incorporating additional descriptions related to the time series [17, 20]. Nevertheless, these models have limited ability in the power system scenario due to insufficient pre-training data of power systems and the lack of sufficient domain-specific knowledge. Additionally, none of these models are tailored for the scenario of power systems, neglecting the unique characteristics of ETS data. Therefore, existing power systems related works still maintain a large research gap in modeling ETS data with a foundation model.\nIn our scenario, the ETS data contains numerous instances and naturally exhibits a complex hierarchy [41, 23]. As depicted in Fig. 1(a), a city ETS can be disaggregated into district ETS through the administrative divisions, which are further disaggregated into user ETS in this district. For the complex hierarchy of ETS data, modeling ETS data entails the consideration of several challenges:\n(1) Hierarchical Dependency Modeling. The hierarchy of ETS data facilitates information interaction across different granularities. Fine-grained ETS provides detailed insights into individual electricity usage, while coarse-grained ETS from districts and cities captures broader factors, indicating overall trends. For example, user-level data reflects user-specific behaviors and city-level data encompasses demographics and policy effects [29, 35]. Integrating these levels of granularity to provide both macro and micro perspectives is a complex task that requires sophisticated modeling.\n(2) Temporal dependencies within ETS window. An ETS window refer to a piece of electricity time data over a period of time. The temporal dependencies within an ETS window refer to the correlations and dependencies between observations at different timestamps. As shown in Fig. 1(b), the city-level ETS exhibits daily and weekly dependency. Moreover, the temporal dependencies are often influenced by exogenous variables, such as weather, temperature, and seasonal effects. Integrating these factors into the model is challenging because their impact may interact with the temporal dynamics in complex ways. Accurately capturing the temporal dependencies with the impact of exogenous variables is a key challenge in modeling ETS data.\n(3) Discrepancy across ETS windows. The patterns observed in ETS windows can vary significantly across different instances and different timestamps. For instance, as shown in Fig. 1(c), residential electricity consumption (User A) reaches its peak in the mornings and evenings, used for lighting, appliances, and heating. However, due to residents typically being away for work or education"}, {"title": "2 Methodology", "content": "Overview. As shown in the middle part of Fig. 3: Firstly, the hierarchical graph G is constructed according to the naturally existing hierarchical relationship of ETS data. The ETS windows in G and its corresponding exogenous variables are denoted as $\\{x_i\\}_{i=1}^N$ and $\\{o_i\\}_{i=1}^N$, where N is the number of instances, $x_i \\in \\mathbb{R}^{T_w}$, $o_i \\in \\mathbb{R}^{T_w \\times K}$, each instance ETS window spans $T_w$ time points starting at $T_a$ and ending at $T_b$. Each time point has K kinds of exogenous variables. Our objective is to perform pre-training on an encoder $f(\\cdot)$ to encode each window into a latent representation $z_i \\in \\mathbb{R}^{N \\times d}$, where d indicates the dimension of the latent representation. More specific, PowerPM consists of an exogenous variable enhanced temporal encoder $f_T(\\cdot)$ and a hierarchical encoder $f_h(\\cdot)$, with the process: $z_i = f(x_i, o_i, G) = f_h(f_T(x_i, o_i), G)$. In addition, a novel self-supervised strategy, which combines masked ETS modeling and dual-view contrastive learning, is used for pre-training PowerPM. Next, we will detail the techniques in both model architecture and pre-training strategy."}, {"title": "2.1 Hierarchical Graph Construction.", "content": "The cities, districts, and users in ETS data naturally form a hierarchical relationship, based on which we can construct a hierarchical graph. However, the imbalance in the number of users and districts means there will be multitude of edges between user nodes and district nodes, which significantly increases the complexity of graph modeling. To address this, we employ a clustering strategy to create intermediary nodes, a common approach to implement graph sparsification [13] and a user group policy in the power systems [36, 44, 12]. As depicted in Fig. 3 (c), we use clustering method to categorize users into several clusters, the detailed process can be found in App. D.1. The cities are bidirectionally connected to districts, and these user clusters are also bidirectionally connected to districts, while users are unidirectionally connected to districts. By sparsifying the edges, we enhance the efficiency of graph modeling. Mathematically, we represent the hierarchy as a directed graph $G = (V,E,R)$, where V is the set of nodes, each node corresponds to an instance, $\\varepsilon$ is the set of directed edges, and R is the set of type of edges (e.g. user cluster \u2192 district, district \u2192 user, etc.)."}, {"title": "2.2 Temporal Encoder with Exogenous Variables.", "content": "Patching. In the G, each node's feature $x_i$ is a window of ETS data corresponding to instance i. Due to the semantic sparsity of time series, we patch each window $x_i$ into $N_p$ segments, each of length $P$, resulting in $p_i \\in \\mathbb{R}^{N_b \\times P}$, where $N_p = [\\frac{T_w}{P}] + 1$, and this method proved its validity in many works [21, 17, 20]. Subsequently, a linear projection is applied to each segment to obtain the window representation $h_i \\in \\mathbb{R}^{N_p \\times d}$\nExogenous Variables Encoding. To efficiently interact with exogenous variables, we model these variables using learnable embeddings $E \\in \\mathbb{R}^{(M_1 \\cdots M_K) \\times d}$, where K indicates the number of exogenous variables (e.g. weather type and temperature), $M_k$ represents the number of value types of the k-th exogenous variable (e.g. sunny and rainy in weather type variable). The exogenous variables $o_i^{(k)} \\in \\mathbb{R}^{N_b \\times P}$ corresponding to $p_i$ of the k-th exogenous variable are used to obtain the exogenous variables representations from E, indexing out $e_i^{(k)} \\in \\mathbb{R}^{N_p \\times d}$, as illustrated in Fig. 3 (b). Subsequently, we derive a representation $u_i \\in \\mathbb{R}^{N_p \\times d}$ that considers the window's exogenous variable influence: $u_i = h_i + \\sum_{k=0}^{K-1} e_i^{(k)}$\nTemporal Encoder. To model the complex temporal dependency and interaction with exogenous variables, we use the vanilla Transformer encoder [34] to encode $u_i$, resulting in an augmented temporal representation $z_i' \\in \\mathbb{R}^{N_p \\times d}$."}, {"title": "2.3 Hierarchical Encoder", "content": "To model the complex correlation across different hierarchies, we employ Graph Neural Networks (GNNs). GNNs have gained significant popularity recently for modeling relationships among time series, thereby enhancing temporal representation [7, 26, 40]. In addition, considering that the correlation relationships of different edges are distinct, we adopt R-GCN [25] to integrate information across various hierarchies and instances, as depicted in Fig 3 (a). Specifically, we use R-GCN to update the representation $z_i'$ by considering its neighboring nodes in G, with the final node representation denoted as $z_i \\in \\mathbb{R}^{N_p \\times d}$. Moreover, we use $z_i$ to perform self-supervised pre-training."}, {"title": "2.4 Self-supervised Pre-training", "content": "2.4.1 Masked ETS Modeling\nTo model temporal dependency within an ETS window, we have adopted the widely utilized masked reconstruction strategy. Nevertheless, existing random masking methods may encounter an issue: they reconstruct the missing part based on the known surrounding part [21, 8], without considering the prediction of future parts relying solely on the past part, which not only diminishes the difficulty of the pre-training stage but also lacks consistency across pre-training task and forecasting task.\nTo address this issue, we propose a novel masking approach that combines random and casual masking as shown in Fig. 3 (d) (left). Specifically, we randomly select one of the masking approaches for a given patched window $p_i$, resulting in masked $p_i$. This approach not only retains the benefits of the random masking strategy but also ensures that the model learns to predict future parts based solely on past information, thereby more comprehensively capturing the temporal dependencies within a window. Mathematically, this can be formulated as: masked $p_i = \\begin{cases} Mask_r(p_i) & \\text{if } \\alpha < 0.5 \\\\ Mask_c(p_i) & \\text{otherwise} \\end{cases}$, where $Mask_r$ and $Mask_c$ denote the random and causal masking, respectively, and $\\alpha \\in [0, 1]$ is a uniformly distributed variable. Specifically, after the $x$ is inputted into PowerPM for masked ETS modeling, we will obtain a reconstructed $\\hat{z_i}$. The corresponding reconstruction loss is: $L_{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} \\|\\hat{z_i} - z_i\\|^2$"}, {"title": "2.4.2 Dual-view Contrastive Learning", "content": "The objective of contrastive learning is to learn representations by bringing positive pairs closer and pushing negative pairs farther apart in the latent space [5, 6]. Motivated by this, to make PowerPM aware of the discrepancy across ETS windows, we employ dual-view contrastive learning (DVCL) to discern subtle differences in electricity usage behavior.\nPositive and Negative Sample Pairs. These pairs are determined from two views: one is temporal view, which is based on the time difference between the two windows. Another is the instance view, which depends on whether two windows belong to the same instance. For the same instance, the closer the time difference between two windows, the closer their representations are likely to be. This idea is also presented in [31, 42]. Conversely, windows from different instances or the same instance with a larger time difference are likely to have more distinct representations. Overall, we consider adjacent windows from the same instance as positive samples, while windows from different instances or non-adjacent windows from the same instance are negative samples. As depicted in Fig. 3 (d) (right), for the district node V in G, the original start timestamp about this window is $T_a$. After shifting several time steps $\\delta$ on, we obtain another window $V^+$ starting at $T_a + \\delta$, which serves as a positive sample. Meanwhile, we select windows from other nodes in G, such as city P, starting at $T_a$, as well as windows from the same node V but starting at $T_c$, where $T_c - T_a \\gg \\delta$. These windows serve as instance and temporal negative samples, respectively, and are denoted as $P^-$ and $V^-$.\nMathematically, given an ETS window $x_i$, we obtain a positive sample $x_i^+$ by shifting it by $\\delta$ time steps. The other samples in this batch serve as negative samples, totaling B \u2013 1 negative samples, where B is the batch size during pre-training. The DVCL loss is: $L_{DVCL} = -\\sum_{i=1}^{N} \\log \\frac{exp(sim(f(x_i),f(x_i^+)) / \\tau)}{\\sum_{j \\in I} exp(sim(f(x_i), f(x_j)) / \\tau)}$, where I is the boolean function to select the negative pairs and $sim(\\cdot)$ is cosine similarity function."}, {"title": "3 Experiments", "content": "3.1 Experiment Setup\nPre-training Dataset PowerPM is pre-trained on 987.42GB ETS data, a private dataset collected by the State Grid Corporation of China in Zhejiang province. This pre-training dataset encompasses ETS data from 11 cities, 90 districts, and 1530826 users, with over 1000 days records. The ETS data is collected at a frequency of one data point every 15 minutes. More details are in App. C\nDownstream Dataset To evaluate the performance of PowerPM, we conduct comprehensive experiments on eleven downstream private and public datasets. Seven private datasets are also collected from the State Grid in Zhejiang, China. These datasets have different labels for different tasks. Among them, the solar generation dataset does not have a hierarchical structure due to its particularity. Four public datasets are obtained from CSISO 2, ISONE\u00b3, NYISO 4, and PJM 5, which all exhibit a hierarchical structure. Further details can be found in Appendix C.\nSettings. For the model configurations, the temporal encoder contains a 26-layer Transformer encoder with model dimension 1024, inner dimension (FFN) 2048 and 16 attention heads, and the hierarchical encoder contains 2-layer R-GCN. PowerPM contains about 250M parameters. During pre-training, the 40% segments in each input window are masked in the form of random mask and casual mask, the user cluster numbers is set to 12. See further details in App. D.1\nBaselines. We compare with 8 state-of-the-art methods: including Large Language Model (LLM) enhanced models: GPT4TS [51], Time-LLM [17], UniTime [20]; pre-train models: PatchTST [21], COST [37], TS2Vec [42]; supervised models: DLinear [43], TimesNet [38]. More implementation details are provided in App. D.2.\nEvaluation Metrics. For forecasting and imputation tasks, we use mean squared error (MSE): $\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$ and mean absolute error (MAE): $\\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|$ as the evaluation metric. For classification tasks, we use accuracy as the metric. The metric of the anomaly detection task includes precision, recall, $F_{0.5}$, and $F_1$ scores. The $F_\\beta measure$ is a metric defined as the weighted harmonic mean of precision and recall, with the following equation: $F_\\beta = \\frac{(1 + \\beta^2) \\times precision \\times recall}{\\beta^2 \\times precision + recall}$. We use $F_{0.5}$ for anomaly detection, as precision is more important than recall in power systems scenario [15]."}, {"title": "3.2 Downstream Tasks", "content": "Demand-side Management. Demand-side management aims to optimize and balance the power system by managing and adjusting the electricity demand of end-users. We develop tasks to predict load at different levels (such as cities and users) and tasks to forecast solar generation. With demand-side management, we can better plan and schedule power resources, improve energy efficiency, promote the development of renewable energy, and achieve sustainable energy management.\nGrid Stability. To ensure the stability of the power grid, we have implemented a series of measures, including electricity theft detection, load imputation, and clock anomaly detection, to address the impact of potential appliance failures within the grid and external electricity theft on the quality of power data and grid operations. Internal appliance malfunctions within the grid, such as clock anomalies or the inability to record electricity usage accurately, decrease the accuracy of power data, making it challenging for power dispatch and management. Additionally, external electricity theft\ncan lead to economic losses and pose a threat to the stable operation and reliability of the power grid, potentially causing power outages and other adverse effects.\nConsumer Behavior Analysis. To provide users with more assistance, we have implemented tasks such as detecting elderly living alone, high-power appliance detection, gender classification, age classification, and family structure classification. Additionally, we can provide more flexible power scheduling plans for special groups, optimizing power dispatch. We also aim to understand the energy usage differences among different genders and age groups and provide personalized energy management recommendations and services for different users."}, {"title": "3.3 Main Results", "content": "Overview. As a foundation model for power systems, PowerPM achieves SOTA performance on various tasks when compared to other baseline models, highlighting its ability to generalize effectively across a wide range of tasks. We derive more detailed comparisons of each task in the following paragraphs, where in all tables we mark the best results in bold, the second-best in underlined, and the third-best in * asterisk in each column.\nDemand-side Management. The forecasting results for load and solar generation are presented in Tab. 1 (upper part). The results cover various forecast horizons, including 4 (1 hour), 96 (1 day), 288 (3 days), and 672 (1 week). The choice of these forecast horizons holds physical significance as it aligns with real-world scenarios. The results demonstrate that not only PowerPM achieves near SOTA performance, but also PowerPM freeze surpasses most baseline models. This highlights the superiority of PowerPM in modeling temporal dependencies and capturing the impact of exogenous variables through the use of a temporal encoder and a novel masked ETS modeling approach. Furthermore, PowerPM attains near SOTA performance at different hierarchical levels, particularly at the macro level (district and city), highlighting the importance of modeling the hierarchical correlation within ETS data in PowerPM. Notably, among the baselines, none of the baselines capture the hierarchical correlation of ETS data, resulting in a performance decrease in comparison to PowerPM.\nGrid Stability. To assess the efficacy of PowerPM in grid stability application, we conduct comprehensive experiments encompassing load imputation across various masked ratios (12.5%, 25%, 37.5%, 50%), anomaly detection (including electricity theft and clock anomaly detection), encompassing a total of 18 tasks. The results, detailed in Tab. 1 (middle part), illustrate PowerPM's consistent superiority over all baselines, with the PowerPM freeze variant also surpassing the majority of baselines. Notably, in imputation tasks, PowerPM demonstrates marked superiority over other pre-trained models (such as PatchTST and CoST), underscoring the advantages of hierarchical modeling in ETS data. Furthermore, in anomaly detection tasks, as shown in Tab. 1 (middle part), our model consistently achieves near-optimal results. While GPT4TS records the highest F0.5 score among the baseline methods, attributed to its generation of GPT-2, PowerPMfurther enhances the F0.5 score over GPT4TS. This improvement stems from our temporal encoder's broader receptive field and the hierarchical encoder's capacity to capture hierarchical correlations across all levels, which are both pivotal for modeling ETS data.\nConsumer Behavior analysis. We explore two anomaly detection tasks: elderly living alone and high-power appliance detection, and three classification tasks: gender, age, and family structure classification. The results in Tab. 1 (bottom part) demonstrate PowerPM's SOTA performance, illustrating its capacity for deep semantic insight and contextual awareness. Furthermore, PowerPM freeze sustains high performance, highlighting the model's innate ability to extract and generalize features."}, {"title": "3.4 Model Analysis", "content": "Generalization ability analysis. To further verify the generalization ability of PowerPM on more datasets from other domains, we evaluate PowerPM on 4 public datasets mentioned above. The results in Tab. 2 demonstrate that not only PowerPM outperforms nearly all SOTA methods but alsoPowerPM freeze surpasses most SOTA methods, highlighting the superiority of PowerPM in terms of generalization ability.\nAblation Study. To assess the effectiveness of each component in our model, we conduct several ablation experiments. Specifically, we remove the following components from our model to examine their effects on performance: the hierarchical encoder (PowerPM-H), the dual-view contrastive learning strategy (PowerPM-C), and the exogenous variables encoding module (PowerPM-E). Furthermore, we replace the masked ETS modeling module with vanilla random masking (PowerPM-M). We categorize the 44 tasks into four traditional time series analysis tasks: forecasting, missing value imputation, anomaly detection, and classification. The evaluation metrics are Mean Squared Error (MSE) for forecasting and missing value imputation, F0.5 score for anomaly detection, and accuracy (Acc.) for classification. The performance is averaged to provide a comprehensive assessment.\nThe results of the ablation study are in Fig. 4 (a). The results indicate that PowerPM outperforms other variants of it, providing evidence for the contribution of each component in our model. Among the different variants, PowerPM-H exhibits the most substantial decrease in performance compared to the full PowerPM, emphasizing the significance of interactions occurring between micro- and macro-levels in modeling hierarchical ETS data. The observed performance degradation in PowerPM-M, particularly in forecasting tasks, provides evidence that causal masking can capture more complex temporal dependency. Moreover, the decline in the performance of PowerPM-C, particularly in anomaly detection and classification tasks, suggests that dual-view contrastive learning is effective in capturing subtle discrepancies between instances. Furthermore, PowerPM-E also exists in performance degradation. This emphasizes the effectiveness of the exogenous variables encoding module in capturing the impact of exogenous factors. For the full results of 44 tasks, please refer to App. 7.\nFew-shot Learning. In power systems, collecting abundant ETS data for downstream tasks is a significant investment. To demonstrate the value of the practical application of our work, we conduct a performance comparison between PowerPM and baseline models on downstream tasks, considering the limited availability of ETS data. Specifically, models are fine-tuned on 10%, 30% and 60% of the downstream dataset, respectively. Similar to an ablation study, we present our results grouped by task type. The result can be seen in Fig. 4 (b), the performance of PowerPM exhibits a slight decrease when there is a significant reduction in the proportion of fine-tuning data. This observation serves as evidence of the effectiveness of our novel pre-training strategy, including masked ETS modeling and dual-view contrastive learning. Additionally, it highlights that the PowerPM adeptly captures temporal dependencies and hierarchical correlations present in the ETS data during pre-training, enabling easier adaptation to downstream tasks. More detailed results can be referred to App. 8.\nModel Scale Evaluation. To explore the impact of model size on performance, we design three variants of PowerPM with smaller sizes: PowerPM-Tiny (about 30M), PowerPM-Small (about 70M), PowerPM-Medium (about 120M), PowerPM (about 250M), and pre-train them on the same datasets. For the pre-training details, please refer to App. D.1. After pre-training, we evaluate these variants on all downstream tasks and present the results grouped by task type, similar to the ablation study. As shown in Fig. 4 (c), as the size of the model increases, we observe an overall improvement in the performance of all downstream tasks. Specifically, PowerPM outperforms the other variants in all metrics. In addition, larger models exhibit almost a decrease in standard deviation, indicating a more stable performance. Therefore, the utilization of a larger model with higher capacity and vast amounts of ETS data enables better generalization across a wide range of downstream tasks."}, {"title": "4 Conclusion", "content": "This paper introduces the PowerPM, a foundational model designed to model ETS data within power systems. PowerPM consists of a temporal encoder and a hierarchical encoder. Furthermore, PowerPM leverages a novel self-supervised pre-training framework consisting of masked ETS modeling and dual-view contrastive learning. Our experiments involve two real-world scenario datasets, comprising private and public data. Through pre-training on massive ETS data, PowerPM achieves SOTA performance on diverse downstream tasks within the private dataset. Moreover, when transferred to the public dataset, PowerPM maintains its superiority, showcasing its remarkable generalization ability across various tasks and domains. Further analysis shows the effectiveness of a foundation model in the field of power system. PowerPM is an off-the-shelf model with its code and weights, which significantly alleviates the issue of sample and label efficiency and can directly participate in other power systems."}, {"title": "A Related Work", "content": "Self-supervised Pre-training model. Large-scale model based on self-supervised pre-training has become more and more significant in both industrial and academic domains due to the versatility and impressive performance. It initially developed and matured in the fields of computer vision [14] and natural language processing [8, 11]. Self-supervised pre-training in time series is typically classified into two paradigms: contrastive learning and mask modeling. The objective of contrastive learning is to learn representation by pushing positive pairs closer and negative pairs far from each other in the embedding space [16]. TS2Vec [42] proposes contextual consistency for positive pair selection. Afterward, CoST [37] extracts the trend and seasonal feature representations, and takes advantage of both time and frequency domain contrastive loss to encourage discriminative seasonal representation. And TF-C [47] applies time-frequency consistency for embedding time-based and frequency-based neighbors. In mask modeling, The core idea is to recover the masked content from the unmasked part. To extract the contextual semantic information, PatchTST [21] masks at the series-level.\nSupervised learning model. Since the self-attention mechanism in Transformer [33] showed the great ability to seize the global dependencies between input and output, recently many variants of Transformer have been proposed to tackle power system tasks. LogTrans [19], Informer [48] reduce the complexity by optimizing the vanilla self-attention mechanism. Autoformer [39] leverages auto-correlation mechanism to achieve series-wise representation aggregation. FEDformer [50] incorporates frequency-domain information to enhances prediction performance while reducing complexity to linear levels. Besides, DLinear [43] questions the effectiveness of transformers as it outperforms most Transformer-based SOTAs, which employs a simple linear model. TimesNet [38] has treated time series as a 2D signal and utilized a convolution-based inception net backbone to function as a comprehensive time series analysis model.\nLarge Language models Enhanced Model. Recently, with the development of Large Language Models (LLMs), time series modeling has unveiled new prospects. Many LLMs have demonstrated the capability to capture complex dependencies and understand varied textual data, while producing reasonable generation results, such as llama [32], GPT-3 [11], GPT-4 [1], ChatGLM [9]. Therefore, many reserachers begin to apply LLMs to assist time series modeling. Time-LLM [17] and TEXT [28] employs reprogrammed input time series with text prototype embedding and incorporate textual prompts for time series analysis. GPT4TS [51] and UniTime [20] apply fine-tuning to selected components of LLMs to improve performance in time series analysis tasks. TEMPO [3] incorporates the decomposition of time series and retrieval-based prompt design for non-stationary time series data.\nHowever, despite the existence of numerous methods for self-supervised and supervised of time series, the research on foundation models specifically designed for power systems in time series remains relatively sparse. And LLMs are limited capabilities in power systems scenario, which is lack of enough textual descriptions for domain knowledge."}, {"title": "B Application Scenario", "content": "For practical applications in power systems, PowerPM is integrated into a smart grid framework referred to as the Advanced Power Management System, as shown in Fig. 5. This AI-driven system targets three principal applications: demand-side management, grid stability, and consumer management, and has been developed by Hangzhou Huayun Technology Co., Ltd., China.\nInitially, the system aggregates large amounts of data from various sources, such as electricity meters, ensuring a continuous data currency. This system is easy to use. For events triggering tasks (e.g., forecasting), the model autonomously predicts outcomes using the most recent data. Additionally, for non-trigger tasks, users can select specific time series segments for input into PowerPM, catering to particular requirements. Moreover, we list three basic but important features of the system below:\nOn-demand configuration. This allows users to adapt model parameters based on varying needs. For enhanced precision, a model with more extensive parameters may be chosen, whereas a model with fewer parameters may be preferred to conserve computational resources without significantly compromising performance.\nContinuous update. This is critical as electricity demand fluctuates due to various factors, including policy changes. Should a model's training data not incorporate the latest ETS information, its predictive accuracy may wane, affecting its practical applicability. Consequently, the system features regular model updates to accommodate new data and evolving requirements.\nTask generalization. This enables system to fine-tune PowerPM with a minimal dataset tailored to new tasks, thereby lowering the training overhead.\nThe upper section of Fig.5 depicts the overall layout of the system. The top toolbar contains various functional modules, such as demand side management, user management, and model management. In the middle, there is an overview of Zhejiang Province, where each city can be clicked to view detailed electricity consumption information. The left and right sections display the corresponding results of the scheduled electricity consumption. In the bottom of Fig.5, the scheduled electricity completion status in 2023 is clearly illustrated. The horizontal axis represents different levels of scheduled electricity consumption projects, while the vertical axis represents electricity consumption. Additionally, the yellow bar chart () indicates the electricity loss from actual orderly reductions in electricity use, whereas the blue bar chart () represents the electricity loss from scheduled orderly reductions in electricity use. As observed from the figure, the energy loss resulting from actual reductions consistently exceeds that of the scheduled orderly reductions, demonstrating the effectiveness of PowerPM."}, {"title": "C Dataset Description", "content": "We conduct experiments on 5 real-world hierarchical electricity time series datasets, one of which was collected from the State Grid in Zhejiang, China. The other four are collected from CSISO 6, ISONE7, NYISO 8, and PJM 9. Our experiments include four typical time series analysis tasks on these datasets to evaluate the effect of our approach in both in-domain and cross-domain settings: prediction, missing value imputation, anomaly detection, and classification, which include different sampling frequencies (5 minutes, 15 minutes, 1 hour, 1 day). Moreover, it covers a variety of application scenarios in power systems (load forecasting, solar generation forecasting, electricity theft detection and consumer analysis, etc.)."}, {"title": "C.1 Private Dataset", "content": "Private dataset is collected from the load data of the State Grid Corporation of China in Zhejiang province, covering the period from 2016 to 2022. Following data preprocessing, we extract a subset of the data totaling 1.24TB. This subset encompasses 11 cities, approximately 90 districts, and around 1.5 million users. In order to effectively support our research objectives, we divide the dataset into 9 distinct sub-datasets. One biggest of these sub-datasets is served as the pre-training dataset, totaling 987.42GB, while the remaining 7 sub-datasets are utilized as downstream datasets for downstream tasks. These downstream datasets are partitioned into train, validation, and test sets according to a 6:2:2 ratio, ensuring that the training set contain data from the earlier time period. Further details are provided below:\nPre-training Dataset. The pre-training dataset is derived from a subset of the private dataset, encompassing the period from June 2016 to June 2020. It consists of unlabeled data recorded at a frequency of one data point every 15 minutes. The dataset is structured hierarchically, including information at the user, district, and city levels.\nLoad Forecasting and Missing Value Imputation Dataset. This dataset is extracted from a portion of the private dataset spanning from July 2020 to June 2022. The dataset includes hierarchical information at the user, district, and city levels, with data points recorded every 15 minutes. For the missing value imputation task, the dataset is structured to output 672 data points. As for the forecasting task, there are four different prediction horizons: one hour (4 data points), one day (96 data points), three days (288 data points), and seven days (672 data points).\nSolar Generation Forecasting Dataset. The dataset is collected from 192 distributed photovoltaic power stations spanning from July 2021 to June 2022. The dataset has not a hierarchical structure,"}, {"title": "C.2 Public Datasets", "content": "Four public datasets as cross-domain datasets are selected to validate the generalization ability of our model. These four datasets are named CSISO", "relationships": "state-area, region-state, state-city.\nCAISO. It is sampled from California, including 34 areas loads and an aggregated load for the state, recorded every hour from April 25, 2023, to April 23, 2024."}]}