{"title": "Transformers and Their Roles as Time Series Foundation Models", "authors": ["Dennis Wu", "Yihan He", "Yuan Cao", "Jianqing Fan", "Han Liu"], "abstract": "We give a comprehensive analysis of transformers as time series foundation models, focusing on their approximation and generalization capabilities. First, we demonstrate that there exist transformers that fit an autoregressive model on input univariate time series via gradient descent. We then analyze MOIRAI [1], a multivariate time series foundation model capable of handling an arbitrary number of covariates. We prove that it is capable of automatically fitting autoregressive models with an arbitrary number of covariates, offering insights into its design and empirical success. For generalization, we establish bounds for pretraining when the data satisfies Dobrushin's condition. Experiments support our theoretical findings, highlighting the efficacy of transformers as time series foundation models.", "sections": [{"title": "Introduction", "content": "The advancement of foundation models is reshaping the field of time series forecasting. Recent studies demonstrate the empirical success of transformer-based time series foundation models [1, 2, 3, 4]. However, a theoretical understanding of how these models succeed is yet missing. In this paper, we aim to provide a comprehensive analysis of time series foundation models, with a focus on transformer-based models. We are interested in how these models achieve the one-model-for-all-datasets paradigm in time series forecasting. Specifically, our results cover both uni-variate [2, 4, 5], and multi-variate time series foundation models [1]1.\nOur main discovery is twofold. First, to address universality, we prove that there exists a transformer that fits an autoregressive model [6, 7] on any given uni-variate time series. Furthermore, we show that the special design of MOIRAI allows transformers to further handle arbitrary number of covariates, making it process any dimension of time series in a principled way. Second, to address learnability, we establish a generalization bound for pretraining when the data satisfies Dobrushin's condition [8, 9]. We refer to these two aspects as approximation and generalization, respectively, throughout the rest of this paper, as they form the theoretical foundation for the success of these models.\nOur approximation result is inspired by recent studies on in-context learning [10, 11, 12, 13, 14, 15]. In- context learning refers to the ability of large foundation models to make accurate predictions for unseen tasks by observing training examples without parameter updates [16, 17]. This capability explains how time series foundation models achieve their universality. By treating the input time series as in-context examples, we show that transformers are able to implement gradient descent to estimate the parameters of the autoregressive model that best explains the given input.\nThe contribution of this paper is threefold:\n\u2022 From an algorithmic approximation perspective, we prove the existence of a transformer capable of fit- ting an autoregressive (AR) model on any given uni-variate time series via gradient descent. Extending this to the multi-variate setting, we show that a MOIRAI transformer can automatically adjust the"}, {"title": "Problem Setup", "content": "This section describes our problem setup. We introduce the architecture of transformer-based time series foundation models and how we construct our datasets."}, {"title": "Transformers", "content": "We consider a sequence of N input vectors {hi}1 CRD, where H := [h1,\u2026,hn] \u2208 RD\u00d7N. Given any H\u2208RD\u00d7N, we define the attention layer as follows.\nDefinition 2.1 (Attention layer). A self-attention layer with M heads is denoted as Attno (\u00b7) with param- eters 00 = {(Vm), (Qm), (Km)}m\u2208[M] CRD\u00d7D. The self-attention layer processes any given input sequence HERDXN as\nAttn00 (H) := H + \\frac{1}{N} \\sum_{m=1}^{M} V_{m} H\\sigma \\left(Q_{m} H\\right)^{T}\\left(K_{m} H\\right),\nwhere \u03c3 := t \u2192 ReLU(t)/N."}, {"title": "Any-variate Attention", "content": "Next, we introduce the any-variate attention, where [1] uses it to replace the standard attention in transformers. The any-variate attention introduces two learnable variables: Attention Bias U1, U2 \u2208 R, for disambiguation between variates.\nDefinition 2.2 (Any-variate Attention.). An any-variate attention layer with M heads is denoted as Attne\u2081 (\u00b7) with parameters 01 = {(Vm), (Qm), (Km), (um), (um)}m\u2208[M]. With any input H \u2208 RD\u00d7N, we have\nAttne\u2081 (H) := H + \\frac{1}{N} \\sum_{m=1}^{M} V_{m} H\\sigma \\left(\\left(Q_{m} H\\right)^{T}\\left(K_{m} H\\right)+u_{m} * U+u_{m} * \\bar{U}\\right),\nwhere \u03c3 := t \u2192 ReLU(t)/N, U \u2208 RN\u00d7N is a block diagonal matrix with block size T \u2208 N\u207a, such that each block consists of 1s, \\bar{U} = I \u2013 U, and * denotes a constant multiply to all entries of a matrix.\nRemark 2.3. In [1], the attention score is calculated with the RoPE embedding [18]:\n\\sigma \\left(\\left(Q_{m} H\\right)^{T} R\\left(K_{m} H\\right)+u_{m} *U+u_{m} * \\bar{U}\\right).\nWe omit the notation of rotary matrix R as it is not learnable and is invertible and thus merged into Q, K in our analysis."}, {"title": "MLP Layer", "content": "We denote an MLP layer with hidden state dimension D' as MLP\u0665(\u00b7) with parameters 02 = (W1, W2) \u2208 RD'\u00d7D \u00d7 RD\u00d7D'. The MLP layer processes any given input sequence H \u2208 RDXN as\nMLP02 (H) := H + W2\u03c3(W1H).\nFinally, we define a transformer with L\u2265 1 layers, each consisting of any-variate attention and an MLP layer."}, {"title": "MOIRAI Transformer", "content": "We define the L-layer MOIRAI transformer [1], TFo(\u00b7), as\nTF0(H) = MLP (Attno (MLP (Attno (H)))) .\nNote that this transformer is equipped with any-variate attention instead of the standard attention. For transformers with standard attention, we denote it as TF(.).\nWe use to denote the vectorization of all parameters in a transformer and super-index l to denote the parameter of the l-th layer. Thus, the parameter of a transformer is defined by\n0 = {{(Qm, Km Vm, um, um } me[M], WI, W\u2082)}e\u2208[L]\nWe denote the \"attention-only\u201d transformers with W(e), W(e) = 0, as TF(\u00b7) for shorthand. We define the following norm of a MOIRAI transformer as\n||\u03b8||_{op} := max \\left\\{\\max_{\\ell \\in [L], m \\in [M^{\\ell}]} ||Q_{m}^{\\ell}||_{2}, ||K_{m}^{\\ell}||_{2}, \\sum_{m=1}^{M^{\\ell}}||V_{m}^{\\ell}||_{1,2}+\\sum_{\\ell=1}^{L} ||W_{1}^{\\ell}||_{2}+ ||W_{2}^{\\ell}||_{2}\\right\\},\nwhere Me is the number of heads of the l-th Attention layer."}, {"title": "Data Generation", "content": "Here, we first consider the case where we aim to find a multi-layered transformer that performs least squares regression via In-context learning (ICL). Specifically, we assume our data is generated from an autoregressive process ARd(q) as follows, where q, d denotes the steps of lag and number of covariates, respectively. Consider a sequence of data x \u2208 Rd\u00d7T := (x1,...,x), where xt = (x+,\u2026\u2026,xq) \u2208 Rd. Assuming our target (variate of interest) is in dimension 1, we assume the ARd (q) process generates x as follows:\nx_{t}^{1}=\\sum_{i=1}^{q} \\sum_{j=1}^{d} w_{i}^{j} x_{t-i}^{j}+\\epsilon_{t}, \\quad x_{t}^{1}=\\sum_{i=1}^{q} \\sum_{j=1}^{d}\\left(\\mathbf{w}^{j}\\right)^{\\prime} x_{t-q: t-1}+\\epsilon_{t},\\quad(2.1)"}, {"title": "Approximation", "content": "We study the algorithmic approximation perspective of transformer-based time series foundation models. We first investigate transformers as uni-variate time series foundation models as a warm-up. Next, we will move on to MOIRAI [1] and analyze how its unique design and pre-processing methods enable its universality."}, {"title": "Warm Up: Autoregressive Regression", "content": "We start our analysis with a warm-up example on the AR\u2081 (q) model. We show that standard transformers are capable of performing gradient descent via in-context learning on autoregressive data. Here, we consider an input sequence with the following form\nH :=\\left[\\begin{array}{ccccc} x_{1} & x_{2} & \\cdots & x_{T} & 0_{d^{\\prime}} \\\\ p_{1} & p_{2} & \\cdots & p_{T} & p_{T+1} \\\\ e_{1} & e_{2} & \\cdots & e_{T} & 1 \\end{array}\\right] \\in \\mathbb{R}^{D \\times(T+1)},(3.1)\n\n\n\n\n\\mathbf{p}_{i}:=\\left[\\begin{array}{c} 0_{d^{\\prime}-q} \\\\ e_{i} \\\\ 1[1\\{i<T\\}] \\end{array}\\right] \\in \\mathbb{R}^{d^{\\prime}+T+3}(3.2)\nwhere ei is an one-hot vector with 1 at the i-th entry, and d' +T+3 = D. Here, our goal is to predict x.\nRemark 3.1. Most in-context learning studies [10, 12, 19] make an assumption on the input data, where they assume it is formatted with features and labels in the same column, i.e.,\n\\left[\\begin{array}{cccc} x_{1} & x_{2} & \\cdots & x_{N} \\\\ y_{1} & y_{2} & \\cdots & y_{N} \\\\ p_{1} & p_{2} & \\cdots & p_{N} \\end{array}\\right].(3.3)\nIn contrast, we adopt a natural approach that leverages the raw structure of the data, particularly for the ARd(q) process. In this setting, each time step's label also serves as a feature for future steps. Further, the unknown value of q complicates the task of achieving such a format in Equation equation 3.3.\nOur next lemma shows that transformers are indeed capable of reformatting H into the form of Equation 3.3. Notably, the following lemma relaxes the assumption in Remark 3.1 of previous studies as well.\nLemma 3.2. Given a sequence of token H in the form of Equation 3.1, there exists a one-layer, qmax head attention layer, such that for any q \u2264 qmax, the columns of Attn (H) has the following form:\nAttn_{\\theta_{0}}(H):=\\left[\\begin{array}{c} x_{i} \\\\ x_{i-1} \\\\ \\vdots \\\\ x_{i-q} \\\\ \\mathbf{p} \\end{array}\\right], \\quad \\mathbf{p}:=\\left[\\begin{array}{c} 0_{d^{\\prime}-q} \\\\ e_{i} \\\\ 1[1\\{i<T\\}] \\end{array}\\right],(3.4)"}, {"title": "Approximation Error of MOIRAI Transformer", "content": "In this subsection, we extend our results to the multivariate autoregressive process (d > 1) and the encod- ing method of MOIRAI. Note that in the multi-variate case, we only focus on MOIRAI as it is the only"}, {"title": "Any-Variate Encoding", "content": "[1] propose to flatten a d-dimensional time series, x \u2208 Rd\u00d7T, into a 1-dimensional sequence, i.e., x' \u2208 R1\u00d7Td. This operation transforms time series with arbitrary number of covariates (d), into a long sequence with fixed dimension, enabling consistent input dimension for transformers. Following the flattening operation, [1] also proposes to add two types of indices into the input sequence: the time and variate ID. We term the above operations as the any-variate encoding, which transforms a multivariate sequence x \u2208 Rd\u00d7T, as follows:\n\\left[\\begin{array}{cccc} x_{1}^{1} & x_{2}^{1} & \\cdots & x_{T}^{1} \\\\ x_{1}^{2} & x_{2}^{2} & \\cdots & x_{T}^{2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{1}^{d} & x_{2}^{d} & \\cdots & x_{T}^{d} \\end{array}\\right] \\rightarrow \\left[\\begin{array}{cccc} x_{1}^{1} & x_{2}^{1} & \\cdots & x_{T}^{1} \\\\ p_{1} & p_{T} & \\cdots & p_{1} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ e_{1} & e_{1} & \\cdots & e_{d} \\end{array}\\right],(3.7)\n\n\n\nwhere ei is the variate index, a one-hot vector with i-th entry being 1, and pi is the time index, which is defined the same as Equation equation 3.1. This is without loss of generality because the discrete-time and variate ID used in [1] can be easily transformed into a high-dimensional vector with the embedding layer. Note that only the target variate has length T, we highlight x as it is our prediction target and will be masked as 0.\nNow we define the history matrix A\u00bf(q) \u2208 Rq+1\u00d7T for the i-th covariates (x\u2081,\u2026\u2026,x), with order q, such that\nAi(q)\u03bc,\u03bd := x\u00b2-\u03bc+1, for \u03bc\u2208 [d], \u03bd\u2208 [q].\nwhere in the j-th column of Ai(q), it contains historical values of x with lag q > 0.\nLemma 3.5. Fix qmax, D\u2208 N\u207a. Given any T > 0,d' > q > 0, d > 0 such that T > q, qmax \u2265 q. For any input matrix H in the form of any-variate encoding in Equation 3.7, such that \u0397 \u2208 RD\u00d7dT. There exists a one layer, qmax head any-variate attention that performs the following operation.\n\\left[\\begin{array}{cccc} x_{1}^{1} & x_{1}^{2} & \\cdots & x_{1}^{d} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_{T}^{1} & x_{T}^{2} & \\cdots & x_{T}^{d} \\\\ p_{1} & p_{T} & \\cdots & p_{1} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ e_{1} & e_{1} & \\cdots & e_{d} \\\\ A_{1}(q) & A_{2}(q) & \\cdots & A_{d}(q) \\\\ 0_{d^{\\prime \\prime}} & 0_{d^{\\prime \\prime}} & \\cdots & 0_{d^{\\prime \\prime}} \\end{array}\\right](3.7)\n\n\n\nwhere d'\u201d = d' qmax.\nThe proof is in Appendix D.2. Intuitively, the above operation performs the same operation in Lemma 3.2 but in a variate-wise fashion. Lemma 3.5 shows that any-variate attention is capable of organizing the history of each variate efficiently. To again achieve the format in Equation equation 3.3, one has to stack all Ai(q) in the same columns, which can be easily done by a single layer of attention via Lemma 3.2 and [10, Proposition A.5] (details in Appendix D.2). This lemma serves as a foundation for MOIRAI to handle multi-variate time series with in-context learning which we present as the theorem below.\nRemark 3.6. Comparing to Lemma 3.2, Lemma 3.5 is specifically for any-variate attention in our construc- tion, where we demonstrate that several special mechanisms in any-variate attention enables variate-wise operations efficiently.\nRemark 3.7. Lemma 3.2 and Lemma 3.5 can be generalized to Softmax and linear attention by considering perturbations, making them applicable to a wide range of transformers.\nTheorem 3.8 (Any-variate Autoregressive Regression via MOIRAI). Assume Assumption 3.3 holds. For any 0 \u2264 a \u2264 \u03b2 with \u03ba := 2, Bw > 0, and \u20ac < BxBw/2. there exists an (L1 + L2)-layer of MOIRAI"}, {"title": "Generalization", "content": "In this section, we investigate the generalization bound of pretraining transformer-based time series founda- tion models. This section will focus on learning MOIRAI on multi-variate time series, one can easily adapt our proofs into learning uni-variate time series with standard transformers.\nLet be a meta distribution, and each distribution drawn from it P(T) ~ \u3160, satisfies Dobrushin's condition [8] (which we will introduce shortly). For pretraining data, we first sample n distributions P(T) i.i.d. from \u03c0, and for each distribution, we sample a time series (X1j,\u2026\u2026,XTj), for j \u2208 [n], and each of them contains no more than d covariates and with lag step no more than q.\nFor each time series, we encode it with any-variate encoding into an input matrix denoted as H\u2208RD\u00d7N, 2 We define each pretraining sample as zj := (Hj, yj), where yj = x+j. We consider the squared loss between model prediction and the label, i.e.\n\\ell(\\Theta, z_{t}):= \\frac{1}{2} \\left[y_{t}-\\text { Clip}_{B_{\\pounds}}\\left(\\text {read}_{i}\\left(T F_{\\Theta}^{i}(H)\\right)\\right)\\right]^{2},\nwhere Clipp (t) := max{min{t, B\u2081}, -B\u00a3}, and TF is the MOIRAI transformer defined in Definition 2.5 with Clip() applied after each layer. The pretraining loss and test loss is defined as the following:\n\\hat{L}(\\Theta):=\\frac{1}{n T} \\sum_{t=1}^{T} \\sum_{j=1}^{n} \\ell(\\Theta, z_{j t}), \\quad L(\\Theta):=E_{z, p(\\mathcal{T})}[\\ell(\\Theta, z)] .(4.1)"}, {"title": "Weakly-Dependent Time Series", "content": "In this scenario, we consider the training data x to be drawn from a distribution P satisfying Dobrushin's condition. Under this condition, we are able to present several generalization bounds on pretraining."}, {"title": "Generalization Bounds of MOIRA\u0399", "content": "Theorem 4.5 (Pretraining Generalization Bound). Let OL,M,D',B be the parameter space defined in Equa- tion 4.2. Assume alog (P(T)) < 1/2. Then with probability at least 1 \u2013 \u03b5, ERM satisfies the following:\n\\hat{L}(\\Theta) \\leq \\inf _{\\Theta \\in \\mathcal{O}_{L, M, D^{\\prime}, B}} L(\\Theta)+\\mathcal{O}\\left(\\frac{B^{2}}{\\sqrt{n}\\left(1-\\alpha(\\mathcal{P}(\\mathcal{T})) \\sqrt{\\mathcal{V}}\\right.}\\left[\\frac{L(M D^{2}+D D^{\\prime})+\\log (1 / \\epsilon)}{n}\\right]\\right)."}, {"title": "Example: Stationary AR(1)", "content": "Here we provide an example of the application of Corollary 4.6 on AR(1) process with the following form\nXt+1 = <w, xt) + \u20act, \u03b5 ~ N(0, \u03c3\u00b2),\nwhere xt \u2208 Rd, w \u2208 Rd, \u0454 \u2208 R and Yt+1 = x+1.\nTo satisfy the condition of a(P) <, we assume the following holds\nB_{x}^{2}<\\ln \\frac{1}{\\frac{1}{2}}+\\left(\\sigma^{2}\\right), \\quad||w||<1.(4.5)\nThe first condition comes from the fact that we require the pair-wise potential of this time series to be less than 1/2 (For more details, see Appendix D.5). The second condition comes from the requirement of it being stationary.\nProposition 4.8 (Generalization Bound for Any-Variate Transformer on AR(1)). Considering an AR(1) process with Dobrushin's coefficient bounded by 1/2. With probability at least \u03b4(1 \u2212 \u03b5), ERM\u0398 satisfies the following:\n\\hat{L}(\\Theta)=\\mathcal{O}\\left(\\frac{\\sigma^{2}}{\\sqrt{n}}+\\sqrt{\\frac{\\epsilon}{\\delta}} \\exp \\left(\\frac{\\epsilon}{\\delta}\\right)+\\frac{\\sigma^{2}}{\\sqrt{1-\\alpha(\\operatorname{AR}(1))}} \\sqrt{\\frac{L(M D^{2}+D D^{\\prime})+\\log (1 / \\epsilon)}{n}}\\right),\nwhere ( = O(log(2 + max{B, R, Bx, d}).\nIf we further optimize the bound by viewing the hyperparameters as constants, the test error obeys O(e-L +\n\u221a) with high probability whenever \u03c3\u03b5 is small."}, {"title": "Experiments", "content": "To verify our analysis, we first train transformers on synthetic datasets generated from AR process with different parameters. The goal of this experiment is to verify the existence of a transformer that performs least squares regression on input time series with bounded lag window and number of covariates. Next we study whether a pretrained transformer is capable of generalize such an ability to unseen values of d, q. More empirical results are in Appendix E.5."}, {"title": "Datasets", "content": "Synthetic Data Generation. We generate the AR synthetic data similar to Equation equation 2.1 but use normalization to stabilize the values. Consider a sequence of data x \u2208 Rd\u00d7T := (x1,...,XT), where xt = (x+,\u2026\u2026,x) \u2208 Rd. Assuming our target (variate of interest) is in dimension 1, we generate our data as follows:\nx_{t}^{1}=\\frac{1}{q d} \\sum_{i=1}^{q} \\sum_{j=1}^{d} \\alpha_{i}^{j} x_{t-i}^{j}+\\epsilon_{t},(5.1)\nwhere et ~ \u039d(0,\u03c3\u00b2), a ~ N(0,1) \u2208 R. We have \u03c3\u00b2 ~ unif(0.1,1). After recursively generating the time series, we remove its first 50 time steps as burn out. Each AR time series has the number of covariates between"}, {"title": "Generalization to Unseen d, q", "content": "Here we are interested in whether a pretrained transformer is capable of generalizing to unseen values of d and q. Therefore, we train transformers (MOIRAI) on synthetic data generated with AR with d \u2208 {4,5}, and q \u2208 {4,5}. In our construction, pretrained transformer is compatible with lower order and dimension AR data. We evaluate the trained model on data with unseen values of d. We select d = 2, d = 10, to represent the scenario when the number of covariates is lower and higher than pretraining data."}, {"title": "Conclusion", "content": "In this paper, we investigate the theoretical foundations of transformers as time series foundation models. First, we show that there exists a multi-layer transformer capable of performing least squares regression on any input uni-variate time series. Next, when considering MOIRAI, we demonstrate the existence of a multi-layer MOIRAI that adapts to the dimensionality d of the input (i.e., the number of covariates) and"}]}