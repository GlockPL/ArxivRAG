{"title": "Occam's model: Selecting simpler representations for better transferability estimation", "authors": ["Prabhant Singh", "Sibylle Hess", "Joaquin Vanschoren"], "abstract": "Fine-tuning models that have been pre-trained on large datasets has become a cornerstone of modern machine learning workflows. With the widespread availability of online model repositories, such as Hugging Face, it is now easier than ever to fine-tune pre-trained models for specific tasks. This raises a critical question: which pre-trained model is most suitable for a given task? This problem is called transferability estimation. In this work, we introduce two novel and effective metrics for estimating the transferability of pre-trained models. Our approach is grounded in viewing transferability as a measure of how easily a pre-trained model's representations can be trained to separate target classes, providing a unique perspective on transferability estimation. We rigorously evaluate the proposed metrics against state-of-the-art alternatives across diverse problem settings, demonstrating their robustness and practical utility. Additionally, we present theoretical insights that explain our metrics' efficacy and adaptability to various scenarios. We experimentally show that our metrics increase Kendall's Tau by up to 32% compared to the state-of-the-art baselines.", "sections": [{"title": "1. Introduction", "content": "Using models pre-trained on large datasets like ImageNet (Deng et al., 2009) has become a standard practice in real-world deep-learning scenarios. For example, the top five models on HuggingFace have been downloaded more than 200M times. HuggingFace hosts more than 15K models for image classification. The performance and efficiency gain from using models pre-trained on large datasets like ImageNet 21k (Deng et al., 2009) and LIAON (Schuhmann et al., 2022) is enormous. However, these performance gains can vary considerably depending on model architecture, weights, and the dataset it was pre-trained on (source dataset). This leads to the pre-trained model selection problem. Although the model selection task has strong roots in AutoML, applying classic model selection paradigms is computationally too expensive in this scenario. Fine-tuning each model on the target data for a search strategy like Bayesian optimization is not feasible since the fine-tuning step is too expensive. This raises the question: \"How can we find a high-performing pre-trained model for a given target task without fine-tuning our models and without access to the source dataset.\"\nThis question is answered by transferability estimation methods. The idea behind transferability estimation is to assign a score to each pre-trained model of a given set for a target task, such that one can select the top-performing model for the given task. In the last decade, there have been multiple works, addressing this problem from various perspectives. For example, TransRate (Huang et al., 2022) treats the problem from an information theory point of view, and ETran connects the problem of transferability estimation to energy-based models. There have been numerous methods that treat this problem from multiple perspectives like linearization, Bayesian modeling, matrix analysis, etc. However, we found that these approaches fall short in many practical scenarios.\nIn this work, we introduce a new transferability estimation"}, {"title": "2. Related work", "content": "The idea behind transferability estimation is simple: to estimate which model from a zoo would perform best after fine-tuning the model. Transferability estimation as a field is fairly new, the H-Score (Bao et al., 2019) and NCE (Tran et al., 2019) can be considered as early works on this topic, introducing the evaluation of transferability, and the assignment of models corresponding to an estimate of their transferability, for a given target task.\nThere are two widely accepted problem scenarios for transferability estimation: source-dependent transferability estimation (where one has access to the source and target dataset) and source-independent transferability estimation (where one does not have access to the source dataset)."}, {"title": "2.1. Source Dependent Transferability Estimation (SDTE)", "content": "The SDTE scenario assumes access to the source data sets where the models have been pre-trained. Apart from the fact that this assumption is often not met, a drawback of common SDTE metrics, they use distribution matching methods like optimal transport (Tan et al., 2021), which are typically very expensive to compute. In addition, SDTE metrics are usually not reliable when the discrepancy between the source and target dataset is very high, for example, when comparing entire the ImageNet21K (Deng et al., 2009) to Cars (Krause et al., 2013) or Plants (G. & J., 2019) dataset."}, {"title": "2.2. Source Independent Transferability Estimation (SITE)", "content": "The Source Independent Transferability Estimation (SITE) assumes access to the source model but not the source training data. This is a more realistic transferability estimation as we might not always have access to the source dataset, nor have the capacity to store the typically very large source datasets like ImageNet (Deng et al., 2009) or LAION (Schuhmann et al., 2022) in our local setup. SITE methods typically rely on evaluating the feature representation of the source model on the target dataset and its relationship with target labels.\nThere are several transferability metrics inspired by various viewpoints. LogME (You et al., 2021) formalizes the transferability estimation as the maximum label marginalized likelihood and adopts a directed graphical model to solve it. SFDA (Shao et al., 2022) proposes a self-challenging mechanism, it first maps the features and then calculates the sum of log-likelihood as the metric. ETran (Gholami et al., 2023) and PED (Li et al., 2023) treat the problem of SITE with an energy function, ETran uses energy-based models to detect whether a target dataset is in-distribution or out of distribution for a given pre-trained model whereas PED utilizes potential energy function to modify feature representations to aid other transferability metrics like LogMe and SFDA. NCTI (Wang et al., 2023) treats it as a nearest centroid classifier problem and measures how close the geometry of the target features is to their hypothetical state in the terminal stage of the fine-tuned model. LEEP (Nguyen et al., 2020) is the average log-likelihood of the log-expected empirical predictor, which is a non-parameter classifier based on the joint distribution of the source and target distribution. N-LEEP (Li et al., 2021b) is a further improvement on LEEP by substituting the output layer with a Gaussian mixture model. TransRate (Huang et al., 2022) treats SITE from an information theory point of view by measuring the transferability as the mutual information between features of target examples extracted by a pre-trained model and their labels.\nWe suggest the survey by Ding et al. (2024) for a complete view of transferability metrics.\nOf these existing methods, the approach of NCTI is closest to ours. NCTI checks to which extent the neural collapse criteria (Papyan et al., 2020) are satisfied on the target embedding. We argue (in Section 3.2) that neural collapse is a byproduct of properties of the loss function, that incentivize in later training stages the embedded points of one class to collapse to their mean. However, if we want to assess the transferability, checking for effects taking place late in training might not be a failproof approach."}, {"title": "3. Occam's model: Transferability Estimation with finding simpler representation", "content": "We assume that we are given a target dataset $\\mathcal{D} = \\{(x_n, y_n)\\}_{n=1}^N$ of $N$ labeled points and $M$ pre-trained models $\\{\\Phi_m = (\\phi_m, \\psi_m)\\}_{m=1}^M$. Each model $\\Phi_m$ consists of a feature extractor that returns a $d$-dimensional embedding $\\phi_m(x) \\in \\mathbb{R}^d$ and the final layer or head $\\psi_m$ that outputs the label prediction for the given input $x$. The task of estimating transferability is to generate a score for each pre-trained model so that the best model can be identified via a ranking list. For each pre-trained model $\\Phi_m$ a transferability metric outputs a scalar score $T_m$ that should be coherent in its ranking with the performance of the fine-tuned classifier $\\Phi_m$. That is, the goal is to obtain scores $T_m$ such that\n$T_m \\geq T_n$\n$\\frac{1}{N} \\sum_{n=1}^N P(y_n | x_n; \\Phi_m) \\geq \\frac{1}{N} \\sum_{n=1}^N P(y_n | x_n; \\Phi_n)$,\nwhere $p(y_n | x_n; \\Phi_m)$ indicates the probability that the fine-tuned model $\\Phi_m$ predicts label $y_n$ for input $x_n$. A larger $T_m$ indicates better performance model on target data $\\mathcal{D}$.\nIn this work, we aim to asses transferability by finding the degree of simplicity of our representations. We hypothesize that a classifier can be more easily fine-tuned subject to a target dataset if the embedding $\\{\\phi(x_n) | 1 \\leq n \\leq N\\}$ already has a simple structure in relationship to the labels. As a simple structure, we consider for example an embedding where the points exhibit clustering properties, where the clusters coincide with the classes, or a simple distribution of points in one class. Correspondingly, we define our metrics, one that is inspired by clustering properties (Bezdek & Pal, 1998) and one that is motivated by concept characterization and variation (Rendell & Cho, 1990; P\u00e9rez & Rendell, 1996)."}, {"title": "3.2. Clustering Approach: Measuring cluster separability as a means of transferability", "content": "Deep neural network classifiers are trained with the cross-entropy loss. Given a classifier $f_\\theta : \\mathbb{R}^d \\rightarrow [0, 1]^C$, indicating the confidence $f_\\theta(x)$ for class $1 < c < C$ and input $x$, the cross-entropy loss is defined as\n$CE(y, f_\\theta) = \\frac{1}{N} \\sum_{n=1}^N log f_\\theta(x_n)_{y_n}$\nThe classifier $f_\\theta$ can be seen as a softmax regression (multinomial logistic regression) classifier $h_{W,b}(z) = softmax(Wz + b)$ applied to an embedding $\\phi(x)$. This way, we write $f_\\theta(x) = h(\\phi(x))$.\nThe cross entropy loss is low if there exist vectors $W_.y$ and biases $b_y$ for each class $y$, such that the linear function value $Wz + b_y$ achieves its maximum value for points from class $y$ at $c = y$. If we want to estimate how well a multinomial logistic regression model would fit the embeddings $\\phi(x)$, then we could try to train a multinomial logistic regression on the target embedding. However, this procedure does not take into account that the embedding is flexible. Small changes in the embedding, performed during finetuning, have possibly big impacts on the classifier accuracy. Hence, we rather want to estimate how close the embedding is to a representation that is well-classifiable.\nTo gain insights into the classifiability of an embedding, we consider the formulation of the multinomial logistic regression objective as a Linear Discriminant Analysis (LDA) model.\nTheorem 3.1. For any multinomial regression model $h_{W,b}(z) = softmax(Wx + b)$ exist class centers $\\mu_1,..., \\mu_c$ such that\n$h_{W,b}(x)_y = \\frac{exp(-||x - \\mu_y ||^2)}{\\sum_{c=1}^C exp(-||x - \\mu_c||^2)}$\nThe proof can be found in Appendix A. The theorem shows that we can analyze the classifier's performance also from the viewpoint of a nearest-center classifier. This formulation is close to unsupervised objectives such as k-means and it explains the effect of neural collapse (Papyan et al., 2020). Once the class boundaries are sufficiently optimized, meaning that the class centers don't change much anymore, the objective still incentivizes the embedded points to be close to its class center. Over time, the class centers hence become centroids and the effects of neural collapse are taking place."}, {"title": "3.3. Concept Variation", "content": "Our second metric is based on concept variation (P\u00e9rez & Rendell, 1996), a measure that reflects the irregularity of class label distributions. Understanding this variation helps to assess the structural consistency of a concept $C$, which can be evaluated by analyzing how class labels are distributed across the feature space. A highly irregular feature space consists of numerous disjoint regions, often requiring lengthy concept representations. Conversely, a more uniform space contains expansive regions where examples share the same class labels, enabling more concise concept descriptions.\nThe concept variation metric $v$ estimates the likelihood that two neighboring examples belong to distinct classes, thereby approximating the extent of irregularity in the class label distribution. However, the original definition of $v$ is limited to Boolean spaces and assumes that all possible examples in the space are accessible.\nFormally, let $x_{i_1},..., x_{i_n}$ be the $n$ closest neighbors at Hamming distance one of an example $x_i$ in an $n$-dimensional Boolean space. The concept variation for a single instance $x_i$ is defined as\n$v(x_i) = \\frac{1}{n} \\sum_{j=1}^n \\delta(y_i, y_{i_j})$\nwhere $\\delta(y_i, y_{i_j}) = 1$ if $y_i \\neq y_{i_j}$, and 0 otherwise. Concept variation is computed as the average of this factor across all examples in the feature space:\n$\\upsilon_{total} = \\frac{1}{2^n} \\sum_{i=1}^{2^n} v(x_i) \\in [0, 1]$\nThe applicability of $\\upsilon_{total}$ is limited to artificial domains where all possible examples can be generated. (P\u00e9rez & Rendell, 1996) show how the concepts with high $\\upsilon_{total}$ are difficult to learn for most conventional inductive algorithms. Since in real-world scenarios this assumption no longer holds, a distance matrix $D$ is used to determine the contribution of each example to the amount of concept variation. The contribution to the amount of concept variation depends on the distance when $x_i, x_j$ differ in value by using the following function: $w_{ij} = 2^{-\\alpha \\cdot (D_{ij} / (\\sqrt{d} - D_{ij}))}$, where $\\alpha$ modulates the effect of distance. The higher the $\\alpha$, the less weight is assigned to the distance as it increases between the two examples. The upper bound is reached if $dist(x_i, x_j) = 0$ and the lower bound is reached when $dist(x_i, x_j) = \\sqrt{n}$ which corresponds to the maximum distance between two examples in an $n$-dimensional feature space. We describe our implementation of concept variation in Algorithm 1."}, {"title": "4. Experiments and Results", "content": "In this section, we evaluate our metrics against the current state-of-the-art SITE metrics in image classification (Section 4.1), image classification with a low data regime(Section 4.2), self-supervised learning (Section 4.3), source selection (Section 4.4) and larger network size (Section 4.5). We also provide ablation studies in Section 4.6 and Section 4.7.\nFine-Tuning Implementation details We can obtain the ground-truth ranking by fine-tuning all pre-trained models with hyper-parameters sweeping on target datasets. We use a similar fine-tuning setup as SFDA (Shao et al., 2022) for our experiments. To obtain test accuracies, we fine-tune pre-trained models with a grid search over learning rates {10-1,10-2,10-3,10-4} and a weight decay in {10-3,10-4,10-5, 10-6,0} with early stopping. We determine the best hyper-parameters based on the validation set, and fine-tune the pre-trained model on the target dataset with this parameter and without early stopping. The resulting test accuracy is used as the ground truth score $G_m$ for model $\\Phi_m$. This way, we obtain a set of scores $\\{G_m\\}_{m=1}^M$ as the ground truth to evaluate our pre-trained model rankings. To compute our metrics, we first perform a single forward pass of the pre-trained model through all target examples to extract their features. We compute the interclass distances with the Euclidean distance metric and we use the default value of $\\alpha = 2$ as defined by (P\u00e9rez & Rendell, 1996) for our second metric. We provide the implementation of our metrics in Appendix D, together with the wall clock time analysis (Experiment 4.8).\nFollowing the previous works (Shao et al., 2022; You et al., 2021; Li et al., 2021b) we use weighted Kendall's tau $\\tau_\\omega$ (Vigna, 2015) to evaluate the effectiveness of transferability metrics. Kendall's tau $\\tau$ returns the ratio of concordant pairs minus discordant pairs when ennumerating all $\\binom{M}{2}$ pairs of $\\{T_m\\}_{m=1}^M$ and $\\{G_m\\}_{m=1}^M$ as given by:\n$\\tau = \\frac{2}{M(M-1)} \\sum_{1 \\leq i < j \\leq M} sgn(G_i - G_j) sgn(T_i - T_j)$\nWhere $sgn(x)$is the signum function returning 1 if x > 0 and -1 otherwise. In the weighted version of Kendall's tau $\\tau_\\omega$, the ranking performance of top-performing models is measured to evaluate transferability metrics. In principle, a higher $\\tau_\\omega$ indicates that the transferability metric produces a better ranking for pretrained models."}, {"title": "4.1. Experiment 1: Image Classification", "content": "We propose a new and challenging experimental setup for the transferability assessment of neural networks. We use the following datasets for our experiments from the Meta-Album suite (Ullah et al., 2022): Flowers (Nilsback & Zisserman, 2008), Plant Village (G. & J., 2019), DIBaS (Zielinski et al., 2017), RESISC (Cheng et al., 2017), Cars (Krause et al., 2013), Textures (Fritz et al., 2004), and 100-sports (Piosenka). For our pre-trained model zoo we use Data-efficient Image Transformer (DeiT) (Touvron et al., 2020), Co-scale Conv-Attentional Image Transformer (CoaT) (Xu et al., 2021), Multi-Axis Vision Transformer(MaxViT) (Tu et al., 2022), MobileViT (Mehta & Rastegari, 2022), Multi-scale Vision Transformer(MVit) (Fan et al., 2021), and Cross-Covariance Image Transformer(XCiT) (El-Nouby et al., 2021) pretrained on ImageNet-1k from Huggingface timm library (Wightman, 2019). For image classification tasks, we select LogMe, SFDA, N-LEEP, ETran, LDA Baseline (LDA was available with ETran codebase), NCTI, and TransRate. We describe our model zoo and dataset selection, along with the limitations of the current experimental design, in detail in Appendix B.3.\nThe results in Table 1 show the effectiveness of our methods for datasets with INT achieving the highest average $\\tau_\\omega$. INT seems to outperform on every dataset except DIBaS."}, {"title": "4.2. Experiment 2: Limited data setting", "content": "In a realistic transfer learning setting, we have only a small amount of data available for the target task. To emulate this more challenging setting, we perform an experiment with a limited set of 40 examples per class, reported in Table 2. Here as well, we observe that Concept Variance performs much more robustly than other baseline methods and also with respect to INT in the DIBaS and Flowers dataset. The combination of both INT and Concept Variation where the classes are balanced with a limited set of examples. We observe that INT obtains the highest score as well in this scenario and Concept Variance obtains the second highest score in this scenario. We also observe that the sum of INT with Concept Variance obtains higher $\\tau_\\omega$ than INT or Concept Variance separately."}, {"title": "4.3. Experiment 3: SSL Experiments", "content": "For our third set of experiments, we apply our metrics to the Self-Supervised Learning(SSL) task. For self-supervised learning we use BYOL (Grill et al., 2020), Deepcluster-v2 (Caron et al., 2018), Infomin (Tian et al., 2020), In-Dis (Wu et al., 2018), MoCo-v1 (He et al., 2020), MoCo-v2 (He et al., 2020), PCL-v1, PCL-v2 (Li et al., 2021a), Sela-V2 (YM. et al., 2020) and SWAV (Caron et al., 2020) with a pretrained ResNet-50 backbone. We use CIFAR10, CIFAR100 (Krizhevsky et al., 2009) and Caltech101 (Fei-Fei et al., 2007) for our experiments. We report the $\\Tau_w$ of LogMe, SFDA and our metrics in Table 3. In SSL experiments we show that INT works on par with SFDA with a tiny difference of average $\\tau_\\omega$ of 0.752 whereas SFDA average $\\tau_\\omega$ is 0.749. We did not get result on TransRate for the other two datasets after 120 minutes of compute for the other two datasets."}, {"title": "4.4. Experiment 4: Source Selection", "content": "In this experiment, we evaluate whether our proposed transferability estimate works well if our source models are trained on more specific source datasets (e.g., where all classes belong to one domain), as opposed to very broad source datasets, such as ImageNet. We use the models CoaT, DeiT, MAXVit, MVitv2, and XciT pre-trained on ImageNet-1k and then fine-tuned on the following datasets: Flowers, RESISC, DIBaS, and Plant Village. Our resulting model zoo consists of 24 models. The target datasets in this experiment are Sports, Textures, and Cars. We report the performance for this experiment in Table 4. We observe that INT performs here together with TransRate best."}, {"title": "4.5. Experiment 5: Bigger Networks", "content": "In this experiment, we evaluate bigger models with larger embedding sizes as well as parameter range. We take the larger version of models introduced in Section 4.1, details of selected models can be found in Appendix B.2. The embedding size also quadruples in most of the models. We report our findings for this experiment in Table 5. We observe every metric performance drop in this scenario. This also implies that it is harder to estimate the representational complexity when embeddings are larger. Our methods still show much better performance than other metrics. The performance is notably worse for the DIBaS and Cars datasets, where every metric yields a negative $\\tau_\\omega$. We found that this decline is primarily due to two networks that performed exceptionally poorly, achieving accuracies between 10-35%-the lowest observed across all networks and datasets."}, {"title": "4.6. Experiment 6: Effect of \u03b1 on Concept Variance", "content": "We analyze the effect of class weights on Concept Variance performance. We show the effect of $\\alpha$ in Figure 3. The results suggest that the optimal $\\alpha$ value is between 2-5."}, {"title": "4.7. Experiment 7: INT distance metric studies", "content": "In this study, we analyze how INT performs under various distance metrics. We compare the following metrics: Euclidean, Squared Euclidean, Manhattan, and Cosine. We used the implementations of the pairwise distances from the CuML (Raschka et al., 2020) and CuPy (Okuta et al., 2017) libraries. We report the performance of INT with different distance metrics in Table 6. We observe that Euclidean gives the most optimal performance out of all very close to squared Euclidean with the difference in the Textures dataset. Cosine distance reports the worst performance with negative $\\tau_\\omega$."}, {"title": "4.8. Experiment 8: Wall clock analysis/time taken", "content": "We benchmark the computation time for scoring MVitv2 Base on the DIBaS dataset. We observe that our metrics provide better performance than second and third-best performing metrics(Transrate and SFDA) in a fraction of time with INT performing 5 times faster than SFDA and 43 times better than TransRate and Concept Variance performing 30 times faster than SFDA and 280 times faster than TransRate, making these metrics suitable for large scale assessment of pretrained models."}, {"title": "5. Conclusion and Discussion", "content": "With the current proliferation of pretrained models, finding the best model for a given target task becomes an increasingly important component of transfer learning. In this work, we propose two novel transferability metrics for classification tasks that are based on the idea that the 'simplest' representation, i.e. the one that leads to the clearest separation of classes, will be the best starting point for future finetuning. We evaluate both metrics, Pairwise Normalized Interclass Distance (INT) and Concept Variance, on a wide set of in-depth experiments across many image classification problems. We find that the INT metric significantly outperforms all of the state-of-the-art transferability metrics (by 38% in Experiment 4.1 and 33% in Experiment 4.2), although depending on the target task, Concept Variance or a combination of both works better. Moreover, both metrics can be computed efficiently, faster or on par with existing methods."}, {"title": "5.1. Future work", "content": "An important direction for future work is extending transferability metrics to tasks beyond classification, such as object detection, keypoint regression, semantic segmentation, and depth estimation. These tasks pose unique challenges, as they require processing continuous and multidimensional target variables rather than discrete labels. Nevertheless, we believe that metrics that are similarly based on the complexity of the pre-trained embeddings can provide valuable insights and improvements for pretrained model selection and transfer learning in these domains as well."}, {"title": "Impact Statement", "content": "This paper presents the work around transferability estimation and its links to simplicity of representations.\nOur work can help in reducing the time taken by ML practitioners to select a pretrained model hence also reducing computational costs of training multiple models and reducing carbon footprint of a ML workflow."}, {"title": "A. Proofs", "content": "We state first the following theorem from (Hess et al., 2020) for completeness, since we need it for our analysis.\nTheorem A.1. Let $h_{W,b}(x) = softmax(Wx + b)$ be a multinomial regression model with $W \\in \\mathbb{R}^{C \\times d}$ and $b \\in \\mathbb{R}^{C}$, computing class predictions as $y = arg\\ max_{1 \\leq c \\leq C} x^T W_.c + b_c$. If $W$ has at least a rank of $r > C$, then there exist $C$ class centers $\\mu_c \\in \\mathbb{R}^d$ such that every point $x$ is assigned to the class having the nearest center:\ny = arg\\ min_{1 \\leq c \\leq C} ||x - \\mu_c||^2$.\nProof. We show that for any dataset and network there exists a set of class centers such that the classification does not change when classifying according to the nearest center.\nWe gather given data points in the matrix $D$:\n$D^T = (x_1 \\dots x_N) \\in [\\mathbb{R}^{d \\times N}$\nWe define $Z = W + v\\mathbb{1}_C$, where $v \\in \\mathbb{R}^d$ and $\\mathbb{1}_C \\in \\{1\\}^C$ is a constant one vector. The (soft)max classification of all data points in $D$ is then given by the one-hot encoded matrix $Y \\in \\{0,1\\}^{N \\times C}$ that optimizes the objective\n$arg\\ max_Y tr(Y^T (W^T D + b\\mathbb{1}^T)) = arg\\ min_Y ||D - YZ^T||^2 + tr((2b\\mathbb{1}^T - Z^T Z)Y^T Y)$\nThe matrix $Z \\in \\mathbb{R}^{d \\times C}$ indicates a set of $C$ centers by its columns. The first term of Equation (7) is minimized if $Y$ assigns the class with the closest centroid to each data point in $D$. Hence, if we can show that there exists a vector $v \\in \\mathbb{R}^d$ such that the second term of Equation (7) is equal to zero (given $D$ and $W$) then we have shown what we wanted to prove. Since $|Y_j.| = 1$ (every point is assigned to exactly one class), the matrix $Y^T Y$ is a diagonal matrix, having the number of data points assigned to each class on the diagonal: $Y^T Y = diag(|Y_.1|, \\dots, |Y_.c|)$. Hence, the trace term on the right of Equation (7) equals\n$\\sum_c (2b\\mathbb{1} - Z^T Z)_.cc |Y_.c| = \\sum_c (2b_c - ||W_.c||^2 - 2v^T W_.c) |Y_.c| - ||v||^2 m$\nWe define the vector $u \\in \\mathbb{R}^C$ such that $u_c = b_c - ||W_.c||^2$. The right term of Equation (8) is constant for a vector $v$ satisfying $u_c = v^T W_.c$ for $1 \\leq c \\leq C$. That is, we need to solve the following equation for $v$:\nu = W^T v = V\\Sigma U v.\nSince the rank of $W$ is $C$ (full column rank), this equation has a solution. It is given by the SVD of $W = U \\Sigma V^T$, where $U \\in \\mathbb{R}^{d \\times C}$ is a left orthogonal matrix ($U^T U = I$), $\\Sigma \\in \\mathbb{R}^{C \\times C}$ is a diagonal matrix having only positive values, and $V \\in \\mathbb{R}^{C \\times C}$ is an orthogonal matrix ($V^T V = V V^T = I$). Setting $v = U \\Sigma^{-1} V^T u$, this vector solves the equation.\nTheorem A.2 (Restatement of Thm 3.1 ). For any multinomial regression model $h_{W,b}(z) = softmax(Wx + b)$ exist class centers $\\mu_1,..., \\mu_c$ such that\n$h_{W,b}(x)_y = \\frac{exp(-||x - \\mu_y ||^2)}{\\sum_{c=1}^C exp(-||x - \\mu_c||^2)}$\nProof. According to the proof of Thm. A.1, exists a vector $v$ such that the class boundaries do not change when we replace the standard multinomial regression prediction with the nearest center prediction, where the centers are defined by $\\mu_c = W_.c + v$. For this set of centroids we have the following relationship of the multinomial regression confidence to the center-based confidence.\n$softmax(Wx + b)_y = \\frac{exp(x W_.y + b_y)}{\\sum_{c=1}^C exp(x W_.c + b_c)} = \\frac{exp(x^T \\mu_y)}{\\sum_{c=1}^C exp(x v)} = \\dots = \\frac{exp(-||\\mu_y ||^2 + x^T \\mu_y + b_y + \\frac{1}{2} ||\\mu_y ||^2) exp(\\frac{1}{2} ||x||^2)}{\\sum_{c=1}^C exp(-\\frac{1}{2} ||\\mu_c||^2 + x^T \\mu_c + b_c + \\frac{1}{2} ||\\mu_c||^2) exp(\\frac{1}{2} ||x||^2)} = \\frac{exp(-||\\mu_y - x||^2) exp(b_y + \\frac{1}{2} ||\\mu_y ||^2)}{\\sum_{c=1}^C exp(-||\\mu_c - x||^2) exp(b_c + \\frac{1}{2} ||\\mu_c ||^2)}$\nSince the class boundaries of the nearest-center prediction do not change the original class boundaries, we have for any point $x$ on the decision boundary between class a and b $softmax(Wx + b)_a = softmax(Wx + b)_b$. At the same time we have $exp(-||\\mu_{y_1} - x||^2) = exp(-||\\mu_{y_2} - x||^2)$ because the nearest center classifier with centers $\\mu_c$ is not changing the decision boundary according to Thm A.1. As a result we have (using the equivalence above)\n$\\frac{1}{2} exp(b_{y_1} + \\frac{1}{2} ||\\mu_{y_1} ||^2) = exp(b_{y_2} + \\frac{1}{2} ||\\mu_{y_2} ||^2)$.\nSince this equation holds for any point on the decision boundary, we have proven our final result."}, {"title": "B. Omitted Experiment Details in Section 4", "content": "B.1. Dataset details\n1. DIBaS: Digital Image of Bacterial Species (DIBaS). The Digital Images of Bacteria Species dataset (DIBaS) (https://github.com/gallardorafael/DIBaS-Dataset) is a dataset of 33 bacterial species with around 20 images for each species.\n2. Flowers: Flowers dataset from"}]}