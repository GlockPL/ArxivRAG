{"title": "A Comparative Study on Dynamic Graph Embedding based on\nMamba and Transformers", "authors": ["Ashish Parmanand Pandeya", "Alan John Varghese", "Sarang Patil", "Mengjia Xu"], "abstract": "Dynamic graph embedding has emerged as an important technique for modeling complex time-evolving networks across diverse\ndomains. While transformer-based models have shown promise in capturing long-range dependencies in temporal graph data,\nthey face scalability challenges due to quadratic computational complexity. This study presents a comparative analysis of dy-\nnamic graph embedding approaches using transformers and the recently proposed Mamba architecture, a state-space model with\nlinear complexity. We introduce three novel models: TransformerG2G augment with graph convolutional networks, DG-Mamba,\nand GDG-Mamba with graph isomorphism network edge convolutions. Our experiments on multiple benchmark datasets demon-\nstrate that Mamba-based models achieve comparable or superior performance to transformer-based approaches in link prediction\ntasks while offering significant computational efficiency gains on longer sequences. Notably, DG-Mamba variants consistently\noutperform transformer-based models on datasets with high temporal variability, such as UCI, Bitcoin, and Reality Mining, while\nmaintaining competitive performance on more stable graphs like SBM. We provide insights into the learned temporal dependencies\nthrough analysis of attention weights and state matrices, revealing the models' ability to capture complex temporal patterns. By ef-\nfectively combining state-space models with graph neural networks, our work addresses key limitations of previous approaches and\ncontributes to the growing body of research on efficient temporal graph representation learning. These findings offer promising di-\nrections for scaling dynamic graph embedding to larger, more complex real-world networks, potentially enabling new applications\nin areas such as social network analysis, financial modeling, and biological system dynamics.", "sections": [{"title": "1. Introduction", "content": "Studying the evolving dynamics exhibited in complex real-world graphs (or networks) across spatial and temporal\nscales plays a key role in diverse fields such as biological networks, social networks, e-commerce, transportation sys-\ntems, functional brain connectomes, and financial markets. We can model these graphs as dynamic graphs in either\ncontinuous time or discrete time based on the nature of the data. Understanding how these networks change over\ntime and space can provide great insight into their underlying structures and behaviors, enabling more accurate future\nforecasting in areas such as disease progression modeling, transportation planning, anomaly detection, and risk as-\nsessment. Conventional graph neural networks (GNNs) [1, 2, 3, 4] assume a fixed graph structure without considering\nevolving dynamics. However, time-aware temporal graph embedding techniques, including continuous-time [5, 6, 7]\nand discrete-time methods [8, 9, 10, 11, 12, 13], facilitate learning temporal information for future forecasting by\nintegrating conventional GNNs with recurrent neural networks (RNNs) or long short-term memory (LSTM) or gated\nrecurrent unit (GRU). However, RNN or LSTM-based temporal graph embedding models often suffer from issues like\ncatastrophic forgetting, vanishing gradients, and exploding gradients. Moreover, these approaches face significant\nchallenges while dealing with very large sequential graph snapshots.\nOn the other hand, attention-based transformers [14] have empowered many advanced Large Language Mod-\nels (LLMs) with impressive in-context learning (ICL) capability. Transformers have been increasingly applied to\ncapture long-range dependencies across graph snapshots, enabling parallel processing of long graph sequences and\nsignificantly enhancing context awareness. Dynamic graph representation learning applications have made impres-\nsive progress with the development of attention mechanisms for diverse applications, such as learning temporal graph\nembeddings with uncertainty quantification [15], anomaly detection [16], traffic flow forecasting [17, 18], financial\ntime series prediction [19], weather forecasting [20], heating environment forecasting [21], and 3D object detec-\ntion [22]. Diverse PyTorch implementations of the aforementioned temporal graph embedding methods can be found\ninNevertheless, the quadratic computational com-\nplexity of transformers when dealing with long graph snapshots remains a major shortcoming despite their promising\nperformance and distinct advantage in capturing long-range dependency.\nMore recently, an emerging alternative to transformers, known as \"state-space sequence models (SSMs)\" [23, 24],\nhas gained substantial attention by addressing \"quadratic complexity\" bottleneck inherent in the attention mechanism.\nIn particular, a selective SSM architecture, Mamba [24], achieved impressive performance on the Long Range Arena\n(LRA) benchmark with only linear complexity. The Mamba model outperformed the initially developed structured\nSSM model (i.e., S4 model [23]) by incorporating the \"selective scan\" technique to compress the data selectively\ninto the state and improved efficiency through the adoption of hardware-aware algorithms on modern hardware (e.g.,\nGPUs and TPUs). Due to temporal graphs that can be modeled as continuous-time temporal events or discrete-time\ngraph snapshots [25], Li et al. [26] developed an SSM-based framework (i.e., GraphSSM) for learning temporal graph\ndynamics by integrating structural information into the online approximation objective with a Laplacian regularization\nterm.\nHerein, we conducted a comparative study on dynamic graph embedding based on Mamba and Transformers for\ndiverse benchmarks with smooth and heterogeneous temporal dynamics."}, {"title": "2. Preliminaries", "content": ""}, {"title": "2.1. Dynamic graph modeling", "content": "Dynamic graphs provide a powerful framework for representing complex systems that evolve over time in various\ndomains [27, 28]. Unlike static graphs, dynamic graphs capture the temporal evolution of relationships between\nentities, offering a more realistic model of real-world networks [29]. Generally, a discrete-time dynamic graph G can\nbe represented by a sequence of graph snapshots G = {G1, G2, ..., Gr}, where G\u2081 = (V\u2081, E\u2081), t \u2208 [0, T] denotes the state\nof the graph at time step t, where V, is the set of nodes and E, is the set of edges [29]. More precisely,\nV\u2081 = {V1, V2, ..., v|v,\\}, Et \u2286 {(u, v)\u013cu, v \u2208 V\u2081}.\n(1)\nThe discrete-time representation allows us to model systems where changes occur at specific intervals, such as\ndaily social media interactions or weekly financial transactions [30].\nIn practice, the set of nodes V, can change over time, reflecting the dynamic nature of entity participation in the\nsystem. These dynamics can be represented as V; \u2260 V; for time steps i \u2260 j, and i, j\u2208 [0, T]. The corresponding\nadjacency matrix A\u2081 \u2208 R|V|\u00d7|V| of graph G\u2081 at time t can be defined as\nAt\n= [a\u2081(u, v)], where a\u2081(u, v) ={\nWuv if (u, v) \u2208 Et;\nor\notherwise.\n(2)\nHere, wuv denotes the weight associated with the edge between node pair (u, v) when the graph G\u2081 is weighted;\nOtherwise, in a binary graph, the edge weight is uniformly 1. The key challenge in dynamic graph modeling lies in\neffectively and accurately capturing both the spatial and temporal dependencies. The primary goal of temporal graph\nembedding is to learn a nonlinear mapping function f that maps each node z at time t to a low-dimensional embedding\nvector:\nf : (z, t) \u2192 emb(z, t) \u2208 RL\u00ba, ze V\u2081\n(3)\nwhere Lo is the dimensionality of the node embedding vector, typically much smaller than |V|."}, {"title": "2.2. State space models (SSMs)", "content": "Recently, control theory-inspired State-Space Models (SSMs), an emerging alternative to transformers, offer a\nmore general technique with linear complexity that outperform transformers particularly in long-sequence modeling\ntasks (e.g., the Google LRA benchmark2). Due to their advantages, deep SSMs have been increasingly applied in\nvarious domains, such as speech recognition, language processing, time series prediction, music generation, computer\nvision, temporal graph modeling, and bioinformatics [32]. Mathematically, SSMs model a system's internal state and\nits temporal evolution through a recursive state transition equation, which enables capturing long-range dependencies\nand modeling complex dynamics, without the need for attention mechanisms. SSMs typically contain two main\ntypes of representation: continuous-time and discrete-time. Continuous SSMs can be simply described as linear\nordinary differential equations [23], while discrete SSMs can be modeled in both recurrent and convolutional forms.\nThe reccurrent form of discrete SSMs is shown in Eq. 6. It is equivalent to an RNN without nonlinear recurrent\nactivations.\nstate equation: h\u2081 = \u0100ht-1 + Bxt\noutput equation: y\u2081 = Ch\u2081 + Dx\u2081\n(6)\nwhere x \u2208 Rdin is the input sequence, yt \u2208 Rdout is the output sequence, h\u2081 \u2208 Rm is the state vector, \u0100 \u2208 Rmom is\nthe state transition matrix, B \u2208 Rm\u00d7din is the input matrix, \u0108 \u2208 Rdout\u00d7m is the output matrix, and D \u2208 Rdout\u00d7m is the\nskip connection matrix. \u0100, B, C and D are learnable SSM parameters. In the context of dynamic graph embedding,\nh, represents the latent node or edge embedding of the graph, x, could be new node or edge features, and y, is the\npredicted label (classification problem) or value (regression problem) at time t.\nDiscrete-time SSMs can also be formulated as a global convolution operation in terms of an SSM convolutional\nkernel, K, which has the same shape as the input sequence length L; see the detailed formula in Eq. 7. The SSM\nconvolutional kernel is composed of the three SSM parameters (\u0100, B, and C). The Convolutional representation of\nSSMs offers several advantages, including computational efficiency through the use of fast algorithms like fast Fourier\ntransform (FFT)\nyt = K * Xt, where \u0158\u2208 RL : (\u010c\u0100B)k\u2208[L]\u00b7\nke\n(7)\nThe structured SSMs (S4) were initially developed as linear time invariant (LTI) models, with all parameters\n(\u0100, B, C and \u2206) fixed across time steps for the entire sequence. In particular, the S4 model parameterized the state\nmatrix (A) as a HIPPO (History Preserving Polynomial Operator) matrix [23], which allows it to effectively capture\nthe underlying temporal dynamics by projecting discrete time series onto polynomial bases. However, the fixed\nparameterization approach restricts the effectiveness of S4 in content-based reasoning tasks. To address this issue,\nthe Selective State Space Models (SSMs), also known as S6 or Mamba [24], incorporate the selectivity mechanism\nthat updates the state selectively based on the input, i.e., B = SB(x\u2081),\u0108 = Sc(x\u2081), \u25b3\u2081 = S\u2081\u2081(\u25b3\u2081) are now input-\ndependent functions, allowing selective update of the hidden state based on the relevance of incoming inputs. Hence,\nMamba is also known as a linear time-variant (LTV) SSM. Moreover, Mamba employs hardware-aware algorithms,\nincluding parallel scan, kernel fusion, and activation recomputation, to achieve greater efficiency when processing\nlong sequences.\nThe aforementioned advances in SSMs offer promising directions for modeling dynamic graphs, potentially pro-\nviding more efficient and effective alternatives to attention-based models to capture long-range dependencies in tempo-\nral graphs [26]. Each of these approaches offers unique trade-offs in terms of expressiveness, computational efficiency,\nand the ability to capture long-term dependencies. In this paper, we will explore how various architectures, including\ntransformers and state-space models, can be adapted to address the dynamic graph embedding problem. Our focus will\nbe on their capacity to capture complex spatial-temporal patterns and effectively scale to large, evolving networks. To\naddress the aforementioned challenges, we have introduced three new probabilistic temporal graph embedding models\nbased on our prior works [15, 11], i.e., ST-transformerG2G, DG-Mamba, and GDG-Mamba with GINE Convolution.\nThese models effectively integrate spatial and temporal dependencies along as well as incorporate important evolving\nedge features, using transformers and emerging Mamba models to enhance performance in temporal link prediction."}, {"title": "3. Proposed Methods", "content": "Problem Definition. We consider a discrete-time temporal graph G = {G\u2081}7_1, represented by a sequence of graph\nsnapshots over T timestamps. Each graph snapshot G, consists of a vertex set V\u2081 = {21, 22, 23, ..., v\u2081},t \u2208 [T]\nand an edge set E\u2081 = {e\u00b8\u00a1\\i, j\u2208 V\u2081}. The main objective here is to transform the representation of nodes from a\nhigh-dimensional sparse non-Euclidean space to a lower-dimensional density function space as multivariate Gaussian\ndistributions at each timestamp. Representing each node as a Gaussian distribution allows us to capture the uncertainty\nassociated with that representation. To this end, our aim is to learn node representations for the timestamp t using graph\nsnapshots from the previous l + 1 timestamps, i.e., Gt-1, . . ., Gt-1, Gt, while capturing the long-range dependencies at\nboth spatial and temporal scales. Here, the look-back I is a predefined hyperparameter that represents the number of\ntimestamps to look-back, i.e., the length of the history considered.\nProbabilistic Node Representation. To achieve the main aims described above, three temporal graph embedding\nmodels were developed in the paper. Specifically, all proposed models learn non-linear mapping function that encodes\nevery single graph snapshot in the graph sequence G into lower-dimensional embedding space as probabilistic density\nfunctions, i.e., each node z in the latent space is represented as multivariate Gaussian distributions \u039d(\u03bc, \u03a3), where\n\u03bc' \u2208 RL\u00ba denotes the embedding means; \u03a3 \u2208 RL\u2030\u00d7Lo = diag(0) represents the square diagonal covariance matrix with\nthe embedding variances on the diagonal, Lo is the size of node embedding.\nData Preprocessing. Due to the varying sizes of input graph snapshots across time in the temporal graph, all adja-\ncency matrices ({A}}_\u2081) in the graph sequence are initially normalized to adjacency matrices of same size ({\u00c3\u2081}-1)\nwith zero-padding technique. As such, all the normalized adjacency matrices have the same shape of (n, n), where n\nrepresents the maximum number of nodes across all timestamps. If a node is absent in the graph snapshot at a par-\nticular timestamp t, the corresponding row and column in the normalized adjacency matrix A\u2081 are filled with zeroes,\nindicating that the node has no connections to any other nodes during that timestamp."}, {"title": "3.1. ST-TransformerG2G: Improved TransformerG2G with GCN model", "content": "In this model, we build on our previous TransformerG2G model [15] by including GCNs blocks to explicitly\ncapture spatial dependencies within each graph snapshot, see detailed architecture in Fig. 1. The model takes as input\na series of graphs (Gt-1, ..., G\u2081), and each graph is passed through a GCNs block. The GCN block consists of 3\nGCN layers that help in learning spatial information. From the output of the GCNs block, we obtain a sequence of\nlearned the representation of node i over l + 1 timestamps: (v\u00af\u00b9,\u06f0\u06f0\u06f0,v\u22121,v), where v \u2208 Rd and v corresponds to\nnode i's representation at timestamp t. We also added a vanilla positional encoding to this sequence of vectors [14].\nThe resulting sequence is then processed by a standard transformer encoder block [14] with single-head attention,\nwhich outputs another sequence of d-dimensional vectors. The sequence of output vectors from the encoder block is\ncombined using a \"point-wise convolution\u201d to obtain a single vector, and a tanh activation is applied to obtain h\u2081. This\nvector is then fed through two projection heads to obtain the final probabilistic node embedding means \u2013 \u03bc \u2208RL\u3002\nand variance \u2013 \u03c3 \u2208 RL, representing the node z. The projection head to obtain the mean vector consists of a linear\nlayer, i.e., \u00b5\u2081 = h\u00a1W\u00b5 + b\u00b5, and the projection head to obtain the variance vector consists of a linear layer with elu\nactivation, i.e., \u03c3\u2081 = elu(h\u00a1W\u2082 + b) + 1, to ensure the positivity of the variance vector. The self-attention mechanism\nin the encoder block enables the model to learn the information from the node's historical context, and the GCN block\nhelps learn the spatial dependencies in each graph."}, {"title": "3.2. DG-Mamba model", "content": "The DG-Mamba model offers an alternative to our earlier TransformerG2G architecture [15] for temporal graph\nembedding. It utilizes selective scan-based SSMs (i.e., Mamba) [24] to efficiently capture long-range dependencies\nwithout the attention mechanism. The newly proposed developed DG-Mamba framework makes use of Mamba's\nenhanced efficiency to overcome the quadratic autoregressive inference complexity bottleneck of transformers, while\nalso effectively managing long-range dependencies in dynamic graphs.\nThe main architecture of DG-Mamba is illustrated in Fig. 2. It follows four main steps: a) data preprocessing,\nb) temporal graph embedding based on Mamba, c) embedding aggregation, and d) output probabilistic distributions.\nInitially, it processes discrete-time input graph snapshots using the aforementioned zero-padding technique to stan-\ndardize their sizes. In order to capture long-range dependencies for each graph node, the history length parameter,\ni.e., look-back l, is set to l = {1, 2, 3, 4, 5}. Each graph snapshot G\u2081 = (A1, X\u2081) has a corresponding adjacency matrix At\nand a node feature matrix X\u2081, it follows the following procedure to obtain the node-level Gaussian embeddings.\nThe core operation starts with the Mamba Layer. The input to this layer is processed using a selective scan-based\nstate-space model (SSM) [24]. For each input graph snapshot x, the Mamba layer outputs embedding e:\ne = Mamba(x),\n(8)\nwhere x = {X1, X2, . . ., X7 } is the input sequence of graph snapshots consisting of node feature matrices. Next the\nembedding e is passed through the Mean Pooling layer for aggregating the graph representation:\ne = MeanPool(e).\n(9)\nThe pooled embedding is further transformed through a linear layer followed by the tanh activation function.\nx = tanh(Linear(e)).\n(10)\nThe nonlinear exponential linear unit (ELU) activation function is added to capture richer features: x = ELU(x).\nThe processed embedding is then parameterized as a Gaussian distribution by computing the mean and variance as\nfollows.\n\u03bc = Linear(x),\n\u03c3 = ELU(Linear(x)) + 1 + \u0454.\n(11)\nHere, e is a small constant (1e-14) added for numerical stability.\nThe DG-Mamba model preserves the probabilistic embedding characteristic of TransformerG2G [15], represent-\ning each node as a multivariate Gaussian distribution in the latent space. Furthermore, it enhances the previous model's\nability to capture both spatial and temporal dynamics. This allows for uncertainty quantification in the learned rep-\nresentations, which is particularly valuable for dynamic graphs where node properties and relationships may change\nover time."}, {"title": "3.3. GDG-Mamba Model: Enhanced DG-Mamba with GINE Conv", "content": "To enhance spatial representation by incorporating node features and crucial edge features simultaneously, before\nlearning temporal dynamics with the Mamba layer, we introduce a DG-Mamba variant (GDG-Mamba) based on the\nGraph Isomorphism Network Edge (GINE) convolutions [33]; the detailed architecture is shown in Fig. 3.\nThe main procedure of this enhanced DG-Mamba model involves the following steps. First, the input graph\nnode feature matrix x\u2081, t \u2208 [T] at each timestamp t is processed using Graph Isomorphism Network Edge (GINE)\nconvolution to enhance spatial representation.\nZi = GINEConv(x\u2081, edge_index, edge_attr)\n(12)\nThe GINEConv operation captures the structural information and generates enriched node representation z\u0142 by con-\nsidering both node and edge features. The processed node representations for all timestamps are concatenated\nZ = [Z1, Z2, ..., zr]. Then this sequence is passed through the Mamba layer, followed by mean pooling, linear pro-\njection with tanh activation function to generate intermediate representation for each node. Additional linear layers\nfor mean and log-variance of Gaussian embeddings as shown in Eq. 13. in the same way we process DG-Mamba\nearlier, except here we pass GINE Convolution processed output of x instead of directly passing x.\ne = Mamba(z)\ne = MeanPool(e)\nx = tanh(Linear(e))\nx = ELU(x)\n\u03bc = Linear(x)\n\u03c3 = ELU(Linear(x)) + 1 + \u20ac\n(13)\nThe GINE convolution is defined as:\nGINEConv(x, edge_index, edge_attr) = ((1+e) \u00b7 x + \u2211 ReLU(xj|ledge_attr(i,j)))\njeN(i)\n(14)\nwhere & is a learnable function (typically an MLP), e is a learnable parameter, N(i) denotes the neighbors of node\ni, and || represents concatenation."}, {"title": "3.4. Loss function and training methodology", "content": "All models considered in this work employ a triplet-based contrastive loss, inspired by [34], to train the network.\nThe triplet-based contrastive loss is defined in Eq. 15. In this process, each node acts as a reference point, for which\nwe sample the k-hop neighborhoods - nodes that are k hops away from the reference node. Using these neighborhoods,\nwe construct a set of node triplets T\u2081 = {(vi, vrear, var)|v; e V\u2081} for each timestamp. Each triplet consists of a reference\nnode vi, a nearby node vrear, and a distant node var. These triplets satisfy the constraint that the shortest path from\nvi to vrear is shorter than that to vfar, i.e., sp(vi,vrear) < sp(vi, var). The function sp(., .) measures the shortest path\nlength between two nodes. The triplet-based contrastive loss is defined in as follows:\ni\n-E\nL=\u03a3\u03a3\u0395)+ e-Bear)],\nt\n(vi,vrear, var) ETt\nE2\n(vi,vnear)\n(15)\nwhere E(vi,v,ear) and E(vi,vfar) represent the Kullback-Leibler (KL) divergence between the embeddings of the reference\nnode and its nearby and distant nodes, respectively. The KL divergence quantifies the dissimilarity between the joint\nnormal distributions of two nodes in the embedding space. The formula for calculating the KL divergence between\nthe multivariate Gaussian embeddings of two nodes (vi and vj) is shown in Eq. 16.\nE(vi,v;) = DKL (N(\u03bc\u03af, \u03a3\u03afi), ||N(\u03bcj, \u03a3;))\n=\ntr(\u03a3; \u03a3\u2081) + (\u03bc; \u2212 \u03bc\u2081) \u03a3; (\u03bc; - Mi) - L + logi\n(16)"}, {"title": "4. Experimental Results and Analysis", "content": ""}, {"title": "4.1. Dataset descriptions", "content": "We used five different graph benchmarks to validate and compare our proposed models. A summary of the\ndataset statistics \u2013 including the number of nodes, number of edges, number of timesteps, and the distribution of\ntrain/validation/test splits with specific numbers of timesteps, as well as the embedding size \u2013 is provided in Table 1.\nReality Mining dataset\u00b3: The network contains human contact data among 100 students of the Massachusetts\nInstitute of Technology (MIT); the data was collected with 100 mobile phones over 9 months in 2004. Each node\nrepresents a student; an edge denotes the physical contact between two nodes. In our experiment, the dataset contains\n96 nodes and 1,086,403 undirected edges across 90 timestamps.\nUC Irvine messages (UCI) dataset\u2074: It contains sent messages between the users of the online student community\nat the University of California, Irvine. The UCI dataset contains 1,899 nodes and 59,835 edges across 88 timestamps\n(directed graph). This dataset exhibits highly transient dynamics.\nStochastic Block Model (SBM) dataset: It is generated using the Stochastic Block Model (SBM) model. The\nfirst snapshot of the dynamic graph is generated to have three equal-sized communities with an in-block probability\nof 0.2 and a cross-block probability of 0.01. To generate subsequent graphs, it randomly picks 10-20 nodes at each\ntimestep and moves them to another community. The final generated synthetic SBM graph contains 1000 nodes,\n4,870,863 edges, and 50 timestamps.\nBitcoin-OTC (Bit-OTC) dataset: It is a who-trusts-whom network of people who trade using Bitcoin on a\nplatform called Bitcoin OTC. The Bit-OTC dataset contains 5,881 nodes and 35,588 edges across 137 timestamps\n(weighted directed graph). This dataset exhibits highly transient dynamics.\nSlashdot dataset: It is a large-scale social reply network for the technology website Slashdot. Nodes represent\nusers, and edges correspond to users' replies. The edges are directed and start from the responding user. Edges are\nannotated with the timestamp of the reply. The Slashdot dataset contains 50,824 nodes and 42,968 edges across 12\ntimestamps."}, {"title": "4.2. Implementation details", "content": ""}, {"title": "4.2.1. ST-TransformerG2G", "content": "The GCN block in Fig. 1 comprises three GCN layers, each followed by a dropout layer and utilizing tanh acti-\nvation. The initial GCN layer projects the input vector from an n-dimensional space to a d-dimensional vector and\nperforms GCN updates, where n represents the maximum number of nodes in each graph dataset (see the first column\nin Table 1 for all datasets). We use d = 256 for SBM, Reality Mining, Slashdot, and Bitcoin datasets and d = 512\nfor the UCI dataset. For all datasets, we used a dropout rate of 0.5 in the dropout layer. In the transformer encoder,\nwe used a single encoder layer with single-head attention. In the nonlinear layer after the encoder, a tanh activation\nfunction and the vectors were projected to a 512 dimensional space, similar to our prior DynG2G work [11]. In the\nnonlinear projection head, we used an ELU activation function. The Adam optimizer is used to optimize the weights\nof the ST-TransformerG2G model across all datasets. During our experiments, we tested different lookback values\n(I = 1, 2, 3, 4, 5), and the optimal lookback values that achieved the best link prediction performance for each dataset\nare presented in Table 2. Moreover, all relevant hyperparameters, including the learning rates (lr) for different datasets,\nare summarized in Table 2."}, {"title": "4.2.2. DG-Mamba", "content": "The DG-Mamba model uses the Mamba model as its core component. The Mamba layer was configured with the\nhyperparameters listed in Table 3."}, {"title": "4.2.3. GDG-Mamba", "content": "The GDG-Mamba model incorporates Graph Isomorphism Network Edge (GINE) convolutions to enhance spatial\nrepresentation by incorporating crucial edge features before processing temporal dynamics with the Mamba layer. The\nhyperparameters used to configure the GDG-Mamba model during its implementation are presented in Table 4.\nThe model was trained over 50 epochs using a triplet-based contrastive loss and the Adam optimizer, incorporating\nearly stopping based on validation loss to optimize performance. We applied the same amount of training data (70%),\nvaluation data (10%) for the evaluation of ST-transformerG2G, DG-Mamba, and GDG-Mamba. The parameters\npresented in Tables 3 and 4 were determined through a comprehensive fine-tuning process. To identify the optimal\nhyperparameters for our models, we utilized Optuna [35], a hyperparameter optimization framework. We defined\na specific search space for each hyperparameter and employed Optuna's Tree-structured Parzen Estimator (TPE)\nsampler to navigate this space effectively. Our primary goal was to maximize the Mean Average Precision (MAP) for\nthe temporal link prediction task, a critical metric for assessing the performance of dynamic graph embedding models."}, {"title": "4.3. Temporal link prediction results", "content": "To evaluate node embeddings for link prediction, we employ a logistic regression classifier that uses concatenated\nnode embeddings to estimate the probability of a link between them. The classifier is trained on node pairs labeled as\nlinked or unlinked, sampled as positive (existing edges) and negative (non-existing edges) from each graph snapshot.\nTo balance the training data, we maintain a ratio of 1:10 for positive to negative samples.\nMean Average Precision (MAP) loss was used for link prediction, which improves link prediction by prioritizing\nthe ranking of true links over non-links. The classifier is trained with the Adam optimizer and a learning rate of\n1e-3. Moreover, the performance of the link prediction classifier is evaluated using two key metrics: MAP and Mean\nReciprocal Rank (MRR). MAP measures the average precision across all queries, providing an overall measure of\nlink prediction accuracy. MRR calculates the average of the reciprocal ranks of the first relevant item for each query,\nfocusing on the model's ability to rank true links at the top. The corresponding link prediction results for Reality\nMining, UCI, SBM, Bitcoin and Slashdot datasets, are presented in the Tables 5, 6, 7, 8, and 9, respectively. All\nresults are based on five random initializations.\nReality Mining (Table 5): GDG-Mamba consistently outperforms all other models. This likely stems from its\nability to effectively leverage both node and edge features through GINEConv, which might be crucial for capturing\nthe nuances of human contact patterns.\nUCI (Table 6): GDG-Mamba again demonstrates superior performance. Its success can likely be attributed to\nthe same reasons as in Reality Mining, highlighting its ability to handle highly transient dynamics in online social\ninteractions where edge features play a significant role.\nSBM (Table 7): Both DG-Mamba and GDG-Mamba achieve comparable performance to ST-TransformerG2G,\nshowing that Mamba-based models can effectively capture temporal patterns even in synthetic datasets. This suggests\nthat the inductive biases of state-space models are well-suited for learning the evolving community structures in SBM.\nBitcoin-OTC (Table 8): GDG-Mamba shows the best performance, likely because it effectively captures the com-\nplex transaction patterns and evolving trust relationships in the Bitcoin network by incorporating edge features.\nSlashdot (Table 9): ST-TransformerG2G exhibits the best performance, indicating the benefits of incorporating\nGCNs for learning spatial dependencies in social networks. However, GDG-Mamba shows lower performance, po-\ntentially due to overfitting to the limited number of timestamps and the sparsity of the dataset. The edge features in\nSlashdot might also be less informative for the link prediction task, making the added complexity of GINEConv less\nbeneficial.\nThe bar plot figure below offers a detailed visualization of each model's best performance in the temporal link\nprediction task, comparing the capabilities of our prior two models-DynG2G and TransformerG2G\u2014with three new\nproposed models: ST-TransformerG2G, DG-Mamba, and GDG-Mamba. This comparison highlights the respective\nstrengths of each model in handling dynamic graph data. For a comprehensive view of the results, including error bars\nthat illustrate variability based on 5 random initializations, see Figure 4.\nNote that the DG-Mamba and GDG-Mamba models demonstrated superior and more stable (i.e., smaller standard\ndeviations) performance in link prediction across most benchmarks, except for the Slashdot dataset, when compared\nto the ST-TransformerG2G and TransformerG2G models. The underperformance of GDG-Mamba with GINEConv\non the Slashdot dataset could be due to a combination of factors related to the dataset's unique characteristics and\nthe model's complexity. The limited number of timestamps in Slashdot (only 12) might make GDG-Mamba, which\nincorporates the additional complexity of GINEConv for integrating edge features, prone to overfitting. Moreover, the\nsparsity and specific temporal dynamics of interactions within the Slashdot dataset could challenge GDG-Mamba's\nability to effectively capture the relevant information necessary for accurate link prediction. Moreover, the optimal\nlookback value that achieves the best performance varies for each benchmark, reflecting the model's ability to capture\nthe inherent temporal dynamics specific to each dataset."}, {"title": "Furthermore, we compare the performance of our three proposed models against six baseline models in link\nprediction tasks. As shown in Table 10, the Mamba-based models (DG-Mamba and GDG-Mamba) consistently\noutperformed transformer-based approaches and other baselines in temporal link prediction tasks, particularly for the\nBitcoin, UCI, and Reality Mining datasets, with highly transient dynamics. However, due to the limited number of\ntimesteps in the Slashdot dataset, the Mamba-based models achieved lower performance compared to the transformer-\nbased models. With the SBM dataset, our proposed models achieved similar performance, as SBM represents a stable\ntemporal graph.", "content": "DynGEM [8]: This is a dynamic graph embedding method that uses autoencoders to obtain graph embeddings.\nThe weights of the previous timestamp are used to initialize the weights of the current timestamp, and thereby\nmakes the training process faster.\ndyngraph2vecAE [36]: This method uses temporal information in dynamic graphs to obtain embeddings using\nan autoencoder. The encoder takes in the historical information of the graph, and the decoder reconstructs the\ngraph at the next timestamp, and the latent vector corresponds to the embedding of the current timestamp.\ndyngraph2vecAERNN [36]: This method is similar to dyngraph2vecAE, except that the feed-forward neural\nnetwork in the encoder is replaced by LSTM layers. This method also incorporates historical information while\npredicting graph embeddings.\nEvolveGCN [10]: A graph convolution network (GCN) forms the core of this model. A recurrent neural network\n(RNN) is used to evolve the GCN parameters along time.\nROLAND [12]: A graph neural network is combined with GRU to learn the temporal dynamics. The GNN has\nattention layers to capture the spatial-level node importance."}, {"title": "4.4. Analysis of the State Matrix A in the Mamba-based Models", "content": "To gain deeper understanding into how the Mamba model captures temporal dependencies in dynamic graphs, we\nanalyze the learned state transition matrix A, which plays a crucial role in the state-space model formulation and in\ncapturing temporal dynamics. In our prior work [15"}]}