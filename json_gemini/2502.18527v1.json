{"title": "GOD model:\nPrivacy Preserved AI School for Personal Assistant", "authors": ["PIN AI Team"], "abstract": "Personal AI assistants (e.g., Apple Intelligence, Meta AI) offer proactive recom-\nmendations that simplify everyday tasks, but their reliance on sensitive user data\nraises concerns about privacy and trust. To address these challenges, we introduce\nthe Guardian of Data (GOD), a secure, privacy-preserving framework for training\nand evaluating AI assistants directly on-device. Unlike traditional benchmarks,\nthe GOD model measures how well assistants can anticipate user needs such\nas suggesting gifts\u2014while protecting user data and autonomy. Functioning like\nan \"AI school\", it addresses the cold start problem by simulating user queries and\nemploying a curriculum-based approach to refine the performance of each assistant.\nRunning within a Trusted Execution Environment (TEE), it safeguards user data\nwhile applying reinforcement and imitation learning to refine AI recommendations.\nA token-based incentive system encourages users to share data securely, creat-\ning a data flywheel that drives continuous improvement. By integrating privacy,\npersonalization, and trust, the GOD model provides a scalable, responsible path\nfor advancing personal AI assistants. For community collaboration, part of the\nframework is open-sourced at https://github.com/PIN-AI/God-Model.", "sections": [{"title": "1 Introduction", "content": "On-device AI agents show promise for contextual understanding, privacy controls, and responsive\nuser interactions. However, as these agents move beyond responding to explicit user queries and\nbegin to anticipate future needs, they face a trust barrier. Recommending a birthday gift, notifying\nusers of an upcoming flight deal, or suggesting a break from work can all be very helpful, but these\nproactive features depend on gathering, interpreting, and sometimes inferring from private data. Users\nmay not fully expect such inferences, raising concerns about transparency, autonomy, and responsible\ndata handling.\nTo address these challenges, we present the Guardian of Data (GOD) model, a secure framework\ndesigned to evaluate and improve personal AI on device. Unlike generic benchmarks that focus on\ngeneral knowledge or factual accuracy, the GOD model focuses on proactive tasks that leverage\nsensitive data, both to measure performance and to ensure responsible usage. By operating within a\nTrusted Execution Environment (TEE), the framework maintains strong privacy guarantees while\nperforming quality assessments. The GOD model generates its own 'exam papers', curriculum-based\nqueries of varying difficulty, and distributes them to personal AI agents around the world. The\nanswers of each assistant are based on different types of user data (e.g., emails, social networks,\nreceipts), yet the classification and analysis occur securely, with the raw user data never leaving the\ndevice. Over time, the model can act as a teacher, offering step-by-step demonstrations to help each\non-device assistant refine its behavior.\nA key advantage of this approach is its ability to mitigate the cold start problem in personalization\nand recommendation. The GOD model generates realistic user queries and usage patterns, enabling\nnewly created personal AIs to train before engaging with real users. Additionally, a token-based\nincentive mechanism promotes secure data sharing, driving a data flywheel effect. This continuous\ncycle of data collection, evaluation, and refinement enhances personalization while preserving user\nautonomy and security. Below, we outline our main contributions:\n1. TEE-Based Secure Evaluation: We introduce a trusted execution environment (TEE) that\nenables robust scoring and feedback while safeguarding user data from unauthorized access.\n2. Curriculum-Based Assessment: Our approach evaluates AI proficiency by starting with\nfactual queries and progressively introducing more complex, context-rich, preference-based\nrecommendations.\n3. Data Value Estimation: We propose a structured method to quantify the impact of personal\ndata on recommendation quality, helping stakeholders balance personalization benefits with\nprivacy concerns.\n4. Anti-Gaming Safeguards: By implementing cross-verification, identity checks, and anti-fraud\nmechanisms, we ensure the integrity of test results and rewards, promoting fair and transparent\nevaluation.\nThese elements collectively establish a framework for both assessing and enhancing on-device AI\nsystems, enabling them to deliver proactive support without undermining user trust."}, {"title": "2 Related Work", "content": "The evaluation of personalized large language models (LLMs) has been a growing area of research,\nwith existing works primarily focusing on measuring how well models adapt to user-specific contexts,\nstyles, or preferences. However, most benchmarks emphasize reactive personalization\u2014assessing\nhow well an LLM responds to explicit user prompts\u2014rather than evaluating proactive personalization,\nwhich is crucial for personal AI assistants."}, {"title": "2.1 Existing Personalization Benchmarks", "content": "Benchmarks such as LaMP [1] and LongLaMP [2] provide user-specific ground-truth datasets,\nenabling direct comparisons between model outputs and actual user-authored content. Others, such\nas PersoBench [3] and PrefEval [4], test persona consistency and adherence to stated preferences.\nAlthough these datasets help quantify personalization gains, they rely on static corpora, lack real-time\nadaptation, and do not assess privacy-preserving or on-device AI performance."}, {"title": "2.2 Limitations of Existing Evaluation Metrics", "content": "Traditional evaluation metrics for personalization primarily assess response accuracy, coherence,\nand user preference alignment. Intrinsic metrics such as BLEU [9], ROUGE [10], and METEOR\n[11] measure textual similarity but do not account for the proactive nature of personal AI assistants.\nMore personalized evaluation methods, such as Win Rate [12], Hits@K [13], and Word Mover's\nDistance [14], provide better alignment with user preferences, but still focus on reactive tasks rather\nthan anticipatory interactions.\nExtrinsic metrics, including recall, precision, and normalized discounted cumulative gain\n(NDCG) [15], are widely used in recommendation systems but assume centralized data aggre-\ngation, making them impractical for privacy-preserving, on-device personalization. Furthermore,\nthese metrics do not quantify the actual value of user data in personalization, a critical aspect to\nbalance personalization with privacy."}, {"title": "3 System Components", "content": "We outline four key components\u2014(a) the GOD model, (b) the Personal AI, (c) Data Connectors, and\n(d) the HAT Node that collectively preserve user privacy while enabling proactive AI recommenda-\ntions."}, {"title": "4 Evaluation Methodology", "content": "Evaluating the ability of a Personal AI to leverage private data for proactive recommendations\nrequires a structured, transparent, and privacy-preserving approach. The GOD model accomplishes\nthis through a curriculum-based assessment, a well-defined scoring function, and explicit methods to\nquantify the value of personal data."}, {"title": "4.1 Curriculum-Based Assessment", "content": "The GOD model follows a curriculum evaluation that begins with basic data retrieval tasks and\nadvances to more complex, preference-based queries. This progression reveals how well the AI\nevolves as it tackles increasingly sophisticated user queries:\n\u2022 Level 1 (Easy): Simple factual recall (e.g., \"What was the subject of the user's last email?\").\n\u2022 Level 2 (Medium): Cross-referencing tasks that require integrating data from multiple\nsources (e.g., linking calendar events with email content).\n\u2022 Level 3 (Hard): Context-rich queries that demand personalized recommendations (e.g.,\n\"Suggest a restaurant based on dietary preferences and location history.\")."}, {"title": "4.2 Scoring Function", "content": "The GOD model employs a scoring function that combines coverage, quality, and freshness:\nTotal Score = $w_c$ \u00b7 Coverage + $w_q$ \u2022 Quality + $w_F$ \u00b7 Freshness."}, {"title": "4.3 Measuring the Value of Personal Data", "content": "A key objective of our framework is to quantify the tangible impact of personal data on AI recom-\nmendations. By comparing AI performance under personalized and baseline conditions, the GOD\nmodel highlights the value of user-specific data contributes, helping developers and users balance\npersonalization with privacy.\n1. Task Selection: Define common AI use cases (e.g., trip planning, email summarization).\n2. Dual Execution: Run AI tests in two configurations:\n\u2022 Personalized Access: AI utilizes user-specific data (purchase history, emails, prefer-\nences).\n\u2022 Baseline (Control): AI operates without personal data, relying only on general knowl-\nedge.\n3. Output Comparison: Assess results using:\n\u2022 User Feedback: Blind ratings on relevance and satisfaction.\n\u2022 Automated Metrics: Evaluation using measures like ROUGE and efficiency scores.\n\u2022 A/B Testing: Present different outputs to user groups to gauge engagement.\n4. Value Quantification: Compute the personalization gain by comparing outcomes across both\nsettings."}, {"title": "4.4 Progressive Curriculum Examples", "content": "In Figure 4, 5, and 6, we provide concrete examples at Levels 1, 2, and 3 to illustrate the GOD\nmodel's curriculum-based progression, demonstrating how AI competency increases in complexity.\nAs tasks escalate in complexity, the GOD model assesses both factual accuracy and the AI's ability to\nintegrate, infer, and proactively assist users."}, {"title": "5 Anti-Gaming Protections", "content": "The GOD model rewards users whose Personal AI effectively utilizes their data. However, without\nsafeguards, the system could be exploited through fake records or coordinated fraud. To prevent\nthis, the GOD model enforces a multi-layered anti-gaming strategy by verifying users through KYC,\nvalidating user data, and confirming provider transactions. This ensures fairness, privacy, and trust.\nBelow are the key measures in place."}, {"title": "5.1 High-Level Overview", "content": "\u2022 User Verification (KYC)\nA HAT Node verifies user identities using on-chain records or real-world documents.\nHigher reward tiers are available to users with stronger verification, discouraging fake\naccounts and Sybil attacks.\n\u2022 User Data Validation\nReal-Time Data: New events (e.g., ride receipts, email notifications) are instantly verified\nin a Trusted Execution Environment (TEE), preventing backdated or fabricated records."}, {"title": "5.2 Further Safeguards and Penalties", "content": "Privacy-Preserving Proof in a TEE. All verification occurs inside a TEE, ensuring that raw user\ndata stays on the device. Only pass/fail results or summarized trust scores are shared, preventing\nexternal access to personal data.\nVerifiable vs. Non-Verifiable Data. Data that can be confirmed via APIs or cryptographic\nproofs such as airline tickets, blockchain transactions, and platform-issued receipts-carry more\nweight than free-text or self-reported information. This discourages unverifiable submissions.\nCross-Verification and Random Data Injection. The system checks user data against real-time,\nhistorical, and provider records to identify inconsistencies. It also injects fake prompts (e.g., non-\nexistent coupons) to test if a Personal AI incorrectly accepts false data. Previously verified data may\nbe rechecked; contradictions result in stronger penalties than initial errors.\nCollaborative Fraud Mitigation. Users attempting to manipulate the system through coordinated\nfake transactions are detected through external data sources and pattern analysis. When suspicious\nbehavior is found, the GOD model halts new data ingestion for those accounts and triggers further\ninvestigation."}, {"title": "6 Future Evolution: A Simulated AI School with RL-Based Curriculum", "content": "Beyond static exams, the GOD model aims to build a dynamic reinforcement learning (RL) system\nwhere on-device personal AIs continuously improve their proactive support. In this phase, personal\nAls progress from simple recall and basic suggestions to deeper reasoning and context awareness.\nThey learn to understand user schedules, preferences, and habits and to use advanced tools (e.g.,\nfunction calling for restaurant bookings) without sending raw data off the device. As they develop,\nthese Als can better predict needs such as scheduling, meal planning, or entertainment while keeping\ndata private."}, {"title": "6.1 Online Learning from Personalized Human Feedback", "content": "Online learning from personalized human feedback helps language models adjust to individual\npreferences on edge devices with limited power. Recent methods tailor responses efficiently without\nheavy user input.\nPersonalized Reinforcement Learning from Human Feedback (P-RLHF) lets models match\nindividual needs instead of assuming one size fits all. Using user IDs and feedback histories, P-RLHF\nbuilds unique profiles without explicit instructions. It also uses personalized Direct Preference\nOptimization (P-DPO) to generate responses that fit diverse preferences. This approach works well\nfor both familiar and new users, showing strong adaptability and scalability [16].\nGiven a personalized preference dataset\n$Z_p = \\{(x_i, y_{i,1}, y_{i,2}, u_i)\\}_{i=1}^N$,\nwhere u \u2208 U is user information, $x_i$ is the prompt, and $y_{i,1} \\succ y_{i,2}$ means that user $u_i$ prefers $y_{i,1}$\nover $y_{i,2}$, P-RLHF learns:\n\u2022 A user model $f_\\theta: U \\rightarrow \\mathbb{R}^{T_U \\times d}$ that maps user data to embeddings $e_u$\n\u2022 A personalized language model $P_\\varphi(y | x, u)$ that generates responses based on both prompt\nx and user u\nThe objective for P-DPO is:\n$\\min -\\mathbb{E}_{(x,y_1,y_2,u)\\sim Z_p} [\\alpha \\log \\sigma(\\beta \\log \\frac{P_\\varphi(y_1|x,u)}{P_{\\varphi_0}(y_1|x)}) +$\n$(1 - \\alpha) \\log \\sigma(\\beta \\log \\frac{P_{\\varphi_0}(y_1|x, u_0)}{P_{\\varphi_0}(y_1|x)}) - \\beta \\log \\frac{P_\\varphi(y_2|x, u)}{P_{\\varphi_0}(y_2|x)}) -$\n$\\beta \\log \\frac{P_{\\varphi_0}(y_2|x, u_0)}{P_{\\varphi_0}(y_2|x)})]$\nHere, \u03b1 \u2208 [0,1] balances specific and general preferences, \u03b2 > 0 controls deviation from the\npre-trained model $P_0$, and $u_0$ denotes no user data. This setup supports learning personalized and\ngeneric response patterns.\nKnowledge Graph Tuning (KGT) provides real-time personalization using knowledge graphs\nwithout changing model parameters. KGT extracts triples of personalized knowledge from user\nqueries and feedback to build tailored representations. By optimizing the graph instead of model\nweights, it reduces memory use and latency while boosting personalization. KGT has improved\nefficiency and scalability by up to 61% compared to traditional methods [17].\nCritique and Revise (CnR) refines the responses using natural language feedback. Rather than\nrelying solely on ranking signals, CnR learns from both praise and criticism, improving response\nquality by up to 65.9% after just a few feedback rounds. This method is ideal for data-limited\nenvironments and edge LLMs where efficiency and adaptability are crucial [18]."}, {"title": "6.2 Imitation Learning and RL in Practice", "content": "The framework uses a two-step approach that combines imitation learning (IL) and reinforcement\nlearning (RL):\n1. Teacher Demonstrations. A larger LLM within the TEE shows how to handle tasks that\ncombine different data sources (e.g., calendars and dietary restrictions) with user context\n(e.g., budget or location). The on-device AI watches these examples and mimics them to\nform an initial policy.\n2. Critic and Policy Learning. A separate critic model in the TEE provides reward signals\nbased on user satisfaction. It rewards helpful suggestions and penalizes intrusive ones. Using\nmethods like P-DPO, the on-device AI gradually refines its policy for long-term benefit.\nThese steps help each personal AI improve over time, balancing timely and relevant suggestions with\navoiding unnecessary notifications. Entropy-based exploration further encourages the AI to try new\nstrategies instead of sticking with safe but limited ones."}, {"title": "6.3 Example: Proactive Dinner Reservations", "content": "Consider a user who typically dines out on Fridays. The GOD model first detects this pattern from\nthe user's data (e.g., calendar events and receipts). Next, the LLM teacher outlines a step-by-step\nprocess: checking restaurant availability, considering dietary needs, and timing the suggestion to\navoid interrupting work. The on-device AI mimics this process and refines it using RL, guided by\nuser feedback. Over time, AI becomes skilled at offering the right dining options at the right time,\nwhile keeping sensitive data on device.\nThrough continuous learning, the GOD model evolves from static Q&A to an \"AI school\". Personal\nAls grow with user needs, maintain privacy, and learn from high-level feedback. Ultimately, they can\npredict, assist, and coordinate various tasks, setting a new standard for proactive, privacy-focused\nintelligence."}, {"title": "7 Conclusion", "content": "We have presented the Guardian of Data (GOD) framework to securely evaluate, train, and improve\npersonal AI systems on devices aimed at proactive recommendations. By introducing curriculum-\nbased tests, trusted execution for data safety, and anti-gaming mechanisms, the GOD model addresses\nthe battle between personalization and privacy. It clarifies how personal data improves recommenda-\ntion quality and provides a blueprint for iterative learning, both through pre-deployment simulation\nand post-deployment adaptation.\nLooking ahead, we plan to refine preference modeling, explore more advanced reinforcement learning\ntechniques, and incorporate metrics that better capture long-term user satisfaction. These extensions\nwill help personal AIs seamlessly anticipate user needs, identify meaningful opportunities for assis-\ntance, and remain transparent and respectful of user autonomy. In doing so, the GOD model paves\nthe way for truly trustworthy AI companions, ones that operate as informed partners while preserving\nthe sanctity of personal data."}, {"title": "A Appendix", "content": "A.1 Example Prompt for GOD model Exam Question List\nListing 1: Prompt for instruction annotation\n## **Hand-Coded Question Design for Personal AI Memory System**\n### **Objective**\nEach question must meet the following criteria:\n1. **Easy to Grade:**\nThe question should be graded by an **Intel SGX TEE compute\nengine**.\nGrading involves checking factual existence in the **user's data\n** (via API or data dump).\nThe system searches JSON data, applies hard-coded Python\nsummarization, and confirms findings using a **local LLM R1 1.5B\nmodel**.\nThe grading process ensures:\n**YES** Data exists and matches source.\n**NO** Data is missing or unavailable, and a **TEE Grader GOD\nmodel** suggests new data sources.\n2. **Useful Personalization for Transactions: **\n3.\nThe question should predict **future transactions** for AI\nrecommendations.\nExample: *\"Your friend's birthday is in 7 days. Do you want to\nbuy a gift?\"*\nAI should have **factual personal information** to enhance user\nexperience.\n* User Impression (\"Wow\" Factor):**\nThe AI should feel like a **loyal best friend & executive\nassistant**.\nIt should infer personal details **without explicit user input**,\nbased on **past behavior history**.\n### **Question Grading Requirements**\n**Automated Grading Simplicity:**\nThe factual data must be **verifiable** by a less-smart TEE grader\nAvoid **hallucinated/made-up** data by the LLM.\nIf **data is missing**, the **GOD model** should recommend a **new\ndata connector**.\n## **Available Data Sources**\nAll questions must be based **only** on the following data sources.\n### **Categories & Data Sources**\n1. **Social**\n**Twitter: ** Username & basic info from OAuth.\n**Discord: ** Username, group names, login time.\n**Telegram:** Username & authentication date.\n**Facebook/Instagram/Other:** Extracted from **Gmail receipts**.\n2. **Productivity**\n**Gmail:** All data.\n**Google Calendar: ** All data.\n3. **Daily Life** (from **Gmail receipts** only)\n**Ride services:** Uber, Lyft, Waymo, Didi, Grab.\n**Food delivery:** Uber Eats, DoorDash, Grubhub."}, {"title": "- **Grocery delivery:** Instacart, Amazon Fresh, Costco.", "content": ""}, {"title": "4. **Shopping** (from **Gmail receipts** only)", "content": "**Shopping Apps: ** Amazon, Shopify, Shein, Macy's, Lululemon, etc\n5. **Web3**\n**Crypto Wallets:** MetaMask, Phantom, WalletConnect.\n**On-Chain Data:** Pulled from **3rd-party search API services**.\n6. **Finance** (from **Gmail receipts** only)\n**Stock Brokers:** Robinhood, IB, Futu, Tiger, Charles Schwab,\netc.\n- **Crypto Exchanges:** Coinbase, Binance, Kraken, OKX, Bybit,\nUpbit, KuCoin, Crypto.com, etc.\n7. **AI Native Chat History**\n**Usage history** from ChatGPT, Gemini, DeepSeek, Doubao,\nPerplexity, Character AI, etc."}, {"title": "## **Task Requirements**", "content": "1. **Generate 10 Questions per Data Connector Type** (for each\ncategory).\n2. **Select the Best 8 Questions per Category**\nEnsure questions **represent all data connectors** in the\ncategory.\n3. **Optimize for Personalization, Grading, and Predictive Power**\nQuestions must extract **objective, verifiable, and stable**\npersonal information.\nAvoid **subjective** questions (e.g., *favorite color*, which may\nchange)."}, {"title": "## **Memory Table Structure**", "content": "Each **data connector** generates a **personal memory table**, which\nis:\n**Continuously updated** when new data arrives.\n**Categorized** under **Productivity/Social/Shopping/Finance/Web3/AI\nNative**.\n- **Verified for consistency** (cross-checking between data dump &\nstreaming APIs).\n- **Formatted as Key-Value Pairs with Reference Sources**.\n### **Special Tokens**\n**NE (Non-Exist):** Data does not exist after thorough verification.\n**NA (Not Available):** Data pipeline failure (e.g., parser or LLM\nfailure)."}]}