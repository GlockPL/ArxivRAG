{"title": "MOTION TRACKS: A Unified Representation for Human-Robot Transfer in Few-Shot Imitation Learning", "authors": ["Juntao Ren", "Priya Sundaresan", "Dorsa Sadigh", "Sanjiban Choudhury", "Jeannette Bohg"], "abstract": "Teaching robots to autonomously complete everyday tasks remains a challenge. Imitation Learning (IL) is a powerful approach that imbues robots with skills via demonstrations, but is limited by the labor-intensive process of collecting teleoperated robot data. Human videos offer a scalable alternative, but it remains difficult to directly train IL policies from them due to the lack of robot action labels. To address this, we propose to represent actions as short-horizon 2D trajectories on an image. These actions, or motion tracks, capture the predicted direction of motion for either human hands or robot end-effectors. We instantiate an IL policy called Motion Track Policy (\u039c\u03a4-\u03c0) which receives image observations and outputs motion tracks as actions. By leveraging this unified, cross-embodiment action space, MT-\u03c0 completes tasks with high success given just minutes of human video and limited additional robot demonstrations. At test time, we predict motion tracks from two camera views, recovering 6DoF trajectories via multi-view synthesis. \u039c\u03a4-\u03c0 achieves an average success rate of 86.5% across 4 real-world tasks, outperforming state-of-the-art IL baselines which do not leverage human data or our action space by 40%, and generalizes to scenarios seen only in human videos. Code and videos are available on our website (https://portal-cornell.github.io/motion_track_policy/).", "sections": [{"title": "I. INTRODUCTION", "content": "Imitation learning (IL) is a widely adopted approach for training robot policies from human demonstrations [1, 2]. However, even state-of-the-art IL policies can in some cases require on the order of hundreds [3, 4] or up to tens of thousands [5\u20137] of crowdsourced, teleoperated demonstrations in order to achieve decent performance. These demonstrations are typically collected by teleoperating robots via virtual reality devices [8] or puppeteering interfaces [4, 9, 10]. However, not only are some devices robot-specific and not universally accessible, prolonged teleoperation with them is time-consuming, labor-intensive, and often requires extensive practice before dataset collection can even begin.\nAn alternative approach is robot learning from passive observations, such as videos of humans demonstrating desired tasks. Human video data is far easier to collect at scale, with existing datasets providing thousands of hours of demonstrations [11, 12]. However, these human videos lack robot action labels that are necessary for training imitation learning policies, presenting a challenge in transferring knowledge from human videos to robot policies.\nRecent works have addressed this problem by pretraining visual representations on human data [11, 13, 14]. However,"}, {"title": "II. RELATED WORK", "content": "Imitation Learning: Imitation learning (IL) is a framework where an agent learns to mimic expert behavior by observing demonstrations [1, 23, 24]. This paradigm has been widely applied in robotics, where policies are trained to map sensory inputs to motor actions [25\u201327]. Recent advancements in policy architectures, such as Diffusion Policy [3] and Action Chunking Transformer (ACT) [4], have significantly improved the real-world applicability of IL. However, these approaches still often require a significant amount of teleoperated data to achieve decent performance.\nSeveral works have aimed to streamline teleoperation with easy-to-use interfaces like puppeteering [4, 10], mirroring of tracked human hands [28\u201330], or intuitive hand-held devices [31]. These setups can make data collection more convenient to scale. Large-scale crowdsourcing efforts have recently brought some open-source robotics datasets to the order of more than 80K trajectories [7, 32]. Nonetheless, in many practical scenarios, collecting data at this scale is infeasible, and learning performant policies from such diverse data remains an open challenge [6, 33]. This drives the need for training policies in the low-robot-data regime.\nSample-Efficient State-Action Representations: To address the issue of sample inefficiency in IL, recent works propose leveraging alternative input modalities beyond images, such as point clouds [34\u201336], radiance fields [37, 38], or voxel grids [39, 40]. These representations better capture 3D geometric features in visual scenes, which can lead to decent policies from fewer demonstrations. However, these approaches are only compatible with robot-only data consisting of RGB-D observations, along with calibrated camera extrinsics. Consequently, images remain a prevalent choice for policy inputs as they allow for fewer assumptions during data collection and thus are more suitable to be used with cross-embodiment data like human video.\nOne effective strategy for methods that take images as input is to additionally condition on 2D representations like optical flow [40\u201342], object segmentation masks [43, 44], bounding boxes [32], or pixelwise object tracks [17, 18, 45]. While these methods focus on reparameterizing the inputs to IL policies, the action space remains rooted in robot proprioceptive states. This formulation limits the policy to learning actions only present in the teleoperated data. We instead propose an embodiment-agnostic, image-based action space which encourages behaviors captured in robot data as well as those that may only be present in human videos.\nLearning from Human Video\nA. Pretrained Representations on Internet-Scale Data\nGiven the challenges associated with collecting teleoperated robot data, human video is a popular alternative source of data that is easier to collect and more widely available. Several approaches use large-scale internet human video datasets [11, 12, 46, 47] to pretrain visual representations for robot policy learning [13, 14, 48]. However, prior works have found these pretrained representations to be brittle when used out-of-the-box or finetuned for specific tasks due to the sheer diversity of the datasets they were trained on [15, 16].\nB. Implicit Priors from Human Video\nAn alternative approach involves collecting task-specific human video data, along with a smaller amount of teleoperated robot data, and jointly learning on the hybrid dataset [20, 21, 49]. The primary challenge with this approach is overcoming the embodiment gap, as human video does not provide robot action labels. To address this, some methods attempt to extract priors from human video that can guide robot learning, such as shared human-robot latent embeddings with which to condition robot policies [20, 50]"}, {"title": "III. PROBLEM FORMULATION", "content": "We aim to learn a visuomotor policy trained on mostly human video and a small amount of robot demonstrations. To do so, we extract actions from a shared representation of pixel-level keypoints and train a policy that outputs actions directly in image space, making the pipeline compatible with either embodiment.\nWe assume access to a joint, single-task dataset D = Dhuman \u222a Drobot where we have access to \u2265 1 camera during data collection, and 2 cameras during test-time. During data collection, we do not make specific assumptions on the viewpoints, but at test-time we assume access to known extrinsics for two of the cameras. The majority of the dataset consists of easy-to-collect human demonstrations Dhuman, while a small set is of teleoperated robot demonstrations Drobot. While we do not assume human and robot demonstrations to be paired (i.e., starting from identical initial object states), we do assume that both embodiments perform similar motions given similar states. During data collection, each demonstration is represented by {(I(i)t, s(i)t, gt)}Nt=1, where t indexes time and i indexes the viewpoint (i.e., Camera 1/2). I(i)t represents the RGB image captured by camera i at time t, and s(i)t = {(uk, vk)}(i)k=1 represents the pixels of k keypoints on the end-effector (human hand or robot gripper) in the image I(i)t. For prehensile tasks, g(i)t \u2208 {0,1} indicates whether a (human/robot) grasp occurs at t.\nGiven an image I(i)t and the corresponding k end-effector keypoints s(i)t, our objective is to learn a keypoint-conditioned motion track network\n$$ (s^{(i)}_{t+1:t+H}, g^{(i)}_{t+1:t+H}) \\sim \\pi_{\\theta}(\\cdot | I^{(i)}_t, s^{(i)}_t). $$"}, {"title": "IV. APPROACH", "content": "We present \u039c\u03a4-\u03c0 (Motion Track Policy), a framework designed to unify human and robot demonstrations by predicting actions for visuomotor control in image-space. Specifically, we map both human and robot demonstrations to a common representation of 2D keypoints on a manipulator in an image, and train a policy to predict the future pixel locations of these keypoints. By casting both human and robot demonstrations to this unified action space, co-training on both human and robot datasets encourages transfer between human and robot motions. At inference time, we only have to map these predictions in image space into 3D space to recover robot actions.\nA. Data Preprocessing\nRobot Demonstrations. To collect robot demonstrations, we assume access to a workspace with \u2265 1 calibrated camera (with known camera-to-robot extrinsics) and robot proprioceptive states. For each demonstration, we capture a trajectory of images I(i)t from each available viewpoint. Using the robot\u2019s end-effector position and the calibrated extrinsics, we project the 3D position of the end-effector into the 2D image plane, yielding k keypoints s(i)t = {(uk, vk)}(i)k=1. In practice, we take k = 5, giving us two points per finger on the gripper, and one in the center (Fig. 2). We choose this positioning of points as it lends itself better to gripper positioning for grasping actions. The gripper\u2019s open/close state is represented as a binary grasp variable g(i)t \u2208 {0,1}.\nHuman Demonstrations. Human demonstrations are collected using RGB cameras without needing access to calibrated extrinsics, making it possible to leverage large-scale human video datasets. We use HaMeR [22], an off-the-shelf hand pose detector, to extract a set of 21 keypoints s(i)t = {(uj, vj)}(i)j=1. To roughly match the structure of the robot gripper, we select a subset of k = 5 keypoints: one on the wrist and two each on the thumb and index finger.\nTo infer per-timestep grasp actions from human videos, we use a heuristic based on the proximity of hand keypoints to the object(s) being manipulated. For each task, we first obtain a pixel-wise mask of the object using GroundingDINO [62] and SAM-v2 [63]. Then, if the number of pixels between the object mask and the keypoints on the thumb plus any one of the other fingertips falls below some threshold, we set g(i)t = 1. By loosely matching the positioning and ordering of keypoints between the human hand and robot gripper, we create an explicit correspondence between human and robot action representations in the image plane.\nB. Training Pipeline\nKeypoint Retargeting Network. Despite the explicit correspondences between points on the human hand and robot gripper, the embodiment gap (e.g. size differences between hand and gripper) still induces notable distinctness in the spacing of keypoints. Directly conditioning on these points may encourage the policy to over-index on these differences, generate distinct track predictions for each embodiment, and thus fail to produce actions captured in the human demonstrations. To address this, we introduce a Keypoint Retargeting Network (Fig. 2) that maps the robot keypoints to positions more aligned with human-hand keypoints.\nFor each human demonstration, we add uniform noise to all keypoints except for an anchor point (e.g., the wrist). A small MLP is trained to map these noisy keypoints back to their original positions. Once trained, the network is frozen and used during both training and testing. Since this network is trained only to map a noised version of keypoints back to their original spacing, it is compatible with either human or robot keypoint as the input. That is, any robot\u2019s keypoints will be treated as \u201cnoisy\u201d and be mapped to positions that more closely resemble those of the human hand. For human keypoints, this network acts as an identity map.\nMotion Track Network. We use a Diffusion Policy objective [3] to train our policy head that predicts motion tracks. The input to the network is a concatenation of image embeddings from a pre-trained ResNet encoder [64] and the current keypoint positions s(i)t in pixel space. The network then predicts offsets to each of the 2D keypoints as well as the gripper state g(i)t for each future timestep over a short horizon H. Importantly, the model takes a single viewpoint image at a time, but is agnostic to the viewpoint pose, making it adaptable to Internet-scale human videos that often come from a single RGB viewpoint.\nImage Representation Alignment. To discourage the policy from over-attending to the visual differences between human and robot input images, we use two auxiliary losses to encourage alignment of their visual embeddings:\n\u2022 KL: KL-Divergence Loss from [20] to minimize the divergence between human and robot feature distributions\n\u2022 DA: Domain Adaptation loss from [65], which encourages the policy to produce indistinguishable human/robot embeddings by fooling a discriminator.\nThe total training loss is a combination of the original diffusion policy objective and a weighted sum over the auxiliary losses laux = \u03bbKLlKL + \u03bbDAlDA where lKL and lDA are scaling factors. In practice, we use \u03bbKL = 1.0, and tune \u03bbDA \u2208 [0,1] depending on the proportion of human to robot demonstrations.\nC. Action Inference\nAt inference, we assume access to 2 cameras with known extrinsics. We first leverage the camera-to-robot extrinsics to get a set of keypoints s(i)t of the robot gripper per viewpoint. We then pass these keypoints through the frozen keypoint retargeting network to obtain s(i)t which are positioned more similarly to what has been seen during training. Then, s(i)t is concatenated with the image embedding of I(i)t and passed as input to the motion track network \u03c0\u03b8 to obtain the predicted tracks in each view, along with the gripper state gt+1:t+H at each step (Fig. 2). Using stereo triangulation with known extrinsics, we recover the 3D positions of tracks at each timestep (Fig. 3).\nGiven the current and predicted future 3D keypoints, we then compute a rigid transformation consisting of a rotation matrix R \u2208 SO(3) and translation vector t \u2208 R3 that"}, {"title": "V. EXPERIMENTS", "content": "Our goal is to evaluate to what extent MT-\u03c0 can benefit from leveraging human video, the impact of our motion tracks action space, and its generalization capabilities.\nA. Experimental Setup\nWe evaluate MT-\u03c0 on a suite of table-top tasks against two commonly used image-based IL algorithms: Diffusion Policy (DP) [3] and ACT [4]. All algorithms are trained from 25 teleoperated robot demonstrations. For Diffusion Policy and ACT, we use the same output space as was chosen in the original implementation, which are 6DoF end-effector delta commands. Further, we equip all baselines with observations from an additional wrist camera (which we found to improve performance). \u039c\u03a4-\u03c0 only receives the two third-person viewpoints, but is provided with roughly 10 minutes of additional human video demonstration per task. We highlight the differences across methods below:\nB. Key Results and Findings\nDoes \u039c\u03a4-\u03c0 outperform baselines which do not leverage human video in the low robot data regime?\nWe first compare MT-\u03c0 to Diffusion Policy (DP) [3] and Action Chunking with Transformers (ACT) [4] on four real-world manipulation tasks: Fork on Plate, Fold Cloth, Serve Egg, and Put Away Socks (Fig. 1). As shown in Table II, \u039c\u03a4-\u03c0 significantly outperforms both baselines in terms of success rate. This result suggests that MT-\u03c0\u2019s ability to leverage human video demonstrations contributes significantly to more robust executions, especially when human videos may capture a broader diversity of motions than robot data. Indeed, due to the extremely low amounts of robot demonstrations (25 trajectories), DP and ACT have difficulty generalizing to small changes in the starting state distribution and throughout rollouts. We note that as we scale up the amount of robot demonstrations (Fig. 4), or when the task is simple enough such that the reset distribution is fully covered (Fig. 5), the performances of DP and ACT reach a much higher percentage. The performance of all policies across these tasks is best understood via videos on our project website (https://portal-cornell.github.io/motion_track_policy/).\nHow sample-efficient are motion tracks as an action representation when trained only on robot data, and how much additional value do human videos provide?\nNext, we study a) whether predicting actions in 2D pixel-space leads to greater sample efficiency even with just robot data, and b) to what extent MT-\u03c0 is able to benefit from the inclusion of human video demonstrations. We compare \u039c\u03a4-\u03c0\u2019s success under varying amounts of robot and human data on a medium-complexity task, Serve Egg, in which the goal is to pick up a pan with a fried egg from a stove top and place it on a small plate roughly the same diameter as the pan. Using a success heatmap in Fig. 4, we visualize the performance of MT-\u03c0 with varying amounts of human and robot demonstrations. We compare to DP [3] and ACT [4] which are trained on only robot demonstrations and predict 6DoF end-effector deltas.\nWe note first that MT-\u03c0 without any human demonstrations matches the success rates of DP and ACT given the same amount of robot demonstrations, suggesting that predicting actions in image-space is a scalable action representation even with just robot data. More interestingly, \u039c\u03a4-\u03c0 matches the performance of baselines despite using 40% less minutes of robot demonstrations by leveraging ~ 10 minutes of human demonstrations. The trends of this plot further suggest that even for a fixed, small amount of teleoperated robot demonstrations, \u039c\u03a4-\u03c0 can obtain noticeably higher policy performance simply by scaling up human video alone on the order of just minutes.\nCan MT-7 generalize to motions and objects only present in human video data?\nA benefit of motion tracks as a representation is that they allow for positive transfer of motions captured in human demonstrations to an embodied agent. This is enabled by explicitly representing human motions within our action space, instead of only implicitly (i.e. via latent embeddings). As a result, the learned policy is no longer restricted to the coverage of actions present in the robot demonstrations. To illustrate this, consider a simple task of closing a drawer, where Drobot contains only demonstrations of the drawer being closed to the right, whereas Dhuman contains demonstrations of the drawer being closed to both the left and right."}, {"title": "VI. LIMITATIONS AND FAILURE MODES", "content": "While MT-\u03c0 demonstrates sample-efficiency, generalization, and reliable performance for the most part, our policy is not without failures. Namely, our policy currently makes predictions on one image at a time, and handles different viewpoints independently. This does not explicitly enforce consistency between tracks across separate views, which can lead to triangulation errors that produce imprecise actions. We try to ensure that teleoperated demonstrations are as unimodal as possible to encourage consistency in motion recovery. In the future, we can consider more explicitly enforcing viewpoint consistency via auxiliary projection/deprojection losses. Further, as a motion-centric method, our approach remains sensitive to noise in human video inputs. This sensitivity currently limits our ability to handle truly in-the-wild videos that feature drastic viewpoint shifts, egocentric motion, or quick temporal changes. While hand tracking is a fairly reliable part of our pipeline, detection of human grasps remains an open challenge. We employ a heuristic approach at present, leveraging foundation models to infer when hands and objects are in contact. Due to some imprecision in ground truth human grasp labels, our policy occasionally prematurely or imprecisely grasps objects. Nevertheless, our framework is designed with modularity in mind, allowing us to incorporate future advancements in hand perception."}, {"title": "VII. DISCUSSION", "content": "In this paper, we propose Motion Track Policy (\u039c\u03a4-\u03c0), a novel and sample-efficient imitation learning (IL) algorithm that introduces a new cross-embodiment action space for robotic manipulation. \u039c\u03a4-\u03c0 forecasts the future movement of manipulators via 2D trajectory predictions on images, which is feasible for both human hands and robot end-effectors. Despite this simplified representation, the approach enables full recovery of 6DoF positional and rotational robot actions via 3D reconstruction from corresponding sets of 2D tracks captured from two different views. Our empirical results show that \u039c\u03a4-\u03c0 outperforms state-of-the-art IL methods that omit human data or our action space on a suite of 4 real-world tasks, improving performance by 40% on average. One of the key benefits is its compatibility with various embodiments, allowing us to rely primarily on easily accessible human video data collected in minutes, while requiring only tens of robot demonstrations for a given task. This drastically reduces the data burden typically associated with IL, while enabling generalization to novel scenarios present only in human video. In the future, we hope to extend MT-\u03c0 to handle truly in-the-wild human videos, combine our motion-centric approach with object-centric representations obtained from foundation models, and move towards more complex manipulation tasks."}]}