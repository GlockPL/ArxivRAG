{"title": "MINIRAG: TOWARDS EXTREMELY SIMPLE RETRIEVAL-AUGMENTED GENERATION", "authors": ["Tianyu Fan", "Jingyuan Wang", "Xubin Ren", "Chao Huang"], "abstract": "The growing demand for efficient and lightweight Retrieval-Augmented Generation (RAG) systems has highlighted significant challenges when deploying Small Language Models (SLMs) in existing RAG frameworks. Current approaches face severe performance degradation due to SLMs' limited semantic understanding and text processing capabilities, creating barriers for widespread adoption in resource-constrained scenarios. To address these fundamental limitations, we present MiniRAG a novel RAG system designed for extreme simplicity and efficiency. MiniRAG introduces two key technical innovations: (1) a semantic-aware heterogeneous graph indexing mechanism that combines text chunks and named entities in a unified structure, reducing reliance on complex semantic understanding, and (2) a lightweight topology-enhanced retrieval approach that leverages graph structures for efficient knowledge discovery without requiring advanced language capabilities. Our extensive experiments demonstrate that MiniRAG achieves comparable performance to LLM-based methods even when using SLMS while requiring only 25% of the storage space. Additionally, we contribute a comprehensive benchmark dataset for evaluating lightweight RAG systems under realistic on-device scenarios with complex queries. We fully open-source our implementation and datasets at: https://github.com/HKUDS/MiniRAG.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advances in Retrieval-Augmented Generation (RAG) have revolutionized how language models access and utilize external knowledge, demonstrating impressive capabilities across diverse applications from question answering to document synthesis (Fan et al., 2024). While these systems achieve remarkable performance through sophisticated retrieval mechanisms and powerful language models, they predominantly rely on Large Language Models (LLMs) throughout their pipeline - from index construction and knowledge retrieval to final response generation (Gao et al., 2023). This extensive dependence on LLMs introduces substantial computational overhead and resource requirements, creating significant barriers for deployment in resource-constrained scenarios such as edge devices, privacy-sensitive applications, and real-time processing systems (Liu et al., 2024). Despite growing demand for efficient and lightweight language model applications, current RAG frameworks offer limited solutions for maintaining robust performance within these practical constraints, highlighting a critical gap between theoretical capabilities and real-world deployment needs.\nThe limitations of existing RAG systems become particularly apparent when attempting to utilize Small Language Models (SLMs) for resource-efficient deployment. While these compact models offer significant advantages in terms of computational efficiency and deployment flexibility, they face fundamental challenges in key RAG operations - from semantic understanding to effective information retrieval. Current RAG architectures (e.g., LightRAG Guo et al. (2024) and GraphRAG Edge et al. (2024a)), originally designed to leverage LLMs' sophisticated capabilities, fail to accommodate the inherent constraints of SLMs across multiple critical functions: sophisticated query interpretation, multi-step reasoning, semantic matching between queries and documents, and nuanced information synthesis. This architectural mismatch manifests in two significant ways: either severe performance degradation where accuracy drops, or complete system failure where certain advanced RAG frameworks become entirely inoperable when transitioning from LLMs to SLMs.\nTo address these fundamental challenges, we propose MiniRAG, a novel RAG system that reimagines the information retrieval and generation pipeline with a focus on extreme simplicity and computational efficiency. Our design is motivated by three fundamental observations about Small Language Models (SLMs): (1) while they struggle with sophisticated semantic understanding, they excel at pattern matching and localized text processing; (2) explicit structural information can effectively compensate for limited semantic capabilities; and (3) decomposing complex RAG operations into simpler, well-defined steps can maintain system robustness without requiring advanced reasoning capabilities. These insights lead us to prioritize structural knowledge representation over semantic complexity, marking a significant departure from traditional LLM-centric RAG architectures.\nOur design of MiniRAG is motivated by three fundamental observations: (1) while SLMs struggle with semantic understanding, they excel at pattern matching and localized text processing; (2) explicit structural information can compensate for limited semantic capabilities by providing navigational cues for retrieval; and (3) decomposing complex RAG operations into simpler, well-defined steps can maintain system robustness without requiring advanced reasoning capabilities. These insights lead us to prioritize structural knowledge representation over semantic complexity, and to leverage graph-based patterns that naturally align with SLMs' strengths while circumventing their limitations. This design philosophy enables MiniRAGto achieve efficient and reliable performance even with lightweight models, marking a significant departure from traditional LLM-centric RAG architectures.\nOur MiniRAG introduces two key technical innovations that leverage these insights: (1) a semantic-aware heterogeneous graph indexing mechanism that systematically combines text chunks and named entities in a unified structure, reducing reliance on complex semantic understanding, and (2) a lightweight topology-enhanced retrieval approach that utilizes graph structures and heuristic search patterns for efficient knowledge discovery. Through careful design choices and architectural optimization, these components work synergistically to enable robust RAG functionality even with limited model capabilities, fundamentally reimagining how RAG systems can operate within the constraints of SLMs while leveraging their strengths.\nThrough extensive experimentation across datasets and Small Language Models, we demonstrate MiniRAG's exceptional performance: compared to existing lightweight RAG systems, MiniRAG achieves 1.3-2.5\u00d7 higher effectiveness while using only 25% of the storage space. When transitioning from LLMs to SLMs, our system maintains remarkable robustness, with accuracy reduction ranging from merely 0.8% to 20% across different scenarios. Most notably, MiniRAG consistently achieves state-of-the-art performance across all evaluation settings, including tests on two comprehensive datasets with four different SLMs, while maintaining a lightweight footprint suitable for resource-constrained environments such as edge devices and privacy-sensitive applications. To facilitate further research in this direction, we also introduce LiHuaWorld, a comprehensive benchmark dataset specifically designed for evaluating lightweight RAG systems under realistic on-device scenarios such as personal communication and local document retrieval."}, {"title": "2 THE MINIRAG FRAMEWORK", "content": "In this section, we present the detailed architecture of our proposed MiniRAG framework. As illustrated in Fig.1, MiniRAG consists of two key components: (1) heterogeneous graph indexing (Sec.2.1), which creates a semantic-aware knowledge representation, and (2) lightweight graph-based knowledge retrieval (Sec.2.2), which enables efficient and accurate information retrieval."}, {"title": "2.1 HETEROGENEOUS GRAPH INDEXING WITH SMALL LANGUAGE MODELS", "content": "In resource-constrained RAG systems, Small Language Models (SLMs) introduce significant operational constraints that impact their effectiveness. These limitations primarily manifest in two critical areas: i) reduced capability to extract and understand complex entity relationships and subtle contextual connections from raw text, and ii) diminished capacity to effectively summarize large volumes of text and process retrieved information containing noise and irrelevant content."}, {"title": "2.2 LIGHTWEIGHT GRAPH-BASED KNOWLEDGE RETRIEVAL", "content": "In on-device Retrieval Augmented Generation (RAG) systems, the limitations of device computational capabilities and data privacy restrict the use of powerful models, such as large language models and advanced text embedding models, necessitating reliance on smaller alternatives. Consequently, currently used pipelines heavily rely on LLMs for a comprehensive understanding of text semantics when computing embedding similarity for retrieval, facing significant challenges. These smaller models often struggle to capture the precise semantic nuances within lengthy texts, complicating accurate matching. To tackle these challenges, it is essential to: i) Reduce the complexity of input content for generation, ensuring that semantic information is clear and concise; ii) Shorten the length of input content for smaller language models, facilitating improved comprehension and retrieval accuracy. Additionally, employing effective graph indexing structures can help mitigate performance deficiencies in semantic matching, thereby enhancing the overall retrieval process.\nIn MiniRAG, we propose a Graph-based Knowledge Retrieval mechanism that effectively leverages the semantic-aware heterogeneous graph G constructed during the indexing phase, in conjunction with lightweight text embeddings, to achieve efficient knowledge retrieval. By employing a graph-based search design, we aim to ease the burden on precise semantic matching with large language models. This approach facilitates the acquisition of rich and accurate textual content at a low computational cost, thereby enhancing the ability of language models to generate precise responses."}, {"title": "2.2.1 QUERY SEMANTIC MAPPING", "content": "In the retrieval phase, the primary goal for a user-input query q is to identify elements related to the query (e.g., text chunks) from the constructed index data, thereby aiding the model in generating accurate responses. To achieve this, it is essential to first parse the query and align it with the index data. Some prior RAG methods utilize LLMs to expand or decompose the query into fine-grained queries (Chan et al., 2024; Edge et al., 2024a; Guo et al., 2024), enhancing the match between the query and the index data. However, this process relies on LLMs to extract high-quality abstract information from the query, which poses challenges for smaller language models. Therefore, in the retrieval process of MiniRAG, we leverage entity extraction\u2014a relatively simple and effective task for small language models-to facilitate the decomposition and mapping of the query q to our graph-based indexing data (i.e., the semantic-aware heterogeneous graph G).\nFor a given q, our approach begins with a two-stage entity processing pipeline. First, we employ a small language model to extract relevant entities V\u2081 from q while simultaneously predicting their potential types (e.g., event, location, person) that may directly contribute to the query's answer. Following this, we leverage a lightweight sentence embedding model to evaluate semantic similarities across all entity nodes Ve in the constructed graph G = {Vc, Ve}, examining various text corpora (i.e., entity names, chunk content) to enable effective node retrieval and grounding."}, {"title": "Query-Driven Reasoning Path Discovery.", "content": "Within our semantic-aware heterogeneous graph G = Vc, Ve, M constructs reasoning paths through an intelligent query-guided mechanism. For any input query q, the model identifies relevant text chunks by jointly considering two key aspects: (1) semantic relevance between query and entity nodes, and (2) structural coherence among entity-entity and entity-chunk relationships. This dual-objective optimization framework simultaneously maximizes q-Ve semantic alignment and preserves (Ve-Ve), (Ve-Vc) relational dependencies, effectively capturing complex reasoning chains within the heterogeneous knowledge graph. The systematic query-relevant reasoning path discovery procedure consists of the following key steps:\n\u2022 Initial Entity Identification (V): We locate high-confidence starting points by matching query entities with graph nodes, establishing reliable entry points for path exploration.\n\u2022 Answer-Aware Entity Selection (Va): Leveraging predicted answer types, we identify candidate answer nodes from the starting set, enabling type-guided reasoning.\n\u2022 Context-Rich Path Formation (V): We enrich reasoning paths by incorporating relevant text chunks, creating comprehensive evidence chains that connect query entities to potential answers.\nThis lightweight framework maintains high efficiency while ensuring semantic accuracy, making it particularly suitable for edge computing scenarios. The subsequent section details our search algorithm for further refining these reasoning paths through importance-based ranking."}, {"title": "2.2.2 TOPOLOGY-ENHANCED GRAPH RETRIEVAL", "content": "To address the fundamental limitations of small language models-based methods in knowledge retrieval, we propose a topology-aware retrieval approach that effectively combines semantic and structural information from heterogeneous knowledge graphs. Approaches relying on small language models with limited semantic understanding capabilities, often introduce substantial noise into the retrieval process due to their constrained ability to capture nuanced meanings, contextual variations, and complex entity relationships in real-world knowledge graphs. Our method overcomes these inherent challenges through a carefully designed two-stage process that synergistically leverages both embedding-based similarities and topological structure of the knowledge graph.\nThe process begins with embedding-based similarity search to identify seed entities (Vs, Va) through semantic matching, followed by a topology-enhanced discovery phase that leverages the heterogeneous graph structure G to discover relevant reasoning paths. By integrating entity-specific relevance scores, structural importance metrics, and path connectivity patterns, our approach achieves superior precision in knowledge retrieval while maintaining computational efficiency, ultimately enabling more accurate and interpretable reasoning paths for enhanced generation tasks.\n\u2022 Key Relationship Identification: We first identify high-quality entity-entity connections within graph G that are relevant to query q through node-edge interactions. In the entity-entity connections Ex, we define an edge as highly relevant if it connects a starting node \u00fbs \u2208 Vs to an answer node \u03bd\u03b1 \u2208 Va along their shortest path. For efficient extraction, we focus on edges proximate to starting or answer nodes. The relevance scoring function we (\u00b7) for each edge ea \u2208 Ea is formally defineds:\n$w_e(e) = \\sum_{\\hat{v}_s \\in V_s} count(\\hat{v}_s, G_{e,k}) + \\sum_{\\nu_a \\in \\nu_a} count(\\nu_a, \\hat{G}_{e,k}),$ (2)"}, {"title": "Query-Guided Path Discovery:", "content": "To systematically discover logically relevant information within our knowledge graph structure, we identify and extract significant paths that serve as meaningful reasoning chains. A reasoning path starts from a carefully selected seed node and progressively advances toward potential answer nodes while maximizing the incorporation of previously extracted key relationships. For each candidate starting node \u00fbs in our graph, we comprehensively define the potential reasoning path set P\u00fbs as the collection of all possible acyclic paths of length n originating from \u00fbs. For each identified query entity vq \u2208 Vq, we systematically evaluate these potential paths using a sophisticated entity-conditioned score function wp(\u00b7) that quantifies both the overall path importance and query relevance through multiple dimensions:\n$w_p(p | V_q) = \\omega_v(\\hat{v}_s | v_q) \\cdot (1 + \\sum_{\\nu \\in (p \\cap V_a)} count(\\nu, p) + \\sum_{\\epsilon \\in (p \\cap E_a)} w_e(\\epsilon)).$ (3)\nThe scoring components in our path discovery framework are defined as follows: \u03c9\u03bd(\u00fbs | Uq) measures the semantic similarity between starting node \u00fbs and query entity vq using cosine similarity of their respective embeddings in the vector space, while count(v, p) serves as a binary indicator function that returns 1 if node v appears in path p and 0 otherwise. For each query entity and starting node pair in the graph, we systematically rank all potential paths according to their computed importance scores and construct the final comprehensive set of reasoning paths Pq by carefully selecting the top-k highest-scoring paths from each ranking list for subsequent steps."}, {"title": "Retrieval of Query-Relevant Text Chunks:", "content": "Building upon our indexing structure from Section 2.1, each entity node maintains connections with its source text chunk through entity-chunk inter-dependencies. These text chunks exist as nodes in our indexing graph, connected via text-attributed edges (e\u00df, de\u00df) \u2208 \u0395\u03b2. By traversing these connections, we collect all chunk nodes Va that are linked to entity nodes present in any reasoning path p \u2208 Pq. Step 1: Candidate Filtering. We first systematically filter the candidates to focus on the intersection V\u0245V\u0105 to ensure coverage of relevant information. Step 2: Similarity Computation. For each candidate chunk in the intersection, we carefully calculate the semantic similarity between the input query and the concatenated content, which combines both the chunk text and its associated edge descriptions. Step 3: Ranking and Selection. Finally, we rank these filtered chunks according to their computed relevance scores and select the top-scoring candidates to form the final optimized set V\u0105 for subsequent reasoning.\n\u2022 Integration for Augmented Generation: Through our proposed topology-enhanced graph retrieval mechanism and multi-stage filtering process, we efficiently obtain two key components of query-relevant graph knowledge: i) Essential relationships \u00caa connecting important entities within the knowledge graph, which capture the semantic dependencies and structural patterns; ii) Optimal text chunks Va containing critical contextual information and supporting evidence necessary for accurate answer generation. By systematically integrating these retrieved components with the previously grounded answer nodes Va through our designed fusion strategy, we construct the comprehensive and well-structured input representation for the final augmented generation process."}, {"title": "3 EVALUATION", "content": "Through the novel design of MiniRAG, we enable efficient on-device RAG systems without relying on large models, preserving data privacy while maintaining robust performance. Our evaluation addresses three key research questions (RQs): \u2022 RQ1: Comparative Performance. How does MiniRAG perform against state-of-the-art alternatives in terms of retrieval accuracy and efficiency? \u2022 RQ2: Component Analysis. What is the contribution of key components to MiniRAG's overall effectiveness? \u2022 RQ3: Case Studies. How effectively does MiniRAG handle complex, multi-step reasoning tasks with small language models, as demonstrated through practical case studies?"}, {"title": "3.1 EXPERIMENTAL SETTINGS", "content": "Datasets. The evaluation of on-device RAG requires careful consideration of their unique context and practical use cases. While traditional server-side RAG systems are designed to process extensive documents such as academic papers, technical reports, and comprehensive web content, on-device RAG applications serve fundamentally different purposes aligned with users' daily device interactions.\nOur dataset selection reflects these requirements, focusing on common on-device scenarios including Instant Messaging (real-time retrieval from chat histories and personal communications, and emails), Personal Content (user-created notes, memos, and calendar entries), and Local Short Documents (lightweight text files). This composition aligns with the core strengths of on-device RAG systems - privacy preservation, real-time processing, and efficient personal content management. By focusing on these everyday user interactions rather than complex document processing, our evaluation framework provides a realistic assessment of on-device RAG capabilities in their intended use cases.\nOur evaluation employs two datasets (detailed in Appendix Section) that capture essential aspects of real-world, on-device RAG scenarios. The key characteristics of these datasets are as follows:\n\u2022 Synthetic Personal Communication Data: To comprehensively capture real-world personal communications, we leverage GPT-4 to generate a year-long dataset that authentically mirrors the full spectrum of daily life interactions. This extensive dataset encompasses diverse aspects of modern living, including daily necessities (e.g., food, clothing, housing, transportation), social activities and entertainment, work and study-related discussions, personal schedule planning, and shopping decisions. The conversations reflect natural communication patterns across various contexts - from casual chats and task coordination to information sharing and decision making. By utilizing GPT-4's advanced generation capabilities with effective prompting mechanism, we ensure the dataset maintains realistic temporal coherence and contextual relationships while preserving privacy, making it ideal for evaluating both \"Instant Messaging\" and \"Personal Content\" use cases.\n\u2022 Short Documents: We utilize a multi-hop RAG dataset based on contemporary news articles, specifically designed to evaluate the system's capability in navigating and retrieving information across multiple short documents. This dataset mirrors the real-world scenarios, where users frequently need to retrieve relevant information from various locally stored files. This setup enables comprehensive assessment of both retrieval efficiency and accuracy when handling cross-document information access - for the \"Local Short Documents\" use case in on-device applications.\nEvaluation Protocols and Metrics. We employ two key metrics to assess the quality and reliability of responses generated by various RAG methods. \u2022 Accuracy (acc): Measures the consistency between the RAG system's response and the expected answer. For instance, given the query \"What does Briar remind everyone to bring to practice?\" with the expected answer \u201cbottle\u201d, semantically equivalent responses like \"water bottle\u201d are considered correct. \u2022 Error Rate (err): Captures instances where the RAG system provides incorrect information without recognizing its mistake. For example, if the system confidently responds with \u201cyoga mat\u201d to the above query, it would count toward the error rate.\nImplementation Details. We configure our experimental setup following established practices from prior work (Guo et al., 2024). For text processing, we set the chunk size to 1200 tokens with an overlap of 100 tokens, and utilize nano vector base for vector storage. In our MiniRAG implementation, we configure the top-k retrieval to 5 documents and set the maximum token limit to 6000 tokens.\nFor the model selection, we employ different efficient configurations for large and small language models. In the advanced LLM setting, we use gpt-40-mini (OpenAI, 2023) as the language model and text-embedding-3-small as the specialized embedding model. For the lightweight SLM setting, we utilize optimized all-MiniLM-L6-v2 (Reimers & Gurevych, 2019) as the embedding model, paired with various small language models including Phi-3.5-mini-instruct (Abdin et al., 2024), GLM-Edge-1.5B-Chat, Qwen2.5-3B-Instruct (Team, 2024b), and MiniCPM3-4B (Hu et al., 2024).\nBaselines. We compare our MiniRAG against several representative RAG systems:\n\u2022 NaiveRAG (Mao et al., 2020) serves as the standard RAG baseline, employing text embedding-based retrieval. It segments documents into chunks stored in a vector database and performs retrieval through direct similarity matching between query and chunk embeddings.\n\u2022 GraphRAG (Edge et al., 2024b) leverages graph-based indexing through language models and the Leiden algorithm for entity clustering. It enhances retrieval by generating community reports and combining local-global information access through a unified retrieval mechanism.\n\u2022 LightRAG (Guo et al., 2024) implements a dual-level retrieval architecture with knowledge graphs. It enhances query understanding by decomposing queries into hierarchical components (low-level details and high-level concepts), enabling more precise document retrieval."}, {"title": "3.2 PERFORMANCE ANALYSIS (RQ1)", "content": "\u2022 Performance Degradation in Existing RAG Systems with SLMs. Current RAG systems face critical challenges when operating with Small Language Models (SLMs), revealing fundamental vulnerabilities in their architectures. Advanced LLM-based RAG methods exhibit severe performance degradation, with LightRAG's accuracy plummeting from 56.90% to 35.42% during LLM to SLM transition, while GraphRAG experiences complete system failure due to its inability to generate high-quality content. While basic retrieval systems like NaiveRAG show resilience, they suffer from significant limitations, being restricted to basic functionality and lacking advanced reasoning capabilities. This performance analysis highlights a critical challenge: existing advanced systems' over-reliance on sophisticated language capabilities leads to fundamental operational failures when using simpler models, creating a significant barrier to widespread adoption in resource-constrained environments, where high-end language models may not be available or practical to deploy.\n\u2022 MiniRAG's Unique Advantages. These innovations enable MiniRAG to maintain strong performance even with simpler language models, making it particularly suitable for resource-constrained environments while preserving the core functionalities of RAG systems.\ni) Semantic-Aware Graph Indexing for Reduced Model Dependency. MiniRAG fundamentally reimagines the indexing process through a dual-node heterogeneous graph structure. Instead of relying on powerful text generation capabilities, the system focuses on basic entity extraction and heterogeneous relationship mapping. This design combines text chunk nodes for preserving raw contextual information with entity nodes for capturing key semantic elements, creating a robust knowledge representation that remains effective even with limited language model capabilities.\nii) Topology-Enhanced Retrieval for Balanced Performance. MiniRAG employs a lightweight graph-based retrieval mechanism that balances multiple information signals through a systematic process. Beginning with query-driven path discovery, the system integrates embedding-based matching with structural graph patterns and entity-specific relevance scores. Through topology-aware search and optimized efficiency, it achieves robust retrieval quality without requiring advanced language understanding, making it particularly effective for on-device deployment."}, {"title": "3.3 COMPONENT-WISE ANALYSIS OF MINIRAG (RQ2)", "content": "Our ablation study examines the individual contributions of key MiniRAG components through two primary experimental variations, as documented in Table2. The first variation (-1) replaces MiniRAG's heterogeneous graph indexing with a description-based indexing approach that requires comprehensive semantic understanding for generating accurate entity/edge descriptions, similar to methods used in LightRAG and GraphRAG. The second variation (-R\u2081) involves selectively deactivating specific modules during graph retrieval. This systematic evaluation framework provides detailed insights into how each component contributes to MiniRAG's overall performance.\nOur experimental results reveal crucial insights into MiniRAG's architectural effectiveness. \u2022 Validating SLM Limitations. One key finding emerges when replacing our streamlined indexing method with text semantic-driven indexing techniques (-1), resulting in substantial performance degradation. This outcome strongly validates our initial hypothesis about Small Language Models (SLMs) and their inherent limitations - specifically their constraints in comprehensive semantic understanding, which impacts both the generation of complex knowledge graphs with entity relationships and the creation of corresponding comprehensive text descriptions. \u2022 Effectiveness of Query-guided Reasoning Path Discovery. The experiments further demonstrate the critical nature of structural components: the removal of either edge information (-Redge) or chunk nodes (-Rchunk) significantly impacts system performance. These elements serve dual purposes: they facilitate effective query matching through query-guided reasoning path discovery while simultaneously compensating for the inherent limitations of SLMs during the data indexing phase."}, {"title": "3.4 CASE STUDY ANALYSIS (RQ3)", "content": "In this section, we demonstrate MiniRAG's practical advantages through a case study with LightRAG, focusing on a complex restaurant identification scenario. This study illustrates how our query-guided reasoning approach, combined with heterogeneous graph indexing, effectively handles multi-constraint queries while overcoming the inherent limitations of small language models.\n\u2022 Challenge: Complex Query Resolution in Restaurant Identification. We conducted a comparative case study between MiniRAG and LightRAG using a complex query scenario: \"What is the name of the Italian restaurant where Wolfgang and Li Hua are having dinner to celebrate Wolfgang's promotion?\" This query presents multiple challenges, requiring the system to identify specific Italian restaurants from various mentions in online chat data while correlating them with the context of a promotion celebration. LightRAG, despite its capabilities, struggled with this task due to the limitations of its underlying small language model (phi-3.5-mini-instruct). The SLM's constraints in extracting appropriate high-level information, combined with noise in the graph-based index, led to ineffective direct embedding matching and ultimately hindered accurate answer retrieval.\n\u2022 MiniRAG's Effective Query-Guided Knowledge Discovery. MiniRAG successfully resolved this challenge through its query-guided reasoning path discovery mechanism, which enables precise and contextually relevant knowledge retrieval. By leveraging its heterogeneous graph indexing structure, MiniRAG effectively constructs query-relevant knowledge paths, starting with answer type prediction (\"Social Interaction\" or \"Location\") and proceeding through targeted entity matching. This structured reasoning approach, combined with strategic decomposition of query elements (focusing on \"Italian place\" and \"restaurant\" contexts), allows MiniRAG to navigate the knowledge space efficiently. The synergy between query-guided reasoning and heterogeneous graph indexing enabled MiniRAG to effectively filter through multiple Italian establishments, ultimately identifying \"Venedia Grancaffe\" as the venue specificaslly connected to the promotion celebration context."}, {"title": "4 RELATED WORKS", "content": "Small Language Models (SLMs). The emergence of Small Language Models (SLMs) is driven by the growing need for lightweight, efficient and privacy-preserving AI solutions that can operate on edge devices, addressing the limitations of large language models in computational resources and deployment costs (Liu et al., 2024; Wang et al., 2024a; Team, 2024b; Abdin et al., 2024; Hu et al., 2024). Recent developments have produced notable models such as MiniCPM3-4B (Hu et al., 2024), phi-3.5-mini (Abdin et al., 2024), Llama-3.2-3B (Grattafiori et al., 2024), Qwen2.5-1.5B (Team, 2024b), gemma-2-2b (Team, 2024a), SmolLM-1.7B (Allal et al., 2024), and MobiLlama-1B (Thawakar et al., 2024), which have demonstrated impressive performance while maintaining significantly smaller parameter counts. These models excel in inference speed, deployment flexibility, and privacy preservation, making them particularly suitable for resource-constrained environments.\nFor facilitating visual-language understanding in resource-constrained environments, researchers have developed Multi-modal SLMs by efficiently extending single-modal SLMs with visual capabilities, as exemplified by MiniCPM-V 2.0 (Yao et al., 2024), Qwen2-VL (Wang et al., 2024b), Phi-3-vision (Abdin et al., 2024), and InternVL2-2B (Chen et al., 2024). These multi-modal SLMs have demonstrated remarkable capabilities in combining visual and textual understanding while maintaining the computational efficiency advantages of small-scale models, enabling their deployment in diverse applications from GUI Agent (Lin et al., 2024) to robotics control (Zheng et al., 2024).\nWhile SLMs have demonstrated impressive capabilities in language understanding and multi-modal tasks, the potential of leveraging efficient models for RAG tasks remains largely unexplored. This work fills this gap by introducing a framework that enables SLMs to effectively perform RAG tasks while maintaining their inherent advantages in computational efficiency and deployment flexibility.\nRetrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) systems enhance language models' responses by retrieving relevant knowledge from external databases (Guo et al., 2024; Qian et al., 2024; Gao et al., 2024b). The process consists of three main components: indexing, retrieval, and generation. Given a raw text set, the system first processes it into a database, then retrieves relevant information based on user query q, and finally generates the answer. Two primary approaches have emerged in database construction: (1) chunks-based methods (Mao et al., 2020; Qian et al., 2024), which segment texts into retrievable units, and (2) graph-based methods (Guo et al., 2024; Edge et al., 2024b), which structure information as knowledge graphs.\nRecent RAG implementations have evolved along these two paths. Chunks-based methods, exemplified by NaiveRAG (Mao et al., 2020), ChunkRAG (ukhvinder Singh et al., 2024), and RQ-RAG (Chan et al., 2024), focus on optimizing text segmentation and chunk retrieval strategies. Graph-based approaches, such as GraphRAG (Edge et al., 2024b), LightRAG (Guo et al., 2024), and SubgraphRAG (Li et al., 2024), leverage graph structures for better corpus comprehension and efficient retrieval. However, most existing methods require either large context windows (Qian et al., 2024; Li et al., 2024) or strong semantic understanding capabilities (Guo et al., 2024; Edge et al., 2024b), limiting their applicability with small and lightweight language models. This gap motivates the development of more efficient RAG systems suitable for resource-constrained scenarios."}, {"title": "5 CONCLUSION", "content": "We present MiniRAG, a novel RAG system designed to address the fundamental limitations of deploying small language models (SLMs) in existing retrieval-augmented generation frameworks. Through its innovative heterogeneous graph indexing and lightweight heuristic retrieval mechanisms, MiniRAG effectively integrates the advantages of both text-based and graph-based RAG approaches while significantly reducing the demands on language model capabilities. Our experimental results demonstrate that MiniRAG can achieve comparable performance to LLM-based methods even when using SLMs. Additionally, to facilitate research in this emerging field, we release the first benchmark dataset specifically designed for evaluating on-device RAG capabilities, featuring realistic personal communication scenarios and multi-constraint queries. These contributions mark an important step toward enabling private, efficient, and effective on-device RAG systems, opening new possibilities for edge device AI applications while preserving user privacy and resource efficiency."}, {"title": "Appendix", "content": "\u2022 Dataset Descriptions. The rapid growth of mobile computing has led to an unprecedented accumulation of content on personal devices, creating a pressing need for efficient on-device information retrieval and generation systems. Traditional RAG benchmarks, primarily focused on well-structured documents like Wikipedia articles or academic papers, fail to capture the unique characteristics of on-device scenarios. These scenarios present distinct challenges and characteristics:\n\u2022 (1) Content Fragmentation and Context-Switching - Unlike traditional RAG systems that process well-structured documents with clear logical flow (e.g., Wikipedia articles, academic papers), on-device RAG must handle inherently fragmented content that frequently switches between different contexts and conversation threads - a direct reflection of how people naturally communicate and interact across various digital platforms in their daily lives.\n\u2022 (2) Temporal Nature and Evolution Patterns - Unlike traditional RAG's static, historically complete documents, on-device RAG must handle inherently dynamic content that constantly evolves through real-time updates, ongoing conversations, and emerging social interactions. This fundamental difference in temporal dynamics creates unique challenges for on-device RAG systems, which must maintain relevance and accuracy while processing an ever-changing stream of information across various digital platforms and communication channels.\n\u2022 (3) Digital-Physical Context Fragmentation. Digital communication related to personal social activities and events typically presents fragmented and incomplete information, as these interactions span both online and offline contexts. Unlike traditional RAG systems that process self-contained documents, on-device content frequently captures only partial context of real-world events - text conversations might reference in-person meetings, shared experiences, or future plans without full details. This hybrid nature means digital communications often contain implicit references and assumed knowledge that only make sense with additional real-world context, requiring sophisticated context-bridging capabilities to effectively process and understand the information."}]}