{"title": "Code Vulnerability Repair with Large Language Model using Context-Aware Prompt Tuning", "authors": ["Arshiya Khan", "Guannan Liu", "Xing Gao"], "abstract": "Large Language Models (LLMs) have shown significant challenges in detecting and repairing vulnerable code, particularly when dealing with vulnerabilities involving multiple aspects, such as variables, code flows, and code structures. In this study, we utilize GitHub Copilot as the LLM and focus on buffer overflow vulnerabilities. Our experiments reveal a notable gap in Copilot's abilities when dealing with buffer overflow vulnerabilities, with a 76% vulnerability detection rate but only a 15% vulnerability repair rate. To address this issue, we propose context-aware prompt tuning techniques designed to enhance LLM performance in repairing buffer overflow. By injecting a sequence of domain knowledge about the vulnerability, including various security and code contexts, we demonstrate that Copilot's successful repair rate increases to 63%, representing more than four times the improvement compared to repairs without domain knowledge.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have been a focus of research for decades and have recently seen a surge in popularity and practical applications. Although initially developed for natural language processing, LLMs are now increasingly used by programmers for tasks such as code generation and bug fixing [1]. Models like GitHub Copilot [2] and Code Llama [3], which have been trained on extensive code repositories, are specifically designed to assist with a wide range of coding tasks and have demonstrated increasing effectiveness in these areas.\nUnfortunately, existing research has demonstrated that LLMs encounter significant difficulties when addressing security-related issues, particularly in repairing vulnerable code [4]\u2013[7]. To tackle these challenges, studies have proposed solutions involving prompt tuning, which entails modifying and intentionally crafting the prompts given to LLMs to increase the likelihood of generating the desired output. However, these prompt tuning techniques are often complex and challenging to generate for regular users, leading to mixed and unreliable outcomes.\nIn this study, we select GitHub Copilot as the LLM model and explore the intricacies of the code repair process to enhance its success rate. We begin by investigating the types of code that Copilot struggles to repair. Our findings reveal that Copilot faces significant challenges in repairing code-dependent vulnerabilities, with a success rate of only 13%."}, {"title": "II. RELATED WORK", "content": "Extensive research efforts have been conducted on the security aspects of Large Language Models (LLMs) [8]\u2013[17]. Particularly, LLMs have shown significant challenges in detecting and repairing vulnerable code. To improve their detection and repair success rates, one viable approach proposed by previous research is fine-tuning [18], [19]. Specifically, Chen et al. [20] demonstrates that using a diverse and large dataset of code, including various vulnerable code samples, can benefit vulnerability detection and repair. However, fine-tuning LLMs requires significant computing resources for training and is not yet widely available to the general public.\nA more practical solution for general developers is prompt tuning [4]\u2013[7], with the primary goal of preventing LLMs from diverging into inconsequential directions during vulnerability detection and repair [21]. Two prominent methods of prompt tuning include Few-shot learning [6], [22] and Chain-of-Thought prompting [23]. Wu et al. [24] investigated state-of-the-art LLMs against real-world Java benchmarks to evaluate automated program repair. Their findings indicated that, for vulnerability detection, fine-tuning outperforms prompt tuning under specific code transformations. However, neither fine-tuning nor prompt tuning significantly improved code repair outcomes. Ding et al. [25] introduced PrimeVul, a dataset containing pairs of vulnerable code and corresponding patches. Their results showed that prompt tuning could achieve similar, if not better, performance than fine-tuning when using intentionally designed prompts.\nThis study builds on existing research on prompt tuning by proposing context-aware prompt-tuning techniques. By incorporating domain-specific knowledge on software security, we demonstrate that effective prompt-tuning techniques can increase the success rate of code repairs from 15% to 63% by injecting a sequence of domain knowledge into the prompts."}, {"title": "III. PROMPTS AND CODE DEPENDENCE", "content": "Every task performed by LLMs begins with a prompt initiated by the user. For code-related tasks, the prompt typically consists of two parts: (1) Code: This part contains the content to which the prompt is referring. The code often includes programs or functions with various components, such as variables and methods. (2) Task: This part outlines the goals of the prompt and may include the expected outcomes or desired actions for the LLMs to perform.\nWhen prompting LLMs to repair code, they need to analyze the code, identify the vulnerability, and replace it with a viable solution. There are two types of vulnerabilities that LLMs may encounter:\n\u2022 Code-Independent Vulnerabilities: These vulnerabilities are independent of the code context, meaning their vulnerable content is hard-coded into the code. Examples of code-independent vulnerabilities include weak cryptographic algorithms (CWE-327) [26] and the use of a cryptographically weak pseudo-random number generator (CWE-338) [27]. For these types of vulnerabilities, Copilot can replace the vulnerable content/value in the code without conducting further analysis.\n\u2022 Code-Dependent Vulnerabilities: These vulnerabilities depend on various code elements, such as variables, function parameters, and user input data. This makes the repair task more complex because LLMs need to perform extensive code analysis to identify the vulnerable sections accurately. Examples of code-dependent vulnerabilities include buffer overflow (CWE-120) [28], out-of-bounds read (CWE-125) [29], and NULL pointer dereference (CWE-476) [30].\nTo investigate the repair success rate for code-dependent and code-independent vulnerabilities, we conduct an experiment in which we prompt Copilot to repair 60 vulnerable code"}, {"title": "IV. CODE REPAIR WITH SECURITY CONTEXT", "content": "To investigate the performance of Copilot repairing vulnerable code with security context, we first assess the current performance of Copilot in code vulnerability detection and repair. To achieve this, we avoid providing any additional information in all prompts. Figure 1a and Figure 1b illustrate an example of the detection and repair prompts used to query Copilot, respectively. The detection prompt is straightforward; we ask Copilot if there is any vulnerability in the provided code snippet. We consider a detection \u201ccorrect\" if the response is \"YES\" and the returned response includes a discussion of buffer overflow vulnerabilities. For the repair prompt, we ask Copilot to rewrite the code if a vulnerability is detected. We manually evaluate whether the repaired code generated by Copilot is a proper fix. We consider a repair \"successful\" if the generated code addresses the buffer overflow vulnerability without introducing any new vulnerabilities.\nIn total, we prompt Copilot with 156 code snippets carrying various types of buffer overflow vulnerabilities. Table I demonstrates a detailed breakdown of these vulnerability categories, as well as our experiment results. More specifically, the success rate of vulnerability detection and repair rate with no additional knowledge are 76% and 15%, respectively."}, {"title": "V. CONTEXT-AWARE PROMPT TUNING", "content": "To successfully repair vulnerable code, Copilot must not only understand the security context of the code snippet but also analyze the code context to identify the exact locations of the vulnerability and fix it with the correct values or content. These code context considerations include:\n1) Buffer Identification: Copilot should accurately identify the overflowed buffer.\n2) Bound Selection: Copilot should be capable of allocating an appropriate memory size to the overflowed buffer.\n3) Range Precision: Copilot should check the buffer range to ensure that the data fits within its allocated buffer, accounting for all potential edge cases.\n4) Suitable Placement: Copilot should ensure that buffer allocation and range checks are appropriately placed within the code and do not interfere with the functionality of other parts of the code.\nWe use the results from Section IV-B, where code repair was performed with security context only, as a baseline and apply context-aware prompt tuning to the same set of 156 code snippets. Figure 4 presents the results of our proposed context-aware prompt tuning approach. As illustrated, the repair success rate increases progressively as we move deeper into each stage of the context-aware prompt tuning process. By the end of the entire tuning process, we successfully improve the vulnerability repair rate to 63%, more than four times the 15% repair rate (discussed in IV-A) observed when no domain knowledge was provided in the prompt. This result suggests that Copilot can indeed take both security and code context into account when generating code output. Moreover, the more domain knowledge furnished to Copilot, the higher the likelihood that it can produce correctly repaired code."}, {"title": "VI. CONCLUSION", "content": "Vulnerability repair is a challenging task for LLMs, with code-dependent vulnerabilities, such as buffer overflow, being particularly difficult to address. This paper demonstrates that the repair success rate can be significantly improved by injecting code-related context into the prompts. We also propose a context-aware prompt tuning approach, a waterfall process where domain knowledge, including security and code context, is progressively introduced into the prompts to enhance the success rate of vulnerable code repairs. Our results show that this prompt tuning process can effectively increase the vulnerability repair rate from 15%, when no context is provided, to 63% after prompt tuning, a more than fourfold improvement in the success rate."}]}