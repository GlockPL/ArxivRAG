{"title": "Improving Multilingual Instruction Finetuning via Linguistically Natural and Diverse Datasets", "authors": ["Sathish Reddy Indurthi", "Wenxuan Zhou", "Shamil Chollampatt", "Ravi Agrawal", "Kaiqiang Song", "Lingxiao Zhao", "Chenguang Zhu"], "abstract": "Advancements in Large Language Models (LLMs) have significantly enhanced instruction-following capabilities. However, most Instruction Fine-Tuning (IFT) datasets are predominantly in English, limiting model performance in other languages. Traditional methods for creating multilingual IFT datasets such as translating existing English IFT datasets or converting existing NLP datasets into IFT datasets by templating-struggle to capture linguistic nuances and ensure prompt (instruction) diversity. To address this issue, we propose a novel method for collecting multilingual IFT datasets that preserves linguistic naturalness and ensures prompt diversity. This approach leverages English-focused LLMs, monolingual corpora, and a scoring function to create high-quality, diversified IFT datasets in multiple languages. Experiments demonstrate that LLMs finetuned using these IFT datasets show notable improvements in both generative and discriminative tasks, indicating enhanced language comprehension by LLMs in non-English contexts. Specifically, on the multilingual summarization task, LLMs using our IFT dataset achieved 17.57% and 15.23% improvements over LLMs fine-tuned with translation-based and template-based datasets, respectively.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in natural language processing (NLP) have showcased remarkable progress, particularly in its instruction-following capabilities. Notably, Large Language Models (LLMs) like GPT-4, Gemini-1.5, Claude-3, Llama-3, and Mistral (OpenAI, 2024; Team et al., 2024; AI@Meta, 2024; Jiang et al., 2023) have demonstrated significant prowess in this area (Brown et al., 2020; Le Scao et al., 2023; Chowdhery et al., 2023). After the pretraining stage, LLMs are fine-tuned on Instruction Fine-Tuning (IFT) datasets followed by an optional Alignment Tuning (AT) based on the availability of the training datasets. IFT datasets consist of instruction prompt-response pairs and have proven instrumental in enhancing the efficacy and overall instruction-following abilities of LLMs (Anil et al., 2023; Sanh et al., 2022; Wei et al., 2023; Iyer et al., 2023; Chung et al., 2022; Wang et al., 2022a; Zhang et al., 2024a). However, a notable disparity persists between the abundance of instruction prompts available in English compared to other languages. While over 7k\u00b9 languages are spoken worldwide, a staggering 73% of prevalent IFT datasets primarily cater to English alone (Longpre et al., 2023).\nWhile LLMs often demonstrate proficiency in understanding and generating text across multiple languages, the language imbalance in training datasets has led to suboptimal performance in non-English contexts (Ahuja et al., 2023; Lai et al., 2023a; Zhang et al., 2023c). To enhance LLMs' ability to follow non-English instructions,\n\u00b9https://www.ethnologue.com/"}, {"title": "2 Method", "content": "A fundamental component in the development of Multilingual Large Language Models (MLLMs) lies in the acquisition of training datasets, crucially needed throughout distinct phases: Pretraining (PT), Instruction fine-tuning (IFT), and Alignment Tuning (AT).\nWhile obtaining the necessary monolingual datasets for pretraining is relatively straightforward, acquiring datasets for instruction fine-tuning and alignment tuning presents significant challenges due to the costs and human effort involved. To address these challenges while maintaining linguistic characteristics and diversity, we propose a framework for creating IFT datasets for multiple languages. The framework consists of five stages, illustrated in Figure 3 and described below:\n(A). Select Responses: We utilize a monolingual corpus as the primary source of response, supplemented by answers from existing NLP datasets for each non-English language (x). We extract several thousand text fragments from these non-English corpora, deduplicate, and apply various heuristics to filter out potentially low-quality fragments. These heuristics include criteria such as the prevalence of capitalized letters and specialized symbols. These text fragments are natural and most likely error-free output since they are from the monolingual corpus or human-curated answers from existing NLP datasets. Each fragment, denoted as \\(R_x\\), which varies in length to resemble responses in real-world scenarios, is then used to generate pseudo instructions through the following steps. By doing this, we ensure the output quality of the multilingual IFT data.\n(B). Translating Responses: Given the availability of competent English LLMs in both open-source and closed environments, we have chosen to generate pseudo-instructions in English. This strategic decision allows us to leverage the strength of these models, ensuring the generation of high-quality and diverse instructions that cater to a wide range of NLP tasks, we translate the selected response (\\(R_x\\)) into English.\n\\[R_{en} = MT_{x\\rightarrow en}(R_x)\\]\n(C). Generating Instruction: We generate English instructions by utilizing English-focused LLM, a translated response (\\(R_{en}\\), and a randomly selected prompt (\\(P_1\\)) from a pool of predefined task prompts. Our approach involves designing a range of prompts specifically tailored to support various NLP tasks, including question-answering, summarization, and sentiment analysis. Additionally, the prompt allows for open-ended instruction generation, providing LLMs with the opportunity to produce the most plausible instructions for a given response. Focusing on generating instructions in English enables us to tap into the extensive resources and capabilities available for this language, thereby enhancing the adaptability and effectiveness of our approach across diverse linguistic contexts. This emphasis on English instruction generation also ensures seamless integration"}, {"title": "with existing English-centric NLP systems, further augmenting the versatility and applicability of our methodology in real-world scenarios. Formally, the English instruction (\\(I_{en}\\)) is generated by:", "content": "\\[I_{en} = LLM(P_1, R_{en}) \\qquad (1)\\]\n(D). Scoring The instructions generated through (1) do not always yield high-quality examples due to misalignment in the prompt-response pair or LLM's failure to generate appropriate instruction. Thus we rely on a scoring function to filter and identify high-quality examples while maintaining diversity in the generated dataset.\nWe use LLM as a judge, employing the prompt \\(P_s\\) to assess the quality of (\\(I_{en}\\), \\(R_{en}\\)) pair. This results in a score, denoted as s:\n\\[s = LLM(P_s, I_{en}, R_{en}) \\qquad (2)\\]\nPairs with a score greater than or equal to a predefined threshold (\\(\\lambda\\)) are used for fine-tuning, while those below this threshold are removed from fine-tuning phase.\n(E). Translating Instruction: Following the scoring phase, we proceed to translate \\(I_{en}\\) into the same language as \\(R_x\\):\n\\[I_x = MT_{en\\rightarrow x}(I_{en})\\]\nSubsequently, we form a training pair (\\(I_x\\), \\(R_x\\)). Here, \\(I_x\\) serves as a pseudo instruction, while \\(R_x\\) represents natural text in the same non-English language. During the LLM fine-tuning stage, despite potential unnaturalness and errors in \\(I_x\\) arising from the instruction generation and translation process, the model is trained to generate \\(R_x\\), which is typically a natural and error-free output sourced from the monolingual corpus or existing human-curated NLP datasets. Leveraging such pairs enhances the model's ability to handle instruction errors and improves its overall language comprehension.\nThe sample task prompts (\\(P_1\\)) and scoring prompt (\\(P_s\\)) used in Equation 1 and Equation 2 are provided in Table 8 and Table 9 in the Appendix."}, {"title": "3 Experimental Settings", "content": "We utilize the CC-100 monolingual dataset (Conneau et al., 2020). We also utilize answers from the templated examples in the aya dataset (\u00dcst\u00fcn et al., 2024). In both cases, the texts are written in the native language not derived from other languages (Wenzek et al., 2020). We selected the text based on the criteria described in Section 2. We choose four languages: Telugu (tel), Hindi (hin), Japanese (jpn), and Spanish (spa) to create IFT datasets through our approach. According to Aya and Okapi (\u00dcst\u00fcn et al., 2024; Lai et al., 2023b), Telugu and Nepali are low-resource, Indonesian and Hindi are mid-resource, and Japanese and Spanish are high-resource languages. We collected approximately one million text fragments for each language.\nIn creating multilingual datasets using the proposed approach, we utilize open source meta-llama/Meta-Llama-3-70B-Instruct (AI@Meta, 2024) as our LLM to generate instructions and also to score instruction-response pairs. However, this LLM can be replaced with more powerful open-source or closed-source LLMs to improve the quality of generated instructions further.\nWe utilize NLLB-200 (Costa-juss\u00e0 et al., 2022)\u00b2, which has support for 200 languages with state-of-the-art translation quality. The same model is used for translating the response (\\(R\\) to English as well as for translating (\\(I_{en}\\) into the language of (\\(R\\). After the translation, we use the COMET score (Rei et al., 2020) to remove low-quality translated responses (\\(R_{en}\\) and generated instructions (\\(I_x\\). Specifically, we use Unbabel/wmt23-cometkiwi-daxl model (Rei et al., 2023), which is a reference-free model with 3.5 billion parameters. We retain examples with COMET scores greater than or equal to 0.7.\nhttps://huggingface.co/facebook/nllb-200-3.3B\nhttps://huggingface.co/meta-llama/Meta-Llama-3-8B"}, {"title": "3.2 Training details", "content": "We use Meta-Llama-3-8B (AI@Meta, 2024)\u00b3 as the base model to fine-tune on our multilingual IFT dataset. We also fine-tune non-English focused models such as Rakuten-ai-7B-Instrcut (Rakuten Group et al., 2024), Aya-23 (Aryabumi et al., 2024). During training, we only optimize the loss on the output tokens, not the input tokens, thus deviating from the standard language modeling loss. We apply the same hyperparameters as existing instruction fine-tuning (IFT) methods (Zhou et al., 2023; Touvron et al., 2023): a learning rate of 1e-5 that linearly decays to 9e-6 by the end of the training, weight decay of 0.1, batch size of 128 examples, and dropout of 0.1. For generation, we use nucleus sampling (Holtzman et al., 2020) with a temperature of T = 0.7 and p = 0.9. We use 8 NVIDIA H100 GPUs for fine-tuning the model."}, {"title": "4 Results", "content": "In Table 1, we present the statistics of datasets created using various approaches. The statistics of datasets created using the template-based and translation-based approaches are from aya_collection (\u00dcst\u00fcn et al., 2024). Please see the Appendix for more details. Using our approach, we generated approximately 500K instruction-response pairs from the initial pool of 1M text fragments for each language.\nWe evaluate the performance of models fine-tuned on datasets collected using our approach against models fine-tuned on datasets obtained through translation and template-based methods. Specifically, we compare the Aya-TM and Llama-3-8B-TM models, which are trained on template-based datasets as described in \u00dcst\u00fcn et al. (2024). Additionally, we assess the Aya-TR and Llama-3-8B-TR models, which are trained on translation-based datasets detailed in \u00dcst\u00fcn et al. (2024). Both types of datasets include the Aya-human annotated dataset\u2074. Furthermore, we compare these with the Bactrian-X model (Li et al., 2023), fine-tuned on a dataset comprising translated English instructions and their corresponding multilingual responses generated using ChatGPT. Our final model, Llama-3-8B-GR, is trained using the created instruction-response dataset along with the Aya human-annotated dataset. In all the approaches, the percentage of training examples collected through the human annotation process corresponds to less than 0.1%.\n\u2074https://huggingface.co/datasets/CohereForAI/aya_dataset"}, {"title": "4.1 Generative Tasks", "content": "We evaluated the models on two generative tasks: summarization using XLSUM (Hasan et al., 2021) and machine translation using FLORES-200 (Costa-juss\u00e0 et al., 2022). These tasks were selected because they include responses written in native languages, not derived from other languages. We present the performance of our model, Llama-3-8B-GR-H, and its variants, comparing them to baseline models across the four languages used for creating multilingual IFT datasets. For the summarization task, we employed the RougeLsum metric (Lin, 2004), and for the translation task, we utilized spBLEU (Goyal et al., 2021) and chrF++ (Popovi\u0107, 2017)\u2075.\nTables 3 and 4 present the results for the summarization and machine translation tasks using the XLSUM and FLORES-200 datasets, respectively. From the results presented in both tables, models trained with translated datasets do not exhibit any improvement over those trained with template datasets. In contrast, the Llama-3-8B-GR model, fine-tuned on datasets created using our method, demonstrates significant performance enhancements across both tasks compared to all other dataset types. Our dataset, free from translation errors and rich in diversity, enables the model to better capture the authentic form of language, leading to superior performance.\n\u2075https://github.com/mjpost/sacrebleu"}, {"title": "4.2 Discriminative Tasks", "content": "We also evaluate the models on a discriminative task to assess whether introducing high-quality, diversified, and native-written responses enhances the model's language comprehension and overall performance. Specifically, we use the multilingual MMLU task (Lai et al., 2023b), a machine-translated version of the English MMLU task (Hendrycks et al., 2021), to compare the performance of models trained extensively on translated datasets versus those trained on native datasets created using our approach. This task was unseen during the models' fine-tuning stage, so we employ a few-shot evaluation to compare performance. The Llama-3-8B and Aya models use a 5-shot evaluation, while the Bactrian-X and Okapi models use a 25-shot evaluation. The task comprises 13,000 questions covering 57 topics, ranging from STEM and humanities to social sciences.\nTable 5 shows the multilingual results in three languages. The model trained with our dataset (Llama-3-8B-GR), outperforms the models trained with datasets collected using other approaches. Our model outperforms Okapi, Aya, and our baseline by 48.74%, 13.8%, and 6.9%, respectively. These results indicate that the diversity and quality of the datasets lead to better performance.\nDespite our dataset being 2.7 and 4.9 times smaller than the templated and translated datasets, respectively, the model fine-tuned on our dataset achieved significant improvements in both generative and discriminative tasks. This underscores the importance of high-quality, diversified datasets in developing efficient multilingual LLMs."}, {"title": "4.3 Analysis", "content": "To understand the diversity of the generated instructions, we plot the verb-noun structure of instructions in Figure 4. The figure visualizes the distribution of the most frequent root verbs and their corresponding most common direct noun objects from 15% of the generated instructions across four languages. These noun-verb pairs represent 8.1% of the entire set, which exhibits diverse intents and patterns in our generated instructions. We also provide a few generated samples in the Appendix.\nWe also report the average length of instructions and responses from all data creation approaches. As shown in Table 2, the average number of characters in the instructions generated using our approach varies significantly compared to the other two approaches. This variation arises from using different types of task prompts when generating an instruction for a given response."}, {"title": "4.3.2 Effect of Scoring Function:", "content": "The frequency of average scores obtained using the LLM judge is shown in Figure 5. To evaluate the impact of the scoring function on the creation of high-quality multilingual IFT datasets, we fine-tuned the Llama-3-8B-GR model on datasets filtered using different scoring thresholds, \\(\\lambda\\)= {1,2,3,4,5}. For each specific threshold \\(\\lambda\\), all examples below that score were excluded from the training set. We then compared the performance of the Llama-3-8B-GR model trained on these filtered datasets against models (Llama-3-8B-TM and Llama-3-8B-TR) trained on template-based and translation-based datasets. As illustrated in Figure 6, the performance of Llama-3-8B-GR improves as the scoring threshold increases up to \\(\\lambda\\) = 3, achieving superior performance compared to the Llama-3-8B-TM and Llama-3-8B-TR models. Beyond \\(\\lambda\\) = 3, performance declines due to the reduced size of the training dataset. These results underscore the critical role of the scoring function in creating high-quality multilingual IFT datasets."}, {"title": "4.3.3 Effect on non-English focused models.", "content": "To evaluate the diversity and quality of our IFT datasets, we conducted further fine-tuning on two robust non-English-focused LLMs using our IFT datasets. First, we assessed the impact on the Japanese-focused model (Rakuten Group et al., 2024). This model was initially pre-trained on Japanese texts and fine-tuned on Japanese instruction-response pairs. Second, we evaluated the performance of a state-of-the-art multilingual LLM named Aya-23 (Aryabumi et al., 2024). This model is based on Cohere's Command model and was instruction-tuned on 23 languages using the template-based dataset from \u00dcst\u00fcn et al. (2024). As shown in Table 6 and Table 7, fine-tuning further on our IFT dataset significantly enhances the performance of these non-English-focused LLMs."}, {"title": "5 Related Work", "content": "Multilingual LLMs. LLMs (Brown et al., 2020; Chowdhery et al., 2023; Touvron et al., 2023; OpenAI, 2024) have achieved remarkable results on various NLP tasks (Hendrycks et al., 2021; Srivastava et al., 2022). With over 7,000 languages spoken worldwide and approximately 2,500 classified as low-resource by Joshi et al. (2020), which are spoken by more than 1 billion people, there is a growing need to expand the language coverage of LLMs. To develop LLMs with multilingual capabilities, one straightforward approach is to pretrain them on a diverse set of languages. For example, BLOOM (Le Scao et al., 2023) is pretrained on 46 languages and 13 programming languages, while Llama-2 (Touvron et al., 2023) is pretrained primarily on English with additional data from 27 other languages. Despite these efforts, the language coverage of these models remains limited and predominantly focused on English. Another approach is to continually train LLMs with additional languages (Cui et al., 2023; Basile et al., 2023; ImaniGooghari et al., 2023). In particular, Chinese-Llama (Cui et al., 2023) continually trains Llama on Chinese corpora and integrates additional Chinese tokens into the original vocabulary to further improve the Chinese ability.\nInstruction Tuning. Instruction tuning has been a key paradigm for LLMs to improve their general performance and ability to follow instructions (Wei et al., 2022; Wang et al., 2022b; Ding et al., 2023). However, these models are predominantly tuned using English, resulting in significant discrepancies in performance across languages (Huang et al., 2023; Etxaniz et al., 2023). Multilingual instruction tuning has effectively narrowed this performance gap (Kew et al., 2023; Chen et al., 2024b). Typically, the data for multilingual instruction tuning is derived through translation from English data (Li et al., 2023; Zhang et al., 2023a; \u00dcst\u00fcn et al., 2024), but this approach often misses cultural nuances and can introduce unnatural responses. Some efforts (\u00dcst\u00fcn et al., 2024) utilize templates to automatically create large amounts of multilingual data, but this method is constrained by limited diversity in the instructions. We propose to generate instructions directly from original multilingual responses, which preserves the naturalness of responses and enhances the diversity of instructions."}, {"title": "6 Conclusion", "content": "In conclusion, our research addresses the notable disparity in Instruction Fine-Tuning (IFT) datasets, predominantly centered on English, by proposing a novel method for collecting multilingual IFT datasets. By leveraging English-focused LLMs and monolingual corpora, our approach maintains the naturalness of specific languages and ensures diversity in the datasets. The quality control through a scoring function further enhances the effectiveness of the generated datasets.\nOur extensive experiments on generative tasks demonstrate that the models trained with our multilingual IFT datasets significantly outperform those trained on traditional translated and templated datasets. Moreover, our models show substantial improvements in discriminative tasks, indicating a better comprehension of language.\nThese results underscore the importance of diverse and high-quality multilingual datasets in enhancing the performance of large language models across various languages. Our method provides a viable solution to the challenges faced in creating effective multilingual IFT datasets, paving the way for more inclusive and capable language models. Future research can build upon this approach to further refine and expand the capabilities of LLMs in a broader range of linguistic contexts."}, {"title": "Limitations", "content": "Since the instructions were generated by LLMs, there may be inherent biases originating from the underlying models used in this study. Nevertheless, the models used are open-source, extensively utilized by the community, and trained with the goals of reducing bias and enhancing safety and usefulness.\nThis study aims to systematically assess the effectiveness of generated instructions for given responses in various languages. Due to limitations in computing resources, we were unable to extend the proposed data creation framework beyond four languages. However, we endeavored to cover low, medium, and high-resource languages and evaluated our approach on several NLP tasks.\nIn our evaluation of LLMs using different IFT-style datasets, we selected two generative tasks and one discriminative task to demonstrate the impact of our dataset. The study was limited to three tasks due to computational and time constraints. However, these tasks are popular and widely used in evaluating multilingual LLMs.\nIn future work, we plan to extend our evaluation to LLMs optimized for additional languages and explore multiple benchmarks within each language to better understand the native aspects of LLM performance."}]}