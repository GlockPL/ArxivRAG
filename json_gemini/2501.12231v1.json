{"title": "InsTALL: Context-aware Instructional Task Assistance with Multi-modal\nLarge Language Models", "authors": ["Pha Nguyen", "Sailik Sengupta", "Girik Malik", "Arshit Gupta", "Bonan Min"], "abstract": "The improved competence of generative models can\nhelp building multi-modal virtual assistants that leverage\nmodalities beyond language. By observing humans\nperforming multi-step tasks, one can build assistants that\nhave situational awareness of actions and tasks being\nperformed, enabling them to cater assistance based on this\nunderstanding. In this paper, we develop a Context-aware\nInstructional Task Assistant with Multi-modal Large\nLanguage Models (InsTALL) that leverages an online visual\nstream (e.g. a user's screen share or video recording)\nand responds in real-time to user queries related to the\ntask at hand. To enable useful assistance, InsTALL 1)\ntrains a multi-modal model on task videos and paired\ntextual data, and 2) automatically extracts task graph\nfrom video data and leverages it at inference time. We\nshow InsTALL achieves state-of-the-art performance\nacross proposed sub-tasks considered for multimodal\nactivity understanding- task recognition (TR), action\nrecognition (AR), next action prediction (AP), and plan\nprediction (PP)\u2013 and outperforms existing baselines on two\nnovel sub-tasks related to automatic error identification.", "sections": [{"title": "1. Introduction", "content": "In recent years, Multimodal Large Language Models\n(MLLMs) have shown remarkable advancements in vari-\nous multi-modal tasks [74]. For example, vision-language\nmodels have achieved significant success in areas such as\nvisual captioning and visual question answering [41, 46].\nIn essence, these models have demonstrated the ability to\nunderstand the visual content in images while being able\nto follow human instructions. To do so, these works often\nuse a lightweight adapter that connects a visual encoder\nto a language model and pre-trains the composite network\non large-scale multi-modal datasets, at times followed by\nfine-tuning on task-specific datasets for downstream appli-\ncations. Beyond images, researchers have further extended\nthe capabilities of MLLMs to consider procedural tasks in\nvideos (VideoLLM [14]; [8]). With the widespread avail-\nability of instructional videos that demonstrate multistep\ntasks [3, 50, 51, 60, 67, 89, 91], there is an opportunity to de-\nvelop systems that can understand the actions that are being\nperformed in the context of a task and provide context-aware\nassistance. In the paper, we seek to empower MLLMs to\nanswer real-time user queries related to various sub-tasks\nrelated to this goal, such as Task Recognition (TR), Action"}, {"title": "2. Related Work", "content": "Multimodal Large Language Models Prior works on\nMulti-modal LLMs consider using pre-trained encoders\nto transform images onto an LLM's input token space\n[31, 34, 64, 90]. Some works improve upon this base multi-\nmodal encoding mechanism, such as Flamingo [4] which\nuses a multi-modal cross-attention mechanism across all lay-\ners, while others like BLIP-2 [41] incorporate a lightweight\ntransformer model to merge image and text before the LLM\ninput stage. Subsequently, works have adopted similar prac-\ntices for other modalities, such as video [27, 62, 84] and\naudio [83]. PandaGPT [65] builds upon this and is able to\ncomprehend six different modalities simultaneously by inte-\ngrating a multimodal encoder [24]. The improvements have\nempowered recent works to explore multi-modal decision-\nmaking problems [30, 61, 73].\nInstructional Video Understanding. Beyond fully au-\ntonomous computer usage [6] or autonomous driving sce-\nnarios, lies a crucial realm of assistance that requires\na contextual understanding of visual cues and temporal\ngrounding [36, 85]. In such cases, the agent takes as in-\nput a video alongside a textual query and seeks to pro-\nvide assistance in textual format. These videos may be-\nlong to various domains, such as cooking [2, 57], daily\nactivities [11], indoor scenes [21], and movies [37]. Pre-\nvious approaches have relied on sliding window-based\nmethods [5, 21, 48] and scanning-and-ranking-based tech-\nniques [13, 15, 23, 45, 77, 82] for visual understanding. The\nleveraged video understanding can then be interpolated into\nthe text space to identify actions and enable procedure/task\nplanning based on the textual predicates/states and video\nframes [10, 12, 19, 42, 54, 66, 70, 71, 86] (where the latter\nworks have leveraged diffusion [28] and/or transformer [69]\nmodels). Recent developments in MLLMs reformulate the\nproblem of visual question answering with online video\nclips [14] by relying on the reasoning capabilities of the\nbackbone LLMs. Recent works have also critiqued the plan-"}, {"title": "2.1. Discussion", "content": "Our approach aims to provide a more comprehensive and\ninteractive experience for the instructional assistant. Specif-\nically, we (i) formally model the procedures involved in\nmulti-step tasks (Alg. 1) and (ii) generalize this knowledge\ninto a representation to support the assistant's understanding\n(Eqn. (6)). Furthermore, contextual awareness enables the\nassistant to (iii) flexibly train on different objectives for the\nlanguage model (Eqn. (1), (2), (3), and (4)). Moreover, our\napproach (iv) diversifies the user's queries, creating an on-\nline streaming dialog that simulates a natural conversation\n(Sec. 4.3 and Sec. 4.4). This is a significant advance over pre-\nvious work that has focused mainly on annotations for single-\nshot question-answering [80]. Through this comprehensive\napproach, our aim is to develop an instructional assistant\nwho not only understands the procedures and knowledge\ninvolved but also provides interactive assistance tailored to\nthe needs of the user and the state of the task at hand. Note\nthat our approach is a multimodal LLM-based technique that\nuniquely employs a procedural graph to model context and\nenhance recognition and forecasting capabilities. It is impor-\ntant to note that the predictions are not simply derived from\nthe graph mining process used in previous works [7, 45, 88]."}, {"title": "3. Objectives for Multi-task Learning", "content": "We seek to design a single Multi-modal LLM (MLLM) that\nis capable of performing well on several sub-tasks necessary\nfor clear instructional assistance. To achieve this, we define\na prompt Qtask for each task that enables the MLLM to adapt\nits behavior and outputs based on the assistance scenario at\nhand. We denote V = {vt | 0 \u2264 t < |V|} as the video\nassociated with a particular activity or task T (e.g. cooking\nomelette), where vt are action clips denoting an action\nat (e.g. fry eggs) used to perform the task. Now, we\ndescribe four tasks:\nTask Recognition (TR) Given a video snippet V and a\ntask prompt QTR, we seek to identify the task being per-\nformed by minimizing the following objective:\n$\\min E_{V,Y} \\left[ - \\sum_{i=1}^{n} Y_{i} \\log \\left( l_{Y} (p(T|V, Q_{TR});) \\right) \\right]$ (1)\nwhere T is the text response from the MLLM. As this is a\nclassification task, we expect a one-hot mapping that maps\nthe response to the set of task categories Y, i.e., denoted as\n1y(), and n = |Y|.\nAction Recognition (AR) Given a clipped video vt and\na task prompt QAR, we seek to identify the action being\nperformed in it by minimizing the following objective:\n$\\min E_{V_{t},y} \\left[ - \\sum_{i=1}^{m} y_{i} \\log \\left( l_{y} (p(a_{t}| V_{t}, Q_{AR});) \\right) \\right]$ (2)\nwhere at is the answer and yi is the action/step annotation\nfor clip vt (\u2208 V), and m = |y|.\nAction Prediction (AP) Given the task prompt QAP, a\nvideo upto a particular point v<t, we learn to predict the\nnext likely step at by minimizing the objective below:\n$\\min E_{V_{ty}} \\left[ - \\sum_{i=1}^{m} y_{i} \\log \\left( L_{y} (p(a_{t}|V_{<t}, Q_{AP});) \\right) \\right]$ (3)\nPlan Prediction (PP) Given the task prompt QPP, a video\nupto a particular point v<t, we seek to predict an ordered\nlist of actions a>t by minimizing the multiple-class mapping\nfunction Ty():\n$\\min E_{V_{t},y} \\left[ - \\sum_{i=1}^{m} y_{i} \\log \\left( T_{y} (p(a_{\\geq t}|V_{<t}, Q_{PP}) ,Q_{PP}) \\right) \\right]$ (4)\nwhere the number of procedural steps in |at| > |at| = 1.\nWith all the task objectives defined, we now explore how\nto teach LLMs all these objectives and incorporate additional\nknowledge from a procedural knowledge graph."}, {"title": "4. Developing InsTALL", "content": "In this section, we relax the assumption imposed by prior\nwork on developing online video assistance [14]; namely,\nits reliance on the dependency understanding capabilities of\nLLMs for procedural tasks. Specifically, we investigate how\nthe integration of procedural graphs can be used to generate\ncontextually accurate responses for the various tasks.\n4.1. Designing Multimodal LLM (MLLMS)\nOur model takes as input a video content V and a query Q,\nand auto-regressively generates a text response of length L\ndenoted as the target answer A = [X0,..., Xi, ..., XL-1].\n$\\p(A|V, Q) = \\prod_{i=0}^{L-1} p(x_{i}|V, Q, X_{<i})$ (5)\nThe model architecture, shown in Fig. 2, is similar to\nLLaVA [47]. It comprises of an image encoder, a temporal\naggregator, a Multi-Layer Perceptron (MLP) layer, and a\nlanguage model. For the image encoder, we utilize CLIP\nViT-L [17, 56] to extract embeddings for each video frame.\nThen, the model extracts spatio-temporal features using a\ngrid of image patches across multiple frames. Each frame\nembedding has N pooled spatial tokens where a temporal\naggregator compresses T \u00d7 N embeddings along the tempo-\nral axis. The resulting video embeddings from the temporal\naggregator are then projected using an MLP to frame tokens\nthat are then interleaved with language tokens as input to\na large language model. In our experiments, we consider\nthe Mistral-7B-Instruct [33] as the language model. Finally,\nwe add LoRA [29] parameters with every linear layer of the\nlanguage model for efficient learning of the tasks in \u00a73.\n4.2. Leveraging Procedural Graph\nIn addition to the video clips and the query, we also consider\na procedural graph G for generating the answer A.\n$\\p(A|V, Q, G) = \\prod_{i=0}^{L-1} p(x_{i}|V, Q, G, X_{<i})$ (6)\nProcedural Graph Construction Before the training\nphase, we construct a procedural graph by mining the train-\ning data using Alg. 1. The graph G = (VG, EG) consists of\na vertex set VG and an edge set EG. We obtain the nodes in\nVG using the function getAnn(\u00b7) : V \u2192 V which gets the\naction annotation (e.g., add milk) vt for clips of vt present\nin a task video V (e.g., how to make latte). The edges repre-\nsent temporally ordered transitions between two consecutive\nactions (Ut-1, Ut) observed in the task videos, which may be\ninstructional [2, 51, 67, 91] or procedural [35, 55] in nature.\nAn example subgraph in G is illustrated in Fig. 3."}, {"title": "Online Assistance", "content": "During the inference phase, we con-\nstruct an online search path Gt as the video scene unfolds.\nFor this, whenever we predict a change in action (using\naction recognition), we map it to a node in G. Given the\nauto-regressive model recognizes action at as free-form text,\nwe use a one-hot (similarity) mapping to select nodes in G\nand add it to a (predicted) online search path G (see Alg. 2).\n$\\node \\hat{v_{t}} = \\arg \\max (1_{v_{G}} (a_{t})), edge (\\hat{v_{t-1}},\\hat{v_{t}})$\n$\\G_{t} = (\\{\\hat{v_{0}}\\} + \\{\\hat{v_{t}}\\}, \\{(\\hat{v_{t-1}},\\hat{v_{t}})\\})$, t\u2208 (0, |V|) (7)\nProjecting an online video onto a predicted subgraph \u011c(E\nG) enables the possibility of leveraging \u011c alongside video\nand query embedding all the aforementioned tasks described\nin \u00a73. We hypothesize this reduces the burden of reasoning\n(needed for plan/action recognition and prediction) of the\nLLM by using plan prefixes G as part of the input.\nThe tasks defined in \u00a73 can now be defined as\n$p(A/V, Q_{TR}, G_{t})$  (TR)\n$p(a_{t}| V_{t}, Q_{AR}, (\\{\\hat{v_{0}}\\} \\{V_{<t}\\}, \\{(\\hat{v_{<t-1}},\\hat{v_{<t}}\\}})  (AR)\n$p(a_{t}| V_{<t}, Q_{AP}, G_{<t})$  (AP)\nFor Plan Prediction, we choose to look at two variants- with\n(PP+) and without (PP) knowing the target task T:\n$p(a_{>t}/V_{<t}, Q_{PP}, G_{<t})$,  (PP)\n$p(a_{>t}/V_{<t}, Q_{PP}, T, G_{<t})$  (PP+)\nwhere the number of procedural steps in |at| > |at| = 1."}, {"title": "4.3. Incorporating Conversational Context", "content": "To support streaming dialog with a user, previous works\nconsider annotation efforts that require significant human\neffort [14] or train models using single-shot question answer-\ning [80]. To overcome these limitations, our approach uses\nthe procedural graph to naturally generate this type of anno-\ntation. Both at training and inference time, a verbalization\nprocess is used for graph nodes. Specifically, we construct a\nconversational context by varying t \u2208 (0, |V|) and use the\nconversation template:\n{Stream:<VO>;User:<Q>;Assistant:<00>;...;\nStream: <vt>; User:<Q>;Assistant:<vt>},0 < t < |V|\nNegative Choice Question. We also augment the dialog\nguided by the procedure graph G by adding negative nodes\nthat are distinct from vt and its neighboring nodes N(vt).\nThese negative nodes v(\u2209 N(vt) Uvt), enhance the ability\nof the model to distinguish between relevant and irrelevant\ninformation in the dialogue stream:\n{Stream: <vt>; User:<Q>, should I <VEN(vt)Uvt>?;\nAssistant: No, you should do <vt> instead.}\nMultiple Choice Question is constructed via the template:\n{Stream: <vt>; User:<Q>, in one of <VEN(vt-1)>;\nYes, please do <vt>.}"}, {"title": "4.4. Error Detection", "content": "Beyond the five main tasks, also studied in prior works [14],\nour graph implementation allows us to perform well on two\n${V_{\\hat{G}} = {v_{0},..., v_{\\hat{V \\notin N (v_{t})\\cup v_{t}}}, \\cdot\\cdot\\cdot, v_{|V|-1}}}$ (8)\n$\\{\\mathcal{E_{G = } ... , (v_{t-1}, v_{\\neq{t}}, ...) \\}}$\n(9)"}, {"title": "5. Experiments", "content": "5.1. Benchmarks and Metrics\nIn our experiments, we consider two prominent video-\nbased datasets- COmprehensive INstructional video analysis"}, {"title": "5.2. Implementation Details", "content": "We employ CLIP-ViT-L-336 [17, 56] as the video frame\nencoder, a 2-layer MLP as the connector, and Mistral-7B-\nInstruct [33] as the LLM. Each video frame is encoded into\n10 tokens. Further, we use LoRA [29] for training, applying\nit to all linear layers with a rank of 128 and a scaling factor\nof 256. With a batch size of 128 and gradient accumulation\nover 16 iterations, we observe a training time duration of \u2248\n12 hours for 2 epochs when these runs are parallelized on 8\nA100 GPUs on AWS' P4d instances. We now consider some\nof the design choices made for our model architecture that\nwere made based on experimental results."}, {"title": "5.3. Procedural Graph Usage for Inference", "content": "After our procedural graph extraction, we can leverage it at\ninference time regardless of the Multi-modal LLM (MLLM)\nused for the online assistance tasks. In Table 8, we high-\nlight that considering the online graph path construction and\nincorporating it as input can unanimously improve the per-\nformance of any MLLM across all tasks (in \u00a73) and datasets.\nFor the VideoLLM-online model [14] on the COIN dataset,\nthe addition of our graph implementation (VQG) led to\nsubstantial improvements (notably, absolute gains of +8.2%\non AR, +13.7 on AP, and +8.7 on PP+) even when the base-\nline (VQ) used an enhanced version of Llama-3-8B. The\nGPT models [1] also benefited significantly from our ap-\nproach. When augmented with our graph implementation,\nGPT-40-mini showed improvements ranging from +2.5 to\n+16.9 percentage points across various tasks on both datasets.\nFurther, GPT-4-turbo and GPT-40 models also exhibited\nsubstantial improvements when integrated with our graph\napproach; notably, absolute gains of +21.7% and +17.8%\non AP for the two models respectively. These results rein-\nforce that augmenting dependencies explicitly (via our graph\napproach) instead of heavily relying on the planning capa-\nbilities of LLMs in a multi-modal setting can improve task\nperformance. We now show that leveraging the procedural\ntask graphs for multi-task learning can provide further gains."}, {"title": "5.4. Efficacy of InsTALL", "content": "In Table 7A, we provide a comprehensive comparison of our\nmethod InsTALL against various State-of-The-Art (SoTA)\napproaches for instructional video understanding on COIN.\nWhile, the InsTALL base model (VQ) achieves the second-best scores on action recognition (AR) and prediction (AP)\nbeating existing baselines, the latest work on VideoLLM-\nonline+ [14] shows second best performance on task recog-\nnition and plan prediction tasks that need longer dependency"}, {"title": "Error Detection", "content": "On the auxiliary tasks for detecting errors\nin action and ordering (\u00a74.4), we maintain the same train/test\nsplits across methods and report the average accuracy of\ncorrectly identifying action and ordering errors in Table 7C.\nInsTALL demonstrates a significant improvement over all\nbaselines when, esp. when using the procedural-graph imple-\nmentation boosing error detection for action from 40.9% to\n51.6%. For incorrect order detection, we observe a smaller"}, {"title": "6. Conclusion", "content": "In this paper, we presented a novel approach InsTALL for\ninstructional video understanding. InsTALL leverages graph-based representations in conjunction with visual and textual\nembedding for adapter-style Multi-modal LLMs (MLLMs).\nInsTALL demonstrates significant improvements across a\nwide range of tasks including Action Recognition, Action\nPrediction, Plan Prediction, Task Recognition, and Error\nIdentification. Injecting procedural task knowledge as graphs\ninto the LLMs, we provide an accurate and rich representa-\ntion of complex, multi-step processes, easing the reasoning\nburden on the LLMs. Extensive experiments showcase the\nconsistent superiority of InsTALL compared to a wide range\nof approaches, ranging from traditional video understand-\ning models and recent LLM-based approaches. Overall, our\nwork contributes to building a solution that achieves SoTA\nacross all tasks which is key for enabling visually aware\nassistants for procedural task videos.\nWhile the online graph yields unanimous\nimprovements, we observed, similar to prior work [58], that\nit can result in prediction errors as the model cannot faith-\nfully follow the dependencies in the graph. We note that such\nerrors compound across prediction steps, limiting their effi-\ncacy on plan prediction. We believe methods that improve\nways to incorporate structured knowledge and improve the\nreasoning abilities of LLMs will be the path forward."}]}