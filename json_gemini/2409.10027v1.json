{"title": "E2Map: Experience-and-Emotion Map\nfor Self-Reflective Robot Navigation with Language Models", "authors": ["Chan Kim", "Keonwoo Kim", "Mintaek Oh", "Hanbi Baek", "Jiyang Lee", "Donghwi Jung", "Soojin Woo", "Younkyung Woo", "John Tucker", "Roya Firoozi", "Seung-Woo Seo", "Mac Schwager", "Seong-Woo Kim"], "abstract": "Large language models (LLMs) have shown sig-\nnificant potential in guiding embodied agents to execute lan-\nguage instructions across a range of tasks, including robotic\nmanipulation and navigation. However, existing methods are\nprimarily designed for static environments and do not leverage\nthe agent's own experiences to refine its initial plans. Given that\nreal-world environments are inherently stochastic, initial plans\nbased solely on LLMs' general knowledge may fail to achieve\ntheir objectives, unlike in static scenarios. To address this limi-\ntation, this study introduces the Experience-and-Emotion Map\n(E2Map), which integrates not only LLM knowledge but also\nthe agent's real-world experiences, drawing inspiration from\nhuman emotional responses. The proposed methodology enables\none-shot behavior adjustments by updating the E2Map based\non the agent's experiences. Our evaluation in stochastic naviga-\ntion environments, including both simulations and real-world\nscenarios, demonstrates that the proposed method significantly\nenhances performance in stochastic environments compared\nto existing LLM-based approaches. Code and supplementary\nmaterials are available at https://e2map.github.io/.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs), pre-trained on Internet-\nscale data, have emerged as a promising method to encap-\nsulate the world's knowledge distilled in language. These\nLLMs have demonstrated a variety of capabilities, including\ninterpreting and responding to natural language instructions,\nperforming logical reasoning, and generating code. Leverag-\ning these capabilities, many studies have explored applying\nthe generalizable knowledge of LLMs to embodied agents,\nenabling them to interact physically in the real world.\nTo effectively utilize LLMs' knowledge in enabling em-\nbodied agents, it is crucial to ground this knowledge in the\nreal-world environments where the agents operate. Previous\nstudies have proposed methods to decompose language in-\nstructions into sequential subtasks, which are then executed\nusing predefined motion primitives [1]\u2013[3]. Other research\nhas explored the use of LLMs' code generation capabilities\nto translate language instructions into executable code for\nrobots, which is then used in conjunction with various APIs\n[4]-[6]. Additionally, some studies have focused on ground-\ning information from LLMs and vision-language models\n(VLMs) in the spatial contexts where robots operate [7]\u2013[9]."}, {"title": "II. RELATED WORKS", "content": "Research on using language instructions to control robots\nhas advanced significantly with the development of LLMs.\nNumerous studies [1]\u2013[7], [13]\u2013[17] have aimed to leverage\nthe general knowledge embedded in LLMs and the benefits\nof few-shot prompting, which eliminates the need for addi-\ntional training. [1]-[3] proposed a method that uses LLMs\nto decompose a given language instruction into a sequence\nof subtasks and employs predefined motion primitives to\nexecute each subtask. [4]-[6] utilized LLMs' capability\nof code generation to transform language instructions into\nexecutable code for robots, using various APIs to carry out\ntasks. [7] proposed utilizing LLMs to construct affordances\nand constraints within a 3D voxel map and employing model\npredictive control (MPC) to compute actions, eliminating the\nneed for predefined motion primitives.\nWhile these approaches have shown notable success in\nlanguage-based robotics, they primarily focus on static envi-\nronments and overlook the integration of the robot's experi-\nences. [17] demonstrated that it is possible to refine a robot's\nbehavior through additional human language instructions, but\nthis refinement does not stem from the robot's own expe-\nriences. In contrast, the proposed methodology addresses"}, {"title": "B. Visual-Language Navigation", "content": "To demonstrate the ability to achieve one-shot behavior\nadjustment in stochastic environments, we evaluated the\nproposed methodology in a mobile robot indoor navigation\nscenario. Navigation environments involve dynamic objects\nsuch as people, which can cause unexpected events, e.g., a\nperson suddenly stepping out of the door.\nVisual-language navigation (VLN), which uses a robot's\nvisual input to execute given language instructions, has been\nextensively researched to date. Early research used methods\nto map language instructions and visual input to discrete\n[19], [20], or continuous actions [21], [22]. However, these\nmethods are data-intensive, which raises concerns about\nthe cost of data collection. To address these problems,\nresearch has emerged that uses foundation models [8], [9],\n[18], [23]. [23] proposed a topological graph-based naviga-\ntion approach using three different foundation models. [18]\ndemonstrated object navigation using open-vocabulary object\ndetector based on CLIP [24] and exploration techniques. [8]\nproposed creating a prebuilt spatial map that stores visual-\nlanguage features of objects, while [9] suggested generat-\ning maps by storing pixel-level visual-language features of\nspaces. By grounding language instructions into these maps,\nboth approaches demonstrated their effectiveness in enabling\nspatial open-vocabulary navigation tasks. However, these\napproaches are limited to static environments and do not\naccount for the refinement of navigation plans based on the\nagent's real-time experiences. Table I presents a comparison\nbetween the proposed method and existing methods."}, {"title": "C. Learning from Experience", "content": "Methods for optimizing robot behavior based on past\nexperiences have been widely explored in reinforcement\nlearning [25]. Reinforcement learning quantifies rewards\nfrom experiences and trains models to maximize these re-\nwards. Similarly, in imitation learning, several studies have\nproposed acquiring additional expert data when the agent\nexperiences a situation it cannot solve on its own during\noperation [26], [27]. However, these approaches require\nextensive interactions with the environment or additional\ndata acquisition for learning, making immediate behavior\ncorrections impractical. In contrast, the proposed method\nleverages LLM prompting, avoiding parameter updates, and\ngrounds the agent's experiences in E2Map for immediate\nbehavior corrections in a one-shot manner."}, {"title": "III. METHOD", "content": "E2Map is a spatial grid map that captures both the\nvisual-language features of the environment and the agent's\nemotional responses to its experiences. In this paper, we\ndefine emotion as the spatial extent reflecting how the agent\nperceives the space to maintain homeostasis, inspired by\n[10]. We ground emotion within the spatial map by modeling\nit as a weighted summation of Gaussian distribution.\nM\nE2Map is mathematically defined as\n$\\RH\\times W \\times (C_{lang}+C_{emo})$, where H and W represent the\nsize of the top-down grid map, $C_{lang}$ denotes the dimension\nof the visual-language features for each grid cell, and $C_{emo}$\nindicates the number of parameters for emotion. Each grid\ncell represents s meters of actual space, so the size of the\nspace represented by E2Map is sH \u00d7 sW meters. The\nproposed methodology first builds $M_{lang} \\in RH\\times W \\times C_{lang}$\nby embedding visual-language features of RGB images of\nthe environment into a corresponding grid cell. Similar to\n[9], we used LSeg [28] as the pre-trained VLM to encode\nRGB images and determine the position of each pixel in\nthe grid map using depth images and camera poses.\nTo embed the emotion for each grid cell, we first define a\nmultivariate Gaussian distribution $\\mathcal{N}(x | \\mu_{p_i}, \\Sigma_{p_i})$ for each\noccupied grid cell $p_i = [x,y]^T$, where x \u2208 RH\u00d7W, $\\mu_{p_i} =$\n$[x,y]$ and $\\Sigma_{p_i} = [\\sigma_x^2,0;0,\\sigma_y^2]$. The covariance matrix is\ninitialized as an identity matrix scaled by a coefficient. In the\nremainder of the paper, we use the terms $\\mathcal{N}(\\cdot | \\mu_{p_i}, \\Sigma_{p_i})$\nand $\\mathcal{N}_{p_i}(\\cdot)$ interchangeably to denote the same distribution.\nFinally, the emotion is calculated as the weighted summation\nof multivariate Gaussian distributions, as follows:\n$$E(x) = \\sum_{p_i \\in O} w_{p_i} \\mathcal{N}_{p_i}(x),$$\nwhere O is the set of occupied grid cells, and $w_{p_i}$ is the"}, {"title": "B. Reflecting Emotion and Updating E2Map", "content": "The proposed methodology leverages the capabilities of\nLLM and LMM to update the E2Map based on the agent's\nnavigation experiences. First, when the agent encounters an\nevent, the event descriptor, an LMM, generates a language\ndescription that explains the image sequence of the situation.\nTo narrow the scope of the problem, we assume that the\nagent's events are indicated through a simulator or sensor\ninformation, rather than relying on a separate event detection\nalgorithm. Then, an emotion evaluator, an LLM, assesses the\nagent's emotional response to the experience. The emotion\nevaluator takes the language description of the situation as\ninput and evaluates the emotion as a score based on two\ncriteria. Finally, E2Map is updated considering the emotion\nscore and the grid cells' location to update. The proposed\nmethodology uses GPT-4o [29] for the event descriptor\nand Llama3 [30] for the emotion evaluator. A detailed\nexplanation of each process is provided below."}, {"title": "1) Event descriptor", "content": "The event descriptor generates a\nlanguage description that explains the sequence of images\nof the agent's experience. To enable the LMM to describe\nthe situation, we input three images to the model: the\nimage captured before the event $I_{evt-h}$, the image at the\ntime of event $I_{evt}$, and the image after the event $I_{evt+h}$,\nwhere $t_{evt}$ is the timestep when the event occurred and h\nis the hyperparameter. The LMM is then prompted with\nthese images to generate a description of the event. To\nenable the LMM to provide a detailed description of the\nsituation, we employ step-by-step reasoning, known for its\nstrong performance in zero-shot prompting [31]. Specifically,\ninstead of merely requesting a single description for the three\nimages, we prompt the LMM by first indicating that the\nimages represent sequential scenes of an event, requesting it\nto describe each scene individually and then combine them\ninto a comprehensive explanation of the entire situation.\n$$l_{evt} = \\mathcal{F}_{ed}(I_{evt-h}, I_{evt}, I_{evt+h}, p_{ed}),$$\nwhere $l_{evt}$ is the description of the event, $\\mathcal{F}_{ed}$ is the event\ndescriptor, and $p_{ed}$ is the prompt of the event descriptor."}, {"title": "2) Emotion evaluator", "content": "The emotion evaluator assesses the\nagent's emotional response based on the language description\nof the situation generated by the event descriptor. To enable\nthe LLM to evaluate the emotions related to the situation, we\nutilize few-shot prompting. An example of manually created\nfew-shot prompts is shown in Fig. 3. We defined two criteria\nfor evaluating emotions: upsetness, which represents the\nimpact the agent feels in the given situation, and guiltiness,\nwhich represents the impact on the environment caused\nby the agent's actions. Our criteria are inspired by human\nemotions that drive avoidance and reparative behavior [32]-\n[34]. We employed few-shot prompting to instruct the LLM\nto provide scores out of three for each criterion and to explain\nthe reasons behind these scores. The final emotion score is\ncomputed by adding the two scores.\n$$s_{emo} = \\mathcal{F}_{ee}(l_{evt}, p_{ee}),$$\nwhere $s_{emo}$ is the emotion score, $\\mathcal{F}_{ee}$ is the emotion evalu-\nator, and $p_{ee}$ is the few-shot prompt of emotion evaluator.\nNote that, although this study addressed negative emotions\nsuch as upsetness and guiltiness, demonstrations of the emo-\ntion evaluator for assessing positive emotions are provided\nin the supplementary material.\nTo update the E2Map, we first identify the locations of\nthe n grid cells, denoted as $p_{update}$, associated with the\nobject or space involved in the event, taking into account\nthe agent's pose at the time of the event. Next, we calculate\nthe unit direction vector $v = (v_x, v_y)$ representing the\nagent's orientation at the time of the event. This vector\nis used to adjust the emotion parameters of each relevant\ngrid cell based on the emotion score. We then update the\nstandard deviation of each diagonal element in the covariance\nmatrix by incorporating both the emotion score and the\ncorresponding component of the direction vector. Our update\nrule is inspired by the Weber-Fechner law [35], which states\nthat the intensity of a sensation increases as the logarithm of\nthe stimulus. The standard deviation is updated as follows:\n$${\\sigma_k^{new}} = \\sigma_k v_k \\log\\left(\\frac{s_{emo}}{\\tau}\\right), \\text{where } k \\in \\{x,y\\}, $$\n$(.)^{new}$ refers to the updated value or function, and $\\tau$ is the\ntemperature parameter.\nBy updating the covariance matrix, the multivariate Gaus-\nsian distribution expands, which means the emotional re-\nsponse to the event affects a broader spatial area. However,\nsince a multivariate Gaussian distribution is a probability\ndensity function that integrates to one over all regions,\nincreasing the covariance reduces the value of the distribution\nat the grid cell's location. This results in a smaller emotion\nvalue at the grid cell according to Eq. (1). To ensure that\nthe emotion at the grid cell is maintained even after the\ncovariance update, we propose a simple method for updating\nthe weight parameter $w_{p_i}$ as follows:\n$$w_{p_i}^{new} = w_{p_i} \\frac{\\mathcal{N}_{p_i}(p_i)}{\\mathcal{N}_{p_i}^{new}(p_i)},$$\nBy updating the emotion parameters according to Eq. (5) and\nEq. (6), we can expand the spatial extent of the emotion's\ninfluence by recalculating Eq. (1), while preserving the\nmagnitude of the emotion within the given grid cell."}, {"title": "C. Navigating with E2Map", "content": "In this section, we explain how the proposed system\nworks using E2Map. The overall algorithm is presented in\nAlgorithm 1. When a language instruction $l_{inst}$ is received\nfrom the user, the goal selector converts this instruction into\ncode that utilizes the goal selection APIs to determine the\ngoal locations (line 1), as shown in Fig. 2-(b). The goal\nselection API returns the location of the grid cell that the\nrobot needs to visit, considering the object mentioned in the\nlanguage instruction and the spatial information related to\nthat object. A detailed explanation of the goal selector is\nprovided in the supplementary material.\nOnce the goal is determined, the agent can use any cost-\nbased off-the-shelf navigation system to move towards the"}, {"title": "IV. EXPERIMENTS", "content": "We conducted experiments in both simulated and real-\nworld environments. Using the ROS Gazebo simulator [40],\nwe created a simulated environment that mirrored the real-\nworld setup used for evaluation, as shown in Fig. 4. In\nthe simulated environment, we designed three scenarios to\nassess our method (Fig. 5). First, after building the initial\nE2Map, we introduced a new static obstacle, such as a danger\nsign (danger sign), to evaluate the method's ability to adapt\nto environmental changes. Second, we positioned a human\nfigure behind a wall and had a human step out unexpect-\nedly (human-wall). Third, we added a door that opened\nunexpectedly as the robot approached (dynamic door). The\nhuman-wall and dynamic door scenarios were designed to\ntest whether our method could adjust behavior based on\nexperiences with dynamic events.\nFor quantitative analysis, we compared our method\nagainst baselines with state-of-the-art performance in open-\nvocabulary object navigation [9], [23]. Our method and\nthe baselines received identical language instructions and\nnavigated accordingly, with success rates calculated for per-\nformance evaluation. Details about the baseline methods are\nprovided in the supplementary material. After evaluation in\nthe simulated environment, we demonstrated the scalability\nand applicability of our method in real-world scenarios."}, {"title": "B. Experiments in Simulated Environment", "content": "In the simulated environment, we provided ten different\nlanguage instructions for both our method and the baselines\nfor each of the three scenarios and calculated the success rate.\nFor each episode, the robot started from the same position,\nand the language instructions referred to a maximum of four\nobjects. Full language instructions are provided in the supple-\nmentary material. Success was defined as the robot reaching\nall goals in order without collisions, with a goal considered\nreached if the robot was within a specified distance from it.\nThe episode was reset if a collision occurred or the agent\nreached the goal. Table. II showed the results.\nWe first evaluated our method and the baselines in a static\nenvironment, without environmental changes or dynamic\nevents, to assess their baseline performance using language\ninstructions from the danger sign scenario. As shown in\nthe table, both our method and VLMaps succeeded in all\nten attempts, while LM-Nav succeeded only five times.\nThe challenges LM-Nav faces in multi-goal navigation were\npreviously noted in [9]. In scenarios involving environmen-\ntal changes and dynamic events, our method outperforms\nbaselines in all three scenarios, as shown in the table. We\nfound that the baselines repeatedly collided with the danger\nsign, human figure, and dynamic door, despite having previ-\nously encountered similar situations. In some cases, LM-Nav\nsucceeded in the danger sign and dynamic door scenarios\nbecause its topological graph-based goal selection algorithm\noccasionally chose goal positions where the robot avoided\nencountering the danger sign and the door. In contrast, in\nthe danger sign scenario, while our method initially collided\nwith the danger sign, it avoided it in subsequent encounters\nby updating the E2Map. Similarly, in the human-wall and\ndynamic door scenarios, after experiencing collisions with\nthe human figure and dynamic door, our method adapted\nits behavior to maintain a safe distance from both the wall\nand the door when passing by them. The qualitative result\nof our method in human-wall scenario is provided in Fig.\n6. The results demonstrated that our method can adjust the\nagent's behavior in a one-shot manner, leading to a higher\nsuccess rate in scenarios with environmental changes and\ndynamic events. Examples of qualitative results from the\nevent descriptor and emotion evaluator for each scenario are\nprovided in the supplementary material."}, {"title": "C. Experiments in Real World", "content": "To evaluate the scalability and applicability of our method\nin real-world settings, we first set up a real-world envi-\nronment by placing objects such as a sofa, chair, table,\nrefrigerator, and microwave in the conference room at Seoul\nNational University, as shown in Fig. 7. We used the same"}, {"title": "V. CONCLUSION", "content": "In this paper, we proposed E2Map, a spatial map that\ncaptures the agent's emotional responses to its experiences,\ninspired by human emotional mechanisms. By updating\nE2Map using various LLM capabilities, the agent can adjust\nits behavior in a one-shot manner after encountering spe-\ncific events. Experiments in both simulated and real-world\nenvironments demonstrated that our method significantly\nimproves navigation performance in stochastic scenarios.\nHowever, there are several promising directions for future\nwork. Our method relies on simulator or sensor information\nto trigger E2Map updates. Integrating anomaly detection\nalgorithms to autonomously identify such events would be\na valuable enhancement. Furthermore, although we demon-\nstrated that our method can also address positive emotions,\nour experiments primarily involved scenarios with negative\nemotions. Future work could explore the extension of the\nmethodology to tasks involving positive emotions to provide\na more comprehensive evaluation of its applicability. We be-\nlieve that our work contributes to advancing the development\nof strong autonomy in robotics."}]}