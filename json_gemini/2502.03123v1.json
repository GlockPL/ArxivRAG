{"title": "Disentanglement in Difference: Directly Learning Semantically Disentangled Representations by Maximizing Inter-Factor Differences", "authors": ["Xingshen Zhang", "Shuangrong Liu", "Xintao Lu", "Chaoran Pang", "Lin Wang", "Bo Yang"], "abstract": "In this study, Disentanglement in Difference (DiD) is proposed to address the inherent inconsistency between the statistical independence of latent variables and the goal of semantic disentanglement in disentanglement representation learning. Conventional disentanglement methods achieve disentanglement representation by improving statistical independence among latent variables. However, the statistical independence of latent variables does not necessarily imply that they are semantically unrelated, thus, improving statistical independence does not always enhance disentanglement performance. To address the above issue, DiD is proposed to directly learn semantic differences rather than the statistical independence of latent variables. In the DiD, a Difference Encoder is designed to measure the semantic differences; a contrastive loss function is established to facilitate inter-dimensional comparison. Both of them allow the model to directly differentiate and disentangle distinct semantic factors, thereby resolving the inconsistency between statistical independence and semantic disentanglement. Experimental results on the dSprites and 3DShapes datasets demonstrate that the proposed DiD outperforms existing mainstream methods across various disentanglement metrics.", "sections": [{"title": "1. Introduction", "content": "Disentangled representation learning has been recognized as crucial for artificial intelligence to achieve genuine understanding of the world (Bengio et al., 2013; Lake et al., 2017). Its core objective lies in acquiring the information-rich latent representation to describe the semantic factors or attributes that consist of the object(Higgins et al., 2018). In the disentangled representation learning, independent properties of an object, such as color, size, shape, and pose, would be distilled and assigned to different latent dimensions without interference. This separation enables precise manipulation and interpretation of specific attributes. Disentangled representations are considered to hold significant potential for controllable image generation(Zhu et al., 2018; Gabbay & Hoshen, 2019; 2021), reducing sample complexity (Bengio et al., 2013; Ridgeway & Mozer, 2018; Van Steenkiste et al., 2019), interpretability (Bengio et al., 2013; Higgins et al., 2017), and performance enhancement in downstream tasks (Locatello et al., 2019a;b; Bottou et al., 2007; Higgins et al., 2018; Peters et al., 2017; Schmidhuber et al., 1996; Tschannen et al., 2018).\nMany existing disentanglement representation learning approaches indirectly encourage disentanglement by minimizing statistical dependencies between latent variables (e.g., Total Correlation(TC)(Watanabe, 1960)). The underlying assumption is that statistical independence may correlate with semantic factor disentanglement. Consequently, if latent variables are statistically independent, each variable is presumed to be more likely to encode a single independent factor from the data generation process. For example, B-VAE (Higgins et al., 2017) enforces alignment of the latent variable distribution $q(z)$ with a Gaussian prior $p(z)$ by increasing the weight 8 of the KL divergence term in the variational lower bound. Although this method does not explicitly penalize statistical dependencies between latent dimensions, the increased \u1e9e value indirectly encourages alignment with independent priors, thereby partially reducing inter-dimensional correlations. FactorVAE (Kim & Mnih, 2018) and B-TCVAE explicitly introduce Total Correlation (TC) as a regularization term, directly measuring and penalizing the KL divergence between the joint distribution $q_{\\phi}(z)$ and the product of marginal distributions $\\Pi_{j} q_{\\phi}(z_j)$. Minimizing the TC term effectively reduces statistical dependencies among latent variables. DIP-VAE (Kumar et al., 2017b) proposes to diminish linear dependencies between dimensions by constraining the covariance matrix of latent variables to align with the identity matrix.\nHowever, the computation of statistical dependencies faces inherent limitations. As noted by (Locatello et al., 2019a),"}, {"title": "2. Related Work", "content": "Current approaches in unsupervised disentangled representation learning are predominantly based on Variational Autoencoder (VAE) (Kingma, 2013) or information-theoretic Generative Adversarial Network (InfoGAN) (Chen et al., 2016) frameworks. These methods generally share a core principle: introducing additional regularization terms into the model's loss function to reduce statistical dependencies among latent variables, thereby promoting disentangled representations.\nVAE-based Methods. Among VAE-based approaches, B-VAE (Higgins et al., 2017; Burgess et al., 2018) stands as a seminal work. By introducing a tunable hyperparameter \u1e9e into the Evidence Lower Bound (ELBO) loss function, this method constrains the posterior distribution of latent variables. While its objective is to encourage alignment between the posterior distribution and a predefined independent prior (typically an isotropic Gaussian), increasing the weight of the KL divergence term (\u03b2 > 1) effectively restricts the latent space capacity and induces lower statistical dependencies across dimensions. However, selecting an optimal \u1e9e remains challenging, as higher values may degrade reconstruction fidelity.\nTo more directly minimize statistical dependencies, Factor-VAE (Kim & Mnih, 2018) employs an adversarial learning strategy. It introduces a discriminator to distinguish between latent codes sampled from the aggregated posterior distribution and those from an independent prior distribution, thereby explicitly penalizing the Total Correlation (TC) of latent variables. This approach directly optimizes the aggregated posterior to approximate an independent distribution.\nBuilding upon this, \u03b2-TCVAE (Chen et al., 2018) decomposes the KL divergence term in the VAE objective into three components: index-code mutual information, total correlation (TC), and dimension-wise KL divergence. By assigning distinct weights to these components, B-TCVAE achieves finer-grained control over latent dependencies. Its central innovation lies in explicitly penalizing TC to reduce interdimensional correlations.\nGAN-based Methods. Within the domain of GAN-based methods, InfoGAN (Chen et al., 2016) a significant contribution, giving rise to a series of InfoGAN-based variants, including InfoGAN-CR and PS-SC GAN. The objective of InfoGAN is to learn interpretable latent representations by maximizing the mutual information between a subset of the latent variables and the output of the generator. Although its primary objective is not the direct minimization of statistical dependencies across all latent variables, InfoGAN indirectly facilitates a reduction in statistical dependencies between specific latent"}, {"title": "3. Methodology", "content": "3.1. Formulation\nMany existing disentangled representation learning methods often indirectly achieve disentanglement by penalizing the statistical dependencies among latent variables. However, reducing statistical dependencies does not fully guarantee improved disentanglement capability. Therefore, this paper aims to design a learning paradigm that directly promotes the enhancement of disentanglement capability, starting from the semantic differences between different latent factors.\nWe assume that the semantic differences between distinct latent factors should be reflected in the variations of a series of samples across their attributes, and these variation patterns should exhibit human-interpretable regularity. In other words, the semantic variations of latent factors arise from the differences and changes among samples, while these differences are not present in individual samples. Since the semantic differences of factors originate from sample differences, the model should learn the semantic differences of factors from these sample differences, thereby directly promoting the improvement of the model's disentanglement capability.\nTo learn sample differences, we first need to map the samples from the dataset into a latent space that conforms to the dataset distribution. At this stage, the latent space is relatively disordered, and individual dimensions do not effectively represent factors. Subsequently, we employ a pairwise contrastive disentanglement prior to organize this disordered latent space, gradually enabling the latent space to exhibit characteristics of mutually independent and orderly arranged factors. Specifically, the pairwise contrastive method posits that for sample pairs dominated by variations in the same latent factor, the vector difference in the representation space should be significantly smaller than that of sample pairs driven by variations in different latent factors. Essentially, variations in samples caused by different latent factors should be mapped to larger distances in the representation space, while variations caused by the same factor"}, {"title": "3.2. Framework of the DiD", "content": "Based on the aforementioned disentanglement principles, this study proposes the Disentanglement in Difference (DiD) model. This model primarily consists of three core modules: a Sample Generation Paradigm responsible for learning the real data distribution and generating high-quality samples; a Difference Encoder, designed to encode the differences between sample pairs caused by distinct latent factors and map these differences to a representation space, thereby capturing the influence of latent factors; and a Sample Encoder, used to map samples to the latent space and obtain their representations within this space.\nAs shown in Figure 1, the workflow of DiD is as follows: As the sample generation component for disentangled representation learning, a vector c is first sampled from an n-dimensional latent space S as input to a generator G, generating a sample x. The generated sample x simultaneously serves as input to both a discriminator D and the Sample Encoder for their respective training. This ensures the model's ability to generate high-quality samples while also obtaining latent space representations of the input samples. To achieve factor disentanglement, we select two latent representations from the latent space S, each controlled by specific dimensions, and input these two representations separately into the generator G, thereby obtaining two sets of sample pairs. Specifically, it is assumed that within each sample pair, the two samples differ only in one latent space dimension, while remaining consistent across other factors. These two sets of sample pairs are then fed into the Difference Encoder, yielding two vectors $v_1$ and $v_2$. These vectors respectively represent the variations between the corresponding sample pairs caused by specific latent factors. Consequently, the distance between $v_1$ and $v_2$ should be maximized. However, as the sample pairs are selected from dimensions within a disordered latent space, $v_1$ and $v_2$ are not initially maximized. By maximizing the distance between vectors $v_1$ and $V_2$, we encourage the generator G to generate disentangled sample variations along independent latent factors within the latent space S."}, {"title": "3.3. Samples Generation Paradigm", "content": "The sample generation paradigm employs a general sample generation network, such as DCGAN, WGAN, etc. In this paper, WGAN-GP is utilized as the sample generation paradigm to maintain high-quality generation performance.\nTo establish a correspondence between each dimension of the latent space and a single factor, it is necessary to construct a low-dimensional latent space that effectively represents the samples of the dataset and supports free and continuous exploration within this space. We need to rationally design the distribution of the latent space. Specifically, we desire the latent space to possess finite boundaries to prevent the model from over-exploration or getting trapped in irrelevant regions. Simultaneously, to encourage the model to fully utilize the capacity of the latent space, learn complete information about the data distribution, and mitigate the phenomenon of latent representation aggregation, we choose a bounded prior distribution that follows a uniform distribution $c \\sim U[-1,1]^n$ as the prior constraint for the latent space. To effectively map samples from the real dataset into this latent space, sample points from this space are used as input to the sample generation paradigm. The generator and discriminator are optimized by minimizing the Earth-Mover distance (also known as the Wasserstein distance) between the generated distribution and the real distribution, ensuring that the latent space can effectively fit the real distribution. The WGAN-GP loss is given by:\n$L_G = -E_{c \\sim p(c)}[D(G(c))]$\n$L_D = E_{x \\sim P_{data} (x)} [D(x)] \u2013 E_{c \\sim p(c)} [D(G(c))] + XE_{x \\sim P}[ (||\\bigtriangledown_x D(x)||_2 \u2212 1)^2]$"}, {"title": "3.4. Difference Encoder", "content": "To enable the model to learn differentiated representations that effectively capture variations in distinct latent factors, we introduce a Difference Encoder, denoted as H. This network architecture can be realized using multi-layer Convolutional Neural Networks (CNNs) or Multi-Layer Perceptrons (MLPs). The objective of the Difference Encoder is to maximize the distance in the representation space between generated sample pairs resulting from variations in different latent factors.\nSpecifically, we first sample a latent code, c, from an n-dimensional latent space, S. We then select two dimensions, denoted $a_1$ and $a_2$, from S centered at c. These dimensions correspond to two mutually independent latent factors that we aim to disentangle. To represent variations controlled by these factors, we perturb c along these dimensions, obtaining new latent codes c' and c\". For instance, c' is obtained by moving a short distance from c along the $a_1$ direction, while c\" is obtained by moving a short distance from c along the $a_2$ direction. Subsequently, we input c, c', and c\" to the generator, G, to obtain the corresponding generated samples: x = G(c), x' = G(c'), and x\" = G(c\"). To learn the semantic difference, two sets of sample pairs are constructed: $g_1 = [x,x']$ and $g_2 = [x,x\"]$. The sample"}, {"title": "3.5. Samples Encoder", "content": "Since disentangled representation models typically need to possess the capability to obtain effective latent space representations of samples, we introduce a Sample Encoder E to achieve this. This encoder takes the samples G(c) generated by the sample generation paradigm as input and aims to output the encoded representation $c'_{enc}$ of this sample in the latent space. Our goal is to make the encoder's output $c'_{enc}$ as close as possible to the original latent code c used to generate the sample. To achieve this objective, we employ a method of minimizing the difference between the encoder's output and the generator's input to train the encoder. Specifically, we optimize the encoder's parameters by minimizing the Mean Squared Error (MSE) loss function $L_{enc} = ||C- C'_{enc}||^2$. Through iterative optimization, we expect the encoder E to learn a mapping from the generated sample space to the latent space, such that for a given generated sample, the encoder can accurately map it back to its corresponding location in the latent space. The encoder reconstruction loss is given by:\n$L_{enc} = ||C - C_{enc}||^2 = ||c \u2013 E(G(c))||^2$\nThe total loss function when training the generator is:\n$L = L_G + L_H + L_{enc}$\nThe total loss function when training the discriminator is:\n$L = L_D$"}, {"title": "4. EXPERIMENTS", "content": "4.1. Datasets\nTo systematically evaluate the effectiveness of the proposed Differentiable Information Disentanglement (DiD) model,"}, {"title": "4.2. Evaluation Metrics", "content": "While a standard metric for measuring disentanglement is lacking, numerous approaches exist to evaluate it based on intervention, predictor, and information. To quantitatively evaluate the disentangling performance of the model and make fair comparisons with existing methods, we adopted three widely recognized and complementary disentangling evaluation metrics: Mutual Information Gap (MIG) , Disentanglement, Completeness, and Informativeness (DCI) , and Separated Attribute Predictability (SAP).\nThe MIG metric evaluates the degree of disentanglement by calculating the mutual information between each latent dimension and its most relevant attribute, then measuring the gap between the highest and second-highest mutual information. A higher MIG value indicates a stronger correlation between the learned representation and the latent factors, reflecting better disentangling performance.\nThe DCI framework evaluates the quality of representations from three dimensions: disentanglement, completeness, and informativeness. In this study, we focus primarily on the disentanglement metric (DCI-D). DCI-D is calculated by training linear classifiers that predict the values of latent factors based on individual latent dimensions. The disentanglement score is computed based on the accuracy of these"}, {"title": "4.3. Implementation Details", "content": "To ensure the reproducibility and fairness of the experiments, we describe the implementation details of the DiD model as follows:\nThe DiD model adopts a modular network design, including a generator, discriminator, encoder, and difference encoder. The generator uses a deconvolutional neural network (DCNN) structure, responsible for generating images from the latent space. Both the discriminator and encoder use convolutional neural networks (CNNs), which distinguish real images from generated ones and map input images to the latent space, respectively. The difference encoder employs a multi-layer perceptron (MLP) to estimate the difference information in the latent representations. All network layers use LeakyReLU activation functions to enhance the model's non-linear fitting capacity and training stability. The parameters of the generator, encoder, and difference encoder are optimized using the RMSProp optimizer with a learning rate of 1 \u00d7 10-4. The discriminator is optimized using the Adam optimizer with a learning rate of 1 \u00d7 10-4, and momentum parameters \u03b2\u2081 = 0.5 and B2 = 0.999. All models are trained using batch gradient descent with a batch size of 128."}, {"title": "4.4. Quantitative Analysis", "content": "To quantify the disentangling performance of the DiD model and compare it with current mainstream disentangling methods, we conducted comprehensive experimental evaluations on the dSprites and 3DShapes datasets. The DiD model was compared with a series of representative baseline models, including B-VAE, \u03b2-TCVAE, FactorVAE, Dynamic-VAE, and InfoGAN-CR. All models were trained for 500,000 steps on both datasets, and the disentangling performance was quantitatively assessed using the MIG, DCI-D, and SAP metrics.\nTo ensure the fairness of the comparison, we conducted a detailed hyperparameter tuning for all baseline models and report the best hyperparameter configurations that achieved the highest disentangling performance on the validation set. Specifically, for \u1e9e-VAE, we selected \u03b2 = 4; for \u03b2-TCVAE, we selected \u03b2 = 6; for FactorVAE, we selected y = 4; and for Dynamic-VAE, we selected K\u2081 = 0.001, Kp = 0.01."}, {"title": "4.5. Total Correlation and Disentangling Metrics", "content": "As mentioned earlier, existing methods encourage disentangling by penalizing the statistical dependence between latent variables (e.g., total correlation). However, these two aspects are not strictly equivalent. To verify this hypothesis, we conducted a comparative analysis of the dynamic relationship between total correlation (TC) and disentangling metrics (MIG) during the training process of \u03b2-VAE, \u03b2-TCVAE, and FactorVAE. The experiment used the dSprites dataset, with hyperparameters fixed at \u03b2 = 4 and batch size = 128 to control variable effects.\nAs shown in Figure 2, while training effectively reduced total correlation among latent variables in all three models, the Mutual Information Gap (MIG) metric displayed considerable volatility and failed to exhibit a corresponding consistent improvement. In fact, a declining trend in MIG was sometimes evident. This indicates that the reduction of statistical dependencies and the enhancement of disentanglement are not directly proportional. Consequently, directly penalizing statistical dependencies alone may not be a robust method for achieving reliable disentanglement."}, {"title": "4.6. Ablation Study on the Difference Estimator", "content": "This section presents an investigation into the core functionality of the Difference Estimator within our proposed model framework. Our analysis revolves around two key directions: firstly, exploring the indispensable role of the Difference Estimator for disentangled representation learning, and secondly, examining the quantitative effect of manipulating the number of comparable dimensions within the Difference"}, {"title": "5. Conclusion", "content": "This paper introduces a novel disentanglement representation learning approach that learns by encoding differences between latent factors. By encoding semantic differences between factors, this approach encourages the model to map disparate factors to more distant regions in the latent space, aligning with the intuition that semantically distinct factors should exhibit greater separation. On simple datasets comprised of explicit factors, we demonstrate our method's effectiveness at achieving strong disentanglement performance. Furthermore, an ablation study confirms that explicitly learning semantic differences between factors significantly enhances the model's disentanglement capabilities."}]}