{"title": "Computable Model-Independent Bounds for Adversarial Quantum Machine Learning", "authors": ["Bacui Li", "Tansu Alpcan", "Chandra Thapa", "Udaya Parampalli"], "abstract": "By leveraging the principles of quantum mechanics, QML opens doors to novel approaches in machine learning and offers potential speedup. However, machine learning models are well-documented to be vulnerable to malicious manipulations, and this susceptibility extends to the models of QML. This situation necessitates a thorough understanding of QML's resilience against adversarial attacks, particularly in an era where quantum computing capabilities are expanding. In this regard, this paper examines model-independent bounds on adversarial performance for QML. To the best of our knowledge, we introduce the first computation of an approximate lower bound for adversarial error when evaluating model resilience against sophisticated quantum-based adversarial attacks. Experimental results are compared to the computed bound, demonstrating the potential of QML models to achieve high robustness. In the best case, the experimental error is only 10% above the estimated bound, offering evidence of the inherent robustness of quantum models. This work not only advances our theoretical understanding of quantum model resilience but also provides a precise reference bound for the future development of robust QML algorithms.", "sections": [{"title": "I. INTRODUCTION", "content": "Machine learning (ML) is increasingly integral in various applications such as speech recognition [1], computer vision [2], and natural language processing [3]. A significant concern in ML is its vulnerability to adversarial attacks, in which malicious actors exploit weaknesses in the models to produce outcomes favorable to the adversary [4]\u2013[6]. Moreover, it is demonstrated that various ML classification models are at risk when adversaries craft examples specifically designed to mislead them [7], [8].\nAlongside classical machine learning (CML), quantum machine learning (QML) has emerged as a promising new paradigm. Conceptually similar to classical counterparts, QML involves iteratively optimizing models across training samples to achieve intended functionalities [9]. The key distinction between quantum and classical models lies in the use of quantum computing. QML exploits quantum mechanical phenomena such as superposition and entanglement, which are core principles of quantum computing. The potential of quantum computing has been demonstrated in various domains [10]-[13]. Moreover, in the context of ML, quantum systems offer the possibility of manipulating exponentially large state spaces efficiently [14].\nQML shows promise but faces challenges in practical implementation, particularly regarding vulnerability to adversarial attacks. While some studies suggest QML models may exhibit resilience to certain adversarial transfer attacks designed for classical models [15], the overall security landscape remains open. Studies have primarily focused on adversarial examples using classical input data in trained quantum classification models, observing similar misclassification phenomena to those in classical models [16]. However, these insights provide limited guidance for designing robust QML models. The field is further constrained by limitations in classical simulation capabilities and noisy quantum hardware, hindering extensive experimental studies. Theoretically, the security of QML in handling exponentially larger spaces remains debated [17]. However, some research indicates that, under certain assumptions, the scaling of security risks in QML is comparable to that in CML [18].\nAlong with these results, in the context of adversarial attacks, it is crucial to have a fundamental understanding of adversarial performance limitations posed by the data distribution and model accuracy. Such limitation can be formulated as a bound on adversarial error rate, which captures the chance of success for an adversary to induce an incorrect outcome for QML models. This provides detailed insights"}, {"title": "II. BACKGROUND AND LITERATURE REVIEW", "content": "Quantum machine learning (QML) is an emerging field first proposed in 2019 [21], [22]. It integrates concepts from classical machine learning (CML) with developing quantum computing technologies. Unlike CML, which relies heavily on classical computing and is driven by extensive trial-and-error experimentation [23]\u2013[25], QML is expected to rely mostly on the development of quantum computing technologies. However, the current stage of quantum technology, primarily Noisy-Intermediate-Scale Quantum (NISQ) devices, presents challenges for establishing far-reaching QML results across different quantum hardware with a wide range of capabilities and limitations [26]-[29].\nApart from the challenges posed by the vastly different quantum hardware in research, there is also a significant gap between the simulation of quantum models and the physical implementation of them. This further exacerbates the challenge of producing far-reaching results via experimentation similar to the CML approach. Various simulated QML models have been tested, including supervised learning [30]\u2013[32], unsupervised learning [33]\u2013[35], and reinforcement learning [36]. Though often utilizing impractical oracle gates such as n-qubit amplitude encoding and n-qubit universal gate and assuming negligible noise, these models show promise in solving complex problems as quantum computing quality and scale improve. On the other hand, some experiments are also conducted on quantum hardware [37], [38]. Compared to the simulated works, these experiments are generally less complex due to limited qubits and high noise levels. These preliminary researches highlight a gap between the future practical quantum model and what we currently can analyze or experiment with. Thus, in our work, we adopt a less experimental approach and investigate the data structure and the allowed robustness for any potential model learning from the data. Even when facing these challenges, our results enable guidance in QML."}, {"title": "B. ADVERSARIAL QUANTUM MACHINE LEARNING", "content": "Many security challenges that CML faces stem from the models' need for trust in the data sources, lack of interpretability, and intrinsic vulnerability to unexpected and deliberate adversarial manipulation. As QML seeks to leverage quantum advantages within the framework of machine learning, it still inherits these classical challenges. Therefore, the security issues faced in CML could transfer or even exacerbate in QML environments. Fortunately, many theories and methods can be transferred or adapted from (classical) adversarial machine learning to QML. For example, the phenomena where a moderate increase in data dimension may be detrimental to the adversarial performance has been analyzed in the QML settings [17], [39], and the classical method of adversarial training has been introduced to QML [16], [17], [39], [40].\nIn the field of adversarial QML, one fundamental and popular problem category is the evasion attack and its defense, where adversaries attempt to stealthily induce incorrect decisions by exploiting the model's weakness [15], [41], [42]. For classification problems, we expect a similar adaptation and knowledge transfer. For classification tasks in CML, advancements in machine learning models have enabled high-accuracy predictions of class labels. However, the accuracy can drop significantly when test samples are perturbed by adversaries [19]. While heuristic defenses against adversarial attacks have been developed, many remain vulnerable to newer, adaptive attacks [4], [43]. For quantum machine learning development with large and deeper layers of quantum circuits and a higher number of qubits, we also expect a steadily increasing accuracy in non-adversarial settings and a long-lasting competition between attack and defense strate-"}, {"title": "C. ADVERSARIAL ATTACKS TO QML", "content": "In the field of (classical) adversarial machine learning, a broad spectrum of attacks and defenses are explored. However, in Quantum Machine Learning (QML), research predominantly revolves around evasion attacks and corresponding defensive strategies [15], [16], [47]. In our work, we align our attack scenarios with the established frameworks of prior research. This alignment ensures that our methodology not only contributes to a deeper understanding of existing results but also aids in guiding future research that seeks to build upon these frameworks. We shall give a rigorous definition of these evasion attacks in Sec. III. The difference between the quantum encoding stage quantum perturbation attack and the classical input perturbation attack is shown in Fig. 1\nIn the context of an evasion attack within a quantum setup, there is a unique step: the encoding of information onto the quantum computer. This step is bypassed when the input is inherently quantum. The attacker might target the system before the encoding of classical information, resulting in a conventional attack scenario restricted to a perturbation strength e under classical distance metrics, such as l\u221e, 12 and 11 distances [15], [16], [47]. Conversely, when the attacker gains access to the encoded quantum state or the model's inputs are quantum states, an attack that alters quantum states"}, {"title": "D. BOUNDS ON ADVERSARIAL ERRORS", "content": "In QML literature, the adversarial error (rate), the error rate of a model under an adversarial attack, is the most commonly used performance metric. The rigorous definition of non-adversarial error rate and adversarial error rate will be discussed in Sect. III-A. To help understand the concepts, we illustrate the non-adversarial and adversarial error in a toy machine learning problem shown in Fig. 2. Providing a bound on such a performance metric can be very beneficial"}, {"title": "III. A THEORETICAL BOUND ON QML ADVERSARIAL ERROR FOR EVASION ATTACKS", "content": "In this section, we investigate the limits of adversarial QML model performance by defining a theoretical lower bound on adversarial risk, which converges to adversarial error when certain conditions are met. By developing an empirical method to estimate the bound, we bridge the gap between machine learning theorists and practitioners. We initially evaluate the adversarial error rate, i.e., the effectiveness of the attack, which depends on the model and the attack. By the section's end, we derive a computable lower bound on adversarial risk independent of specific models and attacks."}, {"title": "A. ADVERSARIAL ATTACK AND ERROR", "content": "In this section, we will provide a universal formalism of adversarial error that applies to both the classical perturbation and quantum perturbation attacks. We shall first consider classical perturbation. Suppose we have input samples and their corresponding labels, $(x_i, Y_i)$ drawn from the testing set $\\Omega_{\\text{test}}$, i.e., $(x_i, Y_i) \\in \\Omega_{\\text{test}}$. For the classical scenario, we have d-dimensional input, $x_i \\in X$, where $X = \\mathbb{R}^d$, and the given labels $y_i \\in \\mathcal{Y}$, where $\\mathcal{Y} = \\{1, .., C\\}$ with C being the number of classes. Now we can construct the classifier f as follows:\n\n$f: X \\rightarrow \\mathcal{Y}$,\n\na mapping between the input and output domain.\nFor the quantum perturbation attack scenario, we replace the input domain $X = \\mathbb{R}^d$ with $X = \\mathcal{H}$, where $\\mathcal{H}$ indicate a Hilbert space where the quantum states live. Thus, the classifier f must classify the quantum state and return a classical label y. In the following evaluations, both scenarios will be considered simultaneously unless otherwise specified.\nDefinition 1 (Adversarial Error Rate). A classifier f(x) is evaluated on a testing set $\\Omega_{\\text{test}} = \\{(x_i, Y_i)\\}$ where $x_i$ are the input and $y_i$ are the label. The perturbation attack A (as a generator of perturbation) on the classifier f constrained by attack strength $ \\epsilon \\in \\mathbb{R}$ generates the perturbed sample x', i.e., $x' = A(f,x, \\epsilon)$ and $||x' -x|| < \\epsilon$. Then, the adversarial error will be:\n\n$\\text{AdvErr}_{\\epsilon, A, f} = \\frac{\\#\\{(x_i, Y_i) \\in \\Omega_{\\text{test}} \\vert f(x'_i) \\neq Y_i\\}}{\\#\\Omega_{\\text{test}}}$,\n\nwhere # denotes the cardinality of a set.\nThe adversarial error rate defined above is one of the most commonly used evaluation metrics for assessing adversarial performance under test-time evasion attacks. Another similar metric for adversarial performance is adversarial risk. However, there is a subtle difference between adversarial error rate and adversarial risk, for which we will provide a bound. We will explain this difference and the conditions"}, {"title": "B. ADVERSARIAL RISK", "content": "Now, to further our theoretical evaluation, we consider another similar performance metric other than the adversarial error for models under test-time evasion attack, the adversarial risk [20], [53], for which we will provide a bound:\nDefinition 2 (Adversarial Risk). Given the ground truth, $f^*$, and the probability metric space $(\\mathcal{X}, \\mu, ||\\cdot||)$, the adversarial"}, {"title": "C. CHARACTERISING ADVERSARIAL ERROR REGION", "content": "Minimizing the adversarial risk from (5) is challenging. Fortunately, studies have demonstrated a different approach to evaluate (5) by a change of variable from the classifier f to the error region $\\mathcal{E}$ defined by any f [58]:\n\n$f \\rightarrow \\mathcal{E} = \\{x \\in \\mathcal{X} \\vert f(x) \\neq f^*(x)\\}$.\n\nWe also define the expansion of the error region $\\mathcal{E}$ as follows:\n\n$\\mathcal{E}_{\\epsilon} = \\{\\tilde{\\mathcal{E}} | \\exists x \\in \\mathcal{E}, ||\\tilde{x} - x|| < \\epsilon \\}$.\n\nAfter the change of variable, the adversarial risk is simply:\n\n$\\text{AdvRisk}_{\\epsilon, \\mathcal{E}} = \\mu(\\mathcal{E}_{\\epsilon})$,\n\nwhere $\\mathcal{E}$ denote an expansion of $\\mathcal{E}$ by $\\epsilon$. The intuition behind the expansion of the error region is that for any point x within the expanded region, there will be a neighboring point x' lying within the original region, which guarantees the existence of x' being an adversarial sample perturbed from x.\nBy transforming (5) to (11), we may evaluate the adversarial risk with only the shape and volume of the error region expansion $\\mathcal{E}$. However, by considering this formalism, we cannot directly recover a practical classifier f from the optimization procedure when we try to find the lower bound of the adversarial risk, as we will discuss later. Nonetheless, the bound remains useful as a reference for developing robust models.\nTo summarize, we have transformed the experimental adversarial error into adversarial risk by considering the optimal attack and assuming the robust ground truth. For the next step, we will minimize the adversarial risk over all possible error regions $\\mathcal{E}$ and, thus, obtain the minimum adversarial risk for all classifiers. Given $\\alpha$, the adversarial risk bound $C_{\\text{adv}}$ will be the global minima of the following constrained non-convex optimization problem:\nDefinition 4 (Minimizing Adv. Risk via Error Region). Given the data distribution on input x and a clean learning error rate $\\alpha$. The problem of minimizing the adversarial risk by minimizing the error region expansion is as follows:\n\n$\\min_{\\mathcal{E}} \\mu(\\mathcal{E}_{\\epsilon})$\n$\\mu(\\mathcal{E}) = \\alpha$.\n\nWe denote the minima of the problem as $C_{\\text{adv}}$.\nBy solving the minimization problem above regarding an input data distribution, we obtain the minimum adversarial risk $C_{\\text{adv}}$ given the input clean learning error rate $\\alpha$ and attack strength $\\epsilon$ for any of the possible classifiers f. This is because by minimizing over all possible error regions $\\mathcal{E}(f) \\in \\text{Pow}(\\mathcal{X})$, which are regions defined by f, we are also minimizing over all classifier f. Thus, the resulting adversarial risk bound, $C_{\\text{adv}}$, is a universal reference for a new model to look up to regarding adversarial performance. A robust model should have adversarial error close to the"}, {"title": "IV. EFFICIENT ALGORITHMS FOR COMPUTATION OF BOUNDS", "content": "In this section, we introduce the methods used to estimate the theoretical bounds outlined in (12) for the first time to the quantum domain and enhance them. We divide our contributions into quantum-specific adaptations enabling the estimation of the bound for quantum perturbation attack and the general improvement that applies to classical and quantum attacks.\nTo facilitate the discussion, we explain the intuition behind the algorithm we will be diving into in this section. Equation (12) highlights an optimization problem concerning both the adversarial and non-adversarial risk. We make an unbiased estimation of these risks by counting the number of samples in the estimated region $\\mathcal{E}$ and $\\tilde{\\mathcal{E}}$. We then parameterize and limit the choice of $\\tilde{\\mathcal{E}}$ to the union of T hyper-spheres,\n\n$\\tilde{\\mathcal{E}} = \\bigcup_{i}^{T} \\text{Sphere}(c_i, r_i)$."}, {"title": "A. COMPUTING ADVERSARIAL QUANTUM ML BOUND", "content": "This section introduces the novel method that evaluates the adversarial QML bound for quantum perturbation attacks for the first time. The algorithm illustrated above requires the computation of pairwise distances and the expansion of hyper-spheres within the Hilbert Space where quantum states live. These adaptations are crucial for evaluating the bounds in the quantum attack scenarios discussed in Section. II-C."}, {"title": "1) QUANTUM PERTURBATION ATTACK", "content": "A quantum perturbation on the quantum input sample $\\vert \\psi\\rangle$ generated by encoding the classical sample x may or may not lay within the quantum data manifold of the encoded quantum states, the subspace of the Hilbert space where the encoding scheme span with arbitrary real input x. Therefore, there are no classical input samples corresponding to these perturbed quantum states and no ground truth in the classical sense for these perturbed inputs outside the quantum data manifold. In this work, we consider attacks within the data manifold. Thus, the perturbed quantum inputs are always pure states."}, {"title": "a: Perturbation detection", "content": "Quantum perturbations introduced after the encoding stage directly modify the quantum state. Certification methods [60], [62] indicate that the number of necessary experimental runs to detect such perturbations scales inversely with the infidelity $ \\epsilon $ between the intended input pure state $|\\psi\\rangle$ and the adversarial input mixed state $\\sigma$:\n\n$\\epsilon = \\frac{1}{2}(1 - \\langle \\psi|\\sigma|\\psi\\rangle)$.\n\nIn this work, we only consider the adversarial inputs that are pure states consistent with the aforementioned in-manifold ground truth discussion."}, {"title": "b: Expansion of quantum hyper-sphere for bound calculation", "content": "The expansion of a region in the metric space defined by sample domain and distance metric $(\\mathcal{X}, ||\\cdot||)$ is given by (10). Here, we consider a special case of the error region where the error region is the union of spheres. In the classical ML case with the real vector space and $l_2$ distance, $(\\mathbb{R}^n, ||\\cdot||_{l_2})$, the expansion of the sphere centered on c, with radius r is simply another sphere centered on c with radius $r' = r + \\epsilon$. For the quantum perturbation attack, we consider the metric space $(\\mathcal{P}(\\mathcal{H}), ||\\cdot||_{TD})$ Where $\\mathcal{P}(\\mathcal{H})$ is the set of general pure state in the Hilbert space $\\mathcal{H}$ and $|| \\cdot ||_{TD}$ is the trace distance.\nFor the quantum case, the radius of the expanded hyper-sphere will be $r' = r\\sqrt{1 - \\epsilon^2} + \\epsilon\\sqrt{1 - r^2}$ instead of the simple addition rule $r' = r+\\epsilon$. The following theorem establishes the relationship between the radii of expanded hyper-spheres and itself in a Hilbert space with trace distance:\nTheorem 1. In the metric space $(\\mathcal{P}(\\mathcal{H}), ||\\cdot||_{TD})$ of pure states and trace distance,\n\n$\\text{Sphere}_{\\epsilon} = \\text{Sphere}_{c,r'}$,\n\nwhere $\\text{Sphere}_{\\epsilon}$ is the expansion of the set defined by the surface and interior of a sphere, i.e., $\\text{Sphere}_{c,r} = \\{x \\vert |x - c| < r\\}$. $\\text{Sphere}_{c,r'}$ is the sphere with the same centre c but a larger radius $r'$, where\n\n$r' = r\\sqrt{1 - \\epsilon^2} + \\epsilon\\sqrt{1 - r^2}$.\n\nProof. Here we provide a constructive proof where we show $\\text{Sphere}_{\\epsilon} \\subseteq \\text{Sphere}_{c,r'}$ and $\\text{Sphere}_{\\epsilon} \\supseteq \\text{Sphere}_{c,r'}$ separately. In the proof below, we use the Dirac Bra-ket"}, {"title": "B. PARALLELIZED BOUND CALCULATION", "content": "Apart from the adaptations required to calculate the bound for the quantum attack scenario, our algorithms achieve high efficiency and accuracy in the bound estimation procedure. In this section, we will first discuss how we improve the speed of computing the bound by using a parallelized subroutine that can be executed more efficiently on modern computing hardware such as CPU clusters or GPUs. Then, we will discuss how adding a regression procedure at the end of the bound evaluation process improves the accuracy of the bound.\nFrom a computational perspective, this approach leverages the power of parallel computing (unlike [20]). Our algorithm employs strategic precomputation of pair-wise distances and k-nearest neighbors, enabling the majority of processing to run on a GPU (Graphics Processing Unit). This innovative GPU-accelerated design drastically reduces computation time while maintaining high-quality results and ensures the method is scalable for large and complex datasets.\nWe also incorporate a linear regression procedure to analyze the relationship between adversarial and non-adversarial errors. This novel approach ensures that the algorithm's output accurately reflects the defined error region $\\mathcal{E}$, where $\\mu(\\mathcal{E}) = \\alpha$, as specified in (12)."}, {"title": "1) PARALLELIZED SUBROUTINE", "content": "Our algorithm detailed in Appendix B is highly parallelized and can be run on conventional GPU. It achieved a comparable complexity scaling to the first algorithm of adversarial risk lower bound estimation [20]. The algorithm we propose utilizes the matrices efficiently stored on conventional RAMS instead of the ball tree in the original algorithm to store the pair-wise distances $D_{ij} = ||x_i - x_j||$. The algorithm procedure is outlined below:"}, {"title": "2) BOUND ESTIMATION THROUGH REGRESSION", "content": "The algorithm is designed to minimize adversarial risk within a given dataset (training set). To ensure an unbiased estimate of the theoretical bounds, it is essential to partition the dataset into training and testing sets, mirroring traditional machine learning practices. We output the adversarial risk estimated on the testing set as the estimated bound. This division typically results in a difference in both non-adversarial and adversarial risks between the training and testing sets. This difference means that the output test non-adversarial risk will not equal the input $\\alpha$ of Algorithm. 1. To rectify this divergence, a regression procedure is employed to estimate the bound value accurately. Assuming that changes in $\\alpha$ linearly affect the bound, we can interpolate the testing set adversarial risk value at exactly input $\\alpha$ through a series of bounds results initialized with different training set non-adversarial risks. The details on and the implementation of the full bound estimation algorithms via regression are discussed in Appendix B. Here, we briefly explain the regression procedure and the hyper-parameters involved."}, {"title": "V. EXPERIMENTS AND RESULTS", "content": "In this section, we demonstrate the utility of the quantum adversarial lower bound by applying the algorithm to a realistic data set. We observe that the bound we have derived and estimated sits close to the experimental adversarial error rate for some instances of the model, whereas further for others. By simulating quantum computers using the pennylane [64] package, we train multiple models on the MNIST and FMNIST datasets and attack them with adversarial attacks for both classical and quantum perturbation attack scenarios. We also compute the estimated bound value for the given dataset, the non-adversarial error rate, and attack strength. Then, we compare the estimated bounds with the adversarial error rates."}, {"title": "A. EXPERIMENT SETUP", "content": "In this demonstration of the bound, we consider image classification tasks on the MNIST and FMNIST datasets. The structure of the quantum models follows the formalism of the Quantum Variational Circuit (QVC) [65], [66]. The model consists of three simple stages: encoding, quantum circuits, and a simple classic post-processing stage.\nThe encoding and quantum circuit stages, as shown in Fig. 3, should be implemented on a 10-qubit quantum computer. However, in our machine learning experiment, we simulate the quantum components in this model to avoid the higher noise level in current quantum hardware and probe the model's performance with a circuit depth much larger than what is allowed on current hardware. For the encoding stage, we adopted amplitude encoding mentioned in Sect.:IV-A2:"}, {"title": "a: Model training", "content": "For the quantum circuit stage, the ansatz of the quantum layer is Pennylane [64] StrongEntanglingLayers with CZ gate as the two-qubit gates. At the end, the circuit output 10 features by reading the measurement output and estimating the expectation value of observable $\\sigma_z$ for each qubit at the end, i.e., $\\langle \\sigma_z^{j}\\rangle$. Thus, the output of the quantum circuit stage is a 10-dimensional vector with each element $\\langle \\sigma_z^{j}\\rangle\\in [-1,1]$. Note that among the three stages, only the quantum circuit stage is trainable, with quantum rotational gate parameters as the trainable parameters."}, {"title": "b: Classical and Quantum Perturbation Attack", "content": "For the classical 12 attack, we apply the Projected Gradient Descent attack to the model. The gradient of the model is obtained directly by simulated backpropagation. In practice, quantum-specific methods such as parameter-shift [67], [68] will be required to propagate the gradient through the quantum circuit. For the quantum perturbation attack under the attack scenario we introduce, the quantum adversarial sample state needs to be generated by the same encoding scheme and evade detections based on quantum state verification successfully, as discussed in Sect. IV-Ala. Thus, the trace distance from the adversarial sample to the original non-adversarial state must be smaller than the distance threshold $ \\epsilon$. Following these rules, we have adapted the classical PGD method for the attack scenario considering the trace distance. We shall call the adapted attack TD-PGD attack. Similar to classical In PGD attack, for each iteration, the sample is first perturbed up to the step size according to the gradient direction of the loss function and projected back to the $ \\epsilon$ sphere in trace distance. For our case of amplitude encoding, the trace distance threshold is equivalent to a similarity threshold on the normalized samples. We will demonstrate the method through the case of 3-dimensional input as illustrated in Fig. 4.\nWhen trying to find the gradient of the model during the attack, we use a slightly different differentiable model from the training model with a softmax layer. We decreased the softmax layer temperature t to \u1f66 to better approximate the Argmax function used for evaluating model accuracy. We have found that using a lower temperature does increase the effectiveness of the attack in general. However, it imposes extra overhead on the gradient calculation using a quantum computer, similar to the training overhead discussed in the Model training section above."}, {"title": "c: Bound estimation", "content": "To estimate the bound, we follow the algorithm proposed in Sec. IV. The hyper-parameter we have selected for the bound estimation procedure is as follows: the number of iterations m is 10. The a range, [$\\alpha_l, \\alpha_u$], has $\\alpha_l = \\alpha$, $\\alpha_u = 1.1 \\alpha$, where a is the non-adversarial error rate given by the model in reference. The number of hyper-spheres T is 20. Our hyperparameter selection for the bound estimation algorithm is far from comprehensive. However, more details on the hyper-parameter selection are shown in Appendix C"}, {"title": "B. RESULTS", "content": "In this section, we compare the experimental adversarial error with the corresponding estimated bound value for 3 instances of QVC models on MNIST and 3 instances of QCV model on FMNIST data sets specified in Sec. V-A. We show that the models' adversarial performance after or during the training is consistently bounded by the estimated bound value."}, {"title": "VI. CONCLUSION", "content": "In this work, we establish a connection between the experimental adversarial error rate and the theoretical adversarial risk lower bound in the quantum machine learning problem, justifying the use of the latter as a benchmark when evaluating the resilience of non-optimal models against potent attacks. To assess the algorithm's effectiveness in evaluating adversarial performance, we attack some quantum models with quantum perturbation attack strategies, including an attack we devised for the quantum perturbation scenario. Then, we compare the bound with the adversarial error rates for the simulated quantum models. We observed the bounding of the error rates to different extents depending on the hyper-parameters chosen, indicating the utility of the"}, {"title": "APPENDIX A PROOF OF TRIANGLE INEQUALITY OF ANGLES BETWEEN COMPLEX UNIT VECTORS", "content": "The Bures angles between the two arbitrary complex unit vectors are defined as $\\theta_{\\mathcal{W}, \\mathcal{U}} = arccos \\vert \\mathcal{W}^\\dagger\\cdot\\mathcal{U}\\vert$. In the context of quantum states, equivalently, we have $\\theta_{\\mathcal{W}, \\mathcal{U}} = arccos \\langle \\mathcal{W}\\vert\\mathcal{U}\\rangle$. Using Bures angle, we have the following theorem:\nTheorem 2 (triangle inequality for Bures angles). Given 3 arbitrary complex unit vector $\\mathcal{V}_1$, $\\mathcal{V}_2$ and $\\mathcal{V}_3$ in any dimension d, their Bures angle between each other satisfy:\n\nTo prove the theorem above, consider the Gram matrix $\\mathcal{G}_{ij} = \\langle \\mathcal{V}_i\\vert\\mathcal{V}_j\\rangle$:"}, {"title": "APPENDIX B THE BOUND ESTIMATION ALGORITHMS", "content": "This section details our algorithms' pseudocode for finding robust error regions for either classical 12 distance or quantum trace distance. We also provide the runtime analysis and compare it with the runtime of a similar algorithm proposed for the classical attack scenario [20]."}, {"title": "A. PSEUDOCODE", "content": "The pseudocode is shown in Algorithm 1, Algorithm 2 and Algorithm 3. Algorithm 1 is the core subroutine in Algorithm 3. The Algorithm 2 shows the construction of the crucial function Condense in Algorithm 1 that enable the parallelised computation of the bound.\nAlgorithm 1 fundamentally follows a heuristic greedy optimization approach. It pre-computes and tracks the pair-wise distances between samples when more and more samples are taken into the error regions. After carving out multiple hyper-spheres from the input data space, we are left with input data that are not included in the hyper-spheres, i.e., $\\mathcal{S}\\setminus\\mathcal{S}_{\\epsilon}$ and $\\mathcal{S}\\setminus \\mathcal{S}_{\\epsilon}'$ shown in Algorithm 1. They represent the non-adversarial and adversarial error rate at this stage. To efficiently optimize the centers and radii for the next iteration utilizing the sorted pair-wide distance $D^{(s)}$, we condense $D^{(s)}$ to $D^{(s,c)}$, i.e., the distances between all N samples (potential centers of hyper-sphere) and the remaining points in $\\mathcal{S} \\setminus \\mathcal{S}_{\\epsilon}$ and $\\mathcal{S} \\setminus \\mathcal{S}_{\\epsilon}'$. In the Algorithm 1, we denote this step as the Condense function. We provide a pseudo-code that demonstrates the Condense function in Algorithm 2.\nThe FindRoot function in Algorithm 1 is a root-finding function utilizing the bisection method to find the element closest to zero in an ascending list. This method is deterministic with a complexity of O(log Nlist), where Nlist is the number of elements in the ascending list. The Expand function in Algorithm 1 is $r' = r + \\epsilon$ for classic In distances and $r' = r\\sqrt{1 - \\epsilon^2} + \\epsilon\\sqrt{1 - r^2}$ for the_quantum trace distance, as discussed in Sect. IV-A.\nAlgorithm 2 outlines the Condense function, which consists of two steps. The first step is to compute the unsorted to sorted location index from the index output I of a common sorting algorithm such as Quicksort. The matrix I' shows where each element in the corresponding location of D is moved to D(s). The second step is to remove all the elements in D(s) that correspond to distances from any sample in S (indexed by i) to samples in $\\mathcal{S}_{\\epsilon}$ (indexed by k). Using the location index I', we can efficiently locate them and remove them from D($). If necessary, each of the two steps above can be programmed to run on GPU with (i, j) or (i, k) parallel separately."}, {"title": "B. RUNTIME ANALYSIS", "content": "In this section, we analyze the complexity of the algorithm and focus on the scaling behaviour of the runtime regarding"}, {"title": "APPENDIX C HYPER-PARAMETER SELECTION", "content": "During the calculation of the estimated bound, we used different total sphere numbers T. As mentioned in Sec. III, The bound estimation problem can be formulated as an optimization problem where we minimize the sample number in the e expansion of the error region parameterized by centers and radii. Thus, the total number of spheres we used to approximate the error region is a hyper-parameter that affects the heuristic solution given by our bound-finding algorithms. In general, for datasets with a more complex data structure, a higher T is required to find the best solution for the algorithm. However, when T is set too high, the generalization of the error region will degrade. For higher T, each sphere that constructs the error region contains a smaller"}]}