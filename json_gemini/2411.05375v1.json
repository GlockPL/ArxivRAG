{"title": "Ev\u00b2R: Evaluating Evidence Retrieval in Automated Fact-Checking", "authors": ["Mubashara Akhtar", "Michael Schlichtkrull", "Andreas Vlachos"], "abstract": "Current automated fact-checking (AFC) approaches commonly evaluate evidence either implicitly via the predicted verdicts or by comparing retrieved evidence with a pre-defined closed knowledge source, such as Wikipedia. However, these methods suffer from limitations, resulting from their reliance on evaluation metrics developed for different purposes and constraints imposed by closed knowledge sources. Recent advances in natural language generation (NLG) evaluation offer new possibilities for evidence assessment. In this work, we introduce Ev2R, an evaluation framework for AFC that comprises three types of approaches for evidence evaluation: reference-based, proxy-reference, and reference-less. We evaluate their effectiveness through agreement with human ratings and adversarial tests, and demonstrate that prompt-based scorers, particularly those leveraging LLMs and reference evidence, outperform traditional evaluation approaches.", "sections": [{"title": "Introduction", "content": "To decide the truthfulness of a claim, professional fact checkers search, retrieve and analyse evidence (Graves, 2018). Their goal is not just to determine if the claim is correct, but also to present the evidence and fact-checking steps transparent, helping the reader understand and trust the fact-checking process (Graves, 2017).\nPast research has utilized three different methods to evaluate evidence retrieved for Automated Fact-Checking (AFC). First, implicitly assessing the evidence via the predicted verdict, assuming that only accurate evidence allows AFC systems to predict the correct verdict (Shahi and Nandini,"}, {"title": "Related Work", "content": "While earlier AFC tasks and datasets evaluated evidence implicitly through the predicted veracity label (Guo et al., 2022; Akhtar et al., 2023), more recent approaches require explicit evaluation of evidence to ensure that the output of systems is grounded in evidence.\nThere are several challenges in the automated retrieval and evaluation of evidence for evidence-based AFC. First, evidence-based AFC datasets released in recent years contain purpose-built claims derived from Wikipedia, such as FEVER (Thorne et al., 2018a), Hover (Jiang et al.,"}, {"title": "Evidence-based AFC", "content": "Reference-based evaluation Reference-based NLG metrics evaluate the generated text 2 by comparing it to a gold reference x:\ny = f(x,x),\nwhere the scoring function f(.) can be conceptualized in various ways, for example, as a non-parametric function based on n-grams or a similarity function based on conceptualized representations of text. Earlier NLG metrics used n-grams to compare generated and reference text based on different heuristics such as string overlap, string distance or lexical diversity (Celikyilmaz et al., 2020; Lin, 2004; Papineni et al., 2002). In recent years, various trained and LLM-based metrics were proposed, which also serve as inspiration for Ev2R. BLEURT is a learned, BERT-based (Devlin et al., 2019) evaluation metric trained on millions of synthetically generated sentence pairs (Sai et al., 2023). NUBIA (Kane et al., 2020) applies a three-step approach for evaluating a generated text against its reference: extracting representations for reference and generated text, calculating a similarity score, and capping the score between 0 and 1. Zhang et al. (2020) propose BERTScore which calculates the pairwise token similarity for token pairs of generated and reference text before maximising the overall similarity score with greedy matching. MoverScore (Zhao et al., 2019) also uses contextualized representations but allows one-to-many matching between n-grams or words for partial matching."}, {"title": "Evaluation Approaches for Natural Language Generation", "content": "Proxy-reference evaluation This family of metrics uses a so-called \"proxy\" for evaluation and does not directly compare the generated text (2) to the annotated reference text (x). In previous research they have been applied if the gold references were not available for evaluation or to focus on specific evaluation criteria (e.g., factuality) for which appropriate proxies are defined as we highlight below (Nema and Khapra, 2018; Durmus et al., 2020; Huang and Zhang, 2021). Multiple proxy-based metrics for NLG evaluation were proposed in the past (Eyal et al., 2019; Scialom et al., 2019). For example, Durmus et al. (2020) use questions and answers as proxy reference to evaluate if relevant information from the input text is available in the generated text. Here, generating correct answers serves as a proxy for NLG evaluation to determine if the generated text is consistent (i.e. faithful) with the input document. Other works have used relational tuples extracted from both generated and reference text as proxies for evaluation (Goodrich et al., 2019). Here, the generated text is assessed based on whether it contains all factual tuples from the reference text (Ji et al., 2023). Based on the idea that generated text (e.g., a summary) should entail its source document, entailment labels have been used as proxy references for evaluating hallucinations in NLG (Ji et al., 2023).\nReference-less evaluation This family of metrics evaluates generated text without the availability of any (proxy) reference. More recently, the capabilities of autoregressive LLMs (e.g., GPT4 (OpenAI, 2023)) have been used for reference-less evaluation. Fu et al. (2023) define evaluation as a text generation problem. Their proposed GPTScore framework is based on the idea that LLMs are more likely, i.e. with higher conditional generation probability, to produce the generated text (e.g., a summary) if it is of high quality and fulfills given specifications based on the input document. FactScore (Min et al., 2023) is a recent LLM-based framework for reference-less evaluation of factuality in LLM-generated text. To evaluate the factuality of generated text, it breaks down the text into atomic facts and verifies if they are supported by the knowledge sources. The overall score corresponds to the ratio of supported facts."}, {"title": "Ev\u00b2R: A Framework for Evidence Evaluation", "content": "This section introduces Ev2R, a framework of scorers we propose to assess evidence in the context of AFC. Based on their evaluation strategy, we categorise the Ev\u00b2R scorers into three groups similar to the categories introduced in Section 2.2: (i) reference-based, (ii) proxy-reference, and (iii) reference-less (see overview in Figure 1).2"}, {"title": "Reference-based", "content": "Ev2R reference-based scorers evaluate the quality of the retrieved evidence (\u00ca) by comparing it to reference evidence (E) annotated in the given dataset (see bottom left boxes in Figure 1). This approach is therefore limited to evaluating AFC systems only using datasets with annotated reference evidence.\nReference-based fine-tuned scorer To evaluate the retrieved evidence against the annotated evidence as a reference, the fine-tuned reference-based scorer compares both evidence sets and outputs a similarity score s as described below. The overall score for a dataset is computed as the average across all evaluated instances. Given as input a set of retrieved evidence \u00ca = {\u00ea1, \u00ea2, ...\u00ean} and a reference evidence set E = {e1,e2, ...en}, the reference-based scorer calculates a similarity score:\ns = Scorer(E, \u00ca)\nAssuming that s \u2208 [0,1], a score of one indicates that both evidence sets are identical, while zero means a high degree of divergence. We train the scorer using BLEURT (Sellam et al., 2020) as a starting point. BLEURT was originally developed for assessing NLG tasks (e.g., machine translation or summarization). It uses BERT as its backbone, which is trained on millions of synthetically generated sentence pairs and multiple objectives. We fine-tune the BLEURT model for evidence assessment using synthetically generated data instances containing a claim c, reference evidence E, and retrieved evidence \u00ca.\nWe generate samples using large-scale AFC benchmarks with multiple evidence sets for a single claim, e.g., FEVER (Thorne et al., 2018a) and VitaminC (Schuster et al., 2021). These fact-checking datasets include multiple evidence sets {E1, E2,..., En} for each claim. For positive training instances with similarity score s = 1, we use two different evidence sets of one claim Ci (i.e., Ea and Eb) as reference and retrieved evidence with Ea \u2260 Eb. For negative samples with"}, {"title": "Proxy-reference", "content": "Ev2R proxy-reference scorers use the veracity label y as a proxy to evaluate the quality of the retrieved evidence \u00ca (see bottom right box in Figure 1). This approach is particularly relevant to rate evidence when no reference evidence is available but the veracity labels of the claims. For example, if a claims is labelled as false, we can use proxy scorers to evaluate if the retrieved evidence supports the claim or not.\nProxy fine-tuned scorer The proxy trained scorer uses the DeBERTa language model (He et al., 2021) as backbone to predict the label y serving as a proxy for evidence evaluation. We use a DeBERTa-large model that was previously trained on multiple benchmarks for the related task of natural language inference, specifically MNLI (Williams et al., 2018), Fever-NLI (Nie et al., 2019), Adversarial-NLI (Nie et al., 2020), LingNLI (Parrish et al., 2021) and WANLI (Liu et al., 2022). To fine-tune this model for the task of claim classification we use the datasets FEVER (Thorne et al., 2018a), VitaminC (Schuster et al., 2021), Hover (Jiang et al., 2020), and AVeriTeC (Schlichtkrull et al., 2023). Given a claim c, its veracity label y, and a retrieved evidence set E as input, the scorer predicts a score s representing its confidence in the gold veracity label:\ns = Scorer(c, y, \u00ca)\nThis score s is derived by transforming the model-predicted output logits into probabilities using softmax. The resulting probability distribution allows the scorer to quantify its certainty in the different labels potentially available (e.g., support, refute, and not enough information). The scorer's confidence in the gold label (e.g., y = support) is used as the output scores \u2208 [0,1]. Hence, evidence contributing to high confidence in the gold label is rated higher accordingly.\nLLM-based proxy scorer The LLM-based proxy scorer follows a similar approach. Instead of measuring the scorer's confidence in the gold label y based on the predicted probability distribution as described before, the scorer's LLM is prompted with an input that consists of the instruction to predict a veracity label for the given retrieved evidence \u00ca and claim c. The predicted score s corresponds to the following conditional"}, {"title": "Reference-less", "content": "The Ev2R reference-less scorers evaluate the retrieved evidence based only on the input claim (see top row in Figure 1). The scorer decomposes the claim c into atomic facts Ac and evaluates whether each fact ac \u2208 Ac is addressed in the retrieved evidence \u00ca and either supported or refuted information is provided. This results in following score:\ns = 1/ |Ac| \u03a3\u2161[ac supported/refuted by \u00ca]\naceAc\nIterating over each atomic fact ac \u2208 Ac, the indicator function I returns 1 if the atomic fact ac is either supported or refuted by the retrieved evidence \u00ca and 0 otherwise. This approach, similar to the reference-based scorer, is inspired by FactScore (Min et al., 2023). However, unlike the reference-based evaluation, which extracts evidence for both gold and retrieved evidence, this method decomposes the claim to evaluate the retrieved evidence without relying on any additional reference. The key advantage of this method is that it does not require any annotated data. Moreover, we compare our proposed evaluation framework to five reference-based baseline metrics that have been used in previous research for evidence evaluation: RougeL, BLEU, BLEURT, METEOR, and Hungarian Meteor."}, {"title": "Evaluation of Ev\u00b2R", "content": "This section provides an overview of the systematic evaluation approaches we conducted as part of the meta-evaluation of Ev2R scorers. Meta-evaluation consists of evaluation with the help of human evaluators (see Section 4.1) and automated evaluation using so-called checklist tests (see Section 4.2)."}, {"title": "Evaluation with Human Ratings", "content": "We conduct a meta evaluation with human raters to assess how well the scorers' predictions correlate with human judgements. We evaluate this based on five dimensions (detailed below) inspired by previous work on NLG metrics.3\nWe asked annotators to rate retrieved evidence based on the following criteria while considering the provided reference evidence:\n(1) Coverage assesses how much of the reference evidence is covered by the retrieved evidence, whether the content, meaning, entities, etc., of the reference evidence are fully represented in the retrieved evidence.\n(2) Coherence captures whether the retrieved evidence is coherent, i.e., all sentences are connected sensibly and the evidence makes sense as a whole.\n(3) Repetition evaluates whether the retrieved evidence exhibits repetition of its content.\n(4) Consistency assesses if the retrieved evidence is logically consistent in itself.\n(5) Relevance measures how relevant the retrieved evidence is for the claim.\n(6) Verdict Agreement measures if the retrieved evidence results in the same verdict label as the reference evidence.\nTo assess the scorers, we considered two different data sources. First, we collected ratings for 100 randomly selected test data instances from the AVeriTeC test set (Schlichtkrull et al., 2023), along with the corresponding retrieved evidence from the AVeriTeC baseline system. Each sample was evaluated by two annotators, resulting in 200 annotations. The overall rating for each annotated sample and dimension was calculated as the average of both annotations. If the annotators disagreed on the verdict assigned to the retrieved evidence, a third annotator was assigned to the data instance, and majority voting was used to determine the final verdict. All human annotators were familiar with AFC research and were either computer science graduate students or postdocs.\nThe second source for evaluation was the AVeriTeC shared task submissions (Schlichtkrull et al., 2024). In collaboration with the organisers we obtained a random sample of evidence across all submitted systems, representing both high- and low-scoring systems. The participants were then asked to evaluate these samples. Thirteen partic-"}, {"title": "Checklist Evaluation", "content": "Additionally, we evaluate the Ev\u00b2R scorers by assessing their sensitivity to automated perturbations in evidence data. Inspired by Ribeiro et al. (2020), we generated large-scale sets of so-called checklist tests by perturbing the claim and/or evidence. We included both semantics-preserving and semantics-modifying tests. For semantics-preserving tests, we change the claim and/or evidence such that the resulting evaluation score should not be affected, such as by introducing small typos. These tests assess the scorers' robustness to semantically equivalent changes in the evidence data. On the other hand, we also create semantics-modifying tests by changing the evidence such that its meaning alters and subsequently scores should drop, for example, by removing a significant part of he evidence. To create tests with automated perturbations, we use the AVeriTeC test set as the basis.\nOur tests focus on the seven different dimensions (see Table 4 in the appendix), the first two categories alter the semantics of the initial evidence, while the remaining ones preserve the evidence's meaning."}, {"title": "Backbone models", "content": "In Section 3, we introduce a framework for evidence evaluation. As highlighted earlier, various language models can be integrated as backbone models within this framework. In this work, we assess the framework using five different LLMs.\nFor the prompt-based scorers, we integrate and assess GPT40, Gemini 1.5 Pro, Gemini 1.5 Flash, and Llama 3.1 70B. GPT40 is a state-of-the-art model that demonstrates high performance across multiple benchmark datasets and domains (OpenAI, 2024). We also evaluate Gemini 1.5 Pro, which has shown strong performance in reasoning and long-context understanding tasks (Reid et al., 2024). To understand the framework's scalability across different model sizes, we also evaluate Gemini 1.5 Flash, which is smaller than"}, {"title": "Results and Discussion", "content": "In this section, we discuss the results obtained by evaluating the evidence scoring framework using three different datasets. First, we evaluate all Ev2R scorers based on the AVeriTeC subset, which was scored by human annotators (see Table 1). Second, we use human-evaluated system predictions from the AVeriTeC shared task (see Table 2). Finally, we compare all Ev2R scorers and baselines against each other based on the automatically generated adversarial tests (see Table 3). Following Fu et al. (2023), we measure the correlation between the Ev\u00b2R scorers and human ratings using Spearman (\u03c1) (Spearman, 1987) and Pearson correlation coefficients (r) (Pearson, 1896).\nEv\u00b2R scorers versus traditional metrics Our evaluation suggests that prompt-based Ev2R scorers provide a more accurate reflection of human ratings compared to traditional metrics, indicating their potential for evidence evaluation. The correlation coefficients for prompt-based scorers, especially the reference-based scorers, show higher correlation with human ratings than traditional metrics (e.g., ROUGE) across all categories. For instance, the reference-based recall scorer based on GPT40 achieves an average correlation score of 0.306 (r), outperforming all traditional metrics to a large extent. On the other hand, the trained Ev2R scorers show a comparable performance to baseline metrics while the proxy-reference scorer clearly outperforms baselines for categories such as verdict agreement and consistency.\nComparison: Reference-based, reference-less, and proxy-reference Among all prompt scorers (reference-based, reference-less, and proxy-reference), the reference-based Ev\u00b2R scorers con-"}, {"title": "Qualitative analysis of scorers", "content": "Through manual assessment of evidence evaluation by different scorers, we observe various strengths and limitations across reference-less, reference-based, and proxy-reference approaches.\nReference-less The reference-less Ev\u00b2R scorers evaluate supporting or refuting evidence accurately, but struggle with cases where there is not enough evidence as they lack any reference (i.e., evidence or verdict label) to compare the predicted evidence against. For example, for the claim \"France24 encouraging Niger Delta militants to fight for their right to equity in resource sharing.\" (id 20) in Table 8 in the appendix, the predicted evidence does not provide any supporting or refuting information. While it discusses groups in the Niger Delta, no information is given on whether France24 encourages them or not.\nMoreover, for some claims, unclear formulations pose challenges for reference-less scorers when extracting facts from the claims to evaluate against the predicted evidence. For instance, in claim \u201cAntifa vandalized the home of Missouri senator Josh Hawley in 4th of January 2021\" (593), it is unclear if 'Antifa' refers to a specific organisation, a left-wing coalition of organisations, or a"}, {"title": "Conclusion", "content": "This paper introduced an evidence evaluation framework for AFC, proposing three types of evidence scorers: reference-less, reference-based, and proxy-reference evaluation. We evaluate the framework using the AVeriTeC dataset, including evidence retrieved by the AVeriTeC baseline system as well as submitted systems from the AVeriTeC shared task. We assess the proposed Ev2R scorers based on human ratings across multiple dimensions (e.g., coverage and relevance) and by testing their sensitivity to automated perturbations in the evidence data. Our results show that the reference-based atomic Ev2R scorers correlate best with human ratings and are robust to automated perturbations. However, they struggle with evidence that uses different information or reasoning chains than the reference evidence-a scenario where LLM-based reference-less and proxy-reference scorers perform better. Overall, our proposed Ev\u00b2R scorers excel compared to traditional evaluation metrics like METEOR and ROUGE."}]}