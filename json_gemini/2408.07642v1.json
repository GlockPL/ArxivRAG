{"title": "Boosting Unconstrained Face Recognition with Targeted Style Adversary", "authors": ["Mohammad Saeed Ebrahimi Saadabadi", "Sahar Rahimi Malakshan", "Seyed Rasoul Hosseini", "Nasser M. Nasrabadi"], "abstract": "While deep face recognition models have demonstrated remarkable performance, they often struggle on the inputs from domains beyond their training data. Recent attempts aim to expand the training set by relying on computationally expensive and inherently challenging image-space augmentation of image generation modules. In an orthogonal direction, we present a simple yet effective method to expand the training data by interpolating between instance-level feature statistics across labeled and unlabeled sets. Our method, dubbed Targeted Style Adversary (TSA), is motivated by two observations: (i) the input domain is reflected in feature statistics, and (ii) face recognition model performance is influenced by style information. Shifting towards an unlabeled style implicitly synthesizes challenging training instances. We devise a recognizability metric to constraint our framework to preserve the inherent identity-related information of labeled instances. The efficacy of our method is demonstrated through evaluations on unconstrained benchmarks, outperforming or being on par with its competitors while offering nearly a 70% improvement in training speed and 40% less memory consumption.", "sections": [{"title": "1. Introduction", "content": "Using large-scale training benchmarks, well-constructed Deep Neural Networks (DNN), and angular-penalty criterion, the state-of-the-art (SOTA) Face Recognition (FR) approaches have performed exceedingly well [60, 10, 58]. However, the release of new evaluation benchmarks reveals the performance bias in favor of independent and identical distributed (i.i.d.) data, i.e., train and test instances ought to be drawn from the same distribution [36, 32, 49]. Specifically, available FR training sets represent Semi-Constrained (SC) distribution and a considerable performance drop is observed on Un-Constrained (UC) benchmarks [45], e.g., performance on IJB-S [28] and TinyFace [?] (SC benchmarks), are about 30% lower than that on LFW [22]. A straightforward remedy is constructing a large-scale training dataset with sufficient UC and SC samples. However, obtaining a scaleable dataset containing a balanced number of SC and UC samples is infeasible [54].\nAs a viable alternative, the mainstream works map UC instances to their SC counterparts [4, 68, 38, 41, 27, 15, 69, 59, 52]. These methods can be categorized into: 1) Facial Image Enhancement (FIE) [4, 68, 38, 41, 27], and 2) Pair-wise Common Space Mapping (PCSM) [15, 69, 59, 52]. Although FIE studies improve the visual quality of UC inputs, FIE is inherently ill-posed since multiple SC faces exist for a single UC image [62, 66, 23]. Furthermore, FIE studies are not tailored to FR, which limits their applicability in boosting FR performance [36]. PCSM methods try to improve performance by explicitly forcing UC instances to be mapped near the SC samples [15]. These methods require negative and positive pairs of (SC, UC) samples to be available during training, limiting their utility due to the combinatorial explosion in the number of possible pairs in large-scale setups [1, 59, 5].\nRecent studies have focused on the integration of image augmentation and conventional angular-margin [32, 10, 50, 36]. Despite promising [32], classical augmentations either do not reach the full potential of UC data or are computationally expensive [45]. Very recently, [50, 36] employed unlabeled datasets to generate UC stylized versions of SC samples; here, we dubbed them Image Space Targeted Augmentation (ISTA). Although innovative, ISTA approaches are fundamentally limited as Generative Adversarial Networks (GANs) are cumbersome to use [63, 35]. Furthermore, training a generative model is not a trivial task and requires relying on strong prior knowledge [2, 63].\nIn this work, we argue that the effect of the input distribution (UC/SC) in the style feature is an essential factor that the model overfits; see Figure 2. To combat this, it is not trivial to: 1) construct more diverse training without employing generative models so it can be integrated into large-scale training and 2) assemble a flexible augmentations module to adjust w.r.t the optimization trajectory and produce plausible styles. In this regard, we utilize Domain Generalization (DG) [72] and Adversarial Training (AT) [57], which have shown exceptional progress in out-of-domain (OOD) generalization [72, 57]. However, these techniques are mainly designed for a close-set setup where the label space is consistent across the datasets, while FR is an open-set problem, which results in a multitude of possibilities for how to boost FR with these ideas.\nWe introduce a novel method dubbed Targeted Style Augmentation (TSA), which operates within the model's hidden space. Notably, TSA distinguishes itself from contemporary DG/AT methods due to: 1) the open-set nature of FR, where target classes remain disjoint for each dataset, and 2) the introduction of a novel adversarial objective tailored to FR, which augments training diversity while upholding the plausibility of generated style. We present an innovative style augmentation strategy meticulously designed to produce diverse yet plausible styles that deviate from the original SC distribution. This augmentation process is carried out within the model's hidden space, effectively sidestepping the computational overhead and complexities associated with image space manipulation. Moreover, our approach incorporates an adaptive capability for discerning unrecognizable instances, ensuring that the augmentation process aligns seamlessly with the training trajectory. Our main contributions are three-fold:\n\u2022 We present a method to manipulate the style information of the features obtained from SC samples while ensuring the validity of generated styles.\n\u2022 We propose an entropy-based approach to discern the unrecognizable instances during the training.\n\u2022 We present a targeted augmentation technique with much less computational overhead compared to its GAN-based predecessors, which is tailored for large-scale FR training."}, {"title": "2. Related Works", "content": "Most recent works map UC instances to their SC counterparts [4, 68, 38, 41, 27, 15, 69, 59, 52], and can be categorized into: 1) Facial Image Enhancement (FIE) [4, 68, 38, 41, 27], and 2) Common Space Mapping (CSM) [15, 69, 59, 52]. FIE methods have focused on solving the inverse problem of retrieving high-quality (SC) samples from their low-quality (UC) counterparts [62, 66, 23]. Despite successful w.r.t human perception, they fail to increase the FR performance on real UC testing [31, 65, 36]. A different avenue of investigation, CSM, tries to improve the UC samples' representation discriminability by finding joint embedding for (UC, SC) pairs. However, large-scale web-crawled datasets lack the mentioned pair-wise tuplet. Also, such sample-level supervision is unstable and data-inefficient [60]. Therefore, CSM application is limited to small-scale controlled datasets [1, ?, 59].\nRecently, Image Augmentation (IA) [32, 50, 36] methods have emerged as an alternative branch of inquiry. Kim et al. [32] use random down-sampling/cropping, which results in a considerable performance improvement on UC benchmarks such as TinyFace. However, the employed augmentation is an oversimplified version of in-the-wild image distortions [45]. On the other hand, more sophisticated classical augmentation is computationally expensive to be integrated into the FR framework. In [50], the author proposes to employ GAN as an augmentation module. Followup work [36], integrate GAN with the quality module to control the level/direction of applied degradation. The major limitation of GAN-based augmentation is that they are cumbersome to deploy during training [63]. Additionally, developing a proper generative model heavily relies on prior knowledge, and preserving the identity information during GAN-based augmentation is controversial [59]."}, {"title": "2.2. Domain Generalization", "content": "The remarkable achievements of deep learning hinge on the premise that the training and test data are derived from an identical distribution [71]. When this assumption is violated, considerable performance degradation occurs [45]. DG methods address the cross-domain challenge of learning a generalizable model when multiple source datasets are accessible [72]. Conventional DG methods employ auxiliary losses, such as Maximum Mean Discrepancy or domain adversarial framework [72]. Recently, [46, 73, 21] employ image generation modules to increase the diversity of the training set by mapping the images from one domain to another arbitrary domain while maintaining the content of the original input. A new line of study considers the DG as the problem of feature distribution matching [72, 30, 70]. Specifically, they augment the cross-distribution feature by utilizing first and second-order statistics. Style perturbation is also utilized in our method as well, but it is being applied in an open-set setup. Furthermore, DG approaches do not deal with degraded imagery to the point that they are not recognizable. We specifically manipulate the augmentation objective to ensure the plausibility of the generated style."}, {"title": "2.3. Adversarial Training in DG and FR", "content": "Adversarial attacks have brought to light the susceptibility of deep learning models to imperceptible perturbations, as demonstrated by Jia et al. [26]. Subsequently, the pioneering work of Goodfellow et al. [16] proposed Adversarial Training (AT) as a pivotal strategy to mitigate the impact of these perturbations. Recently, the Domain Generalization (DG) incorporated with gradient-based AT has revolutionized DG, as showcased by Volpi et al. [57] and Qiao et al. [44]. Within this context, several studies, including Qiao et al. [44, 43] and Fan et al. [14], have incorporated meta-learning and adaptive normalization into AT to cultivate domain-invariant representations.\nBroadly, AT in FR strives to enhance the model's resilience against certain adversarial components in the inputs. Notably, Sharif et al. [48] devised real-world adversarial attacks involving printed glasses, while Komkov et al. [33] explored physical attacks through adversarial hats. Deb et al. [9] initially leveraged Generative Adversarial Networks (GANs) for synthesizing adversarial samples. Subsequently, Yin et al. [67] introduced an adversarial makeup generation framework, while Dong et al. [13, 12] harnessed generative models for creating adversarial attributes. Liu et al. [36] harnessed adversarial manipulation of the GAN hidden state to produce challenging training instances for deep FR models. However, it's worth noting that the computational cost associated with generating adversarial examples using GANs renders these GAN-based approaches impractical for large-scale problems, as observed by Yang et al. [63]. Additionally, the process of training a generative model capable of preserving identity information while transferring from seen to unseen images necessitates a substantial amount of prior knowledge [2]."}, {"title": "3. Problem Formulation", "content": "Notation. Let $D = \\{(x_i, y_i)\\}_{i=1}^{D}$ and $D = \\{x_i\\}_{i=1}^{D}$ represent the labeled and unlabeled datasets, respectively, where |.| reflects the cardinality of the set. Here, we decompose backbone $E_\\theta(.)$ into two sub-networks $E = E_2 \\circ E_1$ with trainable parameter $\\theta = [\\theta_1,\\theta_2]$. $E_1$ maps the input image into intermediate feature map $h = E_1(x) \\in \\mathbb{R}^{c \\times h \\times w}$ where $c, h,$ and $w$ indicate the channel, height, and width of $h$, while $E_2$ maps the feature maps $h$ to $d$-dimensional embedding vector $z = E_2(h) \\in \\mathbb{R}^{d}$. Suppose trainable parameter (prototypes) $W = [W_1,W_2, ..., W_c]$, refers to the classifier head, that maps $z$ to probability distribution over $c$ classes. For the convenience of presentation, we omit the bias from the classifier head, and all the representations and prototypes are $l_2$-normalized."}, {"title": "3.1. Preliminary", "content": "Style. The pioneering work of [56] discovers that instance-specific channel-wise mean and standard deviation of intermediate feature map from a CNN convey the style information of an input image, dubbed as $\\mu = [\\mu_1, \\mu_2,..., \\mu_c]$ and $\\sigma = [\\sigma_1, \\sigma_2,..., \\sigma_c]$, respectively. Accordingly, given the $h$, the style information can be obtained by:\n$\\mu_k = \\frac{1}{hw}\\sum_{i=1}^{h}\\sum_{j=1}^{w} h_{k,i,j},$  (1)\n$\\sigma_k = \\sqrt{\\frac{1}{hw}\\sum_{i=1}^{h}\\sum_{j=1}^{w} (h_{k,i,j} - \\mu_k)^2},$ (2)\nwhere subscript $k$ refers to the channel along which the computation is applied. With the application on neural style transfer, AdaIN [24] replaces the style of the original feature map $h$ with target style $(\\hat{\\mu}, \\hat{\\sigma})$:\n$h' = \\frac{\\hat{\\sigma}}{\\sigma}(h - \\mu) + \\hat{\\mu}$. (3)\nOur method draws inspiration from the AdaIN application in generative models. However, rather than generating an image with a novel style, we leverage adversarial perturbation in the model's hidden style space to implicitly increase the hardness of the training samples and force SC instances to illustrate the characteristics of UC images."}, {"title": "3.2. Recognizability", "content": "Conventionally, $L$ in Equation 4 is the task cost function, and adversarial manipulation is performed in the image space by directly changing the pixels' intensity [16, 51, 63, 35, 2]. Instead of altering the pixels' intensity, we manipulate the intermediate features obtained from SC inputs toward representing UC characteristics. To this end, we interpolate between the style of unlabeled (UC) and labeled (SC) samples. However, unlabeled face datasets naturally comprise a large amount of detectable but unrecognizable faces [11]. Thus, adversarial interpolation using gradient ascent on the training objective, may result in unrecognizable styles, which has detrimental effects on model training. We propose a metric to constrain the recognizability of the features resulting from the synthesized style.\nRecent studies [45, 11, 3] reveal that unrecognizable instances form a cluster well-separated from other identities, dubbed UR cluster. Inspired by this finding, we measure the recognizability as the function of distance to the UR cluster:\n$L_r = \\frac{1}{\\epsilon + 1 - <\\Phi, E_2(h)>}$ (6)\nwhere <...,...> represents the inner product, and $\\Phi$ denotes the centroid of UR cluster. The first step for computing $L_r$ is to compute the $\\Phi$, which requires to distinguish UR instances. [11, 3] detect UR samples and precompute $\\Phi$ in an offline manner before the training. We argue that since the encoder $E$ evolves progressively, synchronizing $\\Phi$ with the training trajectory is essential. Thus, we aim to distinguish unrecognizable samples through the information entropy of their representation as the training progresses [55, 42, 8, 47].\nSpecifically, we aim to measure the recognizability of a feature vector $z_i$ by its information entropy $H(z_i)$. Yet, feature embedding of a deep network follows a complex and unknown distribution, and it is infeasible to explicitly compute the $H(z_i)$ [7, 18]. As a viable alternative, it has been proven that under the Maximum Entropy Principle, the entropy of an arbitrary distribution is upper bounded by a Gaussian with the same mean and variance [53, 34, 25]. In particular, if z is sampled from a Gaussian with mean m and variance $v^2$, $z \\sim N(m, v^2)$, the upper bound to differential entropy of z can be defined as:\n$H(z) = -\\int p(z) \\log p(z) dx$ \n$= -E[N(m, v^2)]$ \n$= -E[\\log ((2\\pi v^2) ^{-1/2} e^{-\\frac{((z-m))^2}{2v^2}})]$  (7) \n$= \\frac{1}{2} E[\\log (2\\pi v^2)] + \\frac{1}{2v^2}E[(z - m)^2]$  \n$= \\frac{1}{2} \\log 2\\pi v^2 + \\frac{1}{2}$,\nwhich is solely a function of $v$. Therefore, we estimate the upper bound of the entropy (information) of representation $z_i$ using its variance."}, {"title": "3.3. Targeted Style Adversary", "content": "Here, we present the proposed Targeted Style Adversarial training pipeline. Specifically, given the style information from labeled $(\\mu, \\sigma)$ and unlabeled $(\\hat{\\mu}, \\hat{\\sigma})$, novel style can be computed by their convex combination:\n$\\sigma' = \\lambda_1 \\sigma + (1 - \\lambda_1)\\hat{\\sigma},$ (9)\n$\\mu' = \\lambda_2 \\mu + (1 - \\lambda_2)\\hat{\\mu},$ (10)\nwhere $\\lambda_1$ and $\\lambda_2$ define the amount each labeled style should move toward that of unlabeled. Then, optimal $\\lambda_1$ and $\\lambda_2$ are obtain using PGD with $L$ defined as:\n$L = L_{fr} - \\beta L_r,$ (11)\nwhere $L_{fr}$ is an arbitrary FR objective function, e.g., ArcFace [10]. The objective in Equation 11 has a similar form as the original cost function for the gradient-based adversarial attack in that they both strive to find the optimal augmented version of the clean input. Additionally, here the augmentation function needs to minimize the constraint, in addition to maximizing $L_{fr}$. The augmentation that is obtained in this manner avoids collapsing the inherent identity-related information because the constraint will explode when the transformed representation drifts toward the $\\Phi$. In essence, Equation 11 requires the augmentation function to transform style so that they are hard for the FR model in the range where valid for FR. Subsequent to obtaining the desired synthetic style information, the FR model is training using original and synthesized styles, as illustrated in Figure 3 and Algorithm 1."}, {"title": "4. Experiments", "content": "We utilize cleaned version MS-Celeb-1M [19] as our labeled training dataset. The original MS-Celeb-1M includes a significant amount of labeling noise. Pioneering work of [10] provides a cleaned version of this dataset with almost 4M images from 85K identities. As per the conventional FR framework, all used datasets in our work are aligned and transformed to 112 \u00d7 112 pixels. For unlabeled datasets, we employed WiderFace [64], which is known to be one of the most challenging benchmarks in face detection studies due to diversity in face scale and nuances. Thus, WiderFace works for our purpose since it contains both recognizable and unrecognizable faces. Following [50, 36] and for a fair comparison, we utilize 70k face images from WiderFace. We report performance on TinyFace [6], IJB-B [61], IJB-C [40], IJB-S [29], and SCFace [17] datasets to evaluate the proposed approach.\nTinyFace [6] is a low-quality FR evaluation dataset comprising 5,139 labeled identities with 169,403 images. The images are designed for 1:N recognition tests and have an average size of 20\u00d716 pixels. The images in TinyFace were collected from public web data and captured faces under various uncontrolled conditions, including different poses, illumination, occlusion, and backgrounds.\nIJB-B and IJB-C. IJB-B [61] contains around 21.8K images (11.8K faces and 10K non-face images) and 7k videos (55K frames). A total of 1,845 identities are presented in this dataset. Our experimental protocols follow the standard 1:1 verification, which contains 10,270 positive and 8M negative matches. There are 12,115 templates in the protocol, each of which consists of multiple images or frames. Consequently, a template-based matching process is used. Specifically, we average over the instances in a template to obtain the global feature vector for each template. IJB-C [40] is the extended version of IJB-B, including 31.3K images and 117.5K frames from 3,531 identities. The testing protocol of IJB-C is similar to IJB-B.\nIJB-S [29] is one of the most challenging FR benchmarks with samples from surveillance videos. It consists of 350 surveillance videos spanning 30 hours in total, from 202 identities, with an average of 12 videos per subject. Also, there is 7 high-quality photo for each subject with different poses. There are three keywords that define the evaluation metrics in this dataset:\n\u2022 Surveillance: denotes the surveillance video.\n\u2022 Single: denotes high quality enrollment.\n\u2022 Booking: denotes multiple enrollment images taken from different viewpoints.\nEvaluation protocols for this data are: 1) Surveillance-to-Single, 2) Surveillance-to-Booking, and 3) Surveillance-to-Surveillance. The first notation indicates the source of gallery inputs, and the second refers to the probe source.\nSCFace [17] is a challenging cross-resolution FR evaluation benchmark. It contains mugshots and images captured by surveillance cameras. The images were taken from 130 subjects in an uncontrolled indoor environment using five video surveillance cameras at three different distances {4.2, 2.6, 1.0} (meter); five images at each distance. Also, one frontal mugshot image for each subject is obtained using a digital camera."}, {"title": "4.2. Implementation Details", "content": "We adopt a modified version of ResNet [10] for the backbone. The model is trained for 24 epochs with ArcFace loss. The optimizer is SGD, with the learning rate starting from 0.1, which is decreased by a factor of 10 at epochs {10, 16, 22}. The optimizer weight-decay is set to 0.0001, and the momentum is 0.9. During training, the mini-batch size on each GPU is 512, and the model is trained using four RTX 6000. Following [20], $\\alpha$ in Equation 8 is 0.99. Given a pair of images, the cosine distance between the representations is the metric during inference."}, {"title": "4.3. Comparison with SOTA Methods", "content": "For a fair comparison, we employed the official code released by ArcFace, and CFSM and utilized the optimal hyper-parameters recommended in their original paper to reproduce their results on MS1MV2. Table 1, compares the proposed method with others in TinyFace dataset. The presented method outperforms its competitors across evaluation metrics, backbones, and training sets. Specifically, our method surpasses CSFM by 1.19% and 1.29% accuracy on rank-1 and rank-5 identification, respectively. These improvement emphasise the generalizablity of our framework and that our proposal is not restricted to certain training set or backbone.\nFurthermore, Table 2 and Table 3 present the verification performance on IJB-B and IJB-C datasets, illustrating that the proposed approach achieves new SOTA performance. These consistent improvements on IJB-B and IJB-C showcase the generalizability of our approach on both SC and UC evaluation samples, since these datasets consists SC and UC instances. To evaluate the proposed method on cross-resolution scenario, we report evaluation on SCFace. As shown in Table 4, our approach can effectively map both high and low-quality samples of a given identity close to each other and obtain new SOTA performance in this evaluation benchmark. Finally, to evaluate the presented approach in a more challenging scenario, Table 5 shows the performances of IJB-S demonstrating on-par performance with CFSM. It is worth mentioning that we reduce the computational burden of CFSM by avoiding image space manipulation."}, {"title": "4.4. Orthogonal Improvement to Angular Margins", "content": "The proposed method works by adjusting sample difficulty for the model training. Therefore, we want to investigate the result of different angular margin loss functions on its effectiveness. Specifically, we adjusted the training code of the following SOTA methods to add our augmentation policy: CosFace (margin = 0.35), ArcFace (margin = 0.5), and AdaFace (margin = 0.4). As shown in Table 6, the proposed method improves the performance across loss functions, indicating the orthogonality of the proposed method to existing angular penalty losses."}, {"title": "4.5. Effect of Constraint", "content": "To validate the role of constraint in producing valid training instances for the FR model, we train models with varying $\\beta$. Fig. 5 shows the performance of different models. Accordingly, lowering the constraint, i.e., $\\beta < 1.0$, brings marginal improvement in TinyFace; however, it causes performance degradation on IJB-B, showcasing that the model generalization across samples with better quality is reduced. On the other hand, too much restriction, large $\\beta$, produces easy samples for the training and does not significantly benefit performance in either IJB-B or TinyFace."}, {"title": "4.6. Blind VS. Targeted Adversary", "content": "To better understand the efficacy of targeted augmentation in improving model generalization, we conduct experiments where the procedure of producing an adversarial style does not involve utilizing unlabeled samples, and the style is blindly produced with arbitrary direction. Table 7 compares the performance of the proposed method and non-targeted attack, reflecting the significance of employing unlabeled samples during the training. Note that without employing unlabeled samples, the lagrangian constraint is not accessible, and we don't have access to the unrecognizable samples. We argue that this performance degradation is due to the arbitrary direction of adversarial attack. In other words, the augmentation module produces novel and challenging styles for the training; however, the styles are unrealistic and convey less information about real-world SC instances."}, {"title": "4.7. Trainin Speed and Memory Consumption", "content": "One significant benefit of the TSA module is the augmentation without needing image space manipulation. In Table 8, we compare the proposed method's training speed and GPU memory usage with CFSM. Accordingly, by avoiding image space augmentation, we increased the training speed of the CFSM by almost 70%. Furthermore, our model strives to dramatically reduce GPU memory consumption by removing the need for a generative module during the training."}, {"title": "5. Conclusion", "content": "This paper presented a simple yet effective training paradigm for face recognition. The proposed procedure, mixes the feature statistics from the labeled and unlabeled face datasets to synthesize novel style information, which is inspired by the observation of the susceptibility of current face recognition models to the style information of the training set. Our method dynamically identifies the unrecognizable instances from unlabeled datasets to avoid synthesizing unrecognizable styles. This framework, allows the face recognition model to become exposed to novel and realistic styles during the training and improve its generalization. The efficacy of our method is evaluated through various experiments and evaluation across different benchmarks, including IJB-B, IJB-C, IJB-S, TinyFace, and SCFace."}]}