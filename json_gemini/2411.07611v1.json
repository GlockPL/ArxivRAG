{"title": "Multimodal Clinical Reasoning through Knowledge-augmented Rationale Generation", "authors": ["Shuai Niu", "Jing Ma", "Liang Bai", "Zhihua Wang", "Yida Xu", "Yunya Song", "Xian Yang"], "abstract": "Clinical rationales play a pivotal role in accurate disease diagnosis; however, many models predominantly use discriminative methods and overlook the importance of generating supportive rationales. Rationale distillation is a process that transfers knowledge from large language models (LLMs) to smaller language models (SLMs), thereby enhancing the latter's ability to break down complex tasks. Despite its benefits, rationale distillation alone is inadequate for addressing domain knowledge limitations in tasks requiring specialized expertise, such as disease diagnosis. Effectively embedding domain knowledge in SLMs poses a significant challenge. While current LLMs are primarily geared toward processing textual data, multimodal LLMs that incorporate time series data, especially electronic health records (EHRs), are still evolving. To tackle these limitations, we introduce ClinRaGen, an SLM optimized for multimodal rationale generation in disease diagnosis. ClinRaGen incorporates a unique knowledge-augmented attention mechanism to merge domain knowledge with time series EHR data, utilizing a stepwise rationale distillation strategy to produce both textual and time series-based clinical rationales. Our evaluations show that ClinRaGen markedly improves the SLM's capability to interpret multimodal EHR data and generate accurate clinical rationales, supporting more reliable disease diagnosis, advancing LLM applications in healthcare, and narrowing the performance divide between LLMS and SLMs.", "sections": [{"title": "1 Introduction", "content": "The widespread adoption of electronic health records (EHRs) in healthcare has significantly advanced deep learning techniques. EHRs include a variety of data types, such as medical notes, laboratory (lab) test results, and chest X-ray (CXR) images. These datasets are extensively employed in various healthcare applications, including disease diagnosis, mortality prediction, and drug discovery (Niu et al., 2024a; Xu et al., 2018; Laghuvarapu et al., 2024). Our research focuses on using multimodal EHRs, particularly medical notes and lab test results, for disease diagnosis. Usually, disease diagnosis is treated as a classification problem. However, with the development of pre-trained language models, generative methods have gained popularity. These methods excel in identifying clinical relationships between EHR inputs and diagnoses, often surpassing most classification models (Niu et al., 2024a). Despite their effectiveness, these generative approaches lack a clear and intuitive rationale for interpreting diagnosed results, which is essential for trustworthy healthcare.\n\nThe increasing use of prompt learning (Li and Liang, 2021; Niu et al., 2024b), chain-of-thought (COT) (Wei et al., 2022), and instruction tuning (Wei et al., 2021; Chung et al., 2024) has significantly advanced the capabilities of Large Language Models (LLMs) in natural language processing (NLP). These models excel across various tasks and support their outputs with rationales (Touvron et al., 2023; Achiam et al., 2023). In healthcare, advanced medical LLMs such as MedPaLM (Singhal et al., 2023), a 540B model trained on diverse medical Q&A datasets and MEDITRON (Chen et al., 2023), which utilizes LLaMA2 for medical reasoning, have shown notable performance improvements in domain-specific applications. Similarly, AL-PACARE (Zhang et al., 2023), built on LLaMA2 and trained through instruction tuning with a specialized dataset created by GPT-4, demonstrates the potential of targeted training in enhancing the effectiveness of medical LLMs. However, tuning such large-scale LLMs for specific tasks often faces practical challenges due to their substantial computation demands. Moreover, EHRs are typical multimodal data; these models only improve med-"}, {"title": "2 Related Work", "content": "Recent advancements in high-quality language datasets and computational resources have driven significant growth in LLMs (Jiang et al., 2023; Touvron et al., 2023; Achiam et al., 2023). In the healthcare sector, various LLMs have been developed for tasks such as medical Q&A and clinical concept extraction (Singhal et al., 2023; Yang et al., 2022). These models primarily focus on enhancing answer accuracy but often fall short of providing supportive clinical rationales. Although some specialized medical LLMs (Chen et al., 2023; Zhang et al., 2023) could effectively address these needs, the substantial resources required for its training limit its practical application across diverse medical tasks, highlighting the necessity for more resource-efficient methods.\n\nTo address this issue, rationale distillation has emerged as a key technique for transferring the capabilities of LLMs to SLMs (Hsieh et al., 2023; Ho et al., 2023; Kang et al., 2024). Supported by chain-of-thought prompting (Wei et al., 2022), this approach enhances SLMs' performance by leveraging the internal reasoning processes of LLMs. However, current rationale distillation strategies"}, {"title": "3 Methodology", "content": "We present ClinRaGen, a novel approach designed to enhance acute disease diagnosis and clinical rationale in SLMs operating on multimodal EHRS, augmented with domain-specific knowledge. The inputs are represented by {M,T, K}, where M stands for medical notes, T denotes the numerical time series lab test results, and K stands for disease-specific clinical knowledge. Unlike conventional disease classification approaches (Niu et al., 2021; Xu et al., 2018), ClinRaGen uses an instructive chain-of-thought tuning method to generate textual disease tokens D and produces medical notes-based clinical rationale Rm and lab test-based clinical rationale Rt.\n\nTo provide a multimodal clinical rationale for disease diagnosis, our model greatly inherits the power of LLMs, extracting clinical rationales as a chain-of-thought mechanism for SLMs. This approach not only enhances the diagnostic accuracy of SLMs but also provides well-founded multimodal rationales for each diagnosis. Additionally, ClinRaGen addresses the challenge of SLMs understanding time series lab test results by integrating external disease-oriented domain knowledge from the medical knowledge database-PubMed\u00b9. This domain knowledge, refined into concise medical terms by LLMs, supports our model in semantically understanding time series data through token-level attention mechanisms and serves as the knowledge base essential for producing reliable rationales.\n\nOur approach comprises two main components: preprocessing for knowledge augmentation and clinical rationale generation using LLMs and the PubMed medical knowledge base, as outlined in"}, {"title": "3.1 Framework Overview", "content": "We present ClinRaGen, a novel approach designed to enhance acute disease diagnosis and clinical rationale in SLMs operating on multimodal EHRS, augmented with domain-specific knowledge. The inputs are represented by {M,T, K}, where M stands for medical notes, T denotes the numerical time series lab test results, and K stands for disease-specific clinical knowledge. Unlike conventional disease classification approaches (Niu et al., 2021; Xu et al., 2018), ClinRaGen uses an instructive chain-of-thought tuning method to generate textual disease tokens D and produces medical notes-based clinical rationale Rm and lab test-based clinical rationale Rt.\n\nTo provide a multimodal clinical rationale for disease diagnosis, our model greatly inherits the power of LLMs, extracting clinical rationales as a chain-of-thought mechanism for SLMs. This approach not only enhances the diagnostic accuracy of SLMs but also provides well-founded multimodal rationales for each diagnosis. Additionally, ClinRaGen addresses the challenge of SLMs understanding time series lab test results by integrating external disease-oriented domain knowledge from the medical knowledge database-PubMed\u00b9. This domain knowledge, refined into concise medical terms by LLMs, supports our model in semantically understanding time series data through token-level attention mechanisms and serves as the knowledge base essential for producing reliable rationales.\n\nOur approach comprises two main components: preprocessing for knowledge augmentation and clinical rationale generation using LLMs and the PubMed medical knowledge base, as outlined in"}, {"title": "3.2 LLMs for Knowledge Augmentation and Rationale Generation", "content": "LLMs, such as GPT-3.5 Turbo (Brown et al., 2020), demonstrate impressive performance on domain-specific questions, though this comes at the cost of extensive training on large datasets and substantial computational resources. In contrast, SLMs like T5 (Raffel et al., 2020) and OPT (Zhang et al., 2022) are built with significantly fewer resources. Consequently, the knowledge encapsulated in SLMs may be insufficient for effectively handling knowledge-intensive tasks, even when incorporating distilled knowledge from LLMs (Kang et al., 2024; Ho et al., 2023). To address this limitation, our model, ClinRaGen, integrates external domain-specific medical knowledge to enhance the capability of SLMs, aiming to achieve performance comparable to LLMs in disease diagnosis and reliable clinical rationale generation."}, {"title": "3.2.1 Disease-oriented Knowledge Augmentation", "content": "LLMs, such as GPT-3.5 Turbo (Brown et al., 2020), demonstrate impressive performance on domain-specific questions, though this comes at the cost of extensive training on large datasets and substantial computational resources. In contrast, SLMs like T5 (Raffel et al., 2020) and OPT (Zhang et al., 2022) are built with significantly fewer resources. Consequently, the knowledge encapsulated in SLMs may be insufficient for effectively handling knowledge-intensive tasks, even when incorporating distilled knowledge from LLMs (Kang et al., 2024; Ho et al., 2023). To address this limitation, our model, ClinRaGen, integrates external domain-specific medical knowledge to enhance the capability of SLMs, aiming to achieve performance comparable to LLMs in disease diagnosis and reliable clinical rationale generation.\n\nFigure 1 shows the details of knowledge augmentation. Specifically, we utilize the PubMed medical knowledge database to retrieve the number of abstracts of documents for each target disease, denoted as Kabs. Subsequently, we employ GPT-3.5 Turbo to extract disease-related medical terms from these abstracts, forming a robust knowledge base K:\n\nK = argmaxPLLM(K | D, Kabs). (1)\n\nThe number of Kabs is selected to be sufficiently large until the extracted disease-related medical terms exhibit minimal variation. This approach aims to help bridge the knowledge gap between SLMs and LLMs."}, {"title": "3.2.2 Clinical Rationale Generation", "content": "Building on prior research demonstrating that LLMs can effectively enhance the reasoning capabilities of SLMs for various tasks (Kwon et al.,"}, {"title": "3.3 Model Structure and Distillation Paradigms", "content": "Figure 3a) illustrates the structure of our model ClinRaGen, which includes a time series encoder (TSE), a knowledge-augmented (KA) attention module, and a SLM for processing multimodal EHRs and generating disease diagnoses and clinical rationales. We use PatchTST (Nie et al., 2022) as the time series encoder to encode lab test results. The knowledge-augmented attention module aims to reprogram time series features by external domain knowledge tokens, where time series features are set as queries and external domain knowledge tokens are set to key and value for the multi-heads attention mechanism. In addition, to achieve multimodal rationale generation, we propose a sequential multimodal rationale distillation paradigm, as shown in Figure 3b)."}, {"title": "3.3.1 Model Structure", "content": "Figure 3a) illustrates the structure of our model ClinRaGen, which includes a time series encoder (TSE), a knowledge-augmented (KA) attention module, and a SLM for processing multimodal EHRs and generating disease diagnoses and clinical rationales. We use PatchTST (Nie et al., 2022) as the time series encoder to encode lab test results. The knowledge-augmented attention module aims to reprogram time series features by external domain knowledge tokens, where time series features are set as queries and external domain knowledge tokens are set to key and value for the multi-heads attention mechanism. In addition, to achieve multimodal rationale generation, we propose a sequential multimodal rationale distillation paradigm, as shown in Figure 3b)."}, {"title": "3.3.2 Medical Note-based Rationale Distillation", "content": "Initially, we train the SLM of ClinRaGen using medical notes as input. This phase involves generating disease diagnoses and distilling knowledge from LLM-generated multimodal clinical rationales. The outputs of the LLM guide the SLM's learning process and facilitate the transfer of the LLM's reasoning capabilities to the SLM. We adopt a coarse-grained training approach in this stage. Using medical notes M, medical notes-based rationale Rm and lab test-based rationale Rt from the teacher LLM, we train the SLM through a language model generation loss:\n\nLdistillnote (\u03b8) = E[\u2013 log PSLM, (D, Rm, Rt | M)], (4)\n\nwhere \u03b8 represents the trainable parameters of the SLM. This training process enables the SLM to initialize and extract hidden lab test information from medical notes even in the absence of direct lab test results as inputs."}, {"title": "3.3.3 Knowledge-augmented Attention for Lab Test-based Rationale Distillation", "content": "Next, to effectively extract time series features from lab test results, we freeze the parameters of the distilled SLM and use it as a decoder for generating disease diagnoses and lab test-based rationales. In this phase, we train a time series encoder and a knowledge-augmented attention module to encode and understand numerical lab test results. This module helps SLMs process time series data and leverage external domain knowledge for accurate clinical rationales and diagnoses.\n\nTherefore, we employ the PatchTST to encode the lab test results T and then use a multi-head cross-attention layer to reprogram the encoded time series features into language tokens represented by the corpus of external domain knowledge K. The pre-trained SLM, now serving as the decoder, generates lab test-based rationales:\n\nLdistilllab(\u03c6) = E[\u2013 log PSLM, (D, Rt | f\u03c6(T, K))], (5)\n\nwhere fo refers to the encoding function composed of PatchTST and knowledge-augmented attention parameterised by ."}, {"title": "3.3.4 Multimodal Rationale Distillation", "content": "Finally, to develop the multimodal clinical rationale generation capability of ClinRaGen, we integrate textual medical notes with time series lab test results as inputs to the model. This phase involves tuning all components of ClinRaGen to generate multimodal rationales Rm and Rt that support disease diagnosis D:\n\nLdistillmm(\u03b8, \u03c6) =E[-log PSLM, (D, RM, Rt |\nM, f\u03c6(T, K))]. (6)\n\nThese comprehensive training paradigms ensure that our ClinRaGen can effectively integrate and understand multiple data modalities, providing accurate clinical diagnoses and modality-consistent rationales."}, {"title": "4 Experiments", "content": "In this section, we evaluate ClinRaGen on two public EHR datasets, comparing it with several state-of-the-art baseline models to assess the impact of multimodal rationale generation on supporting disease diagnosis. We also compare the quality of our model's rationales with those generated by several advanced LLMs."}, {"title": "4.1 Dataset", "content": "We evaluate our model on two public EHR datasets: MIMIC-III (Johnson et al., 2016) and MIMIC-IV (Johnson et al., 2023). The MIMIC-III dataset, collected from 2001 to 2012, includes 28,456 EHRS with comprehensive medical discharge summaries and lab test results. Similarly, the MIMIC-IV dataset was collected from 2008 to 2019 for 28,900 EHRs. Both datasets employ mimic3-benchmark tools (?) to process time series lab test results. Additionally, we manually refined medical notes (brief hospital course) by removing extraneous elements like numbers and stop words. Our study targets 25 disease phenotypes for both MIMIC-III and MIMIC-IV datasets and a 4:1 training-to-testing split strategy, as defined in the mimic3-benchmark tools. To use LLMs with MIMIC datasets, we follow the \"opting out of the review process\" agreement and utilize LLMs from Azure services."}, {"title": "4.2 Baseline Methods", "content": "To evaluate the effectiveness of ClinRaGen, we compared our model with several state-of-the-art baselines for disease diagnosis generation: Flan-T5-small (Chung et al., 2024), PROMPTEHR"}, {"title": "4.3 Disease Diagnosis Performance", "content": "We evaluate disease diagnosis performance using the metrics of micro and macro precision, recall, and F1 scores. Table 1 presents the performance of our ClinRaGen model in disease diagnosis on the MIMIC-III and MIMIC-IV datasets with the following key observations. In textual modality, LLMs such as LLaMA-7B outperform SLMs like Flan-T5-small and PROMPTEHR, suggesting that a larger number of model parameters significantly enhances predictive power in disease diagnosis. Multimodal models, such as FROZEN and EHR-KnowGen, demonstrate superior prediction performance compared to single-modality models, indicating that lab test results contribute positively to disease diagnosis accuracy. Among all baselines, Clinical CoT exhibits the best performance, highlighting the effectiveness of rationale distillation in aiding SLMs in generating accurate disease diagnosis.\n\nOur model ClinRaGen achieves the highest evaluation performance in both micro and macro F1 scores on the MIMIC-III and MIMIC-IV datasets. Notably, ClinRaGen, despite being a SLM, outperforms LLaMA-7B by leveraging efficient multimodal understanding ability and knowledge-augmented rationale generation. This demonstrates the potential of knowledge augmentation in assisting SLMs for multimodal EHR understanding, enabling SLMs to match or even surpass the performance of LLMs in specific clinical tasks."}, {"title": "4.4 Ablation Study", "content": "In this section, we perform ablation studies to assess the contribution of each component within our ClinRaGen model. ClinRaGen-I omits lab test results and knowledge input. ClinRaGen-II substitutes knowledge-based token corpus with default word embeddings. ClinRaGen-III maintains the structural integrity of the original model but excludes rationale distillation."}, {"title": "4.5 Rationale Evaluation", "content": "To evaluate the quality of the multimodal clinical rationales generated by our model, we, along with a group of professional clinicians, designed several rationale evaluation metrics, referencing (Lin et al., 2024; Kwon et al., 2024). Specifically, we defined the following metrics: 1). Correctness: how medically accurate the rationale supports the diagnosis results. 2). Readability: the extent to which a clinical rationale adheres to proper grammar and structural rules. 3). Soundness: the logical coherence and insight provided by the clinical rationale. 4). Consistency: the degree of alignment between the clinical rationale derived from medical"}, {"title": "4.5.1 LLMs Evaluation", "content": "Currently, there is no gold standard for systematically evaluating clinical rationales based on specific patients' hospital records and diagnostic results, as diagnoses rely on the clinician's knowledge and experience and are not formatted in MIMIC datasets. GPT-4 (Achiam et al., 2023) is recognized as the most powerful LLM globally, achieving expert-level performance in specific domains with appropriate domain-specific instructions (Nori et al., 2023). In text generation research, GPT-4 has also been acknowledged as an effective evaluator (Lin et al., 2024; Chiang and Lee, 2023). To effectively"}, {"title": "4.5.2 Human Evaluation", "content": "For human evaluation, we select 100 randomly selected clinical rationales of disease diagnosis from two MIMIC datasets. Fifteen professionally qualified clinicians were invited to assess the rationale generated by Mistral, LLaMA2, LLaMA3, and our model ClinRaGen."}, {"title": "4.5.3 Evaluation Results", "content": "Figures 4(a) and 4(b) present the evaluation of five criteria by GPT-4 and humans, with closely aligned results. Notably, LLaMA3 outperforms other models, closely approximating the GT, which is attributed to its larger parameter size, extensive pre-training, and longer tokenizer. Conversely, LLaMA2 and Mistral show comparable but inferior performance, primarily due to their significant issue with generating coherent clinical rationales for disease diagnosis, often producing garbled or irrelevant responses. Our model ranks second among the compared models, behind LLaMA3, which is the strongest open-source LLM currently available. It achieves similar readability and correctness as LLaMA3 and surpasses LLaMA2 and Mistral in these aspects. For consistency scores, all baseline models simplify input more easily understandable lab test anomaly captions, whereas our model processes numerical time series lab test data, requiring a knowledge-augmented attention transformation for SLMs' comprehension. Despite this, our model still achieves the second-best performance. In terms of soundness and persuasiveness, despite being based on the Flan-T5-small, this limitation in parameters and token length results in some discrepancies compared to other baselines. However, its effect is superior to that of LLaMA2 and Mistral. Furthermore, Appendix E provides additional rationale evaluation results by using BLEU (Papineni et al., 2002) and BERTScore (Zhang et al., 2019) to evaluate the quality of rationale generation further."}, {"title": "4.6 Case Study on Reasoning Analysis", "content": "To validate the qualitative performance of our model on generating clinical rationales, we compare the rationale generation performance of our model ClinRaGen with GPT-3.5 Turbo."}, {"title": "4.6.1 Effective Multimodal Rationale Generation", "content": "As illustrated in Figure 5, our model ClinRaGen can produce both medical note-based rationales (e.g., \"Based on the medical notes...\") and lab test-based rationales (e.g., \u201cLab test shows...\"), akin to the outputs of GPT-3.5 Turbo. For medical note-based rationale generation, ClinRaGen effectively extracts key medical terms essential for disease diagnosis (highlighted in green). Additionally, for lab test-based rationales, our model accurately identifies abnormal lab test features (highlighted in blue), demonstrating its capability to understand numerical time series lab test data effectively. These results indicate that ClinRaGen competently produces clinically relevant multimodal rationales to support disease diagnosis.\""}, {"title": "4.6.2 Better Generalization", "content": "To assess the correctness of rationales generated by ClinRaGen compared to GPT-3.5 Turbo, we examine the relevance of key medical terms within the rationale to the diagnosed diseases. For instance, in the first example, our model identifies"}, {"title": "5 Conclusion and Future Works", "content": "In this paper, we introduce ClinRaGen, a multimodal EHR understanding and disease diagnosis model designed to simultaneously provide knowledge-augmented multimodal rationales. Our model includes three crucial components: a time series encoder, a knowledge-augmented attention module, and a small language model. To enable multimodal rationale generation, we first use a teacher LLM for knowledge augmentation and multimodal rationale creation. Subsequently, we propose a sequential distillation paradigm for step-by-step model training. Comprehensive experiments on the MIMIC-III and MIMIC-IV datasets, supported by both quantitative and qualitative evaluations, demonstrate that ClinRaGen effectively enables SLMs to interpret multimodal EHRs and generate accurate disease diagnoses and clinical rationales. This advancement enhances the application of LLMs in healthcare and reduces the performance gap between LLMs and SLMs. Future work will focus on extending our methodology to a wider range of SLMs, datasets, and downstream tasks."}]}