{"title": "Few-Shot Multi-Human Neural Rendering Using Geometry Constraints", "authors": ["Qian li", "Victoria Fern\u00e0ndez Abrevaya", "Franck Multon", "Adnane Boukhayma"], "abstract": "We present a method for recovering the shape and radiance of a scene consisting of multiple people given solely a few images. Multi-human scenes are complex due to additional occlusion and clutter. For single-human settings, existing approaches using implicit neural representations have achieved impressive results that deliver accurate geometry and appearance. However, it remains challenging to extend these methods for estimating multiple humans from sparse views. We propose a neural implicit reconstruction method that addresses the inherent challenges of this task through the following contributions: First, we propose to use geometry constraints by exploiting pre-computed meshes using a human body model (SMPL). Specifically, we regularize the signed distances using the SMPL mesh and leverage bounding boxes for improved rendering. Second, we propose a ray regularization scheme to minimize rendering inconsistencies, and a saturation regularization for robust optimization in variable illumination. Extensive experiments on both real and synthetic datasets demonstrate the benefits of our approach and show state-of-the-art performance against existing neural reconstruction methods.", "sections": [{"title": "1. Introduction", "content": "Human reconstruction from single images [15, 41, 53], multiple images [16, 26], RGB videos [2, 45] or RGB-D data [104, 106] has received a lot of attention, much less explored is the task of multiple human scenario, which is essential for scene understanding, behavior modeling, collaborative augmented reality, and sports analysis. The multi-human setting introduces additional challenges, as there is now a higher level of occlusion and clutter which hinders matching and reconstruction. Although in principle one could approach this by first detecting and then independently processing each person, simultaneous reconstruction of multiple humans can help to globally reason about occlusion at the level of the scene [38, 84], and can potentially recover coherent 3D spatial relations among the people.\nSeveral recent works have attempted to recover multiple humans from a single view [14, 22, 38, 61, 83, 84, 87, 108, 109, 111]. However, the majority of these are based on regressing the parameters of a human body model -typically SMPL [57]- which provides coarse reconstructions that lack hair, clothing, and geometric details. Multi-view settings can help resolve some of the occlusions as well as depth ambiguities, but require a dense array of RGB cameras to achieve a detailed reconstruction [16, 39, 89]. A more convenient capture system is the sparse multi-view setting, where only a handful of cameras is required. However, due to the decreased number of views and increased level of occlusion, existing methods require segmentation masks and a pre-scanned template mesh [55, 96], rely on a coarse body model [30, 115], or require temporal information [30, 116].\nA parallel line of work simultaneously tackles the novel-view-synthesis and geometry-reconstruction problems by combining neural coordinate-based representations, e.g. implicit signed distance functions (SDFs) [71], with differentiable rendering [59, 90, 100, 101]. This approach has the advantage of producing, along with geometry, renderings from novel viewpoints that can capture complex surface/light interactions, increasing the scope of applications. NeRF [59], for example, uses volumetric rendering to produce impressive images under novel views, albeit at the cost of sub-optimal geometries due to the unconstrained volumetric representation. SDF-based methods [90, 100, 101], while delivering images of slightly lower quality, have been shown to produce 3D surfaces that are competitive with classical approaches. For humans, this has been leveraged to obtain geometry and appearance from monocular video [12, 37], RGB-D video [19], and sparse multi-view video [46, 52, 73, 76, 91, 93, 99, 116]. However, none of these works, with the exception of [110, 116], were designed to handle the increased geometric complexity and occlusion of the multi-human case. Current works [110, 116] address the multi-human setting, but both require a set of videos, which effectively becomes a dense array of views as long as deformations are modeled correctly.\nIn this paper, we address the problem of multiple 3D human surfaces and volume rendering from sparse static multi-view images. Our key insight is that human-specific geometric constraints can be leveraged to tackle the challenging sparse-view setting.\nSpecifically, we first obtain a SMPL body model from the input data and use it to initialize the implicit SDF network, where we define the surface of a multi-human scene as the zero-level set of the SDF. Then, the geometry network is optimized with multi-view images by leveraging surface and volume rendering [90] along with uncertainty estimation methods [18, 77], where the SMPL meshes are treated as noisy estimations. To achieve higher rendering quality from sparse training views, we additionally propose a patch-based regularization loss that guarantees consistency across different rays and a saturation regularization that ensures consistency for variable image illuminations within the same scene.\nWe evaluate our method quantitatively and qualitatively on real multi-human (CMU Panoptic [40, 81]) and synthetic (MultiHuman [116]) datasets. We demonstrate results on 5,10,15 and 20 training views, where we achieve state-of-the-art performance in terms of surface reconstruction and novel view quality."}, {"title": "2. Related Work", "content": "Single-Human Reconstruction. There is a vast amount of work on reconstructing 3D humans from single images [7, 15, 41, 53, 60], monocular video [3, 45, 107], RGB-D data [9, 104, 106] and multi-view data [16, 26, 33, 82]. We concentrate here on the multi-view setting. High-end multi-view capture systems can achieve reconstructions of outstanding quality [16, 21, 26, 40, 47, 89], but require a complex studio setup that is expensive to build and not easily accessible. To alleviate this, numerous works have been proposed that use instead a sparse set of RGB cameras (e.g. between 2 and 15), where the lack of views and presence of wide baselines is compensated by tracking a pre-scanned template [10, 17, 23, 88, 97], using a parametric body model [6, 32], or more recently, by the use of deep learning [33, 46, 50, 52, 73, 76, 91, 93, 99].\nMulti-Human Reconstruction. In contrast, there has been a limited number of works that address the problem of multiple human reconstruction. This is a difficult task since the presence of several people increases the geometric complexity of the scene, introduces occlusions, and amplifies ambiguities such that commonly used features like color, edges, or key points cannot be correctly assigned.\nFor single images and video, the problem has been mainly tackled by regressing the parameters of the SMPL [57] body model [14, 20, 22, 25, 38, 83, 84, 87, 108, 109, 111, 112]. Although this can work robustly with as little as one view, the reconstructions are very coarse and cannot explain hair, clothing, and fine geometric details. The only exception is the work of Mustafa et al. [61], which performs model-free reconstruction of multiple humans by combining an explicit voxel-based representation with an implicit function refinement. However, the method requires training on a large synthetic dataset of multiple people which hinders generalization. Our work, on the other hand, performs 3D reconstructions, produces renderings of novel views, and can generalize to arbitrary multi-human scenes.\nMulti-view capture setups can help resolve depth ambiguities and some of the occlusions. Classic methods for estimating multiple humans rely heavily on segmentation masks and template mesh tracking [54, 55, 96]. We avoid the use of segmentation masks by adopting volumetric rendering for implicit surfaces [90]. More recently, deep learning-based approaches were proposed, but they either require temporal information [30, 80, 115, 116], pre-training on a large dataset [116] which cannot work on general scenes, or a coarse body model [30, 80, 115] which lacks geometric detail. Here, we focus on the multi-human setting on static scenes and propose a method that recovers accurate reconstructions and at the same time produces renderings of novel viewpoints.\nNeural Surface Reconstruction and Novel-View Synthesis. For generating free-viewpoint video, image-based rendering has been considered as an alternative or complement to 3D reconstruction [10, 46, 52, 52, 73, 93, 98, 99]. When geometry proxies are available, neural rendering [1, 35, 86] can produce competitive novel view synthesis. Recently, NeRF[59] demonstrated impressive rendering results by representing a 3D scene as a neural radiance field, trained only with calibrated multi-view images through the use of volume rendering. However, due to the unconstrained volumetric representation and self-supervised training on RGB values, reconstructed geometries tend to be too noisy to be useful for 3D applications. To recover more accurate 3D geometry along with appearance, DVR [63], IDR [101], and NLR [42] propose to learn an implicit representation directly from multi-view images but require accurate object masks to work. To avoid the need for segmentation masks, recent works propose to combine implicit representations with volume rendering [64, 90, 100]. These methods show remarkable reconstruction results but struggle when the number of input views is low. Implicit neural representations from sparse input can be obtained by using pre-trained pixel-aligned features or 3D feature volumes for input images [4, 27, 28, 34, 36, 48, 78, 79, 103] or point clouds [8, 13, 31, 51, 65, 67, 68, 74, 75, 94], but this requires ground-truth geometry and is limited by the training data, struggling to generalize to new scenes. Sparse variants that do not require generalizable features were proposed in the image input e.g. [43, 49, 56, 62, 102] and point cloud input case e.g. [11, 66, 69, 70, 95]. InfoNeRF [43] regularizes sparse views by adding an entropy constraint on the density of the rays, RegNeRF [62] uses a patch-based regularizer over generated depth maps, and SparseNeuS [56] uses a multi-scale approach along with learned features that are fine-tuned on each scene. Our approach builds on NeuS [90], and tackles the sparse view challenge by adding human-specific geometric priors and novel regularizations."}, {"title": "3. Method", "content": "Given a sparse set of views ${I_i}_{i=1}^N$ of a multi-human scene with camera intrinsics and extrinsics ${K_i, [R|t]_i}$, our goal is to reconstruct geometry and synthesize the appearance of multiple humans from arbitrary viewpoints. The pipeline is illustrated in Fig. 12. Our approach builds on NeuS [90], which combines an implicit signed distance representation for geometry with volumetric rendering. In order to solve the challenging case of multiple humans occluding each other, we hypothesize that a naive RGB reconstruction loss is insufficient and propose to use a strong geometric prior before training with multi-view images. Towards this, we first train the implicit SDF network independently by leveraging off-the-shelf SMPL estimations (Sec. 3.2). To handle details and represent appearance, the geometry network is then fine-tuned considering foreground and background objects. Moreover, we propose the use of hybrid bounding box rendering to handle the multi-human setting (Sec. 3.3). Additionally, we define an explicit SDF constraint based on the uncertainty of the SMPL estimations, together with a ray consistency loss, and a saturation loss to improve image rendering quality for sparse views (Sec. 3.4).\nWe define a multi-human surface $S$ as the zero-level set of a signed distance function (SDF) $f_{\\theta_0}: \\mathbb{R}^3 \\rightarrow \\mathbb{R}$, encoded by a Multilayer Perceptron (MLP) $f_{\\theta_0}$ with parameters $\\theta_0$:\n$S = {p \\in \\mathbb{R}^3 | f_{\\theta_0}(p) = 0}$.\nFollowing NeuS [90], we train the geometry network $f_{\\theta_0}$ along with a color network $c_{\\theta_1}$, with parameters $\\theta_1$, mapping a point $p$ to color values (more details in Sec. 3.3). Combining the SDF representation with volume rendering, we approximate the color along a ray $r$ by:\n$C(r) = \\sum_{i=1}^N w(p_i)c_{\\theta_1}(p_i)$,\n$w(p_i) = T(p_i)\\alpha(p_i)$,\n$T(p_i) = \\prod_{j}^{i-1}(1 - \\alpha(p_j))$,\nwhere $p_i = o + t_iv$ is a sampled point along the ray $r$ starting at camera center $o$ with direction $v$; $c_{\\theta_1}(p_i)$ is the predicted color at $p_i$, $w(p_i)$ is the weight function, $T(p_i)$ is the accumulated transmittance, and $\\alpha(p_i)$ is the opacity value. Following NeuS, $\\alpha(p_i)$ is defined as a function of the signed distance representation:\n$\\alpha(p_i) = max\\left(\\left(\\frac{\\Phi(f_{\\theta_0}(p_i)) - \\Phi(f_{\\theta_0}(p_{i+1}))}{\\Phi(f_{\\theta_0}(p_i))}, 0\\right)\\right)$,\nwhere $f_{\\theta_0}(p_i)$ is the signed distance of $p_i$, $\\Phi(f_{\\theta_0}(x)) = (1+e^{-sx})^{-1}$ is the cumulative distribution function (CDF) of the logistic distribution, and $s$ is a learnable parameter (see [90] for more details)."}, {"title": "3.2. Geometric Prior", "content": "Typically, the SDF function $f_{\\theta}$ and the color function $c_{\\theta_1}$ are simultaneously optimized by minimizing the difference between the rendered and ground-truth RGB values [59, 90, 101]. While this allows to train without the need for geometric supervision, it has been noted that a photometric error alone is insufficient for the challenging sparse-view setting [18, 77], since there are not enough images to compensate for the inherent ambiguity in establishing correspondences between views. For the multi-human setting this becomes more problematic, as correspondences are even more ambiguous due to clutter.\nTo address this, we propose to regularize using geometric information by first independently training $f_{\\theta}$ using off-the-shelf SMPL fittings, which can be robustly computed from the input data. We train this network in a supervised manner by sampling points with their distance values as in [71]. Given that SMPL can only coarsely represent the real surface, we treat this geometry as a \u201cnoisy\u201d estimate that will be later improved upon using the multi-view images. Preparing for this, and inspired by [18, 77], we model the SMPL \"noise\u201d as a Gaussian distribution $\\mathcal{N}(0, S_{\\text{noise}}(p_j)^2)$ with standard deviation $S_{\\text{noise}}(p_j)$, and train $f_{\\theta_0}$ to output an estimate of the uncertainty $S_{\\text{noise}}(p_j)$ along with the distance value; that is, $f_{\\theta_0}(p_j) = (d_j, S_{\\text{noise}})$. The geometry network $f_{\\theta_0}$ is then optimized by minimizing the negative log-likelihood of a Gaussian:\n$\\mathcal{L} = \\frac{1}{n} \\sum_{j=1}^n \\left( \\log(S_{\\text{noise}}(p_j)^2) + \\frac{(d_j - d'_j)^2}{S_{\\text{noise}}(p_j)^2} \\right)$,\nwhere $n$ is the number of sampled points, $d_j$ is the predicted SDF value for point $p_j$, and $d'_j$ is the signed distance sampled directly from the SMPL meshes."}, {"title": "3.3. Hybrid Rendering with Geometry Constraints", "content": "To work with unbounded scenes, NeRF++ [113] proposed to separately model the foreground and background geometries using an inverted sphere parameterization, where the foreground is parameterized within an inner unit sphere, and the rest is represented by an inverted sphere covering the complement of the inner volume. We follow this and train separate models for foreground and background. Specifically, we use a simple NeRF [59] architecture for the background and train the foreground model using $f_{\\theta_0}$ and the color network $c_{\\theta_1}$, where the output color $C(p_i)$ is predicted as:\n$C(p_i) = c_{\\theta_1}(\\gamma(p_i), \\gamma(v_i), f_o, f_1)$.\nHere, $\\gamma(p_i)$ and $\\gamma(v_i)$ are the positional encodings [59, 85] of the sampled point $p_i$ and its ray direction $v_i$, and $f_o$ includes the gradients of predicted SDF and predicted feature from the geometry network $f_{\\theta_0}$ [101]. Additionally, to inject geometric prior knowledge into the appearance network we condition $c_{\\theta_1}$ on the rasterized depth feature from the corresponding SMPL mesh.\nFor reconstructing multiple humans, one difficulty in modeling the foreground as in NeRF++ is that the bounding sphere will contain a large empty space, making it costly to search for the surface during hierarchical sampling and adding non-relevant points to the training. To resolve this, we propose to use instead multiple 3D bounding boxes as the foreground volume. Specifically, we define a bounding box $B$ for the $j$-th human using the SMPL fittings, with minimum and maximum coordinates $[B_{\\text{min}} - \\delta, B_{\\text{imax}} + \\delta]$, where $B_{\\text{min}}$ and $B_{\\text{imax}}$ are the minimum and maximum coordinates of SMPL along the x, y, z axes respectively, and $\\delta$ is a spatial margin (here we set to 0.1). The foreground volume is then defined as $B = \\bigcup_{j=1..M}B_j$, and we define $b(p_i)$ as\n$b(p_i) = \\begin{cases} 1, p_i \\in B, \\\\ 0, p_i \\notin B \\end{cases}$\nFor points that fall inside the foreground, $p \\in B$, we calculate the opacity value $\\alpha^{FG}(p_i)$ using the predictions of $f_{\\theta_0}(p_i)$ according to Eq. 5, and the color $C(p_i)^{FG}$ using $c_{\\theta_1}$. The points that fall outside the bounding box are modeled as background using a NeRF model, where the opacity is calculated as $\\alpha^{BG}(p_i) = 1 - e^{\\sigma(p_i)d(p_i)}$, with $d$ and $\\sigma$ defined as in [59], and the color $C^{BG}$ is predicted using $\\alpha^{BG}$. Given a point $p_i$, its color and opacity values are updated as follows:\n$C(p_i) = b(p_i)C^{FG}(p_i) + (1 - b(p_i))C^{BG}(p_i)$,\n$\\alpha(p_i) = b(p_i)\\alpha^{FG}(p_i) + (1 - b(p_i))\\alpha^{BG}(p_i)$\nFinally, following [5], given a ray $r$ with $n$ sampled points ${p_i = o + t_iv}_{i=1}^n$, the color is approximated as:\n$C(r) = \\frac{\\sum_{i=1}^N W(p_i)C(p_i)}{\\sum_{i=1}^N W(p_i)}$,\nwhere $W(p_i) = T(p_i)\\alpha(p_i)$, $T(p_i) = \\prod_{j=1}^{i-1}(1 - \\alpha(p_j))$. This function allocates higher weights to points near the surface and lower weights to points away from the surface, and is used to improve the rendering quality."}, {"title": "3.4. Optimization", "content": "Given a set of multi-view images, and a pre-trained SDF network $f_{\\theta_0}$ (Sec. 3.2), we minimize the following objective:\n$\\mathcal{L} = \\mathcal{L}_r + \\lambda_{eik} \\mathcal{L}_{eik} + \\lambda_{sdf} \\mathcal{L}_{sdf} + \\lambda_r \\mathcal{L}_r + \\lambda_s \\mathcal{L}_s$,\nwhere $\\mathcal{L}_r$ is a $L_1$ reconstruction loss between the rendered image $I_r$ and the ground-truth $I_{gt}$, and $\\mathcal{L}_{eik}$ is the Eikonal loss [24].\nAdditionally, we propose to use an uncertainty-based SDF loss $\\mathcal{L}_{sdf}$, a novel ray consistency loss $\\mathcal{L}_r$ and saturation loss $\\mathcal{L}_s$ which are explained in the following.\nSDF Loss. As detailed in Sec. 3.2, we treat the SMPL mesh as a noisy estimate of the real surface. When the sampled points are not within the foreground box $B$, or the absolute sdf value predicted by the geometry network $f_{\\theta_0}$ is greater than a pre-defined threshold $\\xi_0$, or the standard deviation $S_j = S_{\\text{noise}}(p_i)_j$ is bigger than the threshold $\\xi_1$, we use the following loss:\n$\\mathcal{L}_{sdf} = \\begin{cases} \\frac{1}{n} \\sum_{j=1}^n \\left(\\log(\\xi_1^2) + \\frac{(\\log(d_j) - d'_j)^2}{\\xi_1^2} \\right), s.t. (p_i \\notin B, |d_j| > \\xi_0 \\text{ or } S_j > \\xi_1) \\\\ 0, \\text{ otherwise} \\end{cases}$\nwhere $d_j$ and $d'_j$ are the SDF predictions from the final $f_{\\theta_0}$ and initial network $f'_{\\theta_0}$, and $\\xi_0$, $\\xi_1$ are set to 0.2 and 0.5 respectively. This function encourages the network to maintain geometry consistency during learning while allowing some freedom to learn the details encoded in the images.\nRay Consistency Loss. We introduce the following ray consistency loss $\\mathcal{L}_r$ to ensure photometric consistency across all images under sparse views:\n$\\mathcal{L}_r = ||C(r_i) - C(r^*)||_1 + D_{KL}(P(r_i)||P(r^*))$\nwhere $C(r_i)$ is the ground truth color of a randomly sampled ray $r_i$ on a small patch and $C'(r)$ denotes the rendered color of an interpolated ray on a small patch. Inspired by [43], we introduce a KL-divergence regularization for the ray density, where $P(r_i) = \\frac{\\alpha_i}{\\sum \\alpha}$. The goal of this loss is to ensure consistency and smoothness of unseen rays by constraining the interpolated rays on a small patch to have a similar distribution, both for color and density.\nSaturation Loss. Finally, we observe that real-world images might contain variable illumination or transient occluders among different views (this is the case for example in the CMU Panoptic dataset [40, 81]), which can degrade the rendering quality due to inconsistency across views. Instead of learning complex transient embeddings as in [58], we propose to simply convert the RGB image into the HSV space, and calculate the L1 reconstruction loss of the saturation value between the rendered image and the ground truth: $\\mathcal{L}_r = ||I_s - I_{gt}||_1$."}, {"title": "4. Results", "content": "In this section we provide implementation details (Sec. 4.1), and demonstrate our performance against baselines on real (Sec. 4.2) and synthetic (Sec. 4.3) datasets, in terms of novel-view synthesis, visual reconstructions, and geometry error. Finally, we show ablation studies (Sec. 4.4) that demonstrate the importance of each of the proposed components."}, {"title": "4.1. Implementation Details", "content": "Our method was implemented using PyTorch [72], and trained on a Quadro RTX 5000 GPU. We use ADAM optimizer [44] with a learning rate ranging from 5 \u00d7 10-4 to 2.5 \u00d7 10-5, controlled by cosine decay schedule. Our network architecture follows [59, 101]. For a fair comparison, we sample 256 rays per batch and follow the coarse and fine sampling strategy of [90]. More network structure and training details are shown in the supplementary material."}, {"title": "4.2. Real Multi-Human Dataset", "content": "We first evaluate our approach on the CMU Panoptic Dataset [40, 81]. Our experiments were performed on five different scenes, where each scene originally includes 30 views containing 3/4/5/6/7 people. The training views were randomly extracted from the HD sequences 'Ultimatum' and 'Haggling'. We uniformly choose 5/10/15/20 views for training and the rest 25/20/15/10 views for testing. We compare with two major baselines: NeuS [90] and VolSDF[100], both in terms of novel-view synthesis and geometry reconstructions (qualitatively). For quantitative evaluation, we report three commonly used image metrics: peak signal-to-noise ratio (PSNR) [29], structural similarity index (SSIM) [92] and learned perceptual image patch similarity (LPIPS) [114]. For qualitative comparison, both rendered images and rendered normal images are shown.\nComparison with baselines. Tab. 1 demonstrates novel view synthesis results with different training views (5/10/15/20) compared to the baselines. Our proposed method outperforms these in PSNR and SSIM in all the scenes, and consistently performs better or equal in terms of LPIPS. For qualitative comparison, we demonstrate both rendered novel views and normal images in Fig. 8. As seen here, when given 5/10 training views the baseline methods fail to reconstruct a good geometry or render a realistic appearance. Although the quality of the geometries improves with 15/20 training views, the results exhibit missing body parts or can mix the background with the subjects. On the other hand, our method can reconstruct a complete geometry for all humans in all sparse-view cases.\nFig. 3 additionally shows the relationship between the number of training views and the quality of the synthesized images. The fewer the number of views, the harder it is for all methods to reconstruct high-quality images, whereas our approach is more robust to fewer training views. For denser inputs (e.g. more than 20 views), our method reaches similar albeit slightly better performance than the baselines, since the proposed work focuses on sparse scenarios.\nComparison to single human NeRF. We compare our method to the single human nerf state-of-the-art method ARAH [91]. We note that adapting such methods to our setup requires tedious manual pre-processing (detecting and segmenting people, associating detections across views), which is not required by our approach. We run a separate ARAH model for each person in the scene using 5 training images (see supp. mat.). Fig. 4 shows novel view and reconstruction results. Learning for each person separately implies providing erroneous supervision to the model whenever the person is occluded in the scene or segmentation masks are not accurate. As a result, ARAH's renderings and geometry display many artifacts compared to our results.\nConversely, our method avoids this by learning through rendering the union of SMLP bounding boxes conjointly. We also noticed that ARAH's results are very sensitive to the sparsity and choice of the training views."}, {"title": "4.3. Synthetic Dataset", "content": "Based on the MultiHuman-Dataset [105, 116], we used Unity 3D to create a synthetic dataset with 29 cameras arranged in a great circle. This includes three scenes with similar backgrounds but different camera locations and orientations. Each of the scenes contains 1/5/10 humans respectively. We train with 5/10/15 views on each scene and test with 14 fixed views. Tab. 3 reports the average error for all testing views in PSNR, SSIM and LPIPS metrics. Our method reaches state-of-the-art performance on synthesized novel-view results. Fig. 5 shows generated novel views and corresponding normal images using 10/15 training images. Our approach can reconstruct complete geometry of all humans in the scene, while the baseline methods might miss some of the people when they have similar color with the background, e.g. the shadow area in Fig. 5.\nIn the 5/10 input views case, the baseline methods usually fail to reconstruct the full geometry of humans due to the sparse inputs. Thus, we report Chamfer distance in Tab. 3 only for the 15-views case. Since the baseline methods usually contain extra floor, for a fair comparison, we sample points from ground-truth meshes and compute the distance towards the reconstructed mesh for all methods. We report the bi-directional Chamfer distance in the supplementary material. Tab. 3 shows that, with an increasing number of humans in the scene, the quality of the reconstructed geometry of all methods decreases. However, compared with the baselines, our method can better handle multiple human scenes, achieving an order of magnitude less error."}, {"title": "4.4. Ablation Study", "content": "To prove the effectiveness of our proposed components we performed ablation studies on the CMU Panoptic dataset [40, 81]. We demonstrate quantitative comparisons in Tab. 4 and qualitative results in Fig. 6. We test the following settings:\nWithout geometry regularization (\u201cw/o geometry\"). We compare our full model against the model without geometry regularization (Sec. 3.2) and SDF uncertainty regularization (Eq. 13). We can see here that, although the method is still capable of isolating humans thanks to the bounding box rendering, both geometry and novel views are much less accurate, and the rendered images exhibit background artifacts and overly smooth results.\nWithout ray consistency loss (\"w/o ray loss\"). Here we remove the proposed ray consistency loss, without which the average rendering quality also degrades.\nWithout saturation loss. Finally, we remove the saturation loss from our methods, which decreases by about 0.5 in PSNR on average. Fig. 6 shows that, without this, the image tone can contain artifacts due to changes in lighting (see for example the back of the rightmost subject).\""}, {"title": "5. Conclusion", "content": "We presented an approach for novel view synthesis of multiple humans from a sparse set of input views. To achieve this, we proposed geometric regularizations that improve geometry training by leveraging a pre-computed SMPL model, along with a patch-based ray consistency loss and a saturation loss that help with novel-view renderings in the sparse-view setting. Our experiments showed state-of-the-art performance for multiple human geometry and appearance reconstruction on real multi-human dataset (CMU Panoptic [40, 81]) and on synthetic data (MultiHuman-Dataset [116]). Our method still has several limitations. For instance, our method does not model close human interactions, as this is a much more challenging case. Addressing this is an interesting direction for future work."}, {"title": "6. Additional Results", "content": "Scene editing. We show here how our method can be used to perform post-learning scene editing without any additional training. Thanks to the human bounding-box-based modeling of the foreground scene, it is straightforward to rigidly transform or omit each person by simply applying, before rendering, the corresponding manipulation to the points sampled inside the defined bounding box. Figure 7 shows qualitative results of such application, trained on scene #5 from the CMU Panoptic dataset [40, 81] using 20 training views. We can see here that our approach can generate realistic new scenes as well as plausible inpaintings of the missing regions.\nComparisons with varying number of people. In Fig. 8 we provide additional qualitative comparisons against NeuS [90] and VolSDF [100], where we show results on the CMU Panoptic dataset [40, 81] with varying number of people in the scene (Going from 3 to 7 people). Note here how increasing the number of people reduces the quality of our baselines results, i.e. mixing the background with humans or generating noisy geometries. Meanwhile, our method performs consistently, independently of the number of people.\nAdditional Quantitative Results. Table 5 provides a full Chamfer distance comparison in the synthetic data setup as an addition to the results reported in Table 3 of the main submission. Symbol '-' represents cases where the baselines fail to reconstruct a meaningful geometry, and hence the error is too large. To favor the baselines NeuS [90] and VolSDF [100] in the main submission, we computed the uni-directional Chamfer distance from ground-truth to source, as the baselines reconstructed the ground of the scene in addition to the people. For a more standard evaluation, we additionally show here the bi-directional Chamfer distance after removing the floor for the competing methods.\nComparison to single human NeRF In Figure 4 in the main submission, we compared our work to the single human NeRF method ARAH [91] on the CMU Panoptic dataset [40, 81]. Figure 9 shows the training images used in this experiment. It also shows the segmentation masks used for ARAH for 3 people in the scene, that we built using a state-of-the-art method. Figure 9 shows additional comparative results for reconstructed appearance and geometry."}, {"title": "7. Implementation Details", "content": "Fig. 11 shows the architecture of our network in more detail (Section 3 in the main submission). The geometry MLP has 8 layers of width 256, with a skip connection from the input to the 4th layer. The radiance MLP consists of additional 4 layers of width 256, and receives as input the positional encoding of the point $\\gamma(p)$, positional encoding of the view direction $\\gamma(v)$, rasterized"}]}