{"title": "SQ-LLAVA: SELF-QUESTIONING FOR LARGE VISION-LANGUAGE ASSISTANT", "authors": ["Guohao Sun", "Can Qin", "Jiamian Wang", "Zeyuan Chen", "Ran Xu", "Zhiqiang Tao"], "abstract": "Recent advancements in the vision-language model have shown notable generalization in vision- language tasks after visual instruction tuning. However, bridging the gap between the pre-trained vision encoder and the large language models becomes the whole network's bottleneck. To improve cross-modality alignment, existing works usually consider more visual instruction data covering a broader range of vision tasks to fine-tune the model for question-answering, which are costly to obtain. However, the image contains rich contextual information that has been largely under-explored. This paper first attempts to harness this overlooked context within visual instruction data, training the model to self-supervised \u2018learning' how to ask high-quality questions. In this way, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant. SQ- LLaVA exhibits proficiency in generating flexible and meaningful image-related questions while analyzing the visual clue and prior language knowledge, signifying an advanced level of generalized visual understanding. Moreover, fine-tuning SQ-LLaVA on higher-quality instruction data shows a consistent performance improvement compared with traditional visual-instruction tuning methods. This improvement highlights the efficacy of self-questioning techniques in achieving a deeper and more nuanced comprehension of visual content across various contexts. Our implementation is available at https://github.com/heliossun/SQ-LLAVA.", "sections": [{"title": "1 Introduction", "content": "The recently emerging large vision-language methods, such as large language-and-vision assistant (LLaVA) and its variants Liu et al. [2023a,b], Chen et al. [2023], Bai et al. [2023], Zhang et al. [2024], fine-tune large language models (LLM) Zheng et al. [2023], Zhang et al. [2024], Bai et al. [2023] on visual instruction data to realize diverse open-world multimodal understanding, demonstrating a surprising efficacy of visual instruction tuning \u2013 the LLM learns to perform complex vision tasks simply by conditioning on a prompt containing image and text clues. Existing visual instruction data is mainly built upon conversations (e.g., ChatGPT/GPT4-V 202 [2023]), each consists of an image and multiple question-answer pairs. Building high-quality visual instruction data usually requires images and texts from different tasks to generate diverse questions, such as \"Please provide the bounding box coordinate of the region this sentence describes A dead leaf on the ground.\" for the object detection task. Empirically, by increasing the diversity of questions, LLaVA has achieved better performance on GQA and VizWiz tasks (26% and 45% over previous state-of-the-art methods Dai et al. [2023]). This evidence strongly suggests the advantage of training models on a broad spectrum and diverse array of tasks for enriching general vision-language understanding.\nLLaVA Liu et al. [2023b] model family usually consists of a pre-trained vision encoder (e.g., CLIP-ViT Radford et al. [2021]), a large generative language model (e.g., Vicuna Zheng et al. [2023], LLaMA Zhang et al. [2024], and QWen Bai et al. [2023]), and a vision-to-language projector implemented by a few linear layers. However, the modality gap between the pre-trained vision encoder and the language model restricts both sides' generalization ability and feature representation. To overcome this challenge, various techniques have been proposed to align the vision and language domains, which could be roughly categorized into three groups: 1) build a more robust image feature extractor Zhang et al. [2024], Zhu et al. [2024], Dai et al. [2023], 2) collect more high-quality training data Liu et al. [2023b], Chen et al. [2023], Zhu et al. [2024], Dai et al. [2023], and 3) fully fine-tune the vision and language models simultaneously during the pre-training stage Chen et al. [2023]. While these methods have shown good progress in mitigating the domain gap,"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Instruction Tuning", "content": "Instruction tuning emerged as a pivotal methodology within the realm of natural language processing (NLP), facil- itating Large Language Models (LLMs) such as GPT-3 Brown et al. [2020], PaLM Chowdhery et al. [2022], and LLAMA Touvron et al. [2023a] to interpret and execute human language instructions across a spectrum of NLP tasks. This approach diverges from traditional fine-tuning mechanisms by incorporating a specialized data structure, termed instruction-following data Wei et al. [2022], which is instrumental in the fine-tuning process of LLMs. Predominantly, there are two principal methodologies concerning instruction tuning. Early scholarly contributions delineated the initial methodology, closed-domain instruction tuning Wei et al. [2022], Sanh et al. [2022], Xu et al. [2024]. These studies engaged LLMs with a comprehensive assortment of publicly accessible datasets and subsequently assessed their performance across diverse NLP tasks Triantafillou et al. [2019]. The empirical evidence from these inquiries consistently indicated that integrating varied NLP task instructions significantly augments the LLMs' efficacy in navi- gating novel tasks. Nonetheless, LLMs calibrated with such closed-form instructions exhibited limitations in real-world user scenarios, prompting the development of an alternative approach. To address these constraints, the concept of open-domain instruction tuning Ouyang et al. [2022a], Wang et al. [2022a] was conceived. OpenAI pioneered this approach by employing human annotators to compile a corpus of real-world question-answer datasets. These datasets were the foundation for training a reward model through reinforcement learning methodologies. The trained reward model then functioned as a supervisory mechanism for further training instruction-oriented language models, such as InstructGPT Ouyang et al. [2022b] and Vicuna Zheng et al. [2023]. This innovation marked a significant advancement in the field, aiming to bridge the gap between LLM performance and real-world applicability by leveraging instruction data derived from authentic user interactions."}, {"title": "2.2 Large Vision Language Models", "content": "As the field of LLMs and instruction tuning undergoes rapid advancements, the academic research community is progressively focusing on integrating visual information into LLM frameworks to facilitate visual instruction tuning. This emerging area of research has witnessed the proposal of various methodologies based on the foundational work of CLIP Radford et al. [2021] and diverse LLM Zheng et al. [2023], Touvron et al. [2023b], Bai et al. [2023] architectures. Notably, LLaVA Liu et al. [2023a] pioneered the integration of an LLM with a CLIP vision encoder to construct a vision-language model, demonstrating remarkable capabilities in image-text dialogue tasks through strategies of pre-training alignment and targeted instruction tuning. Subsequent investigations have sought to refine visual instruction tuning by enhancing the quality and variety of the datasets used during the pre-training and fine-tuning phases. Building upon these advancements, recent studies, including LLaVA-v1.5 Liu et al. [2023b] and ShareGPT4V Chen et al. [2023], have achieved notable success in general vision-language comprehension, showcasing their ability to undertake complex question-answering tasks. This progression underscores the importance of sophisticated data handling and model-tuning strategies in developing effective vision-language models."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Architecture Overview", "content": "The proposed SQ-LLaVA model (see Fig. 2) consists of four main components: 1) A pre-trained vision encoder CLIP-ViT Radford et al. [2021] that extracts a sequence embedding of image tokens Z for an input image Xv; 2) A prototype extractor \u2205(\u00b7) learning visual clusters to enhance the original image tokens; 3) A trainable projection block W(\u00b7) with two linear layers to map the enhanced image tokens to the language domain tokens H, handling the dimension misalignment between the vision and language domain; and 4) Our LLM backbone f(\u00b7) implemented by the pre-trained Vicuna Zheng et al. [2023] to predict the next token upon the previous embedding sequence. Given the input question Xq and answer Xa, a word embedding matrix is used to map them to contextual embeddings Hq and Ha, and the distribution over $H^{(i+1)}$ can be obtained following the auto-regressive model as:\n\u03a1\u03bf(\u0397(i+1) | \u0397\u03c5, Hq, \u0397(1:\u03ad)) = \u03c3(f(Hv, Hq, \u0397(1:2))), (1)\nwhere @ represents all the trainable parameters in our model, \u03c3(\u00b7) is a softmax function, and f(\u00b7) outputs the last token embedding of the whole sequence. We denote pe as the prediction probability for the anticipated answer token H(i+1) at the position i + 1, conditioning on the input image token embedding H\u2081, the question token embedding Hq, and the previous answer token embeddings H(1:2). As shown in Eq. (1), the proposed SQ-LLaVA applies the language model"}, {"title": "3.2 Visual Self-questioning Instruction", "content": "In broad real-world scenarios, proactively asking a question requires more understanding and background knowledge than answering Tofade et al. [2013]. Similarly, this work proposes visual self-questioning to encourage the LLM to discover deeper vision-language alignment and improve the overall instruction-following performance. Particularly, SQ-LLaVA treats questioning as a new learning objective beyond answering, which, to the best of our knowledge, is the first practice in the field of visual instruction tuning. While the current vision-language model can ask questions 202 [2023], such skill is still learned from question-answering through instruction tuning. However, our proposed method shows that the decoder-based LLM has the potential to learn more skills, such as how to ask questions spontaneously when a unique instruction token is given (e.g., we define [vusr] in our work). Furthermore, visual self-questioning can potentially improve general vision-language understanding. To be specific, as shown in Fig. 1, there are a certain amount of questions containing more meaningful image-related information than answers in the existing visual instruction data Liu et al. [2023b], Chen et al. [2023]. Thus, we hypothesize that the vision-language understanding can be improved once the LLM learns how to predict informative questions about a given image.\nSelf-Questioning Prompt Design. We provide ground-truth content for the visual self-questioning, restricting SQ- LLaVA from asking image-related questions. To this end, we leverage questions as another learnable resource and follow the regular auto-regressive training objective. As shown in Fig. 3, the training data for SQ-LLaVA is designed in a format with a pre-defined template. To be specific, the system message as a fixed prompt is added at the beginning of each instruction data, indicating a general job description (e.g., gives helpful answers and asks complex questions) for the LLM. Existing visual instruction tuning methods utilize the unique tokens ([usr], [aswr]) to give the LLM a particular instruction (e.g., questioning understanding, answer prediction, etc.) and apply the delimiter token < od > to mark the ending position. In this work, we propose a new special token [vusr], indicating a specific instruction asking questions. Combining with the delimiter, we can construct instructions for self-questioning as a new training task.\nEach sample of current visual instruction data consists of one image X and P question-answer pairs $(X^{(1)}_{q}, X^{(1)}_{a}, ..., X^{(P)}_{q}, X^{(P)}_{a})$. We collect one question $X^{(j)}_{q}$ and its answer $X^{(j)}_{a}$ with special tokens to construct the jth turn conversation as\n$X_{i} =\\begin{cases}\n([usr], X^{(1)}_{q} \\langle od\\rangle, X^{(1)}_{a} \\langle od\\rangle) & j = 1 or j > 1, R < \u03b4 \\\\\n([vusr], X^{(j)}_{q} \\langle od\\rangle [aswr], X^{(j)}_{a} \\langle od\\rangle) & j > 1, R > \u03b4\n\\end{cases},(2)$"}, {"title": "3.3 Enhanced Visual Representation", "content": "Unlike previous visual instruction tuning methods, SQ-LLaVA jointly benefits from visual self-questioning and question- answering. For better visual self-questioning, we develop a prototype extractor that recognizes and groups similar patterns of visual information from the latent space. Our primary goal is to enhance visual representation through prototype learning.\nSpecifically, we utilize clustering to extract centroid representations of image tokens Zv, where each cluster center is treated as a prototype, which, in return, will be distributed to each of the original image token embeddings. Our proposed prototype extractor \u2205(\u00b7) is a lightweight design involving two parts: 1) cluster center optimization and 2) prototype information distribution. Following Ikotun et al. [2023], Vattani [2009], we randomly initialize K = 256 cluster centers C and deploy the iterative Expectation-Maximization (EM) clustering process to capture representative semantics in the latent space by\nE-step : $M^{(t)} = \u03c3(q(C^{(t)}) *k(Z_{v})^T)$,\nM-step : $C^{(t+1)} = M^{(t)} * v(Z_{v}),(3)$\nwhere M(t) \u2208 [0,1] denotes a soft cluster assignment map at the tth iteration, \u03c3(\u00b7) is a softmax function, and t\u2208 {1,\u2026\u2026,T} indexes the iteration of EM step with T = 2 in this work. Three trainable linear layers q, k, and v are used in (3.3), where q(\u00b7) projects the prototype C to a query vector, and k(\u00b7) and v(\u00b7) project H\u2082 into key and value vectors, followed by a normalization layer, respectively. The prototype extractor iteratively updates cluster map M and centers C.\nAfter the prototype extraction, we train a linear layer z(\u00b7) to adaptively map the visual cluster information to the raw image embedding Z. For the ith token embedding Z(\u00b9), we update it as\n$z(Z^{(i)}_{v}) = Z^{(i)}_{v} + z(\\frac{1}{K} \\sum_{j=1}^{K} S_{c}(C_{j}, Z^{(i)}_{v}) \\times C_{j}),(4)$\nwhere Sc(\u00b7,\u00b7) is a normalized cosine similarity function. The weighted sum over prototypes in Eq. (4) emerges as an indispensable step for contextual understanding from image tokens, recognizing and grouping similar patterns and semantics. It clusters image tokens as prototypes that display homogeneity in semantics, such as \u201cgrass\u201d and \u201cdog\". The prototypes can describe the intrinsic semantic meanings by aggregating entities that exhibit shared attributes. Finally, we map the image sequence embedding Z to language domain H\u2082 with a two-layer linear projector W(.).\""}, {"title": "3.4 Model Training", "content": ""}, {"title": "3.4.1 Stage1: Pre-training for Vision-Language Alignment.", "content": "Unlike text-only LLMs, the general-purpose vision language model also fine-tunes LLM using image tokens as input (see Fig. 2). Therefore, the pre-training stage aims to optimize the LLM by explicitly executing the visual instruction. The proposed SQ-LLaVA adopts Vicuna Zheng et al. [2023] as its instruction LLM, pre-training on massive text corpora to predict the next text token given the previous context, not only containing text but also visual instructions. To achieve this, we organize the pre-training data as DPT = {[$X^{(1)}_{v}, X^{(1)}_{a}$],..., [$X^{(N)}_{v}, X^{(N)}_{a}$]}, where N is the total number of training samples, and each sample has an image and its related descriptions. Each image and text input pair will be mapped to sequence embeddings (H\u2082 and Ha) as elaborated in Section 3.1. During pre-training, we freeze the vision encoder and LLM and mainly train the prototype extractor \u2205 and the vision-to-language projector W. The pre-training goal is to maximize the probability of the predicted image description Ha given an image H. When training a visual instructional LLM, we follow the negative log-likelihood objective function as\n$\\sum_{v,a\u2208DPT}-logpo(Ha | H\u2084) = \\sum_{v,a\u2208DPT}\\sum_{i=1}^{L}-logpo (H^{(i+1)}_{a} | \u0397\u03c5, H^{(1:i)}_{a}),(5)$\nwhere L denotes the sequence length of answer tokens in Ha, \u03b8 is the total trainable parameter of \u2205 and W, p(Ha | Hv) can be computed by Eq. (1), and $H_{1:\u2170)}^{a}$ represents all the answer tokens before the current prediction $H^{(i+1)}_{a}$."}, {"title": "3.4.2 Stage2: Fine-tuning.", "content": "Existing methods, such as LLaVA Liu et al. [2023a], Chen et al. [2023], mainly update the vision-to-language projector (usually a couple of linear layers) and the language model during fine-tuning. Nevertheless, the projector might be too weak to capture the relationship between the image and the questions. Following the previous multi-modal understanding method Radford et al. [2021], we unfreeze the vision encoder and LLM during fine-tuning for a joint optimization further to eliminate the gap between the vision and language domain.\nTo mitigate the heavy computational overhead, we take advantage of LoRA Hu et al. [2022] as a lightweight tuning option that can achieve similar (even better) performance to fully fine-tuning when training large models on a relatively small dataset. We add LoRA in both the vision encoder and LLM. Thus, the learnable parameters @ of the proposed SQ-LLaVA during fine-tuning represent a combination of all the parameters of LLM-LORA, ViT-LoRA, prototype extractor \u2205, and the vision-to-language projector W. Given the instruction tuning data Dir = {[$X^{(1)}_{v}, X^{(1)}_{a}$], ..., [$X^{(N)}_{v}, X^{(N)}_{a}$]}, we take the conversational data X and the image X as input, mapping them to sequence embedding (He and H\u2082) as elaborated in Section 3.1, and minimize the negative log-likelihood loss for the self-questioning and answering tasks as follows\nSelf-questioning: $\\sum_{v,C\u2208DIT}\u2212logp\u03b8(H^{(i+1)}_{q} | H\u03c5, H^{(1:i)}_{q}),(6)$\nAnswering : $\\sum_{v,C\u2208DIT}-logpe (H^{(i+1)}_{a} | H\u03c5, Hq, H^{(1:1)}, H^{(i+1)}_{i}),(7)$\nwhere j\u2208 {1,\u2026\u2026,P}, indicating the index of question or answer within the conversational data X(*). Notably, previous works Liu et al. [2023b], Chen et al. [2023], Zhu et al. [2024] only involve answering tasks, but we introduce visual self-questioning as an additional training task for visual instruction tuning. Eventually, SQ-LLaVA, as a vision- language assistant, not only executes human instructions by optimizing the objective function in Eq. (7) but can raise questions out of the given image after optimizing the Eq. (6), potentially yielding more diverse question-answer guidance and improving multi-modal understanding."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setting", "content": ""}, {"title": "4.1.1 Dataset.", "content": "Our work uses the open-source visual instruction dataset provided by LLaVA Liu et al. [2023b] and ShareGPT4V Chen et al. [2023] for training. Each dataset has large-scale image-text paired data for pre-training and instruction-following"}, {"title": "4.1.2 Compared Methods.", "content": "In this work, we choose baseline models with similar architecture. Precisely, each model consists of a vision encoder and an LLM with different model size and pre-trained weights. For both image description and question answering tasks, we adopt InstructBLIP Dai et al. [2023], Qwen-VL Bai et al. [2023], LLaVA-v1.5 Liu et al. [2023b], and ShareGPT4V Chen et al. [2023] as our baseline models, where all methods were trained on visual instruction data. Besides, we also incorporate ClipCap Mokady [2021] and DiscriTune Dess\u00ec et al. [2023], which are trained on regular image-text pair data."}, {"title": "4.1.3 Implementation.", "content": "We adopt Vicuna Zheng et al. [2023] as the pre-trained language generative model and CLIP-ViT Radford et al. [2021] as the vision encoder. We pre-train the prototype extractor and the vision-to-language projector using AdamW Loshchilov and Hutter [2019] optimizer with a learning rate of 2 \u00d7 10-3 and a constant scheduler for one epoch. Following previous work Liu et al. [2023b], we keep the global batch size as 256 for pre-training and 128 for fine-tuning. During"}, {"title": "4.3 Visual Information Discovery", "content": "In this experiment, we showcase the diversity and reliability of the proposed SQ-LLaVA through various qualitative applications, including detailed image description, visual information summary, and visual self-questioning. We also present quantitative results on the task of image captioning.\nAbilities of SQ-LLaVA Through Qualitative Samples. SQ-LLaVA demonstrates many advanced abilities compared to traditional vision language models. For example, SQ-LLaVA can effectively eliminate object hallucination Li et al. [2023], Rohrbach et al. [2018], delivering the predictions with better trustworthiness. Besides, SQ-LLaVA can generate diverse and meaningful questions about a given image without human textual instructions."}, {"title": "4.4 Ablation Study", "content": "In Table 3, we conduct experiments with different architecture designs and training strategies on five question-answering benchmarks. For a fair comparison, we train the baseline models on our local machine with the same training recipe of LLaVA-LORA Liu et al. [2023b]. Specifically, we present our full model and three ablated models by removing one component each time, where all models are in 7B scale. We adopt the dataset Liu et al. [2023b] with 558k for pre-training (PT) and 665k for fine-tuning (FT). As compared, self-questioning (SQ) brings a consistent performance boost on all the five benchmarks, indicating the effectiveness of visual self-questioning on improving visual language understanding. Besides, we introduce the prototype extractor (Proto) to enhance visual representation, achieving 0.9% improvement in average accuracy among five benchmark. With all three components incorporated, we observe a 2.4% improvement in average accuracy.\nAs shown by the bottom block of Table 3, we conduct experiments with the same ablation settings but with a larger scale of the visual instruction data Chen et al. [2023] (i.e., for both PT and IT). Overall, SQ-LLaVA achieves 2.4% improvement over the baseline model after training on the smaller dataset and achieves 3.0% improvement after training on the larger dataset. We also provide ablation study of SQ-LLaVA in 13B scale (see Appendix A.1 for detail)."}, {"title": "5 Conclusion", "content": "This work introduced SQ-LLaVA, a method to improve general-purpose vision language understanding and image- oriented question answering through the proposed visual self-questioning technique. Our experiments reveal that SQ-LLaVA achieved superior performance with fewer training parameters. We performed a comprehensive study on visual instruction tuning tasks and found that SQ-LLaVA can generalize well to a wide range of unseen tasks and outperform state-of-the-art methods. Notably, qualitative assessments demonstrated that SQ-LLaVA allows enhanced visual representation and domain alignment capability, effectively alleviating the issue of object hallucination and improving the semantic interpretation of images. Our findings underscored the potential of visual self-questioning as a potent training strategy within the field of visual instruction tuning, paving the way for more efficient and effective model training approaches. SQ-LLaVA is a foundational step toward enhancing general vision language understanding by framing question-asking as an intrinsic goal of the question-answering process. We hope the proposed SQ-LLaVA can encourage future endeavors in exploiting questions in visual instruction tuning by following the proposed self-questioning technique."}, {"title": "A Zero-shot Multilingual Capability", "content": ""}, {"title": "A.1 Ablation Study", "content": "In Table 4, we conduct experiments with different architecture designs and training strategies on five benchmarks for SQ-LLaVA-13B. For a fair comparison, we train the baseline models on our local machine with the same training recipe of LLaVA-LORA Liu et al. [2023b]. Specifically, we present the baseline model, our full model, and three ablated models by removing one component each time. We adopt the dataset Liu et al. [2023b] with 558k for pre-training (PT) and 665k for fine-tuning (FT). As compared, self-questioning (SQ) achieves 0.6% improvement in average accuracy, indicating the effectiveness of visual self-questioning in improving visual language understanding. Besides, we introduce the prototype extractor (Proto) to enhance visual representation, achieving 0.6% improvement in average accuracy. With all three components incorporated, we observe a 2.5% improvement in average accuracy among five benchmarks.\nAs shown by the bottom block of Table 4, we conduct experiments with the same ablation settings but with a larger scale of the visual instruction data Chen et al. [2023] (i.e., for both PT and IT). Overall, SQ-LLaVA achieves 2.5% improvement over the baseline model after training on the smaller dataset and achieves 3.2% improvement after training on the larger dataset.\nIn Table 4, we observe that the proposed SQ-LLaVA achieves significant improvement over the baseline models on the LLaVA (in-the-wild) benchmark by 7.3% and 5.6%. This indicates SQ-LLaVA's capability in more challenging tasks, generalizability to novel domains, and robustness to different prompts."}, {"title": "A.2 Evaluation on An Additional Benchmark", "content": "We evaluate SQ-LLaVA on one recent benchmark MME Fu et al. [2023]. MME is designed to assess the performance of vision-language models across a wide range of tasks that require both perception and cognition abilities. In Table 5, we provide a comparison between SQ-LLaVA with other methods. As can be seen, SQ-LLaVA achieves the highest"}, {"title": "B Qualitative Analysis of SQ-LLaVA", "content": ""}, {"title": "B.1 Reliable Detailed Image Description", "content": "The detailed image description examples demonstrate that SQ-LLaVA can capture low-level visual information within the image, such as \u201cKL8-Q17\" in Fig. 6 and \"number 186 206-7\" in Fig. 7. Also, SQ-LLaVA demonstrates a better reliability. In Fig. 7, the image shows a complex environment with various objects, which makes it easy for the models to generate wrong descriptions such as \u201ctotal five people", "faces blurred": "ShareGPT4V). In contrast, SQ-LLaVA describes uncertainty information with a prefix hedge word such as \u201clikely\u201d, \u201cpossibly\u201d, and \u201cperhaps"}, {"title": "B.2 Visual Self-questioning", "content": "In Fig. 10, Fig. 11, and Fig. 12, we show visual self-questioning and answering of SQ-LLaVA. As can be seen, SQ- LLaVA asks meaningful questions based on comprehensive visual understanding. Specifically, the self-asked questions always involve one or more main objects within the image and a relatively divergent problem, where the main objects rely on visual perception, and the meaningful problem requires vision-language alignment. Unlike question-answering with direct instruction, self-questioning encourages the model to discover all possible combinations between visual and semantic information."}]}