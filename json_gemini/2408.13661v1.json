{"title": "Hierarchical Network Fusion for Multi-Modal Electron Micrograph Representation Learning with Foundational Large Language Models", "authors": ["Sakhinana Sagar Srinivas", "Geethan Sannidhi", "Venkataramana Runkana"], "abstract": "Characterizing materials with electron micrographs is a crucial task in fields such\nas semiconductors and quantum materials. The complex hierarchical structure\nof micrographs often poses challenges for traditional classification methods. In\nthis study, we propose an innovative backbone architecture for analyzing electron\nmicrographs. We create multi-modal representations of the micrographs by tok-\nenizing them into patch sequences and, additionally, representing them as vision\ngraphs, commonly referred to as patch attributed graphs. We introduce the Hier-\narchical Network Fusion (HNF), a multi-layered network structure architecture\nthat facilitates information exchange between the multi-modal representations and\nknowledge integration across different patch resolutions. Furthermore, we leverage\nlarge language models (LLMs) to generate detailed technical descriptions of nano-\nmaterials as auxiliary information to assist in the downstream task. We utilize a\ncross-modal attention mechanism for knowledge fusion across cross-domain repre-\nsentations(both image-based and linguistic insights) to predict the nanomaterial\ncategory. This multi-faceted approach promises a more comprehensive and accu-\nrate representation and classification of micrographs for nanomaterial identification.\nOur framework outperforms traditional methods, overcoming challenges posed by\ndistributional shifts, and facilitating high-throughput screening.", "sections": [{"title": "Introduction", "content": "Semiconductors are the foundation of modern electronics, driving advancements in computing, com-\nmunication systems, transportation systems, and space exploration. The precise design, development,\nand testing of semiconductor devices is essential for ensuring the reliability, durability, and perfor-\nmance of high-tech chips. Advanced imaging and analysis techniques[55] are key to fabricating\nand integrating nanoscale components and enabling advanced inspection, which is essential for the\ndevelopment of next-generation miniaturized semiconductor devices[13], with sizes now reaching\nas small as 7 nm or even smaller. However, the increased complexity of producing chips under 7\nnanometers introduces greater potential for error, jeopardizing the consistency of high-quality chip\nproduction and magnifying variability in chip performance. The semiconductor industry utilizes\nvarious electron beam tools, including scanning and transmission electron microscopy, to create\nhigh-resolution images of these devices. These images, known as electron micrographs, reveal the\ncomplex microstructures of materials, which are crucial for the accurate design and evaluation of\nsemiconductor devices. The fabrication of nanoscale components is a challenging task that requires\nprecise control over the manufacturing process. Furthermore, these images facilitate monitoring of\nthe process and defect detection, enabling subsequent process optimization or design adjustments\nto mitigate defects. The autolabeling of electron micrographs for nanomaterial identification, while\nadvantageous, remains a significant challenge. Figure 1 shows the challenges in nanomaterial iden-\ntification tasks. This is largely attributed to distributional shifts such as manufacturing variations\nor material property changes, exacerbated by high intra-class dissimilarity within nanomaterials,\nhigh inter-class similarity between different nanomaterials, and the existence of visual patterns at"}, {"title": "Problem Statment", "content": "In this study, the focus is on the electron micrograph classification task, a type of inductive learning\ntask where the objective is to assign labels to new, unseen micrographs utilizing a labeled dataset"}, {"title": "Proposed Method", "content": null}, {"title": "Formalism", "content": "Let's consider an input electron micrograph denoted by $I^{hwc}$, which has dimensions of $h \\times w \\times c$, where\n$h, w,$ and $c$ represent the height, width, and number of channels of the micrograph, respectively. We\ndivide the micrograph into a grid of patches, each having dimensions of $p \\times p \\times c$, with $p$ representing\nthe patch size. The number of patches along each spatial dimension is given by $n = hw/p^{2}$.\nSubsequently, we reshape the 3D micrograph into a 2D patch tensor, denoted as $I' \\in \\mathbb{R}^{n\\times(p^{2}c)}$.\nThese patches are linearly transformed to create a new tensor, $I' \\in \\mathbb{R}^{n\\times d}$, where $d$ is the patch\nembedding dimension. To account for the position of each patch within the micrograph, we introduce\nposition embeddings represented by a matrix $E_{pos} \\in \\mathbb{R}^{n\\times d_{pos}}$, where $d_{pos}$ denotes the position"}, {"title": "Hierarchical Network Fusion(HNF)", "content": "We tokenize electron micrographs by dividing them into grid-like patches. This approach yields two\ncomplementary representations of micrographs: (a) We represent an electron micrograph as a vision\ngraph, where patches are connected by edges that represent pairwise visual similarity constructed\nusing a nearest-neighbor graph technique. The vision graph captures local patch relationships and\nutilizes graph-structural priors to analyze pairwise spatial dependencies within the micrograph. (b)\nAdditionally, we represent electron micrographs as a patch sequence, capturing pairwise spatial\ndependencies beyond the original sparse graph structure between different patches within a micro-\ngraph. Representing electron micrographs as both patch sequences and vision graphs serves distinct\npurposes in their respective contexts. We append a classification token (<cls>) to a patch sequence\nto obtain an embedding of the entire patch sequence that captures global information. We augment\neach vision graph by introducing a virtual node that is bidirectionally connected to all the other nodes\nin the graph through virtual edges. These virtual edges represent the pairwise relations between each\nreal node and the virtual node. The virtual node embedding captures the long-range dependencies\nbetween nodes by considering the global information of the vision graph. We hypothesize that\nelectron micrographs exhibit hierarchical dependencies among patches, which can be captured using\nmultiple patch sequences or vision graph structures at different spatial resolutions of the patches.\nWe present Hierarchical network fusion (HNF), a cascading network architecture that constructs a\nmulti-scale representation of an electron micrograph by creating a series of patch sequences and\nvision graphs at multiple scales of patch sizes with increasing resolutions."}, {"title": "Beyond Conventional Analysis: Leveraging LLMs for Nanomaterial Characterization", "content": "The advent of large pre-trained language models (LLMs), such as OpenAI's ChatGPT [10], Google's\nPaLM [25], and Meta's LLaMA [93], has significantly revolutionized performance in various natural\nlanguage processing tasks, achieving state-of-the-art results across a wide range of applications.\nIn contrast, small-scale language models (LMs), such as BERT [30] and DeBERTa [52], lack the\nstrong logical reasoning capabilities of LLMs and are limited in their ability to generate coherent\nand contextually relevant responses compared to larger models. However, small-scale LMs are\ncomputationally affordable for fine-tuning using labeled data for specialized task adaptation. In\naddition, they allow access to logits or token embeddings for downstream applications of smaller\nLMs across various tasks, aiding in explainability. Owing to their substantial model complexity and\nscale, general-purpose LLMs require significant computational resources for repurposing through\nfine-tuning for task-specific customization. Additionally, they do not provide access to latent token\nembeddings and logits, this black-box nature can limit the interpretability of LLMs. To overcome the\nchallenges, the Language Modeling as a Service (LMaaS [83]) platform provides access to LLMs\nvia text-based API interaction through cloud-based services. However, the integration of LLMs with\nvision graphs remains an underexplored area, opening up the possibility for innovative techniques\nthat combine language models and graph representation learning algorithms to improve nanomaterial\nidentification applications. To address this, our approach capitalizes on zero-shot chain-of-thought\n(Zero-Shot CoT) prompting of LLMs to generate technical descriptions of nanomaterials. We pre-\ntrain smaller LMs on the generated textual descriptions using the masked language modeling (MLM)\ntechnique (i.e., pre-training for domain-customization) to learn expressive token embeddings for\na better understanding of language structure and semantics. We then fine-tune smaller LMs for\ndownstream supervised multi-class classification task (i.e., fine-tuning for task adaptation) to compute\ncontext-aware token embeddings. We employ weighted sum-pooling attention mechanisms to obtain\ncontextualized text-level embeddings from token embeddings, which are used to perform inference\nin the nanomaterial identification task. Our work evaluates two LLMs: GPT-3.5-turbo, and Google\nBARD\u00b3. GPT-3.5-turbo, a newer and larger extension of GPT-3.5 model from OpenAI, excels in\nvarious language tasks and shows cost-effectiveness, while Google BARD is significantly larger than\nGPT-3.5 models. We also utilize a pre-trained small-scale LM, DeBERTa\u2074[52], which is an improved\nversion of the BERT architecture. The technical details of these language models are given in Table 1."}, {"title": "Formalism", "content": "by LLMs (denoted as $S_{expl}$) into the $LM_{expl}$ model, which then generates expressive, context-aware\nembeddings for each token in the sentence, capturing the semantic relationships between the tokens\nas follows:\n$h_{expl} = LM_{expl}(S_{expl})$\nwhere the context-aware embeddings are denoted as $h_{expl} \\in \\mathbb{R}^{m\\times d}$, where m represents the number\nof tokens in $S_{expl}$ and d is token embedding dimension. We then perform sum-pooling attention\nmechanism to compute a weighted sum of these token embeddings to encode the textual explanations\nto obtain an text-level fixed-length embedding as follows:\n$a_{i} = softmax(q_{i}); q_{i} = u^{T}h_{expl}^{(i)}$\n$h_{text} = \\Sigma_{i=0}^{m} a_{i}h_{expl}^{(i)}$\nwhere u is a differentiable vector. The text-level embedding $h_{text} \\in \\mathbb{R}^{d}$ captures the essence or core\nof the domain knowledge as a whole, extracted from the foundational LLMs for each nanomaterial.\nWe calculate the relevance score between the text-level embedding($h_{text}$) and the electron micrograph\nrepresentations($h_{fus}$) obtained from the hierarchical network fusion(HNF, refer to section 3.2), as\ndetailed below,\n$\\beta^{*} = arg max[softmax(q_{khfus})]; q_{k} = v^{T}[h'_{text}|| . ||h_{text}]$\nwhere the subscript, c denotes the the total number of nanomaterial categories and v is a trainable\nparameter. The above operator computes the list of scores or probabilities for each nanomaterial,\nand the arg max operator selects the nanomaterial for which the probability score is maximized. We\nthen select the appropriate/relevant nanomaterial text-level embedding conditioned on hierarchical\nembedding ($h_{fus}$) as follows:\n$h_{ext} = h_{ext}^{\\beta^{*}}$\n$\\beta^{*}$ denotes the nanomaterial label with the highest probability. This is essentially a matching mecha-\nnism that tries to find the best pairwise alignment among the various nanomaterial text-level embed-\ndings ($h_{ext},..., h_{text}^{c}$) and the hierarchical embedding ($h_{fus}$) obtained from the hierarchical network\nfusion (HNF). We utilize backpropagation error in the downstream supervised multi-classification\ntask to fine-tune the smaller LMs to maximize the pairwise alignment between the complementary\nhierarchical embedding ($h_{fus}$) and its corresponding text-level embedding $h_{est}$. To put it briefly, $h_{ext}$\nincorporates the expert knowledge obtained from foundational LLMs for the appropriate nanomaterial\nunderlying the electron micrographs."}, {"title": "Overall Method", "content": "Figure 2 provides an overview of the \"MultiFusion-LLM\" framework. Our proposed framework\ncomprises three distinct methods: a) Hierarchical Network Fusion (HNF) tokenizes micrographs\ninto patches to obtain patch sequences and construct vision graphs. It introduces a <cls> token\ninto the patch sequence and a virtual node for the vision graph to capture global characteristics. The\nnetwork has a multi-layered structure; each layer of the network consists of bidirectional Neural ODEs\nand graph Chebyshev networks, and regulates the information flow through a gating mechanism to\nlearn hierarchical embeddings with increasing patch sizes across each layer. It computes cross-modal\nembeddings, denoted as $h_{fus}$, by integrating embeddings between modalities at different patch\nresolutions, thereby facilitating the exchange of information and integration of knowledge. For more\ndetailed information, please refer to section 3.2. b) LLMs for Incorporating Domain Knowledge:\nWe generate technical descriptions of nanomaterials, capturing a wide range of information including\nstructure, properties, and applications using Zero-Shot CoT prompting of LLMs. To illustrate, Table\n8 provides a glimpse of the LLM-retrieved text obtained from GPT-3.5 turbo, specifically generated\nto address natural language queries regarding MEMS devices. Initially, we pre-train a smaller LM\non the generated descriptions through masked language modeling (MLM). Later, we fine-tune this\nsmall-scale LM on a downstream supervised task to encapsulate the generated explanations. We\nthen utilize the weighted sum-pooling attention mechanism to compute domain-specific knowledge-\nincorporated text-level embeddings, denoted as $t_{exts}$. For additional details, please refer to subsection\n3.3. (c) We employ the multi-head attention mechanism (MHA)[95] to fuse text-level embeddings\n$h_{ext}$ with hierarchical embeddings $h_{fus}$, enabling the capture of contextually relevant information\nand achieving semantic alignment across different cross-domain embeddings. Simultaneously, by\nfocusing on and aligning high-level textual descriptions (text-level embeddings) with detailed visual\nrepresentations (hierarchical embeddings), we ensure a comprehensive understanding and analysis of\nelectron micrographs from both descriptive and visual perspectives. This approach helps mitigate\nthe inherent limitations arising from high intra-class dissimilarity, high inter-class similarity, and\nspatial heterogeneity in visual patterns across the electron micrographs, ultimately enhancing the\nperformance of nanomaterial identification tasks. We compute the Query, Key, Value projections for"}, {"title": "and", "content": "the text-level embedding $h_{ext}^{text}$ for each head h as follows:\n$Q_{ext}^{h} = h_{texts}W_{ext}^{Q}; K_{ext}^{h} = h_{text}^{text}W_{ext}^{K}; V_{ext}^{h} = h_{text}^{text}W_{ext}^{V}$   \nSimilarly, the Query, Key, Value projections for hierarchical embedding $h_{fus}$ for each head h as\nfollows:\n$Q_{fus}^{h} = h_{fus}W_{fus}^{Q}; K_{fus}^{h} = h_{fus}W_{fus}^{K}; V_{fus}^{h} = h_{fus}W_{fus}^{V}$\nWe concatenate keys and values of text-level and hierarchical embeddings to create a unified repre-\nsentation.\n$K_{concat}^{h} = [K_{ext}^{h}, K_{fus}^{h}]; V_{concat}^{h} = [V_{ext}^{h}, V_{fus}^{h}]$\nWe apply Softmax attention to integrate complementary information from the cross-domain embed-\ndings, focusing on relevant information and aligning them semantically.\n$A_{cross}^{h} = Softmax(\\frac{(Q_{ext}^{h}+Q_{fus}^{h}) K_{concat}^{h}}{\\sqrt{d_{h}}})$\nEach head outputs a new vector representation that highlights the most relevant features in the\nmono-domain embeddings, tailored to specific aspects of the data.\n$O_{cross}^{h} = A_{cross}^{h} V_{concat}^{h}$\nFinally, we concatenate and linearly transform all head-specific outputs to create the final unified\ncross-modal embedding.\n$O_{concat} = [O_{cross}^{1}, O_{cross}^{2},..., O_{cross}^{H}]$\n$y = O_{concat}W_{cross}$\n$p_{i} = softmax(W_{ycross})$\nwhere $W_{ext}^{Q}, W_{ext}^{K}, W_{ext}^{V}, W_{fus}^{Q}, W_{fus}^{V}, W_{ext}^{ext}, W_{cross}$ and W are the trainable weight matrices. $d_{h}$\nrepresents the dimensionality of the key/query/value for each head, and H is the number of heads. $p_{i}$\nrepresents the probability distribution across nanomaterial categories, we apply the argmax operation\nto $p_{i}$ to determine the framework's predictions for the nanomaterial category. In summary, we conduct\nZero-shot CoT prompting of LLMs to generate technical descriptions of nanomaterials and pre-train\nsmall-scale LMs using masked language modeling (MLM). Next, we jointly optimize the smaller\npre-trained LM and the hierarchical network fusion (HNF) method on supervised learning tasks.\nThe objective is to minimize the cross-entropy loss and enhance multi-class classification accuracy.\nIn summary, the MHA offers a multi-faceted approach to capture and align varied information\nsources, making it a powerful tool for multi-modal data integration and analysis. It allows for a\nrobust, synergistic, and comprehensive representation of data, especially in contexts like nanomaterial\nanalysis where both modalities offer complementary insights."}, {"title": "Experiments And Results", "content": null}, {"title": "Datasets", "content": "Our study primarily utilized the SEM dataset[4] to automate nanomaterial identification. The expert-\nannotated dataset spans across 10 distinct categories, representing a broad range of nanomaterials\nsuch as particles, nanowires, patterned surfaces, among others. In total, it contains approximately\n21,283 electron micrographs. Figure 4 provides a visual representation of the different nanomaterial\ncategories included in the SEM dataset. Despite the initial findings by [74] on a subset of the\noriginal dataset, our research was based on the complete dataset since the subset was not publicly\naccessible. Although the original dataset curators, [4], did not provide predefined splits for training,\nvalidation, and testing, we utilized the k-fold cross-validation method to evaluate our framework's\nperformance. This strategy facilitated a fair comparison with popular baseline models in a competitive\nbenchmark setting. Furthermore, we extended our evaluation by leveraging several open-source\nmaterial benchmark datasets relevant to our study. These datasets were used to showcase the efficacy\nof our proposed framework and its applicability in a broader context beyond the SEM dataset."}, {"title": "Results", "content": "We evaluated the effectiveness of our proposed framework through a comprehensive performance\nanalysis, comparing it to commonly used computer vision baseline models. Our comparisons included\nsupervised learning models such as ConvNets and ViTs (as referenced in [2, 1]), along with self-\nsupervised learning techniques like Vision Contrastive Learning (VCL, as discussed in [34]). Table\n2 reports the experimental results from our study. To ensure a fair and rigorous comparison, we\nconducted experiments with consistent settings across all algorithms, measuring performance using\nthe Top-N accuracy metric and evaluating specifically for N \u2208 {1,2,3,5}. Our proposed framework\noutperforms the baseline models, showing a substantial relative improvement of 25.8% in the Top-1\nscore and a marginal improvement of 5.34% in the Top-5 score compared to the next-best baseline\nmodel, T2TViT ([110])."}, {"title": "Conclusion", "content": "To conclude, we have conducted the first in-depth study aimed at achieving state-of-the-art perfor-\nmance in nanomaterial characterization. This study introduces the innovative MultiFusion-LLM\nframework, a robust solution to the challenges associated with nanomaterial identification in electron\nmicrographs. By synergistically integrating multi-modal representations and leveraging the analytical\nprowess of large language models, it promises more nuanced and accurate classification. Our compre-\nhensive framework has outperformed traditional methods, showcasing cutting-edge performance on\ncost-efficient GPU hardware. Furthermore, it has demonstrated effectiveness and computational effi-\nciency, particularly with large datasets, thereby accelerating high-throughput screening and advancing\nresearch holding implications for the advancement of the semiconductor industries."}, {"title": "Technical Appendix", "content": "Table 3 presents experimental findings comparing the proposed framework's performance to various\nsupervised learning-based baseline models, including several GNN architectures ([79, 38]), and we\nuse Graph Contrastive Learning (GCL, [114]) algorithms for additional comparison. Our proposed\nframework achieves SOTA performance on the benchmark dataset [4] compared to the baselines."}, {"title": "Experimental Setup", "content": "The SEM dataset[4] consists of electron micrographs with dimensions of 1024 \u00d7 768 \u00d7 3 pixels.\nTo facilitate our analysis, we downscale these micrographs to 224 \u00d7 224 \u00d7 3 pixels. As part of the\ndata preprocessing, we normalize the electron micrographs by adjusting the mean and covariance\nto achieve a value of 0.5 across all channels. This normalization results in the micrographs falling\nwithin the range of [-1, 1]. We tokenize the downscaled and normalized micrographs into discrete,\nnon-overlapping patches. Subsequently, we represent the electron micrographs as patch sequences\nand construct vision graphs using the Top-K nearest neighbor search algorithm. Specifically, we\nset the value of K to 10, 6, and 4 for each layer in the hierarchical network fusion (HNF) method,\nresulting in a total of three layers. This process generates multi-scale vision graphs and patch\nsequences with patch resolutions increasing of 16, 28, and 32 pixels. The patch dimension (dpos)\nand position embedding dimension (d) are both set to 64. The framework is evaluated using a\n10-fold cross-validation strategy and trained for 50 epochs with an initial learning rate of 1e-3 and a\nbatch size of 48. We have a few more hyperparameters set for the cross-modal attention layer with\nthe number of attention heads(H) to 4, and the dimensionality of Key/Query/Value ($d_{h}$) is 16. To\nenhance the performance of the MultiFusion-LLM framework, we employ two key strategies: (a)\nearly stopping on the validation set, which halts training when the framework's performance on the\nvalidation data plateaus to prevent overfitting; and (b) a learning rate scheduler that systematically\nreduces the learning rate by half if the validation loss stagnates for five consecutive epochs. Reducing\nthe learning rate can help the framework converge to a better solution and avoid overfitting. In\naddition, we utilize the Adam optimization algorithm [62] to update the trainable parameters of\nthe framework. Our proposed framework enhances the accuracy of multi-class classification tasks\nby seamlessly integrating both large language models (LLMs) and small-scale language models\n(LMs). The framework fully leverages the capabilities of LLMs in generating technical descriptions\nof nanomaterials, an approach that can significantly exploit domain-specific linguistic insights critical\nfor nanomaterial identification tasks. The framework interacts with off-the-shelf LLMs through a\nLanguage Model as a Service (LaMaaS) platform through the text-based API interactions. In this\nstudy, we utilized GPT-3.5-turbo and Google Bard as representative LLMs. The hyperparameters\nfor our framework were not individually fine-tuned for each LLM. Instead, they were consistently\napplied across all LLMs. This method underscores our framework's generality, ease of use, and"}, {"title": "Ablation Study", "content": "Figure 5 illustrates the overview of the framework. Our proposed framework comprises three dis-\ntinct methods: (a) The Hierarchical Network Fusion (HNF) is a multi-layered, cascading network\narchitecture designed to enhance the classification accuracy of electron micrographs. It integrates\ntwo complementary representations at multiple layers: (a) patch sequences, which assist in capturing\nspatial dependencies among patches beyond pairwise dependencies, and (b) vision graphs, which\ncapture the local pairwise patch relationships. These techniques provide a detailed multi-scale rep-\nresentation of the micrographs, encapsulating both fine-grained and coarse-grained details. HNF\nuses an inverted pyramid structure, incorporating increasing patch sizes at each layer, and utilizes\nbidirectional Neural ODEs and Graph chebyshev convolution(GCC) networks for iterative patch\nembeddings refinement and the computation of the optimal node-level embeddings, respectively.\nA mixture-of-experts technique further optimizes the integration of these cross-domain modalities,\nfostering efficient knowledge exchange and improving classification accuracy by effectively modeling\nstructural, semantic, and causal information from both techniques. (b) Using Zero-shot CoT prompt-\ning with LLMs, we generate detailed technical descriptions of nanomaterials. We pre-train smaller\nLMs using masked language modeling (MLM) on these descriptions to facilitate domain-specific\ncustomization. These pre-trained LMs are then fine-tuned for task-specific adaptation to generate\ncontextualized token embeddings. We apply a sum-pooling attention mechanism to obtain text-level\nembeddings from these token embeddings, thereby capturing the vast domain-specific knowledge\nembedded in the generated textual descriptions. (c) We use the cross-modal multi-head attention\nmechanism to integrate and align information from different modalities specifically, from hierar-\nchical network fusion (HNF) and language models into a coherent and unified representation that\ncaptures complex, hierarchical, and potentially cross-modal patterns, emphasizing relevant features\nto enhance the accuracy of the multi-class classification task."}, {"title": "An In-Depth Empirical Insights into Nanomaterial Classification", "content": "We have conducted additional experiments to gauge the efficacy of our framework, which sheds\nlight on its ability to categorize electron micrographs across various nanomaterial categories. The\nexperimental results, presented in Table 5, demonstrate that our proposed framework can generalize to\na wide range of nanomaterials, including those with complex patterns. We evaluated the performance\nof our framework using the SEM dataset[4], employing standard metrics such as precision (P in\n%), recall (R in %), and F1-score (F1 in %). We adopt a multi-metric approach to ensure a fair\nand thorough comparison with baseline models. To facilitate this, we utilize a confusion matrix\nencompassing various metrics for multi-class classification. This confusion matrix aids in scrutinizing\nour framework's performance by offering insights into how it categorizes electron micrographs across\ndifferent nanomaterial categories. The metrics included in the confusion matrix are as follows: True\nPositives (TP) represent micrographs that are correctly classified as belonging to a specific category.\nFalse Negatives (FN) represent micrographs that actually belong to a category but are incorrectly\nclassified or missed. True Negatives (TN) represent micrographs that are correctly identified as not\nbelonging to a particular category. False Positives (FP) represent micrographs that are mistakenly\nclassified as belonging to a category despite not actually belonging to that category. These metrics\nevaluate the accuracy and effectiveness of our framework in micrograph categorization. Precision\n(TP / (FP + TP)) measures the proportion of correctly classified micrographs for a specific category,\nwhile recall (TP / (FN + TP)) measures the proportion of all micrographs of a category that were\naccurately identified. The F1-score is computed as the balanced mean of precision and recall. It is\nimportant to note that the SEM dataset is highly class-imbalanced. Our framework demonstrates a\nrelatively higher score in the classification of nanomaterial categories with a large number of labeled\ninstances compared to those with fewer. This favorable performance of our proposed framework can\nbe attributed to its reduced dependency on nanomaterial-specific relational inductive bias, setting it\napart from traditional methods."}, {"title": "Baseline Algorithms", "content": "We have categorized our baseline methods into four distinct groups: Graph Neural Networks (GNNs)\n([79, 38]), Graph Contrastive Learning (GCL) [114]), Convolutional Neural Networks (ConvNets)[2,\n1], Vision Transformers (ViTs) ([2, 1]) and Vision Contrastive Learning (VCL) ([34]) algorithms"}, {"title": "Hyperparameter Studies", "content": "We performed an in-depth hyperparameter tuning to determine the optimal hyperparameters for our\nframework. The hyperparameters of the algorithm are: (1) the dimensionality of the embedding (d),\nand (2) batch size (b). The hyperparameters were chosen from the following ranges: embedding\ndimension (d) \u2208 [32, 64, 128, 256] and batch size (b) \u2208 [32, 48, 64, 96]. We conducted hyperparam-\neter optimization using the random-search technique to achieve the optimal performance of our\nproposed framework on the validation dataset, measured in terms of Top-1 classification accuracy.\nFor each experiment, we altered the hyperparameter under investigation to ascertain its impact on the\nframework's performance. The study determined that the optimal hyperparameters are d = 64 and\nb = 48."}, {"title": "Benchmarking with open-source material datasets", "content": "\u2022 NEU-SDD ([29]) is a comprehensive database comprising 1800 grayscale electron micro-\ngraphs of surface defects on hot-rolled steel strips. The dataset is divided into six distinct\ndefect classes, each containing 300 micrographs with a resolution of 200\u00d7200 pixels. The\ndefect categories include pitted surfaces, scratches, rolled-in scale, crazing, patches, and\ninclusion defects. Figure 7 displays representative images from each category. We conducted"}, {"title": "Graph Chebyshev convolution", "content": "The graph convolution is a powerful tool in the realm of learning from graph-structured data. The\nspectral graph convolution[85] is a popular approach, but it can be computationally expensive for\nlarge graphs. To tackle this issue, Chebyshev graph convolution[28] offers a more scalable approach\nthat can be used to achieve similar performance in capturing the local connectivity and spectral\nproperties of the graph. More precisely, Graph Chebyshev Convolution is a method that approximates\nthe spectral graph convolution by using Chebyshev polynomials. Graph Chebyshev Convolution\nallows us to apply convolutional filters on graph-structured data based on the Chebyshev polynomial\napproximation of the graph Laplacian. The Chebyshev polynomials are calculated based on the\nnormalized Laplacian matrix of the graph. The normalized Laplacian matrix, denoted as $\\mathcal{L}$, is defined\nas:\n$\\mathcal{L} = \\mathbb{I} - \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$\nwhere A is the normalized adjacency matrix and D is the diagonal degree matrix of the graph. The\nChebyshev approximation of the graph Laplacian up to any desired degree is obtained by using a\ntruncated expansion of Chebyshev polynomials, denoted as $T_{k}(\\mathcal{L})$, where k represents the degree of\nthe polynomial. These polynomials are computed recursively using the following recurrence relation\nas follows:\n$T_{k}(\\mathcal{L}) = \\begin{cases} \\mathbb{I}, & \\text{if } k = 0 \\\\ \\mathcal{L}, & \\text{if } k = 1 \\\\ 2\\mathcal{L}T_{k-1}(\\mathcal{L}) - T_{k-2}(\\mathcal{L}), & \\text{otherwise} \\end{cases}$\nwhere I is the identity matrix. Given an input graph feature matrix $I \\in \\mathbb{R}^{n\\times d}$, where n denotes the\nnumber of patches and d is the patch embedding dimension, and the Chebyshev polynomials denoted\nby $T_{k}(\\mathcal{L})$. The Chebyshev graph convolution operation can be defined as follows:\n$E = \\sigma(\\sum_{k=0}^{K-1} T_{k}(\\mathcal{L})I\\Theta_{k})$\nwhere $\\sigma(\\cdot)$ is a non-linear ReLU activation function applied element-wise and $\\Theta_{k} \\in \\mathbb{R}^{d \\times d}$ is the\nparameter matrix (weights) for the k-th order Chebyshev polynomial. It is important to note that the\nparameter matrices $\\Theta_{k}$ are typically learnable and optimized during the training process to adaptively\ncapture the global graph characteristics. K denotes the maximum order of the Chebyshev polynomials\nand influences the expressive power of the approximation. $E \\in \\mathbb{R}^{n \\times d}$ is the transformed node feature\nmatrix, which captures the local structure and relationships within the graph, where $e_{i} \\in \\mathbb{R}^{d}$ denotes\nthe node embedding."}, {"title": "Neural Ordinary Differential Equations (NODE)", "content": "Neural Ordinary Differential Equations (Neural ODE) [20] represent a deep neural network model\ndesigned for continuous-time systems, in contrast to traditional discrete-time neural networks. In the\nNeural ODE framework, we denote the hidden state of a dynamic system at a given time t as z(t).\nThe objective is to determine the evolution of z(t) by calculating its derivative with respect to time\nto capture the temporal dynamics of the system. This derivative is represented by a parameterized\nneural network function, denoted as $f (z(t), t, \\theta)$, as follows:\n$\\frac{dz(t)}{dt} = f(z(t), t, \\theta)$\nHere, $\\theta$ represents the parameters of the neural network $f(\\cdot)$. To compute the output of the Neural\nODE framework, an ODE solver takes the initial hidden state $z(t_{0})$ at the starting time point $t_{0}$ and\nintegrates the hidden state derivative over time to produce the hidden state $z(t_{1})$ at the specified\noutput time point $t_{1}$, as described below:"}, {"title": "Equation Notation", "content": "$\nIn summary", "20": "introduced the adjoint sensitivity method for Neural ODEs. An adjoint", "solver": "n$\\frac{d \\mathcal{L}}{d \\theta} = \\int_{t_{0}}^{t_{1}} \\alpha(t)^{T} \\frac{\\partial f(z(t), t, \\theta)}{\\partial \\theta} dt$\nIn essence, the adjoint sensitivity method solves an augmented ODE backward in time, enabling the\ncomputation of gradients without the need for backpropagation through the ODE solver operations.\nThis means that the model doesn't have to store intermediate results (partial derivatives) from\nforward propagation, resulting in a constant memory cost as a function of the depth. In this work,\nwe incorporate Neural ODEs into computer vision tasks for electron micrograph classification by\nsegmenting an electron micrograph into a sequence of patches. The sequence length is determined\nby the total number of patches generated through the tokenization of the electron micrograph,\nwith each patch serving as an individual token in the sequence. Treating the input sequence of"}]}