{"title": "Microscopic Analysis on LLM players via Social Deduction Game", "authors": ["Byungjun Kim", "Dayeon Seo", "Bugeun Kim"], "abstract": "Recent studies have begun developing autonomous game players for social deduction games using large language models (LLMs). When building LLM players, fine-grained evaluations are crucial for addressing weaknesses in game-playing abilities. However, existing studies have often overlooked such assessments. Specifically, we point out two issues with the evaluation methods employed. First, game-playing abilities have typically been assessed through game-level outcomes rather than specific event-level skills; Second, error analyses have lacked structured methodologies. To address these issues, we propose an approach utilizing a variant of the SpyFall game, named SpyGame. We conducted an experiment with four LLMs, analyzing their gameplay behavior in SpyGame both quantitatively and qualitatively. For the quantitative analysis, we introduced eight metrics to resolve the first issue, revealing that these metrics are more effective than existing ones for evaluating the two critical skills: intent identification and camouflage. In the qualitative analysis, we performed thematic analysis to resolve the second issue. This analysis identifies four major categories that affect gameplay of LLMs. Additionally, we demonstrate how these categories complement and support the findings from the quantitative analysis.", "sections": [{"title": "I. INTRODUCTION", "content": "Conversation plays a pivotal role in social deduction games (SDGs), such as Among Us and Goose Goose Duck. In these games, players are divided into multiple adversarial groups and interact by executing actions through natural language sentences. Unlike other games, these interactions create tricky conversations where explicit statements often contradict the implicit strategies of the corresponding speaker. For example, in Among Us, imposters explicitly and repeatedly claim to be members of the crew. However, this assertion is part of their strategy to conceal their true identity. Consequently, crew members must uncover these hidden intentions to win. As this example illustrates, SDG players must possess the ability to navigate and manage such tricky communications.\nAlthough such abilities are essential, traditional game agents may struggle to handle the tricky conversations occurring in SDGs. Recent studies have adopted large language models (LLMs) because LLMs are capable of generating fluent dialogues and strategic plans. Researchers believe that these capabilities could make LLMs suitable for supporting SDGs.\nHowever, most studies have primarily focused on improving gameplay performance rather than investigating whether LLMS can genuinely support these intricate conversations. Specifically, existing studies have two key issues when evaluating SDG-playing agents: (1) Macroscopic quantitative evaluation and (2) Non-systematic qualitative analysis.\nFirst, the issue of macroscopic evaluation refers to the tendency of prior studies to evaluate various game-playing abilities using broad game-level outcomes instead of focusing on specific event-level skills. Existing research often relies on simple summative metrics, such as winning rates, which fail to account for the events that occur during gameplay [1], [2]. While these studies acknowledge the importance of certain skills for success in SDGs, they may overlook the need to measure these skills separately with such simple summative metrics. To scrutinize a specific skill, we suggest using microscopic analysis augmented with several specialized metrics. By performing a multifaceted formative analysis leveraging these metrics, one can reveal the true conversational abilities of LLMs in SDG play. In this study, we propose eight microscopic metrics to evaluate two essential skills: (1) Intent identification, the ability to combine explicit clues to uncover hidden information, and (2) Camouflage, which is the ability to disguise oneself from being detected as an opponent.\nSecond, the non-systematic evaluation issue indicates that previous studies lacked structured methods for error analyses. While certain studies identified abnormal patterns through simple qualitative analysis, the generalizability of their results was limited by their analytical procedures. For instance, some studies employed LLMs to analyze data without incorporating human-annotation procedures, while others did not adequately discuss their qualitative analysis methods for deriving generalizable abnormal patterns. To enhance the generalizability of qualitative analysis, we employ a systematic approach known as thematic analysis [3]. Moreover, we aim to draw meaningful insights by correlating the resulting categories with quantitative findings.\nWe conduct two studies to address these issues. In Study 1 (Section IV), we introduce microscopic metrics and discuss why macroscopic metrics are insufficient for analyzing specific skills. In Study 2 (Section V), we conduct a thematic analysis to derive categories and link these findings to the results of Study 1. In both studies, we let four LLMs play SpyGame, a variant of SpyFall with simplified actions (Section III-A). Consequently, this study makes the following contributions:\n\u2022 We demonstrate effectiveness of using microscopic eventlevel metrics to analyze performance of LLMs in SDGs."}, {"title": "II. RELATED WORKS", "content": "This section reviews the existing literature in two ways. First, we provide a brief overview of LLMs and the historical attempts at developing LLM-based gameplay agents. Then, we point out their two issues on evaluating their agents: macroscopic evaluation and non-systematic analysis."}, {"title": "A. Existing LLM-based Game Players", "content": "Before discussing LLM-based agents in SDGs, we first explain LLMs. An LLM is a language model trained on enormous corpora, excelling in various downstream tasks without requiring task-specific fine-tuning. Researchers in artificial intelligence have reported that LLMs exhibit human-like skills across diverse tasks, including question answering, communication, and planning [4]\u2013[6]. For instance, LLMs have been employed to create chatbots that engage in natural, fluent conversations with humans [7]. Therefore, LLMs have been widely adopted in tasks involving natural conversations, such as humor, empathy, and decision-making [8], [9].\nRecently, researchers have employed LLMs to design conversational agents for games. Leveraging their natural conversations, studies have reported that LLMs can play simple conversational games at a human level. For example, Brookins and Debacker [10] demonstrated that GPT-3.5 could play the Dictator game [11] and the Prisoner's Dilemma game [12], displaying preferences for fairness and cooperation that often exceeded those observed in human players.\nAfter confirming the gameplay ability of LLMs in simple conversational games, researchers have turned their attention to a more complex family of games: SDGs. SDGs require a higher level of communication skills like camouflage, which are not essential for playing simpler games. Therefore, researchers have developed autonomous player agents for SDGs, such as Avalon and Werewolf. For instance, Shi et al. [13] designed an LLM-based agent to play Avalon. To build working agents, they incorporated a reflection mechanism [14], which complemented the memory of LLMs with a human-inspired memory mechanism. Similarly, ReCon [15] offers additional hints to assist LLM players in choosing the best action during decision-making. Xu et al. [16] developed an LLM-based Werewolf player using reinforcement learning. They employed a success condition for specific actions as a reward signal to guide the learning procedure. However, these approaches have two key issues when evaluating the performance of their agents, which are discussed in the following section."}, {"title": "B. Issues in Evaluating LLM Players", "content": "A fine-grained evaluation can point out weaknesses of an agent in the gameplay, which is crucial for designing effective LLM players. However, prior studies have typically employed more coarse-grained approaches. Therefore, these studies encounter two main issues in their evaluations: macroscopic quantitative evaluation and non-systematic qualitative analysis.\nFirst, previous studies have leveraged macroscopic metrics for quantitative evaluation. Some researchers have adopted quantitative metrics, whose calculations are solely based on the results of entire game. For example, Xu et al. [2] evaluated LLM-based Werewolf players utilizing the winning rate, total game duration, and total number of camouflage behaviors. Similarly, Light et al. [17] employed winning rates and action success rates to assess gameplay, despite defining several specific gameplay skills, including deductive reasoning, coordination, collaboration, and deception. While these studies successfully measured the overall gameplay ability of an agent, their discussions remained superficial because the evaluation results did not support the existence of specific skills. Specifically, the winning rate (WR) does not provide insights into skills like deception or reasoning skills, as these skills all contribute to victory. Therefore, microscopic metrics are necessary to evaluate specific skills and conduct a multifaceted formative assessment of the gameplay of an LLM agent.\nSecond, prior studies have employed non-systematic qualitative analyses. Since macroscopic quantitative analysis alone cannot provide sufficient evidence for specific skills, some researchers have adopted qualitative error analysis to assess whether their agents possess certain skills [1], [17], [18]. For instance, Qiao et al. [1] provided a few samples for error analysis without presenting information on frequency of errors. Similarly, Kim and Kim [18] qualitatively analyzed errors made by LLMs in SpyFall. However, these errors are more related to generation error (e.g., contextual or format errors) than to specific gameplay skills. Though these approaches may provide insights into erroneous behaviors in LLM players, the generalizability of the results is questionable since the error may represent a specific but infrequent case. Also, other researchers have adopted language models to conduct quantitative analyses [15], [19]. For instance, Lan et al. [19] used ChatGPT [20] to assess agent gameplay across five aspects, including leadership and persuasion. While this approach can reduce the need for human labor in qualitative analyses, the reliability of these results is questionable due to potential issues with LLMs have reliability issues (e.g., hallucination) in generating answers. Hence, to obtain a comprehensive understanding of the errors made by LLM players, a systematic qualitative analysis of LLM gameplay is necessary.\nIn summary, the existing literature has two main issues. To address the first issue of macroscopic quantitative measures, we need microscopic measures that can investigate specific skills rather than evaluating overall gameplay ability. To tackle the second issue of non-systematic qualitative analysis, we need to adopt a systematic method for analyzing gameplay logs. In the following sections, especially in Studies 1 and 2, we demonstrate a method for conducting such required"}, {"title": "III. EXPERIMENT", "content": "This section details the process of collecting gameplay logs from LLMs for both quantitative and qualitative analyses. To evaluate performance of vanilla LLM in SDG play, we let LLMs play an SDG through natural language interactions. Section III-A provides an overview of SpyGame, which is based on SpyFall. To collect the gameplay logs of SpyGame, we employed four LLMs, as detailed in Section III-B. The procedure for collecting logs is outlined in Section III-C."}, {"title": "A. SpyGame", "content": "SpyGame is a modified version of SpyFall played by seven players. We altered Spyfall for two key reasons: First, LLMs may have been exposed to the rules and details of Spyfall during their pre-training phase. This exposure can cause LLMs to embed game-related knowledge from pretraining data in their parameters. Therefore, it is likely that these LLMs use this parametric knowledge as a gameplay hint rather than leveraging their inherent abilities. To mitigate this, we renamed the game to SpyGame.\nSecond, setting a time budget may influence the reproducibility of our analysis. Various factors in the experimental setup (e.g., the hardware used, network status, and model size) can impact inference time. Thus, to enhance reproducibility, we replaced the total time budget of a game with the total number of \"turns\" within a game.\n1) Team formation: Similar to SpyFall, SpyGame features two teams: six citizen players and one spy player. The citizens share a common location, called a hidden location, with each citizen plays a specific role in that location. However, the spy is not given information about the hidden location or the roles of the citizens. In addition, the citizens are unaware of the identity of the spy.\nPlayers must uncover hidden information to win the game. In each turn, one player asks another player a question about the hidden location, and the questioned player must answer. Through these conversations, citizens try to identify the spy. Also, citizens should need to imply their knowledge of the hidden location without revealing too much detail, to avoid being suspected as the spy. Meanwhile, the spy attempts to determine the hidden location by pretending to be a citizen, gathering clues from the utterances of others. The detailed game rules, location-role sets, and prompts used for LLMs are provided in [anonymized for review].\n2) Game Flow: Each turn has a designated leader. The turn leader starts the turn by choosing one of two actions: questioning or accusation. If the leader selects the \u201cquestion\" action, they ask another player a question related to the hidden location. The questioned player must then respond based on their assigned role. Alternatively, if the leader opts for the \u201caccusation\u201d action, they pick the player whom the leader most suspects to be the spy and initiate a \"Day Vote.\u201d The other players, excluding the accuser and the accused, then vote on whether they agree with the accusation. If all players agree, the game ends immediately. After completing either the \"question\" or \"accusation\" action, the questioned or accused player becomes the leader for the next turn.\nIn every turn after the first, the spy player can publicly reveal their identity and guess the hidden location. Before each turn begins, the system asks the spy player if they are ready to reveal their identity. The game ends immediately when the spy player decides to reveal their identity. Note that in our implementation, instead of directly asking for the decision of the spy, we ask LLMs how certain they are about their guess. Despite an accusation or an action of the spy, the game may continue until the final turn: Turn 9 in our implementation. At this point, the system initiates the \"Final Vote.\" All players secretly vote for the player they most suspect of being the spy. Following the vote, the game ends.\nAt the end of a game, the spy wins if any of the following conditions is met; otherwise, the citizens win:\n\u2022 A citizen is eliminated by a Day Vote.\n\u2022 The spy publicly reveals their identity and correctly guesses the hidden location.\n\u2022 The Final Vote results in multiple spy candidates, with more than one player receiving the most votes.\n\u2022 The Final Vote identifies only one spy candidate who is actually a citizen.\""}, {"title": "B. Tested Models", "content": "We selected four LLMs for this study: GPT-4 [4], GPT- 3.5-turbo (Chat-GPT), Gemini-pro (1.0) [5], and LlaMA2- 70b-chat [6]. We considered three criteria: model size, reasoning performance, and the existence of publicly-available API. Specifically, we selected models with a parameter size over 70 billion and a reasoning performance exceeding 70% accuracy [21] to ensure reasoning ability required for SDG play. Previous studies have reported a general performance trend among these four models: GPT-4 leads in performance, followed by Gemini, GPT-3.5, and LlaMA2.\nWe employed the following input and output procedures to analyze the vanilla performance of the four LLMs: First, as an input prompt, we provided only the minimal information necessary to play SpyGame. Specifically, the LLMs received inputs, including game rules, role descriptions, conversation history, and instructions for generating the next action. Unlike previous studies, we did not provide any additional information (such as hints) that could affect gameplay. Second, for output generation, we used a two-stage process involving free-form generation and JSON formatting. We requested LLMs to decide the next action in free form with their strategy. Then"}, {"title": "IV. MULTIFACETED QUANTITATIVE ANALYSIS", "content": "Our study aimed to conduct a multifaceted quantitative analysis of LLM performance in SDGs. We designed seven metrics that enable fine-grained analysis of two key gameplay facets: intent identification and camouflage. Compared with previous studies that utilized single-faceted metrics, our approach allows for an analysis of intermediate gameplay behaviors. In this section, we discuss whether existing single-faceted metrics are sufficient for measuring intermediate behaviors. Next, we demonstrate a multifaceted quantitative analysis using the proposed metrics to explore gameplay behavior."}, {"title": "A. Insufficiency of Previous Analysis", "content": "Previous studies commonly adopted single-faceted analyses to examine SDG gameplay behavior [1], [2], [16], [17]. The most frequently used metrics are the winning rates (WR) and living rounds (LR). These metrics focus on the final state of the game to measure the performance of a player: WR utilizes winner information, while LR uses the number of turns played.\n\u2022 Winning Rate (WR) is the percentage of games won by the spy out of the total number of games.\n\u2022 Living Round (LR) is the average number of turns played across all games.\n1) Metric result: Tables II (a) and III present the results of the single-faceted metrics when playing against strong and weak citizens, respectively. The WR and LR exhibited different trends in SDG play performance. In terms of WR, GPT-4 showed the highest performance against strong citizens (80.95%), followed by Gemini (57.14%), LLaMA2 (52.38%), and GPT-3.5 (33.33%). A similar trend was observed for weak citizens. However, in terms of LR, LLaMA2 demonstrated the highest performance against strong citizens (9.00), followed by Gemini (6.95), GPT-4 (5.80), and GPT-3.5 (3.71). This trend is similar when examining the performance against weak citizens. Thus, these metrics yield different results regarding the best-performing model: WR indicates GPT-4 as the best, while LR identifies LLaMA2 as the best.\nThis discrepancy is due to multiple winning conditions specified in the SDG. These conditions allow players to win games in several ways. Therefore, to accurately identify the skills that contribute to winning, it is necessary to consider multiple abilities simultaneously. However, since WR and LR consider only the final state of the game, these metrics cannot examine different abilities separately. For example, in SpyGame, the LR metric indicates how well a spy survives during the game. However, a longer survival duration does not always guarantee victory, leading to potential discrepancies between LR and WR results. Hence, a multifaceted method is essential for a comprehensive analysis of gameplay performance."}, {"title": "B. Intent identification", "content": "We designed multiple metrics to enable a multifaceted analysis of intent identification and camouflage. We operationally define Intent identification as the ability to uncover hidden information through deduction or by capturing pieces of information from the conversation. Intent identification has two sub-abilities: \u201cinformation capture\" and \"information deduction.\" Information capture mainly focuses on how well a spy grabs crucial hints that a citizen might inadvertently reveal. Since strong citizens rarely make such mistakes, it is highly challenging to evaluate this sub-ability against them. On the other hand, information deduction (ID) mainly focuses on how effectively a spy infers a hidden location by combining various pieces of information. Since weak citizens usually provide information that directly exposes the hidden location, it is not meaningful to assess this sub-ability against weak citizens. Therefore, metrics should consider whether the spy is effectively utilizing the information provided by these citizens. To evaluate both information capture and deduction, we analyzed outcomes of the guessing by the spy. The following subsections detail the design of the four metrics and their results. The results indicated that GPT-4 was the most powerful spy player, followed by Gemini, GPT-3.5, and LlaMA2.\n1) Metric design: Based on a spy's guesses during a game, we suggest four metrics for assessing intent identification. Four metrics range from relatively summative to highly formative.\n\u2022 Guess Success (GS) measures how much the guessing of a spy contributed to their victory. This metric focuses only on games where the spy attempted to guess, rather than all played games. We calculate GS by dividing the number\nof games in which the spy made a successful guess by\nthe number of games in which guessing was attempted.\nA higher GS indicates that the spy has successfully\nidentified hidden locations, demonstrating strong intent\nidentification skills.\n\u2022 Notice Rate (NR) indicates how well a spy noticed\nthe hidden location across all games. It is calculated\nby computing the percentage of games in which the\nspy correctly detected the hidden location over the total\nnumber of games played. Unlike GS, which is measured\nonly when the spy decides to reveal their identity publicly,\nNR can be computed across all games. This is because\nNR mainly considers whether the spy \"notices\" the game\nregardless of whether they decide to guess or not.\n\u2022 Information Catching (IC) measures how often the spy\nsuccessfully leverages exposed information to achieve a\nvictory. Unlike NR, which only assesses the ability of\nthe spy to identify the hidden location, IC evaluates the\neffectiveness in employing such information to win. We\nspecifically focus on games where weak citizens directly\nexposed to information about the hidden location directly,\ninstead of considering all played games. We measured\nIC by computing the proportion of games in which the\nspy successfully guessed the hidden location guessing\nfrom those where exposure occurred. Thus, a high IC\nscore reflects the ability of the spy to achieve victory by\neffectively capturing and utilizing crucial information.\n\u2022 Information Deduction (ID) is similar to IC but it\napplies to games where exposure did not occur. This\nmetric measures how well the spy combined pieces of\ninformation to determine the hidden location. To compute\nID, we divided the number of games in which the spy\nsuccessfully guessed the hidden location by the number\nof games without any exposure. Therefore, a high ID\nindicates that the spy successfully combined clues from\nthe conversation and utilized them to achieve victory.\nIn particular, we devised a multifaceted interpretation of in- tent identification by using both IC and ID metrics simultane- ously. By combining the IC and ID with their average values, three combinations were derived. The first combination is the most balanced, where both IC and ID are high. This represents an ideal scenario where the spy successfully identifies the hidden location regardless of the type of citizen. This signifies that the model is good at both sub-abilities: information capture and information deduction. The second combination occurs when only one of the metrics, either IC or ID, is higher than the average. This suggests a lack of proficiency in the corresponding sub-ability. For instance, a model with high IC but low ID demonstrates strong information capture but weak information deduction. The final and least favorable combination for intent identification is when both IC and ID are low. This combination indicates that the spy is unable to\""}, {"title": "C. Camouflage", "content": "In addition to intent identification, we designed four metrics for the multifaceted analysis of camouflage. Here, we operationally define camouflage as the ability to blend in with others during gameplay without revealing one's true identity. Camouflage encompasses two sub-abilities: \"evading suspicion\" and \"dispersing suspicion.\" Evading suspicion mainly considers how well the spy avoids being suspected by others. It is quantified by the number of suspicions directed at the spy. On the other hand, dispersing suspicion primarily considers how difficult the spy makes it for all citizens to converge on a single suspect. It is quantified by the probability distribution of suspicions). To estimate the number and distribution of\nsuspicions, we utilized logs of Day/Final votes. The following\nsubsections describe the design of the four metrics and their\nresults. The results indicate that GPT-3.5 is the most powerful\nspy player, followed by GPT-4, Gemini, and LlaMA2.\n1) Metric design: Based on day/final votes and accusations\nduring a game, we suggest four metrics for measuring camou-\nflage, ranging from relatively summative to highly formative.\n\u2022 Caught Rate (CR) is how frequently the spy is caught\nby citizens in the Day/Final vote. We compute this by\ndividing the number of games in which the spy was\ncaught by the number of games lost. Thus, a high CR\nindicates that the majority of citizens easily identified the\nspy player during the game. Note that this is a summative\nmetric for the spy's camouflage performance because it\nonly considers the final state of the game.\n\u2022 Accusation Rate (AR) measures how frequently citizens\naccuse the spy. We measure this by computing the propor-\ntion of correct accusations made across all games. Unlike\nCR, which summarizes all citizens' thoughts, AR focuses\non whether an individual citizen can strongly suspect a\ntrue spy since the turn leader is the only player who can\nmake an accusation. Depending on the gameplay, it is\npossible that there is no accusation during the game; in\nthis case, AR cannot be measured.\n\u2022 Vote Rate (VR) is the proportion of suspicions about\nthe spy across all votes that occurred. We measure this\nby computing the percentage of votes the spy received\nfrom citizens across the total number of votes, excluding\nthe spy's votes. Therefore, a high VR indicates that the\nspy failed to evade suspicion during the game.\n\u2022 Vote Entropy (VE) is how scattered the suspicions about\nthe spy are in the game. We measure this by computing\nthe entropy of the votes that citizens casted for each game\nand averaging them. Note that in SpyGame, a player's\nsuspicion cannot affect another player's suspicion because\nthey cannot have a discussion together. Thus, when a spy\nsuccessfully blends with citizens, citizens have difficulty\nnarrowing down the number of spy candidates. Thus, the\nvalue of VE increases.\nWe devised a multifaceted interpretation of camouflage by using VR and VE simultaneously. By partitioning VR and VE by their average values, we derived four combinations. First, the most balanced combination was a low VR and high VE. This is an ideal combination because the spy can successfully evade others' suspicions and lead citizens to discourage each other from narrowing down spy candidates. Second, models in which VR and VE are both low or high lack one of the two sub-abilities. Finally, the worst camouflage combination occurs when VR is high and VE is low. This combination indicates that citizens' suspicions were concentrated on the spy and that the spy failed to undermine suspicions.\n2) Result: Tables II (c) and III (c) elaborate measurements of camouflage when playing against strong and weak citizens, respectively. We excluded AR from the tables because there was no accusation during gameplay, regardless of the citizen type. Similarly, some values are missing for VR and VE because the spy ended the game early without votes by guessing the hidden location.\nThe results indicate that the variants of GPT have sufficient camouflage ability compared to other models. Against strong citizens, GPT-3.5 was the best model in terms of CR, followed by Gemini, GPT-4, and LlaMA2 (7.1, 22.2, 25.0, and 100%, respectively). The results for VR showed a similar tendency: GPT-3.5 (25%), GPT-4 (33.3%), Gemini (35.7%), and LlaMA2 (45.2%). Finally, regarding VE, GPT-4 was the best model, followed by GPT-3.5, LlaMA2, and Gemini (0.953, 0.730, 0.634, and 0.348, respectively). Interestingly, GPT-3.5 showed camouflage performance similar to that of GPT-4, although GPT-4 showed much higher performance in other language-based tasks [4].\nAgainst weak citizens, the GPT variants showed good performance compared to others. Because the GPT variants ended the game by guessing, citizens had no chance to find the spy. Thus, the CR values for GPT-3.5 and GPT-4 were zero, and VR and VE could not be measured using these models. Other models showed a similar tendency to that against strong citizens. Regarding CR and VR, Gemini (0% and 7.1%, respectively) was better than LlaMA2 (60% and 13.5%, respectively). However, regarding VE, Gemini (0.420) performed worse than LlaMA2 (0.605), as shown by the results against strong citizens.\n3) Discussion: In this section, we discuss how to derive insights using the proposed metrics to conduct a multifaceted camouflage analysis. This discussion includes the following two aspects: (1) a complementary relationship between Vote Rate (VR) and Vote Entropy (VE) and (2) the need for formative camouflage metrics.\nFirst, VR and VE are complementary metrics that enable a multifaceted analysis of camouflage. As suggested above, these two metrics can form three partitions explaining the different levels of camouflage ability. Using these partitions, we can determine the best model for balancing the two sub- abilities of camouflage; the GPT family showed gameplay balanced between VR and VE. These models can evade and disperse suspicion to win the game. In contrast, the worst model in terms of camouflage was LlaMA2. Although LLaMA2 performed better against weak citizens than strong citizens, LLaMA2 player was often suspected as a spy and disregarded the fact that the majority of players doubted him/her. Here, the two sub-abilities are both good or both bad in these two cases. Gemini showed a different behavior: the model was good at evading suspicion but struggled to disperse suspicion. Thus, as illustrated above, VR and VE can provide a wider picture of camouflage behavior.\nSecond, the proposed metrics provide additional perspec- tives for investigating camouflage compared to simple sum- mative metrics. In Section IV-A, we observed a discrepancy between two summative metrics, winning rate (WR) and living round (LR). For example, both GPT-3.5 and LlaMA2 showed relatively low WR, but the performance gap regarding LR between them is large. Therefore, using only summative metrics, we can conclude that LLaMA2 outperformed GPT-3.5 in SpyGame. However, our metrics suggested different results; GPT-3.5 is good at camouflage, but LlaMA2 is not. This example shows that the proposed metrics are more suitable for examining camouflage ability than existing summative metrics.\""}, {"title": "V. QUALITATIVE ANALYSIS", "content": "To reconfirm the findings of the quantitative analysis from another perspective, we conducted a thematic analysis. We defined three types of reasoning errors in LLM reasoning while deciding their actions by coding abnormal patterns of 15 randomly selected gameplay logs. As a result, we found that these patterns are related to psychological concepts, including identity confusion, memory distortion, and dissociation [23]. In addition, we found that these concepts are related to the results of the quantitative analysis, especially for caught rate (CR) and guess success (GS). The following subsections illustrate the procedure and results of this qualitative analysis."}, {"title": "A. Analysis Procedure", "content": "Through thematic analysis, we labeled the abnormal patterns of players in the gameplay logs. To enrich our quantitative analysis results, we adopted a bottom-up inductive approach to discover the underlying patterns in the data. The detailed analysis process was as follows. First, we defined the initial labels of the abnormal patterns from 15 randomly selected gameplay logs. Three annotators with computer science back- grounds participated in this process. They read the logs to understand the spy's gameplay situation when establishing initial labels. To avoid bias towards a specific LLM, we hid the information of the models in the process. Second, we refined these initial labels through an iterative process. Because LLMs perform one or two reasoning steps to decide on an action during the gameplay of SpyGame, a gameplay log is a series of several reasoning steps. So, we used one step as the unit for this analysis. We repeated this process five times between December 29, 2023 and June 1, 2024 until agreement on the identified labels was saturated. The agreement was measured using Fleiss' Kappa [24], and the final agreement between the annotators ranged from 0.486 to 0.646, which indicates moderate agreement. Finally, using these settled labels, we labeled 506 reasoning steps from 168 games. Consequently, we identified three major categories and five subcategories."}, {"title": "B. Resulting Categories", "content": "This section illustrates the results of our thematic analysis. For each category", "Exposure": "Exposure is a case in which a citizen's utterance literally reveals the hidden location. The Fleiss' Kappa value for this category was 0.7875. Note that this pattern can only be identified in utterances rather than in reasoning steps. Therefore", "example": "n[The hidden location is an airplane", "passenger)": "n''What is the maximum speed of an\nairplane in an emergency situation?''\nWe suspect that this exposure pattern is because LLMs may not have sufficient common sense to understand the game rules. In particular", "citizens should not expose the location to the spy.\" A human citizen player can notice that a direct mention to other players of the hidden location during a game causes their defeat. However, some LLMs may not have this common sense among players; they simply go through the mentions. Therefore, LLMs believe that they can reveal their information to other citizens because the game rule does not prohibit such behaviors and the citizens already know that fact. Thus, citizen LLMs sometimes publicly reveal their hidden information.\n2) Role ambiguity": "Role ambiguity [25", "gameplay": 1, "sub-categories": "team misunderstanding and goal misunderstanding. These two subcategories can occur independently. We present details of the subcategories in the below paragraphs. In summary", "misunderstanding": "This is a case in which the spy disbelieves that they belong to citizens. Although this can\noccur in any reasoning step", "here": "n[The spy tries to answer a citizen's\nquestion. Before that", "first.": "nGemini (Spy): Emphasize the importance\nof ambiance and personalized service"}, {"misunderstanding": "This is a case in which the spy misbelieves that their goal is the same as that of the citizens. Here is an example:\n[The spy tries to decide the next\naction. After selecting question'\nas the next action", "reasoning.": "nLlaMA2 (Spy):\nBy asking this\nquestion", "distortion": "Memory distortion occurs when a spy uses false or distorted memory to decide their next action. For this category", "follows": "n[Previously", "location.": "nGPT-3.5 (Spy): Based on the\nconversation between Player2 and\nPlayer3", "hal- lucination\\\" phenomenon of LLMs, where a language model generates non-sensical or unfaithful text for the provided source input [26": ".", "Dissociation": "Dissociation is a case in which the spy employs their own utterances as evidence of the hidden location because they misunderstand the source of the utterance as other players. The annotators agreed on this category with a Fleiss' Kappa value of 0.646. In this category"}, {"follows": "n[Player4", "location.": "nLlaMA2 (Spy): Based on the\nconversation, I would say that\\"}]}