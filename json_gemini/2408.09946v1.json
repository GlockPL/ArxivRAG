{"title": "Microscopic Analysis on LLM players via Social Deduction Game", "authors": ["Byungjun Kim", "Dayeon Seo", "Bugeun Kim"], "abstract": "Recent studies have begun developing autonomous game players for social deduction games using large language models (LLMs). When building LLM players, fine-grained evaluations are crucial for addressing weaknesses in game-playing abilities. However, existing studies have often overlooked such assessments. Specifically, we point out two issues with the evaluation methods employed. First, game-playing abilities have typically been assessed through game-level outcomes rather than specific event-level skills; Second, error analyses have lacked structured methodologies. To address these issues, we propose an approach utilizing a variant of the SpyFall game, named SpyGame. We conducted an experiment with four LLMs, analyzing their gameplay behavior in SpyGame both quantitatively and qualitatively. For the quantitative analysis, we introduced eight metrics to resolve the first issue, revealing that these metrics are more effective than existing ones for evaluating the two critical skills: intent identification and camouflage. In the qualitative analysis, we performed thematic analysis to resolve the second issue. This analysis identifies four major categories that affect gameplay of LLMs. Additionally, we demonstrate how these categories complement and support the findings from the quantitative analysis.", "sections": [{"title": "I. INTRODUCTION", "content": "CONVERSATION plays a pivotal role in social deduction games (SDGs), such as Among Us and Goose Goose Duck. In these games, players are divided into multiple adversarial groups and interact by executing actions through natural language sentences. Unlike other games, these interactions create tricky conversations where explicit statements often contradict the implicit strategies of the corresponding speaker. For example, in Among Us, imposters explicitly and repeatedly claim to be members of the crew. However, this assertion is part of their strategy to conceal their true identity. Consequently, crew members must uncover these hidden intentions to win. As this example illustrates, SDG players must possess the ability to navigate and manage such tricky communications.\nAlthough such abilities are essential, traditional game agents may struggle to handle the tricky conversations occurring in SDGs. Recent studies have adopted large language models (LLMs) because LLMs are capable of generating fluent dialogues and strategic plans. Researchers believe that these capabilities could make LLMs suitable for supporting SDGs. However, most studies have primarily focused on improving gameplay performance rather than investigating whether LLMS can genuinely support these intricate conversations. Specifically, existing studies have two key issues when evaluating SDG-playing agents: (1) Macroscopic quantitative evaluation and (2) Non-systematic qualitative analysis.\nFirst, the issue of macroscopic evaluation refers to the tendency of prior studies to evaluate various game-playing abilities using broad game-level outcomes instead of focusing on specific event-level skills. Existing research often relies on simple summative metrics, such as winning rates, which fail to account for the events that occur during gameplay [1], [2]. While these studies acknowledge the importance of certain skills for success in SDGs, they may overlook the need to measure these skills separately with such simple summative metrics. To scrutinize a specific skill, we suggest using microscopic analysis augmented with several specialized metrics. By performing a multifaceted formative analysis leveraging these metrics, one can reveal the true conversational abilities of LLMs in SDG play. In this study, we propose eight microscopic metrics to evaluate two essential skills: (1) Intent identification, the ability to combine explicit clues to uncover hidden information, and (2) Camouflage, which is the ability to disguise oneself from being detected as an opponent.\nSecond, the non-systematic evaluation issue indicates that previous studies lacked structured methods for error analyses. While certain studies identified abnormal patterns through simple qualitative analysis, the generalizability of their results was limited by their analytical procedures. For instance, some studies employed LLMs to analyze data without incorporating human-annotation procedures, while others did not adequately discuss their qualitative analysis methods for deriving generalizable abnormal patterns. To enhance the generalizability of qualitative analysis, we employ a systematic approach known as thematic analysis [3]. Moreover, we aim to draw meaningful insights by correlating the resulting categories with quantitative findings.\nWe conduct two studies to address these issues. In Study 1 (Section IV), we introduce microscopic metrics and discuss why macroscopic metrics are insufficient for analyzing specific skills. In Study 2 (Section V), we conduct a thematic analysis to derive categories and link these findings to the results of Study 1. In both studies, we let four LLMs play SpyGame, a variant of SpyFall with simplified actions (Section III-A). Consequently, this study makes the following contributions:\n\u2022 We demonstrate effectiveness of using microscopic event-level metrics to analyze performance of LLMs in SDGs."}, {"title": "II. RELATED WORKS", "content": "This section reviews the existing literature in two ways. First, we provide a brief overview of LLMs and the historical attempts at developing LLM-based gameplay agents. Then, we point out their two issues on evaluating their agents: macroscopic evaluation and non-systematic analysis."}, {"title": "A. Existing LLM-based Game Players", "content": "Before discussing LLM-based agents in SDGs, we first explain LLMs. An LLM is a language model trained on enormous corpora, excelling in various downstream tasks without requiring task-specific fine-tuning. Researchers in artificial intelligence have reported that LLMs exhibit human-like skills across diverse tasks, including question answering, communication, and planning [4]\u2013[6]. For instance, LLMs have been employed to create chatbots that engage in natural, fluent conversations with humans [7]. Therefore, LLMs have been widely adopted in tasks involving natural conversations, such as humor, empathy, and decision-making [8], [9].\nRecently, researchers have employed LLMs to design conversational agents for games. Leveraging their natural conversations, studies have reported that LLMs can play simple conversational games at a human level. For example, Brookins and Debacker [10] demonstrated that GPT-3.5 could play the Dictator game [11] and the Prisoner's Dilemma game [12], displaying preferences for fairness and cooperation that often exceeded those observed in human players.\nAfter confirming the gameplay ability of LLMs in simple conversational games, researchers have turned their attention to a more complex family of games: SDGs. SDGs require a higher level of communication skills like camouflage, which are not essential for playing simpler games. Therefore, researchers have developed autonomous player agents for SDGs, such as Avalon and Werewolf. For instance, Shi et al. [13] designed an LLM-based agent to play Avalon. To build working agents, they incorporated a reflection mechanism [14], which complemented the memory of LLMs with a human-inspired memory mechanism. Similarly, ReCon [15] offers additional hints to assist LLM players in choosing the best action during decision-making. Xu et al. [16] developed an LLM-based Werewolf player using reinforcement learning. They employed a success condition for specific actions as a reward signal to guide the learning procedure. However, these approaches have two key issues when evaluating the performance of their agents, which are discussed in the following section."}, {"title": "B. Issues in Evaluating LLM Players", "content": "A fine-grained evaluation can point out weaknesses of an agent in the gameplay, which is crucial for designing effective LLM players. However, prior studies have typically employed more coarse-grained approaches. Therefore, these studies encounter two main issues in their evaluations: macroscopic quantitative evaluation and non-systematic qualitative analysis.\nFirst, previous studies have leveraged macroscopic metrics for quantitative evaluation. Some researchers have adopted quantitative metrics, whose calculations are solely based on the results of entire game. For example, Xu et al. [2] evaluated LLM-based Werewolf players utilizing the winning rate, total game duration, and total number of camouflage behaviors. Similarly, Light et al. [17] employed winning rates and action success rates to assess gameplay, despite defining several specific gameplay skills, including deductive reasoning, coordination, collaboration, and deception. While these studies successfully measured the overall gameplay ability of an agent, their discussions remained superficial because the evaluation results did not support the existence of specific skills. Specifically, the winning rate (WR) does not provide insights into skills like deception or reasoning skills, as these skills all contribute to victory. Therefore, microscopic metrics are necessary to evaluate specific skills and conduct a multifaceted formative assessment of the gameplay of an LLM agent.\nSecond, prior studies have employed non-systematic qualitative analyses. Since macroscopic quantitative analysis alone cannot provide sufficient evidence for specific skills, some researchers have adopted qualitative error analysis to assess whether their agents possess certain skills [1], [17], [18]. For instance, Qiao et al. [1] provided a few samples for error analysis without presenting information on frequency of errors. Similarly, Kim and Kim [18] qualitatively analyzed errors made by LLMs in SpyFall. However, these errors are more related to generation error (e.g., contextual or format errors) than to specific gameplay skills. Though these approaches may provide insights into erroneous behaviors in LLM players, the generalizability of the results is questionable since the error may represent a specific but infrequent case. Also, other researchers have adopted language models to conduct quantitative analyses [15], [19]. For instance, Lan et al. [19] used ChatGPT [20] to assess agent gameplay across five aspects, including leadership and persuasion. While this approach can reduce the need for human labor in qualitative analyses, the reliability of these results is questionable due to potential issues with LLMs have reliability issues (e.g., hallucination) in generating answers. Hence, to obtain a comprehensive understanding of the errors made by LLM players, a systematic qualitative analysis of LLM gameplay is necessary.\nIn summary, the existing literature has two main issues. To address the first issue of macroscopic quantitative measures, we need microscopic measures that can investigate specific skills rather than evaluating overall gameplay ability. To tackle the second issue of non-systematic qualitative analysis, we need to adopt a systematic method for analyzing gameplay logs. In the following sections, especially in Studies 1 and 2, we demonstrate a method for conducting such required"}, {"title": "III. EXPERIMENT", "content": "This section details the process of collecting gameplay logs from LLMs for both quantitative and qualitative analyses. To evaluate performance of vanilla LLM in SDG play, we let LLMs play an SDG through natural language interactions. Section III-A provides an overview of SpyGame, which is based on SpyFall. To collect the gameplay logs of SpyGame, we employed four LLMs, as detailed in Section III-B. The procedure for collecting logs is outlined in Section III-C."}, {"title": "A. SpyGame", "content": "SpyGame is a modified version of SpyFall played by seven players. We altered Spyfall for two key reasons: First, LLMs may have been exposed to the rules and details of Spyfall during their pre-training phase. This exposure can cause LLMs to embed game-related knowledge from pretraining data in their parameters. Therefore, it is likely that these LLMs use this parametric knowledge as a gameplay hint rather than leveraging their inherent abilities. To mitigate this, we renamed the game to SpyGame.\nSecond, setting a time budget may influence the reproducibility of our analysis. Various factors in the experimental setup (e.g., the hardware used, network status, and model size) can impact inference time. Thus, to enhance reproducibility, we replaced the total time budget of a game with the total number of \"turns\" within a game.\n1) Team formation: Similar to SpyFall, SpyGame features two teams: six citizen players and one spy player. The citizens share a common location, called a hidden location, with each citizen plays a specific role in that location. However, the spy is not given information about the hidden location or the roles of the citizens. In addition, the citizens are unaware of the identity of the spy.\nPlayers must uncover hidden information to win the game. In each turn, one player asks another player a question about the hidden location, and the questioned player must answer. Through these conversations, citizens try to identify the spy. Also, citizens should need to imply their knowledge of the hidden location without revealing too much detail, to avoid being suspected as the spy. Meanwhile, the spy attempts to determine the hidden location by pretending to be a citizen, gathering clues from the utterances of others. The detailed game rules, location-role sets, and prompts used for LLMs are provided in [anonymized for review]."}, {"title": "2) Game Flow:", "content": "Each turn has a designated leader. The turn leader starts the turn by choosing one of two actions: questioning or accusation. If the leader selects the \u201cquestion\" action, they ask another player a question related to the hidden location. The questioned player must then respond based on their assigned role. Alternatively, if the leader opts for the \u201caccusation\u201d action, they pick the player whom the leader most suspects to be the spy and initiate a \"Day Vote.\u201d The other players, excluding the accuser and the accused, then vote on whether they agree with the accusation. If all players agree, the game ends immediately. After completing either the \"question\" or \"accusation\" action, the questioned or accused player becomes the leader for the next turn.\nIn every turn after the first, the spy player can publicly reveal their identity and guess the hidden location. Before each turn begins, the system asks the spy player if they are ready to reveal their identity. The game ends immediately when the spy player decides to reveal their identity. Note that in our implementation, instead of directly asking for the decision of the spy, we ask LLMs how certain they are about their guess. Despite an accusation or an action of the spy, the game may continue until the final turn: Turn 9 in our implementation. At this point, the system initiates the \"Final Vote.\" All players secretly vote for the player they most suspect of being the spy. Following the vote, the game ends.\nAt the end of a game, the spy wins if any of the following conditions is met; otherwise, the citizens win:\n\u2022 A citizen is eliminated by a Day Vote.\n\u2022 The spy publicly reveals their identity and correctly guesses the hidden location.\n\u2022 The Final Vote results in multiple spy candidates, with more than one player receiving the most votes.\n\u2022 The Final Vote identifies only one spy candidate who is actually a citizen."}, {"title": "B. Tested Models", "content": "We selected four LLMs for this study: GPT-4 [4], GPT-3.5-turbo (Chat-GPT), Gemini-pro (1.0) [5], and LlaMA2-70b-chat [6]. We considered three criteria: model size, reasoning performance, and the existence of publicly-available API. Specifically, we selected models with a parameter size over 70 billion and a reasoning performance exceeding 70% accuracy [21] to ensure reasoning ability required for SDG play. Previous studies have reported a general performance trend among these four models: GPT-4 leads in performance, followed by Gemini, GPT-3.5, and LlaMA2.\nWe employed the following input and output procedures to analyze the vanilla performance of the four LLMs: First, as an input prompt, we provided only the minimal information necessary to play SpyGame. Specifically, the LLMs received inputs, including game rules, role descriptions, conversation history, and instructions for generating the next action. Unlike previous studies, we did not provide any additional information (such as hints) that could affect gameplay. Second, for output generation, we used a two-stage process involving free-form generation and JSON formatting. We requested LLMs to decide the next action in free form with their strategy. Then"}, {"title": "IV. MULTIFACETED QUANTITATIVE ANALYSIS", "content": "Our study aimed to conduct a multifaceted quantitative analysis of LLM performance in SDGs. We designed seven metrics that enable fine-grained analysis of two key gameplay facets: intent identification and camouflage. Compared with previous studies that utilized single-faceted metrics, our approach allows for an analysis of intermediate gameplay behaviors. In this section, we discuss whether existing single-faceted metrics are sufficient for measuring intermediate behaviors. Next, we demonstrate a multifaceted quantitative analysis using the proposed metrics to explore gameplay behavior."}, {"title": "A. Insufficiency of Previous Analysis", "content": "Previous studies commonly adopted single-faceted analyses to examine SDG gameplay behavior [1], [2], [16], [17]. The most frequently used metrics are the winning rates (WR) and living rounds (LR). These metrics focus on the final state of the game to measure the performance of a player: WR utilizes winner information, while LR uses the number of turns played.\n\u2022 Winning Rate (WR) is the percentage of games won by the spy out of the total number of games.\n\u2022 Living Round (LR) is the average number of turns played across all games."}, {"title": "B. Intent identification", "content": "We designed multiple metrics to enable a multifaceted analysis of intent identification and camouflage. We operationally define Intent identification as the ability to uncover hidden information through deduction or by capturing pieces of information from the conversation. Intent identification has two sub-abilities: \u201cinformation capture\" and \"information deduction.\" Information capture mainly focuses on how well a spy grabs crucial hints that a citizen might inadvertently reveal. Since strong citizens rarely make such mistakes, it is highly challenging to evaluate this sub-ability against them. On the other hand, information deduction (ID) mainly focuses on how effectively a spy infers a hidden location by combining various pieces of information. Since weak citizens usually provide information that directly exposes the hidden location, it is not meaningful to assess this sub-ability against weak citizens. Therefore, metrics should consider whether the spy is effectively utilizing the information provided by these citizens. To evaluate both information capture and deduction, we analyzed outcomes of the guessing by the spy. The following subsections detail the design of the four metrics and their results. The results indicated that GPT-4 was the most powerful spy player, followed by Gemini, GPT-3.5, and LlaMA2.\n1) Metric design: Based on a spy's guesses during a game, we suggest four metrics for assessing intent identification. Four metrics range from relatively summative to highly formative.\n\u2022 Guess Success (GS) measures how much the guessing of a spy contributed to their victory. This metric focuses only on games where the spy attempted to guess, rather than all played games. We calculate GS by dividing the number"}, {"title": "C. Camouflage", "content": "In addition to intent identification, we designed four metrics for the multifaceted analysis of camouflage. Here, we operationally define camouflage as the ability to blend in with others during gameplay without revealing one's true identity. Camouflage encompasses two sub-abilities: \"evading suspicion\" and \"dispersing suspicion.\" Evading suspicion mainly considers how well the spy avoids being suspected by others. It is quantified by the number of suspicions directed at the spy. On the other hand, dispersing suspicion primarily considers how difficult the spy makes it for all citizens to converge on a single suspect. It is quantified by the probability distribution of suspicions). To estimate the number and distribution of"}, {"title": "V. QUALITATIVE ANALYSIS", "content": "To reconfirm the findings of the quantitative analysis from another perspective, we conducted a thematic analysis. We defined three types of reasoning errors in LLM reasoning while deciding their actions by coding abnormal patterns of 15 randomly selected gameplay logs. As a result, we found that these patterns are related to psychological concepts, including identity confusion, memory distortion, and dissociation [23]. In addition, we found that these concepts are related to the results of the quantitative analysis, especially for caught rate (CR) and guess success (GS). The following subsections illustrate the procedure and results of this qualitative analysis."}, {"title": "A. Analysis Procedure", "content": "Through thematic analysis, we labeled the abnormal patterns of players in the gameplay logs. To enrich our quantitative analysis results, we adopted a bottom-up inductive approach to discover the underlying patterns in the data. The detailed analysis process was as follows. First, we defined the initial labels of the abnormal patterns from 15 randomly selected gameplay logs. Three annotators with computer science backgrounds participated in this process. They read the logs to understand the spy's gameplay situation when establishing initial labels. To avoid bias towards a specific LLM, we hid the information of the models in the process. Second, we refined these initial labels through an iterative process. Because LLMs perform one or two reasoning steps to decide on an action during the gameplay of SpyGame, a gameplay log is a series of several reasoning steps. So, we used one step as the unit for this analysis. We repeated this process five times between December 29, 2023 and June 1, 2024 until agreement on the identified labels was saturated. The agreement was measured using Fleiss' Kappa [24], and the final agreement between the annotators ranged from 0.486 to 0.646, which indicates moderate agreement. Finally, using these settled labels, we labeled 506 reasoning steps from 168 games. Consequently, we identified three major categories and five subcategories."}, {"title": "B. Resulting Categories", "content": "This section illustrates the results of our thematic analysis. For each category, we explain its definition, report its frequency, and provide some examples.\n1) Exposure: Exposure is a case in which a citizen's utterance literally reveals the hidden location. The Fleiss' Kappa value for this category was 0.7875. Note that this pattern can only be identified in utterances rather than in reasoning steps. Therefore, we report the frequency of this category based on utterances; among 729 citizen utterances, 125 utterances were labeled as exposure. The following quote provides a representative example:\n[The hidden location is an airplane]\nLlaMA2 (Citizen, 1st-class passenger):\n''What is the maximum speed of an\nairplane in an emergency situation?''\nWe suspect that this exposure pattern is because LLMs may not have sufficient common sense to understand the game rules. In particular, SpyGame has a rule \"citizens should not expose the location to the spy.\" A human citizen player can notice that a direct mention to other players of the hidden location during a game causes their defeat. However, some LLMs may not have this common sense among players; they simply go through the mentions. Therefore, LLMs believe that they can reveal their information to other citizens because the game rule does not prohibit such behaviors and the citizens already know that fact. Thus, citizen LLMs sometimes publicly reveal their hidden information.\n2) Role ambiguity: Role ambiguity [25] is a case in which the spy confuses their own identity with that of a citizen. In an SDG, understanding a player's identity is related to two major concepts affecting gameplay: (1) the team of the player and (2) the goal of the player. Therefore, we divided abnormal patterns of role ambiguity into two sub-categories: team misunderstanding and goal misunderstanding. These two subcategories can occur independently. We present details of the subcategories in the below paragraphs. In summary, Fleiss' Kappa for this category was 0.563. Among 506 reasoning steps in the data, we identified 21 steps with role ambiguity.\na) Team misunderstanding: This is a case in which the spy disbelieves that they belong to citizens. Although this can"}, {"title": "C. Reconfirmation of Quantitative Analysis", "content": "Combining the results of the two analyses confirms the findings of the quantitative analysis. Specifically, we discovered that these quantitative labels, except exposure, can be connected to two abilities in the quantitative analysis: camouflage and intent identification.\nFirst, role ambiguity increases the likelihood of a spy's camouflage failure. When we compared the frequency of role ambiguity and the results of the three camouflage metrics, the trend of frequency was similar to that of the metrics. For example, against strong citizens, GPT-3.5 showed the lowest errors in role ambiguity and achieved the highest performance in camouflage metrics. Similarly, LLaMA2 showed more errors while achieving a lower performance than the other models. We suspect that this similarity between the trends is due to the loss of the spy's need for camouflage. When role ambiguity occurs in the spy's reasoning steps, the spy confuses their role with that of a citizen. Such confusion leads the spy to make an unusual utterance, which draws suspicion.\nSecond, memory distortion and dissociation may hinder a spy's intent identification during the game. When we compared the frequency of memory distortion and dissociation with the results of the four intent identification metrics, the trend of frequency was similar to that of the metrics. For example, against strong citizens, GPT-3.5 showed the highest errors in both memory distortion and dissociation and achieved the lowest performance in intent identification. Similarly, GPT-4 showed fewer errors while achieving a higher performance than the other models. We suspect that this similarity between trends is due to the quality of information that the spy considers during their reasoning process. The spy player should select and combine pieces of information from the conversation to identify the hidden location. When memory distortion occurs, the spy can regard misinterpreted pieces as appropriate because they distorted the information. Similarly, when dissociation occurs, the spy regards the apocryphal information that they previously said as information provided by citizens. Therefore, such misunderstood pieces can be obstacles to reaching the correct answer."}, {"title": "VI. CONCLUSION", "content": "To assess whether LLMs can effectively support tricky conversations used in SDGs, we tackled two key issues with previous evaluation methodologies. The first is the reliance on macroscopic quantitative evaluations, which overlook the need for microscopic formative assessment within quantitative analyses. The second is the lack of non-systematic qualitative evaluation, highlighting the necessity for thematic analysis to uncover the logical reasoning behind gameplay. In our demonstration, we chose SpyGame, a variant of SpyFall, as the experimental environment. While human gameplay often incorporates nonverbal and paraverbal cues to identify the hidden intention of others, we adopted the simplest implementation of LLM players to simplify our demonstration. We analyzed the gameplay behaviors using four different LLMs in SpyGame logs. Our analysis was conducted in two ways. First, through quantitative analysis, we employed eight metrics to address the macroscopic evaluation issue. This analysis revealed that our metrics are more effective than existing macroscopic metrics for explaining intent identification and camouflage behavior. Second, in qualitative analysis, we performed thematic analysis to resolve the non-systematic evaluation issue. From the analysis, we discovered four major categories that affect LLMs' gameplay as a spy. These major categories supported the findings of our quantitative analysis. Despite the limitations of our study, our work underscores the significance of microscopic analysis in evaluating LLMs\u2019 gameplay. In the future, we plan to extend this work to multimodal LLMs to better capture the differences between human and LLM players. We hope that this study provides valuable insights into the behavior of LLM-based players in SDGs."}]}