{"title": "Vision Language Models Can Parse Floor Plan Maps", "authors": ["David DeFazio", "Hrudayangam Mehta", "Jeremy Blackburn", "Shiqi Zhang"], "abstract": "Vision language models (VLMs) can simultaneously reason about images and texts to tackle many tasks, from visual question answering to image captioning. This paper focuses on map parsing, a novel task that is unexplored within the VLM context and particularly useful to mobile robots. Map parsing requires understanding not only the labels but also the geometric configurations of a map, i.e., what areas are like and how they are connected. To evaluate the performance of VLMs on map parsing, we prompt VLMs with floorplan maps to generate task plans for complex indoor navigation. Our results demonstrate the remarkable capability of VLMs in map parsing, with a success rate of 0.96 in tasks requiring a sequence of nine navigation actions, e.g., approaching and going through doors. Other than intuitive observations, e.g., VLMs do better in smaller maps and simpler navigation tasks, there was a very interesting observation that its performance drops in large open areas. We provide practical suggestions to address such challenges as validated by our experimental results.", "sections": [{"title": "I. INTRODUCTION", "content": "A key to mobile robotics is a deep understanding of the geometric configuration of the world that mobile robots live in. As a result, many mobile robots need some forms of a map for localization, obstacle avoidance and navigation. To build such maps, the robots use a predefined data structure, e.g., an occupancy grid [1] or visual features [2], and then perform simultaneous localization and mapping (SLAM). Human beings have a long history of building and using maps. These days one can easily read floorplan maps of airports and shopping centers, and figure out a plan for navigation. By comparison, robots can hardly reach comparable competency in map reading and task planning. It is a non-trivial task for the robots due to the many labels in the map, their ambiguous associations to different areas, and complex geometric configurations. As a result, there is no existing method for addressing the map parsing problem, i.e., computing a navigation plan given a map image and a goal text, which motivated this research.\nFor complex navigation tasks, a robot needs to compute a task plan, i.e., a sequence of navigation actions, and continuous trajectories for realizing those actions. Example actions can be entering an area and going through a door. Extensive engineering efforts are needed to realize such navigation systems, from building the map itself to labeling areas of the map. At the same time, professional architectural drafters have generated blueprints that accurately reflect the geometric configurations, which unfortunately cannot be used by current robots. From an application perspective, this research aims to leverage the readily accessible floorplan maps in human environments to fulfill the robots' need of maps for navigation.\nVision language models (VLMs) are foundation models that learn from and reason about both images and texts, supporting a variety of downstream tasks from visual question answering to image captioning. VLMs have demonstrated impressive successes in a variety of applications [3], [4], [5] including those in robotics [6], [7]. We, for the first time, apply VLMs to the novel task of map parsing and evaluate its performance in navigation, a foundational problem in mobile robotics. Our approach is simple and intuitive. A floorplan map and a problem description, including the start and goal positions, are provided to a VLM, and the task is to compute a plan (i.e., an action sequence) to achieve the goal. Despite the straightforward idea, the results are surprisingly good. Navigation plans generated by VLMs which can require a sequence of nine actions are generated correctly up to 90% of the time on some floorplans.\nThe main contribution of this research includes the introduction of the map parsing problem, evaluations of VLMs on this problem, practical suggestions that paves the way for future research on VLM-based map parsing, and a complete demonstration of real-robot system.\nThere are limitations in this research that can be addressed in future work. One is that we still need to slightly edit the map image, such as thickening walls and removing architectural annotations, to produce the best performance. Such steps can be automated in future work. Another is that the robot needs to stand close and forward-facing when capturing the map image. This can be a challenge to small robots because most floorplan maps are placed at human heights. In this paper, we focus on highlighting the remarkable performance of VLMs on map parsing, and leave those topics to future work."}, {"title": "II. RELATED WORK", "content": "In this section, we discuss existing work in autonomous navigation for mobile robots, VLM prompting strategies, and integrating large pre-trained models in robotics. We highlight how our work differs from existing works in each of these categories."}, {"title": "A. Existing Map Representations and Navigation", "content": "Existing works demonstrating autonomous navigation on mobile robots usually require generating an occupancy grid [8], [9], [10], [11], or leveraging vision-based methods for simultaneous localization and mapping (Visual SLAM) [2], [12], [13], [14], [15]. While such map representations have proven to be effective for autonomous navigation tasks, generating an accurate map is oftentimes labor-intensive. For instance, in vision-based settings, the robot has limited knowledge of the global environment, either leading to lengthy exploration, or navigational commands from a human. In this work, we greatly reduce the effort of generating accurate maps while still leveraging detailed information of the environment through the use of existing, and potentially in situ, floor plans."}, {"title": "B. VLM Prompting Strategies", "content": "The output of a large pre-trained model largely relies on the way it is prompted. Strategies like chain-of-thought [16] and in-context learning [17] are leveraged on LLMs to improve performance. Similar to language prompts, image prompting strategies can improve VLM outputs. Set-of Mark prompting, which segments and labels objects in an image has shown to improve VLM responses [18]. In our work, we design a new visual prompting strategy designed for obtaining a spatial understanding of floor plan images."}, {"title": "C. Large Models in Robotics", "content": "To improve the common-sense reasoning capabilities of robots, large pre-trained LLMs and VLMs have been integrated in robots for various tasks like housekeeping [19], object rearangement [20], navigation [21], [22], [23], [24], [25], and quadruped locomotion [26], [27] to name a few. Alignment of the large model with the environment and robot's skills is critical to perform tasks in the real world [28]. Various approaches have demonstrated planning capabilities [29], [30] and uncertainty estimation [31] of large models. Various visual prompting strategies designed for different robotic manipulation tasks have also been developed [6], [7]. In line with recent works that leverage large models to incorporate common-sense knowledge on robots, we generate feasible navigation plans directly from an image of a floor plan."}, {"title": "III. METHODOLOGY", "content": "In this section, we present our approach for leveraging Vision-Language Models (VLMs) to interpret floor plan images and generate navigation instructions. We discuss the two key aspects of our approach: visual prompting strategy, and VLM-based plan generation."}, {"title": "A. Visual Prompting Strategy", "content": "Our study uses floor plan images that include detailed architectural layouts for various building types. To generate accurate plans from a VLM, it's important to design a visual prompt which can facilitate learning the structure of the floor plan. Unfortunately, raw floor plan images tend to contain various markings (i.e. windows, furniture symbols, non-uniform wall thickness) which can potentially confuse the VLM in understanding the general layout of the floor plan. Thus, we remove such markings to produce a cleaner map which can be better leveraged by a VLM.\nWe find that removing extraneous details from the floor plan is insufficient for the VLM to understand the map layout. In particular, we find the VLM has limited spatial awareness for sparsely labelled rooms with lots of open space, and near key decision points like doors and intersections. To alleviate this, we add duplicate room labels in open spaces and near doors and intersections. This provides the VLM with further guidance in understanding the general structure of the floor plan. We later demonstrate the importance of such additional labels in Section IV. In this work, we manually remove extraneous markings and add additional room labels for visual prompt construction. We leave automated floor plan editing as future work.\nMap enhancement with dense labeling: A key finding in this research is the importance of dense labeling in map enhancement. We apply a methodology in which rooms are labeled strategically at decision points (e.g., near doors or intersections). We later show that such label placement brings a considerable performance boost, which is particularly significant in complex environments where navigation paths can involve multiple rooms and transitions. This process can be automated (though not in this paper) to make it scalable for broader applications without manual intervention."}, {"title": "B. VLM-Based Plan Generation", "content": "In our VLM-based framework, as shown in Fig. 2, VLMs formulate navigation plans based on a floor plan image and a text prompt. This method leverages an instruction-based text prompting strategy, which involves providing the VLM with explicit and detailed guidelines for the navigation task, along with the floor plan image. We now elaborate on the prompting strategy and the resulting output format.\nThe text prompt given to the VLM is shown in Fig. 3. The prompt explicitly defines the starting point and the destination. This allows the VLM to understand the required navigation path and objectives clearly. It provides detailed instructions on interpreting the floor plan and managing door interactions. These guidelines include specific protocols for door operations and decision-making processes.\nA key aspect of this prompt is the request for all door and room connections. By generating this information at the start, we speculate that the VLM integrates it with the floor plan image to produce an accurate navigation sequence. We believe this step helps the VLM better understand the spatial relationships in the map, leading to accurate navigation path planning.\nBased on the text prompt and floor plan image, the VLM generates a sequence of actions required to navigate from the initial to goal location. This sequence includes specific steps, e.g., approaching, opening, and passing through doors. The output from the VLM is a detailed sequence of actions formatted as follows:\n\u2022 ApproachDoor(x): Move in front of door x.\n\u2022 OpenDoor (x): Open door x.\n\u2022 GoThrough(x): Move through open door x to the location on the other side.\nThe final navigation plan is output in JSON format, specifying each action. This structured format facilitates easy interpretation and execution of the navigation instructions. This plan is then parsed and executed by the robot."}, {"title": "IV. EXPERIMENTS", "content": "In order to evaluate whether the VLM produces accurate navigation plans, we design and run experiments over a dataset of floor plans. The experiments are designed to measure the effect of floor plan size, task difficulty, and label density on the VLM's plan accuracy."}, {"title": "A. Experimental Setup", "content": "Our study uses floor plan images from a publicly available dataset CVC-FP [32], which includes detailed architectural floor plans for various building types. Three floor plans were randomly selected from those that contain 9-11 rooms and clear labels. Those maps are referred to as \"original maps\" and are shown in Fig. 4. Our experiments use two state-of-the-art VLMs: GPT-40 [33] and Claude-3.5 Sonnet [34].\nFor each floor plan, we generate five pairs of start and goal locations. To account for the stochastic nature of VLM responses, we run each VLM ten times per navigation task, resulting in a total of 50 navigation trials per floor plan. We evaluate performance based on the correctness of the generated plans. A plan is considered correct if it uses only those actions defined in the text prompt, includes feasible actions, and leads a sequence of transitions from the start location to the goal. Example infeasible actions include navigating to a room that is not connected to the current room and opening a door that belongs to a distant room.\nOur experimental design focuses on three key dimensions:\n1) Map Size: We hypothesize that increasing the map size will result in a decrease in VLM's map parsing performance. This hypothesis is based on the assumption that larger maps introduce greater complexity.\n2) Task Difficulty: We hypothesize that when the start and end locations are far away (based on number of rooms required to traverse), the VLMs will have a low accuracy in map parsing and plan generation.\n3) Label Density: We hypothesize that a densely labelled floor plan map will facilitate accurate navigation plan generation from VLMs.\nNext, we describe our experiment setup for evaluating each of the three hypotheses.\n1) Map Size: To examine the impact of map size on navigation performance, we developed two types of maps: Original Maps that are enhanced versions of the maps selected from the CVC-FP dataset, and Doubled Maps that were created by connecting two copies of an original map through an additional door. Fig. 5 presents an example of a doubled map. A door denoted as D10 is added to establish connectivity, thereby forming the final doubled map.\n2) Task Difficulty: We design two types of navigation tasks to evaluate model performance across two levels of difficulty. Easy Tasks are the navigation tasks that can be completed by navigating from Room A to Room B without traversing any intermediate rooms. Hard Tasks are those that require a robot to navigate through at least two intermediate rooms before the goal can be achieved. As a result, an optimal solution of hard tasks will involve four rooms in total. The increased complexity introduces more decision points and possible paths, producing a more challenging task.\n3) Label Density: We evaluated the impact of label density on model performance by implementing two labeling schemes. Sparse-Labeled maps are those where each room was labeled with a single label, usually placed at the center. This minimalistic approach offered fewer cues for the model to base its navigation decisions on. Dense-Labeled maps include multiple labels for each room, where the placement is described in Section III-A."}, {"title": "B. Experimental Results", "content": "1) Hypothesis 1 (Map Size): The first hypothesis explores how the size of the map influences the model's accuracy. We compared the performance between original maps and doubled maps over hard tasks to assess the same.\nThe results indicate that accuracy decreases as map size increases in the overall domain analysis, with the VLMs performing better in smaller maps. The difference is significant, as a T-test on trials with GPT-40 revealed a drop in accuracy (t = 6.13, p < 0.0001). This supports our hypothesis that accuracy decreases as map size increases. The results are reported in Fig. 6 and 7. Both GPT and Claude exhibit a similar pattern of performance decline with larger maps.\n2) Hypothesis 2 (Task Difficulty): The second hypothesis investigates how task difficulty impacts accuracy. Easy tasks involve straightforward navigation between rooms, while hard tasks, require traversing multiple intermediate rooms, making the tasks more complex. We compared the performance between doubled maps over easy tasks and hard tasks.\nThe accuracy of the GPT-40 models generally decreased with increased task difficulty, as more complex tasks led to lower accuracy in two out of the three maps. For example, a T-test on doubled_map_2 indicated a drop in accuracy with t = 2.88 and p = 0.0047. The results from both VLMs generally support our hypothesis that more complex tasks lead to lower accuracy, as navigating through multiple rooms introduces additional challenges. One example task is detailed in Table I.\n3) Hypothesis 3 (Label Density): The third hypothesis examines the impact of label density on accuracy. This is evaluated by comparing the performance of original maps from sparse-label and dense-label datasets over hard tasks. Dense labels provide more contextual information, while sparse labels offer minimal context, making the navigation task more challenging.\nThe accuracy of the GPT-40 models significantly improved with dense labels compared to sparse labels across all maps. For instance, a T-test on original_map_1 showed a significant improvement with t = 10.72 and p < 0.0001. This result indicates that dense labels substantially enhance accuracy, supporting our hypothesis that dense labels provide crucial contextual information, enabling the models to navigate more effectively. The improvement in accuracy with dense labels was consistent across all maps for GPT-40, highlighting the importance of label density in successful navigation.\nThese findings suggest that our approach can effectively handle a range of navigation challenges while demonstrating the critical importance of label density, task difficulty, and map size in determining overall performance."}, {"title": "V. HARDWARE DEMONSTRATION", "content": "Our VLM-based planning and navigation system is demonstrated on a DEEPRobotics Lite3 quadruped robot. An image of the floor plan of a building on a college campus is captured from the robot's camera. This image is then edited as described in Section III to make it suitable for the VLM query. Another version of the raw image with space whited out is directly used for robot localization, as seen in Fig. 8. The VLM is then queried with the edited photo, and a navigation plan consisting of a sequence of navigation and door opening actions is generated. The robot executes this navigation plan, to move from the robotics lab (room N09), to a classroom (room T01). The robot localizes itself directly on a grayscale version of the floor plan, without the need for generating an accurate occupancy grid via SLAM. The robot successfully avoids the obstacles not present on the floor plan, asks for doors to be opened as needed, and achieves the desired navigation goal."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In this work, we introduce a novel task, named map parsing, that is unexplored in the VLM literature while pointing to the foundation of mobile robotics. We demonstrate remarkable performance of two VLMs on map parsing tasks, as applied to robot navigation. We develop a VLM-based planning system that generates navigation plans directly from a floor plan image and validated our approach through experiments on a floor plan dataset and on hardware, demonstrating the feasibility of VLM-driven navigation across different environments.\nWhile our results show that this approach is viable, several challenges remain, opening up avenues for further research. The process of modifying floor plans to optimize VLM performance is still a manual task. Future research could investigate leveraging segmentation models [35], [36] or other techniques to automate map refinement. Also, we can investigate strategies to improve the VLM's ability to handle larger and more complex maps, e.g., outdoor environments [37]. Another direction is that VLMs (and all transformer based autoregressive models) have a host of known issues, such as hallucinations and biases [38], [39], [40], [41] that can be addressed in robot planning. We anticipate that this work will inspire further studies that expand upon the ideas on VLM-based map parsing in this paper."}]}