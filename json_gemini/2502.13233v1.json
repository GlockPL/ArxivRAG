{"title": "SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question Answering?", "authors": ["Yucheng Shi", "Tianze Yang", "Canyu Chen", "Quanzheng Li", "Tianming Liu", "Xiang Li", "Ninghao Liu"], "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in general domains but often struggle with tasks requiring specialized knowledge. Conventional Retrieval-Augmented Generation (RAG) techniques typically retrieve external information from static knowledge bases, which can be outdated or incomplete, missing fine-grained clinical details essential for accurate medical question answering. In this work, we propose SearchRAG, a novel framework that overcomes these limitations by leveraging real-time search engines. Our method employs synthetic query generation to convert complex medical questions into search-engine-friendly queries and utilizes uncertainty-based knowledge selection to filter and incorporate the most relevant and informative medical knowledge into the LLM's input. Experimental results demonstrate that our method significantly improves response accuracy in medical question answering tasks, particularly for complex questions requiring detailed and up-to-date knowledge.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have shown proficiency in handling general-domain question answering tasks (Brown et al., 2020). However, they often face challenges when dealing with specialized fields that require up-to-date or domain-specific knowledge, such as medical question answering (Jin et al., 2021; Pal et al., 2022). The limitation arises because the LLM parameters remain fixed after training, preventing the model from incorporating new medical knowledge essential for precise decision-making.\nRetrieval-Augmented Generation (RAG) methods have been proposed to address this issue by incorporating external knowledge sources into the LLMs' input, thereby enhancing performance with additional context (Guu et al., 2020; Wang et al.,"}, {"title": "2 Methodology", "content": "In this section, we introduce SearchRAG, an innovative RAG framework aimed at enhancing LLMs' ability to effectively use search engines for knowledge retrieval, especially when addressing complex and specialized medical questions."}, {"title": "2.1 Problem Formulation and Overview", "content": "Retrieval-Augmented Generation (RAG) integrates external knowledge into the input of LLMs to enhance their responses (Guu et al., 2020; Wang et al., 2021). Formally, given an input prompt $x$, RAG retrieves a set of knowledge snippets $K = {K_1, K_2, ..., K_n}$ using a retrieval function $f_{ret}: K = f_{ret}(x)$, and generates a response $y$ using model $f_{LLM}$ conditioned on both the input and the retrieved knowledge:\n$y = f_{LLM}(x, K)$.\nHowever, when dealing with complex and verbose medical questions, such as the one shown in Figure 2, directly using $x$ for retrieval can lead to suboptimal results. This is because such questions often include extraneous details that can mislead search engines and result in the retrieval of irrelevant information (Jin et al., 2020; Shi et al., 2023; Xiong et al., 2024a). To overcome these limitations, our proposed SearchRAG modifies the RAG framework by introducing two key components:\n\u2022 A Synthetic Query Generation Module that generates candidate queries by transforming original questions into search-engine-friendly form."}, {"title": "2.2 Synthetic Query Generation Module", "content": "Medical questions are often too complex to be directly used as search engine queries. To address this challenge, SearchRAG proposes to use synthetic queries generated by LLMs to improve the retrieval of knowledge pertinent to answering the original question. The need for synthetic queries stems from two fundamental challenges in medical question answering:\n\u2022 Complexity of Medical Questions: Medical questions often contain specialized terminology and multi-hop clinical logic (Jin et al., 2020). A single condition may be referred to by different medical terms (e.g., \"myocardial infarction\" and \"heart attack\"), making it challenging to match relevant content comprehensively. Additionally, critical retrieval keywords may not appear explicitly in the original question, which hinders effective retrieval.\n\u2022 Constraints of Search Engine: Existing search engines employ algorithms and ranking systems that favor well-structured, concise queries with clear intent (Google, 2022). Medical questions usually fail to meet these requirements, thus preventing the retrieval of relevant medical content.\nOur solution leverages the same LLM we aim to enhance through RAG to generate synthetic queries, eliminating the need for additional models. We design a specialized prompt template $p$ (detailed in the Appendix) that guides the LLM to decompose and transform the original medical question into search queries. More specifically, the LLM first analyzes the medical question to extract key clinical entities, reformulates them using standardized medical terminology, and finally structures the queries with search-friendly syntax. Given an original prompt x and our template p, we generate a set of synthetic queries through repeated sampling:\n${q_1, q_2,..., q_m} = \\cup_{i=1}^{m} f_{LLM}(x, p)$.\nHere, we employ a high-temperature sampling strategy to generate a large and diverse set of candidate queries ${q_1,q_2,...,q_m}$. Example queries for a question x are shown in Figure 2 with {q1:\"ototoxic effects of platinum-based drugs\"; q2: \u201cmechanisms cisplatin hearing loss\"; q3: \u201cProteasome inhibitor drug effects on inner ear\"}. These queries contain concise keywords suitable for search engines and cover multiple aspects of question x. Among them, q2 is the most informative one\u00b9. We provide more details in Section 3.3.3"}, {"title": "2.3 Align Search Engines to LLMs via Uncertainty-Based Query Selection", "content": "While we generate a large number of synthetic queries, identifying the most effective ones is a non-trivial task. To address this, we propose to leverage the LLM's internal uncertainty as a reward signal to guide query selection. Specifically, we assess the uncertainty reduction associated with each retrieved snippet, treating it as a measure of information gain (Shi et al., 2024). Queries that retrieve knowledge leading to greater uncertainty reduction are prioritized, ensuring that only the most informative snippets contribute to RAG.\nOur approach aims to align search engine to LLMs by helping LLMs effectively use web search to retrieve knowledge to answer given questions. By using uncertainty as a reward, our system selects queries that maximize knowledge utility, refining the alignment process on a per-query basis. Unlike tuning-based alignment (Ma et al., 2023), our method optimizes in inference-time without modifying model parameters, making it robust to diverse retrieval sources and evolving topics.\nFor each query qi, our search engine function $f_{SE}$ retrieves structured knowledge snippets:\n$K_i = f_{SE}(q_i), i = 1,2,...,m$.\nThe retrieved snippets are concise extracts from search engine results, including titles and short descriptions for each webpage. We provide a case study of the search engine results in the Appendix. Each retrieved snippet augments the original prompt through concatenation: $x'_i = [x; K_i]$.\nWe estimate the LLM's uncertainty in generating responses based on these augmented inputs using Shannon entropy (Cover, 1999). Let Y be the random variable representing possible responses y generated by LLMs. Since computing the full response distribution is infeasible (Shi et al., 2024;"}, {"title": "2.4 Integration into the RAG Framework", "content": "The complete integration of SearchRAG's components into the RAG framework operates as Algorithm 1. The framework begins by generating diverse search queries through repeated sampling from the LLM (line 3). Next, each query is input into a search engine to retrieve knowledge (line 5). The system then evaluates the retrieved content by measuring how effectively each snippet reduces the model's prediction uncertainty (line 6). Finally, confidence-filtered knowledge is aggregated for the final answer generation (lines 8, 9), ensuring that only clinically relevant information is utilized to assist the diagnosis."}, {"title": "3 Experiments", "content": "Our experimental evaluation addresses three key research questions (RQs): RQ1: How does SearchRAG perform compared to existing methods on medical QA tasks? RQ2: How effective is our uncertainty-based knowledge selection strategy in improving performance? RQ3: How does scaling the number of synthetic queries affect the model's performance? We evaluate through comparative benchmarks on medical datasets, focusing on quantitative performance metrics and qualitative analysis of retrieved knowledge."}, {"title": "3.1 Experimental Settings", "content": "We evaluate our approach against four baseline methods: one non-RAG method and three RAG-based methods. The non-RAG baseline is Chain-of-Thought (CoT) prompting (Wei et al., 2022). For RAG-based methods, we compare against: MedRAG using textbooks as the knowledge source with MedCPT retriever, MedRAG using PubMed as the knowledge source with MedCPT retriever, and i-MedRAG using textbooks as the knowledge source with MedCPT retriever (Xiong et al., 2024a,b). A comparison of the different knowledge sources is shown in Table 1. Additional implementation details, including hyperparameter choices,"}, {"title": "3.2 RQ1: Main Results", "content": "Table 2 demonstrates the effectiveness of SearchRAG across model scales and medical QA datasets. We have four findings:\nFirst, conventional RAG approaches exhibit critical sensitivity to knowledge source selection, particularly in smaller models. Our experiments reveal that PubMed-based retrieval degrades 8B model performance by 9.11% on MedMCQA compared to chain-of-thought baselines, while textbook-based retrieval provides only marginal gains (+0.62% on MMLU_Med). This validates our core hypothesis that static knowledge bases struggle to provide reliable augmentation, as noted in our analysis of"}, {"title": "3.3 Ablation Studies", "content": ""}, {"title": "3.3.1 RQ2: Effectiveness of Knowledge Selection", "content": "To validate our uncertainty-based knowledge selection mechanism, we conduct experiments on 350 randomly sampled instances from each dataset using both model variants. Contrary to the intuition that unfiltered knowledge provides richer context, Table 3 reveals that selective filtering consistently improves performance, particularly for smaller models.\nThere are two key insights as follows: (1) The LLaMA 8B model shows 4-7% absolute gains across all datasets, indicating smaller models' vulnerability to irrelevant information: unfiltered knowledge introduces distracting noise that misleads reasoning. (2) While the LLaMA 70B model exhibits more robustness, it still benefits from filtering, suggesting even powerful models cannot fully compensate for low-quality context. This demonstrates the effectiveness of our uncertainty-based selection mechanism: By selectively presenting only the most relevant information, we help models avoid distraction from excessive content while maintaining useful knowledge signals."}, {"title": "3.3.2 RQ3: Impact of the Number of Total Generated Queries", "content": "We conducted an evaluation to understand how the number of generated queries affects model performance, using 350 randomly sampled instances. The configurations tested included using 0 (the original question), 4, 16, and 32 generated queries. Figure 3 illustrates the impact of varying the number of queries on performance.\nOur analysis reveals two key findings: First, using only the original questions (0 query) resulted in limited RAG effectiveness, with performance even degrading compared to the non-RAG baseline, achieving only 62.9% on the MedQA. This suggests unmodified medical questions fail to effectively leverage search engines for retrieving relevant information, and the low performance confirms that these medical questions do not have direct matches in existing online knowledge sources.\nSecond, introducing synthetic query generation led to improvements across all datasets. Performance scaled with the number of generated queries, peaking with 32 queries. This clear correlation between query count and accuracy indicates that a larger query set helps capture more diverse and relevant knowledge to enhance the RAG process."}, {"title": "3.3.3 Case Study", "content": "This case study demonstrates how SearchRAG handles a clinical question about oral contraceptive effects. The scenario involves a 17-year-old patient prescribed contraceptive pills, examining which condition's risk is decreased by this medication. Through high-temperature sampling, the model generates diverse search queries, retrieving evidence that oral contraceptives reduce ovarian cancer risk while increasing risks for cervical and breast cancers (See Facts [1][2][7]). While the system initially incorrectly suggested cervical cancer as having reduced risk, the retrieved facts enabled the model to correct its response to ovarian cancer. This case shows how SearchRAG's query generation and knowledge selection can transform"}, {"title": "4 Related Work", "content": "While large language models excel at question answering, they struggle with specialized medical knowledge (Shi et al., 2023). Traditional RAG systems have been proposed to mitigate these limitations by integrating external knowledge sources into the LLMs' input, thereby enhancing their ability to generate accurate and contextually relevant responses (Brown et al., 2020; Guu et al., 2020). Nonetheless, these early RAG approaches predominantly relied on retrieving information from structured databases or knowledge graphs, which can be insufficient for handling the complexity and nuanced requirements of specialized medical queries (Jin et al., 2020; Xiong et al., 2024a).\nRecent advancements in RAG systems have focused on improving the accuracy and relevance of retrieved medical information. Xiong et al. (Xiong et al., 2024b) propose i-MedRAG, which uses iterative follow-up questions to help LLMs better"}, {"title": "5 Conclusion", "content": "In this paper, we presented SearchRAG, a novel retrieval-augmented generation framework designed to enhance large language models' performance in complex medical question-answering tasks. Our approach integrates synthetic query generation and uncertainty-based knowledge selection to optimize the retrieval of relevant information from search engines. Through extensive experiments, we demonstrated that SearchRAG significantly improves the accuracy and coherence of the model's responses compared to traditional RAG approaches. These findings suggest that our framework can effectively address the limitations of conventional RAG systems."}, {"title": "6 Limitations", "content": "One potential limitation of our approach is the reliance on search engines for knowledge retrieval. The results returned by search engines may sometimes include incorrect or unreliable information sources. In practical applications, it is crucial to have domain experts review the retrieved content to ensure its accuracy. Alternatively, restricting the search to a curated list of trusted websites can help mitigate this issue and improve the reliability of the information used by the model."}, {"title": "7 Ethical Impact", "content": "This research utilizes the LLaMA foundation model (Dubey et al., 2024), operating within the scope of its academic licensing agreement. Our implementation strictly adheres to the academic-use provisions specified in the license, with all applications limited to scholarly research purposes. The study draws upon three medical domain datasets: MedQA (Jin et al., 2021), MMLU_Med (Hendrycks et al., 2020), and MedMCQA (Pal et al., 2022), each employed in accordance with their respective usage guidelines and data governance frameworks. We have conducted thorough reviews to ensure compliance with data protection protocols, confirming the absence of personally identifiable information such as patient names or unique identifiers. Furthermore, our data processing protocols have verified that the content is appropriate and free from inappropriate material, maintaining high standards of research ethics and data integrity. Additionally, we utilized ChatGPT (Hurst et al., 2024) to assist with grammatical refinements during the writing process."}, {"title": "A Experimental Details", "content": "This section provides additional details about our experimental settings, dataset sources, retriever configurations, and prompt design used throughout the paper."}, {"title": "A.1 Model Configurations and Hyperparameters", "content": "Base LLMs. We use two variants of the LLaMA 3.1 model (Dubey et al., 2024), at 8B and 70B parameter scales. Both variants are instruction-tuned but not further fine-tuned for our experiments. For the 8B model, we employ bfloat16 for inference. For the 70B model, we employ INT4 quantization to reduce memory usage.\nInference Setup. All experiments run on A6000 GPUs (48GB memory). The average GPU hours for our SearchRAG is roughly 0.018 hours/per medical question. Unless noted otherwise, we set the maximum generation length to 512 tokens, with a do_sample=False. For synthetic query generation, we increase the temperature up to 2.0 to promote query diversity. We generate up to 32 candidate queries for each input question; in ablation studies, we vary this number to assess its impact."}, {"title": "A.2 Datasets", "content": "MedQA. We use the benchmark from Jin et al. (2021), which contains multiple-choice questions from US medical licensing exams. We follow the standard split of 1,273 questions for evaluation. Each question has four answer choices (A-D).\nMMLU_Med. We use the medical subset of the Massive Multitask Language Understanding (MMLU) benchmark (Hendrycks et al., 2020), consisting of six biomedical subject areas (anatomy, clinical knowledge, professional medicine, human genetics, college medicine, and college biology), for a total of 1,089 test questions. Each question also has four multiple-choice options.\nMedMCQA. We adopt the dataset introduced by Pal et al. (2022), which consists of 4,183 medical multiple-choice questions from real-world medical entrance exams. Each question has four options, and we follow the official evaluation split for testing."}, {"title": "A.3 Knowledge Sources and Retriever Setup", "content": ""}, {"title": "A.3.1 PubMed Subset", "content": "PubMed is a comprehensive repository of biomedical and life sciences articles (Lu, 2011). In total, it indexes over 36 million articles. For our study, we follow Xiong et al. (2024a) and utilize a curated subset of around 23.9 million entries where each entry has a valid title and abstract."}, {"title": "A.3.2 Textbooks Collection", "content": "We also incorporate a set of 18 widely-used medical textbooks that cover fundamental subjects relevant for medical board examinations (Jin et al., 2020). These textbooks offer a broad range of clinically relevant facts and explanations."}, {"title": "A.4 Retriever and Search Engine Details", "content": "Retriever Implementation. For PubMed and Textbooks, we follow a standard dense retrieval approach. We encode the document chunks using"}, {"title": "A.5 Prompt Design", "content": "Synthetic Query Generation Prompt. We design a specialized template to elicit concise, search-optimized queries from the LLM. Below is a simplified illustration:"}, {"title": "A.6 Additional Notes and Practical Considerations", "content": "Ethical, Privacy, and Data Protection Considerations. A critical concern when handling medical questions is avoiding patient data exposure. Directly inputting raw patient information into language models for retrieval risks potential data leakage and privacy violations. Our query rewriting approach addresses this by extracting only disease-related keywords and conceptual terms from the original question, rather than processing sensitive patient details. When retrieving web snippets, we further minimize risks by using only short search engine excerpts (titles and summaries) rather than full documents. This dual protection - question sanitization through rewriting and restricted snippet usage - prevents sensitive health information from entering the model's processing pipeline while avoiding unintended use of copyrighted materials."}, {"title": "B Case Study", "content": ""}]}