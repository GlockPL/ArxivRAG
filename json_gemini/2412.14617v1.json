{"title": "How good is GPT at writing political speeches for the White House?", "authors": ["Jacques Savoy"], "abstract": "Using large language models (LLMs), computers are able to generate a written text in response\nto a us er request. As this pervasive technology can be applied in numerous contexts, this study\nanalyses the written style of one LLM called GPT by comparing its generated speeches with\nthose of the recent US presidents. To achieve this objective, the State of the Union (SOTU)\naddresses written by Reagan to Biden are contrasted to those produced by both GPT-3.5 and\nGPT-4.0 versions. Compared to US presidents, GPT tends to overuse the lemma \u201cwe\u201d and\nproduce shorter messages with, on average, longer sentences. Moreover, GPT opts for an\noptimistic tone, opting more often for political (e.g., president, Congress), symbolic (e.g.,\nfreedom), and abstract terms (e.g., freedom). Even when imposing an author's style to GPT, the\nresulting speech remains distinct from addresses written by the target author. Finally, the two\nGPT versions present distinct characteristics, but both appear overall dissimilar to true\npresidential messages.", "sections": [{"title": "1 Introduction", "content": "With the development of large language models (LLMs) (Zhao et al., 2023), generative Al\ndemonstrates its capability to generate a short text in response to a user request. Currently, such\napplications are freely available and can help users produce various types of writing (e.g., e-\nmail, CV, short letter, etc.). From this perspective, this study investigates the writing style of\nGPT when asked to generate State of Union addresses for a president. Annually expressed in\nfront of Congress, these speeches explain the world situation and political agenda of the occupant\nof the White House. The main objective is to inform and persuade the audience that the propo-\nsitions and actions of the president are the most appropriate. To reach such an objective, the\nstyle and rhetoric play an important role in reinforcing the president's words.\nBased on recent developments in automated text analysis designed by communication and psy-\nchological scholars (Jordan, 2022), this study analyses the style and rhetoric of six US presidents\n(Reagan, Clinton, Bush, Obama, Trump, and Biden) as well as that of two GPT versions (GPT-\n3.5 and GPT-4.0). In this study, rhetoric is defined as the art of effective and persuasive\nspeaking, and the way to adopt a tone to motivate an audience. An author's style is evaluated"}, {"title": "2 State of the Art", "content": "Numerous studies have been published on authorship attribution and on recognising author de-\nmographics characteristics (e.g., gender, age, social status, native language, etc.) (Kreuz, 2023).\nOther stylometry studies have additionally been performed on the detection of plagiarism or fake\ndocuments, the identification of suspects in criminology (Olsson, 2018), the determination of\ntext genre, and even the dating of a document. To resolve these questions, various natural lan-\nguage processing models have been applied by scientists from different domains such as com-\nputer science (Savoy, 2020), (Karsdorp et al., 2021), linguistics (Crystal, 2019), (Yule, 2020),\npsychology (Pennebaker et al., 2014), (Jordan, 2022) and communication studies (Hart et al.,\n2013), (Hart, 2020).\nThe main objective of this study is to analyse the style and rhetoric of true political speeches and\nto compare them with those automatically generated by GPT. This emerging technology is based\non LLM (large language model) technology grounded on a deep learning architecture (Goodfel-\nlow et al., 2016), which is based on a sequence of transformers with an attention mechanism\n(Vaswami et al. 2017). The most important notion to understand LLM is the following: given a\nshort sequence of tokens (e.g., words or punctuation symbols), the computer is able to automat-\nically supply the next token. More precisely, knowing four tokens, the model must first deter-\nmine the list of possible next tokens to complete the given sequence (Wolfram, 2023). For"}, {"title": "3 Corpus Overview", "content": "To ground our conclusions on a solid basis, the same text genre has been selected: namely,\nwritten speeches given in the same context, to achieve similar objectives, and written in the same\ntime period. To compare the style of recent US presidents with messages created by a machine,\nwe asked GPT API (Application Programming Interface) to generate the State of the Union\n(SOTU) addresses for six presidents, namely Reagan, Clinton, Bush, Obama, Trump, and Biden."}, {"title": "4 Stylometric Analysis", "content": "As a first stylometric measure, one can focus on the language complexity that all political leaders\ntend to reduce. For example, L. B. Johnson (presidency: 1963\u20131969) specifies to his ghostwrit-\ners, \u201cI want four-letter words, and I want four sentences to the paragraph.\u201d (Sherrill, 1967). The\ncomplexity of the language could be measured by the mean number of letters per words. In this\ncase, the larger the mean, the higher the language complexity.\nAs an additional characteristic, one can count the percentage of words composed of six letters or\nmore, defined as big words (BW) in the English language. One can observe, for example, that\ndepending of the length of words, some are easier to understand than others. It is the difference\nbetween \"ads\u201d and \u201cadvertisements\u201d, for example, or \u201cdesks\u201d and \u201cfurniture\u201d. Such a relation-\nship between complexity and word length is clearly established:\n\"One finding of cognitive science is that words have the most powerful effect on our\nminds when they are simple. The technical term is basic level. Basic-level words tend\nto be short. Basic-level words are easily remembered; those messages will be best re-\ncalled that use basic-level language.\" (Lakoff & Wehling, 2012)\nFinally, one can evaluate the mean sentence length (MSL). It has been observed that long sen-\ntences tend to render the speech more complex to understand."}, {"title": "5 Psychological and Emotional Analysis", "content": "A psychological and emotional analysis of political speeches can be grounded on LIWC7. This\ntext-based analysis system is built around several wordlists according to syntactical, emotional\nor psychological categories. The main hypothesis is to assume that the words serve as guides to\nthe way the author thinks, acts, or feels (Jordan, 2022). In LIWC, categories may match gram-\nmatical categories such as personal pronouns, as well as broader ones (e.g., verbs), or more spe-\ncific ones (verbs in the past tense, auxiliary verbs). On a semantics level, the LIWC defines\npositive emotions (Posemo) (e.g., happy, hope, peace), or negative ones (Negemo) (e.g., fear,\nblam*8). With these categories, the emotional aspect (optimism or pessimism) of a speaker can\nbe evaluated. Presidents (or prime ministers) tend to voice positive words more frequently to\nappeal to the audience and to persuade the public. In particular, populist leaders more often\nemploy emotional terms to incite strong sentiments in the population, usually to obtain a larger\nmedia coverage (Obradovi\u0107 et al., 2020), (Hart, 2020), (Savoy & Wehling, 2022).\nThe category Cogproc contains terms related to self-reflection (e.g., think, refer*) and causal\nwords (e.g., cause, understand). This measure corroborates with an active thinking and narrative\ntone (Tausczik & Pennebaker, 2010). Under Achieve (e.g., plan, win, lead*, etc.), one can eval-\nuate the confidence of the author to resolve or to propose a solution to a problem in a successful\nway.\nAs a second approach, Hart et al. (2013) have developed the DICTION system, which groups\ndifferent wordlists specifically created to analyse political messages. For example, in the Fa-\nmiliarity category (e.g., a, at, to, with, etc.), one can see words that occur in everyday expres-\nsions, and that correspond to terms which are easily understood (Ogden, 1968). Such an"}, {"title": "6 Intertextual Distance", "content": "To evaluate more globally the similarity between all presidents and both GPT versions, an\nintertextual distance between all pairs of texts can be computed (Labb\u00e9, 2007). The computation\nof this measure between Text A and Text B is defined according to the entire vocabulary.\nEquation 1 specifies this measure with $n_A$ indicating the length of Text A (in number of tokens),\nand $tf_{i,A}$ denoting the absolute frequency of the ith term (for $i = 1, 2, ..., m$). The value m\nrepresents the vocabulary length. Usually, both texts do not have the same length, so we may\nassume that Text B is the longest. To reduce the longest text to the size of the smallest, each of\nthe term frequencies (in our case $tf_{i,B}$) is multiplied by the ratio of the two text lengths, as\nindicated in the second part of Equation 1."}, {"title": "7 Conclusion", "content": "Some experiments performed in this study demonstrate that both GPT models can generate po-\nlitical speeches sharing some similarities with real State of the Union (SOTU) addresses. In\naddition, the newest version (GPT-4.o) exposes distinct characteristics compared to GPT-3.5.\nFor example, the messages generated by GPT-4.0 are significantly longer: on average, 645 to-\nkens vs. 494 for GPT-3.5.\nThe two models share some common features, such as a higher language complexity compared\nto true presidents. In this regard, GPT generates longer words (the mean is 4.96 letters per word),\nwith a higher percentage of big terms (on average, 39%), and longer sentences (20.76). Among\npresidents, Biden tends to present the lowest language complexity, with the shortest words and\nsentences.\nWhen focusing on personal pronouns, both GPT versions opt for a large percentage of we-words\n(we, us, our) with few other pronouns (e.g., the third singular pronouns occur very rarely). Even\nif the increased frequency of we-words is a characteristic of political leaders in power, GPT\nemploys them more often than true presidents. Between presidents, Biden presents a distinct\nfigure with a relatively high number of I-words and second-person pronouns.\nWhen inspecting emotional terms, both GPT models employ almost only positive terms (on av-\nerage, 7.3%), leading to an optimist tone. True presidents also favour positive sentiments (on\naverage 4.3%), along with some negative ones (2.1%). Among presidents, Bush writes with the\nhighest number of emotional terms (on average, 4.99% are positive, 3.09% negative). This fea-\nture can be explained by the war in Iraq and against terrorists. Again, Biden uses the lowest\npercentage of positive terms (3.33%), and a low number of negative ones (1.74%).\nWhen considering other categories, the two GPT versions opt for a larger percentage of Achieve\n(on average, 4.9%), Symbolism (5.3%), and Politics (4.7%) terms. This can be explained by the\nwish to anchor the speech in political parlance (e.g., nation, Congress, America) and to underline\nthe results or actions already planned (e.g., win, plan). For the presidents, the average percent-\nages are significantly lower (Achieve: 2.8%, Symbolism: 3.8%, Politics: 3.7%).\nWhen considering other psychological measurements, both GPT models expose a clear lan-\nguage, belonging to a high-status person (Clout), but with a low value in authenticity. The re-\nsulting tone could appear authoritative and distant. Among presidents, Biden opts for a less\noptimistic and less confident tone that could also appears as being more honest."}]}