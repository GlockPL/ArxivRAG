{"title": "AMPO: Active Multi Preference Optimization for Self-play Preference Selection", "authors": ["Taneesh Gupta", "Rahul Madhavan", "Xuchao Zhang", "Chetan Bansal", "Saravan Rajmohan"], "abstract": "Multi-preference optimization enriches language-model alignment beyond pairwise preferences by contrasting entire sets of helpful and undesired responses, enabling richer training signals for large language models. During self-play alignment, these models often produce numerous candidate answers per query, making it computationally infeasible to include all of them in the training objective. We propose Active Multi-Preference Optimization (AMPO), which combines on-policy generation, a multi-preference group-contrastive loss, and active subset selection. Specifically, we score and embed large candidate pools of responses, then pick a small but informative subset-covering reward extremes and distinct semantic clusters for preference optimization. The resulting contrastive training scheme identifies not only the best and worst answers but also subtle, underexplored modes crucial for robust alignment. Theoretically, we provide guarantees of expected reward maximization using our active selection method. Empirically, AMPO achieves state-of-the-art results on AlpacaEval with Llama 8B. We release our datasets at huggingface/MPO.", "sections": [{"title": "1. Introduction", "content": "Preference Optimization (PO) has become a standard approach for aligning large language models (LLMs) with human preferences (Christiano et al., 2017; Ouyang et al., 2022; Bai et al., 2022). Traditional alignment pipelines typically rely on pairwise or binary preference comparisons, which may not fully capture the subtleties of human judgment (Rafailov et al., 2024; Liu et al., 2024a; Korbak et al., 2023). As a remedy, there is increasing interest in multi-preference methods, which consider entire sets of responses when providing feedback (Cui et al., 2023; Chen et al., 2024a; Gupta et al., 2024b). By learning from multiple \"good\" and \"bad\" outputs simultaneously, these approaches deliver richer alignment signals than purely pairwise methods. At the same time, an important trend in alignment is the shift to on-policy or \"self-play\u201d data generation, where the policy learns directly from its own distribution of outputs at each iteration (Chen et al., 2024b; Kumar et al., 2024; Wu et al., 2023; 2024). This feedback loop can accelerate convergence ensuring that the training data stays relevant to the model's behavior.\nHowever, multi-preference alignment faces a serious bottleneck: modern LLMs can easily generate dozens of candidate responses per query, and incorporating all of these into a single training objective can become computationally infeasible (Askell et al., 2021). Many of these sampled responses end up being highly similar or near-duplicates, providing limited additional information for gradient updates (Long et al., 2024). Consequently, naive attempts to process all generated responses cause both memory blow-ups and diminishing returns in training (Dubey et al., 2024). Given these constraints, identifying a small yet highly informative subset of candidate responses is critical for effective multi-preference learning.\nOne way to conceptualize the problem is through an \u201cisland\" metaphor (See Figure 1). Consider each prompt's answer space as a set of semantic islands, where certain clusters of responses (islands) may be exceptionally good (tall peaks) or particularly poor (flat plains). Focusing only on the tallest peaks or the worst troughs can cause the model to overlook crucial middle-ground modes \"islands\" that might harbor subtle failure modes or moderate-quality answers. Therefore, an ideal subset selection strategy should cover the landscape of responses by sampling from each island (Yu et al., 2024). In this paper, we show that selecting representatives from all such \u201cislands\u201d is not only about diversity but can also be tied to an optimal way of suppressing undesired modes under a mild Lipschitz assumption (see Section 6).\nFundamentally, the process of deciding which responses deserve feedback naturally evokes the lens of active learning, where we \"actively\" pick the most informative data samples (Cohn et al., 1996; Ceravolo et al., 2024; Xiao et al., 2023). By selecting a small yet diverse subset of responses, the model effectively creates a curriculum for itself. Rather than passively training on random or exhaustively sampled data, an active learner queries the examples that yield the greatest improvement when labeled. In our context, we actively pick a handful of responses that best illustrate extreme or underexplored behaviors\u2014whether very good, very bad, or semantically distinct (Wu et al., 2023). This helps the model quickly eliminate problematic modes while reinforcing the most desirable responses. Crucially, we remain on-policy: after each update, the newly refined policy generates a fresh batch of responses, prompting another round of active subset selection (Liu et al., 2021).\nWe propose Active Multi-Preference Optimization (AMPO), a framework that unifies (a) on-policy data generation, (b) group-based preference learning, and (c) active subset selection. Specifically, we adopt a reference-free group-contrastive objective known as REFA (Gupta et al., 2024a), which jointly leverages multiple \u201cpositive\" and \"negative\" responses in a single loss term. On top of this, we explore various active selection schemes-ranging from simplest bottom-K ranking (Meng et al., 2024) to coreset-based clustering (Cohen-Addad et al., 2021; 2022; Huang et al., 2019) and a more theoretically grounded \"Opt-Select\" method that ties coverage to maximizing expected reward. Our contributions are: (i) a unifying algorithmic pipeline for multi-preference alignment with active selection, (ii) theoretical results demonstrating that coverage of distinct clusters \u00e0 la k-medoids, can serve as an optimal negative-selection strategy, and (iii) empirical evaluations showing that AMPO achieves state of the art results compared to strong alignment baselines like SIMPO. Altogether, we hope this approach advances the state of multi-preference optimization, enabling models to learn more reliably from diverse sets of model behaviors."}, {"title": "1.1. Our Contributions", "content": "\u2022 Algorithmic Novelty: We propose Active Multi-Preference Optimization (AMPO), an on-policy framework that blends group-based preference alignment with active subset selection without exhaustively training on all generated responses. This opens out avenues for research on how to select for synthetic data, as we outline in Sections 4 and 8.\n\u2022 Theoretical Insights: Under mild Lipschitz assumptions, we show that coverage-based negative selection can systematically suppress low-reward modes and maximizes expected reward. This analysis (in Sections 5 and 6) connects our method to the weighted k-medoids problem, yielding performance guarantees for alignment.\n\u2022 State-of-the-Art Results: Empirically, AMPO sets a new benchmark on AlpacaEval with Llama 8B, surpassing strong baselines like SIMPO by focusing on a small but strategically chosen set of responses each iteration (see Section 7.1).\n\u2022 Dataset Releases: We publicly release our AMPO-Coreset-Selection and AMPO-Opt-Selection datasets on Hugging Face. These contain curated response subsets for each prompt, facilitating research on multi-preference alignment."}, {"title": "2. Notations and Preliminaries", "content": "We focus on aligning a policy model to human preferences in a single-round (one-shot) scenario. Our goal is to generate multiple candidate responses for each prompt, then actively select a small, high-impact subset for alignment via a group-contrastive objective.\nQueries and Policy. Let \\(D = \\{x_1, x_2, ..., x_M\\}\\) be a dataset of M queries (or prompts), each from a larger space X. We have a policy model \\(P_\\theta(y | x)\\), parameterized by \\(\\theta\\), which produces a distribution over possible responses \\(y \\in V\\). To generate diverse answers, we sample from \\(P_\\theta(y | x)\\) at some fixed temperature (e.g., 0.8). Formally, for each \\(x_i\\), we draw up to N responses,\n\n\\{\ny_{i,1}, y_{i,2},..., y_{i, N}\n\\},\n\nfrom \\(P_\\theta(y | x_i)\\). Such an on-policy sampling, ensures, we are able to provide preference feedback on queries that are chosen by the model."}, {"title": "3. Algorithm and Methodology", "content": "We outline a one-vs-k selection scheme in which a single best response is promoted (positive), while an active subroutine selects k negatives from the remaining N-1 candidates. This setup highlights the interplay of three main objectives:\nProbability: High-probability responses under \\(P_\\theta(y | x)\\) can dominate even if suboptimal by reward.\nRewards: Simply selecting extremes by reward misses problematic \"mediocre\" outputs.\nSemantics: Diverse but undesired responses in distant embedding regions must be penalized.\nWhile positives reinforce a single high-reward candidate, active negative selection balances probability, reward and diversity to systematically suppress problematic regions of the response space.\nAlgorithm. Formally, let \\{\\texttt{y}\\_1,...,\\texttt{y}\\_n\\} be the sampled responses for a single prompt x. Suppose we have:\n1. A reward function \\(\\texttt{r}\\_i = R(\\texttt{x}, \\texttt{y}\\_i) \\in [0, 1]\\).\n2. An embedding \\(\\texttt{e}\\_i = E(\\texttt{y}\\_i)\\).\n3. A model probability estimate \\(\\pi\\_i = P\\_\\theta(\\texttt{y}\\_i | \\texttt{x})\\).\nSelection algorithms may be rating-based selection (to identify truly poor or excellent answers) with coverage-based selection (to explore distinct regions in the embedding space), we expose the model to both common and outlier responses. This ensures that the SWEPO loss provides strong gradient signals across the spectrum of answers the model is prone to generating. In Algorithm 1, \\(\\texttt{ActiveSelection}(\\cdot)\\) is a generic subroutine that selects a set of k \u201chigh-impact\" negatives. We will detail concrete implementations (e.g. bottom-k by rating, clustering-based, etc.) in later sections."}, {"title": "3.1. Detailed Discussion of Algorithm 1", "content": "The algorithm operates in four key steps: First, it selects the highest-reward response as the positive example (lines 3-4). Second, it actively selects k negative examples by considering their rewards, probabilities \\(\\pi\\_i\\), and embedding distances \\(\\texttt{e}\\_i\\) to capture diverse failure modes (lines 5-7). Third, it constructs the SWEPO objective by computing normalized scores so using the mean reward and forming a one-vs-k contrastive loss (lines 8-12). Finally, it updates the model parameters to increase the probability of the positive while suppressing the selected negatives (line 13). This approach ensures both reinforcement of high-quality responses and systematic penalization of problematic outputs across the response distribution."}, {"title": "4. Active Subset Selection Strategies", "content": "In this section, we present two straightforward yet effective strategies for actively selecting a small set of negative responses in the AMPO framework. First, we describe a simple strategy, AMPO-BottomK, that directly picks the lowest-rated responses. Second, we propose AMPO-Coreset, a clustering-based method that selects exactly one negative from each cluster in the embedding space, thereby achieving broad coverage of semantically distinct regions. In Section D, we connect this clustering-based approach to the broader literature on coreset construction, which deals with selecting representative subsets of data."}, {"title": "4.1. AMPO-BottomK", "content": "AMPO-BottomK is the most direct approach that we use for comparison: given N sampled responses and their scalar ratings \\{\\texttt{r}\\_i\\}\\_{i=1}^N, we simply pick the k lowest-rated responses as negatives. This can be expressed as:\n\n\\texttt{S}^- = \\texttt{argtopk}\\_i(-\\texttt{r}\\_i, k),"}, {"title": "4.2. AMPO-Coreset (Clustering-Based Selection)", "content": "AMPO-BOTTOMK may overlook problematic modes that are slightly better than the bottom-k, but fairly important to learn on. A diversity-driven approach, which we refer to as AMPO-CORESET, explicitly seeks coverage in the embedding space by partitioning the N candidate responses into k clusters and then selecting the lowest-rated response within each cluster. Formally:\n\\texttt{i}\\_j = \\texttt{arg min}\\_{\\texttt{i} \\in \\texttt{C}\\_j} \\texttt{r}\\_\\texttt{i}, j = 1, ..., k, \\texttt{S} = \\{\\texttt{i}\\_1, ..., \\texttt{i}\\_k\\}\nwhere \\(\\texttt{C}\\_j\\) is the set of responses assigned to cluster j by a k-means algorithm (Har-Peled & Mazumdar 2004; Cohen-Addad et al. 2022; see also Section D). The pseudo-code is provided in Algorithm 2.\nThis approach enforces that each cluster-a potential \"mode\" in the response space-contributes at least one negative example. Hence, AMPO-CORESET can be interpreted as selecting representative negatives from diverse semantic regions, ensuring that the model is penalized for a wide variety of undesired responses."}, {"title": "5. Opt-Select: Active Subset Selection by Optimizing Expected Reward", "content": "In this section, we propose Opt-Select: a strategy for choosing k negative responses (plus one positive) so as to maximize the policy's expected reward under a Lipschitz continuity assumption. Specifically, we model the local \u201cneighborhood\" influence of penalizing each selected negative and formulate an optimization problem that seeks to suppress large pockets of low-reward answers while preserving at least one high-reward mode. We first describe the intuition and objective, then present two solution methods: a mixed-integer program (MIP) and a local search approximation."}, {"title": "5.1. Lipschitz-Driven Objective", "content": "Let \\{\\texttt{y}\\_i\\}\\_{i=1}^N be candidate responses sampled on-policy, each with reward \\(\\texttt{r}\\_i \\in [0, 1]\\) and embedding \\(\\texttt{e}\\_i \\in R^d\\). Suppose that if we completely suppress a response \\(\\texttt{y}\\_j\\) (i.e. set its probability to zero), all answers within distance \\(||\\texttt{e}\\_i - \\texttt{e}\\_j ||\\) must also decrease in probability proportionally, due to a Lipschitz constraint on the policy. Concretely, if the distance is \\(d\\_{i,j} = ||\\texttt{e}\\_i - \\texttt{e}\\_j ||\\), and the model's Lipschitz constant is L, then the probability of \\(\\texttt{y}\\_i\\) cannot remain above \\(L d\\_{i,j}\\) if \\(\\texttt{y}\\_j\\) is forced to probability zero.\nFrom an expected reward perspective, assigning zero probability to low-reward responses (and their neighborhoods) improves overall alignment. To capture this rigorously, observe that the penalty from retaining a below-average answer \\(\\texttt{y}\\_i\\) can be weighted by:\n\n\\texttt{w}\\_i = exp(\\overline{\\texttt{r}} - \\texttt{r}\\_i),\n\nwhere \\(\\overline{\\texttt{r}}\\) is (for instance) the mean reward of \\{\\texttt{r}\\_i\\}. Intuitively, \\(\\texttt{w}\\_i\\) is larger for lower-reward \\(\\texttt{y}\\_i\\), indicating it is more harmful to let \\(\\texttt{y}\\_i\\) and its neighborhood remain at high probability.\nNext, define a distance matrix\n\n\\texttt{A}\\_{i,j} = ||\\texttt{e}\\_i - \\texttt{e}\\_j ||^2, 1 \\leq i, j \\leq n.\n\nSelecting a subset S \\(\\subseteq\\) \\{\\texttt{1},..., \\texttt{n}\\) of \u201cnegatives\" to penalize suppresses the probability of each i in proportion to \\(min\\_{\\texttt{j} \\in S} \\texttt{A}\\_{i,j}\\). Consequently, a natural cost function measures how much \"weighted distance\" \\(\\texttt{y}\\_i\\) has to its closest chosen negative:\n\nCost(S) = \\sum\\_{\\texttt{i}=1}^n w\\_i min\\_{\\texttt{j} \\in S} A\\_{i,j}\n\nMinimizing equation 8 yields a subset S of size k that \"covers\" or \"suppresses\u201d as many low-reward responses (large \\(\\texttt{w}\\_i\\)) as possible. We then add one positive index \\(\\texttt{i}\\_{\\texttt{top}}\\) with the highest \\(\\texttt{r}\\_i\\) to amplify a top-quality answer. This combination of one positive plus k negatives provides a strong signal in the training loss.\nInterpretation and Connection to Weighted k-medoids. If each negative j \u201ccovers\u201d responses i within some radius (or cost) \\(A\\_{i,j}\\), then equation 8 is analogous to a weighted k-medoid objective, where we choose k items (negatives) to minimize a total weighted distance. Formally, this can be cast as a mixed-integer program (MIP) (Problem 9 below). For large n, local search offers an efficient approximation."}, {"title": "5.2. Mixed-Integer Programming Formulation", "content": "Define binary indicators \\(x\\_j = 1\\) if we choose \\(y\\_j\\) as a negative, and \\(z\\_{i,j} = 1\\) if i is assigned to j (i.e. \\(min\\_{j \\in S} A\\_{i,j}\\) is realized by j). We write:\n\nProblem P: \\quad \\underset{x\\_j \\in \\{0,1\\}, z\\_{i,j} \\in \\{0,1\\}, y\\_i \\geq 0}{min} \\sum\\_{i=1}^n w\\_i y\\_i\n\ns.t. \\sum\\_{j=1}^n x\\_j = k, z\\_{i,j} \\leq x\\_j, \\sum\\_{j=1}^n z\\_{i,j} = 1, \\forall i,\n\ny\\_i \\leq A\\_{i,j} + M (1 \u2013 z\\_{i,j}),\n\ny\\_i \\geq A\\_{i,j} - M (1 \u2013 z\\_{i,j}), \\forall i, j,\n\nwhere \\(M = max\\_{i,j} A\\_{i,j}\\). In essence, each i is forced to assign to exactly one chosen negative j, making \\(y\\_i = A\\_{i,j}\\), i.e. the distance between the answer embeddings for answer \\{\\texttt{i}, \\texttt{j}\\}. Minimizing \\(\\sum\\_{i} w\\_i y\\_i\\) (i.e. equation 8) then ensures that low-reward points (\\(\\texttt{w}\\_i\\) large) lie close to at least one penalized center.\nAlgorithmic Overview. Solving P gives the k negatives Sneg, while the highest-reward index itop is chosen as a positive. The final subset \\{\\texttt{i}\\_{\\texttt{top}}\\}\\) \\(\\cup\\) Sneg is then passed to the SWEPO loss (see Section 3). Algorithm 3 outlines the procedure succinctly."}, {"title": "5.3. Local Search Approximation", "content": "For large n, an exact MIP can be expensive. A simpler local search approach initializes a random subset S of size k and iteratively swaps elements in and out if it lowers the cost equation 8. In practice, this provides an efficient approximation, especially when n or k grows.\nIntuition. If \\(y\\_i\\) is far from all penalized points \\(j \\in S\\), then it remains relatively \"safe\" from suppression, which is undesirable if \\(\\texttt{r}\\_i\\) is low (i.e. \\(\\texttt{w}\\_i\\) large). By systematically"}, {"title": "5.4. Why \"Opt-Select\"? A Lipschitz Argument for Expected Reward", "content": "We name the procedure \"Opt-Select\" because solving equation 9 (or its local search variant) directly approximates an optimal subset for improving the policy's expected reward. Specifically, under a Lipschitz constraint with constant L, assigning zero probability to each chosen negative \\(y\\_j\\) implies neighboring answers \\(y\\_i\\) at distance \\(d\\_{i,j}\\) cannot exceed probability \\(Ld\\_{i,j}\\). Consequently, their contribution to the \"bad behavior\" portion of expected reward is bounded by\n\nexp(r\\_{\\textrm{max}}-\\texttt{r}\\_i) (Ld\\_{i,j}),\n\nwhere \\(r\\_{\\textrm{max}}\\)- is the rating of the best-rated response. Dividing by a normalization factor (such as exp(r\\_{\\textrm{max}} - \\overline{\\texttt{r}}) L), one arrives at a cost akin to \\(\\texttt{w}\\_i d\\_{i,j}\\) with \\(\\texttt{w}\\_i\\) = exp(-\\texttt{r}\\_i). This aligns with classical min-knapsack of minimizing some costs subject to some constraints, and has close alignment with the weighted k-medoid notions of \"covering\" important items at minimum cost."}, {"title": "6. Theoretical Results: Key Results", "content": "In this section, we present the core theoretical statements used throughout the paper. Full extended proofs appear in Appendices B\u2013D."}, {"title": "6.1. Setup and Assumptions", "content": "(A1) L-Lipschitz Constraint. When a response \\(y\\_j\\) is penalized (probability \\(p\\_j = 0\\)), any other response \\(y\\_i\\) within embedding distance \\(A\\_{i,j}\\) must satisfy \\(p\\_i \\leq L A\\_{i,j}\\).\n(A2) Single Positive Enforcement. We allow one highest-reward response \\(Y\\_{\\textrm{itop}}\\) to be unconstrained, i.e. \\(P\\_{\\textrm{itop}}\\) is not pulled down by the negatives.\n(A3) Finite Support. We focus on a finite set of n candidate responses \\{\\texttt{Y}\\_1,..., \\texttt{Y}\\_n \\} and their scalar rewards \\{\\texttt{r}\\_i\\}, each embedded in \\(R^d\\) with distance \\(A\\_{i,j} = ||\\texttt{e}\\_i \u2013 \\texttt{e}\\_j ||\\)."}, {"title": "6.2. Optimal Negatives via Coverage", "content": "Theorem 6.1 (Optimality of OPT-SELECT). Under assumptions (A1)-(A3), let S* be the set of k \"negative\" responses that minimizes the coverage cost\n\\textrm{Cost}(S) = \\sum\\_{i=1}^n exp(-\\texttt{r}\\_i) \\underset{j \\in S}{\\textrm{min}} A\\_{i,j},\n\nwhere is a reference reward (e.g. average of \\{\\texttt{r}\\_i\\}). Then S* also maximizes the expected reward among all Lipschitz-compliant policies of size k (with a single positive). Consequently, selecting S* and allowing \\(P\\_{\\textrm{itop}} \u2248 1\\) is optimal."}, {"title": "6.3. Local Search for Weighted k-Medoids", "content": "(A4) Weighted k-Medoids Setup. We have n points \\{\\texttt{1}, ..., \\texttt{n}\\) in a metric space with distance d(\\cdot,\\cdot) \u2265 0, each with weight w\u2081 \u2265 0. Our goal is to find a subset S of size k minimizing\n\nCost(S) = \\sum\\_{i=1}^n w\\_i \\underset{j \\in S}{\\textrm{min}} d(i, j).\n\nTheorem 6.2 (Local Search Approximation). Suppose we apply a 1-swap local search algorithm to select k medoids. Let S be the resulting local optimum and let S* be the globally optimal subset. Then\n\nCost(\\texttt{5}) \\leq 5 \\times \\textrm{Cost}(S^*).\n\nThe running time is polynomial in n and k."}, {"title": "6.4. Coreset and Bounded Intra-Cluster Distance", "content": "(A5) Bounded-Diameter Clusters. We assume k clusters of diameter at most dmax in AMPO-CORESET.\nTheorem 6.3 (Distribution-Dependent Coreset Guarantee). Suppose we form k clusters each of diameter < dmax in an embedding space \\(R^d\\), and from each cluster we pick one response as a \"negative.\" Under a Lipschitz constant L, this subset induces pi \u2264 L dmax for all i in each cluster. With sufficiently many samples (drawn from a distribution of prompts/responses), with probability at least 1 \u2013 \u03b4, for a (1 \u00b1 \u03b5) fraction of new draws, the resulting Lipschitz-compliant policy is within a factor (1 \u00b1 \u03b5) of the optimal k-subset solution."}, {"title": "7. Experiments", "content": "7.1. Experimental Setup\nModel and Training Settings: For our experiments, we utilize a pretrained instruction-tuned model (meta-llama/MetaLlama-3-8B-Instruct), as the SFT model. These models have undergone extensive instruction tuning, making them more capable and robust compared to the SFT models used in the Base setup. However, their reinforcement learning with human feedback (RLHF) procedures remain undisclosed, making them less transparent.\nTo reduce distribution shift between the SFT models and the preference optimization process, we follow the approach in (Tran et al., 2023) and generate the preference dataset using the same SFT models. This ensures that our setup is more aligned with an on-policy setting. Specifically, we utilize prompts from the UltraFeedback dataset (Cui et al., 2023) and regenerate the resonses using the SFT models. For each prompt x, we produce 32 responses by sampling from the SFT model with a sampling temperature of 0.8. We then use the reward model (Skywork/Skywork-Reward-Llama-3.1-8B-v0.2) (Liu et al., 2024b) to score all the 32 responses. Then the response are selected based on the Active Subset selection strategies a.) AMPO-Bottomk b.) AMPO-Coreset c.) AMPO-OptSelect\nIn our experiments, we observed that tuning hyperparameters is critical for optimizing the performance . Carefully selecting hyperparameter values significantly impacts the effectiveness of these methods across various datasets. We found that setting the \\(\\beta\\) parameter in the range of 5.0 to 10.0 consistently yields strong performance, while tuning the \\(\\gamma\\) parameter within the range of 2 to 4 further improved performance. These observations highlight the importance of systematic hyperparameter tuning to achieve reliable outcomes across diverse datasets.\nEvaluation Benchmarks We evaluate our models using three widely recognized open-ended instruction-following benchmarks: MT-Bench (Zheng et al., 2023), AlpacaEval2 (Dubois et al., 2024), and Arena-Hard v0.1. These benchmarks are commonly used in the community to assess the conversational versatility of models across a diverse range of queries.\nAlpacaEval 2 comprises 805 questions sourced from five datasets, while MT-Bench spans eight categories with a total of 80 questions. The recently introduced Arena-Hard builds upon MT-Bench, featuring 500 well-defined technical problem-solving queries designed to test more advanced capabilities.\nWe adhere to the evaluation protocols specific to each benchmark when reporting results. For AlpacaEval 2, we provide both the raw win rate (WR) and the length-controlled win rate (LC), with the latter being designed to mitigate the influence of model verbosity. For Arena-Hard, we report the win rate (WR) against a baseline model. For MT-Bench, we present the scores as evaluated by GPT-4-Preview-1106, which serve as the judge model."}, {"title": "7.2. Experimental Result", "content": "Impact of Selection Strategies on Diversity. Key analysis from Fig. 2 demonstrate that our selection strategies significantly improve response diversity compared to traditional baselines. By actively optimizing for coverage-aware selection, our methods mitigate redundancy in selected responses, leading to better preference modeling and enhanced LLM alignment.\nImpact of Temperature Sampling for Different Active Selection Approaches To analyze the impact of temperature-controlled response sampling on different active selection approaches, we conduct an ablation study by varying the sampling temperature from 0 to 1.0 in increments of 0.25 on AlpacaEval2 benchmark as demonstrated in Figure 3. We evaluate our active selection strategies observe a general trend of declining performance with increasing temperature. Key observation: AMPO-Coreset and AMPO-OptSelect demonstrate robustness to temperature variations, whereas WR-SimPO and bottom-k selection are more sensitive.\nEffect of gamma for Active Selection Approaches To further investigate the sensitivity of core-set selection to different hyper-parameter settings, we conduct an ablation study on the impact of varying the gamma parameter as show in Figure 4. As gamma increases from 1 to 3, we observe a consistent improvement in both LC-WR and WR scores. Key findings highlight the importance of tuning gamma appropriately to maximize the effectiveness of active-selection approaches."}, {"title": "8. Discussion & Future Work", "content": "Iteration via Active Synthetic Data Generation. When we combine reward signals and output-embedding signals in active sampling, we naturally create a pathway to synthetic data creation. Through multi-preference optimization on diverse queries, the model continually improves itself by receiving feedback on different modes of failure (and success). Crucially, because this process is on-policy, the model directly surfaces new candidate answers for which it is most uncertain or prone to errors. The selection for coverage ensures that we efficiently address a large portion of the measurable answer space, rather than merely focusing on obvious or extreme failures.\nOver multiple epochs, such a growing corpus of synthetic data can be used to refine or re-check the reward model, establishing a feedback loop between policy improvement and reward-model improvement. We believe this to be an important direction of future work."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "SUPPLEMENTARY MATERIALS", "content": "These supplementary materials provide additional details, derivations, and experimental results for our paper. The appendix is organized as follows:\n\u2022 Section A provides a more comprehensive overview of the related literature.\n\u2022 Section B provides theoretical analysis of the equivalence of the optimal selection integer program and the reward maximization objective.\n\u2022 Section C shows a constant factor approximation for the coordinate descent algorithm in polynomial time.\n\u2022 Section D provides theoretical guarantees for our k-means style coreset selection algorithm.\n\u2022 Section E provides the code for computation of the optimal selection algorithm.\n\u2022 Section F provides t-sne plots for the various queries highlighting the performance of our algorithms."}, {"title": "A. Related Work", "content": "Preference Optimization in RLHF. Direct Preference Optimization (DPO) is a collection of techniques for fine-tuning language models based on human preferences (Rafailov et al., 2024). Several variants of DPO have been developed to address specific challenges and improve its effectiveness (Ethayarajh et al., 2024; Zeng et al., 2024; Dong et al., 2023; Yuan et al., 2023). For example, KTO and TDPO focus on different aspects of preference optimization, while RAFT and RRHF utilize alternative forms of feedback. Other variants, such as SPIN, CPO, ORPO, and SimPO, introduce additional objectives or regularizations to enhance the optimization process (Chen et al., 2024b; Xu et al., 2024; Hong et al., 2024; Meng et al., 2024).\nFurther variants, including R-DPO, LD-DPO, sDPO, IRPO, OFS-DPO, and LIFT-DPO, address issues like length bias, training strategies, and specific reasoning tasks. These diverse approaches demonstrate the ongoing efforts to refine and enhance DPO, addressing its limitations and expanding its applicability to various tasks and domains (Park et al., 2024; Liu et al., 2024c; Pang et al., 2024; Qi et al., 2024; Yuan et al., 2024).\nMulti-Preference Approaches. Recent work extends standard RLHF to consider entire sets of responses at once, enabling more nuanced feedback signals (Rafailov et al., 2024; Cui et al., 2023; Chen et al., 2024a). Group-based objectives capture multiple acceptable (and multiple undesirable) answers for each query, rather than only a single \u201cbetter vs. worse\u201d pair. Gupta et al. (2024b) propose a contrastive formulation, SWEPO, that jointly uses multiple \"positives\" and \"negatives.\" Such multi-preference methods can reduce label noise and better reflect the complexity of real-world tasks, but their computational cost grows if one attempts to incorporate all generated outputs (Cui et al., 2023; Chen et al., 2024a).\nOn-Policy Self-Play. A key advancement in reinforcement learning has been self-play or on-policy generation, where the model continuously updates and re-generates data from its own evolving policy (Silver et al., 2016; 2017). In the context of LLM alignment, on-policy sampling can keep the training set aligned with the model's current distribution of outputs (Christiano et al., 2017; Wu et al., 2023). However, this approach can significantly inflate the number of candidate responses, motivating the need for selective down-sampling of training examples.\nActive Learning for Policy Optimization. The notion of selectively querying the most informative examples is central to active learning (Cohn et al., 1996; Settles, 2009), which aims to reduce labeling effort by focusing on high-utility samples. Several works incorporate active learning ideas into reinforcement learning, e.g., uncertainty sampling or diversity-based selection (Sener & Savarese, 2017; Zhang et al., 2022). In the RLHF setting, Christiano et al. (2017) highlight how strategic"}, {"title": "B. Extended Theoretical Analysis of OPT-SELECT", "content": "In this appendix, we present a more detailed theoretical treatment of AMPO-OPTSELECT. We restate the core problem setup and assumptions, then provide rigorous proofs of our main results. Our exposition here augments the concise version from the main text."}]}