{"title": "Deciphering the Interplay between Local Differential Privacy, Average Bayesian Privacy, and Maximum Bayesian Privacy", "authors": ["Xiaojin Zhang", "Yulin Fei", "Wei Chen", "Hai Jin"], "abstract": "The swift evolution of machine learning has led to emergence of various definitions of privacy due to the threats it poses to privacy, including the concept of local differential privacy (LDP). Although widely embraced and utilized across numerous domains, this conventional approach to measure privacy still exhibits certain limitations, spanning from failure to prevent inferential disclosure to lack of consideration for the adversary's background knowledge. In this comprehensive study, we introduce Bayesian privacy and delve into the intricate relationship between local differential privacy and its Bayesian counterparts, unveiling novel insights into utility-privacy trade-offs. We introduce a framework that encapsulates both attack and defense strategies, highlighting their interplay and effectiveness. Our theoretical contributions are anchored in the rigorous definitions and relationships between Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP), encapsulated by equations $E_{p,a} \\leq \\frac{1}{\\sqrt{2}} \\sqrt{(E_{p,m} + \\epsilon) \\cdot (e^{E_{p,m} + \\epsilon} - 1)}$ and the equivalence between \u00a7-MBP and 2\u00a7-LDP established under uniform prior distribution. These relationships fortify our understanding of the privacy guarantees provided by various mechanisms, leading to the realization that a mechanism satisfying \u00a7-LDP also confers \u00a7-MBP, and vice versa. Our work not only lays the groundwork for future empirical exploration but also promises to enhance the design of privacy-preserving algorithms that do not compromise on utility, thereby fostering the development of trustworthy machine learning solutions.", "sections": [{"title": "1 Introduction", "content": "Burgeoning fields in machine learning continually bring people's concern\u2014jeopardizing privacy when training models with sensitive data (Li and Li (2009); Alvim et al. (2012); Li et al. (2024)). Yet in the mean time, the pursuit of granular information risks compromising the privacy of those supplying the data. The mutual perennial goal of countless companies and governments is to extract meaningful insights from data without compromising individual's privacy (Erlingsson et al. (2014); Cormode et al. (2018)). To accomplish this objective, certain scholars like McMahan et al. (2017) have suggested a distributed learning framework, for instance, federated learning. This marvelous design lets clients train their own model locally without transmitting their own private dataset. However, this method is not a panacea (Mammen (2021)). Geiping et al. (2020) and Nasr et al. (2019) found that potential attackers also can conduct gradient attack when clients' gradients are transmitted to central server. So we still need some ways to protect privacy more strictly, and the first question is how to measure it by a reasonable way. To respond to this challenge, a variety of privacy metrics have been proposed to measure privacy.\nAmong these, Local Differential Privacy (LDP), a concept formalized by Dwork et al. (2006), has attained the status of the benchmark. LDP provides strong privacy guarantees by ensuring that any two different data point will have very similar output of computation. The extent to which the output reveals information about the value of $x$ is constrained by a function of $\\epsilon$. This restriction curtails the confidence that potential attackers can possess concerning the actual value of $x$. Despite its widespread adoption, LDP faces critical limitations, particularly in terms of lacking consideration for the adversary's background knowledge and the data distribution at play. According to du Pin Calmon and Fawaz (2012), it is contended that in a general context, differential privacy does not furnish privacy assurances regarding average or maximum information disclosure. Li et al. (2013) highlighted that the efficacy of differential privacy in averting inferential disclosures is lacking. The possibility remains for deducing potentially sensitive details about an individual with notable precision from differentially private outputs.\nRecent research done by Zhu et al. (2019) and He et al. (2019) has begun to address these limitations by proposing privacy concepts aligned with the Bayesian inference perspective of adversary, such as Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP). These concepts have emerged from an understanding that adversaries may possess prior knowledge and use their Bayesian reasoning capabilities to infer sensitive information from the output of machine learning algorithms. While these new paradigms offer a more tailored approach to privacy, there's a pressing need to thoroughly understand their relationships with traditional privacy measures like LDP.\nMotivated by these, our work seeks to (a) present a rigorous framework that characterizes both adversarial attacks and defense mechanisms, critically examining the effectiveness of various strategies within this dichotomy, and (b) advance the field of machine learning privacy by exploring the theoretical relationship between Bayesian privacy measures and traditional privacy metrics like LDP. Our contributions are listed below:\n\u2022 Firstly, we introduce a comprehensive privacy attacks and defenses framework in Section 3, which describes threat model by characterizing the adversary's objectives, capabilities, knowledge base, and methodologies employed in executing attacks, and outline the defender's strategic approaches foundational to the mechanisms designed for the protection of sensitive information.\n\u2022 We establish $\\epsilon$-ABP and $\\epsilon$-MBP in Section 4 (refer to Definition 4.1 and Definition 4.2), which gauge privacy leakage through Jensen-Shannon (JS) divergence\u2014a symmetric"}, {"title": "2 Related Work", "content": "Differential Privacy (usually abbreviated as DP) stands as a rigorously formalized privacy framework pioneered by Dwork et al. (2006). Techniques for enforcing DP in deep neural network training were proposed by Abadi et al. (2016), encompassing to diverse optimization methods, including Momentum Rumelhart et al. (1986) and AdaGrad Duchi et al. (2011). An adaptation of DP known as Bayesian Differential Privacy (BDP), which incorporates considerations of data distribution, was introduced by Triastcyn and Faltings (2020a), along with a novel privacy assessment approach. By employing average total variation distance (abbreviated as TV distance) for privacy measurement and minimum mean-square error (MMSE), mutual information, and error probability to assess utility, Rassouli and G\u00fcnd\u00fcz (2019) quantified trade-offs between utility and privacy.\nAchieving pure DP can be challenging in practical learning scenarios. The loss of privacy was quantified by Eilat et al. (2020) through the difference between a designer's prior and posterior beliefs regarding an agent's type, utilizing Kullback-Leibler (KL) divergence. The constraints of obtaining privacy via posterior sampling were discussed by Foulds et al. (2016), while the benefits of Laplace mechanism against Bayesian inference were underscored both from the practical and theoretical perspectives.\nPrivacy measures akin to our approach have been considered in other works. du Pin Calmon and Fawaz (2012) introduced metrics for average information leakage and maximum information leakage, intended to balance privacy against utility constraints, with privacy leakage assessed using KL divergence by Gu et al. (2021). Conversely, our work adopts Jensen-Shannon (JS) divergence for measuring privacy leakage, in line with Zhang et al. (2022, 2023c). JS divergence is favored over KL divergence due to its symmetric nature and because its square root meets the triangle inequality (Nielsen (2019); Endres and Schindelin (2003)), which aids in quantifying privacy-utility trade-offs more effectively."}, {"title": "2.2 Relation between Distinct Privacy Measurements", "content": "Numerous algorithms have been developed to attain differential privacy. And inspired by advent of differential privacy, many scholars have tried to propose new privacy metrics. Concentrated differential privacy (Concentrated-DP), proposed by Dwork and Rothblum (2016), offers a relaxation of differential privacy's conventional notion. They defined the concept based on the mean of the privacy leakage of a randomized mechanism, stipulating that the mechanism satisfies that definition if it is small and subgaussian. Based on R\u00e9nyi divergence, R\u00e9nyi Differential Privacy (RDP), was introduced by Mironov (2017) as a variant of differential privacy offering relaxation. RDP unifies concentrated-DP, $\\epsilon$-DP, and ($\\epsilon$, $\\delta$)-DP (Dwork and Rothblum (2016) and Bun and Steinke (2016)). Mironov et al. (2009) proposed computational differential privacy (CDP), measuring privacy against computationally-bounded adversaries. They introduced two variants, IND-CDP and SIM-CDP to adapt to the computational setting. The definition of IND-CDP originated from ($\\epsilon$, $\\delta$)-differential privacy. It was relaxed by ignoring some additive distinguishing advantage. SIM-CDP was further extended to two new definitions called SIM$\\forall\\exists$-CDP and SIM+-CDP. And it was proved that (a) a SIM-CDP algorithm naturally provides SIM$\\forall\\exists$-CDP, (b) a SIM$\\forall\\exists$-CDP naturally provides IND-CDP. Dong et al. (2019) gave a new way to relax the privacy definition, called f-differentially privacy (f-DP). They further proposed a single-parameter family of privacy in the class of f-DP called Gaussian differential privacy. Triastcyn and Faltings (2020a) introduced a novel variant of differential privacy, called Bayesian differential privacy (BDP), that takes into account the dataset's distribution. They also proposed a privacy accounting approach. Eilat et al. (2020) further contributed to this field by measuring privacy loss through the difference between designer's prior belief and posterior belief regarding type of agent, utilizing KL divergence. However, Foulds et al. (2016)"}, {"title": "3 Privacy Attack and Defense Framework", "content": "This section delineates the adversarial landscape confronting data privacy in machine learning applications. We systematically outline the threat model, characterizing the adversary's objectives, capabilities, knowledge base, and methodologies for launching attacks. Concur-rently, we describe the defender's strategies that underpin the protection mechanisms aimed at safeguarding sensitive information. It is worth noting that this privacy attack and defense framework has been widely investigated, including Zhang et al. (2023a,b); Kang et al. (2022); Zhang et al. (2023e); Asi et al. (2023); du Pin Calmon and Fawaz (2012)."}, {"title": "3.1 Privacy Attack Model", "content": "The threat model provides a structured perspective on the potential vulnerabilities and risks to client data privacy. Herein, we examine the various facets of an attacker's strategy, including their intents, resources, intelligence, and tactics."}, {"title": "Attacker's Objective", "content": "Within our framework, we posit a scenario where a protector operates an algorithm M($\\cdot$), processing input data $D$ to yield output $W$. This result must then be conveyed to a recipient\u2014who is presupposed to be the attacker. The attacker aspires to extract confidential details regarding $D$ from the observable $W$, while balancing the need to maintain utility within certain bounds. The underlying objective is to refine the attacker's probabilistic beliefs about $D$ through analysis of $W$."}, {"title": "Attacker's Capability", "content": "Our model assumes a semi-honest (or \u201chonest-but-curious\u201d) attacker. Such an adversary adheres to protocol specifications in terms of computations but seeks to glean additional private information by inferring it from $W$. By performing statistical inference on the output $W$, the attacker aims to ascertain insights into the protector's confidential data $D$."}, {"title": "Attacker's Knowledge", "content": "Based on previous descriptions, the attacker has access to the output $W$ (though post-transformation by the protector's mechanism, they receive a modified version $W$). Furthermore, the attacker may possess pre-existing knowledge about the private data $D$, potentially acquired through past breaches or leaks. Additionally, the attacker is cognizant of the utility constraints associated with their analyses."}, {"title": "Attacker's Method", "content": "With knowledge of a distorted parameter $W$, we assume that the attacker adopts a Bayesian inference attack, i.e., find a data $d$ that optimizes posterior belief, denoted by:\n$\\arg \\max I_H(d|w) = \\arg \\max[I(w|d) + H(d)],$ \nwhere $I_H(d|w)$ represents the logarithm of the posterior probability density function $f_{D|W}(d|w)$, $I(w|d)$ is the likelihood function's logarithm based on observed parameter $w$, and $H(d)$ symbolizes the prior probability distribution's logarithm $f_P(d)$. By applying Bayesian theorem, Optimizing the logarithmic form of the posterior $f_{D|w}(d|w)$ with respect to $D$ entails aggregating both $\\log(f_{w|D}(w|d))$ and $\\log(f_P(d))$"}, {"title": "3.2 Privacy Protection Model", "content": "We now turn our attention to the defensive measures employed by the protector whose design is to shield sensitive datasets from unauthorized exposure."}, {"title": "Protector's Objective", "content": "To safeguard privacy post-computation by algorithm M($\\cdot$) : $D \\rightarrow W$, the protector needs to devise a privacy-preserving mechanism, which is a stochastic transformation, having the ability to map $W \\rightarrow W$, where $W$ is a less-informative derivative. This ensures compliance with privacy standards such as $\\epsilon$-differential privacy."}, {"title": "Protector's Capability", "content": "It is incumbent upon the protector to transmit $W$ in lieu of $W$ to conceal the original data $D$. To this end, the protector must be adept at generating and communicating a perturbed variant $W$, which duly obscures the private dataset $D$. This task may be complicated by historical disclosures of $D$."}, {"title": "Protector's Knowledge", "content": "The protector is privy to the private dataset $D$, the computed output $W$, and the intricacies of the employed algorithm M($\\cdot$). They are also versed in the workings of the privacy-preserving transformation. Additionally, an understanding of the adversary's utility limitations is crucial for the protector to tailor the distortion of $W$ into"}, {"title": "Protector's Method", "content": "In defence of privacy (i.e., the confidentiality of dataset $D$), the protector must execute the randomized privacy-preserving mechanism, which transforms $W$ to $W$. Execution of the algorithm is expected to equip it with the requisite properties for safeguarding dataset $D$ against intrusion while adhering to the adopted privacy standard. For instance, optimized algorithms for protecting privacy under the defense and attacking framework have been designed as detailed in the studies by Zhang et al. (2023e) and du Pin Calmon and Fawaz (2012), while Asi et al. (2023) choose to apply exponential mechanism(i.e., add random noise to $W$) to protect privacy."}, {"title": "4 Privacy Measurements", "content": "In the following section, we outline various metrics used to assess privacy and robustness in the context of our proposed defense and attacking framework. Privacy measurement in machine learning has emerged as a critical area of study, with various approaches proposed to quantify and evaluate the privacy guarantees of different algorithms and models. This section provides a detailed exposition on the metrics used to quantify privacy under the previously discussed privacy attack and defense framework, including concepts such as Maximum Bayesian Privacy, Average Bayesian Privacy, and Local Differential Privacy. These approaches enable researchers to quantitatively evaluate the privacy risks associated with different models and algorithms. As privacy concerns continue to grow, further advancements in privacy measurement techniques are critical for developing robust and privacy-preserving machine learning solutions."}, {"title": "Information privacy", "content": "Information privacy, particularly through frameworks like ABP and MBP, offers several advantages over LDP. ABP and MBP provide a more nuanced approach to privacy by considering prior knowledge and the Bayesian perspective of an adversary, allowing for a tailored balance between privacy and utility. Unlike LDP, which applies noise uniformly to data, ABP and MBP can adjust the privacy guarantees based on the actual data distribution and the adversary's potential background knowledge, potentially leading to better data utility for the same privacy level. This is particularly beneficial in scenarios where it's crucial to maintain high data fidelity for analysis or machine learning purposes. Additionally, with the concept of \"privacy loss budgets\" tailored to individual data points, these models might reduce the overall quantity of noise necessary to achieve desired level of privacy compared to LDP."}, {"title": "Definition 4.1 (Average Bayesian Privacy)", "content": "Let $M$ represent the protection mechanism that maps from the private information $D$ to the model information $W$, and $G$ represent the attacking mechanism. Let $\\epsilon_p$ be average privacy leakage, which is defined as:\n$\\epsilon_{p,a}(M,G) = \\sqrt{JS(F_A||F_B)},$\nIn this formula, $JS(F_A||F_B) = \\frac{1}{2} \\int_D f_A(d) \\log \\frac{f_A(d)}{f_M(d)} d\\mu(d) + \\frac{1}{2} \\int_D f_B(d) \\log \\frac{f_B(d)}{f_M(d)} d\\mu(d),$  At- tacker's belief distribution about $D$ upon observing the protected information and without observing is $F_A$ and $F_B$, and $f_M(d) = \\frac{(f_A(d) + f_B(d))}{2}$. We say $M$ is $\\epsilon$-ABP when $\\epsilon_{\\rho,\\alpha}(M,G) \\leq \\epsilon$.\nRemark: We operate under the assumption that the private information $D$ takes on discrete values, and then\n$JS(F_A||F_B) = \\frac{1}{2} [KL (F_A||F_M) + KL (F_B||F_M)]$\n$= \\frac{1}{2} \\sum_{d\\in D} f_A(d) \\log \\frac{f_A(d)}{f_M(d)} + \\frac{1}{2} \\sum_{d\\in D} f_B(d) \\log \\frac{f_B(d)}{f_M(d)}$"}, {"title": "Definition 4.2 (\u00a7-Maximum Bayesian Privacy)", "content": "Let $F$ denote the belief distribution held by the attacker regarding the private data, and let $f$ indicate the associated probability density function. We define the system to possess \u00a7-Maximum Bayesian Privacy if $\\forall w \\in W, \\forall d \\in D$, the following is satisfied:\n$-\\xi < \\log \\frac{f_{D|w}(d|w)}{f_D(d)} \\leq \\xi$.\nThe \u00a7-Maximum Bayesian Privacy quantifies the level of privacy protection afforded by measuring the disparities in the conditional and marginal distributions of the data. A smaller value of $\\xi$ implies stronger privacy guarantees against potential adversaries."}, {"title": "5 Theoretical Relationship Between LDP, MBP and ABP", "content": "In the following, relations among Maximum Bayesian Privacy (MBP), Average Bayesian Privacy (ABP) and Local Differential Privacy (LDP) are investigated. Through mathematical definitions and theorems, we demonstrate how these concepts of privacy are interconnected and how one can transition from one privacy framework to another. Specifically, we will prove through theorems that an algorithm satisfying the requirements of LDP also satisfies those of MBP, and vice versa. Additionally, we will discuss the connection between MBP and ABP, along with how to convert from a mapping that provides maximum privacy protection to one that offers an average level of privacy protection. These theoretical results not only deepen our understanding of the connections between various privacy measures but also offer significant guidance for crafting and analyzing algorithms that preserve privacy.\nLDP aims to safeguard individual data contributors' privacy, enabling the extraction of aggregate statistics or insights from the data. The concept of LDP is articulated as follows."}, {"title": "Definition 5.1 (\u00a7-LDP)", "content": "A randomized function $M$ defined as $\\epsilon$-local differentially private if, $\\forall$ possible datasets $d$ and $d'$, forall subset $S$ of the output space, the following inequality is true\n$\\frac{Pr[M(d) \\in S]}{Pr[M(d') \\in S]} < e^\\epsilon.$\nIn this formula, $\\epsilon$ is non-negative, which quantifies the level of privacy protection, and $Pr[M(d) \\in S]$ represents the probability that the randomized mechanism $M$ outputs a result in the set $S$ when given the dataset $d$.\nThis definition states that the probability of any outcome being produced by the mechanism when given dataset $d$ should be similar to the probability of the same outcome being produced when given a dataset $d'$, smaller $\\epsilon$ values indicate stronger privacy guarantees. LDP ensures that individual contributions to the dataset are protected by adding noise or randomness to the results of queries or computations, making it difficult to infer specific details about any individual in the dataset."}, {"title": "Lemma 5.1", "content": "The following Lemma 5.1 implicates that if an algorithm provides \u00a7-LDP, then it also offers \u00a7-MBP. In other words, the same mechanism that provides local privacy protection, where individual data points are protected, also ensures a certain level of global privacy in a Bayesian sense. This means that the mechanism is not only useful for protecting individual data points but also for safeguarding the privacy of the dataset as a whole. Please check Appendix A.1 for detailed analysis and proof.\nLemma 5.1 Let MBP be defined using Definition 4.2. Consider a case where the ad- versary's prior belief distribution follows a uniform distribution. That is, $f_P(d) = f_P(d')$, $\\forall d', d \\in D$. If mapping $f_{w|D}(\\cdot|\\cdot)$ satisfies that $\\forall d', d \\in D$,\n$\\frac{f_{w|D}(w|d)}{f_{w|D}(w|d')} \\in [e^{-\\xi}, e^{\\xi}],$"}, {"title": "Lemma 5.2", "content": "Conversely, the following lemma states that if an algorithm offers \u00a7-MBP, it also provides 2\u03be-LDP. This suggests that mechanisms crafted to safeguard the privacy of the dataset as a whole, as evaluated by Bayesian privacy, also afford a degree of protection for individual data points within a local differential privacy framework.\nLemma 5.2 Let MBP be defined using Definition 4.2. Consider a situation where the adversary's prior belief distribution follows a uniform distribution. Consequently, $f_D(d) = f_D(d')$ for any $d,d' \\in D$. If an algorithm is \u00a7-Maximum Bayesian Privacy, then it is (2g)- differentially private. Suppose privacy-preserving mapping $f_{w|D}(\\cdot|\\cdot)$ ensures \u00a7-maximum Bayesian privacy. That is, $\\xi = \\max_{w\\in W, d\\in D} |\\log (\\frac{f_{D|W}(d|w)}{f_{D}(d)})|$. Then, the function $f_{w|D}(\\cdot)$ achieves (2\u03be)-differential privacy\n$\\frac{f_{w|D}(w|d)}{f_{w|D}(w|d')} \\in [e^{-2\\xi}, e^{2\\xi}].$"}, {"title": "Theorem 5.3", "content": "This lemma establishes a bridge between local differential privacy and maximum Bayesian privacy, showing that mechanisms satisfying one privacy criterion also offer a certain level of privacy protection according to the other criterion. This provides insights into the relationship between different privacy definitions and can be useful when designing and analyzing privacy-preserving algorithms in various contexts.\nWith Lemma 5.1 and Lemma 5.2, we are now ready to derive the relationship between MBP and LDP.\nTheorem 5.3 (Relationship between MBP and LDP) Consider a situation in which the adversary's prior belief distribution follows a uniform distribution. Let MBP be defined using Definition 4.2. If an algorithm is \u00a7-LDP, then it is \u00a7-MBP. If an algorithm is \u03be-\u039c\u0392\u03a1, then it is 2\u00a7-LDP.\nRemark: We might also explore the situation where the prior belief distribution of the adversary is e-close to a uniform distribution and the analytical approach would be akin to the current scenario.\nThe first conclusion of Theorem 5.3 addresses a scenario in which the prior belief distribution of an adversary closely resembles a uniform distribution, meaning that the adversary has a relatively unbiased and uniform belief about the data points in the set d. The lemma introduces an algorithm with a privacy-preserving mapping denoted as \u0192W|D(\u00b7|\u00b7). This mapping provides a maximum Bayesian privacy guarantee, where privacy is quantified by the parameter \u00a7. The parameter & quantifies the maximum privacy loss resulting from the mapping, determined by the logarithm of the ratio between the conditional posterior distribution of a data point d given an observation w and the prior distribution of d. In other words, & measures the maximum privacy leakage under this mechanism."}, {"title": "Theorem 5.4", "content": "The second conclusion of Theorem 5.3 implies that if the algorithm provides \u00a7-maximum Bayesian privacy, then it also provides (2g)-differential privacy. This implies that the privacy-preserving mapping not only demonstrates robustness in terms of Bayesian privacy but also fulfills a corresponding concept of privacy, namely, differential privacy. The differential privacy guarantee of mapping is expressed in terms of the ratios of conditional probabilities. Specifically, the ratio of the conditional probabilities of observing w given two different data points d and d' falls within the range of [e-25, e25]. This range specifies the extent to which the mapping distorts the likelihood of observing w given different data points. The larger \u00a7 is, the wider this range becomes, indicating stronger privacy protection. This conclusion shows a connection between maximum Bayesian privacy and differential privacy. It illustrates that a mapping ensuring maximum Bayesian privacy, particularly when the adversary's prior belief distribution approximates uniformity, also confers a degree of differential privacy. This finding holds significance in comprehending and quantifying the privacy assurances offered by such mappings within the realm of privacy-preserving data analysis.\nThe following theorem demonstrates the relationship between average Bayesian privacy and maximum Bayesian privacy.\nTheorem 5.4 (Relationship Between MBP and ABP) Assume that the prior belief of the attacker satisfies that\n$\\frac{f_B(d)}{f_D(d)} \\in [e^{-\\epsilon}, e^{\\epsilon}].$\nLet MBP be defined using Definition 4.2. For any $\\epsilon_{p,m} \\geq 0$, suppose privacy-preserving mapping $f_{w|D}(\\cdot|\\cdot)$ ensuring $\\epsilon_{p,m}$-Maximum Bayesian Privacy. Namely,\n$\\frac{f_{D|W}(d|w)}{f_D(d)} \\in [e^{-\\epsilon_{p,m}}, e^{\\epsilon_{p,m}}],$\nfor any $w \\in W$. Then, we have that the ABP (denoted as $\\epsilon_{p,a}$) is bounded by\n$\\epsilon_{p,a} \\leq \\frac{1}{\\sqrt{2}} \\sqrt{(\\epsilon_{p,m} + \\epsilon) \\cdot (e^{\\epsilon_{p,m}+\\epsilon} - 1)},$"}, {"title": "6 Conclusions", "content": "This paper delves into the core of attack and defense mechanism and construct a universal model for both. This model encompasses a variety of protection mechanisms and attack methods. Within this unified framework, the effectiveness of various attack and defense mechanisms can be assessed. We rigorously explore the conceptual and mathematical intricacies that interlink MBP with LDP-cornerstones in the quest for privacy-preserving machine learning systems. Our seminal contributions, articulated through the relationship between MBP and ABP, demonstrate a fact that MBP usually implies ABP, thus is a theoretically stricter metric. While our theoretical contributions push the envelope in privacy research, they also acknowledge limitations due to the reliance on Bayesian priors."}, {"title": "Appendix A. Relationship between LDP and MBP", "content": "In this section, we derive the relationship between LDP and MBP."}, {"title": "A.1 LDP implies MBP", "content": "The adversary has a prior belief distribution, denoted as $f_P(d)$, that is very close (within $\\epsilon$) to a uniform distribution. In other words, the adversary doesn't have a strong prior bias towards any particular data point in the set D.\nThere exists a mapping function $f_{W|D}(\\cdot|\\cdot)$, which takes a data point $d$ and maps it to a data point $w$ such that for any two data points $d$ and $d'$ in D, the ratio of the conditional probabilities of mapping to $w$ remains within a certain range, specifically between $e^{-\\xi}$ and $e^{\\xi}$. This means that the mapping process is not overly sensitive to the specific input data.\nThe mapping function $f_{w|D}(\\cdot|\\cdot)$ is said to provide a \"privacy-preserving\" transformation that guarantees a maximum Bayesian privacy level of \u00a7. In other words, for any observed value $w$ and any data point $d$ in D, the logarithm of the ratio of the conditional posterior distribution of $d$ given $w$ to the prior distribution of $d$ (i.e., the Bayesian privacy leakage) does not exceed \u00a7 in absolute value. This means that the mapping function provides strong privacy protection by limiting the information leakage about the original data point $d$ when the adversary observes $w$.\nThe following lemma asserts that when the adversary's prior belief distribution is close to uniform, and a certain condition on the mapping function is met, the mapping function provides a high level of privacy protection by limiting the information leakage to a maximum of & in a Bayesian sense when the adversary observes a transformed data point $w$.\nLemma A.1 (LDP implies MBP) Consider the scenario where the prior belief distribu- tion of the adversary is a uniform distribution. That is, $f_P(d) = f_P(d')$ for any $d, d' \\in D$. If $f_{w|D}(\\cdot|\\cdot)$ is \u00c9-LDP. That is, the mapping $f_{w|D}(\\cdot|\\cdot)$ satisfies that\n$\\frac{f_{WD}(w|d)}{f_{w|D}(w|d')} \\in [e^{-\\xi}, e^{\\xi}],$\nthen $f_{w|D}(\\cdot|\\cdot)$ is a privacy-preserving mapping that guarantees \u00a7-maximum Bayesian privacy. That is, $\\log (\\frac{f_{D|W}(d|w)}{f_D(d)})| \\leq \\xi, \\forall w \\in W, d\\in D$"}, {"title": "Proof", "content": "Proof From the definition of \u00a7-LDP, we know that\n$\\frac{f_{W|D}(w|d)}{f_{w|D}(w|d')} \\in [e^{-\\xi}, e^{\\xi}],$\nfor all $d, d' \\in D$. Notice that\n$\\frac{f_{WD}(w|d)}{f_{W|D}(w|d')} = \\frac{f_{D|w}(d|w)f_D(d')}{f_{D|w}(d'|w)f_D(d)}$\nIt implies that $\\frac{f_{D|W}(d|w)}{f_D(d)} < \\frac{f_{D|w}(d'|w)}{f_D(d')}. e^{\\xi}$."}, {"title": "A.2 MBP implies LDP", "content": "The following lemma illustrates that if an algorithm is \u00a7-maximum Bayesian privacy, then it is 2g-Differentially Private.\nLemma A.2 (MBP implies LDP) Consider the scenario where the prior belief distribu- tion of the adversary is a uniform distribution. That is, $f_P(d) = f_P(d')$ for any $d,d' \\in D$. Let $f_{w|D}(\\cdot|\\cdot)$ be a privacy-preserving mapping that guarantees \u00a7-maximum Bayesian pri- vacy. That is, $\\xi = \\max_{w\\in W, d\\in D} |\\log (\\frac{f_{D|W}(d|w)}{f_D(d)})|$. Then, the mapping $f_{w|D}(\\cdot|\\cdot)$ is 2\u00a7-Local Differential Private:\n$\\frac{f_{w|D}(w|d)}{f_{w|D}(w|d')} \\in [e^{-2\\xi}, e^{2\\xi}].$\nProof From the definition of maximum Bayesian privacy leakage, we know that\n$\\frac{f_{D|W}(d|w)}{f_D(d)} \\in [e^{-\\xi}, e^{\\xi}],$\nfor any $w \\in W$ and $d \\in D$. Therefore, for any $d, d' \\in D$ we have\n$\\frac{f_{w|D}(w|d)}{f_{w|D}(w|d')} = \\frac{f_{D|w}(d|w)f_D(d')}{f_{D|w}(d'|w)f_D(d)} \\in [e^{-2\\xi}, e^{2\\xi}].$"}, {"title": "Theorem A.3", "content": "From the definition of Differential Privacy, we know that it is 25-Differentially Private.\nCombining the results in Appendix A.1 and Appendix A.2, we are now ready to analyze the relationship between MBP and LDP, which is shown in the following theorem.\nTheorem A.3 (Relationship between MBP and LDP) Consider the scenario where the prior belief distribution of the adversary is a uniform distribution. That is, $f_P(d) = f_P(d')$ for any $d, d' \\in D$. If an algorithm is 2\u00a7-LDP, then it is 2F-MBP. If an algorithm is \u03be- maximum Bayesian privacy, then it is 2\u00a7\u00c9-LDP."}, {"title": "Appendix B. Relationship Between Average Bayesian Privacy and Maximum Bayesian Privacy", "content": "In this section, we introduce the relationship between average Bayesian privacy and maximum Bayesian privacy.\nTheorem B.1 (Relationship Between MBP and ABP) Assume that the prior belief of the attacker satisfies that\n$\\frac{f_B(d)}{f_D(d)} \\in [e^{-\\epsilon}, e^{\\epsilon}].$"}, {"title": "D.1 Estimation for", "content": "C\u2081 quantifies the average square root of the Jensen-Shannon divergence between the ad- versary's belief distribution about the private information before and after observing the unprotected parameter", "as": "n\\begin{aligned}\n&JS(\\hat{F}^{\\mathcal{O}}|| F^{\\mathcal{B}})=\\frac{1}{2} \\int_{\\mathcal{D}} f_{\\mathcal{O}}(d) \\log \\frac{f_{\\mathcal{O}}(d)}{\\frac{1}{2}\\left(f_{\\mathcal{O}}(d)+f_{\\mathcal{B}}(d)\\right)} d \\mu(d)+\\frac{1}{2} \\int_{\\mathcal{D}} f_{\\mathcal{B}}(d) \\log \\frac{f_{\\mathcal{B}}(d)}{\\frac"}]}