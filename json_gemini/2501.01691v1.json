{"title": "VidFormer: A novel end-to-end framework fused by 3DCNN and Transformer for Video-based Remote Physiological Measurement", "authors": ["Jiachen Li", "Shisheng Guo", "Longzhen Tang", "Guolong Cui", "Lingjiang Kong", "Xiaobo Yang"], "abstract": "Remote physiological signal measurement based on facial videos, also known as remote photoplethysmography (rPPG), involves predicting changes in facial vascular blood flow from facial videos. While most deep learning-based methods have achieved good results, they often struggle to balance performance across small and large-scale datasets due to the inherent limitations of convolutional neural networks (CNNs) and Transformer. In this paper, we introduce VidFormer, a novel end-to-end framework that integrates 3-Dimension Convolutional Neural Network (3DCNN) and Transformer models for rPPG tasks. Initially, we conduct an analysis of the traditional skin reflection model and subsequently introduce an enhanced model for the reconstruction of rPPG signals. Based on this improved model, VidFormer utilizes 3DCNN and Transformer to extract local and global features from input data, respectively. To enhance the spatiotemporal feature extraction capabilities of VidFormer, we incorporate temporal-spatial attention mechanisms tailored for both 3DCNN and Transformer. Additionally, we design a module to facilitate information exchange and fusion between the 3DCNN and Transformer. Our evaluation on five publicly available datasets demonstrates that VidFormer outperforms current state-of-the-art (SOTA) methods. Finally, we discuss the essential roles of each VidFormer module and examine the effects of ethnicity, makeup, and exercise on its performance.", "sections": [{"title": "I. INTRODUCTION", "content": "PHYSIOLOGICAL signals measurement has long been a significant research domain that can provide insights into the health condition of the human body [1]. Physiological signals such as heart rate (HR), heart rate variability (HRV), and respiratory rate (RR) are particularly crucial for human body health accurate assessment [2], [3]. In recent years, the Remote Photoplethysmography (rPPG) has gained considerable attention as a research focal point [4]\u2013[10]. In comparison to traditional contact-based methods like Electrocardiography (ECG) [11] and Photoplethysmography (PPG) [12], rPPG offers distinct advantages by eliminating the physical contact with patients. Consequently, it mitigates concerns related to allergies, disease transmission, and patient discomfort [13]. Therefore, rPPG is widely used in fields such as emotion computing, intelligent assistants, and biometric recognition [5], [14].\nThe rPPG approaches utilize RGB cameras for the purpose of capturing facial skin and subsequently analyzing it in order to obtain PPG signals [15]\u2013[17]. This principle of rPPG is that the absorption rates of hemoglobin, oxyhemoglobin, and melanin in the blood of human skin exhibit variations across different spectra, and their concentrations fluctuate in response to the movements of heart [18]. Specifically, during cardiac contraction, the blood volume within the blood vessels increases, causing an escalation in the absorption of specific light spectra. Conversely, when the cardiac diastole, the blood volume within the vessels diminishes, resulting in a reduction in the absorption of light within that particular spectrum [5], [19], [20]. Accordingly, through the analysis of facial skin features observed in the video, the PPG signal of the human body can be obtained from facial video. Through the analysis of remote Blood Volume Pulse (BVP), the physiological parameters including HR, HRV, and RR of human body can be feasible to measure and evaluate. Nevertheless, BVP are susceptible to interference from external factors such as variations in ambient lighting, head movements, obstructions on the face, and different facial expressions, thereby generating non-periodic noise signals [21], [22]. To overcome these challenges, the traditional approaches applies blind source separation techniques and skin reflection models to evaluate alterations in the illumination of human skin, thus facilitating the extraction of BVP [23], [24]. Nonetheless, these methods heavily depend on prior knowledge, e.g., De Hann et al. assumes that under white light, people with different skin tones have the same color standard [5], hindering their application in diverse environmental conditions.\nWith the progressive advancement of deep learning, an increasing number of methodologies employing deep neural networks (DNNs) for acquiring BVP have been proposed. These methodologies accomplish the mapping of blood volume fluctuations in facial video skin to BVP through the training and fitting of models. The most extensively trained neural network type is the convolutional neural network (CNN). Initially, Chen et al. introduced the utilization of 2DCNN for processing rPPG signal tasks [25], subsequently leading to the development of numerous other two-dimensional neural networks [9], [26], [27]. Nonetheless, the strong temporal correlation between BVP and facial video data makes it arduous to model long-term time series exclusively employing two-dimensional convolution [28]. Thus, Yu et al. proposed the adoption of 3D Convolution Neural Network (3DCNN) [9], and Yu et al. recommended the use of Transformer network architectures [29]. 3D convolution operations enable effective modeling of video data, facilitating the acquisition of temporal and spatial information [30], while Transformers inherently possess the capability of long-term global modeling [31]. However, convolution operations lack effective global modeling capabilities, and Transformers lack a corresponding inductive bias and local perception abilities [32]. Consequently, effectively addressing rPPG tasks by overcoming the deficiencies of both approaches is a considerably intriguing issue.\nIn this paper, we proposed a novel fusion framework that combines CNN with Transformer named VidFormer. To the best of our knowledge, this is the first fusion framework that integrates CNN and Transformer for processing rPPG tasks. Our framework successfully combines the Transformer's exceptional global modeling capability and the CNN's inductive bias, facilitating multi-level information exchange and fusion. As a result, it effectively accomplishes the reconstruction task of BVP signal. The proposed framework comprises of five modules: the Stem module, Local Convolution Branch, Global Transformer Branch, CNN and Transformer Interaction module (CTIM), and rPPG generation module (RGM). The Stem module serves to localize and eliminate redundant video information while preserving the primary video information. Subsequently, the simplified video information is fed into the CTIM to extract relevant information. Owing to the non-uniform blood distribution in the human face and the impact of effective scattering area and temporal variations of facial blood vessels, we utilized the CNN branch in our proposed CTIM method to capture local information of the human face. Additionally, we developed a spatial-temporal multi-head attention module for the CNN branch, enabling us to effectively capture the temporal variation patterns exhibited by the human face. Moreover, we employed the Transformer branch in CTIM to extract global information encompassing facial movements, changes in ambient lighting, and other relevant factors. Furthermore, we devised a multi-head attention mechanism specifically tailored to the characteristics of video data. In the RGM module, we processed the feature information obtained from CTIM to generate an rPPG signal, which integrates the spatial-temporal information, global and local information of the input data."}, {"title": "II. RELATED WORK", "content": "The measurement techniques for estimating the BVP from facial videos encompass blind source separation, skin reflection model, and deep learning approaches.\nBlind source separation was one of the initial methods employed for rPPG measurement. This technique postulates that the facial skin tone variations arise as a linear combination of both the PPG signals and the ambient environmental signals. It assumes the independence of each signal source and estimates the coefficients for each source [4]. By calculating the confusion matrix, the signals can be successfully separated. Macwan et al. utilized prior knowledge, namely auto-correlation constraints and skin chromaticity change constraints, to guide ICA in extracting BVP [38], while Favila et al. utilized ICA for preprocessing to improve the accuracy of rPPG signal extraction [39].\nThe skin reflection model mainly utilizes the principle of skin's reflection of light and converts the skin chromaticity in the RGB color space to other color spaces, in order to obtain better rPPG estimation. Haan et al proposed that noise caused by motion can be eliminated by projecting skin chromaticity into different color spaces [5]. Moreover, to enhance the efficacy of mitigating shadow noise and the influence of diverse skin tones resulting from motion, a technique based on standardizing skin tones under white-light conditions has been proposed [20].\nWith the development of DNNs, numerous rPPG detection methods based on DNNs have been proposed [9], [17], [25]\u2013[27]. Chen et al. introduced DeepPyhs, a system based on"}, {"title": "III. METHOD", "content": "The skin reflection model proposed in [5], [20] is based on the dichromatic model [53], which can be expressed as\n$C_k(t) = I(t)(v_s(t) + v_a(t)) + v_n(t)$\nwhere $C_k(t)$ denotes the RGB channels (in column) of the k-th skin-pixel. $I(t)$ denotes the luminance intensity level, which absorbs the intensity changes due to the light source as well as to the distance changes between light source, skin tissue and camera. $v_s(t)$ and $v_a(t)$ are the specular reflection and diffuse reflection. $v_n(t)$ indicates the quantization noise of the camera sensor.\nHowever, the model focuses more on displaying the results of light reflected and absorbed by the skin in the RGB image of the camera, which is not suitable for rPPG reconstruction. Actually, the rPPG signal reconstruction task involves modeling the mapping relationship between BVP signals and skin color. Assuming the ground truth BVP signal is yt, then we have\n$C_k(t) = I(t)[l_s(y_t(t), p, t) + l_a(y_t(t), p,t)] + v_n(t)$\nwhere p denotes the location of a single skin pixel, $l_a$ and $l_s$ are the mapping of the specular reflection and diffuse reflection. We posit a robust correlation between variations in BVP signals and changes in skin color. This correlation arises from the fluctuating volume of blood within superficial blood vessels, which governs both specular and diffuse light reflections, consequently eliciting temporal shifts in skin color. pelucidates that diverse lighting conditions across distinct skin regions yield disparate specular and diffuse reflection [18].\nActually, when the face is completely stationary, the illumination remains constant, and the video frame rate and BVP signal sampling rate align, the skin color and blood flow of the face are in a bijective relationship as shown in Fig. 1. However, the movement of the human head and changes in lighting can modulate this relationship, resulting in non-bijection. We believe in that the l mapping function should include random functions related to human motion, changes in lighting, and other time factors. Therefore, l is not only related to p and $y_t(t)$, but also to t.\nBased on the above analysis, the influence of changes in blood flow within cutaneous blood vessels on the absorption spectra of the human skin results in discernible periodic alterations within the BVP. Simultaneously, the rPPG signal becomes intricately entwined with noise signals stemming from ambient lighting conditions and human head movements. These coupled signals are reflected in the RGB video data. Therefore, we utilize the prior knowledge mentioned above to design VidFormer and achieve accurate reconstruction of BVP signals."}, {"title": "B. Model Overview", "content": "VidFormer employs a dual-branch architecture, comprising the Local Convolution Branch and Global Transformer Branch, to capture both local and global features as shown in Fig. 2. Firstly, the training data will be fed into a stem block consisting of a 3D convolution layer with a kernel size of 3 for preliminary feature extraction. Subsequently, the Local Convolution Branch is characterized by a hierarchical structure and a spatiotemporal convolutional attention mechanism. This branch employs convolutional operations to extract local information progressively, thus enlarging the receptive field. This approach directs the attention of network towards to the skin regions where color changes most aptly align with the rPPG signal in time domain. Concurrently, the Transformer branch undertakes a comprehensive modeling of the representation of BVP within video data from a global perspective. By designing ST-MHSA, the emphasis of transformer is placed on delineating strong correlations among disparate image regions from a spatial standpoint and exploring the periodicity of signals in the time domain. This facilitates an effective modeling of the global influences of environmental lighting and surrounding ambient noise, thus it achieves decoupling between environmental noise and rPPG signal. CTIM promotes the fusion of locally and globally extracted information from the two branches, thereby synthesizing a comprehensive feature for rPPG signal. In this framework, the Local Convolution Branch refines its hierarchical features by incorporating the gleaned global information, thereby mitigating undue local influences on the branch. Simultaneously, the Transformer branch leverages the acquired local information to induce bias effectively, thereby enhancing the convergence speed of the Transformer branch. This sophisticated interplay within CTIM optimally integrates local and global features, contributing to the refined and expedited convergence of the overarching model."}, {"title": "C. Local Convolution Branch", "content": "We design Local Convolution Branch for local feature extraction of data to facilitate the attention of network to key areas of input data. We expect these regions to better reflect the mapping relationship between BVP signals and skin color changes, as well as to be less affected by environmental noise. This branch consists of two parts: GA-3DCNN and BS-3DCNN.\n1) GA-3DCNN: GA-3DCNN is based on BS-3DCNN and designs temporal and spatial attention mechanisms to enhance the weights of input data as shown in Fig. 3. The GA-3DCNN model incorporates Spatial Attention, Time Attention, and a BS-3DCNN component. The Spatial Attention focuses on the global spatial information across the entire image frame and accentuates correlations among different spatial patches. Consequently, this process encourages the BS-3DCNN to prioritize patches exhibiting pronounced correlations within each other when analyzing local information. Addtional, the Time Attention places greater attention on the temporal correlations in the input features of GA-3DCNN. It posits that signals exhibiting periodicity need to increase weighting of it to effectively capture features associated with periodic signals. This approach ensures that GA-3DCNN effectively captures both spatial and temporal dynamics changes, thereby enhancing its capacity to discern and model intricate patterns within the input data.\nSpecifically, assuming that the input matrix of GA-3DCNN is $X^{(k-1)} \\in R^{B \\times C \\times T \\times H \\times W}$, where $B, C, T, H, W$ represent the batch size, number of channels, frame length, height, and width of the input features respectively. k is the input data of the kth layer. After undergoing different adaptive average pooling, $X_1^{(k)}$ can be written as\n$X_s^{(k)} = \\xi_s(X^{(k-1)})$\n$X_t^{(k)} = \\xi_t(X^{(k-1)})$\nwhere $\\xi_s$ and $\\xi_t$ denote the adaptive avg-pool function. Additional, $X_s^{(k)} \\in R^{B \\times C \\times 1 \\times H \\times W}$ and $X_t^{(k)} \\in R^{B \\times C \\times T \\times 1 \\times 1}$ are the input data for Spatial Attention and Time Attention respectively and the form of multi-head attention mechanism is shown in the Fig. 4. Therefore, the output after multi-head attention in Fig. 4 can be written as\n$X_{o,i}^{(k)} = (\\xi(\\frac{\\mathcal{T}}{ \\sqrt{C}}(\\phi_1(X_i^{(k)})\\phi_2(X_i^{(k)})))\\phi_1(X_i^{(k)})), (i = s,t)$\nwhere s denotes the convolution operation, $\\xi$ is the Softmax function and $X_o^{(k)}$ represents the output of Spatial-Attention or Time-Attention. As both Spatial Attention and Time Attention excavate features from input data, the neural network undergoes a reconsideration of feature distributions, prompting a consequential realignment of model parameters. This phenomenon engenders an amplification in the variance of layer weights within the network and alleviate effect on the disappearance of gradients. Therefore, the weighted input obtained by 3DCNN can be represented as\n$X_g^{(k)} = \\sigma (X_s^{(k)} + X_t^{(k)}) \\odot X^{(k-1)}$\nwhere $\\sigma$ is the sigmoid function and $\\odot$ denotes the matrix dot product. Subsequently, the weighted features will be fed into BS-3DCNN for feature extraction.\n2) BS-3DCNN: The BS-3DCNN is employed for the purpose of extracting localized features from the input data. This is facilitated by its inherent hierarchical architecture, which spans the entirety of the 3DCNN branch. Consequently, the 3DCNN consistently iterates through the extraction of pertinent local features, thereby progressively enlarging its receptive field. This augmentation empowers the network to model the global information of input data and the BS-3DCNN framework can be seen in the Fig. 5. With the extraction of BS-3DCNN, the input from Spatial Attention and Time Attention $X_g^{(k)}$ can be transformed into $X_{tc}^{(k)}$\n$X_{tc}^{(k)} = Gelu(GroupNorm(\\phi_1(\\phi_2(X_g^{(k-1)} + X_g^{(k)}))))) + \\phi_2(X_g^{(k-1)}+X_g^{(k)}))$\nwhere $X_{tc}^{(k-1)}$ indicates the output of the (k \u2212 1)th Trans-Conv Block in CTIM. $\\phi_1$ and $\\phi_2$ are the Convolution layer with kernelsize = 3 and kernelsize = 1 respectively. After the aforementioned architectural design, the BS-3DCNN adeptly captures localized feature information within the input feature. This extracted features encompasses both periodic signal components and environmental noise signals. Subsequently, Spatial Attention and Time Attention mechanisms are employed to weight the input feature information, leveraging a spatiotemporal perspective. This weighting compels a reorganization of input feature distributions along the weighted direction, thereby enhancing the capacity of network for information fitting. Nonetheless, convolutional operations primarily focus on amalgamating the local information of features. Despite the gradual expansion of the receptive field facilitated by pool function within hierarchical feature extraction structures, it is undeniable that certain features may undergo information loss during the pooling process. Hence, to address this concern, we introduce a Transformer branch."}, {"title": "D. Global Transformer Branch", "content": "As the commendable ability of transformer in global modeling, we devised a dedicated Transformer branch to extract comprehensive global information from the input data. We aim to devise a Global Transformer branch to effectively capture the nuanced dynamics cause by head movements and lighting variations. This is because during a short time window, the changes in lighting are very slow as indicated in Fig. 2.\nAlthough Local Convolution Branch is structured hierarchically and incorporates GA-3DCNN, due to the structural limitations of small convolution kernels, using Local Convolution Branch alone requires a sufficiently deep network to expand the receptive field and be sensitive to changes in lighting, personnel movement, and so on. However, deeper network architectures are susceptible to challenges such as unstable training and convergence difficulties. To address these concerns, we integrate Transformer architecture to facilitate comprehensive global information extraction.\nGiven the format of input data as video data, we undertook a redesign of the multi-head attention mechanism as ST-MHSA, tailored specifically for the extraction of intricate video information features. Firstly, based on the ViViT [55], we slice the video data into cube patches to ensure that each ptach contains both temporal and spatial information as show in Fig. 7. The obtained cube slices are embedded and add the positional information for each cube slice, then the cube slices are input into the Transformer branch. This process can be expressed in mathematical form as\n$X^{(T)} = Lin(N(I)) + Pos$\nwhere $X^{(0)} \\in R^{B \\times P \\times D}$ denotes the input of the Global Transformer Branch, P is the number of patches and D indicates the embedding dim. Lin is the linear layer. $I \\in R^{B \\times C \\times T \\times H \\times W}$ represents the input of the VidFormer and $N$ is the cube slice operation. In addition, Pos is a random number that satisfies a Gaussian distribution with a mean of 0 and a variance of 1. Subsequently, $X^{(T)}$ is inputted into the Global Transformer Branch.\nConsequently, we accord equal importance to both temporal and spatial features, advocating for feature extraction methodologies that exhibit parity in efficacy across both domains as shown in Fig. 9.\nIt is worth noting that in Fig. 9, the multi-head self attention mechanism in terms of time and space is designed as a dual branch structure and provide the abundant spatial-temporal information for the input $X_{ct}^{(k)}$ and $X_T^{(k-1)}$. $X_T^{(k-1)}$ is the output of the (k\u2212 1)th Transformer block. $X_{ct}^{(k)}$ indicates the output of kth Conv-Trans Block in CTIM. Before inputting data into Spatial-MHSA and Time-MHSA separately, it is necessary to rearrange $X_{ct}^{(k)}$ and $X_T^{(k-1)}$ into the form fitting Spatial-MHSA and Time-MHSA, which can be expressed as\n$X_{TS}^{(k)} = rearrange(X_T^{(k-1)} + X_{ct}^{(k)})$\n$X_{TT}^{(k)} = rearrange(X_T^{(k-1)} + X_{ct}^{(k)})$\nwhere $X_{TS}^{(k)} \\in R^{(B \\times n_t) \\times (n_h \\times n_w) \\times D}$ and $X_{TT}^{(k)} \\in R^{(B \\times n_h \\times n_w) \\times n_t \\times D}$ indicate the input of Spatial-MHSA and Time-MHSA respectively. $n_t, n_h, n_w$ represent the patch numbers obtained by dividing along T, H, and W of I. Furthermore, $n_t \\times n_h \\times n_w = P$.\nWe follow the approach proposed by [55], which entails the partitioning of temporal and spatial patches into $X_{TS}$ and $X_{TT}$, respectively. This division aims to guide the network towards prioritizing $X_{TS}$ and $X_{TT}$ during the implementation of the multi-head self-attention mechanism. It is noteworthy that subsequent to traversing the LayerNorm module and the multi-head self-attention module across temporal and spatial dimensions, we leverage their outputs as spatial-temporal features for the $X_T$. This choice is predicated on the intricate interplay between temporal and spatial features, where processing these intertwined data dimensions in isolation poses the loss of information. Consequently, employing the resultant outputs as attention weights for input modulation serves to deal with the information loss in decoupling tightly coupled data. In summary, the output of ST-MHSA can be represented as\n$X_{MT} =[M_S(L(X_{TS}^{(k)})) + X_{TS}^{(k)}+M_T(L(X_{TT}^{(k)})) + X_{TT}^{(k)}]$\nwhere $M_S$ and $M_T$ indicate the Spatial-MHSA and Time-MHSA respectively and $L$ denotes the LayerNorm. Subsequently, the output obtained with both temporal and spatial dimensions of information $X_{MT}$ will be fed into the subsequent steps of the Transformer for further processing. The design of Transformer follows the architecture of [31]. It is of significance to observe in Fig. 8 that the input of the ST-MHSA encompasses not only the output derived from the ST-MHSA module but also incorporates the output generated by the Conv-Trans Block in CTIM. The latter serves the purpose of incorporating pertinent local information into the input of ST-MHSA. Furthermore, it is notable that the architectural design of the FeedForward and LayerNorm follow the principles delineated in the work by [31]. Therefore, the output of the transformer can be written as\n$X_{FF}^{(T)} = FFN(L(X_{MT})) + L(X_{MT})$\nwhere FFN is the FeedForward block. By passing the Global Transformer Branch and Local Convolution Branch, both global and local features are extracted, thereafter feeding into the rPPG generation module to reconstruct the rPPG signal. Nonetheless, the utilization of distinct branches for global and local feature extraction poses inherent risks of information incompleteness. Moreover, owing to the lack of inductive bias capability of Transformers, Transformer is susceptible to overfitting on small datasets and convergence challenges. Conversely, convolution operations are born with prior assumptions, but shortage of Global modeling capability in comparison to Transformers. Consequently, by devising information exchange modules between these branches, it is envisaged that the deficiencies in the two branches can be effectively mitigated."}, {"title": "E. CTIM", "content": "CTIM is devised to facilitate seamless information exchange between the two branches, aiming to engender a amalgamation of multi-level information and impose inter-branch constraints. It is noteworthy that the inherent challenge associated with extracting global information features through convolutional methodologies is difficlut to solve. This challenge stems from the inherent focus of convolution operations on local correlation information dictated by the convolution kernel size. Conversely, while Transformers excel in global-level modeling, their lack of inherent assumptions akin to those embedded within convolutional paradigms renders their convergence on small datasets intricate. How to effectively overcome the shortcomings of these two is also our original intention in designing CTIM. CTIM includes Trans-Conv Block and Conv-Trans Block."}, {"title": "F. RGM", "content": "RGM is used as a downstream task module for the generation of BVP and the structure can be seen in the Fig. 11. RGM generates BVP based on the features extracted from Local Convolution Branch and Global Transformer Branch, respectively. Given the invariant shape of the T-dimension across the features within the Local Convolution Branch, we employ a 3-dimensional convolution to reduce the number of feature channels while simultaneously amalgamating the spatial dimensions via 2-dimensional adaptive average pooling, ultimately obtaining the rPPG signal. For the Global Transformer Branch, we diminish the feature channel of the output from the Global Transformer Branch by leveraging 1-Dimension convolution. This is followed by feature flattening and rPPG signal facilitated by Multilayer Perceptron (MLP). Ultimately, after undergoing RGM, we have\n$R_1 = \\varphi_1(\\xi(X_{tc}^{(k)}))$\n$R_2 =Lin(\\varphi_2(\\xi(X_T^{(k)})))$\nwhere $\\xi$ is the adaptive Avg-pool function, $R_1$ and $R_2$ are the estimated BVP signals of RGM with the features originating from Local Convolution Branch and Global Transformer Branch. Subsequently, the obtained $R_1$ and $R_2$ will be used for network optimization, HR measurement and HRV estimation."}, {"title": "G. Network Optimization", "content": "To recover the rPPG signal with accurate systematic peak instants, the negative Pearson coefficient is used as a partial loss $L_p$ in network optimization to gradually approximate the actual rPPG signal, following [13], [47]. In addition, the combination of negative Pearson coefficient loss $L_p$ and Smooth L1 loss $L_1$ is used as the overall loss for network optimization [9], [56], which can be represented as\n$L_p = 1 - \\frac{\\sum_{i=1}^{l}(y_i - \\bar{y})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^{l}(y_i - \\bar{y})^2\\sum_{i=1}^{l}(Y_i - \\bar{Y})^2}}$\nwhere $y_i$ and $Y_i$ present the ith sampling points of the predict rPPG signal and the true rPPG siganl. And l is the length of rPPG signal. In addition, $\\bar{y}$ and $\\bar{Y}$ indicate the mean value of the predict rPPG signal and the true rPPG siganl. And the $L_1$ can be expressed as\n$L_1 =\\begin{cases}\n0.5(Y_i - y_i)^2, & |(Y_i - y_i)| < 1\\\\\n|(Y_i - y_i)| - 0.5, & otherwise\n\\end{cases}$\nEventually, $L_p$ and $L_1$ are combined to the final loss L\n$L = \\alpha * L_p + (1 - \\alpha) * L_1$\nwhere \u03b1 is a hyper-parameter to balance the $L_p$ and $L_1$ and 0.5 in this paper. It is worth noting that VidFormer will simultaneously output estimated BVP signals from both Local Convolution Branch and Global Transformer Branch. Therefore, it is necessary to optimize $R_1$ and $R_2$ separately, and average the output HR from $R_1$ and $R_2$ as the final HR."}, {"title": "IV. EXPERIMENTS", "content": "We conduct our experiments on five public datasets: UBFC-rPPG [33], PURE [34], DEAP [35], ECG-fitness [36], and COHFACE [37].\nOur model is trained for 150 epochs using one single NVIDIA GeForce RTX 4090 and PyTorch 1.13 [58] on UBFC-rPPG, PURE and 100 epochs on COHFACE, ECG-fitness. For DEAP, our model is trained for 30 epochs. We use the AdamW optimizer [59] and Cosine Annealing Warm Restart learning rate adjustment strategy to adjust the learning rate. The maximum learning rate and the Initial learning rate are set to 8 \u00d7 10\u22125, the minimum is set to 2 \u00d7 10\u22129, and the batch size is set to 2. Meanwhile, the weight decay is set to 5 x 10-4."}, {"title": "C. Evaluation Protocol", "content": "[25], [28], [46] utilized estimated BVP signal to calculate HR, RF, and HRV. These estimated values were then compared with corresponding ground truth measurements to evaluate the performance of the network. We follow their methods to calculate HR and HRV using estimated BVP on five datasets UBFC, PURE, COHFACE, ECG fitness, and DEAP. Moreover, we conducted cross dataset testing between datasets UBFC, PURE and COHFACE to validate the effectiveness of our proposed model. The calculation method for HR, RF and HRV is implemented using the HeartPy toolbox [60].\nWe follow the work of [15], [20] to use mean absolute error (MAE), root mean square error (RMSE) and Pearson's correlation coefficient (r) as evaluation metrics for HR. For HRV, we follow [61], [62] calculating the three attributes of HRV, i.e., low frequency (LF), high frequency (HF), and the LF/HF ratio. The results of LF and HF are obtained by calculating the interbeat intervals of BVP under the low frequency (0.04 Hz to 0.15 Hz) and high frequency (0.15 Hz to 0.4 Hz) bands. For each attributes of HRV, we follow [9], [16], [63] to employ standard deviation (STD), RMSE and r as evaluation metrics. For RF, we also report the STD, RMSE and ras per most comparable methods [5], [9], [10], [16], [19], [20], [27], [47], [63], [64]. In the context of MAE, RMSE, and STD, smaller values indicate better estimated HR result, whereas for r, the closer the value of r is to 1, the better the result."}, {"title": "D. Results", "content": "Notably, our comparative analysis underscores a pronounced discrepancy between traditional methodologies and those rooted in deep learning paradigms. This disparity arises from the inherent limitations of traditional methods, which often hinge on predefined assumptions that may inadequately capture the intricacies of diverse environments. Furthermore, the efficacy of traditional feature extraction techniques is impeded by constraints pertaining to information density of features and the presumptions underlying feature selection, thereby impeding the reconstruction of BVP signals. The DNN-based"}, {"title": "V. ABLATION EXPERIMENTS", "content": "We conduct ablation study from five aspects: GA-3DCNN, ST-MHSA, Local Convolution Branch, Global Transformer Branch and CTIM.\nThe GA-3DCNN model is specifically engineered to discern and extract intricate local spatial and temporal features within the input data. Consequently, in order to ascertain the efficacy"}, {"title": "VI. DISCUSSION", "content": "In this section, we discuss the impact of ethnicity, human exercise, and makeup on our model separately.\nAnother challenge in rPPG tasks is the presence of makeup on testers. rPPG estimates HR changes based on the absorption and reflection of light, which are influenced by blood flow volumn in the blood vessels. When a tester wears makeup, it can alter the absorption and reflection intensity of light, introducing additional environmental noise and potentially compromising the accuracy of HR estimations. Therefore, we categorized the COHFACE dataset into two subsets based on the presence of makeup: COHFACE without makeup (COHFACE-woM) and COHFACE with makeup (COHFACE-wM). Each subset was further divided into training and testing sets. Separate training and testing were then conducted on their respective training and testing sets.\nIn the final chapter, we will examine the impact of tester movement on the performance of VidFormer."}, {"title": "VII. CONCLUSION", "content": "In this paper, we introduce VidFormer, which comprises five key modules: Stem, Local Convolution Branch, Global Transformer Branch, CTIM, and the RGM. We propose an improved dichromatic model tailored to BVP signal reconstruction and VidFormer is designed based on this enhanced model. The Stem module is responsible for extracting preliminary features from the input data. The Local Convolution Branch captures local features and provides prior information to the Global Transformer Branch. Local Convolution Branch includes GA-3DCNN and BS-3DCNN, where GA contributes global spatiotemporal features, mitigating data bias. The Global Transformer Branch extracts global features from the input data. It includes S-MHSA, which establishes relationships between different skin regions, and T-MHSA, which models movement and lighting changes over time. The CTIM module facilitates the exchange and fusion of features between the Local Convolution Branch and the Global Transformer Branch, reducing feature biases and introducing inductive bias to the Global Transformer Branch. The RGM is designed to produce BVP signals."}]}