{"title": "PDM-SSD: SINGLE-STAGE THREE-DIMENSIONAL OBJECT\nDETECTOR WITH POINT DILATION", "authors": ["Ao Liang", "Haiyang Hua", "Jian Fang", "Wenyu Chen", "Huaici Zhao"], "abstract": "One of the important reasons why grid/voxel-based three-dimensional (3D) object detectors can\nachieve robust results for sparse and incomplete targets in Light Detection And Ranging (LiDAR)\nscenes is that the repeated padding, convolution, and pooling layers in the feature learning process\nenlarge the model's receptive field, enabling features even in space not covered by point clouds.\nHowever, they require time- and memory-consuming 3D backbones. Point-based detectors are more\nsuitable for practical application, but current detectors can only learn from the provided points, with\nlimited receptive fields and insufficient global learning capabilities for such targets. In this paper,\nwe present a novel Point Dilation Mechanism for single-stage 3D detection (PDM-SSD) that takes\nadvantage of these two representations. Specifically, we first use a PointNet-style 3D backbone for\nefficient feature encoding. Then, a neck with Point Dilation Mechanism (PDM) is used to expand the\nfeature space, which involves two key steps: point dilation and feature filling. The former expands\npoints to a certain size grid centered around the sampled points in Euclidean space. The latter fills\nthe unoccupied grid with feature for backpropagation using spherical harmonic coefficients and\nGaussian density function in terms of direction and scale. Next, we associate multiple dilation centers\nand fuse coefficients to obtain sparse grid features through height compression. Finally, we design\na hybrid detection head for joint learning, where on one hand, the scene heatmap is predicted to\ncomplement the voting point set for improved detection accuracy, and on the other hand, the target\nprobability of detected boxes are calibrated through feature fusion. On the challenging Karlsruhe\nInstitute of Technology and Toyota Technological Institute (KITTI) dataset, PDM-SSD achieves\nstate-of-the-art results for multi-class detection among single-modal methods with an inference\nspeed of 68 frames. We also demonstrate the advantages of PDM-SSD in detecting sparse and\nincomplete objects through numerous object-level instances. Additionally, PDM can serve as an\nauxiliary network to establish a connection between sampling points and object centers, thereby\nimproving the accuracy of the model without sacrificing inference speed. Our code will be available\nat https://github.com/AlanLiangC/PDM-SSD.git.", "sections": [{"title": "1 Introduction", "content": "LiDAR (Light Detection and Ranging) is an active sensor with excellent anti-interference capability, and its output\npoint cloud can provide an accurate 3D representation of the scene. 3D object detection from point clouds has become\nincreasingly popular thanks to its wide applications, such as autonomous driving and virtual reality. Currently, many\npoint cloud-based 3D detection models have been proposed and achieved state-of-the-art performance on various public\ndatasets, such as KITTI [1] and Waymo [2]."}, {"title": "2 Related Work", "content": "In this paper, we focus on single model 3D object detectors based on pure point clouds. Based on the structural types of\npoint clouds input into the 3D object detectors, we provide a brief overview."}, {"title": "2.1 Grid-based 3D object detectors", "content": "Grid-based 3D object detectors first convert point cloud into discrete grid representations such as voxels, pillars, BEV\nfeature maps, and FV feature maps. Voxels are 3D cubes that contain points inside voxel cells. They can preserve\nthe spatial information of the original point cloud to a great extent while reducing the number of computational units.\nVoxelNet [21] is a pioneering work that uses sparse voxel grids and proposes a novel voxel feature encoding (VFE)\nlayer to extract features from the points inside a voxel cell. Building upon VoxelNet, VoxelNeXt [22] utilizes a fully\nsparse structure for feature extraction and detection heads, partially overcoming the high memory consumption issue\nof VoxelNet. Currently, most state-of-the-art detectors on public datasets use voxel-based feature extractors, such as\nDistillBEV [23] and LidarMultiNet [24]. Pillars can be seen as special voxels with a voxel size of 1 in the vertical\ndirection. PointPillars [25] is a seminal work that introduces the pillar representation, and then PillarNet [26] and\nFastPillars [27] add additional encoder networks to enhance feature learning in the model structure. Compared to\nvoxel-based detectors, pillar-based ones have fewer grid cells, making them more similar to 2D image detection\nand enabling faster inference speed. The Bird's-eye view (BEV) feature map is a dense 2D representation typically\nobtained by projecting voxel, pillar, and raw point cloud data. BEV-based detectors [4, 5, 6, 7, 8, 9] usually require the\naddition of cell-level features to expand the feature space, such as height and density. The feature learning process\nfor those detectors is similar to 2D detection tasks. It is worth noting that the BEV representation of the scene and\nperception results greatly simplifies the environmental space in which the objects are located, making it easy to\npropagate information to downstream tasks such as path planning and control [28]. Therefore, BEV-based detectors are\ncurrently a hot research direction and are gradually forming a new generation of BEV perception paradigm with multiple\ntasks and modalities [29, 30, 31]. The range image is a dense 2D representation that contains 3D distance information\nin each pixel. LaserNet [32] is a pioneering work that detects 3D objects from range images, utilizing the deep layer"}, {"title": "2.2 Point-based 3D object detectors", "content": "Different from grid-based methods, point-based methods [36, 13, 37] directly learn geometry from unstructured point\nclouds. Specifically, point clouds are first passed through a point-based backbone network, where points are gradually\nsampled and features are learned by point cloud operators, further generating specific proposals for objects of interest.\n3D bounding boxes are then predicted based on the features of these proposals. The implementation of point-based\ndetectors relies on the early work on point-based point cloud classifiers. PointNet [38] pioneers this class of methods,\nwhich consists of two basic structures: a multilayer perceptron (MLP) network for feature learning and max-pooling\nas a symmetric function. The former can transform features to specified dimensions and learn global features of the\npoint cloud, while the latter can overcome the disorder of point clouds, and its symmetry ensures that the model has\nthe same output regardless of the input order. Currently, the main differences among point-based detectors lie in\nthe downsampling method and the structure of the local feature extractor. To ensure global coverage of the sampled"}, {"title": "2.3 Point-voxel based 3D object detectors", "content": "Point-voxel based approaches combine points and voxels in the 3D object detection process to overcome the limitations\nof point-based and voxel-based methods. Liu et al. [42] and Tanget al. [43] attempt to bridge the features of points\nand voxels by using point-to-voxel and voxel-to-point transformations in the backbone networks. Points provide\ndetailed geometric information, while voxels are computationally efficient. PV-RCNN [44], a two-stage 3D detector,\nincorporates the first-stage detector from Second [45] and introduces the RoI-grid pooling operator for second-stage\nrefinement. Qian et al. [46] propose a lightweight region aggregation refine network (BANet) that constructs a local\nneighborhood graph to improve box boundary prediction accuracy. Additionally, Shi et al. [44] propose PV-RCNN++,\nwhich utilizes VectorPool aggregation for better aggregating local point features with reduced resource consumption.\nCompared to pure voxel-based detection approaches, point-voxel based methods integrate features in the 3D backbone,\nallowing them to benefit from both fine-grained 3D shape and structure information obtained from points, resulting in\nimproved detection accuracy, especially for two-stage detectors. However, this comes at the cost of increased inference\ntime.\nTo ensure the inference speed of the model, our 3D backbone network retains a point-based design instead of using\npoint-voxel-based approaches that integrate two types of features to gain benefits in the feature learning process. Instead,\nin the neck structure, we rely on the proposed point dilation mechanism to lift points to a new dimension and combine\nthe detection head to jointly learn features of non-occupied space, thus ensuring the continuity of the model's receptive\nfield. In summary, PDM-SSD provides a new approach to balance the inference speed and detection accuracy of the\nmodel."}, {"title": "3 Method", "content": null}, {"title": "3.1 Overview", "content": "As aforementioned, we aim to combine the advantages of grid-based and point-based detectors by addressing the issue\nof discontinuous receptive fields in current point-based models. To achieve this, we propose PDM-SSD, a novel and\ngeneric single-stage point-based 3D detector. The overall workflow of PDM-SSD is illustrated in Fig. 2, where the input\nLiDAR point clouds are first passed through the embedding network to expand the feature space of the points. Then, a\nPointNet-style 3D backbone network is employed to extract point-wise features. The 3D backbone network consists\nof several stages of downsampling modules, local feature aggregation modules, and multi-scale feature aggregation\nmodules, ensuring that the sampled points learn rich geometric and semantic features. The neck network includes our\nproposed point dilation mechanism, where points are lifted to the grid level and feature filling is performed for the\nunoccupied space in the original point cloud using a special mechanism. The grid-wise features are used to regress the\nheatmap of the scene, providing global features of the objects through joint learning with the hybrid detection head.\nIn the following, we discuss in detail the implementation process of PDM-SSD. In Section 3.2, we examine the changes\nin the receptive fields of the grid-based and point-based models during the formulation learning process. In Sections 3.3,"}, {"title": "3.2 Problem formulation", "content": "Let $P = \\{p_n\\}_{n=1}^N$ be a set of $N$ observed LiDAR points belonging to a scaene, where $p_n \\in R^3$ is a 3D point represented\nwith spatial coordinates. Let $C$ be the centers location $C = \\{c_m\\}_{m=1}^M$ of the annotated ground-truth $M$ bounding\nboxes, $c_m = [C_{mx}, C_{my}, C_{mz},]\\in R^3$. Due to the limitations of the resolution of the LiDAR, climate conditions, and\nocclusions, sparse and incomplete targets often appear in the scene, as shown in Fig. 3. Assuming the point cloud $P$ is\nvoxelized into initial pillars $V = \\{v_k\\}_{k=1}^K$, where $K$ represents the number of pillars. Taking 2D sparse convolution as\nan example, after learning in the Grid-based detector with the convolutional layers stacked in the backbone network, the\nfeature map can be simplified as:\n$F^{i+1} = Pool(Conv(Padding(F^i)))$\n$i$ is the number of convolutional layers of the backbone. The padding layers in the network allocate the unoccupied\nspace in the original point cloud, and the convolution layers and pooling layers fill these spaces with features. The\nreceptive field of the current layer is:\n$RF^{i+1} = RF^i + (k \u2212 1) \\times S_i$\n$S_i = \\prod_{i=1} Stride_i$\nAmong them, $RF^{i+1}$ represents the receptive field of the current layer, $RF^i$ represents the receptive field of the\nprevious layer, and $k$ represents the size of the convolution kernel. It can be seen that the receptive field of the model\ncontinuously increases, even learning features for the unoccupied space in the original point cloud. The feature maps of\nthe Point-based backbone network can be represented as follows:\n$FP^{i+1} = Pool(Group(Sampling(F^i)))$\n$i$ represents the stage of feature learning, $Sampling$ is the downsampling operation, $Group$ is the local feature\naggregation operation, $Pool$ is the pooling layer, which is treated as a symmetric function to address the unordered\nnature of point clouds. Here, $F^0 = E_{\\theta}(P)$, where $E_{\\theta}(. . .)$ is the feature augmentation network, usually a multi-layer\nperceptron or graph neural network, which expands the feature space of point clouds. As the number of learning stages\nincreases, the query radius of the local feature extractor becomes larger to increase the receptive field of the model.\nHowever, the model only takes the original point cloud as input, and in the case where the target has points only in the\nlocal region, even if the query radius increases, the receptive field of the model remains the same, and the learning of\nlocal features still stays at the previous stage. Fig. 3 illustrates this phenomenon intuitively. Under this problem, the\nmodel's ability to predict the semantics and geometry information of upsampled points on the target will decrease. The\nmain purpose of PDM-SSD is to alleviate this problem."}, {"title": "3.3 PointNet style 3D backbone", "content": "The currently popular point-based detectors, such as 3DSSD, IA-SSD, and DBQ-SSD, all use PointNet-style 3D\nbackbones as the point semantic and geometric information extractors, and their advantages in detection accuracy\nand inference speed have been well demonstrated. In order to ensure lightweightness, our PDM-SSD also adopts this\nstructure of 3D backbone. To highlight the performance gains after solving the problem of discontinuous receptive\nfields, we do not make too many changes to the backbone, following the design of IA-SSD in general. The specific\nstructure is shown in Fig. 2.\nThe point cloud is first passed through a vanilla feature augmentation network stacked with MLP to increase the\nfeature dimension. Then, it enters a feature extractor consisting of four repeated SA (Set Abstraction) modules to learn\ngeometric and semantic information, as shown in Eq. 4. The point-wise features outputted at each stage are denoted\nas $\\{F_1^P, F_2^P, F_3^P, F_4^P\\}$. Specifically, before each stage, we downsample the point cloud to reduce the model's spatial\ncomplexity and feature redundancy. Then, we use PointNet for local feature extraction, which can be divided into three\nsteps: 1) Point indexing: using the current stage sampled points as centers, perform vanilla ball query within the range\nof the sampled points in the previous stage to index the k nearest points to each center within a certain radius r. 2)\nFeature learning: use MLP to learn the features of the points within the ball, further improving the learning depth. 3)\nPooling: perform max-pooling operation on the features of the points within the ball in the feature dimension. Due\nto the unordered nature of point clouds, this operation ensures that even if the order of the point cloud is changed,\nthe pooled features remain unchanged. In each stage, we simultaneously use two sets of combinations of radius and\nsampling number for multi-scale feature extraction, and aggregate the multi-scale features after the pooling layer. In\nsummary, the point quantities of $F_1^P, F_2^P, F_3^P, F_4^P$ decrease while the feature dimensions gradually increase.\nIt should be noted that in the model, $F_2^P, F_3^P$ adopt Farthest Point Sampling (FPS) downsampling method, while $F_3^P, F_4^P$\nadopt foreground point downsampling method with semantic embedding, following the approach of IA-SSD. The\nformer ensures the global coverage of sampling points in the presence of a large number of redundant points, reducing\nglobal information loss. The latter ensures a high recall rate of foreground points, reducing target information loss. The\nspecific implementation is shown in Fig. 2. We add a network branch S(\u00b7) to the SA module in the second and third\nstages to extract the semantic information of sampling points. Then, the sampling points in the next stage are selected\nfrom the points with the highest probability of being foreground points, as shown in the following equation:\n$sample\\_index_{i+1} = topK(S(F_i)) i = 2,3$\nFrom this, we obtain a small number of foreground sampling points with their rich local geometric and semantic\ninformation in point-wise features."}, {"title": "3.4 Neck with Point Dilation Mechanism", "content": "The point-wise receptive field is still limited to the space occupied by the original point cloud until 3.3. In order to\nexpand the learning scope of the model, we propose the Point Dilation Mechanism, which consists of two steps: point\ndilation and feature filling.\nPoint Dilation (PD). Dilation (usually represented by $\\oplus$) is one of the basic operations in mathematical morphology.\nImage dilation is a commonly used morphological processing algorithm in the field of image processing, which utilizes\na structuring element to probe and expand the shapes present in the input image, aiming to connect connected regions"}, {"title": "3.5 Hybrid Head", "content": "We propose a hybrid detection head to simultaneously extract point-wise features and grid features, as shown in Fig. 7.\nFor the point-wise features $F_4^P$, inspired by VoteNet, they are first input into a voting network to move the sampled\npoints towards the center of the objects as much as possible. Then, the context features of the objects are extracted using\nthe moved positions as centers. Finally, these features are used for semantic classification and regression of geometric\nparameters. This part follows the basic structure of a traditional point-based detection head. However, as described in\nSection 3.2, due to the limited receptive field, the information of objects in the scene is not fully explored, especially for\nsparse and locally scattered objects, where their voting centers may be inaccurate or the context features may have a\nprobability of being classified as objects below the certain threshold.\nThe grid feature, with the support of angle and scale coefficients, not only expands the receptive field of point-wise\nfeatures but also enhances the connectivity of features from multiple inflated centers, potentially enabling the extraction\nof holistic features from sparse and incomplete targets. To achieve this goal, we construct a sparse heatmap using\nannotation information to supervise the learning of grid features, following the design of the dense CenterHead in\nthe training process. Additionally, we find that even treating this part of the network as an auxiliary learning task can\nsignificantly improve the performance of the point-based detection head. Therefore, we design the model from the\nperspectives of both auxiliary learning and joint learning.\nAuxiliary Learning (AL). The $F_1$ used for heatmap regression is essentially obtained by dilating $F^P_4$ used for prediction.\nWe believe that supervising $F_1$ with sparse heatmaps will also increase the receptive field of $F_4^P$, thereby obtaining\nmore accurate detection results. In addition, another advantage of auxiliary learning is that the auxiliary network does\nnot participate in prediction and inference stages of the model, thus preserving the inference speed of the point-based\ndetector completely. We will demonstrate its superiority in the experimental section.\nJoint Learning (JL). In joint learning, we will fully utilize grid features and learned scene heatmaps. As shown in\nFig. 7, in addition to the votes generated by the vote network from point-wise features, we supplement the centers\nof the top K maximum values in the predicted heatmap. These two sets of points are aggregated together to learn\ncontextual features through the point-based head. Then, we contact the contextual point-wise features with the grid\nfeatures of the cells where these points are located in the depth dimension. Finally, after a simple channel attention,\nthe features are used for semantic classification and detection box parameter regression. The above operations can not\nonly supplement the spatial position information of the scene targets provided by the heatmap, but also adaptively fuse\npoint-wise features and grid features, thereby compensating for the inaccuracies caused by the limited receptive field of\nthe point-based detector and the low semantic probabilities of the targets. We will explain this in detail in 4.5."}, {"title": "3.6 End-to-End Learning", "content": "The proposed PDM-SSD can be trained in an end-to-end fashion. In the 3D backbone, the sampling loss is calculated.\n$L_{sample} = \\sum_{c=1}^C (Mask_i CELoss_i)$\n$CELoss_i = s_i log(s_i) + (1 \u2212 s_i) log (1 - s_i)$\n$Mask_i = \\frac{min f^*, b^*}{max f^*, b^*} \\cdot \\frac{min l^*, r^*}{max l^*, r^*} \\cdot \\frac{min u^*, d^*}{max u^*, d^*}$\n$CELoss$ is the cross-entropy loss for predicting the category of sampled points, which first appeared in IA-SSD. It\ngreatly improves the recall rate of foreground points in sampled points. $Mask$ is the weight for the centrality of sampled\npoints. $f^*,b^*,1^*, r^*, u^*, d^*$ represent the distances between the sampled points and the six faces of the target box. We\nborrowed the design from 3DSSD, which believes that points closer to the center are more conducive to accurately\nregressing semantic categories and geometric parameters. By multiplying the centrality with the cross-entropy loss,\nthe closer the sampled points are to the center, the greater the loss will be. The model will prioritize improving the\nforeground probability $s_i$ of these points, so that during inference, the model will prioritize selecting these points.\nAlthough SPSNet has shown that points closer to the center are not necessarily better, it is still better than methods that\nsample foreground points equally. Besides, SPSNet requires extra time to learn the stability of points.\nThe loss in the detection head is divided into two parts: the point-based loss $L_p$ and the grid-based heatmap prediction\nloss $L_{heatmap}$. The former consists of three components: the loss of the vote points $L_{vote}$, the loss of point semantics\nprediction $L_{cls}$, and the loss of target box geometric parameter regression $L_{reg}$. Additionally, we also include a\nregularization loss in the training process of PDM-SSD.\n$L_{all} = L_{sample} + L_p + L_{heatmap} + L_2$\n$L_p = L_{vote} + L_{cls} + L_{reg}$\nIn particular, the box generation loss can be further decomposed into location, size, angle-bin, angle-res, and corner\nparts:\n$L_{reg} = L_{loc} + L_{size} + L_{angle-bin} + L_{angle-res} + L_{corner}$\nAll these losses will be jointly optimized using a multi-task learning approach."}, {"title": "4 Experiments", "content": "In this section, we will provide detailed experimental results to demonstrate the efficiency and accuracy of PDM-SSD.\nSpecifically, we introduced the specific settings and implementation details of the experiments in Section 4.1. Then, the\ncomparison results between PDM-SSD and current state-of-the-art methods were reported in Section 4.2. Following\nthat, in Section 4.3, a ablation study was conducted to demonstrate the rationality of the model design. Furthermore, the\ninference efficiency of PDM-SSD was analyzed in Section 4.4. Finally, in Section 4.5, we provided a large number of\ninstances to demonstrate the superiority of PDM in sparse and incomplete object detection."}, {"title": "4.1 Setup", "content": "Benchmark datasets. The KITTI dataset is a dataset sponsored by the Karlsruhe Institute of Technology and the\nToyota Technological Institute at Chicago for research in the field of autonomous driving. The widely-used dataset\ncontains 7481 training samples with annotations in the camera field of vision and 7518 testing samples. Following the\ncommon protocol, we further divide the training samples into a training set (3,712 samples) and a validation set (3,769\nsamples). Additionally, the samples are divided into three difficulty levels: simple, moderate, and hard based on the\nocclusion level, visibility, and bounding box size. The moderate average precision is the official ranking metric for both\n3D and BEV detection on the KITTI website.\nEvaluation metrics. To provide a comprehensive performance evaluation, we evaluated our PDM-SSD on both the\nKITTI 3D and BEV object detection benchmarks. Generally, average precision (AP) based on Intersection over Union\n(IoU) is commonly used for both the 3D and BEV tasks. The experiments primarily focused on the commonly-used Car\ncategory and were evaluated using the average precision metric with an IoU threshold of 0.7. To ensure an objective\ncomparison, we utilized both the AP with 40 recall points (AP40) and the AP with 11 recall points (AP11). The\n3D-NMS threshold for metric calculation was set at 0.1, and the object score threshold was set at 0.1."}, {"title": "4.2 Comparison with State-of-the-Arts", "content": "Note. Our model is trained in a single-stage multi-class manner. Thanks to the contributions of mmlab [62], the\nPDM-SSD model was trained using their open-source OpenPCDet\u00b2 architecture. Additionally, to show respect and\nfair competition to the authors of the baseline models, we retested their models with our environment on the val set,\nmarked as X*. In the following sections, unless otherwise specified, PDM-SSD refers to the results of joint training,\nPDM-SSD(J).\nEvaluation on KITTI Dataset. Tables 1 and 2 present the detection performance of PDM-SSD and some state-of-\nthe-art models on Car objects in the KITTI test and val benchmarks. We report their metrics in both 3D and BEV\nperspectives. In the KITTI benchmark, Car objects are divided into three subsets (\"Easy,\" \"Moderate,\" and \"Hard\")\nbased on difficulty levels. The results on the \"Moderate\" subset are commonly used as the primary indicator for final\nranking. To provide a more intuitive comparison of PDM-SSD's superiority, we categorize the comparative models into\nthree types: point-based, voxel-based, and point-voxel-based, with PDM-SSD belonging to the first category. Table\n3 shows the detection results of PDM-SSD on Cyclist objects. Table 4 displays the detection metrics of PDM-SSD,\nIA-SSD, and SPSNet at an IoU threshold of 0.5. Although this threshold is not commonly used for comparing model\ndetection performance, it can reflect the differences in the models' object recall rates."}, {"title": "4.3 Ablation Study", "content": "Next, we will conduct ablation studies to evaluate the performance of our proposed modules in PDM-SSD on the KITTI\nvalidation split.\nSH degree. In the illumination representation model, the higher the degree, the higher the fidelity to real scenes.\nHowever, higher degrees also require more coefficients to describe, leading to increased computational complexity. In\npractical applications, a suitable order is typically chosen to balance accuracy and computational efficiency based on\nthe requirements. Table 5 shows the performance of PDM-SSD with orders 2, 3, and 4. It can be observed that the\nmodel performance is comparable between orders 3 and 4, and superior to order 2. Therefore, in order to reduce the"}, {"title": "4.4 Runtime Analysis", "content": "One of the advantages of PDM-SSD is that it uses a point-based 3D backbone, which allows the model to maintain\nfast inference speed. We analyze the parameter count and GFLOPs of each part of PDM-SSD. PDM-SSD has a very\nsmall parameter count, only 3.3M. The main difference between PDM-SSD and many other point-based detectors is\nthe addition of the Neck module, which only contains 0.53M parameters and accounts for only 13% of the overall\nGFLOPs, so it has a minimal impact on the model's inference speed. Table. 7 analyzes the inference speed of IA-SSD,\nPDM-SSD(A), and PDM-SSD. Our experiments were conducted on a single NVIDIA RTX 3090 with Intel i7-12700KF\nCPU@3.6GHz. The results show that the inference speed of PDM-SSD is 68FPS, slightly lower than the 84FPS of\nIA-SSD, but it fully meets the hardware limitations of current LiDAR devices and practical application requirements"}, {"title": "4.5 PDM Analysis", "content": "In the previous sections, we have quantitatively analyzed the advantages of PDM-SSD in terms of overall performance\n(4.2) and runtime (4.4). In this section, we will analyze the role of PDM at the object level, particularly in detecting\nsparse and incomplete targets. As mentioned earlier, current point-based detectors suffer from two important issues due\nto discontinuous receptive fields. I: The error in the voting point position regressed from the sampling points increases\nthe difficulty of predicting the target box. II: The detection box regressed from the features learned from context has a\nprobability lower than the threshold. We will analyze the contributions made by PDM-SSD to address these two issues\nseparately.\nIssue I. The features predicted by querying the vote points are crucial for determining the class and geometric parameters\nof the target bounding box. The position of the vote points has a significant impact on the prediction results. For sparse\nand particularly incomplete targets, the model's receptive field is limited when learning from only occupied points. It is\ndifficult for the model to actively learn the overall features of the target and establish connections between points. This\ncan easily lead to the deviation of vote points from the target center, resulting in a low IoU of the predicted bounding"}, {"title": "5 Dicussion", "content": "PDM-SSD provides a new approach to address the issue of discontinuous receptive fields in point-based detectors. It\neliminates the complex process of integrating point and grid representations in the backbone and instead utilizes PDM\nto directly lift and fill features for sampled points within the neck module. This not only maintains a lightweight model\nbut also exploits the advantages of both representations. We have extensively demonstrated the superiority of PDM-SSD\nthrough numerous experiments, as described in Section 4. In terms of detection accuracy, PDM-SSD surpasses the\nstate-of-the-art point-based detectors and can compete with some voxel-based models. In terms of inference speed,\nPDM-SSD can run efficiently at 68 FPS, which is much higher than the scanning frequency of current LiDARs, making\nit suitable for practical applications. The auxiliary trained PDM-SSD (A) achieves even higher detection accuracy\nwithout sacrificing inference efficiency. Additionally, the model parameters of PDM-SSD are only 3.3MB, greatly\nreducing the deployment difficulty. In the object-level experimental analysis, we found that PDM-SSD effectively\nmitigates the issues of large vote point errors and low object box prediction probabilities caused by limited receptive\nfields in current point-based detectors. Overall, PDM-SSD has indeed identified new problems and made further\nadvancements in the current research, demonstrating its value.\nLimitations and outlook. Objectively speaking, PDM-SSD still has the following issues: 1) We avoided the detection\nof pedestrians in the KITTI dataset because the small volume and limited impact of pedestrians make the 5 \u00d7 5 structural\nelement unsuitable for detecting such targets. In future work, we will select different sizes of B for different classes of\nobjects. 2) In the scale coefficient of feature padding, we set two variables as independently and identically distributed,\nfollowing the setting of heatmap in CenterNet [20]. However, we believe this is not optimal, and in future work, we will\nsplit the Gaussian into a mixture of scale and rotation to learn the covariance and embed more geometric information\ninto the learning process. 3) The final context learning module we used is still PointNet, which may not fully utilize the\nmixed features provided by the mixed head. In future work, we will design more sophisticated learning modules."}, {"title": "6 Conclusion", "content": "In this article, we propose single stage point-based 3D object detector called PDM-SSD. Our goal is to alleviate the\nlimited receptive field issue while maintaining the fast inference speed of point-based detectors. Currently, point-based\ndetectors can only learn features from existing points, and their receptive field is discontinuous and limited when the\nquery radius expands, especially for sparse or extremely incomplete objects. This not only leads to large position errors\nin regression vote points but also results in prediction probabilities for object boxes lower than the threshold. PDM-SSD\nexpands the learning space of the model through Point Dilation, specifically covering the unoccupied space near the\ncenter of the object in the original point cloud. Then, it fills these spaces with features that can be backpropagated, and\nthe information from multiple dilation centers is connected through height compression. Finally, these features are\njointly learned through a hybrid head. The experimental results show that PDM-SSD achieves competitive performance\nin terms of detection accuracy, surpassing all current point-based models in multiple metrics. In terms of inference\nspeed, it fully meets the current application requirements. More importantly, from the object-level experiments, we can\nintuitively see the contributions of PDM-SSD in addressing the issues of current point-based detectors."}]}