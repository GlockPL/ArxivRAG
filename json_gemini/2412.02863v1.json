{"title": "Proximal Control of UAVs with Federated Learning for Human-Robot Collaborative Domains", "authors": ["Lucas Nogueira Nobrega", "Ewerton de Oliveira", "Martin Saska", "Tiago Nascimento"], "abstract": "The human-robot interaction (HRI) is a growing area of research. In HRI, complex command (action) classification is still an open problem that usually prevents the real applicability of such a technique. The literature presents some works that use neural networks to detect these actions. However, occlusion is still a major issue in HRI, especially when using uncrewed aerial vehicles (UAVs), since, during the robot's movement, the human operator is often out of the robot's field of view. Furthermore, in multi-robot scenarios, distributed training is also an open problem. In this sense, this work proposes an action recognition and control approach based on Long Short-Term Memory (LSTM) Deep Neural Networks with two layers in association with three densely connected layers and Federated Learning (FL) embedded in multiple drones. The FL enabled our approach to be trained in a distributed fashion, i.e., access to data without the need for cloud or other repositories, which facilitates the multi-robot system's learning. Furthermore, our multi-robot approach results also prevented occlusion situations, with experiments with real robots achieving an accuracy greater than 96%.", "sections": [{"title": "I. INTRODUCTION", "content": "ROBOTS have been proven useful in several dimensions of human life. They have, for example, fostered telepresence in critical environments by allowing humans to benefit from their mobility and bidirectional audio and video feeds [1]. The interaction between humans and robots often occurs through devices such as controllers and screens. Alternatively, other forms of control like wearable sensors (e.g., accelerometers), as well as in-palm and optical devices, have gained popularity [2]. Fostering new ways of human-robot interface, embedded cameras have been used as a mainstream form of robot control via human gesture recognition. For Unmanned Aerial Vehicles (UAVs), also known as drones, this form of control is anchored on the fact that drones are increasingly present in our daily lives. For example, drones can be used to monitor and assist workers in their activities without requiring the use of physical controlling devices. This proximal form of control enables a seamless interaction experience where the human is free to perform other tasks requiring both hands while the device becomes more likely to be perceived as a smart autonomous companion with an appropriate level of rationality [3]. An example of a drone-assisted human task is described by Uzakov et al. [4] where the authors have used three drones to follow a power line worker using computer vision to detect clothing of the human. In addition, Chaudhary et al. [5] presented an action recognition approach to control a multi-robot system through vision. However, training multiple robots in a distributed fashion is still an open challenge. To overcome this problem, Federated Learning (FL) has been proposed in the literature [6]. Thus, in this work, we focus on experiments in the proximal control of UAVs in human-robot collaboration environments. We address the direct control of the drone via action recognition, taking into account the limitations of the platform w.r.t. computing, memory, and power management. For such a goal, we resort to FL [7], a distributed learning paradigm that enables multiple devices to collaboratively train the Machine Learning-based action recognition model without sending the raw data out, improving experienced latency and relieving bandwidth and energy burden [8]."}, {"title": "II. RELATED WORK", "content": "As we mentioned above, we propose a Federated Learning and LSTM-based approach for proximal control of UAVs for human-robot interaction (HRI). To the best of our knowledge, this is the first work that uses Federated Learning in HRI for proximal control of UAVs. Therefore, in the literature, it is only possible to find papers with techniques that are only part of our contribution. For example, some works use Convolutional Neural Networks (CNN) and LSTM only for static and dynamic gesture recognition. Works such as the one from Celebi et al. [9], Rwigema et al. [10], and Hakim et al. [11] focus on using CNN and LSTM to recognize more than 20 gestures (static and dynamic) for general purposes such as controlling a TV. Furthermore, specifically for action recognition of general purposes, Sozinov et al. [12] proposed a federated learning approach to training a human activity recognition classifier to tackle the increase in communication costs and possible privacy infringement. In addition, these techniques can also be used to control robots, which is the main application of our work. The literature presents some works that use CNN and LSTM to recognize such gestures and actions in order to control a single UAV [13]-[15]. However, when controlling robots, some problems are still open. For example, the type of the gesture (static or dynamic), the size and variance of the dataset, the robustness of the control approach, latency, and the multi-robot system cases are still open. Thus, the first problem we aimed to tackle is the dataset for UAV control. The literature presents similar works, such as the one proposed by Kassab et al. [14] that created dataset and learning models for static gesture recognition to control one UAV. However, it is a small dataset and is limited to only hand and face recognition and only one UAV. In addition, Liu and Szir\u00e1nyi [15] used a generic COCO dataset [16] that has more than 80 different objects to detect humans. Although this work now uses the full body for gesture and action recognition, the dataset is generic. In contrast, Perera et al. [17], [18] proposed a dataset of two static gestures and eleven actions. These gestures/actions are based on the general aircraft handling signals and helicopter handling signals to command a UAV. The dataset is available with body joints and gesture classes to be reusable, calculated by Openpose [19]. However, in our tests, we also found that the size and variance of this last dataset are not sufficient for proximal control of UAVs. A second problem we aimed to tackle is the human-swarm interaction, i.e., the use of human-robot interaction techniques to control a multi-robot system. The use of machine learning techniques to recognize features together with a multi-UAV system for general purposes is not new [20], [21]. However, only recently that some preliminary works have used such approaches (e.g., k-Nearest Neighbor) to recognize actions and control a multi-UAV system [5]. One of the problems that exist in a multi-UAV system controlled by HRI approaches is within the training process. Thus, with Federated Learning, we aim to be able to train our LSTM-based approach in a distributed fashion."}, {"title": "III. PROPOSED PIPELINE", "content": "Thus, this work proposes a solution for the human-swarm interaction problem, using computer vision in a machine-learning architecture with FL. For that solution, we created experiments to compare the neural network performance between two different datasets and evaluate if the FL reduced the time spent by the swarm to detect an action. By using the proposed architecture, it is expected that personal data will be protected and also, with the sharing of learning between the clients, will be able to converge more rapidly [22]. Furthermore, this work also proposes improvements in the relationship between humans and drones, in which by implementing natural human interactions (NHI) as actions recognized through embedded machine learning, the robot can react to help the related human being. Finally, we reaffirm that, to the best of our knowledge, this is the first work that uses FL in HRI for proximal control of UAVs. Thus, we aimed for this to be a preliminary but significant work. In this work, we utilized two different UAVs: a DJI and a x500. The x500 UAV uses a standard control system, the MRS System [23] (i.e., a system for UAV flight control and state estimation) as a navigation system. In contrast, the DJI uses only its standard flight controller. The DJI UAV also uses a Real-Time Streaming Protocol (RTSP) server to capture and send the video feed to an offboard computer, which in turn converts it into ROS images. In contrast, the x500 captures the images and converts them into ROS images within its onboard computer. Both the onboard computer of the x500 and the offboard computer use our action recognition algorithm in a distributed fashion (see Fig. 2). In this work, for the sake of simplicity, we assume the DJI is only hovering in order to keep the operator always within the field of view of at least one of the UAVs. Within our approach, our action recognition has a priority check. This means that if the main drone (i.e., x500 UAV) detects the person, the commands from the secondary pipeline (DJI UAV) are ignored. If not, process the key points to generate the command just like the main pipeline."}, {"title": "A. Gesture Detection", "content": "The Operator Detection block within the pipeline (Fig. 2) uses a series of filters (the first nine blocks from Fig. 3) to process the acquired video feed. This is done to isolate the drone operator against persons that can appear on the scene. Then, the filtered images are inserted into the Mediapipe\u00b9block, which is an open-source framework for building and deploying machine-learning pipelines, and which has a human joint detector. The output of Mediapipe is used to calculate the 33 points within the X, Y, and Z coordinates in the world frame. The frame key points are then sent to the action recognition algorithm."}, {"title": "B. Action Recognition", "content": "Our action recognition approach works on a streaming basis reconstructed from each processed frame that comes from the Gesture Detection block. Thus, every time the Action Recognition block receives the position key points from the Operator Detector block, the following operations are performed:\n1) Creates an array with the first 60 frames received;\n2) Performs the action recognition using the trained LSTM model using FL;\n3) Insert the recognized action into a Moving Average Filter;\n4) Calculate the average of the first three filtered actions;\n5) Sends the action's respective command every 10 seconds. The actions in our dataset are characterized by the annotated reference points. This information is used to define the architecture of the LSTM we use to classify the actions. The input for the model is defined as N \u00d7 M, where N denotes the number of frames and M is the multiplication of the number of key points by the number of coordinates. In our experiments, the input for the first LSTM layer has the dimensions of (60,99) for the used dataset. Therefore, our architecture resulted in a model with six layers, in which the first two are LSTM layers, the third is a dense layer using the ReLU activation function, a third layer is a dropout layer, the fourth layer is another dense layer using the ReLU, and finally a Softmax activation function was added in the last layer. This last layer was responsible for classifying which class the input data belonged to. Finally, the network architecture can be summarized below in Table I. The third step of our approach is the application of a moving average filter, which is necessary in order to avoid false positive situations that usually happen during transitions between actions, i.e., when the human operator is finishing one action and starting another action. Such situations may happen when the human operator transitions between action a\u2081 and a\u2082 or during the intersection between two complete actions, initiating within the last 30 frames of the first execution and ending at the first 30 frames of the second execution. On the other hand, we also implemented a digital debounce to prevent an overload of commands while the operator sends only one command. In resume, our action recognition approach applies a window function considering the average of three actions to avoid detection errors. The entire response time of the Action Recognition approach takes 10.4 seconds, as depicted in Fig. 4. Then, every recognized action is transformed into a respective command to be sent to the control system. We also apply a ten-second wait window between commands to avoid sending the same command multiple times. Finally, we convert the desired command into control commands that are sent to the x500 UAV MRS system, allowing the UAV to perform the desired actions."}, {"title": "IV. DATASET", "content": "1) Dataset from Literature: Since we focus on the proximal control of drones via action recognition, we opt for a dataset of UAV commanding signals. In this work, we use a dataset created by Perera et al. [17], which provides 11 actions and 2 static gestures (i.e., do not contain any limb motion) suitable for basic UAV navigation and command from general aircraft and helicopter handling signals across 37.151 high-definition video frames (see Fig. 5), all of which are annotated with body joints and action classes. This dataset contains rich variations in the recorded actions in terms of the phase, orientation, camera movement, and body shape of the actors. These properties are a suitable representation of real-world variations and make the dataset extensible to be used in human-robot collaboration environments. Finally, the dataset uses Openpose [19] and has 18 reference points to identify a person. Each point has three coordinates, referring to the X, Y, and Z axes. For more details on the dataset, please refer to Perera et al. [17]. 2) Created Dataset: In addition, we created a second dataset aiming to improve the accuracy of our proposed method\u00b2. Like the UAV Gesture Dataset from Perera et al. [17], our dataset also has 11 actions and 2 static gestures (i.e., do not contain any limb motion) as commands sent to an operator, a pilot. However, we also have three different points of view of the same action/gesture. Thus, we multiplied the number of gestures/actions, resulting in 33 unique actions and 6 unique static gestures, with a total of 21.294 video frames. Furthermore, this second dataset also uses Mediapipe, with X, Y, and Z coordinates to characterize a subject. A portion of the video (60 frames) and six videos for each action were created to identify the subject. In addition, we also performed a data augmentation operation on key points, a rotation from -15 to 15 degrees in the X, Y, and Z axis. Thus, for the sake of simplicity, let us name it as an Action, both static gestures and moving gestures. Finally, we can formulate mathematically the dataset creation and operations as follows:\nData:\n\u2022 G is the set of actions;\n\u2022 V is the set of videos for each action;\n\u2022 x is the frame's index inside a video.\n\u2022 A = {i, j, k} is the set of vectors.\n$FR_{i,j,x} = R_{v,\\alpha} * F_{i,j,x}$ (1)\nWhere:\n$F_{ijx}, \\forall i \\in G, \\forall j \\in V, \\forall x \\in j$ (2)\n$B_{v\\alpha}, \\forall v \\in A; \\alpha\\forall{a \\in Z : -15\u00b0 < \\alpha < 15\u00b0}$ (3)\nwhere the FR in (1) is the resulting frame from the operation between the rotation matrix from a unit vector and an angle in degrees (equation (3)), times the individual frame in each video from each action (equation (2))."}, {"title": "V. FEDERATED LEARNING-BASED ACTION RECOGNITION IN UAVS", "content": "In this section, to help mitigate the aforementioned issues related to the control of UAVs in human-robot collaborative scenarios, especially with respect to the training process, we detail our FL-based (FL-based) proposal. Let us begin considering several UAVs (u\u017c) in a multi-robot system (U), where u\u017c \u2208 U, i \u2208 {1... N} and N is the total number of robots. Let us also consider the existence of a global dataset (D) that describes multiple instances of the actions a that we target, where a \u2208 D. Thus, in this work, we let each UAV U\u017c own a possibly overlapping partition of the global dataset D. In other words, for each UAV ui \u2208 U we associate a partition di such that U\u2081di = D. Naturally, we admit the possibility that di \u2229 dj \u2260 0 for any arbitrary pair of UAVs, u\u017c and uj. Following the FL protocol, we use a central server to organize the model inference on D, using the N available datasets. The process starts with the server, which sends a copy of the initial model parameter values, winit, for each robot ui. Then, each robot updates its local model, di, using its own dataset di. Following that, each updated local model parameter wi is sent back to the server where they are aggregated often via an averaging operation \u2013 into a new winit. The process continues until convergence criteria are reached. Note that in this setup, no actual di is transmitted to the central server, but only the model parameters wi. Recall that the motivation for considering an isolated partition di is related to the constraint imposed by the operation of each ui. These devices can operate in isolation, avoiding the exposure to personal data via each di, while sharing their unique experiences in the field via wi. Additionally, each device can update its model on the fly, which is beneficial, especially in case the available computing power does not meet the demand imposed by the time complexities involved. To capture the patterns involved in the categorization of each action a, we pose the training of an LSTM model via the FL framework. Such a process returns a converged LSTM model over the dataset D represented by each UAV partition di. The training cycles start with each UAV Ui training their model on their data di after receiving the initial parameters from the server. This training process happens through scheduled rounds of parameter learning, as shown in Fig. 6. Naturally, this implies that communication between the devices is established. Although outside the scope of our work, this can also be extended to defined cross-swarm training cycles to allow maximum adaptation of individual swarms to patterns that are cross-regional w.r.t their operation. Algorithm 1 details the standard federate average process for LSTM within the Server block of Fig. 6. In this work, we used only two UAVs. Thus, we can see in Fig. 7 that our system only needs three interactions of the FL process for the training since no significant improvement would have been achieved if more federated iterations had been executed."}, {"title": "VI. RESULTS", "content": "Our experiments and simulations relied on Flower [24], a wildly popular framework for FL and from which we used the federated average parameter aggregation strategy described in Algorithm 1. The computation was performed by a portable computer with an i7 processor, equipped with an Nvidia graphics card, running Ubuntu LTS and ROS Noetic. A. Realistic Simulations\nThe performed simulations aimed to evaluate the accuracy and time elapsed or our proposed architecture and model when using each of the abovementioned datasets. The best training parameters were found empirically by using MinMaxScalar from Scikit-Learn [25] with 750 epochs and an Early Stopping of 300 with five iterations of federated average on the server. The simulations used dataset 1 [17] to validate the model after the training. With dataset 1, we analyzed the case where we have only two visual observation points (i.e., client A and client B), where each client had the proposed architecture implemented, using the same dataset for both clients. In these simulations, each client was associated with a different camera and also a different field of view. The actions from the dataset were semantically connected to UAV commands. Fig. 8 and Fig. 9 present examples of the simulations carried out, with the acquisition of images from two cameras in two different positions detecting the operator's actions and sending commands to two simulated UAVs. Fig. 10 shows the realistic simulation of the UAVs. The performance of the neural network was measured in terms of accuracy. Although the accuracy during the training procedure was satisfactory, the validation results were different from the expected. The evaluation values for loss and accuracy can be seen in Table II. With an accuracy of 99% (client A), the simulated UAV was able to detect the person and send the correct command, as is shown in Fig. 8. Although the accuracy of the second UAV was 75% (client B), the simulated UAV was also able to detect the person and send the correct command, as is shown in Fig. 9. Then, these detected commands triggered the UAV-1 drone corresponding to client A towards its final position (with coordinates (-1.1;1.1)) and the UAV-2 drone corresponding to client B towards its final position (with coordinates (-0.95;0.90)). The final path towards the desired destination point can be seen in Fig. 11."}, {"title": "B. Real-robot Experiments", "content": "For real robot experiments, we used three clients to test the trained model, representing three visual observation points: client A, client B, and client C. In real robot experiments, it is expected to have a drop in accuracy in human action detection. Therefore, we needed to improve the dataset used by creating a second dataset and re-training the architecture. The simulations performed to validate the trained model improved significantly. The results from the model validation are in Table III. The time elapsed compared to the other datasets increased, but also did the detection accuracy, with all the clients learning with an end-around accuracy of 97%. With these validation results, we performed real robot experiments with dataset 2 and two clients, client A, an X500 UAV, and client B, a DJI UAV. The main UAV (i.e., the X500 UAV) used the MRS System [23] to fly, whereas the DJI used its benchmark software. A video of this experiment can be seen at https://youtu.be/AO01r8JV5fw. In these experiments (see Fig. 12), the objective was to command the X500 UAV to perform movements based on the actions performed by the human operator. Consequently, by performing desired actions, the X500 UAV moves to a position where the human operator cannot be visualized. Meanwhile, the DJI UAV is flying in a fixed position, always having the human operator in its field of view. This configuration allows our FL-based architecture to always detect the actions performed by the operator. The human operator performed the following commands in order: (X500) UAV Hover (Fig. 12(a)), Have Command (Fig. 12(b)), Move to Left ((Fig. 12(c))), (DJI) Have Command (Fig. 12(d)), Move to Right, Land (Low detection by X500), Land (Detected by DJI, landed). For the commands to be translated into position commands, the operator must perform a main command action (i.e., Have Command) to turn on the recognition system and send control commands to the drone. After detecting the Have Command action, the operator can now perform the Move to Left command. Once the X500 UAV moves, the human is visually partially blocked (e.g., the feet of the human cannot be seen anymore), and thus, the detection fails. To overcome this problem, the human operator turns to the DJI UAV, enabling the second drone to visualize the human fully. Fig. 12(d) shows that it was possible to see that the DJI realized the detection and later sent it to the UAV X500. With the human within the field of view of the DJI, the commands Move to Right and Land were successfully sent to the X500 UAV. Finally, it is important to mention that this code can also support more drones or cameras to control one or more drones. We can also state the absence of overfitting since there is a difference between a wrong action prediction and no action detected. In the first case, the person is detected by the UAV, but the predicted action is wrong. To avoid this problem, we used the moving average filter. The second case is when the UAV is unable to detect the operator at all, leading to our system also being unable to predict the action. In the real-time experiments, the Accuracy of our system was 0.6867, with the confusion matrix shown in Fig.13, formed by comparing the true action (label) vs. the predicted action. This drop in accuracy is explained by several sources of uncertainties within the robotic system (e.g., distance from the operator, quality of the sensor, changes in light conditions, etc.). Finally, the measured precision of our system was 0.93 while the calculated F1 score was 0.83."}, {"title": "VII. CONCLUSIONS", "content": "This work proposes a Long Short-Term Memory (LSTM) Deep Neural Networks-based action control detector composed of two layers in association with three densely connected layers and FL for multi-UAV proximal control. This approach relies on visual detection performed by the UAVs with onboard cameras. We demonstrated that the proposed architecture performs with a precision of 93% (similar to the state-of-the-art for a benchmark dataset UAV-Gesture [17] and for our own proposed dataset), reaching acceptable accuracy values during training and evaluation, making it possible to control a drone with actions safely. This improvement was achieved by pre-processing the video before the prediction of the machine learning algorithm and removing destructive noises that cause outliers. Finally, the model was tuned with more use cases and possible noises that improve the algorithm performance and validated with real robot experiments. In future works, we aim to improve the robustness of our approach, validating it with experiments within complex environments with different lighting conditions and wind."}]}