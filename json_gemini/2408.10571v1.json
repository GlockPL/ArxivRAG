{"title": "Prompt-Agnostic Adversarial Perturbation for Customized Diffusion Models", "authors": ["Cong Wan", "Xiang Song", "Yuhang He", "Yihong Gong"], "abstract": "Diffusion models have revolutionized customized text-to-image generation, allowing for efficient synthesis of photos from personal data with textual descriptions. However, these advancements bring forth risks including privacy breaches and unauthorized replication of artworks. Previous researches primarily center around using \"prompt-specific methods\" to generate adversarial examples to protect personal images, yet the effectiveness of existing methods is hindered by constrained adaptability to different prompts. In this paper, we introduce a Prompt-Agnostic Adversarial Perturbation (PAP) method for customized diffusion models. PAP first models the prompt distribution using a Laplace Approximation, and then produces prompt-agnostic perturbations by maximizing a disturbance expectation based on the modeled distribution. This approach effectively tackles the prompt-agnostic attacks, leading to improved defense stability. Extensive experiments in face privacy and artistic style protection, demonstrate the superior generalization of our method in comparison to existing techniques.", "sections": [{"title": "Introduction", "content": "Generative methods based on diffusion models [1-4] have made significant improvements in recent years, enabling high quality text-to-image synthesis [5, 6], image editing [7], video generation [8, 9], and text-to-3D conversion [10] by prompt engineering. One of the most representative methods in this field is the Stable Diffusion [11, 12], which is a large-scale text-to-image model. By incorporating customized techniques such as Text Inversion [13] and DreamBooth [14], Stable Diffusion only requires fine-tuning on a few images to accurately generate highly realistic and high-quality images based on the input prompts.\nDespite this promising progress, the abuse of these powerful generative methods with wicked exploitation raises wide concerns [15], especially in portrait tampering [16] and copyright infringement [17]. For example, in Figure 1(a), given several photos of a person, attackers can utilize diffusion models to generate fake images containing personal information, leading to reputation defamation. Even worse, attackers can easily plagiarize unauthorized artworks using diffusion models, leading to copyright and profit issues. There is an urgent technology need to protect images from diffusion model tampering.\nTo this end, recent research studies delve into adding human-invisible adversarial perturbations onto the images to prevent prompt-based tampering using diffusion models. The method Photoguard [18] maximizes the distance in the VAE latent space. Glaze [19] aims to hinder specific style mimicry, while Anti-DreamBooth [20]introduces an alternating training approach between the model and adversarial examples. The AdvDM series [21, 22], Adv-Diff [23] present theoretical frameworks and improved methods for attacking LDM.\nTo meet this challenge, we propose a novel Prompt-Agnostic Adversarial Perturbation (PAP). Different from existing prompt-specific methods that require pre-defining and enumerating the attackers' prompts, PAP models the prompt distribution and generates prompt-agnostic perturbations by maximizing a disturbance expectation based on the prompt distribution. Specifically, using a Laplace approximation, we derive the prompts distribution in a text-image embedding feature space satisfying a Gaussian distribution, whose mean and variance can be estimated by the second-order Taylor expansion and Hessian approximation with an input reference prompt. Then, by sampling on the Gaussian prompt distribution and maximizing a disturbance expectation, we generate the PAP perturbations for the input images. As shown in Figure 1(c), the PAP perturbation is trained using a Ref Prompt, while is robust to unseen Prompts B and C at inference. To verify the efficiency of our proposed method, we conduct comprehensive experiments on three widely used benchmark datasets, including VGGFace2, Celeb-HQ and Wikiart. The experimental results show that the proposed PAP method 1) steadily and significantly outperforms existing prompt-specific perturbation on 6 widely used metrics by a large margin and 2) is robust and effective to different diffusion models, attacker prompts, and diverse datasets. These results demonstrate the efficiency and superiority of the proposed PAP method.\nIn summary, our contributions are as follows:\n\u2022 We propose a novel Prompt-Agnostic Adversarial Perturbation (PAP) for customized text-to-image diffusion models. To the best of our knowledge, this is the first attempt at prompt-agnostic perturbation for customized diffusion models.\n\u2022 We model the prompt distribution as a Gaussian distribution with Laplace approximation, where the approximation error is guaranteed. We then derive algorithms to estimate the mean and variance by Taylor expansion and Hessian approximation.\n\u2022 Based on the prompt distribution modeling, we compute the prompt-agnostic perturbations by maximizing a disturbance expectation via Monte Carlo sampling."}, {"title": "Related Work", "content": "Text-to-image generation models. In the past decade, there have been significant advancements in text-to-image generation models. Early models, such as cGANs [24], introduce label-based control, while diffusion models further advance the field by pushing the boundaries of application"}, {"title": "Prompt-Agnostic Adversarial Perturbation", "content": "Background and Motivation\nPrompt-based Diffusion Models. A vanilla diffusion model [1, 3] aims to gradually transfer a simple Gaussian noise into high-quality images through a series of denoising steps. It mainly contains a forward process (i.e., diffusion) and a backward process (i.e., denoising). Begin with an image xo, the forward process iteratively adds Gaussian noise $\\epsilon \\sim \\mathcal{N}(0, I)$ with a noise scheduler $a_t$, $a_t \\in (0, 1)_{t=1}^{T}$ to the input image. The t-step obtained noised image $x_t$ can be written as:\n$$x_t = \\sqrt{\\bar{a}_t}x_0 + \\sqrt{1 - \\bar{a}_t}\\epsilon$$\nwhere $\\bar{a}_t = \\prod_{i=1}^{t}a_i$. When $t \\rightarrow \\infty$, $x_t$ is a Gaussian noise (i.e., $x_t = \\epsilon$). In the backward process, the objective is to learn a noise predictor $\\epsilon_\\theta(x_t, t)$ predicting the added Gaussian noise at each step t. On this basis, taking a Gaussian noise (i.e., $x_{t(t \\rightarrow \\infty)}$) as input, the diffusion model denoise the image $x_t$ using $\\epsilon_\\theta(\\cdot)$, i.e., $x_{t-1} = (x_{t-1} - \\sqrt{1-\\bar{a}_t}\\epsilon_\\theta(x_t, t)) + \\sigma_t z$, $z \\sim \\mathcal{N}(0, I)$, and iteratively generates a high-quality image $x_0$ by t denoising steps.\nThe prompt-based diffusion models [11] aim to generate a semantic guided image $x_0$ from the Gaussian noise $\\epsilon$. To this end, in the backward process, they additionally take a text prompt c as the input of noise predictor $\\epsilon_\\theta(x_t, t, c)$ alongside the Gaussian noise $x_t$, and align the image and text representation by cross-attention mechanism. The image generation objective of the prompt-based"}, {"title": "PAP: Prompt-Agnostic Perturbation by Prompt Distribution Modeling", "content": "Different from the existing methods that compute a prompt-specific perturbation by enumerating prompt instances, we attempt to compute a prompt-agnostic perturbation by prompt distribution modeling, wherein the obtained perturbation is robust to both seen and unseen attack prompts.\nTo this end, we first model and compute a prompt distribution by Laplace approximation, wherein two estimators $\\hat{c}_x$ and $\\hat{H}^{-1}$ are developed to compute the distribution parameters. And then we perform Monte Carlo sampling on each input distribution to maximize a disturbance expectation.\nSpecifically, for the prompt distribution modeling, we consider a protecting image $x_0$ as input and assume a probability-distance correlation between the attacker prompt c and $x_0$, i.e., the further c is from $x_0$, the lower probability of c is in the distribution, and vice versa. The distribution relies on $x_0$ is ambiguous, thus we introduce an auxiliary text prompt $c_0$ roughly depicting $x_0$ into the modeling. Based on this foundation, we model the prompt distribution in the embedding space as $c \\in \\mathcal{Q}(x_0, c_0)$, where $\\mathcal{Q}(x_0, c_0)$ represents the theoretical distribution with a probability density function $p(c|x_0, c_0)$. Based on this setup, we approximate the original distribution $\\mathcal{Q}(x_0, c_0)$ using a Gaussian distribution $\\mathcal{Q}(x_0, c_0)$ by Laplace approximation, i.e., $c \\in \\mathcal{Q}(x_0, c_0) \\sim \\mathcal{N}(\\hat{c}_x, \\hat{H}^{-1})$, where $\\hat{c}_x = arg \\underset{c}{max} p(c|x_0, c_0)$, and H is the Hessian matrix of $c_x$. As we derived in Section 3.3.1, the approximation error is $\\mathcal{O}(|c - c_x|^3)$, which is negligible as the sampled $c \\in \\mathcal{Q}(x_0, c_0)$ is close to $c_x$.\nOn this basis, we propose two estimators $\\hat{\\psi}$ and $\\hat{\\phi}$ used to estimate $c_x$ and $H^{-1}$, respectively. For $\\hat{\\psi}$, to compute $c_x = arg \\underset{c}{max} p(c|x_0, c_0)$ that best describe $x_0$, we approximate $\\hat{c}_x = \\psi(x_0, \\epsilon)$ by minimizing the generation loss in Eq. (2) with momentum iterations starting from $c_0$. This approach accelerates convergence and avoids getting trapped in local minima. For $\\hat{\\phi}$, we approximate $\\hat{H}^{-1} = \\phi(x, \\epsilon, c_0, t)$ by performing a Taylor expansion around the flattened $\\hat{c}_x$ and incorporating prior information from $c_0$. In Appendixes A.2 and A.3, we have proven that the estimation errors of $c_x$ and $H^{-1}$ are with explicit upper bounds, and more detailed descriptions are provided in Section 3.3.2.\nThen, we compute the prompt-agnostic adversarial perturbation $\\delta$ by maximizing the expectation of $\\mathcal{L}_{cond}$ in Eq.(2) over the prompt distribution $\\mathcal{Q}(x_0, c_0)$. The objective can be formulated as:\n$$\\delta^* = arg \\underset{\\delta}{max} \\mathbb{E}_{c \\sim \\mathcal{Q}(x_0, c_0)} [\\mathcal{L}_{cond}(x_0 + \\delta, c_0; \\theta)]$$\n$$= arg \\underset{\\delta}{max} \\int p(c|x_0, c_0) \\cdot \\mathcal{L}_{cond}(x_0 + \\delta, c_0; \\theta) dc, \\quad s.t. ||\\delta||_p \\leq \\eta,$$"}, {"title": "Modeling the Prompt Distribution $\\mathcal{Q}(x_0, c_0)$", "content": "In this subsection, we first approximate the form of $\\mathcal{Q}(x_0, c_0)$ and then estimate its mean and variance."}, {"title": "Laplace Modeling", "content": "Definition 3.1. Since $x_0$ and $c_0$ are independent of c, we consider $\\mathcal{Z} = p(x_0, c_0)$ as a constant. Denote $g(c) := p(x_0, c_0|c) \\cdot p(c)$, $c_x := arg \\underset{c}{max} g(c)$, and $H := -\\nabla\\nabla_c log g(c)|_{c_m}$ for convenience.\nWe adopt Laplace approximation to model $\\mathcal{Q}(x_0, c_0)$. Using Bayes' theorem, we obtain:\n$$p(c|x_0, c_0) = \\frac{p(x_0, c_0|c) \\cdot p(c)}{p(x_0, c_0)}$$\nWe then approximate log g(c) in Definition 3.1 around $c_x$ using a second-order Taylor expansion:\n$$log g(c) \\approx log g(c_x) - \\frac{1}{2} (c - c_x)^T H (c - c_x).$$\nFrom Eq.(6) and by ignoring terms that are independent of c, we infer that\n$$p(c|x_0, c_0) \\propto exp(-\\frac{1}{2} (c - c_x)^T H (c - c_x)^T),$$\nwhich means $p(c|x_0, c_0)$ could be approximated as a normal distribution, i.e.,\n$$\\mathcal{Q}(x_0, c_0)(c) \\sim \\mathcal{N}(c_x, H^{-1}).$$\nThe derivation is provided in Appendix A.1, wherein the error of the Gaussian approximation is the third-order derivatives of log p(x) around $c_x$, i.e., $\\mathcal{O}(|c - c_x|^3)$."}, {"title": "Parameter Estimators", "content": "Estimator $\\hat{\\psi}$. According to Definition 3.1, $c_x$ is defined as the text feature that maximizes the joint probability of $x_0$ and $c_0$. As directly maximizing the likelihood is untrackable, similar to [43], we convert the likelihood maximization problem into an expectation minimization problem with a proper approximation (please kindly refer to Appendix A.2) for more details), which can be written as:\n$$\\hat{c}_x = \\psi(x_0, \\epsilon) = arg \\underset{c}{min} \\sum_{t=0}^{T} \\mathcal{L}(x_0, \\epsilon, t, c; \\theta) = arg \\underset{c}{min} \\sum_{t=0}^{T} ||\\epsilon - \\epsilon_\\theta(x_t, t, c)||^2$$\nTo solve for $\\hat{c}_x$ in Eq.(9) and avoid local minimal, we derive a momentum-based iterative method [31] with the initial value set as the reference prompt $c_0$:\n$$m_i = \\beta m_{i-1} + (1 - \\beta) \\nabla_c \\mathcal{L}(x, \\epsilon, t, c; \\theta),$$\n$$c_i = c_{i-1} - r \\cdot m_i,$$\nwhere $m_i$ represents the momentum term at iteration i, and r, $\\beta$ are learning rates.\nEstimator $\\hat{\\phi}$. To compute $H$ in Definition 3.1, we adopt three operations: substituting $-log g(c)$ with the loss function $\\mathcal{L}$ in Eq.(2), incorporating prior information from $c_0$, and applying the Taylor approximation of $\\mathcal{L}$ around the flattened $\\hat{c}_x$ (Detailed in Appendix A.3.1),which enable us to compute:\n$$(c_0 - c_x)^T H (c_0 - c_x) = 2 \\cdot (\\mathcal{L}(x, \\epsilon, t, c_0; \\theta) - \\mathcal{L}(x, \\epsilon, t, \\hat{c}_x; \\theta)),$$\nTo obtain $H^{-1}$ from Eq.(11), we simplify the effective dimensionality of H in Appendix A.3.2). This allows us to estimate the $H^{-1}$ using the following expression:\n$$H^{-1} = \\phi(x, \\epsilon, c_0, t) = \\frac{||c_0 - c_x||^2}{2 \\cdot (\\mathcal{L}(x, \\epsilon, t, c_0; \\theta) - \\mathcal{L}(x, \\epsilon, t, \\hat{c}_x; \\theta))} I,$$\nwhere I represents the identity matrix and x are input images. As we analyzed in Appendix A.3.2, the cosine distance between the approximated $\\hat{H}^{-1}$ and $H^{-1}$ (diagonalized assumption) is with an upper bound of 0.0909 under our standard experimental settings. This simplification significantly reduces the computational complexity with a minor approximation error."}, {"title": "Maximizing the disturbance expectation", "content": "In this subsection, we devise the strategy of maximizing the disturbance expectation based on the modeled prompt distribution, thereby obtaining the final PAP algorithm outlined in Algorithm 1.\nSampling Distributions for maximization. To maximize the optimization objective Eq. (4) from a global perspective, we adopt Monte Carlo sampling on all input distributions, including $\\mathcal{Q}(x_0, c_0)$. Drawing inspiration from established adversarial attack methods [21, 28, 44], we iteratively sample values for t, $\\epsilon$, and $\\epsilon_c$. Subsequently, we perform a gradient ascent step of $\\mathcal{L}(x, \\epsilon, t, c; \\theta)$ with respect to x, which can be summarized as:\n$$x_{i+1} = x_i + \\alpha \\cdot sgn(\\nabla_{x_i} \\mathcal{L}(x_i, \\epsilon, t, c; \\theta)|_{\\epsilon \\sim \\mathcal{N}(0, I), t \\in \\mathcal{U}(0, T), c \\sim \\mathcal{Q}(x_0, c_0)}),$$\nwhere sgn() refers to the sign function, and $\\alpha$ controls the step size of the gradient ascent.\nFurther Discussion. To further enhance the effectiveness of our proposed algorithm, we seamlessly integrate it with other techniques in Appendix C, such as ASPL [20], to reach a better performance. We also discuss modifying the perturbation space with the tanh function for smoother optimization with better flexibility [44] than clipping-based constraints in Appendix B. Lastly, Appendix H explores the limitations and future directions of our model."}, {"title": "Experiments", "content": "In this section, we experimentally compare PAP with other protection methods for customized models, specifically targeting DreamBooth, the leading customized text-to-image diffusion model for personalized image synthesis. DreamBooth customizes models by reconstructing images using a generic prompt that includes pseudo-words like \"sks,\" while also addressing overfitting and shifting through a prior preservation loss. We evaluate PAP on various tasks, including privacy and style protection, using diverse datasets."}, {"title": "Experimental setup", "content": "Datasets. Our experiments involve face generation tasks using CelebA-HQ [45] and VGGFace2 [46] datasets, as well as style imitation task using the Wikiart dataset [47]. For CelebA-HQ and VGGFace2, we select subsets of 600 images respectively, with each of 12 photos from an identical individual. For Wikiart, we choose 100 paintings, with each set consisting of 20 paintings from the same artist.\nImplementation Details. We optimize adversarial perturbations over 50 training steps and 20 text sampling steps. The step size of image, text and momentum is set to 1/255 and 0.001, 0.5 respectively, and the default noise budget is 0.05. For the Dreambooth, LoRA,TI models, we train the models"}, {"title": "Comparison with State-of-the-Art Methods", "content": "Face Privacy Protection\nWe first conduct experiments in preserving face privacy. During training, the reference and training prompt are both set as \u201ca photo of sks person\". Then we use ten prompts related to sks person to evaluate the models' ability to synthesize images. The average results are presented in Table 1, showcasing the superior performance of our method in terms of all metrics on both the CelebA-HQ and VGGFace2 datasets. For example, on the Celeb-HQ dataset, our method exceeds the second-best Anti-DB by 13.55% in LPIPS, and reduces by 13.06% and 3.948% in CLIP-I and LAION. Also, on the VGGFace2 dataset, our method exceeds the second-best method by 6.347% in BRISQUE, and reduces by 5.818% and 6.833% in CLIP-I and CLIP. These results highlight the effectiveness of our method in preserving face privacy as well as robustness to datasets and various prompts' attacks. In Figure 2 (left), we visualize some of the comparative protection results.\nStyle Imitation\nWe also evaluate methods' ability to prevent artistic style imitation using the Wikiart dataset. The reference and training prompt are both set as \"a sks painting\". Ten prompts related to the style of"}, {"title": "Ablation Study", "content": "Text Sampling Steps. We evaluate PAP under different text sampling steps N (ranging from 0 to 25) used for sampling the prompt c during training. We use the Wikiart dataset and \"a sks painting\" prompt to conduct this ablation experiment (see Table 2). Taking into account the cycle time and model performance, we set the text sampling step N to 15.\nInference Prompt Combination. To analyze the effect of prompt variation, we design combinations of prompt categories and quantity: 4\u00d720, 8\u00d710, 10\u00d78, 16\u00d75, 20\u00d74. This keeps the total number of generated images constant while varying the number of prompt categories, allowing us to isolate the effect of prompt categories on the results. As Figure 3 shows, PAP consistently surpasses all other comparison methods and maintains a robust defense against changes in prompt categories.\nPseudo-word. In our experiments, we conduct evaluations using the commonly used pseudo-word \"sks,\" which is representative but may not cover all possible cases. To further validate our method, we included additional less commonly used pseudo-words. Results in Table 10 clearly demonstrates that our method consistently outperforms the others with other pseudo-word.\nOther Customized Models. To verify the robustness of proposed methods to fine-tuning methods, we apply PAP to Textual Inversion and DreamBooth with LoRA. LORA [52], a widely used efficient"}, {"title": "Extending Experiments", "content": "DiffPure. DiffPure [53] utilizes SDEdit [54] to purify adversarial images by adding noise and denoising them using diffusion models. In Table 4, we conduct experiments on the Wikiart dataset using DiffPure (t=100). We can see that, 1) compared to No Defense, PAP+DiffPure achieves much better adversarial perturbation performance (0.565(\u2193), 3.38(\u2191), 0.0408(\u2193) advances on LAION, BROSQUE and CLIP metrics). 2) Compared to other methods+DiffPure, PAP+DiffPure still achieves the best performance on all metrics.\nPreprocessing. A recent study [55] reveals that current data protections in text-to-image models are fragile and demonstrate limited robustness against data transformations like JPEG compression. To assess the resilience of our proposed Prompt-Agnostic Adversarial Perturbation (PAP) method, we conduct targeted evaluations using the LAION and BRISQUE metrics. Despite a slight decrease in performance, our method still achieves favorable outcomes in terms of image quality metrics For a detailed discussion and results, please see Appendix F.6."}, {"title": "Conclusion", "content": "This work mitigates risks from misusing customized text-to-image diffusion models. We introduce subtle perturbations optimized from a modeled prompt distribution, fooling such models for any prompt. Demonstrating resilience against diverse attacks, our framework surpasses prior prompt-specific defenses through robustness gains. By efficiently perturbing content via a distribution-aware method, our contributions effectively safeguard images from diffusion model tampering under unknown prompts."}, {"title": "Derivation of Prompt Distribution Modeling", "content": "In this section, our goal is to develop an algorithm for modeling the prompt distribution given a reference prompt $c_0$ and protecting images $x_0$. Since the exact form of this distribution is impractical to derive, we employ the idea of Laplace approximation. By considering a second-order Taylor expansion at a critical point, we approximate the distribution near this critical point as a Gaussian distribution. To reduce computational complexity, we make assumptions and approximate estimates for the mean and variance. The algorithm is presented in Algorithm 2."}, {"title": "Laplace Approximation Details", "content": "Motivation and Basic Idea\nLaplace approximation is a simple yet widely used method for approximating probability distributions, especially in the context of Bayesian inference and machine learning. The main idea behind Laplace approximation is to approximate a target distribution $p(x)$ by a Gaussian distribution $\\mathcal{N}(\\mu, \\Sigma)$, where the mean $\\mu$ and covariance $\\Sigma$ are determined by matching the extreme point and curvature of the target distribution at its extreme point.\nSpecifically, let $c_x$ be the maximum point of $p(x)$, i.e., $c_x = arg \\underset{x}{max} p(x)$. Laplace approximation uses a second-order Taylor expansion of $log p(x)$ around $c_x$ to construct the approximate Gaussian distribution:\n$$log p(x) \\approx log(c_x) - \\frac{1}{2} (x - c_x)^T H(c_x) (x - c_x) + \\mathcal{O}(|x - c_x|^3),$$\nwhere $H(c_x)$ is the Hessian matrix (the matrix of second-order partial derivatives) of $log p(x)$ evaluated at $c_x$. Exponentiating both sides and ignoring the higher-order terms, we obtain the Laplace approximation:\n$$p(x) \\approx \\mathcal{N}(c_x, -H(c_x)^{-1}).$$\nError Analysis\nThe error of the Laplace approximation comes from neglecting the higher-order terms in the Taylor expansion. The approximation error can be quantified by the remainder term $\\mathcal{O}(|x - c_x|^3)$, which represents the third and higher-order derivatives of $log p(x)$ evaluated at some point between x and $c_x$.\nMore precisely, let $\\xi(x)$ denote the remainder term, then the Laplace approximation can be written as:\n$$p(x) = \\mathcal{N}(c_x, -H(c_x)^{-1})exp(\\xi(x)),$$"}, {"title": "Estimation of $c_x$", "content": "From Definition 3.1 and derivations in AdvDM [21], we have:\n$$log g(c) = log p(c) + log p(x_0, c_0 | c)$$\n$$= log p(c) + \\mathbb{E}_t [log p(x_{t-1}|c, x_t)],$$\nwhere $x_t$ is defined in Eq.(1).\nSince minimizing the diffusion loss to maximize likelihood is a common practice, we have:\n$$c_x = arg \\underset{c}{max} \\mathbb{E}_t [log p(x_{t-1} | c, x_t)]$$\n$$= arg \\underset{c}{min} \\mathbb{E}_{t, \\epsilon \\sim \\mathcal{N}(0, 1)} \\mathcal{L}_{cond}(x_0 + \\delta, c; \\theta).$$\nTo reduce computational cost, we approximate Eq.(18) with bounded error, and estimate $c_x$ using a single sample of $\\epsilon$ instead of averaging. The detailed derivation and justification can be found in Appendix A.2.1 and A.2.2. The final estimation of $c_x$ can be derived as follows:\n$$\\hat{c}_x = arg \\underset{c}{min} \\sum_{t=0}^{T} \\mathcal{L}(x, \\epsilon, t, c; \\theta).$$\nEmpirical ablation experiments validate the effectiveness of this estimation, as demonstrated in Table 8.\nTo solve for $\\hat{c}_x$ and avoid local optima, we employ an iterative method with momentum to estimate $c_x$ instead of directly solving for a local minimum. In this approach, we leverage the ensemble of loss functions from different time steps in Eq. (2). Inspired by transferable adversarial attacks [31], we aim to find a transferable $c_x$ that navigates the diverse loss landscape using the momentum iteration algorithm.\n$$m_i = \\beta m_{i-1} + (1 - \\beta) \\nabla \\mathcal{L}(x, \\epsilon, t, c; \\theta),$$\n$$c_i = c_{i-1} - r \\cdot m_i.$$\nHere, $m_i$ represents the momentum term at iteration i. The details of the information about the transferable adversarial attacks can be found in Appendix D.\nAdditionally, considering that $c_0$ serves as a reference prompt input that is typically expected to be highly correlated with the content of the image, we assume that $c_0$ and $c_x$ are very close in the textual space. Then the iterative solution for $c_x$ can be initialized from $c_0$."}, {"title": "Upper bound of $c_x$ estimation error.", "content": "Theorem A.1. Assume $g: R^{m \\times m} \\rightarrow R$ is Lipschitz continuous under $L_1$ norm. Then, as $n \\rightarrow \\infty$, we have\n$$\\frac{1}{n} | \\sum_{i=1}^{n} g(x_i) - g (\\frac{1}{n} \\sum_{i=1}^{n} x_i) | < 2L \\frac{2}{\\sqrt{n}},$$\nwhere $x_i \\stackrel{iid}{\\sim} \\mathcal{N}(0, I)$ for i = 1 : n.\nProof. The Lipschitz continuity condition for the 1-norm can be expressed as follows: For any x, y $\\in R^{m \\times m}$, there exists a constant L > 0 such that $|g(x) - g(y)| \\leq L ||x - y||$. By taking the"}, {"title": "Single sample for $\\epsilon$", "content": "Based on the theorem's conclusion, we have the flexibility to choose between weighting before the forward pass or weighting after the forward pass during Gaussian sampling, while ensuring that their errors are within a certain upper bound. By introducing the scaling factor $\\sqrt{n}$, we are able to preserve the distribution characteristics of the input samples, which is crucial for subsequent diffusion processes.\nAn easy corollary of Theorem A.1 further states:\nCorollary A.2. Let L($\\epsilon$) denotes Eq.(2) for convenience, given t, $x_0$, c, $\\theta$. As n $\\rightarrow \\infty$, we have\n$$|\\frac{1}{n} \\sum_{i=1}^{n} \\mathcal{L}(\\epsilon_i) - \\mathcal{L} (\\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_i)| < 2K \\frac{2}{\\sqrt{n}},$$\nwhere $\\epsilon_i \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(0, I)$ for i = 1 : n, K is finite.\nTo enable backpropagation, the model loss L($\\epsilon$) must be differentiable with bounded gradients, satisfying the Lipschitz continuity condition with respect to $\\epsilon$. In Corollary A.2, we observe that $\\sum_{i=1}^{n} \\epsilon_i \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(0, I)$. Instead of computing the entire average, we can directly sample $\\epsilon$ from $\\mathcal{N}(0, I)$ for the forward process. This allows estimating Eq.(18) practically with a single sample $\\epsilon$, while ensuring that the estimation error is within a certain range as introduced in Eq.(31).\nPlease note that this is a very coarse upper bound estimation. In practice, we find that the actual difference is far less than this upper bound. This may be because we use a pretrained large model which has already been well-trained and generalized well to lots of inputs, making the loss typically very close to and small (always within 1 $\\mathcal{L}(0)$). Therefore, we corroborate with experimental results and find that it unnecessary to resample $\\epsilon$ repeatedly in the process of optimizing $c_x$.\nMoreover, merely comparing the L values is insufficient. We must show that when L values are close, the difference between the corresponding c values is sufficiently small. When the loss difference L($c_x$) - L($c_y$) is constrained within a certain range, the text embedding difference $c_x - c_y$ will also typically be limited, due to the following reasons:\n\u2022 Pretrained language generation models are extremely sensitive to even minor changes in input sequences [56], such as the prompt. These subtle alterations can result in significant variations in the model's predicted outcomes and loss function. Therefore, when the difference in loss is constrained within a certain range, we can infer that the semantic disparity in the prompts is also effectively controlled.\nTherefore, restricting the loss difference helps bound the text embedding difference, validating that the proposed c values indeed represent meaningful alternative text options rather than arbitrary or randomly perturbed sequences. This ensures the method can generate semantically-meaningful alternatives as expected."}, {"title": "Estimation of $H^{-1}$", "content": "Derivation of H\nDirectly computing the H matrix requires quadratic complexity", "vector)": "n$$g(c) \\approx g(c_x) + \\nabla g(c_x)^T (c - c_x) + \\frac{1"}, {"to": "n$$g(c) \\approx g(c_x) - \\frac{1"}, {"g(c)": "n$$H = \\nabla\\nabla \\mathcal{L"}, "c)|_{c_x},$$\nTherefore, we have:\n$$(c_0 - c_x)^T H (c_0 - c_x) = 2(L(c_0) - L(c_x)),$$\nSimplification for Estimation of $H^{-1}$\nResearch [57"]}