{"title": "Generalisation to unseen topologies: Towards control of biological neural network activity", "authors": ["Laurens Engwegen", "Daan Brinks", "Wendelin B\u00f6hmer"], "abstract": "Novel imaging and neurostimulation techniques open doors for advancements in closed-loop control of activity in biological neural networks. This would allow for applications in the investigation of activity propagation, and for diagnosis and treatment of pathological behaviour. Due to the partially observable characteristics of activity propagation, through networks in which edges can not be observed, and the dynamic nature of neuronal systems, there is a need for adaptive, generalisable control. In this paper, we introduce an environment that procedurally generates neuronal networks with different topologies to investigate this generalisation problem. Additionally, an existing transformer-based architecture [1] is adjusted to evaluate the generalisation performance of a deep RL agent in the presented partially observable environment. The agent demonstrates the capability to generalise control from a limited number of training networks to unseen test networks.", "sections": [{"title": "1 Introduction", "content": "Training reinforcement learning (RL) controllers for real-world applications is challenging. On the one hand, the amount of training interactions required by modern deep RL methods prohibits learning in the wild. On the other hand, simulated environments always differ in one way or another from the real world, and the trained neural networks generalise to this difference in unexpected ways. The gap between the training environment and the real world is particularly large in biotechnology, where the controller faces noise, simplified simulation models, incomplete information about many simulation parameters, variability between individual instances of the entities to be controlled, and limited lifetimes of those entities. Meaningful generalisation over all of these factors is a key challenge in achieving optimal control in biotechnological applications, which would allow leveraging reinforcement learning to tackle some of the most intractable societal problems including green energy storage; mediation of climate change effects; and diagnostics and therapeutics for neurological and oncogenic disorders.\nIn this paper, we investigate the topological generalisation of deep reinforcement learning (RL) architectures for closed-loop control of activity in networks of biological neurons. While we use these biological systems as an example, the developed concepts hold interest for control in other environments with incomplete information, heterogeneity and limits on data and training availability. We introduce a simulated environment that allows to evaluate generalisation abilities of deep RL agents with different architectures. A transformer-based architecture [1] shows the capability to infer connections between neurons based on their activity, necessary for generalisable control, indicating that this architecture can be effective in domains where dynamics are governed by non-observable graph-like structures. The experiments show that an RL agent with this architecture is successful in the multitask setting, where control of neuronal activity is evaluated in different (unseen) topologies. The environment that is being introduced allows for the investigation of more factors of variation"}, {"title": "2 Neuronal activity", "content": "Biological neurons communicate with each other through action potentials. Disturbed generation and propagation of these signals is associated with various neurological disorders, such as Parkinson's disease, epilepsy, and anxiety disorders, among others [2, 3]. Diagnosis can be difficult and time- consuming, and treatment is even more complex. Novel imaging and stimulation techniques allow us to measure and manipulate neuronal activity simultaneously, with high spatial and temporal resolution. This has the potential to advance our knowledge of neuronal dynamics and develop novel diagnostic tools and treatment methods. However, the high-dimensional and complex nature of neuronal activity data make a complete analysis and control of the activity difficult, exacerbated by the fact that we cannot observe (all) the connections between neurons, nor every individual neuron in the network, and by the limited lifetime of the sample inhibiting exhaustive training routines. Therefore, a key feature of the system must be the capability to transfer learned control from a set of training samples to new, unseen samples. Additionally, every observed neuronal network will have a different topology in which edges can not, or only partially, be observed. Developing closed-loop adaptive stimulation and detection procedures that can unravel the dynamical properties of activity propagation in such networks, thus, remains challenging.\nEstablished recording modalities, such as fMRI, EEG and microelectrode arrays, are widely used to study brain tissue, both in vitro or in vivo. These modalities are all subject to a relatively low spatial or temporal resolution, which de- prives researchers from studying brain tissue on a cellular or circuit level. Advances in voltage imaging [4, 5] have shown potential to overcome these hurdles. Voltage indicators are molecules that change fluorescence when the voltage in their environment changes. By introducing (e.g. genetically encoding) these indicators into a neu- ron, changes in the voltage activity of the neuron can be measured and observed. Although the performance of genetically encoded voltage in- dicators (their signal-to-noise ratio, sensitivity, and response time) can still be improved, various indicators have been developed that can already be used to study neuronal dynamics on a circuit level with high resolution.\nOptogenetics [7\u20139] enables the control of activity in individual neurons with action potential precision. This biological technology relies on the introduction of light gated ion pumps to make neurons of interest become responsive to light. These pumps can be activated by illumination to manipulate the cell's membrane potential with high specificity, both in vitro and in vivo. Optogenetic approaches have already shown to be useful for the investigation of neuronal circuits in animal models of various"}, {"title": "3 Reinforcement Learning and Generalisation", "content": "A Markov Decision Process (MDP) [11] is often used to model decision-making problems. In many real-world applications, the control agent can not fully observe the state of the environment. The MDP framework can be extended to a Partially Observable Markov Decision Process (POMDP) [12] that is defined by a tuple M = (S, A, \u03a9, T, R, O, $p_o$), consisting of a set of states S, a set of actions A, and a set of observations \u03a9. Additionally, it contains a transition function, T(s'|s, a), that maps (current) state s \u2208 S and action a \u2208 A to a probability distribution over (next) states s' \u2208 S, a reward function R : S \u00d7 A \u00d7 S \u2192 R, and an observation function that maps a state to a probability distribution over observations O : S \u00d7 \u03a9 \u2192 [0, 1]. In any state, the agent can only observe the observation that is produced from that state by O. The last element of the tuple, $p_o$, is the initial state distribution.\nThe goal of RL is to learn a policy \u03c0, with which an agent chooses actions based on available information, that maximises the expected (discounted) cumulative reward J = $E_\u03c0$ [$\\Sigma_{t=0}^{100} \\gamma^t r_t$], with discount factor \u03b3\u2208 [0,1) and the reward at time step t in the environment $r_t$ = R($s_t$, $a_t$, $s_t$). A POMDP can be solved as an induced belief-state MDP in which the complete action-observation history (AOH), which is defined at time step t as $\u03c4_t$ = ($o_0$, $a_0$,..., $o_{t\u22121}$, $a_{t\u22121}$, $o_t$), is summarised by the agent [12] to be able to construct a belief over the current state it is in. In this way, a policy \u03c0(\u03b1|\u03c4), mapping \u03c4 to a probability distribution over actions, can be learned with any RL algorithm.\nQ-learning [13, 14] is often used to learn the optimal Q-function that maps an action a and \u03c4 to the expected cumulative reward when following policy \u03c0: $Q^\u03c0 (\u03c4, a)$ = $E_\u03c0$ [$\\Sigma_{k=0}^{100} \\gamma^k r_{t+k}$|$\u03c4_t$ = \u03c4, $a_t$ = a]. If we find the optimal Q-function, $Q^*(\u03c4, a)$ = max${_\u03c0}Q^\u03c0(\u03c4, a)$, an optimal policy \u03c0* can be derived by greedily taking actions with respect to these Q-values. When dealing with large state and/or action spaces, it is infeasible to learn Q-values for each AOH and action pair individually. In this case, we can learn a parameterised Q-function, e.g. a Deep Q-Network (DQN) [15] in MDPs, and a Deep Recurrent Q-Network (DRQN) [16] in POMDPs. In the latter, a recurrent neural network (RNN) is exploited to learn a latent representation of \u03c4.\nIn real-world environments an agent will often encounter many variations (called contexts) of a (PO)MDP, and training an agent on each of these variations individually is usually infeasible. In such cases, an agent is required to learn a policy that solves all training contexts, in the hope that it also generalises control to new contexts that are not seen during training. Such contexts determine the task of an RL agent, e.g. the configuration of objects in the environment or the level of a game. The Contextual Markov Decision Process (CMDP) [17] framework explicitly allows to evaluate generalisation capabilities [18]. In a CMDP, the state space is factored into the underlying state space and the context space S : S' \u00d7 C. When the context is partially observable, the CMDP can be thought of as a POMDP in which the context is fixed throughout an episode. The CMDP allows us to construct a set of training and testing POMDPS, $M|_{C_{train}}$ and $M|_{C_{test}}$ respectively, where $C_{train}$, $C_{test}$\u2286C and $C_{train}$\u2229$C_{test}$ = 0. To this extent, the (zero-shot) generalisation capabilities of an agent trained on a training set to an unseen test set of POMDPs can be evaluated."}, {"title": "4 Related Work", "content": ""}, {"title": "4.1 Neuronal simulation", "content": "To study neuronal network dynamics in silico, various models for the initiation and propagation of activity have been developed. Oftentimes, neuronal dynamics in individual neurons are defined by an integration process of incoming activity, together with a mechanism that initiates an action potential when the neuron reaches a critical threshold voltage [19]. The Leaky Integrate-and-Fire (LIF) model [20], and its various extensions, is widely used to model spiking neurons [21\u201324], due to its computational efficiency and the emergent realistic behaviour in networks of LIF neurons (with the right hyperparameter values)."}, {"title": "4.2 Closed-loop control of neuronal activity", "content": "Control of neuronal activity has shown to be useful for therapeutic and research purposes. Deep brain stimulation (DBS) [37] is a neurosurgical method that is currently being used for the treatment of Parkinson's and Alzheimer's disease, and the prevention of epileptic seizures, among others. DBS is typically applied in an open-loop fashion, where fixed patterns of stimulation are induced to reduce pathological symptoms. Attempts have been made to close the loop to reduce adverse effects and increase the efficiency of DBS systems. Moreover, the dynamical nature of neuronal systems often requires an adaptive strategy to be effective in real-life systems. Ganzer et al. [38] developed a closed-loop neuromodulation approach to improve recovery from spinal cord injury. This was possible by utilising synaptic plasticity and, thus, required adaptive control. Other studies presented RL approaches to minimise stimulation energy needed to control epileptic seizures [39], and treat Parkinson's disease [40], in computational models of those disorders. Both studies trained and tested agents on the same network, and did not investigate generalisation to different instances of diseased networks.\nDBS traditionally utilises electrical stimulation and recordings through injected electrodes, which can cause injury and hamper real-time readouts of activity. Advancements in adaptive, closed-loop, DBS are motivated by reduced risk of such effects, and accelerated by cutting-edge technologies like voltage imaging and optogenetics that allow for real-time feedback and control. Closed-loop optogenetic control systems have already been developed. Zhang et al. [41], for example, conditioned target neurons on trigger neurons by means of photostimulation and demonstrated long-lasting changes in activity of neuronal ensembles. This study suggested the possibility to correct aberrant activity patterns in diseased conditions by reconfiguration of functional connectivity. Other studies demonstrated closed-loop optogenetic control to suppress pathological seizure-like patterns [42], or to restore bladder function [43].\nWork on closed-loop control of in silico neuronal systems that is most closely related to our paper was presented by Mitchell and Petzold [44], in which stochastic LIF neurons were simulated and spiking patterns had to be produced by deep RL agents. They (re)trained and tested agents on individual networks of neurons and made the strong assumption that external stimulation (in their case both excitatory and inhibitory) could override internal activity propagation in the network. Furthermore, they learned a separate policy for each neuron in the network, which limits the scalability of their approach. In the current work, we do not make their assumption, and a single policy is learned to control activity in neuronal networks. Besides, we investigate the performance of our RL agent on any network, rather than on a single instance. The latter is important for practical applications, where neuromodulation approaches to investigate or reinforce neuronal network activity are desired to be effective immediately. This requires transferring knowledge and, thereby, generalising control to unseen networks."}, {"title": "4.3 Multitask learning", "content": "The core objective of multitask RL is to improve generalisation performance [45]. When training on a distribution of tasks, agents need to leverage domain-specific, rather than instance-specific information to learn a general representation of the system. This representation should be informative and suitable for any task the agent can find itself in. To be consistent, we will refer to tasks as contexts, as introduced in the previous section.\nAs described by Kirk et al. [18], the generalisation performance (or generalisation gap) can be defined by the difference between an agent's performance on a training set and a test set of contexts. A popular benchmark for the development and evaluation of multi-tasking agents is ProcGen [46], that consists of different environments in which different contexts are procedurally generated. In the current work, an environment in which contexts are procedurally generated is introduced to be able to evaluate an agent's generalisation performance, data efficiency, scalability, and robustness, with respect to different contexts."}, {"title": "4.4 Architecture", "content": "To generalise control to different, unseen topologies in which the edges between nodes can not be observed, an architecture through which the agent can infer relationships between nodes is required. Tang and Ha [1] introduced a combination of RNNs and a transformer [47] to construct a permutation invariant agent. They showed that their architecture is invariant to permutations of the input features during an episode. We adopted and adjusted this architecture, using the self-attention mechanism, to induce permutation equivariance in the RL agent. Transformers are starting to be adopted in RL, specifically by viewing the objective as a sequence modelling problem [48, 49], or to learn a world model of the environment [50]. Here, we use it as a representation learning model to capture dependencies between specific objects (neurons in a graph). The self-attention mechanism is at the heart of the transformer, through which attention scores are calculated as A = \u03c3($Q K^T/\\sqrt{d}$)V, where Q, K, V \u2208 $R^{n\u00d7d}$, are the query, key and value matrices that are projections of the input X \u2208 $R^{n\u00d7d}$, such that Q = X$W_Q$, K = X$W_K$ and V = X$W_V$, with learnable parameter matrices $W_Q$, $W_K$ and $W_V$, and \u03c3(\u00b7) a row-wise softmax function.\nWhere Tang and Ha [1] showed robustness to permutations of the input on a single task, we experi- ment with the adjusted version of their architecture in a multitask setting. Moreover, we evaluate generalisation to different topologies, rather than permutations of the same topology. The use of transformers has shown to be effective in other domains, for example in agent-agnostic control [51], where the self-attention mechanism could be exploited to control robots in the absence of morphological information, but where generalisation to new morphologies was limited."}, {"title": "5 Method", "content": "The environment and conceivably effective architectures are described here. More details can be found in Appendix A, B and C"}, {"title": "5.1 Environment", "content": "The goal in this environment is to create a specific target spiking pattern in a target neuron, by manipulating the activity in other neurons. Performance is evaluated in a multitask setting. The agent has to create arbitrary spiking patterns in neuronal networks with different topologies of which the connections are can not be observed: relevant relationships between neurons have to be inferred through interaction. The elements of the partially observable CMDP are briefly explained."}, {"title": "States and observations.", "content": "States in the environment are defined by a multitude of variables, such as the activity of the different neurons, their underlying topology, and the simulation parameters of activity propagation. Only the activity of neurons (membrane potential at the soma), however, can be observed. In the current work, we omit rendering images. It is assumed that we can accurately transform a voltage image to activity measures (i.e. the membrane potential) for each neuron by making use of available segmentation and analysis software. In a simulated environment with n observable neurons, an observation o \u2208 $R^n$ thus contains the membrane potential of the neurons."}, {"title": "Actions.", "content": "The agent can activate (i.e. initiate an action potential in) any of the n observable neurons or do nothing (indicated by -1): A = {\u22121, 0, ..., n\u2212 1}. In the current set-up, the neuron that the agent needs to control is excluded from the set of possible actions in order to make the task more challenging in smaller networks (e.g. n = 8).\nTransitions. The environment simulates LIF neurons with short-term synaptic depression [32, 30, 31]. Details on the simulation procedure can be found in Appendix A. In short, the neuronal networks consist of excitatory and inhibitory neurons (with a ratio of 4:1 on average). Active excitatory neurons will propagate their activity to connected neurons, delivering a positive input current in the next time step. Active inhibitory neurons will propagate a negative input current. When an agent chooses to activate a neuron, this neuron will receive a strong positive (external) input current. In the present setup, transitions are deterministic and only depend on the current state and action."}, {"title": "Procedural context generation.", "content": "Contexts in the environment consist of (1) $C_{topology}$, the non- observable topology of the biological neuronal network, (2) $C_{neuron}$, the observable target neuron that the agent has to control, and (3) $C_{pattern}$, the observable target spiking pattern that the agent has to create in the target neuron. The target neuron is defined by the ID of the neuron: $C_{neuron}$ \u2208 {0, ..., n-1}. The target pattern is a binary vector that indicates at which time steps the target neuron must be spiking: $C_{pattern}$ \u2208 {$0,1$}$^N$ during an episode of N time steps. The procedural generation of the elements that constitute the partially observable context, c = [$C_{topology}$, $C_{neuron}$, $C_{pattern}$], is explained in detail in Appendix B. To test generalisation to different (non-observable) topologies, the training and test context sets consist of unique topologies only: \u2200c, c' \u2208 $C_{train}$ \u222a$C_{test}$, (c \u2260 c') \u2192 ($C_{topology}$ \u2260 $C_{topology}$)."}, {"title": "Rewards.", "content": "A reward only depends on the current state st. If the activity (i.e. spiking or not spiking) of the target neuron $C_{neuron}$ corresponds to the desired spiking pattern at the current time step, ($C_{pattern})_t$, the agent receives a positive reward. To reward correct spikes as much as correct inactive time steps, the active reward $R_A$ is scaled by the number of spikes m in the target pattern: $R_A$ = $\\frac{R_{max}}{m}$, and the inactive reward $R_I$ by the number of inactive time steps N - m: $R_I$ = $\\frac{R_{max}}{N-m}$. When the target neuron is either incorrectly active or inactive, the agent receives a reward of 0.\nFor the experiments discussed in this work, various (possible) properties of realistic voltage images are not considered, such as non-observable neurons, neurons with different dynamics, background noise, photobleaching effects, and stochastic activity propagation, among many others. Nonetheless, the environment can be (and has been) extended to include such properties, each of which comes with its own challenges for the RL agent. In this paper, however, we investigate generalisation to different topologies and, therefore, reduce the environment's complexity."}, {"title": "5.2 Agent architectures", "content": "Recurrent neural networks are often used to learn a representation of the AOH, \u03c4, in POMDPS, as in DRQN [16]. Such architecture enables the agent to implicitly learn a belief of the state it is in to make the process Markovian. DRQN is therefore implemented for one of the baseline agents in the introduced partially observable CMDP, where a crucial part of the context (the topology) is non-observable. Long Short-Term Memory (LSTM) [52] is used to encode the AOH. The target neuron and target pattern are the part of the context that is observable, to which we refer as $C_{obs}$. The observable context is encoded by feed-forward (FF) networks (see Appendix C for more details) and concatenated to the latent representation of \u03c4. This embedding is mapped to Q-values for each action.\nWe compare 4 different architectures that are used as parameterisation for Q-functions, which are schematically visualised in Figure 2.\nTang and Ha [1] distribute the mapping of the AOH over different RNNs. In the current environment, this can be achieved by mapping the observation and previous action, $o_t^i$ and $a_{t\u22121}^i$, at time step t of each individual neuron i with its own LSTM. In practice, we use one LSTM (i.e. one set of trainable parameters), but keep track of a different hidden state for each neuron, which encodes the local AOH $\u03c4^i_t$ for that neuron. The previous action, $a_{t\u22121}^i$, here indicates whether or not neuron i was"}, {"title": "6 Results", "content": "The introduced architectures are trained and tested on the two different sets of partially observable contexts, $C_{train}$ and $C_{test}$, to evaluate generalisation performance. Each set consists of 100 contexts. For the current experiments, networks contain n = 8 neurons and connections can not be observed. More detailed experimental information can be found in Appendix D. Since the active and inactive rewards are weighted, as described in the previous section, the return of an episode in which an agent did not do anything is equal to 0.5 (which is why lower values are not shown in the Figures)."}, {"title": "7 Discussion", "content": "In this paper, an environment that simulates biological neuronal networks on a circuit level was introduced. Although numerous abstractions have been made, neurons are simulated with an often used (LIF with synaptic plasticity) model for activity propagation based on biophysical properties. Besides, the environment allows to tackle some key challenges that arise in real-life biological tissue, such as its partial observability and the necessity to generalise control to unseen samples. To start addressing these issues, an existing Transformer-based architecture was slightly adjusted and evaluated to control the activity in neuronal networks with procedurally generated contexts. Where it was already shown that such architecture can be robust to permutations of the input, here we have shown that it can generalise to unseen contexts while requiring a limited number of training samples. The Transformer encoder effectively learns to associate the dependence between objects in an environment, in our case simulated neurons, from their local action-observation history, which could not be achieved through simple feed-forward networks.\nThe investigated architecture could be useful for generalisation in other domains with an underlying topological structure, such as traffic light control [53, 54], or robotic control in a morphology-agnostic manner, i.e., to generalise control to different robots without the need of morphological information [51]. Furthermore, it would be interesting to investigate its generalisation performance in the visual domain. For example, the experiments on visual tasks in the original paper of Tang and Ha [1] could be extended to evaluate performance in different contexts, rather than in a permuted version of the same context. However, the quadratic complexity of self-attention and the need to encode a separate local action-observation histories require a lot of compute and memory. It would, thus, be beneficial for scalability to develop more efficient architectures [e.g. 55].\nThe presented experiments were performed on simulations that are simplifications of actual neu- roimaging data that we can currently acquire. Although these experiments tackled the fundamental problem of partial observability of the context of the task, realistic features of the real-world setting could be introduced to complicate control. For example, voltage images are inherently noisy, spiking behaviour is naturally stochastic and governed by a multitude of synapses rather than a single connec- tion, and we can often not observe all neurons in the sample. Future extensions of the environment could incorporate such aspects to challenge RL agents along the way to investigation of neuronal tissue through closed-loop control. Nonetheless, the introduced environment enables experimenta- tion in different RL research areas, such as multi-agent RL, causality, and meta-learning, besides generalisation in partially observable environments, with potential applications in neuroscience."}, {"title": "A Simulating neuronal networks", "content": "Neurons are simulated with the LIF model and short-term synaptic depression. The current simulation method is a simplification of [32]. At each time step t (of 1ms) the membrane potential $V_j$ of neuron j is updated by the following differential equation:\n$\\frac{dV_j}{dt}$ = $\\frac{V_j - V_r}{RC}$ + $\\frac{I(a,t)}{C}$ + $\\Sigma_i \\Sigma_k H(p_r U_k(t) - 0.5)I_{in}(i,t)$ (1)\nwhere $V_r$ indicates the resting potential, R the resistance, and C the capacitance of the cell membrane. In the current setup, when an agent acts in the environment, it can induce external activity I(a,t) = $w_e \\delta(j - a)$ of input current $w_e$, when its action a corresponds to neuron j. The input current from other neurons i in the network to neuron j is defined by:\n$I_{in}(i,t)$ = $w_{in} \\delta(t - t_i) G_{i,j}$ (2)\nwith $w_{in}$ the weight (synaptic strength) of incoming internal input. $t_i$ contains the (last) spike time of neuron i, and the connectivity matrix G indicates whether there is an excitatory (1), inhibitory (-1), or no (0) connection between two neurons.\nH is the heavy-side step function and $p_r$ is a probability that influences the possibility of a pre- synaptic synapse releasing its vesicles. Short-term synaptic depression is governed by $U_k$, which relates to the availability of vesicles and neurotransmitters at pre-synaptic site k:\n$U_k(t)$ = 1 - $e^{-\\frac{t-t_k}{\\tau_R}}$  (3)\nwhere $t_k$ indicates the last time step at which vesicles were released at site k, and $\u03c4_R$ is the time constant for the uptake of vesicles and neurotransmitters.\nWhenever a neuron's membrane potential reaches the threshold $V_{th}$, it emits a spike or action potential. Directly after this spike, the membrane potential is reset to the resting potential and the neuron enters a refractory period, $\u03c4_{rp}$, during which it is unable to spike (even from external input)."}, {"title": "B Procedural context generation", "content": "The procedural generation of contexts consists of three steps: network topology generation, target neuron assignment and target spiking pattern creation.\nNetwork topologies, in the current work, are defined by a connectivity (i.e. adjacency) matrix that indicates for all n neurons to which other neurons they are connected. Small-world networks are generated by giving neurons that are closer to each other a higher probability to form a connection, similar to experiments in [32]. Specifically, we make an abstraction in which neurons are placed on a line, where each pair of neurons i, j \u2208 {0, ...n} have a probability $P_{i,j}$ to form a connection from i to j. The distance between neuron i and j, d(i, j), is defined by |i \u2013 j|.\n$P_{i,j} = \\{\\begin{array}{ll} 0.3 & \\text{ if } d(i, j) \\leq 4,\\\\ 0.2 & \\text{ if } 5 \\leq d(i, j) \\leq 8,\\\\ 0.1 & \\text{ if } 9 < d(i, j) < 12,\\\\ 0.01 & \\text{ otherwise } \\end{array}$"}, {"title": "C Encoding observable context", "content": "The target neuron and target spiking pattern, $C_{neuron}$ and $C_{pattern}$, are observable for the agent. The parameterised Q-function can therefore additionally be conditioned on this information. A simple FF network is used to encode $C_{neuron}$. The target pattern, of length N, is encoded slightly more elaborate. The pattern is shifted in such a way that the element $(C_{neuron})_t$ at current time step t is in front when encoding the pattern with a FF network. As we are constrained by a fixed input length in a FF network, the input pattern is padded with -1. Thus, the input of the FF network at time step t is: $(C_{neuron})_t:N-1$ concatenated with { -1}$^t$. The encoded target neuron and target pattern are thereafter concatenated to shape the encoding of the observable context."}, {"title": "D Experimental setup", "content": "The hyperparameters that are used in the experiments are listed in Table 2. We used double Q-learning [14], target networks with soft updates, epsilon-greey exploratino and uniform sampling from the replay buffer. The hidden size in the different components of the architectures are chosen in such a way that there are no extreme differences in total number of parameters between the architectures."}]}