{"title": "Generalisation to unseen topologies:\nTowards control of biological neural network activity", "authors": ["Laurens Engwegen", "Daan Brinks", "Wendelin B\u00f6hmer"], "abstract": "Novel imaging and neurostimulation techniques open doors for advancements in\nclosed-loop control of activity in biological neural networks. This would allow\nfor applications in the investigation of activity propagation, and for diagnosis and\ntreatment of pathological behaviour. Due to the partially observable characteristics\nof activity propagation, through networks in which edges can not be observed, and\nthe dynamic nature of neuronal systems, there is a need for adaptive, generalisable\ncontrol. In this paper, we introduce an environment that procedurally generates neu-\nronal networks with different topologies to investigate this generalisation problem.\nAdditionally, an existing transformer-based architecture is adjusted to evaluate the\ngeneralisation performance of a deep RL agent in the presented partially observable\nenvironment. The agent demonstrates the capability to generalise control from a\nlimited number of training networks to unseen test networks.", "sections": [{"title": "1 Introduction", "content": "Training reinforcement learning (RL) controllers for real-world applications is challenging. On the\none hand, the amount of training interactions required by modern deep RL methods prohibits learning\nin the wild. On the other hand, simulated environments always differ in one way or another from\nthe real world, and the trained neural networks generalise to this difference in unexpected ways.\nThe gap between the training environment and the real world is particularly large in biotechnology,\nwhere the controller faces noise, simplified simulation models, incomplete information about many\nsimulation parameters, variability between individual instances of the entities to be controlled,\nand limited lifetimes of those entities. Meaningful generalisation over all of these factors is a key\nchallenge in achieving optimal control in biotechnological applications, which would allow leveraging\nreinforcement learning to tackle some of the most intractable societal problems including green\nenergy storage; mediation of climate change effects; and diagnostics and therapeutics for neurological\nand oncogenic disorders.\nIn this paper, we investigate the topological generalisation of deep reinforcement learning (RL)\narchitectures for closed-loop control of activity in networks of biological neurons. While we use\nthese biological systems as an example, the developed concepts hold interest for control in other\nenvironments with incomplete information, heterogeneity and limits on data and training availability.\nWe introduce a simulated environment that allows to evaluate generalisation abilities of deep RL\nagents with different architectures. A transformer-based architecture [1] shows the capability to infer\nconnections between neurons based on their activity, necessary for generalisable control, indicating\nthat this architecture can be effective in domains where dynamics are governed by non-observable\ngraph-like structures. The experiments show that an RL agent with this architecture is successful in\nthe multitask setting, where control of neuronal activity is evaluated in different (unseen) topologies.\nThe environment that is being introduced allows for the investigation of more factors of variation"}, {"title": "2 Neuronal activity", "content": "Biological neurons communicate with each other through action potentials. Disturbed generation and\npropagation of these signals is associated with various neurological disorders, such as Parkinson's\ndisease, epilepsy, and anxiety disorders, among others [2, 3]. Diagnosis can be difficult and time-\nconsuming, and treatment is even more complex. Novel imaging and stimulation techniques allow us\nto measure and manipulate neuronal activity simultaneously, with high spatial and temporal resolution.\nThis has the potential to advance our knowledge of neuronal dynamics and develop novel diagnostic\ntools and treatment methods. However, the high-dimensional and complex nature of neuronal activity\ndata make a complete analysis and control of the activity difficult, exacerbated by the fact\nthat we cannot observe (all) the connections between neurons, nor every individual neuron in the\nnetwork, and by the limited lifetime of the sample inhibiting exhaustive training routines. Therefore,\na key feature of the system must be the capability to transfer learned control from a set of training\nsamples to new, unseen samples. Additionally, every observed neuronal network will have a different\ntopology in which edges can not, or only partially, be observed. Developing closed-loop adaptive\nstimulation and detection procedures that can unravel the dynamical properties of activity propagation\nin such networks, thus, remains challenging.\nEstablished recording modalities, such as fMRI,\nEEG and microelectrode arrays, are widely used\nto study brain tissue, both in vitro or in vivo.\nThese modalities are all subject to a relatively\nlow spatial or temporal resolution, which de-\nprives researchers from studying brain tissue on\na cellular or circuit level. Advances in voltage\nimaging [4, 5] have shown potential to overcome\nthese hurdles. Voltage indicators are molecules\nthat change fluorescence when the voltage in\ntheir environment changes. By introducing (e.g.\ngenetically encoding) these indicators into a neu-\nron, changes in the voltage activity of the neuron\ncan be measured and observed. Although the\nperformance of genetically encoded voltage in-\ndicators (their signal-to-noise ratio, sensitivity,\nand response time) can still be improved, various\nindicators have been developed that can already\nbe used to study neuronal dynamics on a circuit\nlevel, such as QuasAr6a [6] that is visible in\nFigure 1. Voltage imaging enables researchers\nto study action potential propagation on a network or cellular level with high resolution.\nOptogenetics [7\u20139] enables the control of activity in individual neurons with action potential precision.\nThis biological technology relies on the introduction of light gated ion pumps to make neurons of\ninterest become responsive to light. These pumps can be activated by illumination to manipulate the\ncell's membrane potential with high specificity, both in vitro and in vivo. Optogenetic approaches\nhave already shown to be useful for the investigation of neuronal circuits in animal models of various"}, {"title": "3 Reinforcement Learning and Generalisation", "content": "A Markov Decision Process (MDP) [11] is often used to model decision-making problems. In many\nreal-world applications, the control agent can not fully observe the state of the environment. The\nMDP framework can be extended to a Partially Observable Markov Decision Process (POMDP)\n[12] that is defined by a tuple M = (S, A, \u03a9, T, R, O, po), consisting of a set of states S, a set of\nactions A, and a set of observations \u03a9. Additionally, it contains a transition function, $T(s'|s, a)$, that\nmaps (current) state $s \\in S$ and action $a \\in A$ to a probability distribution over (next) states $s' \\in S$, a\nreward function $R : S \\times A \\times S \\rightarrow R$, and an observation function that maps a state to a probability\ndistribution over observations $O : S \\times \\Omega \\rightarrow [0, 1]$. In any state, the agent can only observe the\nobservation that is produced from that state by O. The last element of the tuple, po, is the initial state\ndistribution.\nThe goal of RL is to learn a policy $\\pi$, with which an agent chooses actions based on available\ninformation, that maximises the expected (discounted) cumulative reward $J = E_\\pi [\\Sigma_{t=0}^\\infty \\gamma^t r_t]$, with\ndiscount factor $\\gamma\\in [0,1)$ and the reward at time step t in the environment $r_t = R(s_t, a_t, s_t)$. A\nPOMDP can be solved as an induced belief-state MDP in which the complete action-observation\nhistory (AOH), which is defined at time step t as $\\tau_t = (o_0, a_0,..., o_{t-1}, a_{t-1}, o_t)$, is summarised\nby the agent [12] to be able to construct a belief over the current state it is in. In this way, a policy\n$\\pi(a|\\tau)$, mapping $\\tau$ to a probability distribution over actions, can be learned with any RL algorithm.\nQ-learning [13, 14] is often used to learn the optimal Q-function that maps an action $a$ and $\\tau$ to the\nexpected cumulative reward when following policy $\\pi$: $Q^{\\pi}(\\tau, a) = E_{\\pi}[\\Sigma_{k=0}^\\infty \\gamma^{k} r_{t+k}|\\tau_t = \\tau, a_t = a]$.\nIf we find the optimal Q-function, $Q^*(\\tau, a) = \\max_\\pi Q^{\\pi}(\\tau, a)$, an optimal policy $\\pi^*$ can be derived\nby greedily taking actions with respect to these Q-values. When dealing with large state and/or action\nspaces, it is infeasible to learn Q-values for each AOH and action pair individually. In this case, we\ncan learn a parameterised Q-function, e.g. a Deep Q-Network (DQN) [15] in MDPs, and a Deep\nRecurrent Q-Network (DRQN) [16] in POMDPs. In the latter, a recurrent neural network (RNN) is\nexploited to learn a latent representation of $\\tau$.\nIn real-world environments an agent will often encounter many variations (called contexts) of a\n(PO)MDP, and training an agent on each of these variations individually is usually infeasible. In\nsuch cases, an agent is required to learn a policy that solves all training contexts, in the hope that it\nalso generalises control to new contexts that are not seen during training. Such contexts determine\nthe task of an RL agent, e.g. the configuration of objects in the environment or the level of a game.\nThe Contextual Markov Decision Process (CMDP) [17] framework explicitly allows to evaluate\ngeneralisation capabilities [18]. In a CMDP, the state space is factored into the underlying state space\nand the context space $S : S' \\times C$. When the context is partially observable, the CMDP can be thought\nof as a POMDP in which the context is fixed throughout an episode. The CMDP allows us to construct\na set of training and testing POMDPS, $M|_{C_{train}}$ and $M|_{C_{test}}$ respectively, where $C_{train}, C_{test}\\subseteq C$ and\n$C_{train} \\cap C_{test} = \\emptyset$. To this extent, the (zero-shot) generalisation capabilities of an agent trained on a\ntraining set to an unseen test set of POMDPs can be evaluated."}, {"title": "4 Related Work", "content": "4.1 Neuronal simulation\nTo study neuronal network dynamics in silico, various models for the initiation and propagation\nof activity have been developed. Oftentimes, neuronal dynamics in individual neurons are defined\nby an integration process of incoming activity, together with a mechanism that initiates an action\npotential when the neuron reaches a critical threshold voltage [19]. The Leaky Integrate-and-Fire\n(LIF) model [20], and its various extensions, is widely used to model spiking neurons [21\u201324], due to\nits computational efficiency and the emergent realistic behaviour in networks of LIF neurons (with\nthe right hyperparameter values).\n4.2 Closed-loop control of neuronal activity\nControl of neuronal activity has shown to be useful for therapeutic and research purposes. Deep\nbrain stimulation (DBS) [37] is a neurosurgical method that is currently being used for the treatment\nof Parkinson's and Alzheimer's disease, and the prevention of epileptic seizures, among others.\nDBS is typically applied in an open-loop fashion, where fixed patterns of stimulation are induced to\nreduce pathological symptoms. Attempts have been made to close the loop to reduce adverse effects\nand increase the efficiency of DBS systems. Moreover, the dynamical nature of neuronal systems\noften requires an adaptive strategy to be effective in real-life systems. Ganzer et al. [38] developed\na closed-loop neuromodulation approach to improve recovery from spinal cord injury. This was\npossible by utilising synaptic plasticity and, thus, required adaptive control. Other studies presented\nRL approaches to minimise stimulation energy needed to control epileptic seizures [39], and treat\nParkinson's disease [40], in computational models of those disorders. Both studies trained and tested\nagents on the same network, and did not investigate generalisation to different instances of diseased\nnetworks.\nDBS traditionally utilises electrical stimulation and recordings through injected electrodes, which\ncan cause injury and hamper real-time readouts of activity. Advancements in adaptive, closed-loop,\nDBS are motivated by reduced risk of such effects, and accelerated by cutting-edge technologies\nlike voltage imaging and optogenetics that allow for real-time feedback and control. Closed-loop\noptogenetic control systems have already been developed. Zhang et al. [41], for example, conditioned\ntarget neurons on trigger neurons by means of photostimulation and demonstrated long-lasting\nchanges in activity of neuronal ensembles. This study suggested the possibility to correct aberrant\nactivity patterns in diseased conditions by reconfiguration of functional connectivity. Other studies\ndemonstrated closed-loop optogenetic control to suppress pathological seizure-like patterns [42], or\nto restore bladder function [43].\nWork on closed-loop control of in silico neuronal systems that is most closely related to our paper was\npresented by Mitchell and Petzold [44], in which stochastic LIF neurons were simulated and spiking\npatterns had to be produced by deep RL agents. They (re)trained and tested agents on individual\nnetworks of neurons and made the strong assumption that external stimulation (in their case both\nexcitatory and inhibitory) could override internal activity propagation in the network. Furthermore,\nthey learned a separate policy for each neuron in the network, which limits the scalability of their\napproach. In the current work, we do not make their assumption, and a single policy is learned to\ncontrol activity in neuronal networks. Besides, we investigate the performance of our RL agent on\nany network, rather than on a single instance. The latter is important for practical applications, where\nneuromodulation approaches to investigate or reinforce neuronal network activity are desired to be\neffective immediately. This requires transferring knowledge and, thereby, generalising control to\nunseen networks."}, {"title": "4.3 Multitask learning", "content": "The core objective of multitask RL is to improve generalisation performance [45]. When training\non a distribution of tasks, agents need to leverage domain-specific, rather than instance-specific\ninformation to learn a general representation of the system. This representation should be informative\nand suitable for any task the agent can find itself in. To be consistent, we will refer to tasks as contexts,\nas introduced in the previous section.\nAs described by Kirk et al. [18], the generalisation performance (or generalisation gap) can be defined\nby the difference between an agent's performance on a training set and a test set of contexts. A\npopular benchmark for the development and evaluation of multi-tasking agents is ProcGen [46], that\nconsists of different environments in which different contexts are procedurally generated. In the\ncurrent work, an environment in which contexts are procedurally generated is introduced to be able\nto evaluate an agent's generalisation performance, data efficiency, scalability, and robustness, with\nrespect to different contexts."}, {"title": "4.4 Architecture", "content": "To generalise control to different, unseen topologies in which the edges between nodes can not be\nobserved, an architecture through which the agent can infer relationships between nodes is required.\nTang and Ha [1] introduced a combination of RNNs and a transformer [47] to construct a permutation\ninvariant agent. They showed that their architecture is invariant to permutations of the input features\nduring an episode. We adopted and adjusted this architecture, using the self-attention mechanism,\nto induce permutation equivariance in the RL agent. Transformers are starting to be adopted in\nRL, specifically by viewing the objective as a sequence modelling problem [48, 49], or to learn a\nworld model of the environment [50]. Here, we use it as a representation learning model to capture\ndependencies between specific objects (neurons in a graph). The self-attention mechanism is at the\nheart of the transformer, through which attention scores are calculated as $A = \\sigma(Q K^T / \\sqrt{d})V$, where\n$Q, K, V \\in R^{n \\times d}$, are the query, key and value matrices that are projections of the input $X \\in R^{n \\times d}$,\nsuch that $Q = XW_Q, K = XW_K$ and $V = XW_V$, with learnable parameter matrices $W_Q, W_K$\nand $W_V$, and $\\sigma(\\cdot)$ a row-wise softmax function.\nWhere Tang and Ha [1] showed robustness to permutations of the input on a single task, we experi-\nment with the adjusted version of their architecture in a multitask setting. Moreover, we evaluate\ngeneralisation to different topologies, rather than permutations of the same topology. The use of\ntransformers has shown to be effective in other domains, for example in agent-agnostic control\n[51], where the self-attention mechanism could be exploited to control robots in the absence of\nmorphological information, but where generalisation to new morphologies was limited."}, {"title": "5 Method", "content": "The environment and conceivably effective architectures are described here. More details can be\nfound in Appendix A, B and C\n5.1 Environment\nThe goal in this environment is to create a specific target spiking pattern in a target neuron, by\nmanipulating the activity in other neurons. Performance is evaluated in a multitask setting. The agent\nhas to create arbitrary spiking patterns in neuronal networks with different topologies of which the\nconnections are can not be observed: relevant relationships between neurons have to be inferred\nthrough interaction. The elements of the partially observable CMDP are briefly explained.\nStates and observations. States in the environment are defined by a multitude of variables, such\nas the activity of the different neurons, their underlying topology, and the simulation parameters of\nactivity propagation. Only the activity of neurons (membrane potential at the soma), however, can\nbe observed. In the current work, we omit rendering images. It is assumed that we can accurately\ntransform a voltage image to activity measures (i.e. the membrane potential) for each neuron by\nmaking use of available segmentation and analysis software. In a simulated environment with n\nobservable neurons, an observation $o \\in R^n$ thus contains the membrane potential of the neurons."}, {"title": "Actions", "content": "The agent can activate (i.e. initiate an action potential in) any of the n observable neurons\nor do nothing (indicated by -1): $A = \\{-1, 0, ..., n-1\\}$. In the current set-up, the neuron that the\nagent needs to control is excluded from the set of possible actions in order to make the task more\nchallenging in smaller networks (e.g. $n = 8)^1$.\nTransitions. The environment simulates LIF neurons with short-term synaptic depression [32, 30,\n31]. Details on the simulation procedure can be found in Appendix A. In short, the neuronal networks\nconsist of excitatory and inhibitory neurons (with a ratio of 4:1 on average). Active excitatory neurons\nwill propagate their activity to connected neurons, delivering a positive input current in the next time\nstep. Active inhibitory neurons will propagate a negative input current. When an agent chooses to\nactivate a neuron, this neuron will receive a strong positive (external) input current. In the present\nsetup, transitions are deterministic and only depend on the current state and action.\nProcedural context generation. Contexts in the environment consist of (1) $C_{topology}$, the non-\nobservable topology of the biological neuronal network, (2) $C_{neuron}$, the observable target neuron\nthat the agent has to control, and (3) $C_{pattern}$, the observable target spiking pattern that the agent\nhas to create in the target neuron. The target neuron is defined by the ID of the neuron: $C_{neuron} \\in$\n$\\lbrace0, ..., n-1\\rbrace$. The target pattern is a binary vector that indicates at which time steps the target neuron\nmust be spiking: $C_{pattern} \\in \\lbrace0,1\\rbrace^N$ during an episode of N time steps. The procedural generation\nof the elements that constitute the partially observable context, $c = [C_{topology}, C_{neuron}, C_{pattern}]$, is\nexplained in detail in Appendix B. To test generalisation to different (non-observable) topologies, the\ntraining and test context sets consist of unique topologies only: $\\forall c, c' \\in C_{train} \\cup C_{test}, (c \\neq c') \\rightarrow$\n$(C_{topology} \\neq C'_{topology})$.\nRewards. A reward only depends on the current state $s_t$. If the activity (i.e. spiking or not\nspiking) of the target neuron $C_{neuron}$ corresponds to the desired spiking pattern at the current time\nstep, $(C_{pattern})_t$, the agent receives a positive reward. To reward correct spikes as much as correct\ninactive time steps, the active reward $R_A$ is scaled by the number of spikes $m$ in the target pattern:\n$R_A = \\frac{R_{max}}{m}$, and the inactive reward $R_I$ by the number of inactive time steps $N-m$: $R_I = \\frac{R_{max}}{N-m}$.\nWhen the target neuron is either incorrectly active or inactive, the agent receives a reward of 0.\nFor the experiments discussed in this work, various (possible) properties of realistic voltage images\nare not considered, such as non-observable neurons, neurons with different dynamics, background\nnoise, photobleaching effects, and stochastic activity propagation, among many others. Nonetheless,\nthe environment can be (and has been) extended to include such properties, each of which comes\nwith its own challenges for the RL agent. In this paper, however, we investigate generalisation to\ndifferent topologies and, therefore, reduce the environment's complexity."}, {"title": "5.2 Agent architectures", "content": "Recurrent neural networks are often used to learn a representation of the AOH, $\\tau$, in POMDPS, as\nin DRQN [16]. Such architecture enables the agent to implicitly learn a belief of the state it is in\nto make the process Markovian. DRQN is therefore implemented for one of the baseline agents\nin the introduced partially observable CMDP, where a crucial part of the context (the topology) is\nnon-observable. Long Short-Term Memory (LSTM) [52] is used to encode the AOH. The target\nneuron and target pattern are the part of the context that is observable, to which we refer as $C_{obs}$. The\nobservable context is encoded by feed-forward (FF) networks (see Appendix C for more details) and\nconcatenated to the latent representation of $\\tau$. This embedding is mapped to Q-values for each action.\nWe compare 4 different architectures that are used as parameterisation for Q-functions, which are\nschematically visualised in Figure 2.\nTang and Ha [1] distribute the mapping of the AOH over different RNNs. In the current environment,\nthis can be achieved by mapping the observation and previous action, $o_t^i$ and $a_{t-1}^i$, at time step t\nof each individual neuron i with its own LSTM. In practice, we use one LSTM (i.e. one set of\ntrainable parameters), but keep track of a different hidden state for each neuron, which encodes the\nlocal AOH $\\tau_t^i$ for that neuron. The previous action, $a_{t-1}^i$, here indicates whether or not neuron i was"}, {"title": "6 Results", "content": "The introduced architectures are trained and tested on the two different sets of partially observable\ncontexts, $C_{train}$ and $C_{test}$, to evaluate generalisation performance. Each set consists of 100 contexts.\nFor the current experiments, networks contain $n = 8$ neurons and connections can not be observed.\nMore detailed experimental information can be found in Appendix D. Since the active and inactive\nrewards are weighted, as described in the previous section, the return of an episode in which an agent\ndid not do anything is equal to 0.5 (which is why lower values are not shown in the Figures)."}, {"title": "7 Discussion", "content": "In this paper, an environment that simulates biological neuronal networks on a circuit level was\nintroduced. Although numerous abstractions have been made, neurons are simulated with an often\nused (LIF with synaptic plasticity) model for activity propagation based on biophysical properties.\nBesides, the environment allows to tackle some key challenges that arise in real-life biological\ntissue, such as its partial observability and the necessity to generalise control to unseen samples. To\nstart addressing these issues, an existing Transformer-based architecture was slightly adjusted and\nevaluated to control the activity in neuronal networks with procedurally generated contexts. Where\nit was already shown that such architecture can be robust to permutations of the input, here we\nhave shown that it can generalise to unseen contexts while requiring a limited number of training\nsamples. The Transformer encoder effectively learns to associate the dependence between objects in\nan environment, in our case simulated neurons, from their local action-observation history, which\ncould not be achieved through simple feed-forward networks.\nThe investigated architecture could be useful for generalisation in other domains with an underlying\ntopological structure, such as traffic light control [53, 54], or robotic control in a morphology-agnostic\nmanner, i.e., to generalise control to different robots without the need of morphological information\n[51]. Furthermore, it would be interesting to investigate its generalisation performance in the visual\ndomain. For example, the experiments on visual tasks in the original paper of Tang and Ha [1] could\nbe extended to evaluate performance in different contexts, rather than in a permuted version of the\nsame context. However, the quadratic complexity of self-attention and the need to encode a separate\nlocal action-observation histories require a lot of compute and memory. It would, thus, be beneficial\nfor scalability to develop more efficient architectures [e.g. 55].\nThe presented experiments were performed on simulations that are simplifications of actual neu-\roimaging data that we can currently acquire. Although these experiments tackled the fundamental\nproblem of partial observability of the context of the task, realistic features of the real-world setting\ncould be introduced to complicate control. For example, voltage images are inherently noisy, spiking\nbehaviour is naturally stochastic and governed by a multitude of synapses rather than a single connec-\ntion, and we can often not observe all neurons in the sample. Future extensions of the environment\ncould incorporate such aspects to challenge RL agents along the way to investigation of neuronal\ntissue through closed-loop control. Nonetheless, the introduced environment enables experimenta-\ntion in different RL research areas, such as multi-agent RL, causality, and meta-learning, besides\ngeneralisation in partially observable environments, with potential applications in neuroscience."}, {"title": "A Simulating neuronal networks", "content": "Neurons are simulated with the LIF model and short-term synaptic depression. The current simulation\nmethod is a simplification of [32]. At each time step t (of 1ms) the membrane potential $V_j$ of neuron\nj is updated by the following differential equation:\n$\\frac{dV_j}{dt} = \\frac{V_j - V_r}{RC} + \\frac{1}{C} \\left(a(j,t) + \\Sigma_i \\Sigma_k H(p_r U_k(t) - 0.5) I_{in}(i,t) \\right)$\nwhere $V_r$ indicates the resting potential, R the resistance, and C the capacitance of the cell membrane.\nIn the current setup, when an agent acts in the environment, it can induce external activity $a(j,t) =$\n$w_e \\delta(j-a)$ of input current $w_e$, when its action a corresponds to neuron j. The input current from\nother neurons i in the network to neuron j is defined by:\n$I_{in}(i,t) = w_{in} G_{i,j} d(t - t_i)$\nwith $w_{in}$ the weight (synaptic strength) of incoming internal input. $t_i$ contains the (last) spike time of\nneuron i, and the connectivity matrix G indicates whether there is an excitatory (1), inhibitory (-1),\nor no (0) connection between two neurons.\nH is the heavy-side step function and $p_r$ is a probability that influences the possibility of a pre-\nsynaptic synapse releasing its vesicles. Short-term synaptic depression is governed by $U_k$, which\nrelates to the availability of vesicles and neurotransmitters at pre-synaptic site k:\n$U_k(t) = 1 - e^{\\frac{t-t_k}{\\tau_R}}$\nwhere $t_k$ indicates the last time step at which vesicles were released at site k, and $\u03c4_R$ is the time\nconstant for the uptake of vesicles and neurotransmitters.\nWhenever a neuron's membrane potential reaches the threshold $V_{th}$, it emits a spike or action potential.\nDirectly after this spike, the membrane potential is reset to the resting potential and the neuron enters\na refractory period, $\u03c4_{rp}$, during which it is unable to spike (even from external input)."}, {"title": "B Procedural context generation", "content": "The procedural generation of contexts consists of three steps: network topology generation, target\nneuron assignment and target spiking pattern creation.\nNetwork topologies, in the current work, are defined by a connectivity (i.e. adjacency) matrix that\nindicates for all n neurons to which other neurons they are connected. Small-world networks are\ngenerated by giving neurons that are closer to each other a higher probability to form a connection,\nsimilar to experiments in [32]. Specifically, we make an abstraction in which neurons are placed on a\nline, where each pair of neurons i, j \u2208 {0, ...n} have a probability $P_{i,j}$ to form a connection from i to\nj. The distance between neuron i and j, $d(i, j)$, is defined by $|i \u2013 j|$.\n$P_{i,j} = \\begin{cases}0.3 & \\text{if } d(i,j) \\le 4, \\\\0.2 & \\text{if } 5 \\le d(i,j) \\le 8, \\\\0.1 & \\text{if } 9 < d(i,j) < 12, \\\\0.01 & \\text{otherwise}\\end{cases}$"}, {"title": "C Encoding observable context", "content": "The target neuron and target spiking pattern, $C_{neuron}$ and $C_{pattern}$, are observable for the agent. The\nparameterised Q-function can therefore additionally be conditioned on this information. A simple\nFF network is used to encode $C_{neuron}$. The target pattern, of length N, is encoded slightly more\nelaborate. The pattern is shifted in such a way that the element $(C_{neuron})_t$ at current time step t is in\nfront when encoding the pattern with a FF network. As we are constrained by a fixed input length in\na FF network, the input pattern is padded with -1. Thus, the input of the FF network at time step t is:\n$(C_{neuron})_{t:N-1}$ concatenated with $\\{{-1}\\}^t$. The encoded target neuron and target pattern are thereafter\nconcatenated to shape the encoding of the observable context."}, {"title": "D Experimental setup", "content": "The hyperparameters that are used in the experiments are listed in Table 2. We used double Q-learning\n[14], target networks with soft updates, epsilon-greey exploratino and uniform sampling from the\nreplay buffer. The hidden size in the different components of the architectures are chosen in such a\nway that there are no extreme differences in total number of parameters between the architectures."}]}