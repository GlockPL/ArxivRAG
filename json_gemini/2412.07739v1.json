{"title": "GASP: Gaussian Avatars with Synthetic Priors", "authors": ["Jack Saunders", "Charlie Hewitt", "Yanan Jian", "Marek Kowalski", "Tadas Baltrusaitis", "Yiye Chen", "Darren Cosker", "Virginia Estellers", "Nicholas Gyd\u00e9", "Vinay P. Namboodiri", "Benjamin E Lundell"], "abstract": "Gaussian Splatting has changed the game for real-time photo-realistic rendering. One of the most popular applications of Gaussian Splatting is to create animatable avatars, known as Gaussian Avatars. Recent works have pushed the boundaries of quality and rendering efficiency but suffer from two main limitations. Either they require expensive multi-camera rigs to produce avatars with free-view rendering, or they can be trained with a single camera but only rendered at high quality from this fixed viewpoint. An ideal model would be trained using a short monocular video or image from available hardware, such as a webcam, and rendered from any view. To this end, we propose GASP: Gaussian Avatars with Synthetic Priors. \u03a4\u03bf overcome the limitations of existing datasets, we exploit the pixel-perfect nature of synthetic data to train a Gaussian Avatar prior. By fitting this prior model to a single photo or video and fine-tuning it, we get a high-quality Gaussian Avatar, which supports 360\u00b0 rendering. Our prior is only required for fitting, not inference, enabling real-time application. Through our method, we obtain high-quality, animatable Avatars from limited data which can be animated and rendered at 70fps on commercial hardware. See our project page \u00b9 for results.", "sections": [{"title": "1. Introduction", "content": "Creating high-quality digital humans unlocks significant potential for many applications, including Virtual Reality, gaming, video conferencing, and entertainment. Digital humans must be photorealistic, easy to capture and capable of real-time rendering. The vision and graphics communities have long worked towards this goal, and we are rapidly approaching the point where such digital humans are possible.\nA series of works based first on NeRFs [25] raised the bar in creating exceptional visual quality [2, 3, 6, 9, 19, 47]. However, NeRFs remain slow to render and are unsuitable for real-time applications. Gaussian Splatting-based works have led to significant improvements in both quality and rendering speed [4, 8, 15, 28, 33, 40, 41]. Despite these improvements, the list of suitable applications for these methods is small. Each of these models suffers from one of two drawbacks: either they require expensive capture setups with multiple synchronized cameras, which prevents easy user enrollment [8, 28, 41], or they train on a single camera but exhibit significant quality degradation when rendered from views with more than a minimal variation in camera pose [4, 33, 40]. Furthermore, to maximize visual quality, some of these methods use a large CNN after rendering, which prevents real-time rendering without a powerful GPU [8, 41].\nFor mass adoption, an avatar model should achieve high-quality 360\u00b0 rendering in real-time and require only the amount of data a user can practically provide. In most cases a user can only capture a monocular, frontal image or video using their webcam or smartphone camera. The problem of fitting an avatar to this data is ill-posed; the extreme sides and back of the head are not visible, leading to artifacts in these unseen regions. In order to overcome this, we require a prior model that is able to \u201cfill in the gaps\" left by missing data. Such a model has been shown to be effective in other limited data human-centric models, such as visual dubbing [31] and static NeRF models [2]. Ideally, we would train such a model on a large, multi-view, perfectly annotated and diverse dataset. However, very few multi-view face datasets exist. Those that do either lack full coverage, particularly around the back of the head [19], or have only a small number of subjects [39]. Furthermore, annotations such as camera calibrations and 3D morphable model (3DMM) parameters associated with these datasets have to be estimated using imperfect methods and are a significant source of error.\nWe propose GASP: Gaussian Avatars with Synthetic Priors. We use a large, diverse dataset of synthetic humans [12, 38] to overcome the difficulties associated with training a prior on real data. This data is generated using computer graphics and has perfectly accurate annotations, including exact correspondence to the underlying 3DMM. This enables the large-scale training of a prior for Gaussian Avatars for the first time. However, the use of synthetic data introduces a domain gap problem. We address this by learning per-Gaussian features with semantic correlations. By learning these correlations on synthetic data and then maintaining them when fitting to real data, using a three-stage fitting process, we can cross this domain gap. Our method even enables rendering the back of the head, having fit to only a single front-facing image or video; see Fig. 1.\nTo summarize, we propose a novel system for creating realistic, real-time animatable avatars from a webcam or smartphone enabled by the following contributions:\n\u2022 A prior model over Gaussian Avatar parameters trained using purely synthetic data.\n\u2022 A three-stage fitting process, combined with learned per-Gaussian correlations to overcome the synthetic-to-real domain gap and allow for 360\u00b0 rendering.\n\u2022 Real-time rendering enabled through use of neural networks only during training and fitting, and not at inference time."}, {"title": "2. Related Work", "content": "A significant number of works have attempted to build photorealistic 3D Avatars that can be animated. Most of these works use an existing animatable model, known as a 3D morphable model (3DMM) [1, 20]. Earlier works improve the realism of a 3DMM in image space using compositing [35], a CNN model [17] or pixel-level MLPs [24]. Some work [32, 37] improve the CNN models by adding a learnable latent texture known as a neural texture [36] and evaluating this with a deferred neural renderer. Other works make use of volumetric rendering, either in the form of a point-based representation [46], or a NeRF [9, 26, 43, 47]. Each of these methods has shown great potential but is too slow or too prone to artifacts to provide a complete solution.\nGaussian Splatting [15] has allowed for unprecedented photorealism and real-time capabilities in volumetric rendering. Unsurprisingly, this technology has been adapted for applications in the photorealistic avatar space. We refer to this class of methods as Gaussian Avatars. Most Gaussian Avatar methods have built upon 3DMMs as a coarse representation of the geometry and Gaussian Splatting for finer geometry and appearance. Qian et al. [28] do this by binding Gaussians individual triangles in the mesh. Xiang et al. [40] initialize Gaussians by sampling from a UV map and moving the Gaussians by barycentric interpolation of the posed meshes. They add a dynamic MLP that learns to introduce wrinkles based on the 3DMM expression blend weights. Chen et al. [4] learn an extension to the LBS function of FLAME [20] by extending the blendshape basis to apply per Gaussian. Xu et al. [41] model deformations and dynamics with MLPs and apply a CNN-based super-resolution network. Giebenhain et al. [8] do similar but attach the Gaussians to an implicit geometry model using cycle consistency. These methods can produce photorealistic avatars with real-time rendering and impressive quality, but either train on a single camera and evaluate on the same camera or allow novel view synthesis but only within a small range and rely on multiple cameras. A similar method to our work is the recent Gaussian Morphable Model of Xu et al. [42], which can fit to a single input image. However, as most of the training data used is from the front half of the head, it cannot produce results in the back of the head and as a new task-specific expression basis is learned, control with an existing 3DMM is impossible."}, {"title": "2.2. Few-Shot Avatars", "content": "Several other works have attempted to address a similar problem to ours, in which the goal is to create a photorealistic avatar from limited amounts of data. In each case, the solution is to leverage some form of data-driven prior. Preface [3] uses a large-scale dataset to train an identity-"}, {"title": "3. Method", "content": null}, {"title": "3.1. Background: Gaussian Splatting", "content": "3D Gaussian Splatting is a method for reconstructing a volume from a set of images with corresponding camera calibrations. It involves using a collection of Gaussian primitives, represented by a position \u03bc in 3D space, an anisotropic covariance matrix \u2211, a color c and an opacity \u03b1. Kerbl et al. [15] proposed a system to optimize these parameters to fit the evidence provided by the images by decomposing the covariance \u2211 into the scale, \u03c3, and rotation, r, components, represented as a vector and quaternion respectively. Following projection by the camera and depth sorting, each pixel color P is computed as:\n\n$P = \\sum_{i=1}^{NG} C_i \\alpha_i \\prod_{j=1}^{i-1}(1 - \\alpha_j)$"}, {"title": "3.2. Background: Mesh Attached Gaussians", "content": "Gaussian Splatting is excellent at reconstructing static scenes but, in its basic form, cannot model animation dynamics. Multiple works [28, 33] make the observation that, given a sufficiently good coarse approximation of geometry in the form of a mesh, the problem can be reduced to an approximately static scene. By attaching each Gaussian, Gi, to a specific triangle, t, in the mesh, the Gaussian is assumed to remain static relative to that triangle's pose. There are several successful formulations of this posing transformation:\n\n$\\mu, \\sigma, r = T_{local\\rightarrow global}(\\mu', \\sigma', r' | t)$"}, {"title": "3.3. Prior Model Training", "content": "We train our prior model as a generative model over identities. Following previous work [7, 27, 29, 42], we train this prior as an auto-decoder model. We jointly learn a per-subject identity code, zj \u2208 R512, j \u2208 {1, ..., Nid}, and an MLP decoder, D(z). One may naively think of training this model to directly output the Gaussian Attributes, A, with a single MLP. However, such a method quickly becomes intractable. As a typical model with 100,000 Gaussians may have millions of attributes, the number of parameters in D would be too large. Instead, we augment each Gaussian with a learnable feature vector, fi \u2208 R8, i \u2208 {1, ..., NG}. This feature is analogous to a positional encoding with additional semantic meaning. We then train a network to map these per-Gaussian features to Gaussian attributes, with each Gaussian processed independently and in parallel.\nTo make optimization more stable, we learn a Canonical Gaussian Template, C, and model the per-person variation as offsets from this template. The Canonical Template can be considered the mean Avatar. The i-th Gaussian of the avatar for the subject j is given by:\n\n$A_{i,j} = C_{i,j} + D(f_i, z_j)$\n\nThis is best understood by following Fig. 3. To train this model, we jointly optimize C, D, {zj}1<j<Nid, {fi}1<i<NG to minimize the following loss function:\n\n$\\mathcal{L} = \\lambda_{pix} \\mathcal{L}_{pix} + \\lambda_{\\alpha} \\mathcal{L}_{\\alpha} + \\lambda_{percep} \\mathcal{L}_{percep} + \\mathcal{L}_{reg}$\n\nWhere Lpix is a pixel level loss consisting of L1, the l\u2081 difference between the real and predicted images, and LSSIM which is the differentiable SSIM loss, weighted by A\u2081 and Assim respectively. Lpercep is a perceptual loss based on LPIPS [45], La is the l\u2081 distance between the real and predicted alpha masks, and Lreg is a regularization loss acting on the Gaussians. We regularize scale and displacement:\n\n$\\mathcal{L}_{reg} = \\lambda_{\\sigma} ||max(0.6, \\sigma')||_2 + \\lambda_{\\mu} ||\\mu'||_2$\n\nUnlike previous methods, our 3DMM does not capture course hair, meaning the Gaussians must model it. We, therefore, reduce \u03bb\u03bc by a factor of 100 for Gaussians bound to faces in the scalp region, which we manually define."}, {"title": "3.4. Initialization", "content": "Using just one Gaussian per triangle face of the 3DMM leads to an under-parameterised model that lacks sufficient detail. To overcome this, we use the initialization strategy of Xiang et al. [40]. We generate a UV map of a given resolution for our mesh and take each pixel's corresponding face and barycentric coordinates. The face is used for Gaussian binding. We use the barycentric coordinates to position the origin of each Gaussian's local coordinate system."}, {"title": "3.5. Fitting Process", "content": "Given input data ranging from a single image, to a short video from a single monocular camera, we aim to produce a high-quality avatar that can be viewed from any angle. We have three stages to this fitting process, visualized in Fig. 3:\n1. We find the best in-prior Gaussian Avatar by randomly initializing an identity latent vector, z, and optimizing this with everything else frozen; we call this inversion.\n2. We fine-tune the MLP, D, with the rest of the model frozen.\n3. We refine the resulting Gaussians using the standard Gaussian Splatting optimization procedure [15] to best fit the data.\nTo motivate this three-step process, we can consider two extremes. On the one hand, we could perform inversion only. This relies heavily on the prior. If we had perfectly diverse real-human data and a perfect prior, this may be all we would need to do. However, our prior training was on synthetic data, so we could only generate synthetic-looking avatars with this method. On the other hand, we could use the prior for initialization and then optimize the resulting Gaussians. This would achieve similar results to the existing state-of-the-art but with the unseen regions looking synthetic.\nWe can extract more value from our prior model by considering correlations in the per-Gaussian features, f. Our network is forced to map these to Gaussian attributes and learns to associate similar Gaussians with similar features. By freezing the features in the fitting process, Gaussians with similar semantic features will be mapped to have similar attributes. For example, if D learns to make a Gaussian representing hair at the front of the head blonde, it will also update an unseen one at the back of the head.\nTo prevent stages 2 and 3 from diverging too far from the"}, {"title": "6. Results", "content": "We conduct all of our evaluations on the NeRSemble Dataset [19]. This dataset contains multiple subjects performing dozens of facial expression sequences, including one freeform sequence, across 16 cameras. For each sequence, we preprocess each video using an off-the-shelf background removal [21] and face segmentation tool [44] to get the head region only. We obtain Morphable Model parameters in the format of Wood et al. [38] using the method of Hewitt et al. [12]. We consider three experimental settings using this data; please refer to the supplementary material for the cameras and sequences used:\nMonocular: To best replicate our desired setting, we enroll all avatars using a single frontal camera. We use a subset of the expression sequences for fitting and evaluate them using the unseen freeform sequence. We use the four most extreme view cameras for evaluation, as determined by manual inspection, to test the model's ability to produce good results on regions unseen at training.\nMulti-Camera: To confirm that our model does not sacrifice performance when more data is available, we also enroll avatars using the same configuration above but with all cameras used for input.\nSingle Image: To test the limits of our model, we also"}, {"title": "6.1. Baselines", "content": "We compare our model to state-of-the-art methods. For the first set of methods, we look at Gaussian Avatar models: Gaussian Avatars [28], which is designed for ultra-high quality rendering when trained on multiple views, and Flash Avatars [40], which is designed to be trained and evaluated on monocular data. We train these using the same morphable model, 3DMM fitting process and dataset preprocessing as our method. In addition to Gaussian Avatar models, we look at models designed for few-shot animatable avatar synthesis. We select the publicly available implementations of ROME [16] and DiffusionRig [5]."}, {"title": "6.2. Monocular Training", "content": "The results of this experiment can be found in Table 1. Our model significantly outperforms state-of-the-art across all metrics, including user-perceived quality. Our model produces significantly fewer artifacts in novel views compared to other Gaussian Avatar methods [28, 40]. This is because our prior helps prevent the model from overfitting to the training camera view. Diffusion Rig [5] does not show any visible artifacts, but struggles to preserve the identity of the subject; this is best seen in Fig. 7."}, {"title": "6.3. Multi-Camera Training", "content": "Our model is competitive with the state-of-the-art in the Multi-Camera setting (Tab. 2). We expect our model to perform worse than other Gaussian Avatar methods [28, 40] as the prior regularizes the model towards a synthetic solution, and we do not model lighting or dynamic expressions. Despite this, our model performs similarly to the state-of-the-art, suggesting it can effectively use all available data. Furthermore, using the prior allows our model to converge in fewer steps than other Gaussian Avatar models, making it cheaper and more efficient to train. Our model performs better on all metrics compared to Diffusion Rig [5]."}, {"title": "6.4. Single Image Training", "content": "The results of the single image setting are shown in Tab. 1. With such limited data, other Gaussian Avatar methods overfit and perform poorly. Even on the same camera view as the input image, Gaussian Avatar methods struggle with artefacts; see Fig. 9. Our method also outperforms ROME [16], which is designed to work with a single image."}, {"title": "6.5. Ablations", "content": "We perform an ablation study to demonstrate our model's effectiveness. The results are shown in Tab. 3. We use three subjects in the monocular setting. More details, as well as additional qualitative results, are in the supplementary.\nNo Prior: To validate the use of the prior, we fit person-specific models using our MLP without any prior. We also ablate the use of the prior regularization loss term. It can be seen that the absence of the prior dramatically reduces the quality according to all metrics, while not regularizing towards the prior leads to slightly better ID reconstruction but worse quality according to all other metrics.\nNumber of Subjects: We compare the model quality using priors trained on differing numbers of subjects. The more subjects we have, the better the quality. Interestingly, using one subject in the prior performs worse than not using a prior. This contrasts with the findings of Buehler et al. [2].\nNumber of Gaussians: We consider the effect of initializing with fewer Gaussians. We use texture maps ranging from 64 \u00d7 64 up to the full 512 \u00d7 512. We see that the"}, {"title": "6.6. Runtime", "content": "After fitting a user's Avatar using the prior, we can generate the mesh attached Gaussian Avatar parameters A. Combined with the triangle face bindings and barycentric coordinates, this fully specifies an Avatar. No neural networks, including D, are required for inference. A user's Avatar can be stored as an approximately 15MB file. Without any runtime optimizations, the complete inference pass, from Morphable Model parameters to the final rendered image runs at 70fps on an NVIDIA 4090 RTX GPU. The posing"}, {"title": "7. Limitations and Future Work", "content": "While our model is able to achieve high-quality 360\u00b0 rendering, it has some limitations. For some regions, such as the back of the head, the model produces synthetic-looking results. We would like to address this issue by looking into 2D image-based priors [22, 23] based on diffusion models [30]. To reduce artefacts introduced by overfitting to the monocular view, we used only flat RGB colour and did not model lighting, reducing our model's realism. In future, we may include a lighting model in our prior, enabled by a diverse set of lighting conditions in our synthetic data. As can be seen in our supplementary, our prior serves as a generative model with good interpretability. Given sufficient resources and a good camera/morphable model registration pipeline, we would like to use the findings of this work to train a similar generative prior using real data."}, {"title": "8. Conclusion", "content": "We have presented GASP, a novel method enabling 360\u00b0, high-quality Avatar synthesis from limited data. Our model builds a prior over Gaussian Avatar parameters to \u201cfill in\" missing regions. To bypass issues associated with collecting a large-scale real dataset, such as the need for full coverage and perfect annotation, we use synthetic data. Learned semantic Gaussian features and a three-stage fitting process enable us to cross the domain gap, while fitting to real data, to create realistic avatars. Our model outperforms the state-of-the-art in novel view and expression synthesis with Avatars trained from a single camera (e.g., a webcam or phone camera) using a short video or a single image while retaining the ability to animate and render in real-time."}]}