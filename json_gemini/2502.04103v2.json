{"title": "VTutor: An Open-Source SDK for Generative Al-Powered Animated Pedagogical Agents with Multi-Media Output", "authors": ["EASON CHEN", "CHENYU LIN", "XINYI TANG", "APRILLE XI", "CANWEN WANG", "JIONGHAO LIN", "KENNETH R KOEDINGER"], "abstract": "The rapid evolution of large language models (LLMs) has transformed human-computer interaction (HCI), but the interaction with LLMs is currently mainly focused on text-based interactions, while other multi-model approaches remain under-explored. This paper introduces VTutor, an open-source Software Development Kit (SDK) that combines generative Al with advanced animation technologies to create engaging, adaptable, and realistic APAs for human-AI multi-media interactions. VTutor leverages LLMs for real-time personalized feedback, advanced lip synchronization for natural speech alignment, and WebGL rendering for seamless web integration. Supporting various 2D and 3D character models, VTutor enables researchers and developers to design emotionally resonant, contextually adaptive learning agents. This toolkit enhances learner engagement, feedback receptivity, and human-AI interaction while promoting trustworthy Al principles in education. VTutor sets a new standard for next-generation APAs, offering an accessible, scalable solution for fostering meaningful and immersive human-AI interaction experiences. The VTutor project is open-sourced and welcomes community-driven contributions and showcases.", "sections": [{"title": "1 Introduction", "content": "The advent of large language models (LLMs) [1] has greatly changed the research direction of Human-Computer Interaction (HCI) [17] by enabling systems to generate coherent text, comprehend natural language, and adapt to a variety of contexts. These capabilities have paved the way for novel applications, from conversational agents [2] to task assistants [16] and tutoring systems [22]. However, current implementations of LLM-driven systems are predominantly text-based, which limits the potential for creating more engaging and human-like interactions [17]. While text-based interfaces are functional, they sometimes fall short in scenarios like giving feedback and learning companions scenarios [13].\nAnimated Pedagogical Agents (APAs) offer a compelling avenue to enhance and humanize these text-centric systems [13]. Instead of engaging in purely text-based chat interactions in a chatbot interface or experiencing voice-only input and output akin to a phone call, APAs provide an experience akin to video chatting with a real person. APAs are on-screen characters that guide, tutor, and motivate learners. APAs demonstrated significant promise in educational contexts [10]. By incorporating visual, vocal, and gestural cues, these agents can support adaptive or scripted roles [4] and embody diverse appearances, from simplistic 2D avatars to intricate 3D humanoid figures capable of rich facial and body animations [5, 6, 14, 20]. When well-designed, APAs can not only lower cognitive load but also provide emotional scaffolding, thereby improving learner motivation and retention [15]. However, current APA solutions are often constrained by limited voice realism, poor lip synchronization, and reliance on pre-scripted dialogue, hindering their ability to deliver dynamic, context-specific guidance at scale.\nIn this work, we bridge these gaps by introducing VTutor: an open-source Software Development Kit (SDK) that fuses generative AI techniques with cutting-edge animation technologies to elevate the realism, interactivity, and adaptability of APAs. Leveraging advanced lip synchronization engines [8] and real-time WebGL rendering, VTutor enables seamless integration of naturalistic animated agents within web platforms. Crucially, it harnesses the adaptability of LLMs to timely and personalized dialogue for improved learner engagement. Through the use of VTutor, we aim to contribute to the evolving landscape of human-AI interactions by exploring how they can move beyond static text-based interfaces. VTutor combines generative Al with multi-media output, including synchronized text, speech, facial expressions, and animations, to deliver an engaging and immersive experience. By enabling more responsive and emotionally engaging interactions, we hope to support users in a variety of contexts, fostering guidance, motivation, and connection in a thoughtful and adaptive manner."}, {"title": "2 Related Works", "content": "Animated Pedagogical Agents (APAs) are virtual characters designed to facilitate learning by providing guidance, feedback, and motivation to users. Initially conceptualized as tools to make learning more engaging, APAs have evolved significantly over the years. Their design is grounded in established theoretical frameworks, such as the persona effect, social agency theory, and embodied cognition theory, which emphasizes the importance of human-like interactions in fostering emotional connections, enhancing cognitive engagement, and improving knowledge retention [11, 20, 21].\nAPAs have demonstrated their value across diverse educational domains, including electronics education [7] and language learning [18]. Studies have shown that the use of APAs increases learner engagement and facilitates knowledge transfer by integrating multimodal cues, such as gestures, facial expressions, and speech, into the learning experience [6, 14]. By acting as social partners, APAs create interactive environments that encourage active learning and reduce cognitive load [12, 15]."}, {"title": "3 System Implementations", "content": "VTutor seamlessly integrates text, voice, and animation into a cohesive multi-media output framework, enhancing the quality of learner interaction. To use VTutor, developers need to prepare audio by TTS in a .wav format to integrate with VTutor. Specifically, they can use TTS services such as OpenAI, Azure, Google Cloud Platform, and others to generate voice outputs for interaction with VTutor. Additionally, users can prepare real-time audio streams to enable more fine-grained and responsive interactions. In our demo website, we currently use Azure's speech engine as an example."}, {"title": "3.2 Lip Synchronization (LipSync)", "content": "For the lip synchronization (LipSync) component of VTutor, we leverage uLipSync [8], an open-source tool designed for real-time and preprocessed lip-syncing in Unity. uLipSync works by analyzing audio waveforms in real time using Mel-Frequency Cepstrum Coefficients (MFCC) to extract human voice characteristics. These coefficients represent phonemes (e.g., \"a,\" \"e\", \"i,\" \"o,\" \"u\") by capturing vocal tract features. The tool then maps these phonemes to pre-configured blend shapes in a 3D model's SkinnedMeshRenderer, dynamically adjusting the model's mouth movements to match the audio input. This enables accurate synchronization between speech and visual expressions, significantly enhancing the realism and immersion of the animated pedagogical agents.\nBy integrating uLipSync into our framework, we allow developers to input either live microphone audio or pre-recorded audio clips to drive lip synchronization. The tool also supports pre-baked lip-sync data for high-performance scenarios, such as browser-based WebGL deployments. Moreover, VTutor uses customizable profiles"}, {"title": "3.3 VTutor Character Model Selection", "content": "VTutor incorporates both 2D and 3D character models developed within Unity, ensuring seamless compatibility with TTS and LipSync. This design choice underscores the framework's versatility, enabling users to customize their APAs by importing their preferred character models, including anime-style avatars. By supporting both 2D and 3D assets, VTutor provides unparalleled flexibility for creating tailored and engaging educational agents. The TTS and lip-sync features are designed to be universally adaptable, allowing users to effortlessly integrate models sourced from popular online repositories and 3D model stores 1. Thanks to Unity's standardized skeletal system, pre-defined gestures can be applied seamlessly across various character models, reducing the need for extensive adjustments and streamlining the customization process. That is, developers can easily import a preferred character model and compile a new VTutor instance to use it with the SDK on their website.\nTo showcase the framework's interoperability, we utilized two prominent open-source modeling platforms: Live2D for 2D character representation and VRoid Studio for 3D avatars. Both platforms offer models with foundational lip animation capabilities, which VTutor dynamically synchronizes with generated speech via its lip-sync technology. This ensures that APAs deliver content in a visually realistic and engaging manner, enhancing the learner experience.\nThis robust adaptability positions VTutor as a practical and user-friendly solution for educators and developers, expanding its utility across a diverse range of educational and creative applications. By simplifying the integration and customization process, VTutor empowers users to create personalized animated agents that elevate the quality and interactivity of learning environments."}, {"title": "3.4 API Communication", "content": "The primary application scenario for VTutor is in the web interface. To facilitate communication between the web interface and VTutor running in Unity WebGL, we leverage the instance.SendMessage method to transmit audio files in the WAV format. Once VTutor receives the WAV audio input, it processes the file using uLipSync and synchronizes the APA body and lip animations with the audio playback in real time."}, {"title": "3.5 Software Development Kit (SDK) Implementations", "content": "To enable seamless integration of VTutor into existing websites, we encapsulated the entire VTutor system, including its unity instance, web interface, and communication commands, within an iframe. This approach allows any website to embed VTutor quickly with minimal effort. As demonstrated in our example codes 2, VTutor's iframe-based integration simplifies deployment with just a few line of codes.\nFurthermore, we are developing example codes and React SDK to facilitate smooth integration into React-based websites. This SDK provides developers with a streamlined workflow, enabling them to embed VTutor's features effortlessly while maintaining compatibility with modern web development practices. The React frontend example of VTutor is also open-sourced at https://github.com/VTutorTools/vtutor-vercel-app."}, {"title": "4 Implications", "content": "The VTutor project introduces a powerful framework that combines generative AI and advanced animation technologies to create emotionally engaging and highly adaptable Animated Pedagogical Agents (APAs). This innovation has significant implications for enhancing human-AI interaction and educational technologies, as outlined below."}, {"title": "4.1 Advancing Animated Pedagogical Agents", "content": "VTutor redefines the role of APAs by addressing key limitations in existing research and development. By lever-aging open-source, large language models (LLMs), real-time text-to-speech synthesis, precise lip synchronization, VTutor enables:\n\u2022 Personalized Learning: Adaptive, context-specific feedback tailored to individual learners, improving engagement and knowledge retention.\n\u2022 Immersive Interactions: Emotionally expressive agents with lifelike gestures, speech, and facial expres-sions create a strong sense of presence and relatability.\n\u2022 Accessible Development: A developer-friendly SDK that allows seamless integration of APAs into web platforms, reducing the barrier to APAs adoption and research.\nThese contributions make VTutor a scalable and impactful solution for advancing APA technologies across domains."}, {"title": "4.2 Potential Applications", "content": "The versatility of VTutor's design opens up opportunities for impactful applications across various fields:\n\u2022 Education: Interactive tutoring systems that adapt to learners' progress, providing tailored guidance in STEM, language learning, and other domains.\n\u2022 Corporate Training: Employee training programs that use personalized agents to deliver real-time feedback and improve knowledge retention.\n\u2022 Healthcare: Virtual assistants for mental health support or patient education, offering empathetic, context-aware interactions.\n\u2022 Entertainment: Engaging storytelling agents or interactive gaming characters that respond dynamically to player inputs.\nBy enabling these applications, VTutor demonstrates its potential to reshape how humans interact with Al in educational, professional, and recreational settings."}, {"title": "5 Conclusion", "content": "The VTutor project presents a groundbreaking framework that combines the capabilities of generative AI with advanced animation technologies to redefine the potential of Animated Pedagogical Agents (APAs) in education. By addressing the limitations of traditional APAs, such as scripted dialogues, limited adaptability, and lack of realism, VTutor introduces a versatile and scalable Software Development Kit (SDK) that enables developers and educators to create personalized, interactive, and emotionally engaging learning experiences. Through its integration of large language models (LLMs), cutting-edge lip synchronization engines, and seamless WebGL rendering, VTutor offers a powerful toolkit for building human-like pedagogical agents that foster learner engagement, enhance feedback receptivity, and establish trustworthy AI interactions.\nCrucially, VTutor is an open-source project, emphasizing accessibility, transparency, and community-driven innovation. We welcome contributions from developers, educators, and researchers around the world to further improve the SDK. Whether it involves integrating new character models, enhancing system features, developing additional examples, or suggesting innovative improvements, we invite pull requests and collaborative efforts. By fostering an open and inclusive community, VTutor aims to accelerate the adoption and advancement of generative AI-powered APAs, ultimately shaping the future of educational technology.\nThe source code, examples, and documentation for VTutor are available at https://github.com/VTutorTools, and we encourage the broader community to join us in refining and expanding this transformative toolkit with feedback, pull requests, and example showcases. Together, we can unlock the full potential of human-AI interaction, paving the way for a more engaging, inclusive, and effective interaction experience."}]}