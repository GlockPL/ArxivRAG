{"title": "Complex Logical Query Answering by Calibrating Knowledge Graph\nCompletion Models", "authors": ["Changyi Xiao", "Yixin Cao"], "abstract": "Complex logical query answering (CLQA) is\na challenging task that involves finding answer\nentities for complex logical queries over in-\ncomplete knowledge graphs (KGs). Previous\nresearch has explored the use of pre-trained\nknowledge graph completion (KGC) models,\nwhich can predict the missing facts in KGs,\nto answer complex logical queries. However,\nKGC models are typically evaluated using rank-\ning evaluation metrics, which may result in val-\nues of predictions of KGC models that are not\nwell-calibrated. In this paper, we propose a\nmethod for calibrating KGC models, namely\nCKGC, which enables KGC models to adapt\nto answering complex logical queries. Notably,\nCKGC is lightweight and effective. The adap-\ntation function is simple, allowing the model to\nquickly converge during the adaptation process.\nThe core concept of CKGC is to map the val-\nues of predictions of KGC models to the range\n[0, 1], ensuring that values associated with true\nfacts are close to 1, while values linked to false\nfacts are close to 0. Through experiments on\nthree benchmark datasets, we demonstrate that\nour proposed calibration method can signifi-\ncantly boost model performance in the CLQA\ntask. Moreover, our approach can enhance\nthe performance of CLQA while preserving\nthe ranking evaluation metrics of KGC mod-\nels. The code is available at https://github.\ncom/changyi7231/CKGC.", "sections": [{"title": "1 Introduction", "content": "Knowledge graphs (KGs) are composed of struc-\ntured representations of facts in the form of triplets\nand have been widely used in various domains. One\nof the key tasks associated with KGs is complex\nlogical query answering (Ren et al., 2023). Com-\nplex logical queries are typically expressed using\nfirst-order logic (FOL), which encompasses logical\noperations such as conjunction (\u0245), disjunction (V),\nnegation (\u00ac), and existential quantifier (\u2203). For ex-\nample, the query \"Which universities do the Turing\nAward winners not in the field of deep learning\nwork in?\" can be formulated as a FOL query, as\nillustrated in Figure 1.\nMany well-known knowledge graphs (KGs) suf-\nfer from incompleteness, rendering it challenging\nto answer complex queries through simple KG\ntraversal. Building on the accomplishments of\nknowledge graph completion (KGC) methods (Bor-\ndes et al., 2013; Sun et al., 2018; Trouillon et al.,\n2017) in addressing one-hop KG queries, a research\navenue has emerged focusing on learning embed-\ndings for queries to handle complex logical queries\n(Hamilton et al., 2018; Ren et al., 2020; Zhang\net al., 2021). Nonetheless, these methodologies\noften require extensive training on numerous com-\nplex logical queries, leading to substantial training\ntime overhead and limited generalization to out-of-\ndistribution query structures.\nIn addressing these challenges, CQD (Arakelyan\net al., 2020) introduces a method for CLQA by\nleveraging one-hop atom results derived from a pre-\ntrained KGC model, thereby removing the neces-\nsity for training on complex queries. CQD frames\nCLQA as an optimization problem and employs\ntechniques like beam search or continuous approx-\nimation to estimate the optimal solution. Despite\nits effectiveness, the approximations made during\nthe process could lead to a decrease in accuracy\nfor CQD. Furthermore, CQD is reliant on a KGC\nmodel whose values of output might not be specifi-\ncally calibrated for CLQA, potentially resulting in\ninaccuracies in the outcomes.\nIn this paper, we introduce a method for calibrat-\ning KGC models, namely CKGC, which can make\nKGC models adapt to handling complex logical\nqueries. Our method is lightweight and effective. It\nis lightweight, as the adaptation function is simple\nand the model can quickly converge in adaptation\nprocess. Moreover, it is effective in significantly"}, {"title": "2 Related Work", "content": "Knowledge Graph Completion The task of\nKGC involves predicting missing triplets within a\nknowledge graph, which can be viewed as predict-\ning answers for one-hop queries. Various method-\nologies have been proposed to address this task, en-\ncompassing embedding techniques (Bordes et al.,\n2013; Sun et al., 2018; Trouillon et al., 2017), rein-\nforcement learning approaches (Xiong et al., 2017;\nDas et al., 2018; Hildebrandt et al., 2020; Zhang\net al., 2022), rule learning strategies (Yang et al.,\n2017; Sadeghian et al., 2019; Qu et al., 2020), and\ngraph neural network methodologies (Schlichtkrull\net al., 2018; Vashishth et al., 2019; Teru et al.,\n2020). Embedding methods aim to embed enti-\nties and relations into a continuous space and de-\nfine a scoring function based on these embeddings.\nReinforcement learning methods train an agent to\nexplore the knowledge graph to predict the miss-\ning triplets. Rule learning methods adopt a distinct\napproach by initially identifying confident logical\nrules from the knowledge graph. These rules are\nthen utilized to infer missing triplets. Lastly, graph\nneural network methods utilize graph neural net-\nworks to learn representations of entities and rela-\ntions by leveraging the graph structure.\nComplex Logical Query Answering CLQA\nover KGs extends KGC to predict answers for FOL\nqueries, which additionally requires defining re-\nlationships between sets of entities. Embedding-\nbased methods represent sets of entities as geo-\nmetric objects (Hamilton et al., 2018; Ren et al.,\n2020; Zhang et al., 2021) or probability distribu-\ntions (Ren and Leskovec, 2020), and then minimize\nthe distance between embeddings of queries and\nembeddings of their corresponding answers. How-\never, the quality of representation of sets may be\ncompromised when dealing with large sets. To\novercome this limitation, some studies have incor-\nporated powerful fuzzy set theory to handle FOL\nqueries (Chen et al., 2022; Zhu et al., 2022). For in-\nstance, FuzzQE (Chen et al., 2022) embeds entities\nand queries into a fuzzy space and leverages fuzzy\nset operations to perform logical operations on the\nembeddings. Similarly, GNN-QE (Zhu et al., 2022)\ndecomposes the query into relational projections\noperations and fuzzy set operations over fuzzy sets,\nand subsequently learns a graph neural network to\nexecute relational projections.\nNevertheless, the aforementioned methods gen-\nerally necessitate training on numerous complex\nlogical queries, resulting in significant training\ntime overhead and limited generalization to out-of-ndistribution query structures. Another line of\nmethods first pre-trains a KGC model and then\nintegrates it with fuzzy set theory to infer an-\nswers. CQD (Arakelyan et al., 2020) formulates\nCLQA as an optimization problem and proposes\ntwo strategies to approximate the optimal solution:\nCQD-CO, which directly optimizes in the contin-\nuous space, and CQD-Beam, which utilizes beam\nsearch. Despite CQD avoiding the need to train\non complex logical queries, it suffers from accu-\nracy loss due to the approximated optimization\nand uncalibrated KGC models. To enhance ac-\ncuracy, Bai et al. (2023) introduce QTO, which\nefficiently finds the theoretically optimal solution"}, {"title": "3 Background", "content": "In this section, we introduce the related background\nof our method, complex logical query answering\non knowledge graphs and fuzzy sets."}, {"title": "3.1 Complex Logical Query Answering on\nKnowledge Graphs", "content": "Knowledge Graphs Completion Given a set of\nentities V and a set of relations R, a knowledge\ngraph G contains a set of triplets {(h,r,t)} \u2286\nV \u00d7 R \u00d7 V, where each triplet is a fact from head\nentity h to tail entity t with a relation type r. KGC\nmodels define scoring functions f(h, r, t) to mea-\nsure the likelihood of triplets (h, r, t) based on their\ncorresponding embeddings.\nFirst-Order Logic Queries Complex logical\nqueries can be represented by FOL queries with\nlogical operations including conjunction (\u2227), dis-\njunction (V), negation (\u00ac), and existential quanti-\nfier (\u2203). A first-order logic query q can be described\nin its disjunctive normal form, which consists of\na set of non-variable anchor entities Va \u2286 V, ex-\nistentially quantified bound variables V\u2081, ..., Vk\nand a single target variable V?, which provides the\nanswers of query q. The disjunctive normal form\nof a logical query q is a disjunction of one or more\nconjunctions.\n$q = \\bigvee_j \\exists V_1, ..., V_k : C_1 \\land C_2 \\land,\u2026\u2026\u2026, \\land C_n$.\nEach c represents a conjunctive query with one or\nmore literals e, i.e., $C_i = e_{i1} \\lor e_{i2} \\lor ... \\lor e_{im}$.\nEach literal e represents an atomic formula or its\nnegation, i.e., $l_{ij} = R(v_a, V)$ or $\u00acR(v_a, V)$ or\n$\u00acR(V', V)$ or $\u00acR(V', V)$, where $v_a \\in V_a$, $V \\in$\n{$V_?, V_1, . . ., V_k$}, $V' \\in$ {$V_?, V_1, . . ., V_k$}, $V \\neq V'$,\n$R(\\cdot,\\cdot)$ is a binary function $R : V \u00d7 \u03bd \u2192 {0, 1}$.\nEach relation r \u2208 R corresponds to a binary func-\ntion R(,). If (h, r, t)is a true fact, then R(h, t) =\n1. If (h, r, t)is a false fact, then R(h, t) = 0."}, {"title": "Computation Graph", "content": "Given a FOL query, we\ncan represent it as a computation graph, of which\nnodes represent sets of entities and edges repre-\nsent set operations over sets of entities. The set\noperations include the complement operation, inter-\nsection operation, union operation and projection\noperation. The root node represent the set of an-\nswer entities. See Figure 1 for an example. We\nmap logical operations to set operations according\nto the following rules.\n\u2022 Negation \u2192 Complement Operation:\nGiven a set of entities S \u2286 V, the comple-\nment operator performs set complement to\nobtain $\\overline{S} = V\\backslash S$.\n\u2022 Conjunction\u2192 Intersection Operation:\nGiven n sets of entities {$S_1, S_2, ... S_n$}, the\nintersection operator performs set intersection\nto obtain $\\bigcap_{i=1}^{n}S_i$.\n\u2022 Disjunction\u2192 Union Operation: Given\nn sets of entities {$S_1, S_2, . . . S_n$}, the union\noperator performs set union to obtain $\\bigcup_{i=1}^{n}S_i$.\n\u2022 Relation Projection \u2192 Projection Opera-\ntion: Given a set of entities S \u2286 V and a re-\nlation r \u2208 R, the projection operator outputs\nall the adjacent entities $\\bigcup_{v\\in S}N(v,r)$, where\nN(v, r) is the set of entities such that (v, r, v')\nare true triplets for all $v' \\in N(v,r)$.\nIn order to answer a given FOL query, we can\ntraverse the computation graph and execute the set\noperations. The answers of a query can be obtained\nby looking at the set of entities in the root node."}, {"title": "3.2 Fuzzy sets", "content": "Definition A fuzzy set is a pair (U, m), where U\nis a set and m : U \u2192 [0, 1] is a membership func-\ntion. For each x \u2208 U, the value m(x) measures the\ndegree of membership of x in (U, m). The function\nm = \u00b5A is called the membership function of the\nfuzzy set A = (U, m). Classical sets are be seen\nas special cases of fuzzy sets, if the membership\nfunctions only takes values 0 or 1.\nFuzzy Set Operations Fuzzy set operations are a\ngeneralization of classical set operations for fuzzy\nsets. The three primary fuzzy set operations are\nfuzzy complements, fuzzy intersections, and fuzzy\nunions. For a given fuzzy set A, its complement A\nis commonly defined by the following membership"}, {"title": "4 Method", "content": "We first define four fuzzy set operations over fuzzy\nsets in Section 4.1, and then propose a calibration\nmethod for KGC models in Section 4.2."}, {"title": "4.1 Fuzzy Set Operations", "content": "As demonstrated in Section 3.1, a query can be\nrepresented as a computation graph. To obtain\nthe fuzzy sets of answers, it is necessary to define\nthe fuzzy set operations utilized in computation\ngraphs. These operations encompass the projection\noperation, intersection operation, union operation,\nand complement operation.\nWe represent every fuzzy set of entities as a vec-\ntor e \u2208 [0, 1]d, where d = |V| and ei denotes the\ngrade of membership of entity i. The anchor entity\nis represented by a vector with a single element set\nto 1 and all other elements set to 0."}, {"title": "4.2 Calibration", "content": "Upon establishing the four fuzzy set operations,\nthe sole requirement for computing the results of\nqueries is a calibrated KG tensor X. The entries\nof a calibrated KG tensor X are expected to fall\nwithin the range of 0 to 1, with values associated\nwith true triplets approaching 1, and those linked to\nfalse triplets nearing 0. In the event of possessing\na fully calibrated KG tensor X, accurate answers\nof queries can be attained through the utilization of\nthe four fuzzy set operations.\nThe KG tensor X is furnished by a KGC model,\nwhich defines a scoring function f (h, r, t) to mea-\nsure the likelihood of a triplet (h, r, t). The ranking\nmetrics, MRR and H@N (Bordes et al., 2013), are\ncommonly used to evaluate KGC models. For ex-\nample, the definition of MRR is as follows:\n$MRR = \\frac{1}{|G|} \\sum_{(h,r,t)\\in G} \\frac{1}{rank(h,r,t)}$\nwhere G is a dataset and rank(h, r, t) is the rank\nof tail entity t in the predicted list for the query\n(h,r,?). rank(h,r,t) is computed based on the\nscoring function f(h,r,t). The ranking metrics\nmainly focus on ranking rather than the specific\nnumerical value of f(h, r, t), which is important"}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experimental Settings", "content": "Datasets We evaluate our method on three pop-\nular knowledge graph datasets, including FB15k\n(Bordes et al., 2013), FB15k-237 (Toutanova\net al., 2015), NELL995 (Xiong et al., 2017). We\nuse the standard FOL queries generated in Be-\ntaE (Ren and Leskovec, 2020), consisting of 9"}, {"title": "5.2 Results", "content": "See Table 1 for the results. avg, is the average on\nexistential positive first-order queries. avgn is the\naverage on queries with negation. GQE, Q2B, CK-\nGCO, and CQD-Beam do not support queries with\nnegation, so the corresponding entries are empty.\nWe observe that our model significantly outper-\nforms baseline methods across all datasets. Our\nmodel yields a relative gain of 6.8% and 52.3%\non avgp and avgn compared to previous state-of-\nthe-art model QTO. This shows that our method\nhas better reasoning skills and superior adaptabil-\nity when tackling complex query answering tasks.\nWe attribute this improvement to our calibration\nmethod, which can adapt the KGC models to an-\nswering complex queries very well.\nThe adaptation function of CKGC is simple,\nmaking the model quickly converge during the\nadaptation process. We next show the training\ntime. The running time for FB15k dataset is an\naverage of 67 seconds per epoch, the running time\nfor FB15k -237 dataset is an average of 32 sec-\nonds per epoch, the running time for NELL995\ndataset is an average of 83 seconds per epoch. All\nmodels can converge within 5 epochs, resulting in\na very short training time. In contrast, previous\nembedding-based models, such as BetaE, ConE,\nFuzzQE, GNN-QE and so on, often require hun-\ndreds of epochs to converge. For instance, BetaE"}, {"title": "5.3 Ablation Studies", "content": "As stated in Section 4.2, we obtain a calibrated\nknowledge graph completion model by executing\nfour steps. To analyze the impact of each step, we\nobtain calibrated models by performing only a few\nsteps. We denote the calibrated model with steps 1\nand 2 as S12, the model with steps 1, step 2, and\nstep 3 as $123, and the model with steps 1, step 2,\nstep 3, and step 4 as S1234. S12 primarily serves as\na baseline. S123 is mainly used to study the impact\nof adaptation (step 3). S1234 is mainly used to\nstudy the impact of calibrating the predicted values\ncorresponding to true triplets (step 4).\nSee Table 2 for the results. S123 has an average\nrelative improvement of 1.3% on avgp metric com-\npared to $12, and S1234 has an average relative\nimprovement of 4.8% on avg, metric compared to\nS123. S123 has an average relative improvement of\n64.8% on avgn metric compared to S12, and S1234\nhas an average relative improvement of 64.3% on\navgn metric compared to S123."}, {"title": "5.4 Hyper-parameters Analysis", "content": "We analyze the experimental results of our method\nwith respect to the hyper-parameter \u20ac. We study\nthe changes in memory usage, sparsity level (the\nratio of zero entries in the KG tensor X), avgp and\navgn as e decreases.\nWe show the results in Table 3, which show that\nas e decreases, memory usage increases, sparsity\nlevel decreases, running time increases, avgp in-\ncreases, and avgn increases. Thus, we can select\na proper e to balance the computation or memory\nusage and model performance."}, {"title": "6 Conclusion", "content": "CLQA is one of the crucial tasks associated with\nKGs. In this paper, we introduce CKGC, a calibra-\ntion method developed to enhance the adaptability\nof KGC models for CLQA. Through experimen-\ntal analysis, we illustrate that the implementation\nof CKGC leads to a substantial improvement in\nmodel performance for the CLQA task. Therefore,\nthe calibration of KGC models holds significant\nimportance for the optimization of CLQA mod-\nels. Moving forward, we encourage the exploration\nand proposal of additional calibration methods to\nfurther enhance the performance of CLQA."}, {"title": "Limitations", "content": "One limitation of our method is the space complex-\nity and time complexity of projection operation.\nWe use an approach to make the predicted tensor\nX sparse. This approach can make the model per-\nformance decreases as shown in Table 3. We plan\nto explore methods that can reduce the computa-\ntional complexity and memory usage of the method\nwhile maintaining good performance."}]}