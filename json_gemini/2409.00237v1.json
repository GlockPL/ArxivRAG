{"title": "Deep learning surrogate models of JULES-INFERNO for wildfire prediction on a global scale", "authors": ["Sibo Cheng", "Hector Chassagnon", "Matthew Kasoar", "Yike Guo", "Rossella Arcucci"], "abstract": "Global wildfire models play a crucial role in anticipating and responding to changing wildfire regimes. JULES-INFERNO is a global vegetation and fire model simulating wildfire emissions and area burnt on a global scale. However, because of the high data dimensionality and system complexity, JULES-INFERNO's computational costs make it challenging to apply to fire risk forecasting with unseen initial conditions. Typically, running JULES-INFERNO for 30 years of prediction will take several hours on High Performance Computing (HPC) clusters. To tackle this bottleneck, two data-driven models are built in this work based on Deep Learning techniques to surrogate the JULES-INFERNO model and speed up global wildfire forecasting. More precisely, these machine learning models take global temperature, vegetation density, soil moisture and previous forecasts as inputs to predict the subsequent global area burnt on an iterative basis. Average Error per Pixel (AEP) and Structural Similarity Index Measure (SSIM) are used as metrics to evaluate the performance of the proposed surrogate models. A fine tuning strategy is also proposed in this work to improve the algorithm performance for unseen scenarios. Numerical results show a strong performance of the proposed models, in terms of both computational efficiency (less than 20 seconds for 30 years of prediction on a laptop CPU) and prediction accuracy (with AEP under 0.3% and SSIM over 98% compared to the outputs of JULES-INFERNO). The codes that were used for building and testing the surrogate models using Python language (3.7) are available at github.", "sections": [{"title": "I. INTRODUCTION", "content": "Long-term prediction of wildfire at a global scale has been a long-standing challenge. Shorter intense wet seasons and longer hot seasons increased wildfire intensity and frequency, costing billions to governments [80], [76]. According to [20], Canada and European countries\u00b9 spent respectively US$531 million and \u20ac2.5 billion annually in wildfire prevention, detection or suppression.\nThus, advanced systems like wildfire models, capable of giving robust and accurate predictions of wildfires activities, have revealed themselves as keys to preventing, detecting or managing changing fire risk. Wildfires models that can forecast fire propagation [23], contribute to alleviating damages, managing firefighting resources or identifying at-risk areas to defend or evacuate. In particular, fire models such as Behave [9], [60] are capable of encapsulating fire dynamics across landscapes. However, long-term wildfire activity prediction is fundamentally complex because of the high dimension of the data and the dynamics between wildfire activities and environmental conditions. Therefore various wildfire models have been developed at regional or global scales. The ones applied at regional scales can be used to model wildfire events in given ecoregions [1]. On the other hand, global wildfire models attempt to analyze wildfire occurrences and predict their probability density [37], [10], [49]. According to [65], [66], wildfire models can be mainly split into two categories: physics-based and data-driven models. The latter also includes empirical models.\nPhysics-based models attempt to understand and reconstitute the dynamic relationship between wildfire activities and environmental factors through physical equations. Physics-based models have been widely used in environmental science such as the use of wave equations to model storm runoff [19], ordinary differential equations (ODE) in wind speed prediction [81], or 3D computational fluid dynamics (CFD) and Cellular Automata (CA) for wildfire propagation [32], [73]. Physics-based modelling is also crucial for various climate or land surface models like JULES [62], which simulates global vegetation cover, carbon and moisture exchange between the atmosphere, biosphere, and soil, and can predict the burnt area and fire emissions at a global scale depending on environmental variables [49], [10]. In addition, hybrid coupled-atmosphere wildfire models like WRF-SFIRE [46] and CAWFE [18] enhance prediction accuracy and computational efficiency by combining physical modelling with dynamic atmospheric data integration, often outperforming fully physical models. However, those models typically also rely highly on empirical parameterizations of unresolved processes to reach accurate results. Although some physics-based models show promising prediction results [44], [39], [52], the computational burden to solve those equations has made them mainly regional-specific, making these models impractical for rapid decision-making [87], [70], for instance to explore many different future climate or policy scenarios. In addition, ensemble predictions [59], [82], sensitivity analysis [73] and parameter calibration are [43] often desired for wildfire and climate models. These tasks often require a large number of evaluations of the forward model, making such simulation extremely computationally costly, if not infeasible.\nOn the other hand, data-driven models try to best mimic physics-based models' behaviour by learning statistical representations [25], [45]. Given the same inputs, those models might learn through regression and Machine Learning (ML) techniques, how physics-based models link driving variables such as environmental conditions and wildfire activity [56], [47], [48], [22], [11]. Improvements in remote-sensing technologies, numerical weather prediction and climate models enhance the performance of data-driven models, which rely heavily on the quality and quantity of available data. As a consequence, they offer access to a large panel of various data with finer resolutions and longer forecasting [4], [74], [51]. Consequently various ML techniques are now used in environmental science such as Artificial Neural Networks (ANN) and Support Vector Machines (SVM) for tornadoes prediction and detection [50], [72], [21], Random Forests (RF) for severe weather forecasting [31], [58], or Recurrent Neural Network (RNN) surrogate models for predicting wildfires [36], [13], [86], storms [38], [35] and floods [6], [3] activities. Nevertheless, the computational cost for large dynamic system prediction can sometimes remain heavy. Thus, recently, it is common to apply ML approaches relying on top of reduced-order modelling (ROM) techniques such as Principal Component Analysis (PCA) [61], [28], orthogonal decomposition [2], [79], [78], [26], entropy-based compression [12] or ML methods like Auto-encoders (AEs) [75], [14]. These methods try to summarise high-dimensional arrays to a few principal latent features while keeping a high accuracy of reconstruction. However, most of these data-driven models mimic a regional-specific numerical model. To the best knowledge of the authors, none of these ML surrogate modelling has yet been applied to surrogate global wildfire prediction models and study wildfire occurrence probability at a global scale.\nIn this study, we propose temporal-spatial surrogate models for JULES-INFERNO burnt area to enable fast wildfire forecasting on a global scale. These models used monthly collected/simulated data of soil moisture, vegetation, temperature and previous area burnt as input to predict the subsequent fire burned area on an iterative basis. Different simulations issued from a variety of initial conditions are split into a training and a test dataset. Our objective is to develop a highly efficient surrogate model that accelerates the online prediction process for the JULES-INFERNO system. To achieve this, we employ varied sets of initial conditions during the training and testing phases, enabling robust performance across different scenarios. This work proposes two deep leaning models to train the surrogate model of JULES-INFERNO. One is based on Convolutional Auto-encoder (CAE) and Long Short-Term Memory (LSTM) (named CAE-LSTM) and another is based on convolutional LSTM (named ConvLSTM).\nTo enhance the performance of the proposed models on unseen scenarios with a different range of initial parameters in the test dataset, fine tuning strategies are also proposed in this work. The idea is to fine tune the developed models using simulation data (for example, 10%) from the beginning of the unseen scenarios to improve the future predictions. Numerical results in this work demonstrate that both proposed models achieve a good approximation of the JULES-INFERNO model of burnt area prediction at a global scale with a Average Error per Pixel (AEP) under 0.3% and a Structural Similarity Index Measure (SSIM) over 98% compared to the outputs of JULES-INFERNO. More importantly, for both approaches, it takes roughly 10 seconds to predict the bunred area of 30 years on a laptop CPU. In contrast, running JULES-INFERNO software will cost about five hours of computational time on 32 threads with the JASMIN national High Performance Computing (HPC) system [42].\nThe paper is organized as follows. The generation and the pre-processing of the training and test dataset using JULES-INFERNO are described in Section II. We then introduce the methodology used for computing and fine tuning the two surrogate models in Section III. The numerical results of predicting unseen scenarios with different initial conditions are shown and discussed in Section IV. Finally, we close the paper with a conclusion in Section V."}, {"title": "II. MODEL AND DATASET", "content": "In this section, we present the data used for training and testing our temporal-spatial surrogate models, which are generated using the JULES-INFERNO model."}, {"title": "A. JULES - INFERNO model", "content": "JULES-INFERNO is a computational vegetation and wildfire model combining the fire parameterisation INFERNO and the land surface model JULES. In JULES-INFERNO, JULES vegetation and land surface outputs are considered as input variables of INFERNO to forecast wildfire occurrence and emissions at a global scale [49]. More precisely, INteractive Fire and Emission algoRithm for Natural environments (INFERNO) follows the simplified parameterisation for fire counts, suggested by [54], which models fire occurrence as a relationship between fuel flammability and ignitions. Fuel flammability is a function of temperature, precipitation, and relative humidity (RH). Fire ignitions are anthropogenic (human population density) or natural (lightning). To simulate global area burnt and emissions, INFERNO adds additional inputs in the flammability parameterisation scheme such as the first layer of soil moisture, and fuel load represented by vegetation carbon density [49]. Average burnt area per fire is then modelled as a function of vegetation type, since wildfires tend to be larger, for example, in grasslands than in forests [16], [27]. The JULES-INFERNO model and it's underlying parameterizations themselves have previously been described in depth and validated with respect to global burnt area observations and other global fire models [54], [49], [10], [62], [69], [30]. Experiments have demonstrated that JULES-INFERNO performs effectively in replicating observed global burnt areas and exhibits comparable performance when compared to other widely-used global fire models.\nThe Joint UK Land Environment Simulator (JULES) model simulates on a global scale the state of land surface and soil hydrology. It considers vegetation dynamics, carbon cycle as well as the exchange of the fluxes between vegetation and environment [10], [7], [17]. These fields are therefore used as JULES-INFERNO's topsoil moisture and fuel load inputs. JULES uses a dynamic global vegetation model (DGVM) called Top-down representation of interactive foliage and flora including dynamics (TRIFFID) to predict changes in biomass and fractional coverage of 13 different plant functional types (PFTs).\nThe underlying equations of the INFERNO scheme are detailed in Section 2.1 of [49] and in [10]. With this approach, JULES-INFERNO is effective in capturing global burnt area and diagnosing wildfire occurrences [49], [69], [30]. The JULES-INFERNO fire simulation model could be time-consuming for high-resolution or long-term ensemble simulations due to its use of complex computational algorithms, requiring iteratively solving a large set of coupled equations in order to simulate the evolution of the global land surface and biosphere, and consequent fire behaviour. Additionally, the need to simulate fire behaviour over extended periods under future climate scenarios further increases the computational time."}, {"title": "B. Data generation", "content": "The objective of this study is to build efficient surrogate models for the long-term prediction of global area burnt. Four spatially distributed environmental variables of JULES-INFERNO are considered in this study :\n\u2022 X: field of Total area burnt in fraction of grid-box s-1\n\u2022 V: field of LAI (Leaf Area Index) - a unitless vegetation density indicator\n\u2022 M : field of Soil moisture in kgm-2\n\u2022 T: field of Surface air temperature in K\nAs the aim of our approach is computational efficiency, we choose V, M, and T as a minimal set of predictor variables which represent the leading-order controls on wildfire burnt area of fuel availability and dryness [29]. M and T are both used explicitly as predictors of vegetation flammability in INFERNO, and are also closely related to the additional meteorological variables of relative humidity and precipitation rate. V is closely related to the leaf and litter carbon pools which are used by INFERNO, but using LAI in our surrogate model allows the resulting model to be easily generalised to work with data from other DGVMs or remote sensing. In this study, the output resolution of the JULES-INFERNO model is fixed as 112 \u00d7 192 on the global map where 112 is the number of pixels on the latitude axis and 192 is the number of pixels on the longitude axis. To train and test the surrogate models, we use output from five 30-year simulations (P1, P2, P3, P4, P5) of JULES-INFERNO. These different simulations were each performed with different initial conditions as summarized in Table I. For each simulation of 30 years, data are saved monthly, resulting in total 360 snapshots for each variable in each of the five simulations.\nP\u2081 to P4 simulate a nominal time period from 1961 to 1990, however with shifted and detrended meteorological boundary conditions that represent a cooler climate state, taken from the FireMIP last glacial maximum (LGM) experiment [55]. Meanwhile P5 corresponds to the historical period of 1990 to 2019, and is taken from an experiment run under the TRENDYv9 protocol for the Global Carbon Budget 2020 report [24]. The simulation snapshots are denoted as:\nXPs = {XPs} with t\u2208 {1, ..., 360} and s\u2208 {1, ..., 5}. (1)\nSame definitions are made for XPs, VPS, MPS and TP."}, {"title": "C. Data preprocessing", "content": "The five simulations with four variables and in total 9000 snapshots are split into a training set, a validation set and a test set. More precisely, the 3 first simulations P1, P2 and P3 are used to train the models, i.e.,\nXtrain = {XP1} \u222a {XP2} \u222a {XP3}, (2)\nwith similar definitions for Vtrain, Mtrain, Ttrain.\nThen P4 is used to validate the models and select the most appropriate hyperparameters. Finally, P5, with significantly different initial conditions, is used for fine-tuning and testing the surrogate model performance, i.e.,\nXval = {XP4} and XFT = {XP5}. (3)\nwith similar definitions for Vval, Mval, Tval and VFT, MFT, TFT\nDuring the training process, all four variables are normalized to the range of 0 to 1 so that they can be equally weighted in the training loss. For example, the normalization of the Total Area burnt leads to\n= . (4)\nwhere Xt is the normalized Total Area burnt. An example of the normalized spatially distributed variables is displayed in Fig. 1 with a Logarithmic scale. A land mask is applied to highlight inland points."}, {"title": "III. METHODOLOGY", "content": "In this section, we describe the computation of the two surrogate models and the fine tuning strategies when applying these models to unseen scenarios. Both models use a sequence-to-sequence prediction mechanism which takes p previous time steps as inputs and return the prediction of the n following time steps as outputs at each iteration."}, {"title": "A. CAE - LSTM", "content": "The CAE-LSTM applies Convolutional Autoencoder and Long-Short-Term-memory networks to reduce the dimension of the data and perform predictions in the reduced latent space successively. Fig 2 presents the workflow of this method with four environmental variables including the global burnt area.\n1) CAE: The first part of the CAE-LSTM model is the CAE, used to compress the full space data into a reduced latent space with a minimum loss of information. CAE is a self-supervised approach based on Convolutional Neural Networks (CNNs) to capture the spatial patterns. The dimension of the latent space is fixed as 15 in this study, yielding a compression rate of  = 0.07%. The dimension of the latent space is considered as a hyperparameter in this study and it is determined by numerical experiments following an analysis of principle components.\nCAE consists of two sub-networks: the Encoder \u0190 which compresses the input data into latent variables (of dimension 15 in this study), and the Decoder D which decompresses the latent variables back to their original form (112 \u00d7 192 in this study). Architectures for the Encoder and Decoder are various. However, they are generally composed of convolutional, pooling and dense layers. Convolutional layers manage to extract local multi-dimensional patterns thanks to the convolutional filters. Pooling layers filter the essential features to propagate in the network [57] and either reduce the convolved tensor dimensions by sub-sampling (the case of Encoder) or, on the contrary, increase them by up-sampling (the case of Decoder). As the final step of the Encoder, fully connected dense layers flatten multi-dimensional tensors into a 1D vector of the target dimension. In this study, the Decoder is built as the inverse of the Encoder to reconstruct spatially distributed inputs from the compressed latent variables.\nRelying on the same structure, 4 CAEs are trained separately for each of the 4 environmental variables Xtrain, Vtrain, Mtrain and Ttrain defined in Section II-C. The Encoders and the Decoders are trained jointly, using the Adam optimizer ([8]) and the MSE loss function with 20% of the data assigned to a validation set. The training process continues as long as the validation loss decreases.\nThe performance of data compression methods will be evaluated on unseen scenarios using Structural Similarity Index Measure (SSIM) and Absolute Error per Pixel (AEP) as presented in SECTION IV.\n2) LSTM: Once data compression is achieved, as the second stage of CAE-LSTM, LSTM is used to forecast the dynamics of the latent variables. As a variant of RNN, LSTM has been widely applied in the prediction of time series data or dynamical systems [53]. In particular, compared to standard RNNs, LSTM uses a selective memory, making them perfectly suited to harness data with long-term dependencies [53]. Furthermore, thanks to the gate structure, LSTM can handle the vanishing gradient problem [33] which is cumbersome for traditional RNNs. More precisely, 3 types of gates are used: the input, the output and the forget gates. We denote xt and Yt the input and the output of a LSTM cell at time step t. Each LSTM cell adopts xt and yt-1 through the input gate it. The cell state ct, the forget gate ft and the output gate Ot are updated accordingly,\nft = \u03c3(Wf \u00b7 [yt-1, Xt] + bf), (5)\nit = \u03c3(Wi \u00b7 [yt-1, Xt] + bi), (5)\nOt = \u03c3(Wo \u00b7 [yt-1, Xt] + bo), (5)\n\u010dt = tanh (Wc \u00b7 [ht-1, xt] + bc), (5)\nCt = ft * Ct-1 + it * Ct, (5)\nwhere * denotes a matrix multiplication. (Wf, bf), (Wi, bi), (Wo, bo), (Wc, bc) are the weights and the bias for each gate, respectively, updated by back-propagation during the training process. \u010ct is the updated cell state propagated through the network. The output yt is then computed as\nYt = Ot * tanh (ct). (6)\nIn this study, for comparison, 2 CAE-LSTM models are implemented, i.e.,\n\u2022 Single CAE-LSTM: Only the Total Area burnt variable is considered as model inputs and outputs.\n\u2022 Joint CAE-LSTM: The four environmental variables are concatenated in the latent space and considered as model inputs and outputs (as shown in Fig 2).\nBoth models are trained with input and output length p and n respectively set to 1, 3, 6 and 12 months. Similar to the training of CAE, the validation set takes 20% data from the training set. The Adam optimizer and the MSE loss function are employed, and the models are trained as long as validation loss decreases."}, {"title": "B. ConvLSTM", "content": "The CAE-LSTM structure is widely applied in surrogate modelling [67], [13]. However, the implementation of data compression and dynamics forecasting through two separate networks increases over-fitting risk and complicates the fine-tuning process. Therefore, the second surrogate modelling in this study use ConvLSTM networks [64] which combines CNN and LSTM in a single network structure.\nSimilar to LSTM models, ConvLSTM uses selective memory to capture temporal-spatial patterns from multi-dimensional inputs. The strength of this model has been widely demonstrated in harnessing multi-dimensional data with temporal-spatial dependencies, such as video prediction [85], image recognition [84] and 3D ocean temperature prediction [83]. It has also been applied to wildfire prediction in previous studies [34], [40].\nThe matrix multiplication used in LSTM to update the cell states and outputs is replaced by convolution operations to operate 2D inputs, that is,\nft = \u03c3(Wf \u00b7 [Ht-1, Xt] + bf), (7)\nit = \u03c3(Wi \u00b7 [Ht-1, Xt] + bi), (7)\nOt = \u03c3(Wo \u00b7 [Ht-1, Xt] + bo), (7)\n\u010ct = tanh (Wc \u00b7 [Ht\u22121, Xt] + bc), (7)\nCt = ft 0 Ct-1 + it \u00b0 \u010ct, (7)\nwhere o denotes a convolutional operator. The output of the previous cell Ht-1, the current input X\u2081, the previous and new cell states, Ct-1 and Ct, are 2D tensors in ConvLSTM. The output Ht of the current cell, also in a 2D form, is computed as\nHt = Ot tanh (Ct). (8)\nSame as CAE-LSTM, 2 structures of ConvLSTM are built in this study. Single ConvLSTM which predicts only the Total Area burnt and Joint ConvLSTM based on the four environmental variables is shown in Fig 3. Joint training of multi-channel temporal-spatial systems can be difficult and require large amounts of training data. Therefore, in the Joint model, instead of considering four environmental variables as different channels, four separate ConvLSTM blocks have been implemented and concatenated before the output layer. For choosing the most appropriate input and output length, n and p are respectively set to 1, 3, 6 and 12 months. The validation set for both Joint and Single ConvLSTM training consists of 20% of the training data. Adam and MSE are used as the optimiser and the loss function, respectively."}, {"title": "C. Model Fine tuning", "content": "During training, ML models optimise their parameters to best harness the training data, leading to potential risk of overfitting. Consequently those models often struggle when facing unseen data of different periods or regions and lose prediction accuracy.\nIn ML, Fine-tuning consists of adjusting models parameters by re-training the pre-trained models with (usually a small amount of) unseen data of different initial conditions to improve the model generalizability and robustness. In this study, the first 5 years of the test set (i.e., P5 (1990-1995)) are used to fine-tune the pre-trained surrogate models. More precisely, in our ConvLSTM model, convolutional layers and LSTM cells are fine-tuned simultaneously thanks to the joint structure. In our CAE-LSTM model, only the LSTM network is fine-tuned, since fine-tuning the CAE should require the complete re-training of the LSTM. Since the fine-tuning dataset is of small size, to avoid overfitting, the number of fine-tuning epochs is fixed as 30 in this study. Finally, fine-tuned models are tested on the last 25 years remaining of P5 (1995 - 2020).\nIn other words, despite that the fine-tuning requires to simulate the beginning of the test sequence (16.6% in this study) using JULES-INFERNO, the online computational time can still be considerably reduced compared to running the full simulation. In this work, the training and the fine tuning of both surrogate models are performed on a Tesla P100 GPU using the Google Colab environment while the online prediction is made on a single-core CPU of 2.2GHz. The running time of JULES-INFERNO on JASMIN national HPC is estimated as an average from 4 simulations, range 4.3 \u2013 5.9 hours, using Intel Xeon E5-2640-v4 \"Broadwell\" or Intel Xeon Gold-5118 \"Skylake\" processors with 7 GB RAM available per thread."}, {"title": "IV. NUMERICAL RESULTS AND DISCUSSIONS", "content": "In this section we evaluate and discuss the performance of the surrogate models regarding a variety of different metrics. The optimal neural network structures and hyperparameters are chosen by evaluating the algorithm performance on P4. The capability of the surrogate models in predicting unseen scenarios are assessed on P5 where the boundary conditions are significantly different as shown in Table I."}, {"title": "A. Metrics", "content": "Three metrics are used in this study to measure the model performances for predicting the Total Area burnt. The first metric used is the Absolute Error per Pixel (AEP), which highlights the pixel-wise differences between original (X) and predicted (X') fields defined as\n\u0391\u0395\u03a1 = . (9)\nwhere l = 7771 is the number of land points in the image. However, when evaluating the AEP, predicted and original fields are compared pixel-by-pixel which makes the estimated score highly sensitive to image distortion and translation.\nTo address this limitation, the work of [77] proposed the Structural Similarity Index Measure (SSIM), which measures the perceptive similarity between images (2D vectors). This SSIM for two images I\u2081 and I2 is defined as\nSSIM = , (10)\nwhere (\u03bc\u03b9\u2081, \u03bc\u03b9\u2082) and (\u03c311, \u03c312) are respectively the means and the standard deviations of the two images. 01112 is the covariance of I1 and I2. C1 and C2 are regularization constants [77]. By definition, the value of SSIM ranges from 0 to 1 indicating the similarity between I\u2081 and I2.\nFinally, the third metric is the online computational time for reconstructing or predicting Total Area burnt."}, {"title": "B. Evaluation of data compression", "content": "Different sets of hyperparameters of CAE are tested and compared to select the most appropriate network structure with minimum information loss. PCA and fully connected autoencoders are also implemented in this work as baselines for comparison purposes.\nAll the data compression methods are trained on the training dataset and tested on P4. Table II presents the SSIM, \u0391\u0395\u03a1 and compression/decompression computational time for each method."}, {"title": "C. Evaluation of predictive models", "content": "Here we first compare the performance of the two surrogate models on the validation dataset in terms of forecasting the next Total Area burnt on a global scale. As mentioned in section III, CAE-LSTM and ConvLSTM have been both trained with solely the Total Area burnt variable (i.e., Single surrogate model) or the four environmental variables (i.e., Joint surrogate model). Table III shows the mean AEP and the mean SSIM for different models evaluated on the 30 years of prediction on P4.\nAccording to the results presented in Table III, the accuracy of the Total Area burnt prediction when the four environmental variables were taken into account. As mentioned in [63], [41], soil moisture, LAI and temperature can significantly impact the wildfire burned area. From a data perspective, our results numerically demonstrate this matter of fact. As shown in Table III, in particular, the Joint model of ConvLSTM can reduce more than 50% of AEP while keeping a low online computational time.\nFig 4 presents the cumulative sum of the AEP in CAE-LSTM and ConvLSTM predictions on the test dataset P4 according to different models chosen. Comparing the red and the yellow dashed lines, we can conclude that including all four environmental variables in the system can significantly reduce the prediction error for ConvLSTM. On the other hand, little difference can be found between the light blue and the dark blue curves in Fig 4. This indicates that CAE-LSTM forecasting is essentially based on the previous Total Area burnt sequences and that the contribution of other environmental variables to the predictive model is marginal, albeit still a minor improvement. Overall, when being applied directly to unseen scenarios, CAE-LSTM shows a more robust prediction of the Total Area burnt compared to ConvLSTM with a lower cumulative AEP. For the rest of this paper, we will focus on the Joint models since they are demonstrated to be more accurate compared to single models for both CAE-LSTM and ConvLSTM.\nDetermining the appropriate number of monthly steps (n,p) for input and output sequences is crucial for predictive models. In fact, it can be cumbersome to train predictive models with long-temporal dependencies [5]. On the other hand, iterative predictions using short-term forward models require frequent model forecasts, leading to more computational time, and more importantly, fast error accumulation [13]. Therefore, an optimal tradeoff should be found. To simplify the iterative prediction process, the input and output sequences are set to be equal (i.e.,n = p) in this work.\nTable IV presents the mean SSIM and AEP scores for Joint models with n = p = 3, n = p = 6 and n = p = 12. It can be clearly seen that for both CAE-LSTM and ConvLSTM, the 12 to 12 setting has the best performance in terms of both prediction accuracy and computational efficiency, which is consistent with the annual periodic nature of climate variables.\nSimilar analysis can be performed by investigating the AEP curves as displayed in Fig. 5. As can be seem there, the 12 to 12 predictive models can lead to more reliable and consistent predictions. In particular, a higher sensitivity of CAE-LSTM regarding the length of input and output sequences has been noticed where both the 3 to 3 and the 6 to 6 models have an AEP three times larger than the one of 12 to 12.\nWe have also tested the model performance using the data in P5 where the simulation period and the initial conditions differ significantly from the training set as previously shown in Table I. For comparison purpose, the results of P5 are presented alongside those of P4 in Table V. P5 corresponds to a significantly different time period and climate state from the P1-P3 simulations the algorithm was originally trained on (historical 1990-2019 versus LGM), and so is a good test of the ability of the algorithm to capture the drivers of fire under very different conditions. Unsurprisingly, for both models, the prediction on P5 is less accurate compared to P4, especially in terms of AEP. Consistent with our previous analysis, we notice that CAE-LSTM is more sensitive to the difference regarding study period and range of initial conditions. Contrary to the case of P4, advantages of ConvLSTM compared to CAE-LSTM is noticed in terms of both metrics.\nTo further inspect the algorithm performance, Total Area burnt has been investigated. Fig 6 shows the predicted fields of the Total Area burnt in a logarithmic scale for t = 10, 65 and 230 months after the start of the simulation. These three time steps are chosen, because they correspond to short-, medium- and long-term prediction of burnt area, respectively. At t = 10, ConvLSTM manages to deliver a precise prediction regarding the JULES-INFERNO output. As long as iterative predictions take place, the prediction error can be accumulated, leading to noise in future predictions. However, most at-risk regions such as Central America and South America at t=65, and South Africa at t=230 can still be identified by the ConvLSTM model. As for CAE-LSTM, the model prediction differs from the JULES-INFERNO simulation right from the beginning of the prediction process as shown in Fig 6 (b). These results are coherent with the metrics shown in Table V. In summary, despite that the CAE-LSTM surrogate model shows better performance when the test data is relatively similar (but still significantly different) to the training data in terms of time period and initial conditions (i.e., P4), it is outperformed by ConvLSTM regarding the generalizability when being applied to test data with a different range of initial and meteorological conditions (i.e., P5). To achieve reliable long-term predictions on P5, the performance of both CAE-LSTM and ConvLSTM needs to be improved."}, {"title": "D. Model fine tuning", "content": "Fine tuning pretrained models for unseen scenarios with significantly different conditions or assumptions is a common practice for the deployment of machine learning techniques [15], [71]. In this study, model fine-tunings are performed using the simulation data for the first five years (i.e., 60 snapshots) of P5 with 30 epochs for each surrogate model.\nBoth SSIM and AEP metrics are consistently improved according to the results in Table VI. More importantly, as shown in Fig 7, considerable enhancement on long-term prediction can be noticed for both surrogate models at t = 65, 230. Most of at risk regions (Fig 7 (a)) can be successfully recognized by CAE-LSTM and ConvLSTM. The evolution of Cumulative AEP and SSIM against prediction steps is shown in Fig 8. A consistent improvement of the SSIM score (dashed blue line vs. solid orange line for CAE-LSTM and dashed green line vs. dashed red line for ConvLSTM) thanks to the fine tuning can be observed for both models. On the other hand, it is also noticed in Fig 7 and 8 that after fine-tuning, the two stage surrogate model CAE-LSTM is still outperformed by ConvLSTM. In fact, the fine-tuning of ConvLSTM involves the entire neural network architecture, whereas in CAE-LSTM only the LSTM stage is fine-tuned. Thus, the CAE remains driven by the temporal-spatial patterns specific to the 1960-1990 period and thus struggles to encode and decode data from other periods. In summary, performing fine-tuning can substantially enhance the prediction performance but also increase the computational cost since it requires running the full JULES-INFERNO model for 5 years of initial prediction. However, compared to a complete simulation of 30 years, it can still reduce the computational time from about five hours to less than an hour."}, {"title": "V. CONCLUSION AND FURTHER WORK", "content": "This study presents two surrogate models of JULES-INFERNO which use ROM and ML methods to speed up global burnt area forecasting. Both models, referred as CAE-LSTM and ConvLSTM, forecast monthly global area burnt from antecedent Temperature, Vegetation, Soil Moisture and Total Area burnt fields. Models are trained with JULES-INFERNO simulated forecasts from 1960 to 1990 but can be applied to a different period or conditions using newly available simulation or observation data. The numerical results in this paper demonstrate the efficiency and the robustness of the proposed approach. Their predictions over the period of 1960 to 1990 (where the model has been trained) show more than 99% of similarity with JULES-INFERNO simulation. The ConvLSTM-based surrogate model also shows a good generalizability after fine-tuning, thanks to its joint structure of convolutional and recurrent layers. More importantly, CAE-LSTM and ConvLSTM have considerably improved the computational efficiency. While running a 30-year simulation with JULES-INFERNO requires approximately five hours on JASMIN national HPC (32 threads) our models require less than 20 seconds on a single-core CPU. This paper focuses on the development of a rapid surrogate model for JULES-INFERNO, driven by the motivation to enhance efficiency. It is important to note that since the surrogate model is trained solely with generated data from JULES-INFERNO"}]}