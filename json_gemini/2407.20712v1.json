{"title": "Cocobo: Exploring Large Language Models as the Engine for End-User Robot Programming", "authors": ["Yate Ge", "Yi Dai", "Run Shan", "Kechun Li", "Yuanda Hu", "Xiaohua Sun"], "abstract": "Abstract-End-user development allows everyday users to tailor service robots or applications to their needs. One user-friendly approach is natural language programming. However, it encounters challenges such as an expansive user expression space and limited support for debugging and editing, which restrict its application in end-user programming. The emergence of large language models (LLMs) offers promising avenues for the translation and interpretation between human language instructions and the code executed by robots, but their application in end-user programming systems requires further study. We introduce Cocobo, a natural language programming system with interactive diagrams powered by LLMs. Cocobo employs LLMs to understand users' authoring intentions, generate and explain robot programs, and facilitate the conversion between executable code and flowchart representations. Our user study shows that Cocobo has a low learning curve, enabling even users with zero coding experience to customize robot programs successfully.", "sections": [{"title": "I. INTRODUCTION", "content": "End-user development (EUD) enables non-technical users to modify applications to meet specific needs, enhancing versatility and user engagement [1]. This trend is increasingly relevant in robotics, as buyers now consider software support for defining robot missions a key criterion [2]. The development of mainstream technologies has further stimulated research interest in this approach [3], including Internet of Things (IoT) [4] and robotics [5]. Among EUD methods, natural language programming is intuitive but often imprecise, leading to potential misinterpretations between user intentions and program outcomes. Integration with visual programming can mitigate these challenges by helping users better understand and modify their programs [5].\nRecent advances involve pre-trained large language models (LLMs) like GPT-3.5 and GPT-4 [6], and Llama [7], which excel in code generation and adapting to specific tasks with minimal examples [8], [9]. These models offer the potential for creating more adaptable and user-friendly programming interfaces, especially for personalizing robot tasks.\nResearch on non-experts' interactions with Large Language Models (LLMs) in EUD contexts is limited, highlighting the necessity for interactive interfaces that capitalize on LLMs for tasks like the personalized customization of service robots.\nThis paper investigates how LLMs can assist daily users in programming service robots by leveraging their world knowledge and in-context learning to seamlessly integrate natural language and flow-based programming. This integration facilitates easier program creation, debugging, and modification, thereby lowering the learning curve for users. We developed Cocobo, a proof-of-concept system designed with these capabilities, and conducted an empirical study with 16 participants to evaluate its usability and effectiveness."}, {"title": "II. RELATED WORK", "content": "End-user development for robot task customization.\nEUD empowers non-programmers to modify robot programs, enabling adjustments during runtime rather than only at the design stage [1]. EUD methods include visual programming [10]\u2013[14], natural language programming [11], [15]\u2013[18], and programming by demonstration [19], designed to minimize learning barriers and cognitive load [20], [21]. However, the required domain-specific knowledge presents challenges, deterring non-experts and increasing the likelihood of errors [22]. Additionally, while prior research often catered to experts, it generally neglected consumer-level users [21]. The integration of AI for natural language processing and program synthesis offers potential for more intuitive EUD interfaces [23].\nNatural language programming with LLMs. Natural language programming enhances EUD by allowing code generation via speech or text, supported by AI advancements and large language models [5], [8]. Despite its potential, using large language models effectively in EUD involves challenges like complex prompt design [24]\u2013[30]. Research indicates that structured prompts could improve robot programming with LLMs [31]\u2013[34]. Additionally, LLMs' in-context learning [9] capabilities enable them to learn from the context of the task, supporting the generation of multiple representations [35] of programs. This helps to address the uncertainties in natural language programming and supports debugging and editing through structured representations.\nLLM-based Interactive Systems. LLMs enhance interactive systems by enabling conversational interactions and advanced reasoning, significantly streamlining complex task management. Techniques such as prompt chaining break down"}, {"title": "III. DESIGN AND IMPLEMENTATION OF COCOBO", "content": "We introduced the Cocobo system to assist novice users in creating personalized robot services, detailed in Figure 1. We selected common robot commands to verify the feasibility of Cocobo, as listed in Table I. These commands interact directly with the Temi robots' APIs [40]. Further details on Cocobo's features are discussed in the following sections.\nCocobo's interface includes a conversational UI and a flowchart editor. The conversational UI facilitates the creation or modification of personalized service programs via multi-turn dialogue, while the flowchart UI allows users to check and edit flowcharts. This enables users to understand program behavior in a structured manner and make necessary edits."}, {"title": "B. LLM-powered Interactive Functions", "content": "Authoring programs via conversation: During the authoring phase, users specify their customization requirements through interactions with the dialogue agent. Following each user input, the system provides a list-based representation of the user's customized needs derived from its assessment. Upon user confirmation, the system generates code, flowcharts, and textual explanations(Figure 2).\nModifying programs via conversation: After the program's initial creation, users can further specify modification requirements through dialogue. Cocobo regenerates the program and updates the explanations and the flowchart accordingly (Figure 3).\nModifying programs via Flowchart Editor: Using the flowchart, users can visually confirm that the program aligns with their intentions and can modify the executable code by adjusting the flowchart. After the user taps the Sync Change button (Figure 1, A4), Cocobo updates the program accordingly. Like other node-based visual programming environments, the flowchart editor facilitates adding or removing nodes, adjusting connections between nodes, and modifying node attributes. However, unlike typical visual programming environments, our node attributes feature natural language descriptions of node behaviors rather than direct changes to actual parameters. This approach enables textual modifications to adjust these behaviors (see Figure 4).\nModifying programs via MagicDebug: Cocobo features a MagicDebug function that enables users to select nodes and perform targeted debugging and direct modifications via text dialogue. As shown in Figure 5, upon user selection of one or multiple nodes and activation of the MagicDebug button, the system provides explanations about the behaviors represented by these nodes via the conversational interface. The dialogue then shifts to MagicDebug mode, enabling further inquiries and modifications to these nodes through text input."}, {"title": "C. LLM Pipeline and Prompt Strategies", "content": "As shown in Figure 6, we implemented the core LLM-powered functions of the Cocobo system using the LLM-chain method. Informed by prior studies on LLM prompting, we adopt a chain-of-thought [31] approach to structure prompt words and decompose tasks in our system. The prompt preamble in our system categorizes prompt words into six key segments: [Role], [Context], [Rules], [Workflow], [Output Format], and [Example]. This structured approach assists in building the LLM module within Cocobo's LLM pipeline, enabling it to accomplish various tasks.\nHow to enable the system to determine user intent and execute corresponding actions: We configure LLMs to assess user intent, directing output generation in various formats. Specifically, in the [Workflow] segment, we employ pseudo-code style \"if\" statements to prompt the LLM to identify different user intents and execute corresponding actions. In the [Output Format] segment, we utilize XML to define both the XML tags and the internal format of the content, enabling the system to process various types of content from LLMs' output based on these tags. For instance, the MagicDebug function must determine user intent from the input text and then take appropriate actions, such as providing text explanations or regenerating code, flowcharts, and modification nodes(see Figure 6). The system determines and executes subsequent outputs, such as re-rendering flowcharts, based on the XML tags in the output text generated by MagicDebug's LLM module.\nHow to enable LLMs to convert between code and flowcharts: Cocobo generates the flowchart from code and creates new code based on user modifications to the flowchart. Cocobo uses JSON [41] format data for front-end flowchart rendering, which is complex and lengthy due to extraneous details. This complexity and the token length constraints of LLMs pose stability challenges. To address these, we use Mermaid [42] as an intermediary flowchart description language for its simplicity and scalability. This enables us to generate accurate and stable flowchart descriptions through prompt engineering and examples and convert them into JSON format data for frontend rendering using scripts."}, {"title": "D. System Implementation", "content": "Our system integrates a backend server, a web interface, and a physical robot. The backend is developed using Node.js, integrating OpenAI's GPT-4 API [6] to implement Cocobo's LLM-powered functions and controlling the robot by sending WebSocket [43] messages. The frontend employs AntV X6 [44] for flowchart rendering and interactions. To evaluate the system's usability and user perception, we used the Temi robot, which provides an SDK [40] that allows us to access the robot's API and develop applications for it. We developed a simple robot application that launches a WebSocket server, enabling bidirectional communication between Cocobo's backend server and the robot API. This includes, for example, controlling the robot to move to target locations, user voice input recognized by the robot, the robot's person detection results, and so on. The backend directly controls the robot and obtains its status, so there is no waiting time for the code to be deployed to the robot."}, {"title": "IV. USER STUDY", "content": "We conducted a user study in a lab setting to evaluate the effectiveness of Cocobo's LLM-powered functions for novice users in customizing robot services, focusing on their perceptions of its conversational and flowchart interfaces. For specific task details, please refer to the Appendix. We recruited 16 participants (8 males and 8 females), aged between 16 and 30 (mean age 22.9, SD=6.5), via social media. These participants had limited programming experience and varied familiarity with LLM-based tools.\nThe study comprised four phases: 1) introduction, 2) training, 3) testing, and 4) feedback. Initially, participants were briefed on the goals and structure of the test, and a sample program created by Cocobo was demonstrated. During the training phase, users watched a tutorial video and practiced using the Cocobo system under supervision. In the test phase, participants were given three tasks, each described as a scenario to program the robot accordingly. They could test the written programs at any time by deploying the code to the robot. Each task had a 25-minute time limit, but participants were allowed to continue if they needed more time. After completing the tasks, participants were asked to fill out the System Usability Scale (SUS) questionnaire and participate in a semi-structured interview."}, {"title": "V. RESULTS AND FINDINGS", "content": "The SUS score was 72.3, indicating good usability compared to standard benchmarks [45]. Participants expressed confidence in using Cocobo for complex tasks, appreciating its ability to customize service robot functions. They indicated a higher likelihood of purchasing smart devices equipped with EUD tools similar to Cocobo. All participants completed three tasks, and over half (10/16) finished within the allotted time. However, many users encountered issues, such as the LLMs failing to produce structured results as expected, generated content being too lengthy, causing delays and perceived system malfunctions, and Cocobo not meeting user expectations despite repeated input modifications. These issues suggest a need for further optimization of Cocobo's performance.\nHow do users perceive the conversational interface of Cocobo? Most participants (15/16) reported that the conversational interface felt natural and intelligent, enhancing the programming experience by making it seem like collaborative coding with the system. They found the generated content aligned well with their expectations, reducing the cognitive load for those with limited programming experience. \u201cIt automatically generates it for you, which is very convenient\" (P6). Additionally, it added useful interactive details. \"It would add interactive details that I did not consider\" (P1). Furthermore, a subset of users (5/16) noted that clearer and more logical text inputs lead to better content generation, especially when handling complex customization requirements.\nHow do users perceive the flowchart interface of Cocobo? The flowchart interface was generally found to be intuitive for representing code by the majority of participants (12/16), helping them quickly understand the main steps and key information without extensive reading. \"The flowcharts provide specific details and direct visual representations of program steps,\" noted participants P3, P6, and P5. This feature was particularly useful for program verification and understanding, enabling users to clarify programming logic more effectively than with text descriptions alone. However, some users (3/16) found the interface challenging without basic programming knowledge and felt that duplicated content in updates could be confusing, making it difficult to track changes. \"I'm not sure where it made changes,\" mentioned P3.\nHow do users perceive the relationship between the conversational interface and the flowchart interface? All users agreed that the conversational UI and flowchart UI complement each other well, utilizing different editing methods depending on the task at hand. \"Some modifications are better suited for the flowchart, such as inserting a robot's action in the process. Others are more conveniently done with text, like adjusting the overall flow,\" explained P8. The Magic Debug function was utilized by only a few participants (4/16) to make precise adjustments in the flowcharts and ensure changes were applied accurately to the selected nodes, as noted by P16. However, other participants were neutral about the Magic Debug function, finding it not particularly beneficial or essential to their tasks."}, {"title": "VI. LIMITATION AND FUTURE WORK", "content": "Cocobo demonstrates the potential of integrating AI with EUD [23] to aid everyday users in customizing smart devices and services. However, our preliminary evaluation identified several challenges that need addressing: the study only integrated basic robotic commands and did not extend to more complex IoT and network services, which may limit scalability; LLM-powered functions in Cocobo experienced issues with unstable outputs and prolonged response times due to excessively lengthy outputs; and the current design does not account for varying levels of programming skills among users, which may limit its effectiveness in practical scenarios. Moreover, this work lacks an in-depth comparison and analysis of the various representations within the Cocobo system.\nFor future work, we aim to enhance the performance of Cocobo's LLM-powered functions and expand the system to support additional APIs for robots and IoT devices that align with real-world application scenarios. We plan to conduct 'in-the-wild' experiments to assess the practical benefits and potential improvements of the Cocobo design concept more thoroughly. This includes exploring different representations within the system and examining EUD support for individuals across different age groups and cognitive abilities."}, {"title": "A. Tasks for Training", "content": "The training session is designed to help participants become familiar with Cocobo's functions. Participants are encouraged to ask questions at any time during this stage to ensure a full understanding of the system.\nScenario 1: Design a service where the robot assists in providing guidance services. The task involves programming the robot to travel to a specified location and then return to the Reception Area.\nScenario 2: Create a robot service that greets people nearby and asks if they need navigation help. Depending on their response, the robot should provide the appropriate guidance service."}, {"title": "B. Tasks for Formal Testing", "content": "The formal testing consists of three tasks, each requiring the participants to conceptualize and program robot services based on provided scenarios. Participants are encouraged to debug and refine their designs until satisfaction is achieved. Each task is followed by a 5-minute break, and participants should signal the researcher upon completion of each task.\nTask 1\nScenario: Program a robot to patrol the office after hours, reminding any remaining employees to leave. The patrol route should cover 4 specific locations and be arranged to minimize route repetition.\nTask 2\nScenario: With the company expanding, design a robot service to guide visitors through the exhibition area, introducing the company's projects. This includes a dual-arm robot displayed in the Exhibition Area and a mixed-reality robot testing platform in the Multimedia Studio.\n1) Design a tour route that includes visiting locations and an endpoint.\n2) Provide detailed introductions for each location.\n3) Plan how the robot concludes the tour.\n4) Optionally, provide different tour routes based on user responses.\n5) Optionally, include proactive service initiation by the robot.\nTask 3\nScenario: Design a service where the robot assists in guiding scheduled visitors to designated areas based on prior information provided by employees. For example, Miss Elaine from the administrative department has scheduled Jack to come and fix the air conditioning in the meeting room. Miss Elaine informs the robot about the task of guiding Jack to the meeting room, and when Jack arrives, the robot can guide him to the meeting room.\n1) Allow employees to enter visitor information, including names and destinations.\n2) Create a visitor guidance process to lead visitors to designated locations.\n3) Consider how the robot handles unknown visitors.\n4) Ensure the robot guides each visitor to the correct location."}]}