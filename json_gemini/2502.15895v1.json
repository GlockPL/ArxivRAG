{"title": "DIRECTIONAL GRADIENT PROJECTION FOR ROBUST FINE-TUNING OF FOUNDATION MODELS", "authors": ["Chengyue Huang", "Junjiao Tian", "Brisa Maneechotesuwan", "Shivang Chopra", "Zsolt Kira"], "abstract": "Robust fine-tuning aims to adapt large foundation models to downstream tasks while preserving their robustness to distribution shifts. Existing methods primarily focus on constraining and projecting current model towards the pre-trained initialization based on the magnitudes between fine-tuned and pre-trained weights, which often require extensive hyper-parameter tuning and can sometimes result in underfitting. In this work, we propose Directional Gradient Projection (DiGraP), a novel layer-wise trainable method that incorporates directional information from gradients to bridge regularization and multi-objective optimization. Besides demonstrating our method on image classification, as another contribution we generalize this area to the multi-modal evaluation settings for robust fine-tuning. Specifically, we first bridge the uni-modal and multi-modal gap by performing analysis on Image Classification reformulated Visual Question Answering (VQA) benchmarks and further categorize ten out-of-distribution (OOD) VQA datasets by distribution shift types and degree (i.e. near versus far OOD). Experimental results show that DiGraP consistently outperforms existing baselines across Image Classfication and VQA tasks with discriminative and generative backbones, improving both in-distribution (ID) generalization and OOD robustness.", "sections": [{"title": "1 INTRODUCTION", "content": "Robust fine-tuning has become an essential technique in adapting pre-trained models to downstream tasks, particularly in the face of distribution shifts that challenge model generalization. While pre-trained models excel in capturing a wide range of features from diverse datasets, fine-tuning them on specific tasks often leads to overfitting, reducing their robustness to out-of-distribution (OOD) data (Wortsman et al., 2022; Nguyen et al., 2024). The goal of robust fine-tuning is to strike a balance between task-specific performance and maintaining the generalization abilities of the pre-trained model (Wortsman et al., 2022). This is particularly crucial for real-world applications such as visual question answering (VQA), where models are frequently exposed to varying distributions in images, questions, and answers (Agrawal et al., 2018; Shah et al., 2019). Effective robust fine-tuning strategies aim to mitigate performance degradation by incorporating techniques like regularization (Li et al., 2018) and bi-level optimization (Tian et al., 2023a;b; 2024), ensuring that models retain their learned knowledge while adapting to new domains.\nTo tailor the model for downstream tasks while retaining the capabilities of the pre-trained model (e.g. robustness to distribution shifts), L2-SP (Li et al., 2018) imposes a regularization term on the distance between the fine-tuned and pre-trained weights. More recently, instead of viewing robust fine-tuning as a regularization problem, TPGM (Tian et al., 2023a) and FTP (Tian et al., 2023b) consider the regularization term as the constraint to reformulate the problem from a bi-level optimization prospective and propose to learn different hard constraints for each layer. However, these methods are computationally heavy and often apply overly strong constraints which results in underfitting (See Sec. 4.2 and Sec. 4.3). Moreover, these methods perform weight projection to enforce the distance between fine-tuned and pre-trained weights within a set of projection radii, which is magnitude-wise but does not encode any directional information. This motivates us to think of direction-based methods for this fundamental problem."}, {"title": "2 RELATED WORKS", "content": "Robust Fine-Tuning of Foundation Models. LP-FT (Kumar et al., 2022) proposes a two-step strategy of linear probing then full fine-tuning to prevent the feature distortion of the pre-trained layers. WiSE-FT (Wortsman et al., 2022) interpolates the pre-trained and fine-tuned weights to combine the strengths of the two embedding space. L2-SP (Li et al., 2018) explicitly adds an L2 norm penalty on the deviation between the fine-tuned and pre-trained weights. MARS-SP (Gouk et al., 2021) further studies different forms of norms as the penalty. More recently, TPGM (Tian et al., 2023a) approaches the regularization term as a constraint, reformulating the problem as a bi-level optimization and proposing to learn distinct hard constraints for each layer. FTP (Tian et al., 2023b) further improves the efficiency of TPGM (Tian et al., 2023a) by learning the constraint from training set of previous step instead of the current validation set. We argue that these methods are still computationally heavy and requires lots of tuning, whereas DiGraP reformulates the problems as multi-objective optimization, injects directional information and is more intuitive to tune.\nOOD Robustness in VQA. Previous works have proposed various settings for evaluating robust VQA models. Agrawal et al. (2023) conducts cross-dataset evaluations with four VQA datasets, while Ma et al. (2024) and Li et al. (2021) provides a more comprehensive and detailed robustness analysis by incorporating VQAv2 variants and categorizing different types of distribution shifts. We build on these efforts by introducing further granularity with near and far OOD distinctions and measuring the distance between distributions. More importantly, while previous work has primarily focused on testing different backbone models, they have not yet compared different robust fine-tuning methods."}, {"title": "3 DIRECTIONAL GRADIENT PROJECTION FOR ROBUST FINE-TUNING", "content": "In this section, we first describe the intuition and the mathematical motivation behind DiGraP. Then, we provide our method's concrete algorithmic design."}, {"title": "3.1 ROBUST FINE-TUNING AS A MULTI-OBJECTIVE OPTIMIZATION PROBLEM", "content": "In order to adapt the model to the downstream tasks but preserve the power of the pre-trained model (e.g. robustness to distribution shifts), methods such as L2-SP (Li et al., 2018) impose a regularization term on the distance between the fine-tuned and pre-trained weights. Formally,\n\\[\\mathcal{L}(\\theta) = \\widehat{\\mathcal{L}}(\\theta) + \\frac{\\lambda}{2} ||\\theta - \\theta_0||^2\\]\nwhere \\(\\theta\\) denotes the fine-tuned weights, \\(\\theta_0\\) the pre-trained weights, \\(\\widehat{\\mathcal{L}}(\\theta)\\) the original loss function, and \\(\\lambda\\) the hyper-parameter for regularization strength, i.e., weight decay. In this case, \\(||\\theta - \\theta_0||^2\\) serves as a constraint so that the updated model will not deviate from the initialization too much, thus we can maintain some strengths from the pre-trained model. However, L2-SP is not intuitive to tune \\(\\lambda\\) which often spans over a wide range: a small \\(\\lambda\\) may achieve better in-distribution performance but leads to poor OOD robustness, while a large \\(\\lambda\\) results in underfitting. Besides, L2-SP is also harder to tune if applied differently across layers.\nIn this work, we instead propose to view robust fine-tuning from a multi-objective optimization perspective, leading to a more explicit method to balance this trade-off. Specifically, there are two objectives that we want to optimize,\n\\[\\text{Objective}_1 = \\widehat{\\mathcal{L}}(\\theta), \\text{Objective}_2 = \\frac{1}{2} ||\\theta - \\theta_0||^2\\]\nwhere the first objective represents the original loss function and the second objective represents the distance between the fine-tuned and pre-trained weights. Our goal is to minimize both at the same time to achieve ID generalization and OOD robustness."}, {"title": "3.2 PROJECTING CONFLICTING GRADIENTS", "content": "Viewed from this perspective, we can leverage prior multi-objective methods towards our problem. PCGrad (Yu et al., 2020) hypothesizes that the key optimization issue in multi-objective learning arises from conflicting gradients, where gradients for different objectives point away from each other. Thus, optimizing one of them will lead to the suboptimality of the others. They propose a form of gradient surgery by projecting a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient, therefore benefiting all objectives.\nInspired by PCGrad, we propose the following algorithm for robust finetuning: When the gradients between the two objectives are in conflict, i.e. their cosine similarity is negative, we project the gradient for the original loss function to the orthogonal direction of the gradient for the regularization term. Specifically, the gradients for the two objectives and the projection of the first gradient in the direction of the second gradient are respectively:\n\\[\\widehat{g}_1 = \\nabla_{\\theta} \\widehat{\\mathcal{L}}(\\theta), \\widehat{g}_2 = \\theta - \\theta_0, \\widehat{g}^{\\text{proj}}_1 = \\frac{\\widehat{g}_1 \\widehat{g}_2}{\\|\\widehat{g}_2\\|^2} \\widehat{g}_2\\]\nWe add a hyper-parameter \\(\\omega \\in [0, 1]\\) to further control the projection strength. \\(\\omega = 0\\) is equivalent to an unconstrained gradient update, while \\(\\omega = 1\\) is the same as a full orthogonal projection. The final projected gradient is the following:\n\\[g = \\widehat{g}_1 - \\omega \\widehat{g}^{\\text{proj}}_1 = \\widehat{g}_1 - \\omega \\frac{\\widehat{g}_1 \\widehat{g}_2}{\\|\\widehat{g}_2\\|^2} \\widehat{g}_2, \\omega \\in [0, 1]\\]\nNote that for L2-SP, the gradient for the regularized loss function is formulated similarly:\n\\[g = \\nabla_{\\theta} \\mathcal{L}(\\theta) = \\nabla_{\\theta} \\widehat{\\mathcal{L}}(\\theta) + \\lambda (\\theta - \\theta_0) = \\widehat{g}_1 + \\lambda \\widehat{g}_2\\]\nThus, DiGraP is equivalent to L2-SP with different \\(\\lambda\\) for every layer. In summary, for each layer i:\n*   Gradients are non-conflicting \\((\\widehat{g}_i^T \\widehat{g}_i \\geq 0)\\): \\(\\lambda^i = 0\\)\n*   Gradients are conflicting \\((\\widehat{g}_i^T \\widehat{g}_i < 0)\\): \\(\\lambda^i = -\\omega \\frac{\\widehat{g}_1 \\widehat{g}_i}{\\|\\widehat{g}_i\\|^2}\\)\nCompared to L2-SP, the hyper-parameter \\(\\omega\\) in DiGraP is within the range between 0 and 1, which is more intuitive to tune. Furthermore, even with one fixed \\(\\omega\\), the regularization strength \\(\\lambda\\) varies across both layers and iterations, making the fine-tuning process more flexible to fit the training data."}, {"title": "3.3 LAYER-WISE TRAINABLE DIRECTIONAL GRADIENT PROJECTION", "content": "We emphasize that the regularization problem in Eq. 1 is still not fully equivalent to the multi-objective optimization in Eq. 2. Specifically, for a multi-objective optimization problem, we want to optimize all objective functions, i.e., to minimize both \\(\\widehat{\\mathcal{L}}(\\theta)\\) and \\(\\frac{1}{2} ||\\theta - \\theta_0||^2\\) in our case. However, for a regularization problem, the regularization term does not necessarily decrease. Instead, it acts as a constraint on the original loss function and the regularization term is smaller compared to the one in a model trained without regularization. Projecting the original gradient to the orthogonal direction of the gradient for the regularization term will potentially lead to underfitting. It is especially detrimental at the beginning of the training, where the fine-tuned weights are close to the pre-trained weights, thus it is more benefitial for the model to stick to its original gradient descent direction.\nAs a result, we aim for the projection strength \\(\\omega\\) to be dynamic throughout the training process. Intuitively, \\(\\omega\\) should start small during the early iterations, allowing the model to prioritize fitting to the downstream task. As training progresses and the fine-tuned model diverges further from its initial state, \\(\\omega\\) should gradually increase to guide the fine-tuned gradient direction towards alignment with the regularization gradient direction. In Sec. 5.2 we will visualize the variation of projection strength \\(\\omega\\) throughout training to further validate this motivation.\nTo achieve this, we make the projection strength \\(\\omega\\) trainable, allowing it to adapt throughout the training process. For the t step of unconstrained gradient descent with the learning rate of \\(\\alpha\\), the model weights update as follows,\n\\[\\widetilde{\\theta}_t = \\theta_{t-1} - \\alpha g_{t,1}\\]\nwhere \\(\\widetilde{\\theta}_t\\), \\(\\theta_{t-1}\\) and \\(g_{t,1}\\) denote the unconstrained model weights at current step t, the model updates of previous step t 1 and the gradient for the original loss function at current step t.\nFor one step of directional gradient descent, the model weights update as follows,\n\\[\\theta_t = \\theta_{t-1} - \\alpha (g_{t,1} - \\omega_t \\frac{g_{t,1}^T g_{t,2}}{\\|g_{t,2}\\|^2} g_{t,2}) = \\widetilde{\\theta}_t + \\alpha \\omega_t \\frac{g_{t,1}^T g_{t,2}}{\\|g_{t,2}\\|^2} g_{t,2}\\]\nwhere \\(\\theta_t\\), \\(\\omega_t\\) and \\(g_{t,2}\\) denote the constrained model weights, the projection strength and the gradient for the regularization term at current step t. \\(\\widetilde{\\theta}_t\\) and \\(g_{t,1}\\) are the same as the ones in Eq. 6."}, {"title": "3.4 COMPATABILITY WITH PARAMETER-EFFICIENT FINE-TUNING (PEFT) METHODS", "content": "DiGraP is further compatible with PEFT methods such as LoRA (Hu et al., 2021), which is a prevalent fine-tuning strategy for large foundation models. PEFT methods generally update new parameters to add to the original weights. In this case, instead of optimizing the distance between fine-tuned and pre-trained weights \\(||\\theta - \\theta_0||^2\\), we minimize the distance between the updated weight and origin \\(||\\theta||^2\\) in PEFT. Thus, when combined with PEFT, DiGraP does not need to save an additional pre-trained copy and requires the same amount of memory as PEFT. In Sec. 4.2 and Sec. 4.3, we demonstrate that DiGraP can further improve the results of LoRA on VQA tasks."}, {"title": "4 EXPERIMENTS", "content": "Overview. We test DiGraP on a variety of benchmarks, tasks and architectures to validate its effectiveness. The experiments are split into three sections including image classification (Sec. 4.1), reformulating image classification as VQA tasks (Sec. 4.2) and fine-tuning on VQA datasets (Sec. 4.3). We emphasize that it is important to move robust fine-tuning towards multi-modal models, given their popularity and richness in terms of different types and strenghts of distribution shfit. For Sec. 4.1, we use a discriminative backbone - ImageNet pretrained MOCO-V3 ResNet50 (Chen et al., 2021) as the pre-trained model. We follow the setting of the previous work and further details can be found at Tian et al. (2023a). For Sec. 4.2 and Sec. 4.3, we use a generative backbone - Google's recently released PaliGemma (Beyer et al., 2024) pretrained on a broad mixture of large-scale vision-language tasks, which is lightweight and achieves state-of-the-art performance on VQAv2. In Appendix 7.1, we include additional results with more backbones and a langugae-only experiment.\nDatasets. For Sec. 4.1 and Sec. 4.2, we use DomainNet (Peng et al., 2019) as the benchmark, which consists of six domains (real, sketch, painting, infograph, clipart and quickdraw) with 345 classes. We fine-tune our model on real domain and evaluate on all other domains. For Sec. 4.3, we fine-tune on VQAv2 (Goyal et al., 2017) and test on nine OOD datasets using LoRA (Hu et al., 2021). For the near OODs, we evaluate on VQAv2's six variants, namely IV-VQA (Agarwal et al., 2020), CV-VQA (Agarwal et al., 2020), VQA-Rephrasings (Shah et al., 2019), VQA-CP v2 (Agrawal et al., 2018), VQA-CE (Dancette et al., 2021) and AdVQA (Sheng et al., 2021), which cover uni-modal, multi-modal and adversarial distribution shifts from VQAv2. We also include TextVQA (Singh et al., 2019), VizWiz (Bigham et al.) and OK-VQAv2 (Reichman et al., 2023), which are constructed from different sources than VQAv2, as the far OOD datasets. Further details can be found in Sec. 4.3.\nTraining Details. The hyper-parameter \\(\\mu\\) is found through cross-validation per dataset, and the model with the best ID validation accuracy is taken. We leave all training details to Appendix 7.2"}, {"title": "4.1 IMAGE CLASSIFICATION EXPERIMENTS", "content": "DiGraP outperforms robust fine-tuning baselines on image classification task. We utilize the DomainNet as benchmark and compare DiGraP with several existing methods using ImageNet pre-trained MOCO-V3 ResNet50 as initialization. We follow the training scheme and use the same hyper-parameters of prior work (Tian et al., 2023a). In Tab. 1, we observe that DiGraP achieves the best OOD performance across all OOD domains and a competitive ID performance on the real domain. Specifically, DiGraP outperforms L2-SP (Li et al., 2018) and magnitude-based projection methods (Tian et al., 2023a;b) on both ID and OOD results. Note that we use the reported baseline results from Tian et al. (2023b) where the Quickdraw results are not presented."}, {"title": "4.2 REFORMULATING IMAGE CLASSIFICATION AS VQA TASKS", "content": "Previous work primarily focuses on uni-modal benchmarks but is seldom tested on multi-modal settings. As an additional contribution, we first bridge the gap by using the same benchmark but reformulate it as a VQA task and tested several robust fine-tuning methods. Specifically, inspired by Ging et al. (2024), we change DomainNet to DomainNet-oVQA by using the same images but asking questions such as \"What is in the image?\" with class labels as the ground truth answers. To make the two tasks more comparable, we use the ClipMatch (ClipM) metric from Ging et al. (2024) by embedding the model prediction and class names with EVA-Clip (Sun et al., 2023) and obtain the most similar one by matching them using cosine similarity. We consider this benchmark as one OOD dataset with distribution shifts only in image modality and will conduct a more comprehensive experiments with distribution shifts in other modalities in Sec. 4.3. We use the pre-trained generative vision-language model (VLM) PaliGemma (Beyer et al., 2024) as initialization."}, {"title": "4.3 FINE-TUNING ON VQA DATASETS", "content": "We now conduct a comprehensive experiment on various VQA datasets with different distribution shifts. We consider VQAv2 (Goyal et al., 2017) as the ID dataset. We further evaluate the model on six near OOD datasets which construct different types of distribution shifts from VQAv2 and three far OOD datasets in which the image and text sources are completely different from VQAv2.\nID Dataset. VQAv2 (Goyal et al., 2017) builds upon VQAv1 (Agrawal et al., 2017) by balancing question-answer pairs with complementary images to minimize the bias in language priors, thus is more challenging and is widely used as a benchmark for popular vision-language models.\nOOD Datasets. 1) Distribution Shifts to Images. IV-VQA (Agarwal et al., 2020) and CV-VQA (Agarwal et al., 2020) remove the objects irrelevant to answering the question and generate complementary images with one instance of the object removed respectively. 2) Distribution Shifts to Questions. VQA-Rephrasings (Shah et al., 2019) collects three rephrasings of each question. 3) Distribution Shifts to Answers. VQA-CP (Agrawal et al., 2018) reorganizes the correlation between the question type and correct answer. 4) Distribution Shifts to Multi-modalities. VQA-CE (Dancette et al., 2021) selects a subset of VQAv2 that are counterexamples of potential multi-modal shortcuts. 5) Adversarial Distribution Shifts. AdVQA (Sheng et al., 2021) provides human-adversarial examples for questions where the model's predicted answer is incorrect. 6) Far OODs. TextVQA (Singh et al., 2019) requires models to answer questions by understanding text embedded in images. VizWiz (Bigham et al.) contains user-generated images with diverse challenges like poor quality, ambiguity, and irrelevant content for answering visual questions. OK-VQAv2 (Reichman et al., 2023) represents a knowledge-based VQA task where the visual question cannot be answered without external knowledge.\nEvaluation and Metrics. We follow the metric from VQAv2 (Goyal et al., 2017) and the evaluation is based on the accuracy of predicted answers compared to ground truth human-annotated answers. For each question, the dataset includes 10 human-provided answers. The accuracy is calculated as:\n\\[\\text{Accuracy} = \\frac{\\text{number of humans who gave the answer}}{\\text{total number of humans}}, \\text{min}(\\cdot, 1).\\]\nMeasuring the OOD Distance. We explore shifts on single image modality and joint image question (V+Q) shifts, as well as image question answer (V+Q+A) shifts by computing the test set shift relative to the training domain (i.e. VQAv2 train) using the negative Mahalanobis distance metric. The higher the value, the less the distribution shift. More details are in Appendix 7.3.\nExperimental Results. We fine-tune the PaliGemma-3B model on the VQAv2 dataset with LoRA and evaluate on the other OOD datasets. The results of DiGraP and other robust fine-tuning methods are shown in Tab. 3. We have the following observations.\nSmaller distribution shifts correlate with better OOD performance. The analysis of image and joint shifts reveals a high correlation with VQA performance, evidenced by correlation values of 0.83 and 0.80, respectively. This suggests that larger shifts significantly degrade VQA performance. Such trends validate our methodology in quantifying shifts, as far OOD scenarios align with increased shift levels and diminished performance.\nFull fine-tuning improves zero-shot performance across ID, near OOD, and far OOD datasets. As shown in Tab. 3, we observe no degradation in OOD performance following vanilla fine-tuning, even when PaliGemma is pre-trained on VQA tasks. This may be attributed to reasons similar to"}, {"title": "5 HYPER-PARAMETER TUNING AND ABLATION STUDY", "content": "5.1 VARIATION OF PROJECTION STRENGTH THROUGHOUT TRAINING\nWe visualize the variation of the average projection strength \\(\\omega\\) of all layers over iterations for five different hyper-parameters \\(\\mu \\in \\{0.01, 0.1, 0.5, 1, 100\\}\\) in Fig. 4. As we increase \\(\\mu\\), the projection strength \\(\\omega\\) becomes larger. For all cases, the projection strength \\(\\omega\\) starts from zero and converges at the end of the training. This aligns with our intuition that the projection strength should vary over time to learn dynamic priority of the two objectives during different stage of training."}, {"title": "5.2 IMPACT OF HYPER-PARAMETER SENSITIVITY ON ROBUSTNESS", "content": "We further perform the sensitivity analysis of the hyper-parameter \\(\\mu\\) on ID and average OOD performance for DomainNet-oVQA and VQA experiments. Results from Tab. 4 show that both ID and OOD results fluctuate slightly even when \\(\\mu\\) spans over a wide range from 0.01 to 100. This again proves that DiGrap is more controllable and less sensitive to hyper-parameter change."}, {"title": "5.3 ABLATING FIXED AND TRAINABLE PROJECTION STRENGTH", "content": "To validate the effectiveness of trainable projection strength \\(\\omega\\), we conduct analysis to compare with fixed projection strength with different values \\(\\omega \\in \\{0.1, 0.5, 0.9\\}\\). Tab. 5 shows that trainable DiGraP outperforms the others and achieves the best ID and average OOD results."}, {"title": "6 CONCLUSION", "content": "We present Directional Gradient Projection (DiGraP), a novel method for robust fine-tuning that leverages gradient-based directional information to unify regularization and multi-objective optimization. DiGraP addresses hyperparameter sensitivity and underfitting issues in existing methods. Experiments on image classification and VQA benchmarks show that DiGraP surpasses baselines, improving ID accuracy and OOD robustness, while bridging uni-modal and multi-modal evaluation for robust fine-tuning across domains. However, DiGraP struggles with far OOD datasets due to limited regularization, excelling in near OOD scenarios but facing a trade-off as stronger regularization may harm ID and near OOD performance (Sec. 4.2, 4.3). Future work should balance ID, near OOD, and far OOD performance. Additionally, DiGraP is suited for fine-tuning well pre-trained models, with its efficacy in training from scratch or non-robust initialization yet to be explored."}, {"title": "7 APPENDIX", "content": "7.1 ADDITIONAL RESULTS\n1) More Backbones: CLIP, LLaVA. We include experiments of fine-tuning on DomainNet with CLIP ViT-Base (Radford et al., 2021) (Tab. 8), DomainNet-oVQA (Tab. 7), and VQA (Tab. 6) with LLaVA-7B (Liu et al., 2023). DiGraP consistently achieves the best performance across all experiments.\n2) Language-only Experiments. We also conduct a language-only experiment in Tab. 9, where we use BOSS (Yuan et al., 2023), an NLP benchmark suite designed for OOD robustness evaluation. This suite includes both ID and OOD language-only datasets across multiple tasks (e.g., Sentiment Analysis, Toxic Detection).\n3) LP-FT Controlled Version. In Tab. 10, we conduct an LP-FT-C experiment (a controlled version of LP-FT), similar to TPGM-C in Tian et al. (2023a), by increasing the regularization strength to ensure that the ID performance matches that of DiGraP. Despite this adjustment, DiGraP still outperforms LP-FT-C on OOD datasets."}, {"title": "7.2 TRAINING DETAILS", "content": "Image Classification. For DiGraP, we fine-tune the model using SGD with a learning rate of 1e - 2 and \\(\\mu\\) = 0.1 with a batchsize of 256. The regularization hyper-parameter is found through cross-validation, and the model with the best ID validation accuracy is taken. We use 4 RTX 2080 GPUs for each experiment."}, {"title": "7.3 MEASURING OOD DISTANCE", "content": "We follow procedures similar to typical feature-based OOD detection methods (Shi & Lee, 2024). Specifically, given our input training split \\(X_{\\text{train}}\\), we compute feature representations \\(z\\) of the training samples to estimate the empirical mean \\(\\mu\\) and covariance matrix \\(\\Sigma\\). For each test split, we compute the test set shift relative to the training domain using the Mahalanobis distance metric defined in Eq. 9. The overall shift score for each test dataset, denoted as \\(S_{\\text{Maha}}\\), is calculated as the average \\(S_{\\text{Maha}}\\) across all samples. Let \\(q\\) denote the question, \\(v\\) the image (vision input), and \\(a\\) the answer. The input features used in measuring shifts include uni-modal embeddings \\(f(v)\\), \\(f(q)\\) and joint embeddings \\(f(q, v)\\).\n\\[S_{\\text{Maha}}(z_{\\text{test}}) = \\sqrt{(z_{\\text{test}} - \\mu)^T \\Sigma^{-1} (z_{\\text{test}} - \\mu)}\\]\nWe utilize the vanilla fine-tuned PaliGemma model on the VQAv2 training dataset as our feature encoder. For the image embedding \\(f(v)\\), we obtain it via masking out the question input tokens and mean-pooling the image portion from the final layer of the model before the language model head. Similarly, to get \\(f(q)\\), we mask out the image tokens and extract the question portion from the final layer. To obtain \\(f(v, q)\\), we pass in both image and text tokens as input, compute the average embedding for both modalities and then taking the overall mean."}, {"title": "7.3.1 CORRELATION BETWEEN UNI- & MULTI-MODAL SHIFTS PER DATASET", "content": "Fig. 6 shows the heatmap of the correlation between uni-modal and multi-modal shifts per dataset. Question-joint shift correlations are higher than image-joint shift correlations across all VQA datasets and fine-tuning methods. However, pre-train model maintains similar correlation between both modalities. Vanilla FT and SPD exhibits the lowest question-joint shift correlation shown by the darkest row color across all fine-tuning methods in 6a. Whilst, SPD shows the lowest image-joint shift correlation across the datasets in 6b."}, {"title": "7.3.2 HISTOGRAMS FOR EVALUATING DIFFERENT DISTRIBUTION SHIFTS", "content": ""}]}