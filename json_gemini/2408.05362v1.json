{"title": "MindSpeech: Continuous Imagined Speech Decoding using High-Density fNIRS and Prompt Tuning for Advanced Human-AI Interaction", "authors": ["Suyi Zhang", "Ekram Alam", "Jack Baber", "Francesca Bianco", "Edward Turner", "Maysam Chamanzar", "Hamid Dehghani"], "abstract": "In the coming decade, artificial intelligence systems will continue to improve and revolutionise every industry and facet of human life. Designing effective, seamless and symbiotic communication paradigms between humans and AI agents is increasingly important. This paper reports a novel method for human-AI interaction by developing a direct brain-AI interface. We discuss a novel AI model, called MindSpeech, which enables open-vocabulary, continuous decoding for imagined speech.\nThis study focuses on enhancing human-AI communication by utilising high-density functional near-infrared spectroscopy (fNIRS) data to develop an AI model capable of decoding imagined speech non-invasively. We discuss a new word cloud paradigm for data collection, improving the quality and variety of imagined sentences generated by participants and covering a broad semantic space. Utilising a prompt tuning-based approach, we employed the Llama2 large language model (LLM) for text generation guided by brain signals. Our results show significant improvements in key metrics, such as BLEU-1 and BERT P scores, for three out of four participants, demonstrating the method's effectiveness.\nAdditionally, we demonstrate that combining data from multiple participants enhances the decoder performance, with statistically significant improvements in BERT scores for two participants.\nFurthermore, we demonstrated significantly above-chance decoding accuracy for imagined speech versus resting conditions and the identified activated brain regions during imagined speech tasks in our study are consistent with the previous studies on brain regions involved in speech encoding. This study underscores the feasibility of continuous imagined speech decoding. By integrating high-density fNIRS with advanced AI techniques, we highlight the potential for non-invasive, accurate communication systems with AI in the near future.", "sections": [{"title": "1. Introduction", "content": "An AI model which can effectively translate the thoughts a human is imagining into text or other mediums would revolutionise human-AI communication and communication between humans and computers in general.\nTo produce such an AI model, brain signals of humans imagining thoughts, words and sentences in their mind need to be recorded to supply the AI with the appropriate data to decode. To do this, hardware sensors which can monitor brain activity with high spatiotemporal resolution need to be deployed. Surgically implanted electrodes have shown some potential for speech decoding (Brumberg et al., 2016), but they are invasive. Non-invasive techniques such as fMRI have great potential for decoding neural signals during reading and listening tasks (Huth et al., 2016; Tang et al., 2023; Ye et al., 2024). However, fMRI requires bulky, expensive equipment, which makes this neural recording technology non-portable and more importantly, not under naturalistic conditions. In this work, MindPortal opted to use functional near-infrared spectroscopy (fNIRS) for functional neural imaging due to its portability and cost-effectiveness, monitoring brain oxygenation changes with performance similar to fMRI (Cao et al., 2018; Eggebrecht et al., 2012).\nThere have only been limited recent studies on using fNIRS to study and decode imagined speech such as Cao et al. (2018) and Ryb\u00e1\u0159 et al. (2021) which have explored decoding of semantic content from brain signals. Additionally, Naseer and Hong (2014) applied fNIRS to develop an online binary decision decoding system for responses like \"yes\" and \"no.\" MindPortal's previous work, titled \u2018MindGPT' (Zhang et al., 2024) also demonstrated that an AI model can classify semantically different sentences leveraging fNIRS for neural recording during imagined speech. While these studies highlight fNIRS's potential for semantic decoding, they primarily focus on single-word decoding or sentence classification, limiting their application to more extensive segments of imagined speech. To date, developing an open vocabulary, continuous decoder for imagined speech has remained elusive, and using the portable method of high density fNIRS takes the state of the art one step further towards achieving this goal.\nRecently, large language models (LLMs) have dominated computational language modelling, generating coherent language from text prompts. However, traditional methods separate brain decoding and language generation into two phases, limiting their integration. A recent paper introduced BrainLLM (Ye et al., 2024), a novel approach based on LLM prompt tuning that directly incorporates semantic representations from non-invasive fMRI recordings into the language generation phase. This method eliminates the need for post-hoc candidate selection, enhancing the generation of open vocabulary continuous speech directly from brain signals, outperforming previous models reliant solely on text prompts. This study used fMRI data only from reading and listening tasks, and was not tested on imagined speech data or from other more portable non-invasive neuroimaging methods.\nOur study aims to develop an open vocabulary, continuous decoder for imagined speech high-density fNIRS data. We first introduce a novel paradigm for imagined speech fNIRS data collection, that captures a wide variety of semantic meanings. The novel paradigm prompts participants to imagine sentences of different topics by providing topic words and some additional keywords during the task. The participants are also asked to type out the"}, {"title": "2. Material and Methods", "content": "2.1 Participants\nThis study included a total of 4 participants (male = 3, female = 1), ranging in age from 25 to 28 years, with a mean age of 26.5 years. Participants were required to be fluent in English, free from any known neurological disorders, and not currently undergoing any form of psychological or psychiatric treatment. Participants were also asked to refrain from drinking coffee and make use of any other substance that may alter their mental state before each data collection session. Participants wearing glasses were asked to use contact lenses for data collection for ease when setting up the fNIRS neuroimaging cap. All participants provided informed consent and were compensated for their time. All participants completed the tasks in less than 16 sessions.\n2.2 Experimental Paradigm\nInspired by the related word presentation in Experiment 1 in Pereira et al., 2018, we created a task specifically for continuous imagined speech generation called a 'word cloud' task (Figure 1). Our word cloud task consisted of participants being presented with a selection of words on the screen. Differently to Pereira et al., 2018, participants needed to imagine a sentence from the given words sequentially. The central word in a larger font was the topic word, and the words in a smaller font surrounding the topic word were the keywords related to the topic. During the trial, 3 keywords were selected randomly, one at a time, by providing the word in bold font, each for a period of 7s. As the keyword appeared in the bold font, participants were asked to imagine a sentence relating both the topic word and one of the keywords. Participants were required to use only the last bolded keyword and the topic word not all previously highlighted keywords. After the presentation of all the keywords, the task was completed for the given topic and participants were instructed to type the sentences they imagined relating to the topic to each keyword, using a computer keyboard. This was used as a ground truth for the decoder training. The typing period was not timed, and participants were prompted to move on to the next topic after they had completed all typing. The word\ncloud task aims to tap into participants' imagined speech cognitive function and is distinctly different from silent reading, where participants are asked to read sentences from a screen. In addition, as opposed to our previous work on MindGPT (Zhang et al, 2024), there is no heavy reliance on memorisation in this word cloud task or need for pacing imagined speech words at a specific word/minute rate using a metronome, removing any contamination of other cognitive functions, such as memory or auditory processing, from our paradigm to isolate imagined speech, as well as providing a new way to prompt for imagined speech in participants without much prior training.\n2.3 fNIRS Setup and Data Preprocessing\nIn this experiment, we used a commercially-available Continuous-Wave (CW) high-density 48x48 fNIRS system (NIRx Inc.) for collecting neurovascular data. The montage provided full-head coverage, including 48 sources and 47 detectors (the extra detector is used for the short-distance channels) (Figure 2). The system consists of a total of 388 channels (194 with a light source wavelength of 760 nm and 194 with a light source wavelength of 850 nm), with channel distances ranging from ~21 mm to ~42 mm, and 8 short-distance channels (channel distances: < 10 mm). A sampling rate of 5.9Hz was used. Further details of the experimental setup can be found in our previous work, MindGPT (Zhang et al., 2024).\nBrain data collected during the experiment was streamed through the NIRx acquisition software, Aurora fNIRs (NIRx Medical Technologies LLC), and saved in XDF format. The saved data was then loaded and subjected to a series of preprocessing techniques. For each run, data underwent conversion of raw signals to optical density, detrending, short channel regression correction, motion artefact removal, conversion to haemoglobin concentration using a partial pathlength factor (ppf) of 6, bandpass filtering between 0.01 and 0.7 Hz (Huppert et al., 2009). For an imagined sentence recorded in 7s, the corresponding data has a sequence length of 42 time points by 388 channels (194 oxy-haemoglobin and 194 deoxy-haemoglobin channels).\n2.4 Finite Impulse Response (FIR) model\nA Finite Impulse Response (FIR) model (Dale, 1999) was employed to estimate the brain's hemodynamic response to different stimuli without assuming a specific shape for the response function. Given that the brain's haemodynamic response may vary across individual participants and experimental tasks, a FIR model was estimated for each participant, consisting of two conditions of word cloud and rest. The raw intensity data was converted to optical density and then to haemoglobin concentrations and resampled to 0.5Hz. The FIR model uses a design matrix with columns representing different time lags of the stimulus, allowing the response at each time point following the stimulus to be captured. In a first level design matrix, the hemodynamic response function (HRF) was set to \"fir\" with 5 delays. A cosine drift model with a high-pass filter set at 0.01 Hz was applied to account for low-frequency drifts. The design matrix was created without oversampling. Averaged short channel sequences were used as nuisance regressors in a General Linear Model (GLM) to isolate brain activity from systemic physiology. Individual design matrices were created and estimations were performed for each run, followed by the concatenation of all the results across the runs for each participant. Finally, mixed-effect linear models were used to estimate coefficients separately for each delay, task, and oxy/deoxygenated haemoglobin.\n2.5 Continuous Imagined Speech Decoding Model\nTo decode continuous imagined speech from non-invasive fNIRS brain signals, the decoding model needs to be able to a) extract semantic information from the brain signals, and b) generate legible continuous speech given the semantic information. A recent paper described a novel approach that combines both processes in their BrainLLM model (Ye et al., 2024), which demonstrated satisfactory decoding performances in open fMRI datasets with reading and listening paradigms. Inspired by this model, we adapted the processes and used it to decode imagined speech from our fNIRs data.\n2.5.1 Prompt tuning for foundational LLMs\nTuning-based methodologies have been previously employed to harness the extensive knowledge embedded within large language models (LLMs) for decoding time-series data (Jin et al., 2024, Liu et al., 2023). These approaches generally encompass the processes of segmenting and tokenizing time series signals and associated textual data, followed by fine-tuning the models for specific tasks.\nThe prompt tuning process comprised of the following steps:\n1. Brain recordings were collected during the imagined speech condition, during which participants silently imagined a sentence based on a given topic word and a keyword ('Word Cloud' task, see section 2.2 and Figure 1).\n2. The ground truth sentence, which participants imagined and subsequently typed from memory using a keyboard, was segmented into patches (Figure 3A). The initial segment served as the *context input*, while the subsequent segment served as the *continuation*. Correspondingly, the fNIRS brain recordings associated with the *continuation* segment were extracted. A 6s delay was applied to the fNIRS data, as identified by the FIR model (see section 2.4).\n3. The brain signals underwent processing through a brain decoding model that mapped the time series data to standard LLM embeddings. Simultaneously, the context input text was tokenized and converted into LLM embeddings.\n4. The context input embeddings were concatenated with the brain signal-generated embeddings, forming the prompt input to the LLM (Figure 3B). During training, the brain decoding model was trained to learn the mapping from brain signals to LLM embeddings, while the LLM had its parameters frozen during this process.\n5. The LLM generated the continuation text based on the concatenated prompt input. The predicted continuation was then compared with the ground truth continuation text to facilitate the training of the encoding model.\n2.5.2 Data preparation\nIn our study, Llama2-7b (Touvron et al., 2023, was used as the LLM for text processing and generation. An LLM works by leveraging a deep neural network, typically with billions of parameters, that has been trained on vast amounts of text data to predict and generate human-like text based on input text prompts. It uses this training to understand context, grammar, and semantics, allowing it to generate coherent and contextually-relevant responses.\nFollowing Ye et al., 2024, each ground truth sentence imagined by the participant was divided into segments. Given that the imagined sentences were in general shorter than the original Wikipedia-style reading sentences in Pereira et al., 2018, each sentence was segmented into two parts only (context input and continuation, see Figure 3A). Note that any typed sentence with three words or fewer and their related fNIRS data were excluded from the continuous imagined speech decoder training, as they were considered too short for segmentation. The texts were then tokenized into token IDs with a maximum length of 32 with padding. These token IDs were then converted to Llama2 embeddings afterwards. Additional custom tokens  and  were added to the tokenizer, which were used to separate brain generated embeddings from the context input embeddings. The sample numbers of the brain signals corresponding to the continuation part of the sentence were extracted from the preprocessed fNIRS signals. A fixed delay of 6s was added to the brain data in order to account for BOLD delays (see FIR model section for delay estimation).\nFor permutation inputs, the continuation brain signals from a given sentence were paired with the context input text from another sentence randomly chosen from the participant. This method kept the distribution of the participant's brain signals compared to using random numbers or scrambled brain data. Note that permutation does not refer to the permutation of the time points or channels of the brain signals, but instead the permutation of the sentences.\nAdditional experimental conditions of context text only and brain signal only were also included. Context only refers to where the LLM uses only context input text for generating continuation predictions, and brain signal only refers to where brain signals alone are used for continuation predictions. These conditions followed the same data processing as described for brain and permutation input conditions mentioned above.\nFor all experimental conditions, the LLM embeddings for context input and continuation brain signals for all trials within a participant were stored in a Python pickle file, and were loaded\n2.5.3 Brain encoding model\nThe brain encoding model maps fNIRS data to LLM embeddings. It takes a time-series input of shape [sequence length x number of channels], where sequence length is the number of time points in the brain signals segment corresponding to the continuation part of the sentence, and number of channels is the total number of fNIRS channels of 388.\nA sequence-to-sequence (Seq2Seq) neural network model with transformers was used as the brain encoding model (Sutskever et al., 2014, Vaswani et al., 2017). The Seq2Seq model is a versatile neural network capable of handling variable-length sequences effectively, compared to other simpler deep neural networks. For this study, a transformer model was used for both its encoder and decoder components. Transformers utilise a self-attention mechanism, which allows the model to weigh the importance of different time steps dynamically, regardless of their distance in the sequence. This capability is particularly advantageous for time series data such as fNIRS data, where important patterns and dependencies can occur over various time scales.\nA transformer with a single encoder layer and a fully connected layer was used, with a fixed dropout rate of 0.3. As an encoder, it processes the input sequences of size 388 (number of fNIRS channels), with hidden size of 100. As a decoder, it takes the input sequence of size 100 from the encoder, and outputs sequences of shape 4096 (Llama2 embedding size). During the forward pass, the source sequence was first processed by the encoder, and the encoder's output was subsequently passed to the decoder to generate the final output sequence. The weights of the transformer model were initialised using Xavier uniform distribution for projection and linear weights, while setting biases to zero or a small constant, and applying similar initialization to the linear layers.\n2.5.4 Model training, validation and testing\nThe training process involved two stages, a pretraining step and a main training step. Pretraining was run for 10 epochs before the main training started, with an initial learning rate of 1e-3. The pretraining loss computes the mean squared error (MSE) loss between the brain encoding model's predicted embeddings and the mean value of the ground truth continuation text embeddings. The ground truth texts were converted into embeddings, which were then fused into a mean representation to ensure a constant length. This approach also minimises the probability of information leak from the ground truth text.\nDuring main training, the parameters of the Llama2 LLM were frozen, preserving its inherent knowledge while utilising its ability to generate coherent text outputs given limited input examples (Liu et al., 2023). Instead, the brain encoding model was being trained in an end-to-end pipeline in order to learn to generate LLM embeddings from brain signals. The training function initialised data loaders, where 200 trials were kept out for testing, the rest of the dataset were divided into 80% training and 20% validation. Training batch size was set at 8. Adam optimizer (Kingma et al., 2017) with an initial learning rate of 1e-4 and a learning rate\nscheduler with step=1 was used for training. The model weights are saved if a lower validation loss is achieved in the next epoch, and early stopping is implemented if the validation loss does not improve over 10 epochs. The max number of epochs was set to 50.\nTraining loss in main training was defined as the cross-entropy loss between the Llama2 predicted token logits and the true labels in the continuation text. LLM's predicted token logits were the raw, unnormalized scores output by the LLM for each token in the vocabulary, indicating the likelihood of each token being the next in the sequence, before applying a softmax function to convert them into probabilities. The predicted and true sequences were aligned in lengths, and a mask extracted the portion of the predicted sequence corresponding to the true labels. The loss was then calculated using the cross-entropy function on the filtered logits and true labels, with the `reduction` parameter set to \"mean\" to average the loss over the batch. This process ensures that the loss computation only considers the valid parts of the generated sequence, ignoring padding or other irrelevant tokens. In each training epoch, the gradients were reset to zero, the loss was backpropagated, and gradient norms were clipped to a maximum of 10 before updating the model parameters using the optimizer, with the total loss being accumulated for each batch.\nFinally, the held out test trials were used for text generation and metric calculations. Predicted text tokens were generated given the concatenated prompt input from the context input and the brain predicted embeddings. The tokens were converted back to text using Llama2's tokenizer, where special tokens such as  and  are removed. The LLM predicted text and ground truth text for the continuation part of the sentence are then used as prediction and hypotheses for a series of metric calculations.\n2.5.5 Multi-participant alignment and fine tuning\nWe additionally tried out a method by combining multiple participant's data for training in order to maximise data usage, as well as increasing coverage of the overall semantic map (Figure 3C). Data from multiple participants except for the left out test participant was first aligned for training. This was accomplished by passing the brain data to a ridge regression model that maps the input from 388 channels to 100 features. The ridge regression model consists of multiple linear layers, and processes data based on the given participant ID, so that each participant effectively has a different feature extractor and the resulting features reside in a latent feature space shared by all the training participants. This process aims to extract features and reduce noise from fNIRS signals. The extracted features are used as input to the brain encoding model instead of the full fNIRS signals.\nA leave-one-participant-out test was used, where one participant's data was left out and all other participants' data was used for training. Before testing metrics were calculated, the pre-trained brain encoding model from multiple participants was first finetuned with 100 fine-tune trials from the test participants' data. The data collection time for 1 trial is about 22s (7s for the imagined period, 15s for typing based on a typical typing speed of 40 words/min and the averaged sentence length of 10 words), which means the collection of 100 trials for fine tuning would have taken about 37 minutes to complete if collected in a separate run. The 200 test trials were held out from the finetuning process. The previously saved cache model was loaded and trained with the same procedures and parameters as mentioned in 2.5.4 with the finetune trials, then the test metrics are calculated on the held out test trials.\n2.5.6 Evaluation metrics\nNatural language processing metrics are used to evaluate model performance by comparing the model generated sentences with the ground truth sentences which the participant was imagining. These include BERTScores, a text evaluation metric that assesses the similarity between candidate and reference texts by computing the cosine similarity of their contextual embeddings derived from the BERT model (Zhang et al., 2020). This is further divided into 3 scores - 1) BERT F1, which combines precision and recall to measure accuracy by balancing both false positives and false negatives; 2) BERT P (Precision), a precision metric focusing on the correctness of the positive predictions made by the model; and 3) BERT R (Recall), a recall metric assessing the model's ability to capture all relevant instances. 4) BLEU-1 score, which measures the overlap of n-grams (unigrams) between the generated sentence and the reference, indicating surface similarity (Papineni et al., 2002). 5) METEOR scores, which considers precision, recall, and synonymy, providing a more nuanced similarity score than BLEU (Banerjee & Lavie, 2005). 6) ROUGE L score focuses on the longest common subsequence between the generated and reference sentences, reflecting structural similarity (Lin, 2004). 7) Word Error Rate (WER), which calculates the edit distance between the generated sentence and the reference, indicating accuracy (Klakow & Peters, 2002). These multiple metrics provide a comprehensive evaluation of different aspects of model performance.\n2.6 Imagined Speech Detection Decoder\nIn our previous work with MindGPT (Zhang et al., 2024), we showed that an Extra Trees Classifier (XTC) model was able to differentiate imagined speech from the rest condition with an average accuracy of 66% across four participants (best average accuracy: 71%). Our previous imagined speech paradigm however relied heavily on memory and participant training before data collection. We sought to repeat this test with this new fNIRS dataset of imagined speech data from four participants collected while performing the word cloud paradigm, to show that imagined speech detection using fNIRS is possible without relying on other cognitive functions or need for pacing imagined speech words at a specific word/minute rate using a metronome, removing any contamination of other cognitive functions, such as memory or auditory processing. Therefore, we trained a new XTC model to decode imagined speech from the rest data in fully preprocessed brain signals for each participant from this study. The same parameters and methods as what we used in our previous MindGPT (Zhang et al., 2024) work were used. All models were trained on each participant to determine the best individual accuracy, although results from participants were also averaged to identify overall performance of our models in successfully classifying imagined speech vs rest condition.\n2.7 Imagined speech related brain activations\nIn addition to applying decoding models to the imagined speech data, analyses involving haemodynamic response modelling and GLM-based statistical testing were conducted in order to show imagined speech related activations in the brain. To extend our knowledge of semantic representation in the brain, brain activations from the contrast 'word cloud > rest' were compared across participants."}, {"title": "3. Results", "content": "3.1 Dataset statistics\nStatistics of each participant's total number of topics and sentences completed for the word cloud tasks are summarised in Table 1. Participants completed on average 216 topics in the word cloud task, although the number of topics varied across participants. The average number of words per sentence also varied across participants, where participant 4 had on average the longest imagined sentence length of 10.5 words, while participant 3 had the shortest averaged sentence lengths of 8.67 words. However, participant 1 used the most unique words per sentence of 3.28 words compared to other participants. Summarising language styles using statistics is challenging; however, participants exhibited varied writing styles in addition to differences in sentence lengths and unique word choices. The imagined sentences exhibit a more conversational and informal tone (see Table 3 for examples),\ncharacteristic of spoken language, rather than the structured and formal style typical of Wikipedia-style text in Pereira et al,. (2018).\n3.2 Identifying BOLD delays with FIR models\nTo determine the optimal BOLD delay to use in the imagined speech decoding model training, we employed a FIR model to estimate coefficients across a range of delays. FIR models do not assume a fixed haemodynamic response function, allowing for the estimation of individual haemodynamic responses and accommodating inter-individual differences. The delay with the highest coefficient from these estimations is considered the optimal delay. Figure 4 presents the delay coefficients from a group estimation for all runs within each participant, categorised by word cloud and resting conditions, as well as oxy/deoxygenated haemoglobin (HbO/HbR). For participants 1, 2, and 3, the highest coefficient for HbO occurred at the third delay (equivalent to a delay of 4-6 seconds, given a resampling rate of 0.5 Hz), while for participant 4, it was at the fourth delay (6-8 seconds). The second highest delay for participants 1 and 3 was 6-8 seconds. Based on these findings, a delay of 6 seconds is applied to the fNIRS data to accurately account for the BOLD responses in word cloud tasks. This standardised delay was chosen to maintain consistency across participants and simplify the model implementation, ensuring reliable comparison of results.\n3.3 Imagined speech decoder: individual participant test results\nTable 2 below shows test metrics for imagined speech decoders trained and tested with individual data. The metrics that measure both exact and semantic similarities between the predicted continuation texts as hypotheses, and the ground truth continuation from typed imagined sentences as references. The table heading represents 4 different experimental conditions - 1) Context input only, where only context input text inputs are used for the LLM to generate continuation predictions; 2) Brain signal only, where brain signals without context input are used for continuation predictions; 3) Brain+context input, where brain signals are converted to LLM embeddings and concatenated to context input embeddings, before being inputted to LLM for continuation predictions (the prompt tuning approach); 4) Brain signal Permutation+context only, where brain signals from another permuted trial are paired with context input from the current trial for continuation generation. This condition tests specifically whether the prompt tuning approach with unrelated but distribution conforming brain data improves the quality of generated continuations, acting as a stringent control condition to 3). The table shows both metric values and the corresponding t-test p-values between brain and permutation inputs (experiments 3 and 4). The metrics that are higher for brain compared to permutation inputs in values are highlighted in blue and those that are statistically significant (t-test p<0.05) are highlighted in green and provided in bold.\nUsing either only the context inputs or the brain signals produced generally low test metrics (experiments 1 and 2 in Table 2), however, by combining context inputs with corresponding brain signals (experiment 3) or permuted brain signals (experiment 4), there are sizable improvements across all test metrics (when comparing experiment 1 and 2 with 3 and 4 most t-test p-values < 0.05, except for values marked with \u2020 indicating p>0.05 for comparison with experiment 3, or with \u2021 for comparison with experiment 4 in Table 2). Notably, when comparing generated continuation from brain and permutation inputs with the ground truth continuation (experiments 3 and 4 in Table 2), the exact match metric (BLEU-1) are statistically significantly higher for brain compared to permutation input for 3 out of 4 participants, and the semantic similarity metric (BERT P) are statistically significantly higher for 2 out of 4 participants with brain inputs, with an additional participant approaching significance. For participant 1, BERT P (p = 0.02) and BLEU-1 (p = 0.004) showed significant improvements (see definition in section 2.5.2 Data preparation). For participant 2, BLEU-1 (p < 0.001) and METEOR (p = 0.017) demonstrated significant enhancements, while BERT P (p = 0.09) approached significance, indicating a trend towards improved performance. For participant 4, BERT F1 (p = 0.05), BERT P (p = 0.01), and BLEU-1 (p = 0.027) were significantly better than the permutation. As for participant 3, the metrics BLEU-1, METEOR, ROUGE-L and WER are better than permutation but did not reach statistical significance.\nOverall, these results highlight the capability of our prompt tuning-based continuous imagined speech decoder to leverage brain signals for generating text continuations that are both semantically and exactly similar to the ground truth. The consistent performance across multiple participants, as evidenced by significant improvements in key metrics, shows the robustness and potential of our approach in decoding brain signals for continuous speech decoding.\n3.4 Imagined speech decoder: multi-participant alignment and fine-tuning results\nTable 4 presents the test metrics and statistical significance for the multi-participant alignment and fine-tuning experiment. To address time constraints that prevent participants from covering all topics, combining data from multiple participants can enhance the quantity and diversity of training topics. A simple ridge regression linear model aligns each participant's data to a unified latent feature space shared by all training participants, which is then used as input to the brain decoding model and training procedures described previously. Fine-tuning was conducted with 100 trials from the held-out test participant, followed by test metrics calculation using another 200 left out test trials. The same 4 experimental conditions were used as described in the previous section.\nThe results indicate that participants 2 and 3 achieved significantly higher BERT metrics (precision, recall, and F1 score) when using real brain signals over permutation inputs (experiments 3 and 4), while other metrics were higher but did not reach statistical significance. Participant 3, who did not achieve statistically significant performance when training an individual decoder with their own data, showed improved results when using the model trained with data from other participants. This suggests that combining data from multiple participants potentially enhances the model's ability to generalise across semantic meanings. However, participants 1 and 4 did not exhibit any statistically significant results comparing brain against permutation inputs. Given that participants 1 and 4 had distinct imagined sentence statistics (participant 1 used the most unique words per sentence, and participant 4 had the longest average imagined sentence length) (see section 3.1 for dataset statistics per participant), it is possible that the fine-tuning process could not fully mitigate\nthese variations. For completeness, context input only and brain signals only conditions were also tested (experiments 1 and 2), which showed lower test metrics compared to combined inputs, consistent with results from individually trained decoders. Overall, while multi-participant alignment training combined with fine-tuning shows potential for building a more versatile model, it may still be influenced by individual differences.\n3.5 Imagined speech detection\nOur XTC classification model showed above chance results in distinguishing brain signals from imagined speech from rest (Figure 5). A total of 386, 532, 706, and 710 trials were included for Participant 1, 2, 3, and 4, respectively. The total number of trials consisted of an equivalent number of imagined speech and rest trials (i.e., imagined speech trials = 1\u00bd of total trials, rest trials = 1\u00bd of total trials). Please note that the dataset was balanced to ensure the same number of imagined speech and rest trials. Overall, our XTC model achieved an average accuracy of ~76% (p < 0.001, chance: 50%) when considering averaged accuracies across folds for the 4 subjects included in this test (max accuracy across subjects: 78%, p < 0.001). Our best participant (participant 2) reported a best average accuracy of ~88% across the 3 folds in cross validation (p < 0.001) and max accuracy of ~90% (p < 0.001). The classifier was found to perform worse on participants 1 and 4, although it still performed well above chance (p < 0.001). See Supplementary Table S1 for detailed results.\n3.6 Brain areas underlying imagined speech\nFigure 6 illustrates the surface cortex plots of HbO activations of the contrast 'word cloud > rest' for all four participants included in this study. Stimulus durations of 7s from each word cloud keyword highlight onset was considered. The colour bar represents the z-score values of the contrast. This analysis identified a recurrent pattern of multiple brain regions recruited during imagined speech, including the lateral temporal cortex, the dorsolateral prefrontal"}, {"title": "4. Discussion", "content": "We expanded our previous research MindGPT (Zhang et al., 2024) on the practicality of implementing a high-density fNIRS-based BCI system for deciphering imagined sentences as well as their semantic content. First, instead of classifying a limited number of sentences as in MindGPT, we now focused on developing an open vocabulary, continuous decoder for\nimagined speech fNIRS data. Our study introduced a novel paradigm for collecting high-density fNIRS data on imagined speech by prompting participants with topic words and keywords and having them imagine their own sentences. Ground truth sentences were obtained by having participants type them out on a computer using a keyboard after the imagined period. We then adapted the prompt tuning-based model from Ye et al., (2024) for generating continuous natural text from brain signals. Utilising the LLM Llama2-7b with a custom brain encoding model, brain signal guided inputs demonstrated improved language and semantic similarity in generated text continuation compared to permutation conditions. For individual training, the model with brain inputs achieved statistically significant higher BLEU-1 scores in 3 out of 4 participants as compared with permutation inputs, and significantly higher BERT P scores in 2 out of 4 participants (an additional one approaching significance). For multi-participant alignment, the model trained on multiple participants' data and fine tuned on the held out participant's data achieved statistically significant higher BERT F1/P/R scores in 2 out of 4 participants with brain compared to permutation inputs. Additionally, we were able to surpass our previous results on detecting imagined speech activity versus resting brain activity and achieved a 10% increase in decoder accuracy, and identified brain activations consistent with imagined speech research. Our findings suggest that using high-density fNIRS together with sophisticated artificial intelligence methods like LLM and prompt tuning could elevate prospects for the development of effective human-AI interaction, an interesting potential unveiled through this work.\nImagined speech paradigms in previous studies have predominantly focused on single words (Naseer and Hong, 2014), concept categories (Cao et al., 2018; Ryb\u00e1\u0159 et al., 2021), or memorised paragraphs (Tang et al., 2023; Zhang et al., 2024). These designs may not be optimal for data collection"}]}