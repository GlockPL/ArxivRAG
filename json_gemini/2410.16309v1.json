{"title": "In-the-loop Hyper-Parameter Optimization for LLM-Based Automated Design of Heuristics", "authors": ["Niki van Stein", "Diederick Vermetten", "Thomas B\u00e4ck"], "abstract": "Large Language Models (LLMs) have shown great potential in automatically generating and optimizing (meta)heuristics, making them valuable tools in heuristic optimization tasks. However, LLMs are generally inefficient when it comes to fine-tuning hyper-parameters of the generated algorithms, often requiring excessive queries that lead to high computational and financial costs. This paper presents a novel hybrid approach, LLaMEA-HPO, which integrates the open source LLaMEA (Large Language Model Evolutionary Algorithm) framework with a Hyper-Parameter Optimization (HPO) procedure in the loop. By offloading hyper-parameter tuning to an HPO procedure, the LLaMEA-HPO framework allows the LLM to focus on generating novel algorithmic structures, reducing the number of required LLM queries and improving the overall efficiency of the optimization process.\nWe empirically validate the proposed hybrid framework on benchmark problems, including Online Bin Packing, Black-Box Optimization, and the Traveling Salesperson Problem. Our results demonstrate that LLaMEA-HPO achieves superior or comparable performance compared to existing LLM-driven frameworks while significantly reducing computational costs. This work highlights the importance of separating algorithmic innovation and structural code search from parameter tuning in LLM-driven code optimization and offers a scalable approach to improve the efficiency and effectiveness of LLM-based code generation.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities in generating, optimizing, and refining algorithms autonomously [Fei et al. 2024; Romera-Paredes et al. 2024; van Stein and B\u00e4ck 2024], making them powerful tools for various heuristic optimization tasks. The intersection of LLMs with evolutionary algorithms has led to promising advancements in the automatic design of optimization algorithms and heuristics, as frameworks such as FunSearch [Romera-Paredes et al. 2024], Evolution of Heuristics (EoH) [Fei et al. 2024] and Large Language Model Evolutionary Algorithm (LLaMEA) [van Stein and B\u00e4ck 2024] have shown. These models can generate entire algorithmic structures, providing innovative solutions to complex optimization problems. However, one significant limitation of LLM-driven algorithm generation and optimization is the relatively high financial and computational budget required. Either specialized hardware is required to load large enough LLM models locally, or funding is required to use third-party APIs such as OpenAI's ChatGPT. In addition, it was observed already that parts of the evolutionary search using LLMs focuses purely on the fine-tuning of hyper-parameters of the generated (meta)heuristics [van Stein and B\u00e4ck 2024]. The primary tasks which LLMs are trained for and excel in, are the generation of plausible natural language, and thereby also plausible Python code, but they are generally speaking not trained for generating good numerical hyper-parameter settings, and using an LLM to only perform hyper-parameter tuning is very costly.\nThis paper introduces a hybridization of the open source LLaMEA framework with a dedicated Hyper-Parameter Optimization (HPO) procedure, to handle hyper-parameter tuning. By offloading the task of tuning numerical parameters to an HPO tool, the LLaMEA-HPO framework allows the LLM to focus on more creative tasks, such as generating novel algorithmic structures and control flows. This approach significantly reduces the number of LLM queries needed and enhances the performance of the evolutionary process while lowering financial and computational costs.\nThe contributions of this work are threefold:\n\u2022 We propose LLaMEA-HPO, a novel framework that integrates LLM-driven algorithm design with SMAC-based [Lindauer et al. 2022] hyper-parameter optimization, resulting in more efficient code generation and optimization.\n\u2022 We empirically demonstrate that delegating hyper-parameter tuning to SMAC significantly reduces the LLM query budget and computational costs while achieving state-of-the-art performance across various benchmark problems, including Online Bin Packing, Black-Box Optimization, and Traveling Salesperson Problem.\n\u2022 We provide insights into the balance between algorithmic creativity and parameter tuning in LLM-driven frameworks, offering recommendations for future research and practical applications in computationally expensive domains."}, {"title": "2 Related Work", "content": "The combination of Large Language Models (LLMs) and optimization has led to novel advancements in automated heuristic and algorithm generation. LLMs have demonstrated remarkable capabilities in generating, refining, and optimizing algorithms autonomously.\nLLMs have emerged as effective tools in automating the design and optimization of code. The FunSearch framework leverages LLMs to explore function spaces, producing programs for combinatorial optimization tasks such as the cap-set problem and bin packing [Romera-Paredes et al. 2024]. This approach employs an island-based Evolutionary Algorithm (EA) to ensure diversity, while the LLM iteratively generates and refines solutions. Similarly, Algorithm Evolution using Large Language Models (AEL) [Liu et al. 2023] uses LLMs to evolve optimization heuristics, applying crossover and mutation operators to code snippets. The Evolution of Heuristics (EoH) approach [Fei et al. 2024] extends this by treating each algorithm as a population candidate, with LLMs evolving these through heuristic-based mutations. Although these methods show promise, particularly in small-scale instances of problems like the Traveling Salesperson Problem (TSP), their scalability to more complex domains such as continuous optimization remains limited.\nRecent research by Zhang et al. [Zhang et al. 2024] explored the integration of LLMs with evolutionary program search (EPS) methods for automated heuristic design (AHD) in depth. Their work provides a comprehensive benchmark across several LLM-based EPS methods, aiming to assess the impact of combining LLMs with evolutionary search strategies. The study highlights the importance of evolutionary search in improving the quality of heuristics generated by LLMs, as standalone LLMs often fail to achieve competitive results. They also point out the high variance in performance across different LLMs and problem types, suggesting that further improvements in LLM-based AHD require more sophisticated search strategies and prompt designs.\nAnother recent contribution to this field is the LLaMEA (Large Language Model Evolutionary Algorithm) framework [van Stein and B\u00e4ck 2024], which integrates LLMs within an evolutionary loop to automatically generate and optimize metaheuristic algorithms for solving continuous, unconstrained single-objective optimization problems. LLaMEA iteratively evolves algorithms by leveraging LLMs to generate, mutate, and select candidates based on performance metrics and code feedback, such as runtime error tracebacks. By integrating LLMs with benchmarking tools like IOHexperimenter [de Nobel et al. 2024], LLaMEA can systematically evaluate generated algorithms and optimize them for black-box optimization tasks. This framework has shown that LLMs can generate novel metaheuristics that outperform state-of-the-art methods, such as Covariance Matrix Adaptation Evolution Strategy (CMA-ES) and Differential Evolution (DE), on black-box optimization benchmarks. LLaMEA addresses the limitations of previous evolution frameworks with LLMs by automating the generation and evaluation of more complex algorithms. Unlike approaches such as EoH, which are typically limited to generating small heuristic functions, LLaMEA can generate complete optimization algorithms that consist of multiple interacting components, including classes with variables and functions. The framework also supports iterative refinement based on detailed performance feedback, enabling it to adapt and optimize for a wide range of continuous optimization tasks.\nDespite the significant advancements that frameworks like LLaMEA and EoH have introduced in LLM-driven algorithm optimization, they are not without limitations. One of the primary challenges in deploying LLMs within evolutionary computing is the high computational and financial cost associated with LLM inference. Most state-of-the-art approaches rely on third-party API services for querying LLMs, which incurs substantial costs, particularly when the LLM is repeatedly queried in an evolutionary loop. Alternatively, deploying and running LLM models in-house requires significant computational resources and infrastructure, especially for larger models, further adding to the practical burden. This computational overhead makes it difficult to scale these approaches to larger or more complex optimization tasks without significant investment in computational power or financial resources. Another notable limitation is that LLMs, when employed for algorithm generation, tend to spend a significant fraction of LLM-calls on the tuning of the hyper-parameters of the generated algorithms rather than proposing fundamentally novel control flows or algorithmic components [van Stein and B\u00e4ck 2024]. While hyper-parameter optimization (see e.g. [Baratchi et al. 2024] for a comprehensive overview of the state-of-the-art) is an essential aspect of fine-tuning algorithm performance, the reliance on LLMs to merely adjust existing parameters rather than innovate in terms of new algorithmic structures reduces the potential for groundbreaking improvements. This behavior is likely influenced by the training data of LLMs, which is abundant with examples of traditional optimization techniques and their refinements but lacks substantial exposure to novel algorithmic innovation. As a result, many of the generated solutions are either slight modifications of well-known algorithms or tuned versions of previously generated candidates, limiting the diversity and novelty of solutions."}, {"title": "3 Methodology", "content": "These gaps in the current state of LLM-driven evolutionary computing methods serve as the main motivation for the development of LLAMEA-HPO. By integrating Hyper-Parameter Optimization (HPO) techniques within the LLM-driven framework, LLaMEA-HPO aims to offload the task of hyper-parameter tuning from the LLM, allowing it to focus on higher-level tasks such as generating novel control flows and algorithmic components. In this hybrid framework, the LLM generates candidate algorithms, while an HPO tool is employed to optimize their hyper-parameters in the loop (i.e., for each generated metaheuristic), improving efficiency and reducing the computational costs associated with repeated LLM queries for minor tuning tasks. This separation of concerns not only alleviates some of the financial and computational burden but also enhances the LLM's ability to explore new algorithmic structures, leading to more diverse and innovative solutions."}, {"title": "3.1 The LLaMEA Framework", "content": "The LLaMEA framework operates in the following stages [van Stein and B\u00e4ck 2024]:\n(1) Initial Generation: The process begins with an initial candidate algorithm generated by the LLM given a detailed task prompt and one example algorithm. These algorithms are represented as Python code snippets with a short description.\n(2) Evaluation: The candidate algorithm is evaluated based on its performance on a given set of tasks, such as the optimization over different black-box functions. This performance is quantified using a predefined fitness function, typically based on solution quality, computational efficiency, or convergence speed.\n(3) Selection: The best-performing candidate so-far is selected for the next iteration. In the original LLaMEA framework this is a (1+1) evolutionary algorithm, meaning we have only one solution at a given time and we select the best-so-far as the parent individual to continue the search.\n(4) Mutation: The selected candidate undergoes a mutation-like operation performed by the LLM based on a feedback prompt. This prompt notifies the LLM of the attained fitness value of the best-so-far algorithm and asks the LLM to refine the algorithm in order to improve it.\n(5) Termination: This process is repeated for a predefined number of generations or until a termination criterion is met, such as reaching a performance threshold.\nWhile traditional evolutionary algorithms rely on predefined crossover and mutation operators, LLaMEA leverages the LLM to generate novel variations of solutions by simply asking it to mutate (improve) the best-so-far candidate. By incorporating LLM-driven creativity into the evolutionary process to generate and optimize code, LLaMEA is able to explore a broad search space of algorithmic designs, leading to potentially innovative and effective algorithms for a wide range of problems.\nLLaMEA distinguishes itself from Evolution of Heuristics (EoH) in several factors:\n\u2022 LLaMEA uses a self-debugging procedure, feeding back error traces from occuring run-time errors to improve the solutions generated by the LLM.\n\u2022 LLaMEA generated complete Python classes, including class parameters and functions. This gives a lot of flexibility compared to EoH, which can only produce single functions with predetermined in and output variables.\n\u2022 LLaMEA uses a (1+1) evolutionary algorithm, which is known to be a competitive strategy especially under small budget constraints. In addition, it is also a very simple evolutionary algorithm, making it possible to analyse the evolutionary runs straightforwardly."}, {"title": "3.2 Hyper-parameter Optimization", "content": "It is well known that an algorithm's parameterization can have a significant impact on its behaviour and corresponding performance, both in the context of optimization and in machine learning [Bischl et al. 2023; Feurer and Hutter 2019; Hoos 2012]. As such, hyper-parameter optimization (or algorithm configuration) is often a critical step when developing or benchmarking a new algorithm. A wide variety of automated algorithm configuration tools have been developed over the last decades [Baratchi et al. 2024].\nOne could argue that algorithm configuration is already encompassed in the broader question of automated algorithm design. Indeed, determining the setting of an algorithm's parameters is implicitly included in the design space through which e.g. LLaMEA can search, but this would be a rather inefficient use of prompts. As such, As an alternative, we augment the approach of LLaMEA by including a separate hyper-parameter optimization stage before evaluating the LLM's proposed algorithm.\nFor this hyper-parameter optimization step, we make use of SMAC3 [Lindauer et al. 2022]. Since our goal generally is to find an algorithm which works well on a set of problem instances, rather than a single one, we take a portion of our available instances as the training set that SMAC can use during its search. After a given budget of algorithm runs is exhausted, SMAC's final incumbent solution is evaluated in the full set of problem instances to achieve a consistent score for each algorithm proposed by the LLM.\nIn order to run this hyper-parameter optimization procedure effectively, we need to define a valid search space for the algorithm's hyper-parameters. In our case, we generate this by modifying the LLaMEA prompt to provide an example configuration space (in the format of the ConfigSpace package [Lindauer et al. 2019]) and formatting the requested response to include this configuration space for the proposed algorithm."}, {"title": "3.3 LLaMEA-HPO", "content": "The proposed LLaMEA-HPO procedure builds upon the existing open-source LLaMEA framework\u00b9 by integrating a specialized Hyper-Parameter Optimization (HPO) component, in this case SMAC, to improve the efficiency of LLM-driven evolutionary code generation. As depicted in Figure 1 and highlighted in Algorithm 1, the primary innovation in this hybrid approach is the offloading of hyper-parameter tuning from the LLM to an HPO method. This hybridization allows the LLM to focus on generating and mutating algorithmic structures while the HPO component handles parameter tuning, reducing computational overhead and the number of costly LLM queries.\nThe LLAMEA-HPO procedure follows the key steps outlined below:\n1. Initial Task Prompt and Algorithm Generation: The process begins by providing the LLM with a detailed task prompt S, which specifies the optimization task to be solved and an example of a candidate algorithm (one-shot prompting). In this step, the LLM generates an initial algorithm and, crucially, also provides a configuration space for its hyper-parameters in the form of a Python dictionary. This dictionary is essential for the subsequent HPO step. The task prompt S follows the following structure:"}, {"title": "4 Experimental Setup", "content": "In this work, we utilize three heuristic optimization problems as benchmarks: Online Bin Packing, Traveling Salesperson Problem (TSP) using Guided Local Search (GLS) and black-box optimization using a large variety of noiseless continuous functions. These problems represent challenging combinatorial and black-box optimization scenarios often used to evaluate the performance of (meta)heuristic-based approaches.\nIn each of these experiments we look at the fitness over two different measures of (computational) cost. The first is the number of LLM prompts or queries required by the LLM-driven optimization framework, and the second measure is the number of full benchmark evaluations. With a full benchmark evaluation we mean the validation of one generated (meta)heuristic on all training instances of the specific problem we are trying to solve. The proposed LLaMEA-HPO procedure uses instance-based hyper-parameter optimization inside the LLM-driven optimization loop, meaning that for each LLM query we have several of these benchmark evaluations, depending on the HPO budget we use. To explain this further by a concrete example, for the black-box optimization benchmark we are using 216 problem instances (24 functions, 3 instances per function and 3 random seeds), evaluating an generated metaheuristic on all these 216 problems would count as 1 complete benchmark evaluation. Since we are using 2000 instance evaluations as the budget of our hyper-parameter optimization procedure, we use per iteration  $\\frac{2000}{216} = 9.25$ full benchmark evaluations. Depending on the cost of doing the evaluation and querying the LLM for solving real-world problems, one might prefer more LLM queries versus less evaluation time or the other way around. In this work we primarily aim to reduce the number of LLM queries required, but for fairness and completeness we also report the evaluation time required."}, {"title": "4.1 Online Bin Packing", "content": "The Online Bin Packing problem involves a sequence of items with varying sizes that must be packed into the minimum number of fixed-sized bins with capacity C. The challenge is to assign each item to a bin without knowing the future sequence of items, thus requiring an efficient, real-time decision-making process. We use here the online scenario from [Seiden 2002].\nEvaluation. The evaluation instances used during the LLaMEA evolution loop are 5 Weibull instances of size 5k with a capacity of 100 (as in [Romera-Paredes et al. 2024]). The fitness value is set as the average  $\\frac{lb}{n}$ over all instances, where lb is the lower bound of the optimal number of bins [Martello and Toth 1990] and n the number of bins required by the heuristic solution.\nBaselines. We evaluate our proposed approach here against the Evolution of Heuristics (EoH) approach, where the authors also use this benchmark with the exact same settings. In addition we also test our proposed approach against a vanilla LLaMEA algorithm using 1000 evaluations and LLM queries as budget. For specifics about their hyper-parameter settings for the EoH algorithm we refer to their work [Fei et al. 2024]. In turn, EoH already shows to outperform FunSearch [Romera-Paredes et al. 2024] and human hand-crafted heuristics. Both frameworks use the gpt-40-2024-05-13 LLM [OpenAI 2023b]."}, {"title": "4.2 Black-Box Continuous Optimization using the BBOB Suite", "content": "The second benchmark used in this study is the Black-Box Continuous Optimization, specifically utilizing the 24 noiseless benchmark functions from the well-established BBOB suite [Hansen et al. 2009]. These functions cover a wide range of problem characteristics, enabling a thorough evaluation of an algorithm's performance across diverse optimization landscapes.\nThe BBOB suite includes separable functions, functions with moderate and high conditioning, as well as unimodal and multimodal functions with varying global structure. This diversity challenges algorithms to effectively balance exploration and exploitation while navigating different topologies and constraints.\nWe use the BBOB benchmark function suite [Hansen et al. 2009] within IOHexperimenter [de Nobel et al. 2024] in the same experimental setup as performed in [van Stein and B\u00e4ck 2024]. In our experiments we also set the dimensionality of the optimization problems to d = 5.\nEvaluation. To comprehensively evaluate the performance of the generated algorithms across the full set of BBOB benchmark functions, we employ an anytime performance measure. This measure assesses the performance of an optimization algorithm over the entire budget, rather than focusing solely on the final objective function value. Specifically, we use the normalized Area Over the Convergence Curve (AOCC) [L\u00f3pez-Ib\u00e1\u00f1ez and St\u00fctzle 2014] also used in the experimental setup of [van Stein and B\u00e4ck 2024]. The formulation for the AOCC is provided in Equation (1).\n$AOCC(ya,f) = \\frac{1}{B} \\sum_{i=1}^{B} \\frac{min(max(y_i, lb), ub) \u2013 lb}{ub - lb}$ (1)\nIn this equation, ya,f represents the series of the best log-scaled precision values, i.e., the differences log(yi - f*) between the observed function value yi and the global minimum f* for the corresponding function, obtained throughout the optimization process of algorithm a on the test function f. Here, yi is the i-th element of the sequence, B = 10 000 denotes the budget, and lb and ub are the lower and upper bounds defining the range of function values of interest. The standard bounds applied are lb = 10-8 and ub = 102.\nIn line with established practices [Hansen et al. 2022], these precision values undergo logarithmic scaling prior to the AOCC computation. The AOCC can also be interpreted as the area under the Empirical Cumulative Distribution Function (ECDF) curve, considering an infinite number of target values within the specified bounds [L\u00f3pez-Ib\u00e1\u00f1ez et al. 2024].\nTo aggregate the AOCC scores over all 24 benchmark functions in the BBOB suite, we compute the mean across functions and their instances. For an algorithm a, the aggregation is given by\n$AOCC(a) = \\frac{1}{3 \\cdot 24} \\sum_{i=1}^{24} \\sum_{j=1}^{3}AOCC(ya,f_{ij})$, (2)\nwhere fij refers to the j-th instance of the i-th benchmark function. Finally, the overall mean AOCC score, averaged over k = 3 independent runs of algorithm a across all BBOB functions, is used as the feedback to the LLM in the next stage of optimization. This aggregated score is treated as the best-so-far solution if an improvement is found. In formal terms, the performance metric f(a) used in Algorithm 1 is defined as\n$f(a) = \\frac{1}{k} \\sum_{i=1}^{k}AOCC(a)$. (3)\nAdditionally, any runtime or compilation errors encountered during the validation phase are considered in the feedback to the language model. In the case of critical errors that prevent execution, the mean AOCC score is set to the minimum value, which is zero.\nBaselines. Here we compare against the open-source LLaMEA [van Stein and B\u00e4ck 2024] framework and the EoH [Fei et al. 2024] algorithm for generating metaheuristics. EoH is adapted slightly, as explained in [van Stein and B\u00e4ck 2024] in order for it to work in this task. EoH uses the LLM gpt-40-2024-05-13 [OpenAI 2023b] and the LLaMEA baseline uses gpt-4-turbo-2024-04-09 [OpenAI 2023a], LLaMEA with GPT-40 results are also available in [van Stein and B\u00e4ck 2024] but this version of LLaMEA was performing less stable. Note that GPT-4-Turbo is more expensive to use than GPT-40 (at the time of writing), GPT-40 is only 20% of the costs of GPT-4-turbo. Since we propose to offload numerical optimization from the LLM evolution loop in order to preserve costs and have an as-efficient as possible LLM-driven search algorithm, we decided to use the cheaper GPT-40 as the only model we consider for the proposed approach in this work."}, {"title": "4.3 Traveling Salesperson Problem (TSP) with Guided Local Search", "content": "The TSP [Matai et al. 2010] is a well-known NP-hard problem where the goal is to determine the shortest possible route that visits each city exactly once and returns to the origin city. For this benchmark, we incorporate Guided Local Search (GLS), which enhances the basic local search technique by penalizing frequently used edges in sub-optimal tours. GLS is designed to escape local optima by dynamically adjusting penalties based on the frequency of edge usage, encouraging the exploration of new paths. The to be generated heuristic is a function that determines this edge costs to guide the local search algorithm.\nEvaluation. For evaluation we use 64 TSP100 instances (TSP instances with 100 locations). The locations are randomly sampled from [0, 1]2 as in [Kool et al. 2018]. As fitness score we calculate the average gap from the optimal solutions (in percentages), the optimal solutions are generated by Concorde [Applegate et al. 2009].\nBaselines. We use a similar comparison as presented in [Fei et al. 2024], where a wide range of the state-of-the-art methods is compared. This includes; Graph Convolutional Network (GCN) method for TSP [Joshi et al. 2019]. Attention Model (AM) [Kool et al. 2018], GLS [Voudouris and Tsang 1999], the vanilla version of guided local search for TSP, EBGLS [Shi et al. 2018], KGLS [Arnold and S\u00f6rensen 2019], GNNGLS [Hudson et al. 2021] and NeuralGLS [Sui et al. 2024]. In addition we compare to the heuristics found by the EoH algorithm for three independent runs (available open-source on the EoH repository [Fei et al. 2024]), where we took the results from these runs after 100 LLM prompts, and after the full 2000 prompts used in these runs. Final evaluation of the generated heuristics is on a set of 3000 instances, 1000 per problem size for sizes 20, 50 and 100. In addition, we evaluate these heuristics on all Euclidean TSPlib [Reinelt 1991] problems. Both LLaMEA-HPO and EoH frameworks use the gpt-40-2024-05-13 [OpenAI 2023b] LLM."}, {"title": "5 Results and Discussion", "content": "5.1 Online Bin Packing\nFigure 2 illustrates the convergence behavior of the LLaMEA-HPO algorithm compared to the EoH and vanilla LLaMEA algorithms on the Online Bin Packing Problem (BP). The individual runs are represented by dotted lines, while the solid lines depict the average convergence over these runs. Standard deviation is indicated by the shaded areas. On the left side of the figure, the convergence is shown with respect to the number of LLM prompts used, while the right side displays convergence in terms of the number of full benchmark evaluations. A full benchmark evaluation means that all the training instances (in the case of BP, there are 5 Weibull 5k instances with a capacity of 100) are evaluated exactly once. Since in LLaMEA-HPO we perform additional instance based hyper-parameter optimization, each LLM iteration in LLaMEA-HPO is using 10 full benchmark evaluations. The HPO part of LLaMEA-HPO uses a total of 40 instance evaluations, with the minimum number of instances for comparing is set to 1 and the maximum set to 4. The LLaMEA-HPO shows a more efficient convergence in terms of LLM queries used for the total budget of 100 LLM queries. On the other hand, the proposed LLaMEA-HPO algorithm does not reach the same fitness value as EoH or LLAMEA after a full run of 1000 benchmark evaluations (and in case of EoH that means 2000 LLM prompts while for LLAMEA it means 1000 LLM prompts, as for every solution EoH is quarrying the LLM twice). So while LLaMEA-HPO shows potential when we have a small LLM budget, it requires additional benchmark evaluations for the Online Bin Packing problem. It is also interesting to note that the vanilla LLaMEA version shows large differences between the runs, this can probably be explained by the (1+1) strategy it is using, larger populations would reduce this instability. LLaMEA-HPO does not seem to suffer from that, likely due to the hyper-parameter optimization part."}, {"title": "5.2 BBOB 5D", "content": "Figure 3 presents the convergence curves of the mean AOCC (Area Over the Convergence Curve) for the LLaMEA-HPO algorithm and the EoH algorithm on 5d BBOB problems. The results show that LLAMEA-HPO consistently achieves better performance than EoH, with higher mean AOCC values, as reflected by both the number of LLM prompts and full benchmark evaluations. In this use-case, we gave the HPO part a budget of 2000 instance evaluations and as such the proposed LLaMEA-HPO algorithm requires  $\\frac{2000}{216} = 9.25$ full benchmark evaluations (consisting of 9.24 = 216 instances) per LLM query. EoH used 1800 LLM queries for the same 900 benchmark evaluations (right side of Figure 3). Note that the competing LLaMEA algorithm without HPO is using a 5 times more expensive LLM (GPT-4-Turbo). It is interesting to note that the proposed method achieves state-of-the-art performance after a very few LLM queries, and it finds better algorithms after the first 20 queries than the EoH algorithm is able to find after 1800 LLM queries.\nNext we compare the performance of the algorithms found by LLAMEA-HPO before and after their hyper-parameter optimization procedure to see the extend in which HPO matters.\nIn Figure 4 we can observe that the algorithms generated by the LLM with generated default hyper-parameters have a mean AOCC between 0.25 and 0.50 (purple lines), their optimized counterparts, denoted in blue, show average AOCC scores between 0.45 and 0.52. On average the difference between non-optimized and optimized hyper-parameters is around 0.2 in AOCC score, which is a significant difference. This underscores the effectiveness of the proposed hybridization. It also shows that much of the LLM-driven optimization in the baselines LLaMEA (without HPO) and EoH is actually the tuning of the different hyper-parameters, as also earlier observed in code diffs between parents and offspring in [van Stein and B\u00e4ck 2024].\nNext, we compare the resulting metaheuristic algorithms from three independent runs of LLaMEA-HPO with the best metaheuristic algorithm discovered in the original LLaMEA paper [van Stein and B\u00e4ck 2024], called \"ERADS_QuantumFluxUltraRefined\". For this we use the Glicko2 score procedure [Glickman 2012]. In this procedure there are a number of games per BBOB function, in this case 200. In every round, for every function of the dataset, each pair of algorithms competes. This competition samples a random budget value for the provided runtime (with a maximum of 10 000). Whichever algorithm has the better function value at this budget wins the game. Then, from these games, the glico2-rating is used to determine the ranking."}, {"title": "5.3 Traveling Sales Person", "content": "In Figure 6 we can observe the convergence curves for the proposed LLAMEA-HPO against the EoH and LLaMEA baselines. In both LLM queries and complete benchmark evaluations, we can observe that the proposed approach is more efficient and finds better heuristics after only a fraction of the evaluations and prompts required by EoH or LLaMEA. Since LLaMEA uses one prompt per iteration and EoH two, we can observe a faster convergence for vanilla LLaMEA on the left plot in comparison with EoH and a slower convergence in terms of benchmark iterations in the right plot. For the TSP problem, we use a maximum of 256 instance evaluations for hyper-parameter-optimization, since the total training benchmark contains 64 instances, we use 5 evaluations of the complete benchmark in each LLM iteration.\nNext we compare the performance of the final best heuristics found by the two procedures on the full test dataset of 3000 instances. The results of this evaluation can be found in Table 2. We compare in this table against the heuristics found by EoH after 100 LLM queries and the full 2000 LLM queries. The heuristics found by LLAMEA-HPO were found after using only 20 LLM queries since the optimization on the training instances already converged after this number of iterations.\nIt is interesting to observe that only the TSP100 instances have a gap above 0.000 for both EoH-2000 and LLaMEA-HPO discovered heuristics. Their performance is roughly on-par for these test instances. It can also be observed that most likely these instances and the training instances used in the evolutionary search procedures, are not representative and challenging enough for real world TSP problems and likely causes a form of over-fitting. We therefore also validate the generated heuristics on a large set of test instances from the commonly used TSP-lib archive [Reinelt 1991]. The results on this wide-variety of TSP instances with sizes up to 6000 are provided in Table 3. Using the Wilcoxon-holm rank for each of the generated heuristics we can calculate the critical difference diagram given in Figure 7. From this diagram and from the ranks given in Table 3, we can conclude that most of these heuristics are not significantly different from one-another, with the exception of EoH-100-1 (the heuristic generated by EoH after 100 evaluations on the first independent run), which is clearly worse than the others. It is interesting to note that the proposed LLaMEA-HPO algorithm only required a fraction of the LLM budget that EoH required to find similar performing heuristics."}, {"title": "6 Conclusions and Outlook", "content": "This paper presented the hybridization of the open source LLaMEA framework [van Stein and B\u00e4ck 2024] with a specialized Hyper-Parameter Optimization (HPO) algorithm, SMAC3 [Lindauer et al. 2022], to enhance the efficiency of LLM-driven evolutionary code optimization. Our experimental results demonstrate that offloading the hyper-parameter tuning task from the LLM to an HPO procedure significantly reduces the number of LLM queries required while maintaining high solution quality. This is a crucial development in balancing computational efficiency with the computational and financial costs of querying LLMs.\nOne of the main findings is that LLMs are most effective when tasked with generating novel algorithmic structures and control flows rather than being utilized for simple hyper-parameter tuning. LLaMEA-HPO improves the convergence of solutions across various benchmarks, such as the Online Bin Packing, Black-Box Optimization, and the Traveling Salesperson Problem, while using significantly fewer LLM queries compared to existing approaches like EoH. By delegating hyper-parameter tuning to SMAC, our hybrid approach reduces the computational overhead associated with repeated LLM queries, without sacrificing the quality of the generated heuristics. This separation of concerns allows the LLM to focus on the creative aspects of optimization, further enhancing the diversity of solutions.\nLooking ahead, future work could explore additional synergies between LLMs and more advanced HPO techniques. One promising direction is to investigate ways of dynamically adjusting the HPO budget based on the complexity of the generated algorithm, thereby further optimizing resource allocation. Additionally, expanding the framework to more diverse problem domains could further validate the generalizability of the hybrid approach. Another direction would be to investigate how the HPO-integrated procedure would affect search strategies with larger populations, such as integrating the HPO routine within EoH or within LLaMEA with larger population sizes.\nIn conclusion, this work underscores the importance of specializing LLM-driven optimization frameworks by offloading numerical tasks like hyper-parameter tuning to dedicated methods, ensuring the best use of computational resources while pushing the boundaries of algorithmic discovery."}, {"title": "A Benchmark Dependent Prompts", "content": "Below are the specific task dependent prompts used by LLaMEA-HPO. These prompts are as close as possible to the prompts used"}]}