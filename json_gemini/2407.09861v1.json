{"title": "Towards Systematic Monolingual NLP Surveys: GenA of Greek NLP", "authors": ["Juli Bakagianni", "Maria Gavriilidou", "Kanella Pouli", "John Pavlopoulos"], "abstract": "Natural Language Processing (NLP) research has traditionally been predominantly focused on English, driven by the availability of resources, the size of the research community, and market demands. Recently, there has been a noticeable shift towards multilingualism in NLP, recognizing the need for inclusivity and effectiveness across diverse languages and cultures. Monolingual surveys have the potential to complement the broader trend towards multilingualism in NLP by providing foundational insights and resources necessary for effectively addressing the linguistic diversity of global communication. However, monolingual NLP surveys are extremely rare in literature. This study fills the gap by introducing a method for creating systematic and comprehensive monolingual NLP surveys. Characterized by a structured search protocol, it can be used to select publications and organize them through a taxonomy of NLP tasks. We include a classification of Language Resources (LRs), according to their availability, and datasets, according to their annotation, to highlight publicly-available and machine-actionable LRs. By applying our method, we conducted a systematic literature review of Greek NLP from 2012 to 2022, providing a comprehensive overview of the current state and challenges of Greek NLP research. We discuss the progress of Greek NLP and outline encountered Greek LRs, classified by availability and usability. As we show, our proposed method helps avoid common pitfalls, such as data leakage and contamination, and to assess language support per NLP task. We consider this systematic literature review of Greek NLP an application of our method that showcases the benefits of a monolingual NLP survey. Similar applications could be regard the myriads of languages whose progress in NLP lags behind that of well-supported languages.", "sections": [{"title": "1. Introduction", "content": "Natural Language Processing (NLP) focuses on the computational processing of human languages, enabling machines to understand and generate natural language. Recently, several NLP tasks have ad-vanced significantly with the help of Deep Learning (DL) (Devlin et al. 2019) and more recently with"}, {"title": "2. Background", "content": "2.1 The Language\n2.1.1 Human Languages. Human languages encompass a rich tapestry, totalling to 7,916, as catalogued by ISO 639-3.3 Despite this linguistic diversity, the landscape of Natural Language Processing (NLP) research remains extremely imbalanced, with English being the dominant language under research in this domain. To validate this observation, we aim to assess the level of support for different human"}, {"title": "2.1.2 The Greek Language.", "content": "Understanding the linguistic characteristics of a language can help NLP researchers understand the specific challenges and opportunities for developing and applying NLP technologies in this language context. Greek, or Modern Greek, to distinguish it from previous stages of the language's history, is the official language of Greece and one of the two official languages of Cyprus. It is the mother tongue of approximately 95% of the 10.5 million inhabitants of Greece and of the approximately 500,000 Greek Cypriots. It is also used by approximately 5 million people of Greek origin worldwide as heritage language (Gavriilidou et al. 2012). The Greek alphabet has been the main script for writing Greek for most of its recorded history. The use of the standard variety in education and mass media has led to the prevalence of standard Modern Greek over various dialects. Henceforth, the term \"Greek\u201d is used to refer to Standard Modern Greek, which is a highly inflected language. It has four cases for the nominal system, two numbers, and three genders. The verb conjugation system is even more complex, with multiple tenses, moods, voices, different suffixes per person, and many irregularities. Word length is an additional factor differentiating Greek from other languages, most notably English. The majority of the Greek words, typically, have two or three syllables, but words with more syllables (e.g., eight or nine) are also not rare (Gavriilidou et al. 2012)."}, {"title": "2.2 The Time Period of the Survey", "content": "The selected time span for our survey (2012 to 2022) aimed to capture the evolution of research method-ologies in NLP in response to global technological advancements and shifts in the field. The period under investigation witnessed a transition from Machine Learning (ML) to Deep Learning (DL).5 We aim to explore how this methodological shift influenced research on Greek. In the following sections we present a brief historical overview of the scientific field (i.e., by disregarding the target language) and its evolution over the years under study. The overview of the methods applied to the Greek language is outlined in \u00a74."}, {"title": "2.2.1 The ML Era.", "content": "The predominant approach to NLP research in 2012, marking the beginning of our study period, primarily relied on ML algorithms. ML focuses on developing algorithms and models capable of learning patterns and making predictions or decisions based on data. The key objective was to yield predictive models, learning from seen data and making accurate predictions on unseen data."}, {"title": "2.2.2 The DL Era.", "content": "The surge of DL in NLP can be attributed to its capacity to automatically learn hierarchical representations of data, eliminating the need for extensive feature engineering. This abil-ity, coupled with the vast amounts of data and increased computational power, fueled the rise of DL, enabling more effective handling of complex linguistic structures and yielding superior performance across a spectrum of NLP tasks (Bommasani et al. 2021). These advancements facilitated the usage of Recurrent Neural Network (RNN) models (Pascanu, Mikolov, and Bengio 2013), initially proposed in the 80's (Rumelhart, Hinton, and Williams 1986; Elman 1990; Werbos 1988) for modeling time series. The RNN's network architecture allows for the exploration of temporal correlations between distant elements in the text. Pre-trained Language Models (PLMs) were the next wave of DL. The pre-training task is derived automatically from unannotated data in self-supervised learning. Self-supervised tasks are not only more scalable, depending only on unlabelled data, but they are designed to force the model to predict parts of the inputs, thus making them richer and potentially more useful than models trained on a more limited label space. An additional advantage of PLMs is their ability for transfer learning: a model is trained on a surrogate task (often just as a means to an end) and then adapted to the downstream task of interest via fine-tuning or prompting. Self-supervised learning techniques based on autoregressive language modeling (Dai and Le 2015), which involves predicting the next word given the previous words, gained popularity. These techniques gave rise to models such as GPT (Radford et al. 2018), ELMO (Peters et al. 2018), and ULMFiT (Howard and Ruder 2018). The next wave of self-supervised learning introduced models like BERT (Devlin et al. 2019) GPT-2 (Radford et al. 2019), ROBERTa (Liu et al. 1907), T5 (Raffel et al. 2020), and BART (Lewis et al. 2019), all of which embraced the Transformer architecture (Vaswani et al. 2017). The fascination with the inner workings of these large-scale Transformer-based models has led to the emergence of a trend known as BERTology."}, {"title": "2.2.3 Technological Advancements.", "content": "PLMs led to surprising developments due to their scale. For in-stance, GPT-3 (Brown et al. 2020), with 175 billion parameters compared to GPT-2's 1.5 billion, allows in-context learning, adapting to tasks by the provision of a prompt (a natural language description of the task). This property was neither specifically trained for nor anticipated to arise. An exemplary of PLM application is ChatGPT,7 fine-tuned from GPT-3.5, which applies reinforcement learning from human feedback, aligning Language Models (LMs) with human intent. Furthermore, multimodal PLMs, such as GPT-4 (OpenAI 2023), can handle text and images, performing at human-level. There's also a trend towards unified models that handle various data types. Notable examples in this category include OFA (Wang et al. 2022a), UNIFIED-IO (Lu et al. 2022), FLAVA (Singh et al. 2022), BEiT-3 (Wang et al. 2022b), and others."}, {"title": "2.3 NLP surveys", "content": "2.3.1 Other Greek NLP surveys. Regarding other Greek NLP surveys identified through our search pro-tocol (\u00a73.1), we found one brief survey paper focusing on Greek NLP and three surveys addressing NLP tasks tailored to specific domains. First, Papantoniou and Tzitzikas (2020) provided a brief survey of NLP for the Greek language covering Ancient Greek, Modern Greek, and dialects. This survey included the work of 99 papers published from 1990 to 2020. The authors addressed text, video, and image modalities. For text modality, they presented papers on tasks such as Phonology, Syntax, Semantics, Information Ex-traction (IE), Sentiment Analysis (SA), Argument Mining, Question Answering (QA), Machine Translation (MT), and NLP Applications. For image modality they outlined Optical Character Recognition (OCR), and for video modality, they referred to Lip Reading and Keyword Spotting. Regarding Language Resources (LRs), they presented a limited number of LRs, specifically three online lexica, five online corpora, two downloadable datasets, five tools, and one service.\nThe remaining three surveys are domain-specific. Nikiforos et al. (2021) provided an extensive review of 49 papers published from 2012 to 2020 related to the Social Web in Modern Greek, Greek dialects, and Greeklish script. The NLP tasks covered include Argument Mining, Authorship Attribution, Gender Identification, Offensive Language Detection, and SA. The authors systematically addressed the scientific contributions and unresolved issues of the reviewed papers. They also presented two tools and 21 datasets extracted from the surveyed papers, providing detailed information and links where available. Alexandridis et al. (2021) reviewed 14 papers published from 2014 to 2020 that focus specifically on SA and opinion mining in Greek social media. The authors discussed the methods, tools, datasets, lexical re-sources, and models used for SA and opinion mining in Greek texts. Finally, Krasadakis, Sakkopoulos, and Verykios (2022) surveyed 43 papers related to Legal NLP published from 2012 to 2021. The survey covered tasks such as Named Entity Recognition (NER), Entity Linking, Text Segmentation, Summarization, MT, Rationale Extraction, Judgment Prediction, and QA."}, {"title": "2.3.2 Monolingual NLP surveys in Other Languages.", "content": "Moving beyond Greek, we found that NLP surveys for specific languages are rare. We confirmed this by searching the literature for NLP surveys or overviews that cover a wide range of NLP tasks, similar to our research, for well-referenced and moderately-referenced languages of the 1st and 2nd tiers (\u00a72.1.1). Our search process involved querying Google Scholar for publications between January 2012 and September 2023. We employed a specific query pattern, searching for the name of the language of interest along with the keyword \"Natural Language Processing\", and either \u201csurvey\u201d or \u201coverview\u201d. Notably, we found that for only one out of four well- and moderately-supported languages monolingual surveys for NLP do exist. Specifically, from the six well-referenced languages, only Arabic, a macro-language encompassing various individual varieties, has NLP surveys (Guellil et al. 2021; Darwish et al. 2021; Larabi Marie-Sainte et al. 2019; Al-Ayyoub et al. 2018; Shoufan and Alameri 2015). And from the 25 moderately-referenced languages, seven have surveys, i.e. Hindi (preprint) (Desai and Dabhi 2021), Tamil (Rajendran et al. 2022), Turkish (Oflazer 2014), Finnish (H\u00e4m\u00e4l\u00e4inen and Alnajjar 2021), Greek (Papantoniou and Tzitzikas 2020), Bengali (preprint) (Alam et al. 2021), and Basque (Gonzalez-Dios and Altuna 2022)."}, {"title": "2.3.3 Limitations in Existing NLP Surveys.", "content": "The surveys mentioned above provide valuable insights into the languages they study; however, none disclose the method they used for retrieving the surveyed papers. This lack of transparency hinders the understanding of the criteria and rationale behind the selection of the papers. Additionally, it is unclear whether the NLP tasks presented are comprehensive of the research conducted in the language or if the papers were manually selected to fit the chosen tasks. Similarly, while some surveys provide information about the LRs available for the examined language, it is"}, {"title": "3. Our method", "content": "3.1 Search Protocol\n3.1.1 The Search Strategy. We designed and conducted a thorough search strategy across several rep-utable scientific databases. Our aim was to identify research papers related to Natural Language Process-ing (NLP) for Greek, published from January 2012 to December 2022. The selected databases are: ACL An-thology, a hub for computational linguistics and NLP research; Semantic Scholar, an AI-powered search engine prioritizing computer science and related fields; and Scopus, 10 a globally recognized database. These databases were chosen not only for their reputability but also for their automated publication retrieval capabilities: Semantic Scholar and Scopus offer APIs, while ACL Anthology provides publication metadata in XML format through its GitHub repository.11\nThe search process involved three rounds, with the first two considered core rounds as outlined in Table 1. These rounds were conducted across ACL Anthology, Scopus, and Semantic Scholar on different dates, each using tailored query terms to accommodate the search capabilities of each database. Scopus allows searching in the title, abstract, and the entire paper content (including references), Semantic Scholar searches across the entire paper content, and ACL Anthology limits the search to the title and abstract. Therefore, we focused our search on the language name, i.e., \"Greek\" or \"Modern Greek\", in the title or abstract of the papers and the term \"Natural Language Processing\u201c in the entire paper. This approach was chosen because papers focused on a specific language are likely to mention the language name in these sections, thereby reducing the retrieval of false positive papers (see \u00a714.1). Specifically, Scopus employs Lucene queries, allowing us to search for the language name in titles and abstracts, and the term \"Natural Language Processing\" across the entire paper. Semantic Scholar does not offer specific search area options, so we used combined keywords with the +(AND) operator, initially searching broadly and subsequently filtering results where the language name appeared in the title or abstract. ACL Anthology, being dedicated to NLP, limited the search to the language name in the title or abstract.\nThe third round served as a supplementary phase for quality assurance of our search strategy and to validate the comprehensiveness of the selected query terms during the previous core search rounds. The objectives were two-fold: first, to verify that the selected queries terms retrieved all relevant publications related to NLP research in the Greek language; and second, to address any potential gaps from excluding Google Scholar12 in the core rounds. Despite its widespread usage, Google Scholar was not included in the core rounds due to its lack of an API for automated publication retrieval. During this phase, we cherry-picked specific NLP downstream tasks, and integrated them as additional query terms alongside the language name and the overarching term \u201cNatural Language Processing\u201c in Google Scholar. Further details about this quality assurance check can be found in the Appendix 15."}, {"title": "3.1.2 Filtering.", "content": "We retrieved a total of 1,577 bibliographic records, which reduced to 1,035 after removing duplicates. Each record included metadata such as the title, authors names, abstract, publication date,"}, {"title": "3.1.3 Metadata Extraction.", "content": "In addition to the metadata retrieved in previous steps, such as the number of Google Scholar citations and the publication type, we manually added information about the search process. This information includes the search date, the database queried, and the query used to retrieve the paper. We also classified the papers, distinguishing between research papers, overviews, shared task overviews, and submissions to shared tasks. Furthermore, we added information about the research tasks addressed by the authors (see \u00a73.2.1 for the task taxonomy), the keywords used, and the languages covered. Regarding the Language Resources (LRs) created for each paper, we collected information about their availability (see $3.2.2 for the LR availability taxonomy), annotation type (see \u00a73.2.2 for the LR annotation type classification scheme), size, linguality type (monolingual or bilingual), annotation type (if any), domain, and time coverage."}, {"title": "3.2 The Taxonomies", "content": "3.2.1 The Task Taxonomy. We employed the comprehensive task taxonomy proposed by Bommasani, Liang, and Lee (2023), following a data-driven approach that was aligned with the thematic tracks of the top NLP conference, i.e., ACL (2023).15 These tracks mirror the prevalent areas of study in NLP at the time of writing. Each retrieved publication was assigned to a specific NLP task to which the paper contributed; a list of canonical task names was defined, ensuring consistency. Studies that do not address a canonical task, 16 were also assigned to the task they perform. Subsequently, each task was mapped to the associated conference track, and was assigned to thematic topics as proposed by ACL (2023). Table 2 illustrates the resulting taxonomy of NLP tasks for the Greek language.\nIn specific cases, our taxonomy diverged from the ACL classification. Specifically, we present Au-thorship Analysis separately from SA and Argument Mining, although there is a single ACL track for \"Sentiment Analysis, Stylistic Analysis, and Argument Mining\". This decision was dictated by the fact that Authorship Analysis has attracted increased attention in the NLP community for Greek.\nAdditionally, we have identified studies that do not perform a \"canonical\" NLP task. Consolidation of historical revisions, for example, do not fall under a specific thematic domain, hence were appended under the NLP Applications."}, {"title": "3.2.2 The Language Resource Taxonomies.", "content": "One of our survey objectives was to compile a compre-hensive list of the LRs developed in the reviewed studies, including detailed metadata. This metadata includes the availability of the LR, ensuring it aligns with the FAIR Data Principles - findable, accessible, interoperable, and reusable (Wilkinson et al. 2016). Additionally, we assessed the annotation type of the datasets, which refers to the manner in which resources are annotated, impacting data quality, task suitability, reproducibility, and research transparency.\nAvailability taxonomy. The LRs availability classification scheme is based on three parameters: the pres-ence of a functional URL, valid license information, and a machine-actionable format. We searched for the resources' URLs from the papers in which they were created, without extending our search to other web sources. The scheme, presented in Table 3, classifies LRs availability into four distinct categories. The value \"Yes\" signifies resources with a valid, functional URL and an open license, such as Creative Commons. These datasets and lexica are also in a machine-actionable format (e.g., txt, csv, pkl). The"}, {"title": "4. Machine Learning for NLP", "content": "4.1 Track Description\nMachine Learning for Natural Language Processing (NLP) focuses on the integration of Machine Learn-ing (ML) techniques to enhance the understanding, interpretation, and generation of human language by computers.\nInterpretability and Analysis of Models for NLP is rooted to the rise of Deep Learning (DL), which has changed radically NLP. Neural Network (NN) became the dominant approach. However, their opaque nature poses challenges in interpreting their inner workings, leading to a surge in research dedicated to analyzing and interpreting NN models in NLP (Belinkov, Gehrmann, and Pavlick 2020)."}, {"title": "4.2 Machine Learning for NLP in Greek: Language Models and Methods", "content": "4.2.1 ML vs DL approaches. The predominant approach to Greek NLP research back in 2012 relied pri-marily on ML algorithms. Given the morphological richness of the Greek language, feature engineering was a key step in traditional ML. Typically, a structured pipeline was followed for extracting additional features, such as Part of Speech (POS) tags, lemmas, and/or word stems. Additionally, features such as named entities, dependency trees, and, more recently, word embeddings were often extracted. The majority of the studies in this survey used features derived from frequency, such as n-grams and lexicons (used in 40 studies), or extracted information such as POS tags, lemmas, stems, named entities, or dependency trees (used in 27 studies). Furthermore, most methods that employ word embeddings also use additional features (11 out of 15).\nDL-based approaches in Greek NLP began in 2017 with the introduction of Recurrent Neural Network (RNN)-based methods (Athanasiou and Maragoudakis 2017; Pavlopoulos, Malakasiotis, and Androut-sopoulos 2017; Pavlopoulos et al. 2017) and (CNN)-based methods (Medrouk and Pappa 2017; Pavlopou-los, Malakasiotis, and Androutsopoulos 2017). RNN-based methods became prevalent in Greek NLP while when ML-based was compared to RNN-based approaches, the latter always outperformed the former (Pitenis, Zampieri, and Ranasinghe 2020; Arcila-Calder\u00f3n et al. 2022).\nPre-trained Language Models (PLMs) extend back to the era of the pre-trained non-contextual word embeddings, which were widely used as features for both traditional ML and DL algorithms. Table 5 showcases the publicly accessible Greek PLMs developed for the purposes of the surveyed studies. Proko-pidis and Piperidis (2020) trained fastText (Bojanowski et al. 2017) on newspaper articles and the Greek part of the w2c corpus (presented in \u00a714.4). Similarly, Tsakalidis et al. (2018b) trained Word2Vec (Mikolov et al. 2013b) on political Greek tweets (\u00a78). Subsequently, despite the shift in NLP literature towards contextual word embeddings employing self-supervised learning techniques based on autoregressive language modeling (Dai and Le 2015), these approaches were largely not adopted for Greek NLP, with the exception of the ELMo contextualized embeddings (Peters et al. 2018) used by Bartziokas, Mavropoulos, and Kotropoulos (2020).\nTransformers were used by PLMs in the subsequent wave of self-supervised learning, developed for the Greek language (Table 5). Among monolingual PLMs, GreekBERT (Koutsikakis et al. 2020) quickly became the standard in Greek NLP research, recognized as the state-of-the-art technique in various studies (Koutsikakis et al. 2020; Perifanos and Goutsos 2021; Alexandridis et al. 2022; Rizou et al. 2022; Bilianos 2022; Kapoteli, Koukaras, and Tjortjis 2022). Besides multilingual models also comprising Greek, such as XLM-ROBERTa (XLM-R) (Conneau et al. 2018), other monolingual Transformer-based PLMs for Greek are GreekSocialBERT (Alexandridis et al. 2021), which extends GreekBERT by being further trained on Greek social media data; RoBERTa-based PaloBERT (Alexandridis et al. 2021) and BERTaTweetGR (Perifanos and Goutsos 2021); and GPT-2 fine-tuned in Greek (Papadopoulos, Papadakis, and Matsatsinis 2021)."}, {"title": "4.2.2 Interpretability and Analysis of Models for NLP.", "content": "Studies concerning interpretability and analysis of NN models for Greek NLP are rather multilingual. Ahn and Oh (2021) investigated ethnic bias in BERT models for eight languages, including Greek, focusing on how do these models reflect historical and social contexts. They proposed mitigation methods and highlighted the language-specific nature of ethnic bias. Gar\u00ed Soler and Apidianaki (2021) proposed a method to examine whether PLMs for multiple languages (incl. Greek) have knowledge of lexical polysemy, showing that they do.17 Gonen et al. (2020b) focused on multilingual BERT (mBERT), which is the multilingual version of BERT (Devlin et al."}, {"title": "4.3 Summary of Machine Learning for NLP in Greek", "content": "Historical route. Figure 2 shows the trends of Greek NLP approaches, categorized into traditional ML methods, DL methods, and other non-ML methods, such as rule-based systems. ML methods remained the dominant approach until 2019, with the exception of 2013 when other methods were favoured. From 2017 onwards, researchers began to use and compare both ML and DL approaches. As mentioned in \u00a74.2, in 2017, the first publications employing DL techniques emerged, primarily focusing on RNN-based and CNN-based models, which accounted for approximately 30% of the total papers published that year. Since the release of GreekBERT (Koutsikakis et al. 2020), DL methodologies have surpassed traditional ML approaches. While ML methods still find applications, a significant portion of the studies employing ML techniques, integrate both ML and DL techniques in their research experiments.\nThe Large Language Model (LLM) era. Recently, NLP research is increasingly based on LLMs, with some of the most popular ones being either fully or partially closed-source (Balloccu et al. 2024). Notable examples for Greek include OpenAI's GPT-3.5 and GPT-4.0 (Achiam et al. 2023), which are trained on multilingual data and can therefore process and generate texts in multiple languages, including Greek. Additionally, there are other multilingual LLMs available in open-source environments, such as XLM-R used by Ranasinghe and Zampieri (2021). New LLMs emerge regularly in multilingual and monolingual settings, such as GreekBART (Evdaimon et al. 2023), GreekT5 series of models (Giarelis, Mastrokostas, and Karacapilidis 2024), and Meltemi.19) Although covering all LLMs for Greek is beyond the scope of our study, we highlight the significance of GreekBERT, which has significantly impacted Greek NLP research since its introduction in 2020, leading to a shift from traditional ML to DL approaches."}, {"title": "5. Syntax and Grammar", "content": "5.1 Track Description\nSyntactic processing encompasses various subtasks in Natural Language Processing (NLP) focused on phrase and sentence structure, as well as the relation of words and constituents to each other within a phrase or sentence (Woolf 2010). It involves recognition of sentence constituents, identification of their syntactic roles, and potentially establishment of the underlying semantic structure. It provides valuable features for Natural Language Understanding (NLU) (Cambria et al. 2017), which is discussed in Appendix 15, and it serves as a pre-processing step for more complex NLP tasks, including senti-ment analysis and error correction, among others (Zhang, Mao, and Cambria 2023). Grammatical Error Correction (GEC) is a user-oriented task that aims for automatically correcting diverse types of errors present in a given text, encompassing violations of rules pertaining to morphology, lexicon, syntax, and semantics (Wang et al. 2021). GEC can been used to enhance fluency, render sentences in a more natural manner, and align with the speech patterns of native speakers (Wang et al. 2021)."}, {"title": "5.2 Syntax and Grammar in Greek: Language Models and Methods", "content": "Syntactic Processing in Greek. This task is related to sentence splitting, tokenization, and morphosyntactic processing, such as Part of Speech (POS) tagging, lemmatization, and dependency parsing. Prokopidis and Piperidis (2020) addressed several syntax tasks, using the pre-trained Punkt model (Kiss and Strunk 2006) for sentence splitting and a BiLSTM tagger using the StanfordNLP library (Qi et al. 2019) for POS tagging. Lemmatization involved a lexicon-based approach with a BiLSTM lemmatization model as a fallback for out-of-lexicon words. For dependency parsing, the authors trained a neural attention-based parser (Dozat and Manning 2016) on the Greek Universal Dependencies (UD) treebank (Prokopidis and Papageorgiou 2017). The Greek UD treebank is part of the UD project (Nivre et al. 2016), which offers standardized treebanks that provide consistent and unified annotation practices across languages. Partalidou et al. (2019) conducted POS tagging and Named Entity Recognition (NER) tasks, with the details of their NER system summarized in \u00a77. For POS tagging they used spaCy (Honnibal and Montani 2017), adhering to the UD annotation schema. Additionally, they assessed the model's tolerance towards Out-of-Vocabulary (OOV) words, finding that the model lacked flexibility in handling such instances. Other NLP pipelines widely used by the papers we retrieved are: an ILSP suite of NLP tools (Prokopidis, Georgantopoulos, and Papageorgiou 2011), the Natural Language Toolkit (NLTK) (Bird 2006), polyglot,20 spaCy for Greek, 2122 Stanza (Qi et al. 2020), and UDPipe (Straka and Strakov\u00e1 2017). Additional research in the field of Syntax includes the development of a multilingual sentence boundary detection method based on an incremental decision tree learning algorithm (Wong, Chao, and Zeng 2014). Furthermore,\nSamaridi and Markantonatou (2014) focused on parsing Multi-word expressions (MWEs) with Lexical-Functional Grammar / Xerox Linguistic Environments (LFG/XLEs), extending their analysis beyond traditional syntactic boundaries by incorporating lexical knowledge from lexicons.\nGEC in Greek. Korre, Chatzipanagiotou, and Pavlopoulos (2021) focused on the correction of grammatical errors that vary from grammatical mistakes to punctuation, spelling, and morphology of word. The authors listed 18 main categories of grammatical errors that systems can correct, also developing a rule-based automatic annotation tool for Greek. The tool takes an original erroneous sentence along with its correction as input. Then, it automatically produces an annotation that mainly consists of the error lo-cation and type, as well as its correction. Gakis et al. (2016) created a rule-based grammar checker tool, 23 which analyzes and corrects syntactic, grammatical, and stylistic (i.e., the formal, informal, or oral style of language used) errors in sentences, providing users with error notifications and correction hints. Kavros and Tzitzikas (2022) focused on spelling errors, addressing the issue of misspelled and mispronounced words in Greek. They employed phonetic algorithms to assign the same code to different word variations based on phonetic rules. For example, they successfully grouped \u03bc\u03b7\u0301\u03bd\u03c5\u03bc\u03b1 (correct spelling) with \u03bc\u03c5\u0301\u03bd\u03b7\u03bc\u03b1 (both sounding as /m\u00ednima/). They reported better results compared to stemming and edit-distance approaches. 24"}, {"title": "5.3 Syntax and Grammar in Greek: Language Resources", "content": "Table 6 displays the pertinent monolingual Language Resources (LRs) for this track, where we see that all three resources regarding GEC are publicly available while the one about Syntax is not. Regarding the former, Kavros and Tzitzikas (2022) created word lists, containing words and their misspellings. These misspellings were generated through the addition, deletion, or substitution of a letter, as well as by incorporating words with similar sounds. Korre, Chatzipanagiotou, and Pavlopoulos (2021) developed two datasets, namely the Greek Native Corpus (GNC) and the Greek Wiki Edits (GWE). GNC is comprised of essays written by students who are native speakers of Greek, totalling 227 sentences. Each sentence within this dataset may contain zero, one, or multiple grammatical errors, all annotated with the corre-sponding grammatical error types as defined in the provided annotation schema. On the other hand, GWE consists of sentences extracted from WikiConv (Hua et al. 2018). Each sentence in this dataset includes the original sentence, the edited sentence, the original string that underwent editing, and the specific grammatical error type. The dataset on Syntax is from Gakis et al. (2015), who collected a corpus consisting of 2.05M tokens derived from student essays, literary works, and newspaper articles. They extracted morphosyntactic information automatically for this corpus with the help of a lexicon (Gakis et al. 2012)."}, {"title": "5.4 Summary of Syntax and Grammar in Greek", "content": "Conventionally, syntactic processing was widely used in pre-processing steps for solving higher-level NLP tasks (\u00a74). However,in the era of Deep Learning (DL)-based NLP, syntactic processing is often neglected. Instead, Neural Networks (NNs) are leveraged to implicitly capture syntactic information, surpassing the performance of symbolic methods that rely on manually hand-crafted features. This is also reflected by the number of ACL submissions related to Syntax (i.e., Tagging, Chunking and Parsing),"}, {"title": "6. Semantics", "content": "6.1 Track Description\nThe meaning in language is the focus of Semantics. In the context of Natural Language Processing (NLP)", "examination": "Lexical Semantics", "models": "i) count-based Distri-butional Semantic Models (DSMs```json\n{"}, {"title": "Towards Systematic Monolingual NLP Surveys: GenA of Greek NLP", "authors": ["Juli Bakagianni", "Maria Gavriilidou", "Kanella Pouli", "John Pavlopoulos"], "abstract": "Natural Language Processing (NLP) research has traditionally been predominantly focused on English, driven by the availability of resources, the size of the research community, and market demands. Recently, there has been a noticeable shift towards multilingualism in NLP, recognizing the need for inclusivity and effectiveness across diverse languages and cultures. Monolingual surveys have the potential to complement the broader trend towards multilingualism in NLP by providing foundational insights and resources necessary for effectively addressing the linguistic diversity of global communication. However, monolingual NLP surveys are extremely rare in literature. This study fills the gap by introducing a method for creating systematic and comprehensive monolingual NLP surveys. Characterized by a structured search protocol, it can be used to select publications and organize them through a taxonomy of NLP tasks. We include a classification of Language Resources (LRs), according to their availability, and datasets, according to their annotation, to highlight publicly-available and machine-actionable LRs. By applying our method, we conducted a systematic literature review of Greek NLP from 2012 to 2022, providing a comprehensive overview of the current state and challenges of Greek NLP research. We discuss the progress of Greek NLP and outline encountered Greek LRs, classified by availability and usability. As we show, our proposed method helps avoid common pitfalls, such as data leakage and contamination, and to assess language support per NLP task. We consider this systematic literature review of Greek NLP an application of our method that showcases the benefits of a monolingual NLP survey. Similar applications could be regard the myriads of languages whose progress in NLP lags behind that of well-supported languages.", "sections": [{"title": "1. Introduction", "content": "Natural Language Processing (NLP) focuses on the computational processing of human languages, enabling machines to understand and generate natural language. Recently, several NLP tasks have ad-vanced significantly with the help of Deep Learning (DL) (Devlin et al. 2019) and more recently with"}, {"title": "2. Background", "content": "2.1 The Language\n2.1.1 Human Languages. Human languages encompass a rich tapestry, totalling to 7,916, as catalogued by ISO 639-3.3 Despite this linguistic diversity, the landscape of Natural Language Processing (NLP) research remains extremely imbalanced, with English being the dominant language under research in this domain. To validate this observation, we aim to assess the level of support for different human"}, {"title": "2.1.2 The Greek Language.", "content": "Understanding the linguistic characteristics of a language can help NLP researchers understand the specific challenges and opportunities for developing and applying NLP technologies in this language context. Greek, or Modern Greek, to distinguish it from previous stages of the language's history, is the official language of Greece and one of the two official languages of Cyprus. It is the mother tongue of approximately 95% of the 10.5 million inhabitants of Greece and of the approximately 500,000 Greek Cypriots. It is also used by approximately 5 million people of Greek origin worldwide as heritage language (Gavriilidou et al. 2012). The Greek alphabet has been the main script for writing Greek for most of its recorded history. The use of the standard variety in education and mass media has led to the prevalence of standard Modern Greek over various dialects. Henceforth, the term \"Greek\u201d is used to refer to Standard Modern Greek, which is a highly inflected language. It has four cases for the nominal system, two numbers, and three genders. The verb conjugation system is even more complex, with multiple tenses, moods, voices, different suffixes per person, and many irregularities. Word length is an additional factor differentiating Greek from other languages, most notably English. The majority of the Greek words, typically, have two or three syllables, but words with more syllables (e.g., eight or nine) are also not rare (Gavriilidou et al. 2012)."}, {"title": "2.2 The Time Period of the Survey", "content": "The selected time span for our survey (2012 to 2022) aimed to capture the evolution of research method-ologies in NLP in response to global technological advancements and shifts in the field. The period under investigation witnessed a transition from Machine Learning (ML) to Deep Learning (DL).5 We aim to explore how this methodological shift influenced research on Greek. In the following sections we present a brief historical overview of the scientific field (i.e., by disregarding the target language) and its evolution over the years under study. The overview of the methods applied to the Greek language is outlined in \u00a74."}, {"title": "2.2.1 The ML Era.", "content": "The predominant approach to NLP research in 2012, marking the beginning of our study period, primarily relied on ML algorithms. ML focuses on developing algorithms and models capable of learning patterns and making predictions or decisions based on data. The key objective was to yield predictive models, learning from seen data and making accurate predictions on unseen data."}, {"title": "2.2.2 The DL Era.", "content": "The surge of DL in NLP can be attributed to its capacity to automatically learn hierarchical representations of data, eliminating the need for extensive feature engineering. This abil-ity, coupled with the vast amounts of data and increased computational power, fueled the rise of DL, enabling more effective handling of complex linguistic structures and yielding superior performance across a spectrum of NLP tasks (Bommasani et al. 2021). These advancements facilitated the usage of Recurrent Neural Network (RNN) models (Pascanu, Mikolov, and Bengio 2013), initially proposed in the 80's (Rumelhart, Hinton, and Williams 1986; Elman 1990; Werbos 1988) for modeling time series. The RNN's network architecture allows for the exploration of temporal correlations between distant elements in the text. Pre-trained Language Models (PLMs) were the next wave of DL. The pre-training task is derived automatically from unannotated data in self-supervised learning. Self-supervised tasks are not only more scalable, depending only on unlabelled data, but they are designed to force the model to predict parts of the inputs, thus making them richer and potentially more useful than models trained on a more limited label space. An additional advantage of PLMs is their ability for transfer learning: a model is trained on a surrogate task (often just as a means to an end) and then adapted to the downstream task of interest via fine-tuning or prompting. Self-supervised learning techniques based on autoregressive language modeling (Dai and Le 2015), which involves predicting the next word given the previous words, gained popularity. These techniques gave rise to models such as GPT (Radford et al. 2018), ELMO (Peters et al. 2018), and ULMFiT (Howard and Ruder 2018). The next wave of self-supervised learning introduced models like BERT (Devlin et al. 2019) GPT-2 (Radford et al. 2019), ROBERTa (Liu et al. 1907), T5 (Raffel et al. 2020), and BART (Lewis et al. 2019), all of which embraced the Transformer architecture (Vaswani et al. 2017). The fascination with the inner workings of these large-scale Transformer-based models has led to the emergence of a trend known as BERTology."}, {"title": "2.2.3 Technological Advancements.", "content": "PLMs led to surprising developments due to their scale. For in-stance, GPT-3 (Brown et al. 2020), with 175 billion parameters compared to GPT-2's 1.5 billion, allows in-context learning, adapting to tasks by the provision of a prompt (a natural language description of the task). This property was neither specifically trained for nor anticipated to arise. An exemplary of PLM application is ChatGPT,7 fine-tuned from GPT-3.5, which applies reinforcement learning from human feedback, aligning Language Models (LMs) with human intent. Furthermore, multimodal PLMs, such as GPT-4 (OpenAI 2023), can handle text and images, performing at human-level. There's also a trend towards unified models that handle various data types. Notable examples in this category include OFA (Wang et al. 2022a), UNIFIED-IO (Lu et al. 2022), FLAVA (Singh et al. 2022), BEiT-3 (Wang et al. 2022b), and others."}, {"title": "2.3 NLP surveys", "content": "2.3.1 Other Greek NLP surveys. Regarding other Greek NLP surveys identified through our search pro-tocol (\u00a73.1), we found one brief survey paper focusing on Greek NLP and three surveys addressing NLP tasks tailored to specific domains. First, Papantoniou and Tzitzikas (2020) provided a brief survey of NLP for the Greek language covering Ancient Greek, Modern Greek, and dialects. This survey included the work of 99 papers published from 1990 to 2020. The authors addressed text, video, and image modalities. For text modality, they presented papers on tasks such as Phonology, Syntax, Semantics, Information Ex-traction (IE), Sentiment Analysis (SA), Argument Mining, Question Answering (QA), Machine Translation (MT), and NLP Applications. For image modality they outlined Optical Character Recognition (OCR), and for video modality, they referred to Lip Reading and Keyword Spotting. Regarding Language Resources (LRs), they presented a limited number of LRs, specifically three online lexica, five online corpora, two downloadable datasets, five tools, and one service.\nThe remaining three surveys are domain-specific. Nikiforos et al. (2021) provided an extensive review of 49 papers published from 2012 to 2020 related to the Social Web in Modern Greek, Greek dialects, and Greeklish script. The NLP tasks covered include Argument Mining, Authorship Attribution, Gender Identification, Offensive Language Detection, and SA. The authors systematically addressed the scientific contributions and unresolved issues of the reviewed papers. They also presented two tools and 21 datasets extracted from the surveyed papers, providing detailed information and links where available. Alexandridis et al. (2021) reviewed 14 papers published from 2014 to 2020 that focus specifically on SA and opinion mining in Greek social media. The authors discussed the methods, tools, datasets, lexical re-sources, and models used for SA and opinion mining in Greek texts. Finally, Krasadakis, Sakkopoulos, and Verykios (2022) surveyed 43 papers related to Legal NLP published from 2012 to 2021. The survey covered tasks such as Named Entity Recognition (NER), Entity Linking, Text Segmentation, Summarization, MT, Rationale Extraction, Judgment Prediction, and QA."}, {"title": "2.3.2 Monolingual NLP surveys in Other Languages.", "content": "Moving beyond Greek, we found that NLP surveys for specific languages are rare. We confirmed this by searching the literature for NLP surveys or overviews that cover a wide range of NLP tasks, similar to our research, for well-referenced and moderately-referenced languages of the 1st and 2nd tiers (\u00a72.1.1). Our search process involved querying Google Scholar for publications between January 2012 and September 2023. We employed a specific query pattern, searching for the name of the language of interest along with the keyword \"Natural Language Processing\", and either \u201csurvey\u201d or \u201coverview\u201d. Notably, we found that for only one out of four well- and moderately-supported languages monolingual surveys for NLP do exist. Specifically, from the six well-referenced languages, only Arabic, a macro-language encompassing various individual varieties, has NLP surveys (Guellil et al. 2021; Darwish et al. 2021; Larabi Marie-Sainte et al. 2019; Al-Ayyoub et al. 2018; Shoufan and Alameri 2015). And from the 25 moderately-referenced languages, seven have surveys, i.e. Hindi (preprint) (Desai and Dabhi 2021), Tamil (Rajendran et al. 2022), Turkish (Oflazer 2014), Finnish (H\u00e4m\u00e4l\u00e4inen and Alnajjar 2021), Greek (Papantoniou and Tzitzikas 2020), Bengali (preprint) (Alam et al. 2021), and Basque (Gonzalez-Dios and Altuna 2022)."}, {"title": "2.3.3 Limitations in Existing NLP Surveys.", "content": "The surveys mentioned above provide valuable insights into the languages they study; however, none disclose the method they used for retrieving the surveyed papers. This lack of transparency hinders the understanding of the criteria and rationale behind the selection of the papers. Additionally, it is unclear whether the NLP tasks presented are comprehensive of the research conducted in the language or if the papers were manually selected to fit the chosen tasks. Similarly, while some surveys provide information about the LRs available for the examined language, it is"}, {"title": "3. Our method", "content": "3.1 Search Protocol\n3.1.1 The Search Strategy. We designed and conducted a thorough search strategy across several rep-utable scientific databases. Our aim was to identify research papers related to Natural Language Process-ing (NLP) for Greek, published from January 2012 to December 2022. The selected databases are: ACL An-thology, a hub for computational linguistics and NLP research; Semantic Scholar, an AI-powered search engine prioritizing computer science and related fields; and Scopus, 10 a globally recognized database. These databases were chosen not only for their reputability but also for their automated publication retrieval capabilities: Semantic Scholar and Scopus offer APIs, while ACL Anthology provides publication metadata in XML format through its GitHub repository.11\nThe search process involved three rounds, with the first two considered core rounds as outlined in Table 1. These rounds were conducted across ACL Anthology, Scopus, and Semantic Scholar on different dates, each using tailored query terms to accommodate the search capabilities of each database. Scopus allows searching in the title, abstract, and the entire paper content (including references), Semantic Scholar searches across the entire paper content, and ACL Anthology limits the search to the title and abstract. Therefore, we focused our search on the language name, i.e., \"Greek\" or \"Modern Greek\", in the title or abstract of the papers and the term \"Natural Language Processing\u201c in the entire paper. This approach was chosen because papers focused on a specific language are likely to mention the language name in these sections, thereby reducing the retrieval of false positive papers (see \u00a714.1). Specifically, Scopus employs Lucene queries, allowing us to search for the language name in titles and abstracts, and the term \"Natural Language Processing\" across the entire paper. Semantic Scholar does not offer specific search area options, so we used combined keywords with the +(AND) operator, initially searching broadly and subsequently filtering results where the language name appeared in the title or abstract. ACL Anthology, being dedicated to NLP, limited the search to the language name in the title or abstract.\nThe third round served as a supplementary phase for quality assurance of our search strategy and to validate the comprehensiveness of the selected query terms during the previous core search rounds. The objectives were two-fold: first, to verify that the selected queries terms retrieved all relevant publications related to NLP research in the Greek language; and second, to address any potential gaps from excluding Google Scholar12 in the core rounds. Despite its widespread usage, Google Scholar was not included in the core rounds due to its lack of an API for automated publication retrieval. During this phase, we cherry-picked specific NLP downstream tasks, and integrated them as additional query terms alongside the language name and the overarching term \u201cNatural Language Processing\u201c in Google Scholar. Further details about this quality assurance check can be found in the Appendix 15."}, {"title": "3.1.2 Filtering.", "content": "We retrieved a total of 1,577 bibliographic records, which reduced to 1,035 after removing duplicates. Each record included metadata such as the title, authors names, abstract, publication date,"}, {"title": "3.1.3 Metadata Extraction.", "content": "In addition to the metadata retrieved in previous steps, such as the number of Google Scholar citations and the publication type, we manually added information about the search process. This information includes the search date, the database queried, and the query used to retrieve the paper. We also classified the papers, distinguishing between research papers, overviews, shared task overviews, and submissions to shared tasks. Furthermore, we added information about the research tasks addressed by the authors (see \u00a73.2.1 for the task taxonomy), the keywords used, and the languages covered. Regarding the Language Resources (LRs) created for each paper, we collected information about their availability (see $3.2.2 for the LR availability taxonomy), annotation type (see \u00a73.2.2 for the LR annotation type classification scheme), size, linguality type (monolingual or bilingual), annotation type (if any), domain, and time coverage."}, {"title": "3.2 The Taxonomies", "content": "3.2.1 The Task Taxonomy. We employed the comprehensive task taxonomy proposed by Bommasani, Liang, and Lee (2023), following a data-driven approach that was aligned with the thematic tracks of the top NLP conference, i.e., ACL (2023).15 These tracks mirror the prevalent areas of study in NLP at the time of writing. Each retrieved publication was assigned to a specific NLP task to which the paper contributed; a list of canonical task names was defined, ensuring consistency. Studies that do not address a canonical task, 16 were also assigned to the task they perform. Subsequently, each task was mapped to the associated conference track, and was assigned to thematic topics as proposed by ACL (2023). Table 2 illustrates the resulting taxonomy of NLP tasks for the Greek language.\nIn specific cases, our taxonomy diverged from the ACL classification. Specifically, we present Au-thorship Analysis separately from SA and Argument Mining, although there is a single ACL track for \"Sentiment Analysis, Stylistic Analysis, and Argument Mining\". This decision was dictated by the fact that Authorship Analysis has attracted increased attention in the NLP community for Greek.\nAdditionally, we have identified studies that do not perform a \"canonical\" NLP task. Consolidation of historical revisions, for example, do not fall under a specific thematic domain, hence were appended under the NLP Applications."}, {"title": "3.2.2 The Language Resource Taxonomies.", "content": "One of our survey objectives was to compile a compre-hensive list of the LRs developed in the reviewed studies, including detailed metadata. This metadata includes the availability of the LR, ensuring it aligns with the FAIR Data Principles - findable, accessible, interoperable, and reusable (Wilkinson et al. 2016). Additionally, we assessed the annotation type of the datasets, which refers to the manner in which resources are annotated, impacting data quality, task suitability, reproducibility, and research transparency.\nAvailability taxonomy. The LRs availability classification scheme is based on three parameters: the pres-ence of a functional URL, valid license information, and a machine-actionable format. We searched for the resources' URLs from the papers in which they were created, without extending our search to other web sources. The scheme, presented in Table 3, classifies LRs availability into four distinct categories. The value \"Yes\" signifies resources with a valid, functional URL and an open license, such as Creative Commons. These datasets and lexica are also in a machine-actionable format (e.g., txt, csv, pkl). The"}, {"title": "4. Machine Learning for NLP", "content": "4.1 Track Description\nMachine Learning for Natural Language Processing (NLP) focuses on the integration of Machine Learn-ing (ML) techniques to enhance the understanding, interpretation, and generation of human language by computers.\nInterpretability and Analysis of Models for NLP is rooted to the rise of Deep Learning (DL), which has changed radically NLP. Neural Network (NN) became the dominant approach. However, their opaque nature poses challenges in interpreting their inner workings, leading to a surge in research dedicated to analyzing and interpreting NN models in NLP (Belinkov, Gehrmann, and Pavlick 2020)."}, {"title": "4.2 Machine Learning for NLP in Greek: Language Models and Methods", "content": "4.2.1 ML vs DL approaches. The predominant approach to Greek NLP research back in 2012 relied pri-marily on ML algorithms. Given the morphological richness of the Greek language, feature engineering was a key step in traditional ML. Typically, a structured pipeline was followed for extracting additional features, such as Part of Speech (POS) tags, lemmas, and/or word stems. Additionally, features such as named entities, dependency trees, and, more recently, word embeddings were often extracted. The majority of the studies in this survey used features derived from frequency, such as n-grams and lexicons (used in 40 studies), or extracted information such as POS tags, lemmas, stems, named entities, or dependency trees (used in 27 studies). Furthermore, most methods that employ word embeddings also use additional features (11 out of 15).\nDL-based approaches in Greek NLP began in 2017 with the introduction of Recurrent Neural Network (RNN)-based methods (Athanasiou and Maragoudakis 2017; Pavlopoulos, Malakasiotis, and Androut-sopoulos 2017; Pavlopoulos et al. 2017) and (CNN)-based methods (Medrouk and Pappa 2017; Pavlopou-los, Malakasiotis, and Androutsopoulos 2017). RNN-based methods became prevalent in Greek NLP while when ML-based was compared to RNN-based approaches, the latter always outperformed the former (Pitenis, Zampieri, and Ranasinghe 2020; Arcila-Calder\u00f3n et al. 2022).\nPre-trained Language Models (PLMs) extend back to the era of the pre-trained non-contextual word embeddings, which were widely used as features for both traditional ML and DL algorithms. Table 5 showcases the publicly accessible Greek PLMs developed for the purposes of the surveyed studies. Proko-pidis and Piperidis (2020) trained fastText (Bojanowski et al. 2017) on newspaper articles and the Greek part of the w2c corpus (presented in \u00a714.4). Similarly, Tsakalidis et al. (2018b) trained Word2Vec (Mikolov et al. 2013b) on political Greek tweets (\u00a78). Subsequently, despite the shift in NLP literature towards contextual word embeddings employing self-supervised learning techniques based on autoregressive language modeling (Dai and Le 2015), these approaches were largely not adopted for Greek NLP, with the exception of the ELMo contextualized embeddings (Peters et al. 2018) used by Bartziokas, Mavropoulos, and Kotropoulos (2020).\nTransformers were used by PLMs in the subsequent wave of self-supervised learning, developed for the Greek language (Table 5). Among monolingual PLMs, GreekBERT (Koutsikakis et al. 2020) quickly became the standard in Greek NLP research, recognized as the state-of-the-art technique in various studies (Koutsikakis et al. 2020; Perifanos and Goutsos 2021; Alexandridis et al. 2022; Rizou et al. 2022; Bilianos 2022; Kapoteli, Koukaras, and Tjortjis 2022). Besides multilingual models also comprising Greek, such as XLM-ROBERTa (XLM-R) (Conneau et al. 2018), other monolingual Transformer-based PLMs for Greek are GreekSocialBERT (Alexandridis et al. 2021), which extends GreekBERT by being further trained on Greek social media data; RoBERTa-based PaloBERT (Alexandridis et al. 2021) and BERTaTweetGR (Perifanos and Goutsos 2021); and GPT-2 fine-tuned in Greek (Papadopoulos, Papadakis, and Matsatsinis 2021)."}, {"title": "4.2.2 Interpretability and Analysis of Models for NLP.", "content": "Studies concerning interpretability and analysis of NN models for Greek NLP are rather multilingual. Ahn and Oh (2021) investigated ethnic bias in BERT models for eight languages, including Greek, focusing on how do these models reflect historical and social contexts. They proposed mitigation methods and highlighted the language-specific nature of ethnic bias. Gar\u00ed Soler and Apidianaki (2021) proposed a method to examine whether PLMs for multiple languages (incl. Greek) have knowledge of lexical polysemy, showing that they do.17 Gonen et al. (2020b) focused on multilingual BERT (mBERT), which is the multilingual version of BERT (Devlin et al."}, {"title": "4.3 Summary of Machine Learning for NLP in Greek", "content": "Historical route. Figure 2 shows the trends of Greek NLP approaches, categorized into traditional ML methods, DL methods, and other non-ML methods, such as rule-based systems. ML methods remained the dominant approach until 2019, with the exception of 2013 when other methods were favoured. From 2017 onwards, researchers began to use and compare both ML and DL approaches. As mentioned in \u00a74.2, in 2017, the first publications employing DL techniques emerged, primarily focusing on RNN-based and CNN-based models, which accounted for approximately 30% of the total papers published that year. Since the release of GreekBERT (Koutsikakis et al. 2020), DL methodologies have surpassed traditional ML approaches. While ML methods still find applications, a significant portion of the studies employing ML techniques, integrate both ML and DL techniques in their research experiments.\nThe Large Language Model (LLM) era. Recently, NLP research is increasingly based on LLMs, with some of the most popular ones being either fully or partially closed-source (Balloccu et al. 2024). Notable examples for Greek include OpenAI's GPT-3.5 and GPT-4.0 (Achiam et al. 2023), which are trained on multilingual data and can therefore process and generate texts in multiple languages, including Greek. Additionally, there are other multilingual LLMs available in open-source environments, such as XLM-R used by Ranasinghe and Zampieri (2021). New LLMs emerge regularly in multilingual and monolingual settings, such as GreekBART (Evdaimon et al. 2023), GreekT5 series of models (Giarelis, Mastrokostas, and Karacapilidis 2024), and Meltemi.19) Although covering all LLMs for Greek is beyond the scope of our study, we highlight the significance of GreekBERT, which has significantly impacted Greek NLP research since its introduction in 2020, leading to a shift from traditional ML to DL approaches."}, {"title": "5. Syntax and Grammar", "content": "5.1 Track Description\nSyntactic processing encompasses various subtasks in Natural Language Processing (NLP) focused on phrase and sentence structure, as well as the relation of words and constituents to each other within a phrase or sentence (Woolf 2010). It involves recognition of sentence constituents, identification of their syntactic roles, and potentially establishment of the underlying semantic structure. It provides valuable features for Natural Language Understanding (NLU) (Cambria et al. 2017), which is discussed in Appendix 15, and it serves as a pre-processing step for more complex NLP tasks, including senti-ment analysis and error correction, among others (Zhang, Mao, and Cambria 2023). Grammatical Error Correction (GEC) is a user-oriented task that aims for automatically correcting diverse types of errors present in a given text, encompassing violations of rules pertaining to morphology, lexicon, syntax, and semantics (Wang et al. 2021). GEC can been used to enhance fluency, render sentences in a more natural manner, and align with the speech patterns of native speakers (Wang et al. 2021)."}, {"title": "5.2 Syntax and Grammar in Greek: Language Models and Methods", "content": "Syntactic Processing in Greek. This task is related to sentence splitting, tokenization, and morphosyntactic processing, such as Part of Speech (POS) tagging, lemmatization, and dependency parsing. Prokopidis and Piperidis (2020) addressed several syntax tasks, using the pre-trained Punkt model (Kiss and Strunk 2006) for sentence splitting and a BiLSTM tagger using the StanfordNLP library (Qi et al. 2019) for POS tagging. Lemmatization involved a lexicon-based approach with a BiLSTM lemmatization model as a fallback for out-of-lexicon words. For dependency parsing, the authors trained a neural attention-based parser (Dozat and Manning 2016) on the Greek Universal Dependencies (UD) treebank (Prokopidis and Papageorgiou 2017). The Greek UD treebank is part of the UD project (Nivre et al. 2016), which offers standardized treebanks that provide consistent and unified annotation practices across languages. Partalidou et al. (2019) conducted POS tagging and Named Entity Recognition (NER) tasks, with the details of their NER system summarized in \u00a77. For POS tagging they used spaCy (Honnibal and Montani 2017), adhering to the UD annotation schema. Additionally, they assessed the model's tolerance towards Out-of-Vocabulary (OOV) words, finding that the model lacked flexibility in handling such instances. Other NLP pipelines widely used by the papers we retrieved are: an ILSP suite of NLP tools (Prokopidis, Georgantopoulos, and Papageorgiou 2011), the Natural Language Toolkit (NLTK) (Bird 2006), polyglot,20 spaCy for Greek, 2122 Stanza (Qi et al. 2020), and UDPipe (Straka and Strakov\u00e1 2017). Additional research in the field of Syntax includes the development of a multilingual sentence boundary detection method based on an incremental decision tree learning algorithm (Wong, Chao, and Zeng 2014). Furthermore,\nSamaridi and Markantonatou (2014) focused on parsing Multi-word expressions (MWEs) with Lexical-Functional Grammar / Xerox Linguistic Environments (LFG/XLEs), extending their analysis beyond traditional syntactic boundaries by incorporating lexical knowledge from lexicons.\nGEC in Greek. Korre, Chatzipanagiotou, and Pavlopoulos (2021) focused on the correction of grammatical errors that vary from grammatical mistakes to punctuation, spelling, and morphology of word. The authors listed 18 main categories of grammatical errors that systems can correct, also developing a rule-based automatic annotation tool for Greek. The tool takes an original erroneous sentence along with its correction as input. Then, it automatically produces an annotation that mainly consists of the error lo-cation and type, as well as its correction. Gakis et al. (2016) created a rule-based grammar checker tool, 23 which analyzes and corrects syntactic, grammatical, and stylistic (i.e., the formal, informal, or oral style of language used) errors in sentences, providing users with error notifications and correction hints. Kavros and Tzitzikas (2022) focused on spelling errors, addressing the issue of misspelled and mispronounced words in Greek. They employed phonetic algorithms to assign the same code to different word variations based on phonetic rules. For example, they successfully grouped \u03bc\u03b7\u0301\u03bd\u03c5\u03bc\u03b1 (correct spelling) with \u03bc\u03c5\u0301\u03bd\u03b7\u03bc\u03b1 (both sounding as /m\u00ednima/). They reported better results compared to stemming and edit-distance approaches. 24"}, {"title": "5.3 Syntax and Grammar in Greek: Language Resources", "content": "Table 6 displays the pertinent monolingual Language Resources (LRs) for this track, where we see that all three resources regarding GEC are publicly available while the one about Syntax is not. Regarding the former, Kavros and Tzitzikas (2022) created word lists, containing words and their misspellings. These misspellings were generated through the addition, deletion, or substitution of a letter, as well as by incorporating words with similar sounds. Korre, Chatzipanagiotou, and Pavlopoulos (2021) developed two datasets, namely the Greek Native Corpus (GNC) and the Greek Wiki Edits (GWE). GNC is comprised of essays written by students who are native speakers of Greek, totalling 227 sentences. Each sentence within this dataset may contain zero, one, or multiple grammatical errors, all annotated with the corre-sponding grammatical error types as defined in the provided annotation schema. On the other hand, GWE consists of sentences extracted from WikiConv (Hua et al. 2018). Each sentence in this dataset includes the original sentence, the edited sentence, the original string that underwent editing, and the specific grammatical error type. The dataset on Syntax is from Gakis et al. (2015), who collected a corpus consisting of 2.05M tokens derived from student essays, literary works, and newspaper articles. They extracted morphosyntactic information automatically for this corpus with the help of a lexicon (Gakis et al. 2012)."}, {"title": "5.4 Summary of Syntax and Grammar in Greek", "content": "Conventionally, syntactic processing was widely used in pre-processing steps for solving higher-level NLP tasks (\u00a74). However,in the era of Deep Learning (DL)-based NLP, syntactic processing is often neglected. Instead, Neural Networks (NNs) are leveraged to implicitly capture syntactic information, surpassing the performance of symbolic methods that rely on manually hand-crafted features. This is also reflected by the number of ACL submissions related to Syntax (i.e., Tagging, Chunking and Parsing),"}, {"title": "6. Semantics", "content": "6.1 Track Description\nThe meaning in language is the focus of Semantics. In the context of Natural Language Processing (NLP)", "examination": "Lexical Semantics", "models": "i) count-based Distri-butional Semantic Models (DSMs"}]}]}