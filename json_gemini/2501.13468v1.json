{"title": "STREAMING VIDEO UNDERSTANDING AND MULTI-ROUND INTERACTION WITH MEMORY-ENHANCED KNOWLEDGE", "authors": ["Haomiao Xiong", "Zongxin Yang", "Jiazuo Yu", "Yunzhi Zhuge", "Lu Zhang", "Jiawen Zhu", "Huchuan Lu"], "abstract": "Recent advances in Large Language Models (LLMs) have enabled the development of Video-LLMs, advancing multimodal learning by bridging video data with language tasks. However, current video understanding models struggle with processing long video sequences, supporting multi-turn dialogues, and adapting to real-world dynamic scenarios. To address these issues, we propose STREAMCHAT, a training-free framework for streaming video reasoning and conversational interaction. STREAMCHAT leverages a novel hierarchical memory system to efficiently process and compress video features over extended sequences, enabling real-time, multi-turn dialogue. Our framework incorporates a parallel system scheduling strategy that enhances processing speed and reduces latency, ensuring robust performance in real-world applications. Furthermore, we introduce STREAMBENCH, a versatile benchmark that evaluates streaming video understanding across diverse media types and interactive scenarios, including multi-turn interactions and complex reasoning tasks. Extensive evaluations on STREAMBENCH and other public benchmarks demonstrate that STREAMCHAT significantly outperforms existing state-of-the-art models in terms of accuracy and response times, confirming its effectiveness for streaming video understanding. Code is available at StreamChat.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in Large Language Models (LLMs) [1-3] have led to the development of Video-LLMs [4-9], which aim to interpret visual scenes, actions, and narratives. These models represent significant progress in multimodal learning by bridging video data and language-based tasks, with applications spanning from content analysis to human-robot interaction [9].\nDespite these advancements, current offline models primarily process videos as static clips and rely on single-turn dialogues, incorporating visual information through mechanisms like projection layers [4, 8] or cross-attention structures [2]. However, these models encounter computational bottlenecks when handling extended video sequences, often struggling to compress lengthy video features within limited memory resources [10]. Additionally, their inability to support multi-turn dialogues reduces adaptability for interactive scenarios, and key information may be lost due to insufficient video sampling methods (cf. Fig. 2(a).\nTo address these issues, online models [10, 11] have emerged. They utilize memory-based approaches and temporally aligned instruction-tuning to process long videos and enable multi-round interactions"}, {"title": "2 COLLECTION AND COMPOSITION OF STREAMBENCH", "content": "Previous video understanding benchmarks [12-14, 16-18] primarily focus on offline scenarios, where all video frames and user questions are provided to the model simultaneously for generating answers. To find a more suitable method to assess a model's ability to understand online scenarios, we introduce STREAMBENCH, a benchmark mainly designed to simulate online video scenarios. There are two distinct differences compared with other video understanding benchmarks. (i) Diverse video curation: we collect four major domains and sixteen sub-classes of video sources, including egocentric videos, web videos, working videos, and movies as the database of the benchmark. Each type has its unique characteristics and challenges, which can verify the stability and reliability of the model in a wide range of application scenarios. (ii) Crafted query types: we design six types of questions to meet the specific needs of online video understanding and ensure that these types of questions appear once in a single video, forming a multi-round dialogue. This section introduces how we collect videos and construct annotations. More details about the diversity and distribution of our benchmark are shown in Fig. 3.\nIn selecting videos for our benchmarks, we prioritized diversity in type and length to maintain high data quality. Our primary sources are the EgoSchema [13] and YouTube-8M [19] datasets. EgoSchema offers a rich array of both indoor and outdoor scenes, providing an extensive range of egocentric perspectives and actions, which aligns perfectly with our experimental needs. From YouTube-8M, which features a comprehensive internet-sourced collection spanning over 4,000 classes, we filtered to procure high-quality web, work-related, and cinematic videos. This diverse selection framework ensures our model is tested against a broad spectrum of real-world scenarios.\nFiltering videos. It is a crucial step to ensure the quality of the videos used in the benchmark. To achieve this, our data filtering pipeline consists of machine and human selection. Firstly, a multi-modal language model [20] is utilized to classify the original data. The categories of videos are provided by data source, we feed them with the videos to the machine and make it select the category of the video. Secondly, human judgment is required to assess the redundancy: the change of scenes in videos. Some static video content (e.g., ego view of drawing, watching TV) and high-noise data from web videos (e.g., video games, advertisements) are removed according to human judgment."}, {"title": "2.2 CONSTRUCTION OF TASKS AND ANNOTATIONS", "content": "We have crafted six distinct tasks with annotations to simulate the conversation between the agent and the human. Each task corresponds to a different real-world scenario, ensuring comprehensive coverage of potential communication contexts.\n\n\u2022 Object Search (OS): Challenges include accurately describing an object's position in a video. The task conditions are that the object must appear for less than 5 seconds and the interval from its appearance to the user's request should exceed 30 seconds, enhancing the difficulty of the search.\n\n\u2022 Long-term Memory Search (LM): This task assesses the model's memory by requiring recall of events appearing for more than 5 seconds, with a delay exceeding 1 minute from the event's end to the user's query, testing long-term memory retention.\n\n\u2022 Short-term Memory Search (SM): To simulate the user's interest in recent events, this task sets the interval from event completion to the user's query at less than 20 seconds, evaluating the model's response to recent activities.\n\n\u2022 Conversational Interaction (CI): Sometimes the answer to a user's current question is closely related to conversation history. Therefore, the model must memorize conversation records and retrieve the most relevant text from the memory as contextual support. This type is designed to simulate multi-turn dialogue scenarios. We set the dialogue information associated with the user's current request to come from any previous conversation, with an interval of more than 2 dialogues.\n\n\u2022 Knowledge-based Question Answering (KG): This type of question evaluates the model's internal knowledge, which is retained by the base large language models. In this benchmark, we set the questions must be related to the events or objects occurring in the video so that it can simulate scenarios where users have a specific need to understand background or encyclopedic knowledge.\n\n\u2022 Simple Factual (SF): This type of question focuses on friendly dialogue starting between the user and the model. Therefore, they must be asked within 30s after the beginning of the video. Although the question is simple, the model needs to remember things in the short term to answer correctly."}, {"title": "3 STREAMCHAT", "content": "Given streaming broadcast video and timestamped questions as input, STREAMCHAT is designed to efficiently perform reasoning and deliver accurate answers across multiple rounds. Building upon LongVA [20] (Appen. \u00a7F) as a foundational Video-LLM, our design incorporates two key components: a hierarchical memory storage system (\u00a73.1) that leverages long-term, short-term, and dialogue memories to compress and manage extensive video sequences within constrained resources, thereby facilitating effective video-content reasoning; and a system scheduling strategy (\u00a73.2) that decouples video feature extraction from memory updates, thereby preventing unbounded buffer growth as the input video frames increase."}, {"title": "3.1 HIERARCHICAL MEMORY STORAGE", "content": "STREAMCHAT treats videos as dynamic information repositories, utilizing hierarchical memory to analyze and store the diverse content. This section details two specialized memory structures devised to address the challenges of information storage and retrieval: long-short term memory $M_{UMS} = \\{ l_i \\}_{i=0}^{T/L}$ and dialogue memory $M_d = \\{d_i\\}_{i=0}^D$. These memories manage visual and conversational data, where $T$ is video duration, $S$ is short memory length, $D$ counts dialogues, and $L$ is the chunk size for long memory. The following sections introduce the functions of the above parameters."}, {"title": "3.1.1 LONG-SHORT TERM MEMORY", "content": "Selective Frame Stacking. To reduce the feature storage overhead caused by redundant frames in videos, we use Lucas-Kanade Optical Flow algorithm [21] in the selective frame stacking module to assist in determining the validity of each video frame $\\{F_i \\in \\mathbb{R}^{H \\times W \\times 3}\\}$_0^T. Specifically, we calculate the motion vector $(u, v)$ between i-th frames $F^i$ and the last frame $F^{i-1}$:\n$\\begin{bmatrix} u \\\\ v \\end{bmatrix} = \\begin{bmatrix} \\sum I_x(i)^2 & \\sum I_x(i)I_y(i) \\\\ \\sum I_x(i)I_y(i) & \\sum I_y(i)^2 \\end{bmatrix}^{-1} \\begin{bmatrix} \\sum I_x(i)I_t(i) \\\\ \\sum I_y(i)I_t(i) \\end{bmatrix}$  (1)\nwhere $I_x(i), I_y(i), I_t(i)$ represent the partial derivatives of the frame $F^i$ with respect to position $i(x, y)$ and time $t$. We develop the motion vector magnitude $||\\theta|| = \\sqrt{u^2 + v^2} \\in [0, 1]$ to represent the total motion intensity between frames. If $||\\theta||$ exceeds the predefined threshold $t \\in [0, 1]$, the frame $F^i$ will be encoded into vision embedding $e^i \\in \\mathbb{R}^{n \\times d}$ and pushed into buffer $B_{vision}$.\nShort-term Memory. We intend to design a human-like memory method that simulates the Atkinson-Shiffrin model [22], which emphasizes the role of a short-term storage for maintaining readily accessible, frequently updated information. Specifically, as shown in Fig. 5 (a), we select N vision embeddings from the $B_{vision}$ as vision candidates $C$. Building on the Ebbinghaus Forgetting Curve theory [23], we handle memory updates by randomly selecting $S$ vision embeddings $e^i$ from $C$ to construct the short-term memory $M_s$:\n$C = \\{ \\sigma_{N-1} e^{i_{N-1}}, \\sigma_{N-2} e^{i_{N-2}}, ..., \\sigma_0 e^{i_0} \\}$      (2)\n$M_s = \\{ s_i \\in \\mathbb{R}^{n \\times d} \\}_{i=0}^{S-1}$  random select \nwhere $\\sigma_i$ is the normalized forgetting probability of i-th unit of $C$, $S$ represents the length.\nLong-term Memory. The long-term memory simulates the complex and abstract memory of humans [22]. For this reason, we design two forms of information in long-term memory: text clues, which is used to store declarative text $t_i$ describing events that occurred over a past period, and vision memory, which is used to store compressed visual features $v_i \\in \\mathbb{R}^{C \\times d}$. Text clues serve as an index for retrieving relevant information from the long-term memory (introduced in \u00a73.1.3). Our system"}, {"title": "3.1.2 DIALOGUE MEMORY", "content": "In our approach, each round of question $\\{Q_i\\}_{i=0}^{P_0}$ and answer $\\{A_i\\}_{i=0}^{P_0}$ is viewed as a memory fragment, which is pre-encoded by the encoder model $\\mathcal{E}(\\cdot)$ into a contextual representation $d_i$. Thus, the entire dialogue history $M_d$ is pre-encoded as shown in the following formula: $M_d = \\{d_0, d_1, ..., d_{i-1}\\} \\xlongequal{Push d_i = \\mathcal{E}(\\langle Q_i, A_i \\rangle)}$ where the length of $M_d$ is equal to the conversation number $D$, and we select MiniLM-L6 [24] as our encoder model."}, {"title": "3.1.3 RETRIEVAL", "content": "When a user question $Q_i$ comes in, the memory system will search for the most relevant knowledge as supplementation by the retrieval algorithm. In long memory tree $M_l$, the $Q_i$ and text clue units $\\{t_i\\}_{i=0}^{T/L}$ are encoded by the tokenizer and the embedding layer of LLM. Based on the cosine similarity between encoded $Q_i$ and text clue $t_i$, the memory system will search for the retrieved tokens $M_r \\bigcup \\{v_r \\in \\mathbb{R}^{C \\times d} \\}_{r=0}^{L}$, where $L$ represent the layer number of $M_l$. In dialogue memory $M_d$, the user requests $Q_i$ is encoded by $\\mathcal{E}(\\cdot)$ as a query to search for the context $\\langle Q_{retrieved}, A_{retrieved} \\rangle$ based on the FAISS [25] index. More details about our retrieval algorithm are shown in Appen. \u00a7C."}, {"title": "3.2 SYSTEM SCHEDULING", "content": "As shown in Fig. 4, our method includes three different parts: selective frame stacking, memory formation and contextual summarization. These components are operated as independent threads to optimize inference speed and minimize latency. System scheduling is crucial as it enables concurrent execution of these threads without interference, significantly enhancing processing speed.\nSpecifically, the (i) slective frame stacking thread actively populates the vision buffers $B_{vision}$ with features $e^i$. Once full, these features are cleared from the buffer and passed to the (ii) memory forma-"}, {"title": "4 EXPERIMENTS", "content": "Memory Configurations. To adapt the model to various application scenarios, we configure three versions with different memory settings: Base, Fast, and Slow. These variants adjust key memory parameters, including threshold (t), chunk length (L), group size (g), and clustering goals (C), as summarized in Tab. 3. The Fast model is optimized for rapid video processing, while the Slow model prioritizes accuracy in responses. The Base model balances processing speed and accuracy.\nEvaluation Metrics. We evaluate semantic similarity in single conversations using the LLaMA-3 model [3], which assigns a semantic correctness score (Sco.) ranging from [0, 5], where higher scores reflect responses that more closely align with the expected answers. For assessing coherence in multi-turn dialogues, we compute score fluctuations across turns; smaller fluctuations (Coh.) indicate a smoother dialogue experience. Additionally, we measure request processing delay (RPD), defined as the time (in seconds) from user request submission to the start of response generation. A smaller RPD signifies lower latency, resulting in reduced wait times for users. Appen. \u00a7D offers more details.\nImplementation Details. We utilize CLIP-L-P14 [26] as the vision encoder and we set the number of selected memory units S to 5 and candidate length C to 20. Experiments were conducted on two NVIDIA Tesla A800 GPUs with 80GB of memory each (more details in Appen. \u00a7F). We benchmark our model against state-of-the-art methods, including Video-LLaVA [4], LLaMA-VID [2] and etc."}, {"title": "4.2 COMPARISON WITH STATE-OF-THE-ART METHODS", "content": "Online Scenarios. As shown in Tab. 4, our models demonstrate significant improvements over the previous best method, Video-online [11].\n\n\u2022 Slow: Achieves an 8.3% higher accuracy and a 0.37 higher score than Video-online.\n\n\u2022 Fast: Processes video at 32 FPS, making it much faster than all previous streaming methods, while still improving accuracy by 5.3% and scoring 0.17 higher than [11].\n\n\u2022 Base: Reaches 63.8% Acc. and 3.42 score.\n\n\u2022 Best model: Surpasses [11] with a 0.18 improvement in coherence score and reduces latency by 0.17s, delivering smoother conversations with shorter wait times.\n\nDue to system scheduling, all models maintain nearly the same response time of about 0.9s. Tab. 5 presents the detailed scores across six question types. Using hierarchical memory storage, our method excels in object search (OS), long-term memory search (LM), short-term memory search (SM), and conversational interaction (CI) tasks. Notably, our Slow model increases accuracy by 10.3% in OS, 5.1% in LM, 4.9% in SM, and 5.8% in CI compared to Video-online.\nOffline Scenarios. We compare our Base model against other methods in general offline video understanding benchmarks including MSRVTT-QA [12], ActivityNet [14], NExT-QA [15], MSVD-"}, {"title": "4.3 CASE STUDY", "content": "In Fig. 6, we illustrate the reasoning process of STREAMCHAT with g = 2 to simplify the observation of internal mechanisms. The scenario involves a user asking STREAMCHAT to identify a tool meeting specific requirements and to describe its environment. The memory structure consists of a dialogue memory, $M_d$, with two historical entries, and a layered memory, $M_l$, with two levels. The memory tree visualization shows that the system initially searches for key information at level 1. It computes cosine similarity between the user's query $Q_i$ and two memory units, Summary (1) and (2), obtaining scores of 0.3993 and 0.4751, respectively. Based on these results, STREAMCHAT selects the path from the second node (v1) due to its higher similarity score and continues along this path. Subsequently, the system aggregates value $\\{v_r\\}_{i=0}^8$ from $M_l$ into retrieved tokens that are then incorporated into the reasoning process. Additionally, a high similarity score of 0.6983 between $Q_i$ and the first historical conversation helps provide context, enhancing the depth and relevance of the response."}, {"title": "4.4 ABLATION STUDY", "content": "Exploring Effects of Hierarchical Memory. We conduct ablation experiments using the Base model to assess the impact of different memory components on performance. As shown in Table 7, adding $M_d$ to the base model improved performance on the CI task by 4.1% without affecting other tasks. Adding $M_l$ improved the LM task performance by 6.2%, while the use of $M_s$ boosts SM task performance by 3.2%. The results indicate that the model's performance in each subtask aligns with the inclusion of specific memory attributes. Additionally, we observe that different memory components can complement each other. When both long-term $M_l$ and short-term memory $M_s$ are applied simultaneously, the average accuracy increases by 0.9%.\nTradeoffs in Speed and Threshold Settings. The threshold of the Lucas-Kanade Optical Flow algorithm significantly influences video processing speeds. As illustrated in Fig. 7 (a), increasing the threshold initially accelerates the processing speed. However, this increase saturates when $t$ reaches 0.55, stabilizing at 32 FPS. Importantly, higher processing speeds are discouraged due to their detrimental impact on model performance (64.0%\u219260.7%). Elevating thresholds leads to more pronounced changes in frame differences and loss of original data, thereby limiting the model's ability to effectively utilize the full spectrum of video information.\nDesign of Long Memory Tree. The chunk length (L), group size (g), and clustering goal (C) significantly impact the effectiveness of the memory tree ($M_l$). In Fig. 7 (b-d), we evaluate how these factors influence online video understanding tasks, using the Base model with t=0.35.\n\n\u2022 As shown in Fig. 7 (b), increasing L form 15 to 30 leads to better performance (61.2%\u219264.0%). However, further increasing L to 40 results in a slight decline (64.0%\u219263.1%), and substantially increases latency (0.84s\u21921.26s), due to the computational demands of the clustering algorithm.\n\n\u2022 Increasing g from 2 to 12, which represents the less compression of visual information and increases input sequence length C \u00d7 L, enhances performance (62.0%\u219263.9%) as greater diversity in the knowledge at each node of the long memory tree $M_l$ is achieved. However, it intensifies the load on the retrieval, leading to an increase in RDP (0.76s\u21921.02s), illustrated in Fig. 7 (c).\n\n\u2022 The clustering goal (C) primarily influences the number of tokens ($v_i$) stored in the $M_l$. Fig. 7 (d) shows that increasing the dimension of $v_i$ (3\u219210) enhances model performance (59.4%\u219264.0%) by enriching the stored knowledge, which also exacerbates VRAM limitations (20\u219256 GB)."}, {"title": "5 RELATED WORK", "content": "Multi-modal Language Models (MLMs). Recent developments of large language models [3, 34-38] and multi-modal alignment techniques have significantly advanced MLMs' capability. The LLaVA series [39, 40] utilizes straightforward mapping layers and visual instruction tuning to broaden image understanding tasks to video data. Challenges in video processing primarily involve efficiently compressing video within limited contextual windows. Innovations like ChatUniVi's [5] use of a K-NN clustering algorithm dynamically compress visual tokens, while LLaMA-VID [2] reduces single images to two tokens via cross-attention, and MovieChat [7] leverages long and short-term memory frameworks for extensive data handling. Despite these advances, the transition to effective real-time streaming video understanding in practical applications remains insufficiently addressed. Our research introduces a robust solution designed to meet the real-time demands of online video understanding, aiming to fill this critical gap.\nStreaming Video Understanding. Streaming video understanding demands real-time responses from models to user queries, even as video durations potentially extend indefinitely. This is particularly challenging for traditional benchmarks like action recognition [41], multi-round video dialogue [42], and first-person question answering [13] which rely on uniform frame sampling. In response to these limitations, there is a growing shift towards online models that process only current and past video frames to formulate responses [10, 11]. Despite these advancements, these models often struggle with slow processing speeds and inadequate generalization capabilities, underlining a critical need for further exploration and enhancement in this field.\nRetrieval-Augmented Generation (RAG). RAG combines information retrieval and text generation to produce more precise and informative responses by incorporating external knowledge into language models [43-53]. This technique has become increasingly popular for addressing knowledge retention and real-time information access challenges. MemoryBank [54] enhances interaction by storing real-time conversations and leveraging similarity search to retrieve contextually relevant information, enriching the depth and coherence of dialogue. This approach significantly improves a model's ability to maintain continuity in conversations, particularly in long or multi-turn interactions where maintaining context is crucial. Inspired by RAG's efficiency, we introduce a multi-modal memory system that integrates and updates textual and visual data in real time. Using a RAG-inspired retrieval mechanism, this system efficiently accesses the most relevant information from our memory bank, enabling the multi-modal language model to deliver precise, query-specific responses for enhanced video language understanding."}, {"title": "6 CONCLUSION", "content": "In this work, we introduce STREAMBENCH, a comprehensive benchmark specifically crafted to assess streaming video understanding, covering a broader range of video lengths and types with six question formats to simulate real-world human-robot interactions. This broader scope enhances our ability to evaluate model performance in complex and dynamic scenarios. Alongside, we present STREAMCHAT, a training-free method designed for efficient streaming video understanding, which treats video frames as compressible and storable units and manages them through a hierarchical memory structure. With advanced system scheduling, STREAMCHAT achieves real-time processing speeds and reduced interaction latency, demonstrating robust performance across both online and offline settings in our extensive experiments."}, {"title": "Limitations and Future Works.", "content": "Our current retrieval algorithm relies on basic matching techniques, occasionally leading to incorrect responses. Enhancing this with more fine-grained retrieval mechanisms is an essential next step. Additionally, the VRAM constraints of our tree-structured storage could limit scalability as video duration and complexity further grow. Investigating more efficient or adaptive compression techniques will address these limitations. Moreover, to achieve lower latency, we plan to explore closer hardware integration and the potential adoption of fast-serving, multi-modal distributed systems to accommodate larger model parameters and increased user demands."}, {"title": "A DATA PIPELINE", "content": "Fig. 8 presents our video collection pipeline. It consists of 3 parts: (1) Classification; (2) Human judge; and (3) Annotation check. First, a MLLM [4] is utilized to complete the video classification based on our requirements. The following prompt is used during the first data filtering step:\n\"Based on the observed video information, categorize the video into one of the predefined categories listed in {All_Classes}. Respond exclusively in the format of a Python dictionary string with the keys 'pred' and 'score'. The 'pred' key should contain the uppercase STRING of the chosen category. Refrain from providing any additional text or explanatory output. Your response should strictly follow this example: {'pred': 'A'} .\"\n{All_Class } is the options formation. When dealing with different datasets, we need to change the options. For example, when dealing with Youtube-8M [19], it is {A: Drama, B: Action, C: Cartoon, D: Romance, E: Sci-fi, F: Others} and {A: Cooking, B: Construction, C: Room-Tour, D: Gardening, E: Others} for EgoSchema [13] dataset. We save the output to a JSON file and then find categories from the file as needed. It is worth noting that we also used the original category information in the YouTube data. The above classification process is used primarily for secondary classification of MovieClips data. For EgoSchema, we need to classify all original videos as they don't contain category annotations."}, {"title": "B MORE VISUALIZATIONS", "content": "In Fig. 9-12, we visualize several STREAMCHAT cases applied to different types of videos. Specifically, Fig. 9 illustrates an egocentric video annotation, where the system engages in interactive questioning"}, {"title": "C RETRIEVAL ALGORITHM", "content": "Inspired by the retrieval argumentation system [43], our approach enhances the model's capability to address complex queries by retrieving the most relevant information from long-term memory for contextual support. As outlined in Algorithm 1, we compute the similarity, Sim, between the user's request, Q, and entries Tn in the memory. This process identifies the optimal path for accessing the most pertinent stored knowledge, $T_{best}$ and $C_{best}$. Leveraging our tree-like storage structure, we efficiently focus on the highest-similarity nodes at each layer, minimizing the computational load by avoiding exhaustive sub-node calculations. The selected knowledge, $T_{best}$ and $C_{best}$, is then"}, {"title": "D DETAILS OF METRICS", "content": "We design the following metrics to measure the model's ability to stream video understanding:\n(1) Score and Accuracy: To assess the semantic correctness of a single-turn dialogue, using language models is a mainstream approach [12, 14, 16-18]. We also use this as a key metric in our benchmark. In our test benchmark, we use the open-source language model LLaMA-3 8B [3] Instruct version as our scoring model f. Here is the prompt that we used during scoring:\nPrompt = [{\n\"role\": \"system\",\n\"content\": \"You are an intelligent chatbot designed for evaluating the correctness of generative outputs for question-answer pairs. Your task is to compare the predicted answer with the correct answer and determine if they match meaningfully. Here's how you can accomplish the task: INSTRUCTIONS:Focus on the meaningful match between the predicted answer and the correct answer.Consider synonyms or paraphrases as valid matches.- Evaluate the correctness of the prediction compared to the answer.\"\n},\n{\n\"role\": \"system\",\n\"content\": \"Please evaluate the following video-based question-answer pair: Question: question; Correct Answer: answer; Predicted Answer: prediction Provide your evaluation only as a yes/no and score where the score is an integer value between 0 and 5, with 5 indicating the highest meaningful match. Please generate the response in the form of a Python dictionary string with keys 'llama pred' and 'score', where the value of 'llama pred' is a string of 'yes' or 'no' and the value of 'score' is in INTEGER, not STRING. DO NOT PROVIDE ANY OTHER OUTPUT TEXT OR EXPLANATION. Only provide the Python dictionary string. For example, your response should look like this: {'llama pred': YOUR JUDGE, 'score': YOUR SCORE}.\"\n}\n]\nWe organize the question Q, reference answer R, and model's response M into the Tab. 8 formation and send to scoring model, which then provides a score in the range of 0-5 and evaluates whether the model's response is semantically correct:\n$S_i = f(Q, R, M), Acc = \\frac{1}{N} \\sum I(S_i > T)$   (6)\nA higher score $S_i$ and Acc indicates that the answer is closer to the reference answer.\n(2) Coherence: Given that a single video may involve multiple rounds of dialogue, we need to evaluate the model's ability to provide a coherent experience across different rounds. We introduce"}, {"title": "E FAILURE CASE AND ANALYZE", "content": "We present some failure cases that occurred during testing and explain why they occurred. Most of these cases come from object search, long-term memory, short-term memory and conversational interaction tasks. The problems that occurred are mainly grouped into four types:\n\n\u2022 Temporal Fine-grained: In the object search task, our method still struggles to identify key information when the queried objects or events appear too briefly or sporadically. For instance, as demonstrated in case (1), the user's question pertains to a candle. However, due to the infrequent appearance of the candle and its small size, the model fails to provide an accurate response.\n\n\u2022 Spatial Fine-grained: Whether in object search or short-term memory task, our method faces limitations when the user's target is too small or blends into the background, even if the object appears multiple times in the video. For example, in short-term memory task case (2) and object search task case (3), the target objects (porcelain bowl and red cup) are too small relative to the foreground, making it difficult for the model to accurately detect and locate them. We will continue to improve our method to enhance the perception of small objects.\n\n\u2022 Target Movement: During the reasoning process, we observed that in some cases, even when the model correctly identified the target, its interpretation of the target's actions and relationships with surrounding objects was still inaccurate. For instance, in the long-term memory task case (4) the model failed to recognize the action and position association between the \u201cperson\u201d and the \u201cbox,\u201d leading to an incorrect response.\n\n\u2022 Context Induction: In the conversational interaction task, the model's performance is influenced by the accuracy of its responses to previous related questions. For instance, in case 5, the model retrieved information from the dialogue history, but when that historical information was incorrect, it became challenging for the model to provide the correct answer.\n\n\u2022 Information Loss: According to the experimental results in Tab. 5, although our method shows balanced performance in various tasks of StreamChat, our hierarchical memory storage still has the potential risk of losing information. Since our existing method is too dependent on the accuracy of the retrieval algorithm, we will continue to update our method to minimize information loss."}, {"title": "F MODEL SELECTION AND DEPLOYMENT", "content": "Our research indicates that a suitable model should possess the following key attributes:\n\n\u2022 Long-Form Video Understanding: Effective processing of long videos is crucial. While we utilize K-Means for feature compression, the information retrieved by our memory mechanism remains relatively long, requiring a model capable of handling extended sequences.\n\n\u2022 Robustness to Prompt Variations: For accurate and coherent multi-turn conversations, the model must be robust to changes in prompt wording. This robustness is essential to prevent inconsistencies"}, {"title": "G EXPANSION PLAN OF STREAMBENCH", "content": "\u2022 Video scale: We are trying to expand the number of videos contained in the StreamBench to reach a higher standard. We are working on expanding the number of videos to thousands while maintaining the diversity of video length and types.\n\n\u2022 Annotation scale: We are continuing to promote the development of high-quality annotations. Based on your suggestions, we will use manual annotation methods to expand the annotation of existing benchmarks to the order of ten thousand levels and also use manual inspection to filter out toxic labels and erroneous information.\n\n\u2022 Diverse tasks: Given that the current benchmark only has a single task type, we are continuing to expand the types of tasks included in the benchmark, including but not limited to multiple-choice questions, video captioning, and video grounding and etc."}]}