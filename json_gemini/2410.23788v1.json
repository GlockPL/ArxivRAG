{"title": "EDT: An Efficient Diffusion Transformer Framework Inspired by Human-like Sketching", "authors": ["Xinwang Chen", "Ning Liu", "Yichen Zhu", "Feifei Feng", "Jian Tang"], "abstract": "Transformer-based Diffusion Probabilistic Models (DPMs) have shown more potential than CNN-based DPMs, yet their extensive computational requirements hinder widespread practical applications. To reduce the computation budget of transformer-based DPMs, this work proposes the Efficient Diffusion Transformer (EDT) framework. The framework includes a lightweight-design diffusion model architecture, and a training-free Attention Modulation Matrix and its alternation arrangement in EDT inspired by human-like sketching. Additionally, we propose a token relation-enhanced masking training strategy tailored explicitly for EDT to augment its token relation learning capability. Our extensive experiments demonstrate the efficacy of EDT. The EDT framework reduces training and inference costs and surpasses existing transformer-based diffusion models in image synthesis performance, thereby achieving a significant overall enhancement. With lower FID, EDT-S, EDT-B, and EDT-XL attained speed-ups of 3.93x, 2.84x, and 1.92x respectively in the training phase, and 2.29x, 2.29x, and 2.22x respectively in inference, compared to the corresponding sizes of MDTv2. The source code is released here.", "sections": [{"title": "1 Introduction", "content": "Numerous studies [1\u20135] and practical applications [6\u20138] have validated the effectiveness of Diffusion Probabilistic Models (DPMs), establishing them as a mainstream method in image generation. In past years, predominant works [1\u20134, 9\u201311] have advanced diffusion models by incorporating a convolutional UNet-like [12] architecture as their backbone. On the other hand, transformers [13] have achieved significant milestones in both natural language processing [14, 15] and computer"}, {"title": "2 Method", "content": "We briefly review several fundamental concepts necessary to understand classifier-free guidance class-condition diffusion models [11]. The primary objective of diffusion models is to learn a diffusion process that constructs a probability distribution for a specific dataset, subsequently enabling the sampling of new images. Given a classifier-free guidance class-condition diffusion model $\\epsilon_\\theta(x_t, c)$, the model can generate images of specific class $c$ from Gaussian noise over multiple denoising time steps. The model operates through two main processes: the forward and reverse processes. The forward process simulates training data $x_t$ to be denoised at time step $t$, by adding Gaussian noise $\\epsilon_t \\sim \\mathcal{N}(0, I)$ to the original data $x_0$. This process is mathematically described by $q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t}x_0, (1 - \\bar{\\alpha}_t)I)$, where $\\bar{\\alpha}_t$ denotes a hyperparameter. The reverse process samples noise-reduced data $x_{t-1}$ based on noise data $x_t$ and class-condition $c$. The reverse process is represented as $p_\\theta(x_{t-1} | x_t,c) = \\mathcal{N}(x_{t-1}|\\mu_\\theta(x_t, c), \\Sigma_\\theta(x_t,c))$, where $\\mu_\\theta$ and $\\Sigma_\\theta$ are the statistics of $p_\\theta$. By optimizing the variational lower-bound of the log-likelihood [26] $p_\\theta(x_0)$ and reparameterizing $\\mu_\\theta$ as a noise prediction network $\\epsilon_\\theta$, the model can be trained using simple mean-squared error between the predicted noise $\\epsilon_\\theta(x_t, c)$ and the ground truth $\\epsilon_t$ sampled Gaussian noise:\n$\\mathcal{L}_\\theta(x_t, c, \\epsilon_t) = ||\\epsilon_\\theta(x_t, c) - \\epsilon_t||^2$.\nAdditionally, $\\epsilon_\\theta(x_t, c)$ is a standard class-condition model; when $c = \\emptyset$, it functions as an unconditional model. To allow the controllability of class-condition guidance, the prediction of models is further derived as $\\hat{\\epsilon_\\theta}(x_t, c) = \\epsilon_\\theta(x_t, \\emptyset) + w \\cdot (\\epsilon_\\theta(x_t, c) - \\epsilon_\\theta(x_t, \\emptyset))$, where $w > 1$ is class-condition guidance intensity.\nIn this work, we employ a classifier-free guidance class-condition diffusion transformer architecture operating on latent space. The pre-trained variational autoencoder (VAE) model [26] from LDM [11] remains frozen and is used to encode/decode the image/latent tokens."}, {"title": "2.2 Lightweight-design diffusion transformer", "content": "Transformer-based diffusion probabilistic models (DPMs) have demonstrated greater scalability and superior performance compared to CNN-based DPMs [20\u201322]. However, these models also entail significant computational overhead during both the training and inference phases. In response, we design a lightweight diffusion transformer architecture in this section. We undertake a computational complexity analysis of the transform-based diffusion model. Based on the empirical analysis of the number of tokens, token dimensions, FLOPs, and the number of parameters, we establish two design principles: (1) reducing the number of tokens to decrease the FLOPs in the self-attention module through the down-sampling module; (2) ensuring that the FLOPs of each EDT stage post a down-sampling module are significantly reduced compared to the stages prior to the down-sampling module, to effectively lower the overall FLOPs.\nBuilding on the aforementioned design principles, we have redesigned and incorporated the down-sampling, up-sampling, and long skip connection modules into the transformer-based diffusion model, successfully achieving a reduction in FLOPs and increased inference speed. For instance, in comparison to DiT-S [21], our smaller version model EDT-S achieves an inference speed of 5.5 steps per second, versus 2.7 steps per second for DiT-S, effectively doubling the speed. Figure 2 illustrates the architecture of our lightweight-designed diffusion transformer. The model includes three EDT stages in the down-sampling phase, viewed as an encoding process where tokens are progressively compressed, and two EDT stages in the up-sampling phase, viewed as a decoding process where tokens are gradually reconstructed. These five EDT stages are interconnected through down-sampling, up-sampling, and long skip connection modules. Note that each EDT stage comprises several consecutive transformer blocks. For more details on the computational complexity analysis and model design, please refer to Appendix A.2. It is important to note that the down-sampling and up-sampling phases can be viewed as encoding and decoding processes, respectively, aligning with"}, {"title": "2.3 Making EDT \u201csketch\u201d like a human", "content": "The lightweight design of EDT might compromise the quality of image synthesis. To enhance the detail fidelity in generated images, we have refined the decoding process (up-sampling phase) of EDT by imitating the process of human sketching. We begin by examining how attention shifts during the act of sketching by humans. Human cognition stores visual structures as a top-down hierarchy passing from general shape to the relationships between parts down to the detailed features of individual parts [23, 24]. This hierarchical structuring of visual information in the brain makes humans tend to follow a coarse-to-fine strategy in sketching [25]. As shown in Figure 1, the process of human"}, {"title": "2.3.1 Integrating local attention into the up-sampling phase of EDT", "content": "To imitate the alternation between global and local attention like the act of humans drawing, we integrate local attention into the up-sampling phase of EDT by introducing Attention Modulation Matrix (AMM). In this section, we concentrate on imitating the alternation process of attention. A detailed discussion of AMM is deferred to Section 2.3.2. We align the decoding process of EDT with the humans drawing pictures. Consequently, we incorporate local attention (AMM) into the decoding process (up-sampling phase) of EDT. Figure 4 illustrates the placement of AMM (local attention) in an EDT stage. As depicted in Figure 4(a), we alternately configure EDT blocks with and without the AMM, thereby mimicking the alternation between global and local attention observed in drawing activities. The EDT block with AMM is shown in Figure 4(b). As shown in Figure 4(c), the AMM is integrated into the self-attention module. The AMM and the global attention score matrix are combined via a Hadamard product to modulate global attention into local attention."}, {"title": "2.3.2 Attention modulation matrix", "content": "We develop the Attention Modulation Matrix (AMM) to modulate the default global attention in self-attention mechanisms into local attention, which imitates the local attention of humans during the act of drawing. Humans typically concentrate on either the actively engaged parts or the most salient aspects of a visual scene [31]. When drawing a specific local region of an image, areas closer to the region of interest tend to exhibit stronger contextual relations and thus warrant increased attention. Conversely, areas further from the region of interest generally show weaker contextual relations and can be allocated less attention. Thus, we articulate the principle: for a local region, the strength of attention on contextual relations within a specific region is inversely related to the distance between the local region and the specific region. In the self-attention mechanism, we regard the attention score between tokens as an indicator of the strength of attention on contextual relations between regions. Similarly, we aim to modulate the strength of attention based on the distance among tokens"}, {"title": "2.4 Token relation-enhanced masking training strategy", "content": "The ability to learn relations among object parts in images is crucial for image generation, as highlighted in MDT [22]. However, the down-sampling process in EDT inevitably leads to the loss of token information. Establishing relations among tokens can alleviate performance degradation caused by the loss of token information. To enhance the relation-learning ability in EDT, we introduce a relation-enhanced masking training strategy. Before detailing the proposed masking training strategy, we first explore the integration of MDT into EDT. Figure 5 (a) shows the masking training strategy of MDT. In MDT, the training loss $L$ contains two parts as shown in Eqn. 2.\n$\\mathcal{L} = \\mathcal{L}_{full} + \\mathcal{L}_{masked} = \\mathcal{L}_\\theta(x_t, c, \\epsilon_t) + \\mathcal{L}_\\theta(mask * x_t, c, \\epsilon_t)                                          (2)$\n$\\mathcal{L}_{full}$ is the loss when the input consists of the full token input, the $\\mathcal{L}_{masked}$ is the loss when the input consists of the remained tokens after masking, and $mask$ is a matrix to mask tokens randomly.\nHowever, our analysis reveals that the masking training method used in MDT excessively focuses on masked region reconstruction at the expense of diffusion training, potentially leading to a degradation in image generation performance. Additionally, our evaluation of MDT is observed a conflict between the training objectives of $\\mathcal{L}_{full}$ and $\\mathcal{L}_{masked}$. Specifically, as $\\mathcal{L}_{full}$ decreases, $\\mathcal{L}_{masked}$ increases, and vice versa, demonstrating the conflicting nature of these training objectives. To mitigate this conflict and allow the model to focus on the diffusion generation task, as shown in Figure 5 (b), we"}, {"title": "3 Experiment", "content": ""}, {"title": "3.1 Implementation Details", "content": "Models: We develop three different sizes of EDT including small (EDT-S), base (EDT-B) and extra large (EDT-XL), each using a patch size of two. Details regarding token dimensions, head numbers, and parameter counts are provided in Table 5 of Appendix A.2.2. Training and evaluation: The training dataset is ImageNet [32] with 256x256 and 512\u00d7512 resolution. For a fair comparison, we follow the training settings of MDTv2 [33]. EDT uses the Adan [34] optimizer with a global batch size of 256 and without weight decay. The learning rate linearly decreases from 1e-3 to 5e-5 over 400k iterations. Masking training strategy: We set the mask ratio 0.4 ~ 0.5 in the first down-sampling module, and 0.1 ~ 0.2 in the second. The investigation of mask ratio refers to Appendix A.4.2. GPUs: Training is conducted on eight L40 48GB GPUs, while the speed test for inference is performed on a single L40 48GB GPU. Evaluation metrics: Common metrics such as Fre'chet Inception Distance (FID) [35], sFID [36], Inception Score (IS) [37], Precision, and Recall [38] are used to assess the model performance. The training speed is evaluated by iterations per second, and inference speed is assessed by steps per second using a batch size of 256 in FP32. For fair comparison, we follow [21, 33] and employ the TensorFlow evaluation suite from ADM [4], reporting FID-50K results with 250 DDIM [10] sampling steps. These metrics are reported by default without the classifier-free guidance."}, {"title": "3.2 Comparison with SOTA transformer-based diffusion methods", "content": "To validate the enhancements in speed and generation performance of EDT, we conducted compar-isons with both classical methods [4, 11, 21] and recent advancements [33, 39, 40].\nExperiment on ImageNet 256\u00d7256 The result of image generation without classifier-free guidance is shown in Table 1. Our comparisons across three different sizes demonstrate that EDT consistently achieves the best FID scores: EDT-S scored an FID of 34.2, EDT-B scored 19.1, and EDT-XL scored 7.5. Notably, EDT also showed significant reductions in GFLOPs compared to the second-best MDTv2 across all sizes (2.66 GFLOPs vs. 6.07 GFLOPS, 10.2 GFLOPs vs. 23.02 GFLOPs, 51.83 GFLOPs vs. 118.69 GFLOPs). Moreover, EDT exhibited the lowest memory consumption during inference across all three sizes, underscoring the efficiency of our lightweight design. We further investigated the training speed of EDT. Given that both EDT and MDTv2 incorporate additional training strategies, we specifically compared the training speeds of these two models. Additionally, we assessed the training speed of EDT without the masking training strategy (denoted as EDT*) against other methods. In both scenarios, EDT trained faster than the baseline models. For example, EDT-XL* achieved a training speed of 1.49 iter/s, compared to 0.93 iter/s for DiT-XL. In comparison to MDTv2-XL, which trained at 0.51 iter/s, EDT-XL was nearly twice as fast at 0.98 iter/s. We further perform the experiment on the image generation with classifier-free guidance. The result is shown in Table 13 of Appendix A.5.1. Under the same training cost, EDT-S-G achieves the lowest FID score compared to MDTv2-S-G (9.89 vs. 15.62). Overall, these findings confirm that EDT significantly enhances both the speed and performance of image synthesis. Additionally, the training cost of EDT is efficient. We include a training cost analysis of EDT in Appendix A.2.3."}, {"title": "3.3 Ablation Study", "content": ""}, {"title": "3.3.1 Attention Modulation Matrix", "content": "Quantitative analysis We demonstrate the effectiveness and broad applicability of AMM across various models by comparing the FID scores between models with AMM and without AMM in Table 3. Extensive results show that the pre-trained models with AMM consistently outperform models without AMM, thereby verifying the generality and effectiveness of AMM. For instance, MDTv2-S with AMM achieves a better FID score than MDTv2-S without AMM (31.89 vs. 39.02). Using AMM enhances the FID of DiT-XL from 18.48 to 14.73. EDT-XL also has a lower FID score of 7.52 compared to EDT-XL without AMM of an FID score of 12.8.\nQualitative analysis We demonstrate the effectiveness of AMM by comparing the synthesis images from EDT-XL and DiT-XL with and without the AMM. As shown in Figure 6, the red boxes highlight the unrealistic regions in the images generated by EDT-XL without AMM. In the corresponding regions of the images generated by EDT-XL with AMM, the results appear more realistic. Moreover, the parrot image generated by EDT-XL without AMM is realistic and the parrot image generated by EDT-XL with AMM still remains equally realistic. This visual analysis demonstrates the effectiveness of the AMM plugin. Please refer to A.3, and A.5.2 for more analysis about AMM. While AMM is effective, there is potential for improvement. Please refer to A.6 for details regarding its limitations."}, {"title": "3.3.2 Lightweight-design diffusion transformer", "content": "We investigate the effectiveness of the key components in our proposed diffusion transformer architec-ture. We denote the token information enhancement as TIE and the positional encoding supplement"}, {"title": "3.3.3 Token relation-enhanced masking training strategy", "content": "We investigate the effectiveness of the token relation-enhanced masking training strategy and compare it with the training strategy used in MDT in Table 4. Model C does not employ any masking training strategy, with an FID score of 50.9 and an IS score of 31.0. Model D, a small-size EDT trained using the masking strategy of MDT, with an FID score of 49.6 and an IS score of 33.1. Model D shows only a slight improvement compared to Model C. Model E is a small-size EDT trained with the masking strategy of EDT, with an FID score of 46.9 and an IS score of 35.4. Model E achieved the best performance in terms of both FID and IS. This result suggests that the masking training strategy of EDT successfully improves performance by enhancing the learning ability of token relations."}, {"title": "4 Conclusions", "content": "In this work, we propose the Efficient Diffusion Transformer (EDT) framework, which includes a lightweight-design of diffusion transformer, a training-free Attention Modulation Matrix (AMM) inspired by human-like sketching, and the token relation-enhanced masking training strategy. Our lightweight-design reduces the number of tokens through down-sampling to lower computational costs. We redesigned down-sampling module and masking training strategy to address token information loss caused by the reduction of tokens. During inference, we introduce local attention through AMM, further enhancing image generation performance. Extensive experiments demonstrate that the EDT surpasses existing SOTA methods in both inference speed and image synthesis performance."}, {"title": "5 Related Work", "content": "Diffusion Probabilistic Models: Denoising diffusion probabilistic models (DDPM) [1], have marked a significant advancement in generative models. DDPM improves image generation by progressively reducing noise. ADM [4] innovates further by introducing a classifier-guided approach to refine the balance between image diversity and fidelity. Subsequent developments include a classifier-free method [9], which increases the flexibility of diffusion models by eliminating classifier constraints. DiT [21] replacing U-Net with transformer in LDM [11], achieving superior scalability. However, transformers-based models are computationally intensive. Efficient Diffusion: Various methods have been developed to enhance the efficiency of diffusion models. DDIM [10] redefines the diffusion process as non-Markovian, speeding up-sampling by removing dependencies on sequential time steps in DDPM [1]. LDM [11] reduces computational demands by transforming high-resolution images into a latent space for diffusion, thus balancing complexity with image detail. Current research in lightweight diffusion transformers is limited but offers potential for further efficiency improvements in diffusion model technologies. For more related work, please refer to Appendix A.1."}, {"title": "A.2.1 Applicable scenarios of the conventional down-sampling module", "content": "Firstly, we review the conventional down-sampling module. We have a token sequence of shape $N \\times N$, where $n$ is the number of tokens ($n = N^2$). The dimension of token is denoted by $d$. The number of heads in multi-head attention is $h$, and $D$ denotes the dimension of a head. Conventional down-sampling module [46\u201348] reduces the number of tokens by a factor of $k^2$ and increase the token dimensions by a factor of $k$ at the same time, where $k$ is down-sampling factor ($k \\geq 2$). Large down-sampling factor $k$ will cause too many tokens to be merged, harming performance. Therefore, $k$ is generally equal to 2. That means, in a $N \\times N$ token sequence, we reduce the number of tokens $n$ by down-sampling adjacent $2 \\times 2$ tokens into one. Then we obtain a $\\frac{N}{2} \\times \\frac{N}{2}$ token sequence with fewer tokens and the token dimensions increase to $2d$ from $d$. Table 6 shows FLOPs analysis of a DiT block. The total FLOPs $F$ is $2n^2d + 12nd^2 + 6d^2$ and the number of parameters $P$ is $18d^2$ in a DiT block.\nNow we analyze how much can down-sampling module reduce FLOPs, and its applicable scenarios. To facilitate derivation and calculation, we set $j = \\frac{n}{d}$, where $j$ is the proportional coefficient between the number of tokens and the dimension of token. We explore the relationship between proportional coefficient $j$ and FLOPs drop ratio $\\rho$ after using conventional down-sampling."}, {"title": "A.2.2 Redesign the down-sampling module", "content": "Now, we redesign the down-sampling to make the architecture meet rule (2).\nAccording to Appendix A.2.1, before the down-sampling module, the total FLOPs of a DiT block is $F = 2n^2d + 12nd^2 + 6d^2 = 2j^2d^3 + 12jd^3 + 6d^2$. And after the down-sampling module, the total FLOPs of a DiT block after down-sampling is $F' = \\frac{n^2}{4}d + 12\\frac{n}{2}d^2 + 24d^2$. Among the three items of $F$ and $F'$, the second term $12nd^2$ predominates due to $d > n$. However, this term isn't affected by the down-sampling process. This results from the conventional down-sampling module, which reduces the number of tokens by a factor of $k^2$ and increases the token dimensions by a factor of $k$ at the same time, namely $12nd^2 = 12 \\times \\frac{n}{4} \\times (kd)^2$\nTo reduce the second item $12nd^2$, we should redesign the process of down-sampling module: we reduce the number of tokens by down-sampling factor $k=2$ and increase the token dimensions by a factor of $r$ at the same time, where $r$ is token dimension expansion coefficient and $1 < r < k$. This design makes the second item reduce, namely $12 \\times \\frac{n}{4} \\times (rd)^2 < 12nd^2$, and the total FLOPs $F' = \\frac{rn^2}{4}d + 3n r^2d^2 + 6r^2d^2 = \\frac{rj^2}{4}d^3 + 3jr^2d^3 + 6r^2d^2$ in the blocks after our down-sampling"}, {"title": "A.3.1 The process of modulating the attention", "content": "The process of modulating the attention score matrix and the changes in tensor shape are shown in Figure 8. Image can be split into $N^2$ patches and each token is the feature of a patch. Each token (patch) corresponds to a rectangular area of the image and has a corresponding 2-D coordinate ($x, y$) in the image grid. We calculate an Euclidean distance value $d$ for each pair of tokens, resulting in a distance matrix $D$, which is an $N^2 \\times N^2$ tensor. Based on the distance matrix, we generate modulation values $m$ via the modulation matrix generation function $F(d)$, which assigns lower modulation values to tokens that are farther apart. These modulation values form an Attention Modulation Matrix (AMM), another $N^2 \\times N^2$ tensor. Importantly, we integrate the AMM into the pre-trained EDT without any additional training. The attention modulation matrix is calculated when the model is instantiated. During inference, the modulated attention score matrix is obtained by performing a Hadamard product between the attention modulation matrix and the attention score matrix."}, {"title": "A.4 Discussion about token relation-enhanced masking training strategy", "content": ""}, {"title": "A.5 Additional Results", "content": ""}]}