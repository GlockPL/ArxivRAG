{"title": "Deep Learning-Driven Segmentation of Ischemic Stroke Lesions Using Multi-Channel MRI", "authors": ["Ashiqur Rahman", "Muhammad E. H. Chowdhury", "Md Sharjis Ibne Wadud", "Rusab Sarmun", "Adam Mushtak", "Sohaib Bassam Zoghoul", "Israa Al-Hashimi"], "abstract": "Ischemic stroke, caused by cerebral vessel occlusion, presents substantial challenges in medical imaging\ndue to the variability and subtlety of stroke lesions. Magnetic Resonance Imaging (MRI) plays a crucial\nrole in diagnosing and managing ischemic stroke, yet existing segmentation techniques often fail to\naccurately delineate lesions. This study introduces a novel deep learning-based method for segmenting\nischemic stroke lesions using multi-channel MRI modalities, including Diffusion Weighted Imaging\n(DWI), Apparent Diffusion Coefficient (ADC), and enhanced Diffusion Weighted Imaging (eDWI). The\nproposed architecture integrates DenseNet121 as the encoder with Self-Organized Operational Neural\nNetworks (SelfONN) in the decoder, enhanced by Channel and Space Compound Attention (CSCA) and\nDouble Squeeze-and-Excitation (DSE) blocks. Additionally, a custom loss function combining Dice Loss\nand Jaccard Loss with weighted averages is introduced to improve model performance. Trained and\nevaluated on the ISLES 2022 dataset, the model achieved Dice Similarity Coefficients (DSC) of 83.88%\nusing DWI alone, 85.86% with DWI and ADC, and 87.49% with the integration of DWI, ADC, and eDWI.\nThis approach not only outperforms existing methods but also addresses key limitations in current\nsegmentation practices. These advancements significantly enhance diagnostic precision and treatment\nplanning for ischemic stroke, providing valuable support for clinical decision-making.", "sections": [{"title": "1. Introduction", "content": "Ischemic stroke is a severe and potentially fatal cerebrovascular disorder, marked by the abrupt cessation\nof blood flow to brain tissue, leading to cellular death and compromised neurological function [1]. The\nillness poses considerable difficulties in identification and therapy owing to the varied characteristics of\nischemia lesions and their intricate morphological alterations [2]. The diagnosis of ischemic stroke remains\nchallenging, with timely and accurate identification of lesions being crucial for effective intervention and\nimproved outcomes [3], [4]. Magnetic Resonance Imaging (MRI) is the gold standard for diagnosing\nischemic stroke, offering high-resolution images that provide detailed insights into the extent and severity\nof brain tissue damage [5], [6]. MRI's ability to visualize various tissue contrasts makes it indispensable for\nboth initial diagnosis and monitoring the progression or response to treatment [7], [8]. However, the\nvariability in lesion appearance ranging from acute infarctions to chronic infarcted areas\u2014presents\nsignificant challenges for accurate segmentation and interpretation [9]. This heterogeneity can be ascribed\nto variations in patient anatomy, the timing of the scan in relation to the stroke event, and the existence of\nadditional diseases [10], [11].\nDeep learning technologies have completely changed medical imaging, especially in terms of improving\npicture segmentation accuracy [12], [13]. Convolutional Neural Networks (CNNs), in particular, are deep\nlearning models that have shown remarkably effective in a variety of medical imaging applications by\nutilizing enormous datasets and advanced architectures to identify intricate patterns [14]. Segmenting MRI\nimages to delineate ischemic lesions is critical for assessing the extent of stroke, planning treatment, and\npredicting prognosis [15]. However, the efficacy of these computational approaches hinges on their ability\nto accurately distinguish and segment the diverse features of ischemic lesions, which can be challenging\ndue to the presence of background noise, motion artifacts, and similarities between lesions and normal brain\nstructures [16].\nThis study seeks to overcome significant research deficiencies in the segmentation of ischemia lesions\nfrom MRI images via deep learning methodologies. We propose a revolutionary deep learning system that\naddresses the complexity of ischemia lesion segmentation by advancements in models, representations, and\nmethodologies. Prior works in this domain have exhibited notable limitations, such as inadequate handling\nof background noise and motion artifacts, and the difficulty in distinguishing lesions from normal brain\nstructures. While previous research often focuses on enhancing feature extraction and localization, these\napproaches may fail to fully mitigate model confusion caused by extraneous elements in the images.\nThe integration of a DenseNet121 encoder with a Self-Organized Operational Neural Networks (SelfONN)\nChannel and Space Compound Attention (CSCA) U-Net decoder along with Double Squeeze-and-\nExcitation (DSE) block, within our segmentation model architecture allows for a nuanced understanding of\nboth local and global image features, enhancing segmentation accuracy. The DenseNet121 encoder is\ndesigned to efficiently capture and propagate features through densely connected layers, which helps in\nretaining critical information across the network.\nThe Cross-Layer Feature Fusion (CLFF) technique integrates features from various hierarchical levels\nof the decoder, allowing the model to acquire a more comprehensive and enriched representation of the\npicture data. Our method integrates information across many scales, thereby capturing nuanced details that\nconventional single-scale analysis may miss. This hierarchical feature fusion enables the model to utilize\nboth high-level semantic information and low-level spatial data, resulting in enhanced segmentation\naccuracy.\nWe also leverage multimodal learning by combining Diffusion-Weighted Imaging (DWI) and Apparent\nDiffusion Coefficient (ADC) modalities into a multi-channel image, enhancing the differentiation and\nsegmentation of ischemic lesions compared to using single modality images. DWI and ADC provide\ncomplementary information about the diffusion properties of water molecules in brain tissue, which is\ncrucial for identifying areas affected by ischemia. The varied information from these multiple sources\nenriches the feature representation, enabling the segmentation algorithm to better understand the visual\nproperties of different lesion regions. This multimodal approach improves the model's ability to detect and\nsegment lesions that might be challenging to identify using a single modality.\nThe key contributions of our study can be outlined as follows:\n\u2022\n\u2022\n\u2022\nWe propose a novel model combining a DenseNet121 encoder with a SelfONN CSCA U-Net\ndecoder, advancing feature learning capabilities through effective feature fusion techniques.\nWe leverage multimodal learning by integrating DWI and ADC modalities into a multi-channel\nimage, significantly improving segmentation accuracy across all lesion regions.\nWe propose a custom loss function combining Jaccard loss and Dice loss, enhancing the model's\nperformance in handling class imbalance and improving segmentation accuracy.\nThis article comprises five principal components. Section 2 examines the pertinent literature. Section 3\ndelineates the research technique employed in this study. Section 4 delineates the results and offers a\ncomprehensive examination of the model's efficacy. Finally, Section 5 closes the work."}, {"title": "2. Related Works", "content": "This section reviews significant studies on ischemic lesion segmentation using deep learning techniques\nwith a focus on MRI-based approaches. The key aspects considered in the literature include the\nmethodologies, datasets, imaging modalities, results, and limitations of various studies. Gheibi et al. (2023)\ndeveloped a CNN-Res architecture with a U-shaped structure, incorporating residual units and a bottleneck\nstrategy [17]. Evaluating their approach on datasets from Tabriz University and the SPES 2015 competition\nusing FLAIR and DWI modalities, they achieved DSC of 85.43% and 79.23%, respectively. The study\nhighlighted issues such as small lesion size bias in segmentation and potential gradient vanishing and\ndeepening network problems due to ResNet blocks. Bal et al. (2024) employed various MRI modalities\n(T1, T2, DWI, and FLAIR) from the ISLES2015 dataset in their pre-processing, patch creation, data\naugmentation, and CNN classification approach [18]. They reported a DSC of 0.85. However, they noted\nthe limited use of multiple MRI modalities in existing models and the need for more advanced deep learning\nmethods for ischemic stroke lesion segmentation.\nWu et al. (2023) proposed a W-Net model integrating CNN and transformer-based methods for lesion\nsegmentation [19]. Their approach, tested on the ATLAS (with T1, ADC, DWI, and FLAIR) and\nISLES2022 datasets, outperformed existing methods with DSC of 61.76 and 76.47, Hausdorff Distance\n(HD) of 32.47, and F2 scores of 64.60. Challenges included variability in lesion areas due to individual\ndifferences and difficulties in generating trusted boundaries for stroke lesions. Alshehri et al. (2023)\nintegrated a few-shot learning strategy with a base CNN model for segmentation using the ISLES 2015\ndataset and FLAIR and DWI modalities [20]. With a DSC of 0.68, they were able to identify the difficulties\nin comparing stroke segmentation methods with the help of the ISLES SISS challenge dataset.\nThiyagarajan et al. (2023) utilized an Arithmetic Optimization-based K-Means (AOK-Means) approach\nfor cluster center optimization [21]. Applying their method to the ISLES2015 dataset with DWI modality,\nthey reported DSC of 70.8% and 76.4%, with precision values of 90.4% and 91.9%. Challenges\nencompassed the incapacity to identify clusters prior to segmentation and the obstruction posed by random\ncluster center initialization in attaining a globally optimal solution. Moon et al. (2022) assessed 2D and 3D\nU-Net convolutional neural network architectures for the segmentation of stroke lesions utilizing brain\nscans from 79 acute ischemic stroke patients, employing FLAIR and DWI modalities [22]. The optimal\noutcomes were attained using a 2D multimodal U-Net, resulting in a mean DSC of 0.737. The study\nencountered constraints in delineating brainstem stroke lesions owing to insufficient data and the difficulty\nposed by an uneven distribution of lesion sizes.\nJazzar et al (2022) developed a modified U-Net architecture with multi-path integration, achieving a\nDSC of 0.84 on the ISLES2015 dataset using FLAIR and DWI modalities [23]. They addressed class\nimbalance through optimized loss functions, improving robustness in segmentation. Platscher et al. (2022)\nexplored the application of generative models, specifically Pix2Pix, CycleGAN, and SPADE, for the\npurpose of stroke lesion labeling and brain segmentation [24]. Utilizing a dataset comprising 804 cases\nfrom the University Hospital Basel and employing the DWI modality, they achieved a DSC of 72.8%.\nDespite this, their study encountered notable challenges. The use of synthetic data was constrained by data\nprivacy concerns, and the models trained exclusively on this synthetic data demonstrated insufficient\ncompetitive performance.\nWith the use of the DWI modality from the Houston Methodist Hospital Stroke Registry, Wong et al.\n(2022) presented a deep learning model with rotation-reflection equivariance for stroke volume\nsegmentation [25]. Their model received an 85% Dice score; nevertheless, segmentation performance was\nimpacted by differences in MRI images caused by scan parameters. Using the ISLES 2015 dataset with\nT1, T2, DWI, and FLAIR modalities, Karthik et al. (2020) suggested a Fully Convolutional Network (FCN)\nwith an attention mechanism for ischemic lesion segmentation [26]. They reported a DSC of 0.75,\nemphasizing issues such CNNs' inability to provide local context for accurate segmentation and their\ninability to distinguish between features because of modality differences.\nThe aforementioned research shows how deep learning methods can enhance the precision and\neffectiveness of ischemic lesion segmentation from MRI images. Despite notable advancements, challenges\nsuch as small lesion size bias, variability in lesion areas, and the need for large, annotated datasets remain.\nFuture research should focus on overcoming these limitations, enhancing model robustness, and exploring\nthe integration of multiple MRI modalities to further improve segmentation performance."}, {"title": "3. Experimental Methodology", "content": "This section outlines the experimental framework employed to build and assess our proposed model for\nischemic lesion segmentation in MRI images. The ISLES 2022 dataset, comprising DWI and ADC\nmodalities, underwent pre-processing to improve lesion visibility and maintain uniform input dimensions.\nOur proposed model leverages DenseNet121 as the encoder and SelfONN in the decoder, with the\nintegration of CSCA attention mechanisms and DSE blocks to refine feature extraction. CLFF was\nemployed to retain critical details during up-sampling. The model was optimized using a composite loss\nfunction combining Dice and Jaccard losses for improved segmentation accuracy. The overall process is\ndepicted in Figure 1, illustrating the various stages from dataset preparation to model evaluation."}, {"title": "3.1 Dataset Descriptions", "content": "The ISLES 2022 dataset is a multi-center collection of MRI scans, meticulously annotated for the\nsegmentation of acute and subacute ischemic stroke lesions [27]. This dataset, central to the ISLES\nchallenge, includes 400 MRI cases from various medical centers, utilizing different MRI machines and\nprotocols. The cases vary in lesion size, location, and ischemic stage. Annotations were performed using a\nhybrid human-algorithm approach, enhancing precision through expert radiologist review and refinement.\nThe dataset is divided into 250 training cases, publicly available for model development, and 150 testing\ncases reserved for independent validation. Each training case includes images from three MRI modalities-\nFLAIR, DWI, and ADC-comprising 52,739 FLAIR slices and 15,684 slices each for DWI and ADC, all\npaired with segmentation masks. This dataset supports the development of robust models capable of\ngeneralizing across diverse patient populations and imaging conditions, ultimately aiding in clinical\ndecision-making and improving patient outcomes."}, {"title": "3.2 Data Preprocessing", "content": "In this study, we focused on the DWI and ADC modalities of the ISLES 2022 dataset, excluding FLAIR\nimages to concentrate on the most relevant inputs for ischemic lesion segmentation. The MRI scans, initially\nin Neuroimaging Informatics Technology Initiative (NIfTI) format, were converted to PNG and resized to\n256x256 pixels to ensure consistent input dimensions for our deep learning model. To enhance lesion\nvisibility, we applied Contrast Limited Adaptive Histogram Equalization (CLAHE) to the DWI images,\nimproving the contrast and making subtle features more distinguishable. We then created two types of\ncomposite images: a 2-channel image combining DWI and ADC, and a 3-channel image incorporating\nDWI, ADC, and the enhanced DWI. This approach was designed to provide the model with rich and\ncomplementary information from the different MRI modalities, ultimately enhancing its ability to\naccurately segment ischemic lesions by capturing a broader range of critical features."}, {"title": "3.3 Experimental Details", "content": "To mitigate bias and prevent data leakage, a patient-wise data splitting approach was employed, as\nopposed to the conventional random image splitting method. This approach ensures that all data pertaining\nto a single patient is consistently allocated to either the training, validation, or test set. This approach offers\na more dependable assessment of the model's ability to generalize to unfamiliar patient data [28]. The\ndataset, consisting of 15,684 2D slices from 250 patients, was partitioned into training, validation, and\ntesting sets in the ratios of 70%, 10%, and 20%, respectively. Furthermore, 5-fold cross-validation was\nutilized to provide a rigorous assessment. The computational setup included an Intel Core i9-13900K CPU\nfeaturing 24 cores at 3.00 GHz, an NVIDIA GeForce RTX 4090 GPU with 24 GB of VRAM, and 64 GB\nof DDR5 RAM. The configuration employed CUDA 11.8 and PyTorch 2.3.1 to enable efficient execution\nof deep learning operations."}, {"title": "3.4 Our Proposed Model", "content": "Our proposed model, illustrated in Figure 2, introduces a novel approach to ischemic lesion\nsegmentation by leveraging the strengths of DenseNet121 as the encoder and SelfONN in the decoder.\nAdditionally, the model integrates CSCA blocks and DSE blocks to enhance feature extraction and improve\nsegmentation accuracy. This unique combination aims to provide superior performance in medical image\nsegmentation tasks."}, {"title": "3.4.1 Encoder: DenseNet121", "content": "To achieve efficient and robust feature extraction, we incorporated DenseNet121 as the encoder in\nour model. Figure 3 illustrates how DenseNet121, known for its dense connectivity through densely\nconnected convolutional blocks, facilitates better gradient flow and feature reuse, which are crucial for\nmedical image segmentation [29]. We utilized the pre-trained DenseNet121 up to its last dense block,\nexcluding the classification layer, to extract rich and detailed feature maps from the input images.\nDenseNet121 organizes its layers into blocks, with each layer directly linked to every other layer in\na feed-forward manner. This intricate connectivity structure alleviates the vanishing gradient issue,\npromotes feature reutilization, and markedly decreases the parameter count relative to conventional\nconvolutional networks. The features obtained via DenseNet121 function as the input for our innovative\ndecoder architecture."}, {"title": "3.4.2 SelfONN", "content": "Self-ONNs enhance Operational Neural Networks (ONNs) by incorporating generative neurons that\nadapt and optimize their nodal operators during training [30]. This adaptation results in greater flexibility\nand computational efficiency, allowing each neuron to generate and optimize a combination of nodal\noperators.\nIn conventional CNNs, the input map for the k \u2013 th neuron in the current layer, denoted as $x_k^l$, is\nderived from the following equation:\n$x_k^l = b_k^l + \\sum_{i=1}^{N_{l-1}} conv2D(w_{ki}^l, y_i^{l-1})$\n(1)\n$x_k^l(m, n) = \\sum_{r=0}^{2} \\sum_{t=0}^{2} (w_{ki}^l(r, t)y_i^{l-1}(m+r, n + t)) + ...$\n(2)\nONNs extend this by incorporating nodal and pool operators, described by:\n$x_k^l = b_k^l + \\sum_{i=1}^{N_{l-1}} oper2D (w_{ki}^l, y_i^{l-1})$\n(3)"}, {"title": "3.4.3 Channel and Space Compound Attention (CSCA)", "content": "We integrate CSCA mechanism is designed to enhance feature representations by integrating both\nchannel and spatial attention mechanisms [31]. The detailed architecture of the CSCA mechanism is\nillustrated in Figure 4. The CSCA mechanism starts with an input feature map that is processed along three\ndistinct paths.\nIn path (1), the input feature map is passed through a DSE module, which emphasizes important\nchannel-wise features and produces an intermediate feature map Fr. Subsequently, this map undergoes\nfurther refinement via a series of convolutional layers. A 3x3 convolution is applied, followed by batch\nnormalization (BN), and a further 3x3 convolution layer generates the final channel-wise attention feature\nmap Fv.\nIn path (2), the input feature map is analyzed to provide spatial attention features. The input feature\nmap undergoes initial processing using a 1x1 convolution layer, succeeded by batch normalization. This is\nsucceeded by an additional 1x1 convolutional layer. A sigmoid activation function is subsequently used,\ngenerating a spatial attention map Fq.\nIn path (3), the input feature map undergoes another spatial processing sequence. The input feature\nmap is processed through a 3x3 convolution layer followed by BN, and then another 3x3 convolution layer.\nThis feature map is further refined through a DSE module, emphasizing crucial spatial features and\nproducing the final spatial-wise feature map Fk.\nFinally, the outputs from these paths are combined. The channel-wise feature map Fv is combined\nwith the spatial attention map Fs (obtained by multiplying Fq with Fk elementwise) through a matrix product\noperation, resulting in the intermediate feature map Fatt. The input feature map also generates Fr, through\nanother path, which is then multiplied elementwise with Fatt to produce the final attention-enhanced feature\nmap Ffinal. The CSCA mechanism improves the model's focus on key features by incorporating both\nchannel and spatial attention, leading to enhanced performance in tasks such as segmentation and\nclassification."}, {"title": "3.4.4 Double Squeeze-and-Excitation (DSE)", "content": "We incorporated a channel attention mechanism called the DSE block in the bottleneck layer to\nfurther enhance high-level semantic features. The DSE block adjusts channel weights to better capture\nimportant details in the input features by utilizing global average pooling (GAP) and global maximum\npooling (GMP) [32]. The structure of the DSE block is illustrated in Figure 5.\nFirst, the input feature map $F_{input}$ undergoes global average pooling to produce a weight vector $W_{avg}$."}, {"title": "3.4.5 Cross-Layer Feature Fusion (CLFF)", "content": "To address the loss of information during up-sampling, we employed CLFF in the decoder. The CLFF\ncombines features from different layers, ensuring the retention and effective merging of crucial details [33].\nThis technique aids in recovering lost information, thereby contributing to the improved segmentation\naccuracy of the model. Mathematically, CLFF can be represented as:\n$F_{fused} = \\sigma(W_i * F_i + W_j * F_j)$\n(7)\nwhere Fi and Fj are the feature maps from layers i and j, respectively, $W_i$ and $W_j$ are learnable\nweight matrices applied to $F_i$ and $F_j$, o is an activation function, and * denotes the convolution operation.\nThis fused feature map $F_{fused}$ is then utilized in subsequent layers of the decoder, enhancing segmentation\nperformance by integrating multi-scale contextual information."}, {"title": "3.4.6 Loss Function", "content": "In this study, we proposed a composite loss function by integrating Dice and Jaccard losses to\nimprove the segmentation model's performance. Dice Loss, based on the Dice coefficient, quantifies the\noverlap between the predicted segmentation and the actual labels, mitigating class imbalance by\nemphasizing the regions of interest [34]. The Dice coefficient D is defined as:\n$D = \\frac{2 \\sum_i(p_i g_i)}{\\sum_i(p_i) + \\sum_i(p_i) + \\epsilon}$\n(8)\nwhere $p_i$ is the predicted probability for pixel i, $g_i$ is the ground truth label for pixel i, and $\\epsilon$ is a\nsmall constant added for numerical stability. Dice Loss $L_d$ is then calculated as:\n$L_d = 1 - D$\n(9)\nJaccard Loss, based on the Jaccard Index, similarly quantifies the similarity between the predicted\nand actual segmentations, ensuring accurate boundary predictions [35]. The Jaccard Index J is defined as:\n$J = \\frac{2 \\sum_i(p_i g_i)}{\\sum_i(p_i + g_i - p_i g_i) + \\epsilon}$\n(10)\nand the Jaccard Loss $L_j$ is calculated as:\n$L_j = 1 - J$\n(11)\nBy combining these two loss functions, we leverage their respective strengths to achieve superior\nsegmentation performance. The composite loss function is defined as a weighted sum of Dice Loss and\nJaccard Loss:\n$L = 0.5 \u00b7 L_d + 0.5 \u00b7 L_j$\n(12)"}, {"title": "3.4.7 Evaluation Metrics", "content": "To assess the efficacy of our segmentation model, we employed several critical evaluation metrics:\nDice Similarity Coefficient (DSC), Intersection over Union (IoU), Precision, and Recall.\nThe Dice Similarity Coefficient (DSC) quantifies the overlap between the ground truth and the anticipated\nsegmentation. It is delineated as:\n$DSC = \\frac{2 \\times TP}{2 \\times TP + FP + FN}$\n(13)\nThe IoU quantifies the similarity between the predicted and ground truth segmentations. It is\ncalculated as:\n$IoU = \\frac{TP}{TP + FP + FN}$\n(14)\nPrecision (P) measures the accuracy of positive predictions. It is given by:\n$Precision = \\frac{TP}{TP + FP}$\n(15)\nRecall (R), also known as Sensitivity, assesses the model's ability to identify all relevant instances. It\nis defined as:\n$Recall = \\frac{TP}{FN + TP}$\n(16)\nwhere TP are True Positives, FP are False Positives, and FN are False Negatives."}, {"title": "4. Results and Discussion", "content": "This section presents the results of our experiments and provides an in-depth discussion of the findings.\nWe evaluate the performance of the proposed model against baseline models, assess its effectiveness on\ndifferent imaging modalities, and analyze the impact of various loss functions on segmentation accuracy.\nThe results are quantified using standard performance metrics and illustrated through qualitative examples."}, {"title": "4.1 Comparison with Baseline Models", "content": "Our proposed model outperforms several of the best-performing baseline models, including\nDenseNet121_UNet, DenseNet121_UNet++, and DeepLabV3, as demonstrated in Table 1. For training and\nevaluation, we utilized 3-channel input images consisting of DWI, ADC, and enhanced DWI modalities.\nThe model achieved the highest Dice Similarity Coefficient (87.49%) and Intersection over Union\n(84.63%), indicating superior accuracy and overlap with the ground truth. While the precision (94.87%) is\nslightly lower than that of DenseNet121_UNet (95.01%), our model's higher recall (88.57%) reflects a\nbetter balance between detecting true positives and minimizing false negatives."}, {"title": "4.2 Comparison Multimodal Learning", "content": "We assessed the performance of our proposed ischemic lesion segmentation model with three distinct\ninput setups: (1) using only DWI, (2) combining DWI and ADC, and (3) integrating DWI, ADC, and\nenhanced DWI."}, {"title": "4.3 Comparison of Loss Functions", "content": "The proposed combined loss function, which integrates Dice Loss and Jaccard Loss, demonstrated\nsuperior performance in ischemic lesion segmentation compared to using Dice or Jaccard Loss alone. As\nshown in\nTable 3, The Combined Loss function achieved the highest DSC of 87.49% and the highest IoU of\n84.63%, indicating better segmentation accuracy and overlap with the ground truth. It also delivered the\nhighest precision at 94.87%, with a slight trade-off in recall (88.57%) compared to Dice Loss (88.75%).\nThis balance between precision and recall suggests that the combined loss effectively reduces both false\npositives and false negatives."}, {"title": "4.4 Comparison with State-of-the-Art Models", "content": "We compared our segmentation results against the top-performing models from the ISLES 2022\nChallenge, which serves as a benchmark for ischemic stroke lesion segmentation using modalities like\nDWI, ADC, and FLAIR modalities [36]. The top three models in the challenge were developed by teams\nSEALS, NVAUTO, and SWAN. The SEALS team's model, utilizing a 3D nnUNet architecture with DWI\nand ADC inputs, a dice loss, and categorical cross-entropy, produced a mean DSC of 82.0%. The model by\nteam NVAUTO reached a mean DSC of 82.0% as well, employing a SegResNet-based solution with\nvarious data augmentation techniques and a Dice-Focal loss function [37]. The created by SWAN team\nobtained a mean DSC of 81.0%, utilizing a Swin Transformer architecture integrated with Non-negative\nMatrix Factorization (NMF) layers [38]. In contrast, our Model achieved a significantly higher DSC of\n87.4%, representing an improvement of 5.4% over the SEALS and NVAUTO models and 6.4% over the\nSWAN model, as summarized in Table 4."}, {"title": "5. Limitations and Directions for Future Research", "content": "Although this study has shown strong performance in segmenting ischemic stroke lesions, it has several\nnotable limitations. The study relied on the ISLES 2022 dataset, which is focused specifically on ischemic\nstroke lesions. However, in real-world clinical scenarios, patients may present with various neurological\nconditions beyond ischemic strokes, such as tumors, hemorrhages, or other brain pathologies that might\nexhibit similar imaging characteristics. For instance, certain brain tumors or hemorrhagic lesions could\nmimic the appearance of ischemic stroke lesions due to overlapping signal characteristics on MRI scans\n[39]. This limitation highlights that our model's ability to accurately differentiate ischemic strokes from\nother brain conditions could not be thoroughly evaluated, raising concerns about its generalizability in\nclinical settings where multiple conditions are present simultaneously. Another drawback of this study is\nthe exclusive use of DWI, ADC, and enhanced DWI modalities. Although these modalities are crucial for\nidentifying ischemic strokes, the exclusion of other imaging techniques, such as FLAIR or T2-weighted\nimages, could limit the model's robustness [40]. In clinical settings, these additional modalities offer\ncrucial insights into lesion characteristics, and their exclusion from the current study may limit the\nmodel's effectiveness in more complex diagnostic situations. Additionally, the model sometimes\nstruggles with accurately segmenting very small infarcts. Small infarcts can be challenging to detect and\ndelineate due to their subtle appearance on MRI scans and the limited spatial resolution of the imaging\nmodalities used [41]. This limitation is particularly concerning in clinical practice, where the accurate\nidentification of even the smallest infarcts can be critical for timely and appropriate treatment.\nTo better align with the complexities of real-world clinical needs, future research will focus on\nexpanding the dataset to include a broader range of brain pathologies, such as tumors, hemorrhages, and\nother conditions. This expansion will enable the model to learn and accurately differentiate ischemic stroke\nlesions from other brain conditions, thereby improving its generalizability and clinical applicability. In\naddition, future work should explore the inclusion of other imaging modalities like FLAIR and T2-weighted\nimages. These modalities could enhance the model's robustness and its ability to detect and analyze lesions\nwith greater accuracy. Expanding the scope of the model to integrate these additional modalities will move\nus toward developing a more comprehensive and versatile tool that can serve diverse clinical situations and\nimprove patient care. Efforts should also be directed toward improving the model's sensitivity to very small\ninfarcts. Techniques such as enhancing the model's spatial resolution capabilities or implementing\nspecialized loss functions that emphasize small infarct detection could be explored. Addressing this\nlimitation will be crucial for ensuring that the model can perform effectively in all clinical scenarios,\nincluding those where the identification of small infarcts is vital."}, {"title": "6. Conclusions", "content": "This study presents a novel deep learning framework for ischemic lesion segmentation in MRI images,\nspecifically tailored to the ISLES 2022 dataset. The proposed a novel model architecture, which integrates\nDenseNet121 as the encoder, SelfONN in the decoder, and advanced CSCA and DSE blocks, achieved\nsuperior segmentation accuracy. A critical contribution of this research is the development of a composite\nloss function that equalizes the weights of Dice and Jaccard losses, significantly enhancing the model's\nperformance by ensuring a balanced optimization of segmentation metrics. Additionally, our approach\nleveraged multimodal input images, combining DWI, ADC, and enhanced DWI modalities. This\nmultimodal strategy allowed the model to capture complementary information from different imaging\ntechniques, further improving segmentation accuracy and robustness. The use of these combined modalities\nproved particularly effective in challenging cases, demonstrating the model's ability to generalize across\nvarying lesion presentations. Our model's ability to beat traditional techniques highlights the possibility of\nincorporating it into clinical operations, providing a reliable and efficient tool for stroke diagnosis and\nmanagement. The combination of a novel loss function, advanced architectural components, and\nmultimodal input strategies positions our model as a promising candidate for real-world clinical\napplications. Future work will focus on refining the model further, expanding its applicability across diverse\nmedical imaging challenges, and promoting the adoption of AI-driven solutions in healthcare, ultimately\ncontributing to improved patient care and advancements in medical imaging."}, {"title": "Data Availability Statement", "content": "The dataset utilized in this study can be obtained upon request from the corresponding author."}, {"title": "Conflict of Interest", "content": "The authors have no conflicts of interest to disclose for this study."}]}