{"title": "Edify 3D: Scalable High-Quality 3D Asset Generation", "authors": ["NVIDIA"], "abstract": "We introduce Edify 3D, an advanced solution designed for high-quality 3D asset generation. Our method first synthesizes RGB and surface normal images of the described object at multiple viewpoints using a diffusion model. The multi-view observations are then used to reconstruct the shape, texture, and PBR materials of the object. Our method can generate high-quality 3D assets with detailed geometry, clean shape topologies, high-resolution textures, and materials within 2 minutes of runtime.", "sections": [{"title": "1. Introduction", "content": "The creation of detailed digital 3D assets is essential for developing scenes, characters, and environments across various digital domains. This capability is invaluable to industries such as video game design, extended reality, film production, and simulation. For 3D content to be production-ready, it must meet industry standards, including precise mesh structures, high-resolution textures, and material maps. Consequently, producing such high-quality 3D content is often an exceedingly complex and time-intensive process. As demand for 3D digital experiences grows, the need for efficient, scalable solutions in 3D asset creation becomes increasingly crucial."}, {"title": "2. Multi-View Diffusion Model", "content": "The process of creating multi-view images is similar to the design of video generation (Brooks et al., 2024; Chen et al., 2024). We finetune text-to-image models into pose-aware multi-view diffusion models by conditioning them with camera poses. The models take a text prompt and camera poses as input and synthesize the object's appearance from different viewpoints. We train the following models:\n1. A base multi-view diffusion model that synthesizes the RGB appearance conditioned on the input text prompt as well as the camera poses.\n2. A multi-view ControlNet (Zhang et al., 2023) model that synthesizes the object's surface normals, conditioned on both the multi-view RGB synthesis and the text prompt."}, {"title": "2.1. Ablation Studies", "content": "Scaling with respect to the number of viewpoints. During inference, we can sample an arbitrary number of views while maintaining good multi-view consistency, as shown in Fig. 4. Generating more views allows for broader coverage of the object's regions in the multi-view images. As we later discuss in Sec. 3, the quality of the resulting 3D reconstruction is positively correlated to the number of multi-view observations (Furukawa and Ponce, 2009). Therefore, the ability of the multi-view diffusion model to synthesize denser viewpoints is critical to the final 3D generation quality.\nTraining across different numbers of viewpoints. During training, we sample 1, 4, or 8 views for each training object, assigning different sampling ratios for each number of views. While training with a varying number of views enables sampling an arbitrary number of views during inference, it is still preferable to match the training views to those expected during inference. This helps minimize the gap between training and inference performance. We compare between two models one trained primarily on 4-view images and one on 8-view images -- and sample 10-view images at the same viewpoints. As shown in Fig. 5, the model trained mostly with 8-view images produces more natural looking images with better multi-view consistency across views compared to that trained mostly with 4-view images."}, {"title": "3. Reconstruction Model", "content": "Extracting 3D structure from image observations is typically referred to as photogrammetry, which has been widely applied to many 3D reconstruction tasks (Li et al., 2023; Mildenhall et al., 2020; Wang et al.,"}, {"title": "Training", "content": "We train our reconstruction model using large-scale imagery and 3D asset data. The model is supervised on depth, normal, mask, albedo, and material channels through SDF-based volume rendering, with outputs rendered from artist-generated meshes. Since surface normal computation is relatively expensive, we compute the normal only at the surface and supervise against the ground truth. We find that aligning the uncertainty of the SDF (Yariv et al., 2021) with the corresponding rendering resolution improves the visual quality of the final output. Additionally, we mask out object edges during loss computation to avoid noisy samples caused by aliasing. To smooth noisy gradients across samples, we apply exponential moving average (EMA) to aggregate the final reconstruction model weights."}, {"title": "Mesh post-processing", "content": "After obtaining the dense triangular 3D mesh from isosurface extraction, we post-process the mesh with the following steps:\n1. Retopologize into a quadrilateral (quad) mesh with simplified geometry and adaptive topologies.\n2. Generate the UV mapping based on the resulting quad mesh topology.\n3. Bake the albedo and material neural fields into a texture map and material map, respectively.\nThese post-processing steps make the resulting mesh more suitable for further editing, essential for artistic and design-oriented downstream applications."}, {"title": "3.1. Ablation Studies", "content": "We study the scaling properties of the reconstruction model in the following aspects: (a) the number of input images and (b) the tokens representing the shapes."}, {"title": "Experimental setup", "content": "For validation, we randomly select 78 shapes from a held-out dataset. We report the LPIPS (Zhang et al., 2018) score on the albedo prediction to quantify the base texture reconstruction performance. For material prediction accuracy, we use the L2 error on the roughness and metallic values. We also use the L2 error between the ground-truth and predicted depths as a proxy for evaluating the geometry accuracy of the reconstructed shapes. The camera poses used for input and output are fixed at an elevation of 20\u00b0, pointing towards the origin (Fig. 6)."}, {"title": "4. Data Processing", "content": "Edify 3D is trained on a combination of non-public large-scale imagery, pre-rendered multi-view images, and 3D shape datasets. We focus on pre-processing 3D shape data in this section. The raw 3D data undergo several preprocessing steps to achieve the quality and format required for model training."}, {"title": "Format conversion", "content": "The first step of the data processing pipeline involves converting all 3D shapes into a unified format. We triangulate the meshes, pack all texture files, and convert the materials into metallic-roughness format. We discard shapes with corrupted textures or materials. This process results in a collection of 3D shapes that can be rendered as intended by the original creators."}, {"title": "Quality filtering", "content": "We filter out non-object-centric data from the large-scale 3D datasets. We render the shapes from multiple viewpoints and use Al classifiers to remove partial 3D scans, large scenes, shape collages, and shapes containing auxiliary structures such as backdrops and ground planes. To ensure quality, this process is conducted through multiple rounds of active learning, with human experts continuously curating challenging examples to refine the Al classifier. Additionally, we apply rule-based filtering to remove shapes with obvious issues, such as those that are excessively thin or lack texture."}, {"title": "Canonical pose alignment", "content": "We align our training shapes to their canonical poses to reduce potential ambiguity when training the model. Pose alignment is also achieved via active learning. We manually curate a small number of examples, train a pose predictor, look for hard examples in the full dataset, and repeat the process. Defining the canonical pose is also critical. While many objects such as cars, animals, and shoes already have natural canonical poses, other shapes may lack a clear front side, in which case we define the functional part as the front and prioritize maintaining left-right symmetry."}, {"title": "PBR rendering", "content": "To render the 3D data into images for the diffusion and reconstruction models, we use an in-house path tracer for photorealistic rendering. We employ a diverse set of sampling techniques for the camera parameters. Half of the images are rendered from fixed elevation angles with consistent intrinsic parameters, while the remaining images are rendered using random camera poses and intrinsics. As a constraint, we maintain roughly consistent object sizes within the rendered images. This approach accommodates both text-to-3D use cases (where there is full control over the camera parameters) and image-to-3D use cases (where the reference image may come from a wide range of camera intrinsics)."}, {"title": "Al captions", "content": "To caption the 3D shapes, we render one image per shape and use a vision-language model (VLM) to generate both long and short captions for the image. To enhance the comprehensiveness of captions, we also provide the metadata of the shape (e.g. title, description, category tree) to the VLM."}, {"title": "5. Results", "content": "We showcase text-to-3D generation results from Edify 3D in Fig. 8 and image-to-3D generation in Fig. 9. The generated meshes include detailed geometry and sharp textures, with well-decomposed albedo colors that represent the surface's base color. For image-to-3D generation, Edify 3D not only accurately recovers the underlying 3D structures of the reference object, but it also can generate detailed textures in regions of the surface not directly observed in the input image.\nThe assets generated by Edify 3D come in the form of quad meshes with well-organized topologies, as visualized in Fig. 10. These structured meshes allow for easier manipulation and precise adjustments,"}, {"title": "6. Related Work", "content": "3D asset generation. The challenge of 3D asset generation is often addressed by training models on 3D datasets (Gupta et al., 2023; Jun and Nichol, 2023; Nichol et al., 2022; Zeng et al., 2022), but the scarcity of these datasets limits the ability to generalize. To overcome this, recent methods have shifted towards using models trained on large-scale image and video datasets. Score Distillation Sampling (SDS) (Poole et al., 2023) has been adopted in earlier methods (Huang et al., 2023; Lin et al., 2023; Sun et al., 2024; Tang et al., 2023; Wang et al., 2023, 2024; Yi et al., 2023; Zhu and Zhuang, 2023) and extended to image-conditioned 3D generative models (Liu et al., 2023; Long et al., 2024; Qian et al., 2024; Tang et al., 2023; Wang and Shi, 2023; Yu et al., 2023). However, they often experience slow processing (Lorraine et al., 2023; Xie et al., 2024) and are susceptible to issues such as the Janus face issue (Shi et al., 2023). To improve performance, newer techniques integrate multi-view image generative models, focusing on producing multiple consistent views that can be reconstructed into 3D models (Chan et al., 2023; Chen et al., 2024; Gao et al., 2024; H\u00f6llein et al., 2024; Liu et al., 2024; Shi et al., 2023,; Tang et al., 2024; Weng et al., 2023; Yang et al., 2024). However, maintaining consistency across these views remains a challenge, leading to the development of methods that enhance reconstruction robustness from limited views (Li et al., 2024; Liu et al., 2024).\n3D reconstruction from multi-view images. 3D asset generation from limited views often involves 3D reconstruction techniques, often with differentiable rendering, that can utilize various 3D representations such as Neural Radiance Fields (NeRF) (Mildenhall et al., 2020). Meshes are the most commonly used format in industrial 3D engines, yet reconstructing high-quality meshes from multi-view images is challenging. Traditional photogrammetry pipelines, including structure from motion (SfM) (Agarwal et al., 2011; Sch\u00f6nberger and Frahm, 2016; Snavely et al., 2006), multi-view stereo (MVS) (Furukawa and Ponce, 2009; Sch\u00f6nberger et al., 2016), and surface extraction (Kazhdan et al., 2006; Lorensen and Cline, 1998; Shen et al., 2021, 2023), are costly and time-consuming, often yielding low-quality results. While NeRF-based neural rendering methods can achieve high-quality 3D reconstructions (Gu\u00e9don and Lepetit, 2024; Huang et al., 2024; Kerbl et al., 2023; Li et al., 2023; Wang et al., 2021; Yariv et al., 2021), they require dense images and extensive optimization, and converting radiance fields into meshes can lead to suboptimal results. To address these limitations, Transformer-based (Vaswani et al., 2017) models further improve 3D NeRF reconstruction from sparse views by learning a feed-forward prior (Hong et al., 2023).\nTexture and material generation. Earlier approaches targeting 3D texture generation given a 3D shape include CLIP (Radford et al., 2021) for text alignment (Michel et al., 2022; Mohammad Khalid et al., 2022) and SDS loss optimization (Poole et al., 2023). To improve 3D awareness, some text-to-3D methods combine texture inpainting with depth-conditioned diffusion (Chen et al., 2023), albeit slower and more artifact-prone. To enhance consistency, other techniques alternate diffusion with reprojection (Cao et al., 2023) or generate multiple textured views simultaneously (Deng et al., 2024), though at a higher computational cost. To further enhance realism, some methods have enabled multi-view PBR modeling (Boss et al., 2022; Zhang et al., 2021) to extend support for generating material properties (Chen et al., 2023; Qiu et al., 2024; Xu et al., 2023)."}, {"title": "7. Application: 3D Scene Generation", "content": "In this section, we demonstrate an application of our Edify 3D model to scalable 3D scene generation (Bahmani et al., 2023). High-quality, large-scale 3D scenes are pivotal for content creation and training robust embodied Al agents. However, existing scene creation mostly depends on 3D scans"}, {"title": "8. Conclusion", "content": "In this technical report, we present Edify 3D, a solution designed for high-quality 3D asset generation. We introduce the Edify 3D models, analyze the scaling laws of each model, and describe the data curation pipeline. Additionally, we explore the application of Edify 3D to scalable 3D scene generation. We are committed to advancing and developing new automation tools for 3D asset generation, making 3D content creation more accessible to everyone."}]}