{"title": "Syntriever: How to Train Your Retriever with Synthetic Data from LLMs", "authors": ["Minsang Kim", "Seungjun Baek"], "abstract": "LLMs have boosted progress in many AI applications. Recently, there were attempts to distill the vast knowledge of LLMs into information retrieval systems. Those distillation methods mostly use output probabilities of LLMs which are unavailable in the latest black-box LLMs. We propose Syntriever, a training framework for retrievers using synthetic data from black-box LLMs. Syntriever consists of two stages. Firstly in the distillation stage, we synthesize relevant and plausibly irrelevant passages and augmented queries using chain-of-thoughts for the given queries. LLM is asked to self-verify the synthetic data for possible hallucinations, after which retrievers are trained with a loss designed to cluster the embeddings of relevant passages. Secondly in the alignment stage, we align the retriever with the preferences of LLMs. We propose a preference modeling called partial Plackett-Luce ranking to learn LLM preferences with regularization which prevents the model from deviating excessively from that trained in the distillation stage. Experiments show that Syntriever achieves state-of-the-art performances on benchmark datasets from various domains in nDCG@K.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have become a core technology in various NLP applications such as chatbots and coding assistants. It is essential that the knowledge of LLMs is complemented by up-to-date information from external sources. To this end, retrieval-augmented generations (RAG) have been proposed and actively explored for various knowledge-intensive NLP tasks. RAG enhances the LLM performance without fine-tuning by incorporating external knowledge into LLMs through search and alleviates problems such as hallucination, i.e., plausible but non-factual information generated by LLMs.\nThe retrieval of documents relevant to a given query is a key task of the RAG system. Dense retrieval methods are widely used to capture semantic relationships between queries and documents, in which text encoders are trained to learn dense embeddings of queries and passages for their semantic matching. The encoders can be pre-trained in an unsupervised manner by using large-scale text pairs sampled from sentences and their contexts, and then be fine-tuned on the annotated datasets for retrieval tasks. Meanwhile, recent LLMs have exhibited remarkable generalization abilities in many NLP tasks, including information retrieval. In this paper, we explore how the vast knowledge of LLMs can be effectively utilized in training retrievers. Recently, RePlug has been proposed for distilling the LLMs' knowledge into small retrievers. RePlug calculates the relevance scores of k retrieved passages given a query, from which a likelihood over k passages is computed. The retriever is trained to minimize the KL divergence between this likelihood and the LLM's likelihood over passages based on its probability of predicting the ground truth answer. However, prediction probabilities are mostly unavailable as the output in the latest black-box LLMs. Thus, we consider the distillation of LLM's knowledge into retrievers when only the synthetically generated texts are available as the output from LLMs.\nContribution. We propose Syntriever, a framework to train/fine-tune retriever models based on synthetic data so as to distill the knowledge of black-box LLMs into retrievers effectively. We propose a two-stage framework: in the first stage, called distillation stage, we fine-tune the retriever with LLM-generated synthetic data; in the second stage, called alignment stage, we align the retriever with the preference of LLMs. In the distillation stage, Syntriever exploits synthetically augmented queries using chain-of-thoughts, synthetic positive and hard-negative passages, as well as self-verification to deal with hallucination. The retriever is then trained by modified Soft Nearest-Neighbor loss to cluster multiple relevant passages together in the embedding space. In the alignment stage, we continually fine-tune the retriever trained from the distillation stage, where the goal is to align the retriever with LLM preferences. The retriever fetches top-K passages from which a set of passage pairs is sampled and provided to LLMs for preference feedback. In particular, we propose a preference modeling called partial Plackett-Luce ranking to learn LLM preferences with a regularization effect such that the aligned model does not deviate excessively from the distilled model. We evaluated the performance of Syntriever in various domains of benchmark datasets for the retrieval tasks from BeIR. Syntriever achieves superior performances on all benchmark datasets, by up to 18.6% in nDCG@10, compared to the prior state-of-the-art. Moreover, we show that the Syntriever framework can be combined with diverse base retrievers and LLMs, leading to a significant increase in retrieval accuracy."}, {"title": "Training Retrievers through Passage Synthesis", "content": ""}, {"title": "Problem Statement and Notation", "content": "Neural retrieval is a task of searching for top-K relevant passages C given query q using encoder E from knowledge source Z:\n$C = Retrieval(q, Z, K, E)$\n(1)\nThe retrieval system (retriever in short) is used for retrieval-augmented generations (RAG). Our goal is to train/fine-tune a (pre-trained) text encoder E which outputs embeddings for semantic representations of queries and passages. The semantic similarity of query q and passage p is measured by\n$s_\\tau(q,p) := \\frac{sim(E(q), E(p))}{\\tau}$\nwhere sim(a, b) stands for the cosine similarity of vectors a and b, and $\\tau$ is the temperature hyperparameter which controls the concentration of (normalized) embeddings on the unit hypersphere.\nIn the training dataset, each query $q_i$ is paired with passage $p_i$ manually labeled as relevant or the answer to $q_i$. We will denote a batch of samples during training by B, where B is a set of indices of batch samples. A typical method to train a retriever is metric learning with contrastive loss such as InfoNCE:\n$\\mathcal{L}_{InfoNCE} = -log \\frac{exp(s_\\tau(q_i, p_i))}{\\sum_{j\\in B} exp(s_\\tau(q_i, p_j))}$ .\nThat is, manually labeled $p_i$ is regarded as a positive passage for $q_i$, and the embeddings of $q_i$ and $p_i$ are pulled closer in the embedding space. The other passages in batch B are considered irrelevant to $q_i$, and as negative passages whose embeddings are pushed away from that of $q_i$.\nNext, we outline the proposed method, dubbed Syntriever, which consists of two stages. In Stage 1 (Sec. 2.2), we use LLM-generated synthetic data to distill their parametric knowledge into the retriever. In Stage 2 (Sec. 2.3), we align the retriever with LLM preferences. The two-stage process of Syntriever is analogous to the training of LLMs, i.e., supervised fine-tuning (SFT) followed by alignment with human preferences. An overview of Syntriever is depicted in Fig. 1."}, {"title": "Stage-1. Distillation of LLM's knowledge through Synthesis", "content": "Given query q, our goal is to assimilate q to a set of positive documents, and to disassimilate q from negative documents. We synthesize a variety of positive and negative passages so as to distill the vast knowledge of LLMs into the retriever.\nDecomposing query to easier sub-queries. Neural retrievers struggle with challenging queries , e.g., if a query requires multi-step reasoning, or is too complex to understand. LLMs are capable of decomposing a complex query into multiple easier sub-queries which contain fine-grained planning to answer the query. We leverage the decomposition capability by applying the original query with prompts generating chain-of-thoughts (CoT) , e.g., \u201cLet\u2019s think step-by-step\u201d proposed by. Specifically, given query $q_i$, we generate augmented query $q_i^{cot}$ given by\n$q_i^{cot} = M(P_{cot}(q_i))$\n(2)\nwhere M() denotes the LLM operation, and $P_{cot}$ denotes the prompt operator to generate CoT (see Appendix C for prompt details). $q_i^{cot}$ contains sub-queries relevant to the original query, which are carefully planned out with clarification and details necessary to retrieve relevant documents. We will use $q_i^{cot}$ as a positive document for $q_i$. This helps the retriever with understanding diverse contexts associated with related queries in the future.\nSynthesizing positive and hard-negative passages. We generate synthetic positive and hard-negative passages from query q. Although there exist positive passages manually labeled for q in the dataset, the synthesis of positive passages can distill a broader range of knowledgeable contexts from LLM to the retriever, and provide different perspectives on the query, which prevents overfitting to specific keywords or contexts. We generate synthetic positive passage $p_i^+$ related to query $q_i$ with prompt $P^+$:\n$p_i^+ = M(P_{+}(q_i))$\n(3)\nIn addition, contrastive learning can be made more robust using hard-negatives where hard-negatives are samples that are difficult to distinguish from positive samples. For the retriever, hard negatives are plausible but irrelevant answers to query q. We synthesize hard-negative passage $p_i^-$ with prompt $P_-$ given by\n$p_i^- = M(P_{-}(q_i))$\n(4)\nHallucination as Hard-negatives. We take a step to verify whether synthetic positive passages are indeed relevant to the given query. Using LLMs to generate answers runs a risk of hallucinations. Hallucination is a non-factual but seemingly plausible passage. The synthetic positive $p_i^+$ can potentially be a hallucination. However, the plausible irrelevance of hallucination fits the definition of hard-negatives. Thus, we re-use hallucination as hard negative passages, which differs from prior works which simply discards hallucination outputs.\nTo that end, once positive passage $p_i^+$ is synthesized, LLM checks the passage for hallucination. LLMs are known to have self-verification ability, i.e., they can re-verify the inferred answer. If $p_i^+$ is decided as hallucination, we label $p_i^+$ as a hard-negative, which we call Relabeling step. Specifically,\n$p_i^* = M(P_{Relabel} (q_i, p_i^+))$\n(5)\nwhere $P_{Relabel}$ denote the prompting for relabeling. If $p_i^+$ is relabeled as a hard-negative, query $q_i$ will have two hard-negatives (synthetic and relabeled) and two positives (manually labeled and CoT).\nIn summary, given query $q_i$, positive passages are manually labeled passage $p_i$, CoT $q_i^{cot}$, and synthetic positive $p_i^+$ (if not relabeled as negative). The negative passages are $p_i^-$ (and relabeled passages, if any) and in-batch samples. An example of synthesized passages is shown in Fig. 2.\nPutting positives together: modified Soft-Nearest Neighbor Loss. Next, we train the retriever with synthesized passages. Considering that there are multiple positives for a given query, we propose to use a loss inspired by soft-nearest neighbor (SNN) loss. SNN loss is used in metric learning for supervised classification as follows. Consider batch B from a labeled dataset and a sample $x_i$ in B with label $y_i$. The"}, {"title": "Stage-2. Retriever Alignment from LLM Feedback", "content": "Alignment is a process of aligning language models with human preferences. Alignment provides LMs with a pair of answer candidates for a question, where the preference between the pair is labeled by humans. We propose to align the retriever with LLM preferences as follows. Given a query, the retriever trained in the distillation stage is asked to retrieve top-K passages. Next, a pair of passages is sampled from top-K passages, and LLM is asked to provide the preference between the pair. Since K passages are top passages from a retriever trained through the distillation stage, deciding the preference between the pair is likely to be challenging (for moderately small K, e.g., K = 5). The retriever is continually trained based on the preference feedback from LLMs. The details of the alignment process are outlined as follows.\nStep 1: Retrieve top-K passages. Given query $q_i$, we retrieve top-K passages using encoder E trained through the distillation stage:\n$\\hat{C_i} = Retrieval(q_i, Z, K, \\hat{E}) = \\{c_{i,1}, c_{i,2}, ..., c_{i,K} \\}$\nStep 2: Pair-wise Comparison. A pair of passages, $c_{i,j}$ and $c_{i,k}$, is sampled from $\\hat{C_i}$. We probe LLM to decide which passage is more relevant to answer query $q_i$ using prompt $P_{Compare}$:\n$(q_i, c^{++}, c^{-}) = M(P_{Compare}(q_i, c_{i,j}, c_{i,k}))$\n(6)\nwhere LLM labels the more (resp. less) preferred passage as $c^{++}$(resp. $c^{-}$). We compute the pairwise preferences of N distinct passage pairs sampled from $\\hat{C_i}$ where $N \\le {K \\choose 2}$ is a hyperparameter.\nStep 3: Partial Plackett-Luce ranking. Consider batch B of triples $(q_i, c^{++}, c^{-})$ obtained in Step 2 where $q_i$'s in the batch are distinct. Encoder $\\hat{E}$ is fine-tuned with the following loss function:\n$\\mathcal{L}_{align}(q_i) = -log [-log\\frac{\\sum_{j\\in B}e^{s_\\tau(\\hat{E}(q_i),c^{++})} + e^{s_\\tau(\\hat{E}(q_i),c^{-})}}{\\sum_{j\\in B,j\\neq i}e^{s_\\tau(\\hat{E}(q_i),c^{++})} + (e^{s_\\tau(\\hat{E}(q_i),c^{++})} + e^{s_\\tau(\\hat{E}(q_i),c^{++})})}]$\n(7)\nWe refer to the training under loss (7) as partial Plackett-Luce ranking. The method is explained in detail as follows.\nFrom Bradely-Terry to Plackett-Luce model. Preference modeling has been used for aligning language models with human preferences. Bradley-Terry (BT) model is widely adopted for modeling preference over two choices. Consider a pair of answer passages $y_w$ and $y_l$ given query q. If $y_w$ is preferred over $y_l$ by a human annotator, the preference relation is denoted as $y_w \\succ y_l | q$. In preference modeling, it is typically assumed that there exists some (implicit) reward function r(q, y) for query q and answer y. Given query q and two answers $y_1$ and $y_2$, BT model is defined by the distribution\n$P(y_1 \\succ y_2|q) = \\frac{e^{r(q,y_1)}}{e^{r(q,y_1)} + e^{r(q,y_2)}}$\n(8)\nThe fitting of BT model involves either explicitly formulating and optimizing reward r(\u00b7,\u00b7) , or implicitly doing so by policy optimization through parameterization .\nPlackett-Luce (PL) model generalizes BT model to ranking M \u2265 2 choices. Suppose \u03c0 : [M] \u2192 [M] is a permutation. Given query q and answers $y_1, ..., y_M$, we define the notation:\n$p(\\pi|q) := p(y_{\\pi(1)} \\succ y_{\\pi(2)} \\succ ... \\succ y_{\\pi(M)}|q)$.\nThe PL model defines distribution p(\u03c0|q) as\n$p(\\pi|q) = \\prod_{m=1}^{M} \\frac{e^{r(q,y_{\\pi(m)})}}{\\sum_{j=m}^{M} e^{r(q,y_{\\pi(j)})}}$\n(9)\nwhere the m-th term in the product of (9) is the soft-max probability of the reward for the choice of rank m, $r(q, y_{\\pi(m)})$, along with the rewards of choices of lower preferences.\nPartial Ranking through Marginalization. The key idea of our method is to include in-batch samples in preference modeling. Consider triple $(q_i, c^{++}, c^{-})$ from batch B. Our goal is to model the following preference relation:\n$c^{++} \\succ c^{-} \\succ \\{in-batch samples\\} | q_i$\n(10)\nwhere the preference ordering of in-batch samples can be arbitrary or \"don't care\". Relation (10) is explained as follows. Firstly, $c^{++}$ is preferred over $c^{-}$by LLM given $q_i$. Secondly, since $c^{++}$ and $c^{-}$ are in top-K passages obtained from a retriever trained through the distillation stage, it is highly likely that both $c^{++}$ and $c^{-}$ are preferred over irrelevant samples in the batch. We call this relation partial ranking, since the ranking of the samples is incompletely specified.\nThe preference relation in (10) can be modeled by marginalization of Plackett-Luce distribution given by (9) as follows. Suppose we want to model the preference relation\n$y_{\\pi(1)} \\succ y_{\\pi(2)} \\succ \\{y_{\\pi(3)},..., y_{\\pi(M)}\\}|q$\n(11)\nwhere the top-two choices (\u03c0(1) and \u03c0(2)) are preferred over the rest (\u03c0(3), . . ., \u03c0(M)), and the ordering of the rest can be arbitrary. Since p(\u03c0|q) is a distribution over \u03c0, the distribution modeling (11) can be obtained by marginalizing p(\u03c0|q) over the"}, {"title": "Combining Preference Modeling and Contrastive Learning", "content": "Our observation is that, the training objective for preference modeling invariably takes the form of a contrastive loss. For example, the BT model is trained with the loss which is the negative log of (8). Suppose we use the BT model, in which case $y_1$ and $y_2$ in (8) are replaced by $c^{++}$ and $c^{-}$ respectively. From a contrastive learning perspective, (8) attracts $c^{++}$ ($y_1$) to q, but repels $c^{-}$ ($y_2$) from q. But this may unintentionally move $c^{++}$ and $c^{-}$ closer to samples irrelevant to $q_i$. This is undesirable, because $c^{++}$ and $c^{-}$are among top-K documents retrieved by the model trained through the distillation stage, and thus should be regarded as relatively \u201cpositive\u201d and kept away from irrelevant in-batch negatives. Conventional preference modeling, such as BT model, lacks perspective on learning with negative (irrelevant) samples.\nThe proposed loss directly addresses the problem: it not only captures the LLM's preferences but also maintains separation among irrelevant documents. Thus, our loss combines preference modeling and contrastive learning. It can be seen that (7) is simply a sum of two contrastive-type losses. By having a similar form of contrastive loss as that from the distillation stage, e.g., positive embeddings keeping distances from in-batch negatives, our alignment loss serves as regularization. That is, the model is prevented from excessively deviating from that trained in the distillation stage. Regularization is deemed important in the alignment of LLMs as well. In addition, it is reported that the larger number of negatives leads to better performance in contrastive learning . In the Experiments section, we show that partial PL ranking model achieves robust performances across datasets, whereas BT model occasionally suffers from poor alignment."}, {"title": "Experiment", "content": ""}, {"title": "Experimental Settings", "content": "Datasets. Experiments are conducted on retrieval benchmark datasets from various domains in BeIR. We evaluate the performance of retrievers in two benchmark settings as follows.\n\u2022 Supervised Fine-Tuning. The models are evaluated on BeIR benchmark datasets which contain the training datasets. For each benchmark dataset, every model is fine-tuned on its training dataset, and we report in-domain evaluation results on that benchmark dataset.\n\u2022 Zero-shot Transfer. The models are evaluated on out-of-domain datasets from BeIR benckmark. The zero-shot setting is similar to previous work : the models can be first fine-tuned on large retrieval datasets such as MSMARCO and NQ for generic knowledge, and then are evaluated on unseen datasets.\nWe use Normalised Discounted Cumulative Gain (nDCG@K) as the default performance metric.\nBaselines. We experiment with lexical retriever BM-25, semantic retrievers DPR, SBERT, CoCondenser, RocketQA, Contriever , E5 , English model of BGE-M3-EN, Nomic-embed .\nSettings of Syntriever. Syntriever uses pre-trained E5 as the base encoder E. In the settings of supervised fine-tuning, Syntriever is trained with synthetic data generated from each training dataset. In the settings of the zero-shot transfer, Syntriever is first trained on synthetic data based on training datasets of MSMARCO and NQ, and then is evaluated on out-of-domain datasets"}, {"title": "Main Results", "content": "We first present the supervised fine-tuning results on seven datasets for the retrieval task which are BeIR benchmarks with training datasets. The results are shown in Table 1. Compared to the second-best models, Syntriever improves the retrieval performances by: 18.6% on MSMARCO, 5.9% on HotpotQA, 2.5% on FiQA, 1.8% on SciFact, 8.3% on NFCorpus, and 4% on NQ. The base encoder for Syntriever is a pre-trained E5; still, Syntriever achieves performance gain over E5 by: 18.6% on MSMARCO, 10.7% on HotpotQA, 10.6% on FiQA, 9.2% on SciFact, 20.9% on NFCorpus, 5.6% on Fever and 5.7% on NQ. This shows that Syntriever can successfully distill LLMs' capability into small retrievers and improve their performance by a large margin. Overall, Syntriever shows robust performances on datasets both in generalized and specialized domains. Our results show that small LMs can efficiently learn from the teacher model through synthetic data and can be successfully aligned through feedback, even without access to the output probability of black-box LLMs.\nNext, we present zero-shot transfer results. Table 2 shows that Syntriever achieves the best performances on 8, and the second best on 1, out of 15 datasets. Note that the performances of Syntriever on MSMARCO and NQ are in-domain results, whereas other baselines, e.g., Contriever , E5 , etc., are reported also to be trained on MSMARCO and/or"}, {"title": "Ablation study", "content": "We conduct an ablation study on Syntriever. We add or remove model components, and the effects on the performance are shown in Table 3. The results show that both synthesized query ($q^{cot}$) and passages ($p^+, p^\u2212$) in the distillation stage improve the retrieval performances. Overall, the distillation stage achieves an average gain of 8.2% over the base retriever. Results show that the retriever successfully learns from the parametric knowledge of LLMs during the distillation stage. Also, the alignment component ($c^+ \\succ c^\u2212$) in Table 3 is shown to achieve performance gains of up to 8.8%. Our results show that the alignment component is significant for retrieval performance, considering that nDCG@K is sensitive to the fine-grained ranking of relevant passages."}, {"title": "Performances with different encoders", "content": "Syntriever is a framework for training encoders for retrieval, and thus can be combined with different sentence encoders. We experiment with various well-known encoders, e.g., ColBERT, SBERT, and Contriever, as the base encoders for Syntriever. Syntriever improves the performance by a large margin in all three retrieval models. The performance improvement is particularly high in nDCG@1 which concerns retrieving the exact passage relevant to the query. This is because the alignment stage in Syntriever helps the retriever with a fine-grained ranking of highly relevant passages. Overall, the results show that Syntriever is generally applicable to, and improves the performances of, various retrievers."}, {"title": "Effects of Re-labeling Hallucination Passages", "content": "Table 5 shows that LLM self-verification and relabeling are effective for the synthetic training by Syntriever. The performance improvement of self-verification on HotpotQA is relatively greater than other datasets. We found that, approximately 15% of synthetic positive passages were relabeled as hallucinations in the case of HotpotQA, whereas the proportion was about a few percent in other datasets. This indicates that the performance improvement through relabeling is likely higher for HotpotQA. In conclusion, removing hallucinations (and even re-using them as hard-negatives as in Syntriever) through self-verification is important for data synthesis, which is the case for most tasks utilizing LLMs."}, {"title": "Weaker but Cheaper LLMs can be effective", "content": "We examine how the LLM capabilities affect distillation and alignment performances. We consider two LLMs: GPT-4o vs. GPT-4o-mini, where GPT-4o is the larger and more capable model. First, we"}, {"title": "Comparison of Preference Modeling Methods", "content": "Table 8 compares the preference modeling methods for alignment: Bradley-Terry (BT) and partial Plackett-Luce (PL) ranking model. While BT and partial PL models achieve similar performances on SciFact, BT model shows poor performances on FiQA and NFCorpus. The following is a possible explanation. The search results on SciFact tend to be highly accurate, and most of top-K passages are likely to contain (partly) relevant context. By contrast, top-K passages on FiQA and NFCorpus which are more challenging datasets, will tend to be only marginally relevant to the given query. The partial PL performs preference ranking while keeping those marginally relevant passages away from"}, {"title": "Effects of the number of retrieved passages during alignment", "content": "We examine the effect of the number K in the top-K passage retrieved during the alignment process. Table 9 shows the results with varying K, where we sample all the possible pairs, or $N = {K \\choose 2}$, for comparison. The performance improves with increasing K, up to 12% in FiQA and 5.1% in SciFact. Also, results show that the larger K, the better the performance. In addition, using a larger number of passages is particularly effective when the overall retriever accuracy is low, since it is more likely to retrieve relevant context in top-K-ranked passages for large K. However, large K may incur high computational costs if $N = {K \\choose 2}$, and thus there is a trade-off between performance and computational overheads. In this paper, we chose K = 5 as a good trade-off point.\nIn addition, we experiment with the numbers of passage pairs to be sampled for comparison (N) with varying K. Previously in Table 9, we set"}, {"title": "Quality of Synthetic Positives", "content": "In general, it is difficult to accurately quantify the ratio of hallucination in the synthetic passage. The passage may not have direct clues to the answers, but may contain partial information from which the answer can be deduced. How relevant a passage should be to the query so that the passage is classified as positive? This is very hard to quantify, and thus measuring the quality of synthetic passages is difficult as well.\nWe performed experiments to indirectly measure the quality of synthetic positives as follows. We asked GPT-4o that whether the true answer can be directly derived from synthetic positive passages (after self-verification). We asked the same question, but in this time whether the answer can be derived from the ground-truth passages provided by the dataset. The results are shown in the table below.\nInterestingly, GPT-4o states that only 84% of the ground truth passages have direct clues to the true answer. This is because, a significant portion of the \"ground truth\" passages of the HotpotQA dataset do not contain direct clues to the true answer, but only indirect clues or partial information. By contrast, GPT-4o stated that 88% of synthetic positives after self-verification contain direct contexts to the true answer. Thus, we conclude that synthetic positives after self-verification are of fairly high quality."}, {"title": "Related Work", "content": "Neural Information Retrieval. Neural information retrieval is a key element of retrieval-augmented generation (RAG) which is a retrieve-and-read approach for open domain question answering tasks. Lexical retrieval methods such as BM-25 have been mostly used prior to neural retrievals, which however had difficulties with capturing semantic information at scale. Thus, dense passage retrievers using text encoders have been actively explored. RocketQA is a multi-step training framework for a retrieval system consisting of a retriever and a re-ranker which typically is a cross-encoder to estimate the ranking among retrieved passages. RocketQA further utilizes the re-ranker to sample hard negatives from top-retrieved passages. Meanwhile, Syntriever does not use separate re-rankers, but continually trains the retriever for its alignment with the ranking preference of LLMs. Unsupervised learning for retrieval was proposed to train sentence encoders by contrastive learning using a large collection of text-pair datasets. Subsequently, a hybrid retrieval method which combines lexical, dense, and multi-vector retrievers has been proposed . RePlug proposed a knowledge distillation for retrievers using KL divergence associated with the prediction probabilities of relevant documents from LLMs which, however, are available only from outdated APIs.\nTraining with Synthetic Data. Tiny-stories first proposed training small language models using synthetic data generated by GPT-4 . Motivated by , Phi proposed filtering of code data based on the educational value through the prompting of GPT-4. The next version of Phi-series generated high-quality synthetic data from judiciously selected topics in order to distill GPT-4's knowledge into small LLMs. They demonstrated that distillation through synthetic data of high educational value can boost the performances of small LLMs. proposed to"}, {"title": "Conclusion", "content": "We proposed Syntriever, a training framework for retrieval systems using LLM synthesis. In the distillation stage, Syntriever synthesizes various types of passages including augmented queries, relevant and plausibly irrelevant passages. Relevant passages are clustered in the embedding space using modified soft nearest-neighbor loss. In the alignment stage, the retriever is continually trained based on the preference feedback of LLMs on the retrieved passages. We propose a preference modeling called partial Plackett-Luce ranking to learn LLM preferences while maintaining the similarity relation among embeddings learned during the distillation stage. Experiments show that Syntriever achieves significant performance gains over baselines on benchmark datasets from various domains."}, {"title": "Limitations", "content": "Although Syntriever achieves performance gains compared to baseline retrievers on various benchmark datasets, it requires LLM inferences to generate synthetic data and alignment feedback. This may incur additional costs compared to other methods which only perform a fine-tuning of text encoders. However, the cost of proprietary black-box LLMs has become increasingly cheaper and affordable. Moreover, weaker but cheaper LLMs become increasingly capable of teaching student models . Thus, we believe that the Syntriever framework is widely applicable to retrieval systems in practice."}, {"title": "Acknowledgement", "content": "This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (RS-2022-NR070834), and by the Institute of Information & Communications Technology Planning & Evaluation (IITP)-ICT Creative Consilience Program grant funded"}]}