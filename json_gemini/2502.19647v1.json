{"title": "AutoBS: Autonomous Base Station Deployment Framework with Reinforcement Learning and Digital Twin Network", "authors": ["Ju-Hyung Lee", "Andreas F. Molisch"], "abstract": "This paper introduces AutoBS, a reinforcement learning (RL)-based framework for optimal base station (BS) deployment in 6G networks. AutoBS leverages the Proximal Policy Optimization (PPO) algorithm and fast, site-specific pathloss predictions from PMNet to efficiently learn deployment strategies that balance coverage and capacity. Numerical results demonstrate that AutoBS achieves 95% for a single BS, and 90% for multiple BSs, of the capacity provided by exhaustive search methods while reducing inference time from hours to milliseconds, making it highly suitable for real-time applications. AutoBS offers a scalable and automated solution for large-scale 6G networks, addressing the challenges of dynamic environments with minimal computational overhead.", "sections": [{"title": "I. INTRODUCTION", "content": "The rollout of 6G networks demands higher base station (BS) density due to the use of higher frequencies like millimeter-wave (mmWave), which offers enhanced bandwidth and low latency. However, these frequencies suffer from severe signal attenuation and limited propagation range, particularly in complex urban environments. As a result, dense BS deployment becomes essential to maintain reliable coverage and capacity. Furthermore, cell-free massive MIMO, where a significant number of access points with one of a few antennas each are distributed over an area [1] is anticipated to be widely used in 6G.\nHowever, optimizing BS placement in such environments presents significant challenges due to highly site-specific conditions. Traditional BS deployment methods often rely on manual planning, based on real-world measurement of the propagation conditions [2], and/or computationally intensive ray-tracing simulations using tools like WirelessInsite [3] or SionnaRT [4]. The fact that these approaches are time- and labor- consuming makes them less suited for the dense deployment in 6G. Furthermore, they are not suited for real-time adaptation of network topology, e.g., dynamic addition and placement of (mobile) BSs in reaction to change of user density, e.g., at special events.\nSuch network planning can be also seen as a special case of Digital Twin (DT) networks, which create virtual replicas of physical environments, enabling real-time simulation and optimization of network performance under site-specific conditions. Again, manually optimizing BS placement within DT frameworks remains computationally expensive.\nTherefore, large-scale and real-time network optimization requires the integration of machine learning (ML)-based ap- proaches to automate and expedite the process.\nTo address these challenges, we propose AutoBS, an au- tonomous BS deployment framework that uses Deep Rein- forcement Learning (DRL) and a DT generator to optimize BS placement in a site-specific manner. AutoBS integrates PMNet [5], [6], a fast and accurate ML-based path gain predictor, to calculate reward based on both coverage and capacity metrics. By autonomously learning optimal placement strategies, AutoBS significantly reduces the computational overhead and deployment time compared to conventional methods, making it well-suited for real-time and large-scale 6G network optimization.\nContributions. This paper presents AutoBS, an autonomous BS deployment framework utilizing DRL and a DT generator (PMNet). Our key contributions are summarized as follows:\n\u2022 AutoBS Framework Design: We introduce a novel DRL-based framework for BS deployment that incorporates PMNet for real-time, site-specific channel predictions\u00b9 PMNet enables fast, precise reward calculations, allowing AutoBS to effectively optimize coverage and capacity through an adaptive reward function (see Fig. 2 in Sec. III). Details on reward design are provided in Table V in Sec. VI.\n\u2022 Support for Multi-BS Deploy.: AutoBS handles both static single BS and asynchronous multi-BS deployments, providing flexibility across diverse deployment scenarios (see Fig. 5 in Sec. IV).\n\u2022 Fast and Near-Optimal Deployment: Simulations demonstrate that AutoBS reduces inference time from hours to milliseconds compared to exhaustive methods, particularly in multi-BS deployments, making it practical for large-scale, real-time optimization (see Table IV in Sec. IV)."}, {"title": "II. METHODOLOGY", "content": "The BS deployment problem can be mathematically for- mulated as an optimization task. Let the BS location be represented by coordinates (i,j) on a site-specific building map m. The objective function to optimize is:\n$\\max \\limits_{\\{i,j\\}} \\sum\\limits_{m=1}^M (V_m + vC_m), s.t. \\{i, j\\} \\in B,$\nwhere $V_m$ is the coverage, $C_m$ represents the capacity, and v is a weighting factor that governs the trade-off between coverage and capacity. The set B defines the deployable areas for BS placement, such as rooftops or other designated locations within the environment.\nAutoBS frames the deployment problem as a reinforcement learning task, where a DRL agent interacts with the environment to learn optimal placement strategies. Through contin- uous feedback via rewards, the agent refines its decisions, capturing site-specific channel characteristics and optimizing network performance over time."}, {"title": "B. Reward Calculation via PMNet", "content": "In DRL, efficient reward calculation is essential for guiding the learning process, where the agent explores numerous actions throughout training. However, calculating network performance metrics, such as coverage and capacity, based on BS placements-key components of the reward function-is computationally intensive. Traditional RT simulations, which can take tens of minutes to generate a single pathloss map, are impractical for real-time reinforcement learning.\nTo overcome this challenge, AutoBS integrates the PMNet framework, a DT generator, for rapid reward computation. PMNet predicts pathloss maps with an RMSE on the order of $10^{-2}$ within milliseconds, enabling the Proximal Policy Optimization (PPO) algorithm [7] to immediately evaluate network performance and receive reward feedback after each deployment decision. This fast feedback allows the agent to efficiently simulate and assess multi-BS deployment scenarios, significantly accelerating training by exploring more place- ment options with timely updates. The integration of PMNet ensures both speed and scalability, enabling the AutoBS to generalize effectively across diverse environments."}, {"title": "III. AUTOBS: AN AUTONOMOUS BASE STATION DEPLOYMENT FRAMEWORK", "content": "The AutoBS framework automates optimal BS placement in 6G networks using DRL, with a PPO agent. PPO is well- suited for this task due to its balance between exploration (testing new deployment locations) and exploitation (refining placement strategies). By using a clipped objective function, PPO ensures stable policy updates and prevents large, desta- bilizing changes during training.\nThe framework supports two deployment modes: (1) static single BS deployment, where a single BS is placed in a fixed environment, and (2) asynchronous multi-BS deployment, where multiple BSs are deployed asynchronously.\n1) Single (Static) BS Deployment: In the static single BS deployment scenario, the geographical environment and network conditions are assumed to be fixed. The input to the system is a site-specific map S, which contains detailed information about buildings, terrain, and obstacles. The PPO agent processes this map and outputs the optimal location (i, j) for placing the BS to maximize coverage and capacity.\n2) Multi (Asynchronous) BS Deployment: In the asyn- chronous multi-BS deployment scenario, BSs are deployed incrementally over time. After each deployment, the environ- ment is updated to reflect the changes in network conditions, such as coverage, capacity, and user distribution. The PPO agent adjusts its strategy after each deployment based on real- time feedback, refining its decisions progressively."}, {"title": "B. Evaluation Metrics", "content": "The AutoBS framework's performance is evaluated using two key metrics: Coverage and Capacity. These metrics are essential for assessing the quality of BS placements in terms of both spatial coverage and network throughput.\nCoverage. represents the proportion of the area or users receiving sufficient signal strength, e.g., received power, from the deployed BSs. It is calculated as the total area where the received power $P_{i,j}$ exceeds a predefined threshold $thr$, ensuring satisfactory Quality of Service (QoS). The coverage V is expressed as:\n$V = \\sum\\limits_{\\{i,j\\} \\in R} V_{i,j}$\nwhere R is the region of interest (RoI), e.g., non-deployable and non-building areas, and $V_{i,j}$ is a binary indicator of coverage:\n$V_{i,j} = \\begin{cases} 1, & P_{i,j} \\geq thr, \\\\ 0, & P_{i,j} < thr. \\end{cases}$\nThe threshold thr depends on network QoS requirements (e.g., the minimum signal strength for reliable communication).\nCapacity. represents the maximum amount of data that can be transmitted or received over a network, which depends on the quality of the connection, e.g., signal-to-noise ratio (SNR). The total capacity C is given by:\n$C = \\sum\\limits_{\\{i,j\\} \\in R} log_2 (1 + SNR_{i,j}),$\nwhere $SNR_{i,j}$ is the signal-to-noise ratio at pixel (i, j), calcu- lated as:\n$SNR_{i,j} = \\frac{\\sum\\limits_{k} P_{ki,j}}{\\sigma^2}$"}, {"title": "C. Training", "content": "The AutoBS framework models the base station (BS) de- ployment problem as a Markov Decision Process (MDP), enabling the agent to interact with its environment to make optimal BS placement decisions.\n1) MDP Design: Environment. The environment for the PPO agent is defined by the interactions within a site-specific building map, where each deployed BS impacts the network performance metrics such as coverage and capacity. At each time step t, the agent interacts with this environment by observing a state $s_t \\in S$, which includes details on buildings, obstacles, and terrain specific to the deployment site. The agent then takes an action $a_t \\in A$, selecting a BS deployment location based on its learned policy \u03c0. After the action is executed, the environment transitions to a new state $S_{t+1}$, representing the updated network layout and performance with the new BS in place. The agent receives a reward $r_t$ based on the effectiveness of its decision, guiding it toward an optimal deployment policy $\u03c0^*$.\nState. The state $s_t$ at time step t provides key information for decision-making about BS placement. Formally, the state is expressed as:\n$s_t = \\{S\\},$\nwhere S represents the site-specific building map. This map encompasses critical details about building locations, obsta- cles, and terrain, all of which significantly influence signal propagation (i.e., site-specific channel).\nAction. The action space A consists of potential BS deploy- ment locations. At each time step t, the agent selects an action $a_t \\in A$, which corresponds to placing a BS at a specific geographical coordinate (i, j) on the site-specific map. The action is represented as:\n$a_t = (i, j), \\{i, j\\} \\in B,$\nwhere B is the set of all permissible deployment locations within the map. The agent deploys BSs sequentially, choosing new locations at each time step based on current network needs. This approach is computationally efficient, allowing the agent to adapt its strategy in real-time as it learns from previous decisions.\nReward. The reward function $r_t$ incentivizes the agent to improve network performance by maximizing both coverage and capacity. The reward at time step t is defined as:\n$r_t = v_1 V_t + v_2 C_t + v_3 P_t,$\nwhere $V_t$ and $C_t$ represent the improvements in coverage and capacity, respectively, and $P_t = \\sum_{\\{i,j\\} \\in R} P_{i,j}$ denotes the total pathgain, for the t-th deployment. The weighting parameter v adjusts the relative importance of coverage, ca- pacity, and pathgain in the overall network optimization. For further details regarding the reward design, please refer to Sec. VI. Note also that for more general DT applications (e.g., incorporating user demands), only the reward function needs to be adapted, while the remaining framework of AutoBS stays the same.\nAs mentioned in Sec. II-B, the reward must be recalculated after each BS deployment to reflect the updated network condi- tions. This typically requires coverage and capacity evaluation from pathloss map prediction, which are computationally expensive. To overcome this challenge, the AutoBS framework integrates the PMNet model, which provides fast and accurate pathloss predictions, enabling efficient reward calculations. PMNet allows the agent to quickly assess the network per- formance after each BS placement, ensuring that the training process is both realistic and computationally feasible.\n2) Training Process: The PPO agent is trained using the PPO algorithm [7], which allows the agent to iteratively interact with a simulated environment. The agent selects BS deployment actions and receives rewards based on improve- ments in network coverage and capacity, progressively refining its policy $\u03c0_\u03b8(a_t|s_t)$ over time.\nAt each time step t, the agent updates its policy to maximize the cumulative reward:\n$J(\\theta) = E [\\sum\\limits_{t=0}^T \\gamma r_t ],$\nwhere $\u03b3$ is the discount factor, $r_t$ is the reward received at time step t, and T is the time horizon over which the agent aims to optimize its decisions. The discount factor $\u03b3$ balances immediate and future rewards, promoting long-term planning in BS placement.\nThe PPO agent optimizes the following objective function:\n$L(\\theta) = E_t [min (r_t(\\theta) A_t, clip(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) A_t)],$\nwhere $r_t(\\theta)$ is the probability ratio between the updated and previous policies, $\u03f5$ is a clipping parameter to limit policy changes for stability, and $A_t$ represents the advantage function, which quantifies how much better the current action is compared to the baseline.\nFor clarity and space reasons, we omit detailed descriptions of the PPO algorithm, and instead present a summary of the training process in Fig 2, which outlines the key steps involved in training the PPO agent."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "The simulation environment is based on site-specific maps of the University of Southern California (USC) campus, cover- ing an area of 512\u00d7512 [m\u00b2]. This environment reflects dense urban features, including buildings and terrain, providing a realistic scenario to evaluate AutoBS.\nSimulations are executed on a high-performance machine with a Tesla T4 GPU, 12GiB of RAM, and an Intel Xeon CPU @ 2.20GHz. Developed in Python with PyTorch for model training, the framework leverages PMNet for pathloss predictions.\n\u2022 Exhaustive: The exhaustive search provides an upper bound for performance by systematically evaluating all possible BS placement configurations across the site- specific map. It guarantees a global optimal solution in terms of coverage or capacity based on the configuration criteria. However, the computational cost is exceedingly high, particularly for multi-BS deployments, as the com- plexity grows exponentially with the number of BSs and map size. Due to this limitation, the exhaustive search is restricted to 50 samples in our baseline experiments. Additionally, in multi-BS deployments, the exhaustive method operates synchronously, unlike the asynchronous deployment strategy used by AutoBS. While an asyn- chronous exhaustive method could be defined to reduce computation time somewhat, it would still be computa- tionally intense and would yield suboptimal results. Our focus remains on synchronous exhaustive deployment as it provides a true upper bound for comparison with AutoBS.\n\u2022 AutoBS (Proposed): The AutoBS leverages the PPO agent to make near-optimal BS deployment decisions in real time. It is trained using a fully connected neural network with four layers, each consisting of 128 nodes, to optimize both coverage and capacity. AutoBS achieves rapid deployment decisions at millisecond timescales."}, {"title": "B. Performance Analysis", "content": "Comparison. AutoBS achieves sig- nificant improvements in both coverage and capacity compared to Heuristic deployment. Its performance closely approaches that of the Exhaustive method, which represents the global optimal solution, highlighting the effectiveness of policy- guided BS placement.\nAutoBS focuses on coverage optimization through a reward function based on pathloss predictions from PMNet. Although capacity metrics are not explicitly included in the reward func- tion (i.e., $v_2$ = 0), the observed capacity difference between AutoBS and Exhaustive search remains minimal-which will be further discussed in Sec. VI."}, {"title": "V. CONCLUSION", "content": "This paper presents AutoBS, a RL-based framework for op- timal BS deployment in 6G networks. By leveraging the PPO algorithm and integrating fast, accurate pathloss predictions from PMNet, AutoBS efficiently learns deployment strategies that adapt to site-specific channel dynamics, balancing cover- age and capacity.\nOur results show that AutoBS achieves up to 95% of the capacity obtained by exhaustive search methods, while dramat- ically reducing inference time from hours to milliseconds. This makes AutoBS a practical solution for real-time deployments.\nWith its ability to provide near-optimal performance and scale efficiently, AutoBS stands out as a powerful, automated solution tailored for the demands of large-scale 6G networks.\nWhile AutoBS is designed for BS deployment in 6G net- works, its framework can be easily adapted for other DT-based network optimization tasks, e.g., mobility management and link adaptation, by modifying the reward function and state in- formation to match specific application goals. Furthermore, the flexible design of AutoBS makes it well-suited for additional applications, including Wi-Fi access point (AP) placement and O-RAN radio unit (RU) deployment."}, {"title": "VI. APPENDIX: REWARD DESIGN", "content": "This section presents empirical findings from experiments with different reward designs in the AutoBS framework. The pathgain is defined as $P = \\sum\\limits_{i,j \\in R} P_{i,j}$, and although other transformations, such as log- arithmic scaling (e.g., log V), were tested, they are omitted here for brevity. These findings are empirical and sensitive to training hyperparameters (e.g., learning rate) and reward design parameters (e.g., v).\nThe results provide valuable insights. As expected, the Coverage Only reward achieves the highest coverage, aligning with its direct optimization objective (e.g., $v_2, v_3 = 0$). Interestingly, in terms of capacity, the combination of Pathgain and Coverage (i.e., $v_2$ = 0) outperforms the Capacity Only reward. Notably, Coverage Only also surpasses Capacity Only in capacity, highlighting complex dynamics within the DRL training process. These findings suggest that site-specific chan- nel characteristics are implicitly embedded in rewards derived from PMNet's pathloss maps. While Capacity Only smooths variations through logarithmic scaling, Pathgain captures these fluctuations on a linear scale, providing more granular feed- back for effective learning.\nThroughout this work, the Pathgain + Coverage reward configuration is primarily used."}]}