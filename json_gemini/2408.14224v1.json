{"title": "Fact Probability Vector Based Goal Recognition", "authors": ["Nils Wilken", "Lea Cohausz", "Christian Bartelt", "Heiner Stuckenschmidt"], "abstract": "We present a new approach to goal recognition that involves comparing observed facts with their expected probabilities. These probabilities depend on a specified goal g and initial state s0. Our method maps these probabilities and observed facts into a real vector space to compute heuristic values for potential goals. These values estimate the likelihood of a given goal being the true objective of the observed agent. As obtaining exact expected probabilities for observed facts in an observation sequence is often practically infeasible, we propose and empirically validate a method for approximating these probabilities. Our empirical results show that the proposed approach offers improved goal recognition precision compared to state-of-the-art techniques while reducing computational complexity.", "sections": [{"title": "Introduction", "content": "Goal recognition is the task of recognizing the goal(s) of an observed agent given a sequence of actions executed or a sequence of states visited by the agent. In this paper, we focus on observed action sequences for the theoretical discussion of the goal recognition problem. This task is relevant in many application domains like crime detection [6], pervasive computing [19, 5], or traffic monitoring [12].\nPrevious goal recognition systems often rely on the principle of Plan Recognition As Planning (PRAP) and, hence, utilize concepts and algorithms from the classical planning community to solve the goal recognition problem [13, 14, 17, 1]. Nevertheless, a fundamental limitation of many systems from this area, which require computing entire plans to solve a goal recognition problem, is their computational complexity. Pereira et al. [11] approached this shortcoming by introducing a planning landmark-based heuristic approach, which outperforms existing goal recognition approaches and requires much less computation time. However, as this approach only relies on fact landmarks, it neglects the information from all other observed facts. In the context of classical planning, facts model the properties of the planning environment. A planning state is defined by the subset of facts that hold true at that moment.\nThis paper presents a novel approach to goal recognition that involves comparing observed facts with their probability of being observed. We define the probability of observing a fact f as the probability of f being part of an observation sequence (i.e., plan) that starts at the initial state s0 and ends at a possible goal state sg. The exact meaning of a fact f being part of an observation sequence is defined later in the paper by Definition 9. The proposed approach implements this comparison by mapping the fact observation probabilities per goal, the initial state s0, and the currently observed state st into a real vector space. Based on these vector mappings, the proposed method computes a heuristic value for each possible goal based on these vector mappings. The proposed heuristic estimates the probability that a possible goal g is the actual goal g* of the observed agent. We will refer to our proposed method as Fact Probability Vector Based Goal Recognition, or FPV, for short.\nFPV and the planning landmark-based (PLR) method introduced by Pereira et al. [11] fall in the same class of heuristic goal recognition approaches. PLR uses the fact that a planning landmark l, by definition, has to be observed during an observation sequence that starts at s0 and ends at g. Based on this, PLR uses a heuristic that estimates the probability of a goal g being the actual goal g* by counting how many planning landmarks out of all landmarks were already observed for g. However, this limits the PLR approach to using only observed facts that are planning landmarks. This problem becomes more severe when fewer planning landmarks exist in a planning domain. One extreme case for such a domain is the grid-world domain, as we will discuss in Section 3. Only the initial state and the goal facts are fact landmarks in this domain. To address this problem, FPV considers not solely the facts that are landmarks but all facts that a given planning domain defines.\nMore explicitly, the contributions of this paper are:\n\u2022 In Section 3, we propose a novel approach to goal recognition that involves comparing fact observation probabilities with actually observed facts.\n\u2022 As the fact observation probabilities are usually not given, we propose a method to estimate the fact observation probabilities in Section 4.\n\u2022 In Section 5, we empirically show that FPV achieves better goal recognition precision, especially when dealing with low observability, and is more efficient regarding computation time than existing state-of-the-art methods."}, {"title": "Background", "content": "This section introduces relevant definitions in the context of classical planning and goal recognition."}, {"title": "Classical Planning", "content": "Classical planning uses a symbolic model of the planning domain that defines the properties of the planning environment (i.e., planning facts) and possible actions. These actions are defined by their preconditions and effects. Given an initial state and a goal, planning methods aim to construct an action sequence (i.e., plan) that transforms the initial state into a valid goal state (i.e., a state in which the goal description holds). The generated plans might be strictly or approximately optimal depending on the planning method used. For the theoretical analysis of FPV, we focus on the STRIPS part of the Planning Domain Definition Language (PDDL) [10]. Nevertheless, our empirical evaluation results show that FPV performs very well in domains not restricted to STRIPS. It is important to note that negated facts in the preconditions and goal(s) must be handled properly for the STRIPS principle to be applied to non-STRIPS domains. Our current implementation compiles negated facts away and introduces an additional fact for each negated fact in the grounded planning domain.\nDefinition 1 ((STRIPS) Planning Problem). A Planning Problem is a Tuple P = (F, s0, A, g) where F is a set of facts, s0 \u2286 F and g \u2286 F are the initial state and the goal and A is a set of actions. Each action is defined by its preconditions Pre(a) \u2286 F and its effects Add(a) \u2286 F and Del(a) \u2286 F. Add(a) and Del(a) describe the effects of an action a in terms of facts that are added and deleted from the current state when the planning agent executes a. Actions have a non-negative cost c(a). All facts f \u2208 F that are true in the current planning state define a state s \u2286 F. A state s is a goal state if and only if s \u2287 g. An action a is applicable in a state s if and only if Pre(a) \u2286 s. Applying an action a in a state s leads to a new state s' = (s \u222a Add(a)) \\ Del(a)).\nDefinition 2. (Solution to a Planning Problem) A solution for a planning problem is a sequence of applicable actions \u03c0 = (ai)i\u2208[1,N] that transforms s0 into a goal state. The cost of a plan is defined as c(\u03c0) = \u2211c(ai). A plan is optimal if the cost of the plan is minimal.\nIgnore Delete Effects Relaxation. In this paper, we propose to use concepts developed in the context of the Ignore Delete Effects Relaxation, which has been popular in classical planning since its introduction by Bonet et al. [3]. In the remainder of the paper, we use the term relaxed planning state to refer to a delete relaxed planning state. As the name indicates, this relaxation ignores all delete effects of the actions defined in a planning domain. More formally, a delete relaxed STRIPS planning problem is defined as follows:\nDefinition 3 (Delete Relaxed (STRIPS) Planning Problem). Given a planning problem P = (F, s0, A, g), the corresponding delete relaxed planning problem is defined as P+ = (F, s0, A+, g). The delete relaxed action set A+ contains all delete relaxed actions from A. The preconditions, add list, and delete list for the delete relaxed version a+ of action a are defined as prea+ = prea, adda+ = adda, dela+ = \u00d8. Hence, applying an action a+ in a state s leads to a new state s+ = (s \u222a Add(a+)).\nDefinition 4. (Solution to a Delete Relaxed Planning Problem) A solution for a delete relaxed planning problem is a sequence of applicable actions (relaxed plan) \u03c0+ = (a+i)i\u2208[1,N] that transforms s0 into a goal state. The cost of a relaxed plan is defined as c(\u03c0+) = \u2211c(a+i). A relaxed plan is optimal if the cost of the relaxed plan is minimal."}, {"title": "Goal Recognition", "content": "This paper investigates a solution method for the (online) goal recognition problem. Let us first define the goal recognition problem:\nDefinition 5 (Goal Recognition). Goal recognition is the problem of inferring a nonempty subset G\u0302 of a set of intended goals G of an observed agent, given a possibly incomplete sequence of observed actions O and a domain model D that describes the environment in which the observed agent acts. The observation sequence O is a plan from s0 to the agent's hidden true goal g*. Further, the observed agent acts according to a hidden policy \u03b8. More formally, a goal recognition problem is a tuple R = (D,O,G).\nDefinition 6. (Solution to a Goal Recognition Problem) A solution to a goal recognition problem R is a nonempty subset G\u0302 \u2286 G such that all g \u2208 G\u0302 are considered to be equally most likely to be the true hidden goal g* that the observed agent currently tries to achieve.\nThe most favorable solution to a goal recognition problem R is a subset G\u0302 containing only the true hidden goal g*. In this paper, D = (F, s0, A) is a planning domain with a set of facts F, the initial state s0, and a set of actions A. The online goal recognition problem is an extension to the previously defined goal recognition problem that additionally introduces the concept of time, and we define it as follows:\nDefinition 7 (Online Goal Recognition). Online goal recognition is a special variant of the goal recognition problem, where we assume that the observation sequence O is revealed incrementally. More explicitly, let t \u2208 [1,T] be a time index, where T = |O| and hence, the observation sequence for time index t is defined as Ot = (Oi)i\u2208[1,t]. For every value of t, a goal recognition problem R(t) can be induced as R(t) = (D, G, Ot).\nDefinition 8. (Solution to a Online Goal Recognition Problem) A solution to the online goal recognition problem are the nonempty subsets G\u0302t \u2286 G;\u2200t \u2208 [1, T].\nIt is important to note that, in contrast to many existing methods (i.e., [13], [14], [9], [18], [17]), FPV can deal with observing actions and observing states simultaneously. FPV requires fact observations, as FPV compares the observed facts with the fact observation probabilities. When the observation sequence contains actions, the observed facts can be derived from the given planning domain, which defines the add effects of each action. In the case of state observations, the observed states directly define the observed facts."}, {"title": "Fact Probability Vector Based Online Goal Recognition", "content": "In this paper, we propose to perform goal recognition by comparing the fact observation probability and the set of actually observed planning facts. To discuss how the fact observation probability is defined more formally, we first have to define when a planning fact is considered to be observed.\nDefinition 9 (Observed Planning Fact). Given a goal recognition problem R, we define a planning fact f \u2208 F to be observed during an observation sequence O if and only if f \u2208 \u222aa\u2208O add(a).\nDefinition 10 (Fact Observation Probability). We define the set of fact observation probabilities for each goal g \u2208 G as P = {Pf(B|s0,g)|f \u2208 F}, where variable B can take two values; one representing that f is observed (cf., Definition 9) and the other representing that f is not observed.\nFor example, the distribution Pf(B|s0, g) models the probability that fact f is observed during an observation sequence that starts in s0 and ends in a goal state sg.\nFPV implements the comparison between fact observation probabilities and observed facts based on real vector interpretations of P, s0, and the currently observed state st.\nMapping Planning States And P Into a Real Vector Space. To map a planning state s into its |F|-dimensional real vector representation s \u2208 R|F|, we use the following mapping:\nsf= {1, iff f \u2208 s0, else.\nIn Equation 1, s is a planning state, and sf is the element of s that encodes the planning fact f. The resulting vector mapping s of a planning state s encodes the observational evidence from s in the frame of fact observation probabilities. Algorithm 1, presented later in the paper, will refer to this function under the signature mapState(s).\nTo map the fact observation probabilities P (cf., Definition 10) into an |F| dimensional real vector representation vo \u2208 R|F|, given an initial state so and a goal description g, FPV uses the following mapping:\nvo = Pf(f \u2208 \u222aa\u2208O add(a)|so, g)\nAlgorithm 1, which is presented later in the paper, will refer to this function under the signature mapp(so, g, P). As an example, consider a simple domain in which F = {f1, f2, f3}, s0 = \u00d8, and there is only one possible goal g = {f3}. Further, P is defined as P = {Pf1 = (0.8,0.2), Pf2 = (0.3, 0.7), Pf3 = (0.6, 0.4)}, where we use the following notation (Pf(f \u2208 \u222aa\u2208O add(a)|so,g), Pf(f \u2209 \u222aa\u2208O add(a)|so, g)). For this example, vgo is defined as vgo = (0.8, 0.3, 0.6). The resulting vector representations of P can be interpreted as an encoding of how probable it is that each fact f \u2208 F is added to the current planning state st at any point in time by an observation sequence that starts at so and leads to goal g.\nMethod. For goal recognition, FPV calculates a heuristic score for each goal based on the mappings of the points so and s+ (cf., Equation 1) and the mapping of P for each goal (cf., Equation 2). FPV uses the currently observed relaxed planning state (i.e., s+) instead of the non-relaxed state because it contains additional information about the planning states the observation sequence has already visited. This is because, as defined by Definition 3, under the delete relaxation, facts can only be added to the initial planning state but can never be deleted. Consequently, s+ contains all facts added by any action in the relaxed observation sequence O+, which contains the relaxed action a+ for all actions a \u2208 O. The resulting vector s+ contains a one for all facts that either have been already true in so or were added by an action in the observation sequence and zero otherwise. Based on the vector mappings, FPV calculates the goal recognition heuristic score for each goal as defined by Equation 3.\nh(so, st, vo) = ||(sovo, vo)||2 \u2013 ||(stvo, vo)||2\nIn Equation 3, \u2299 is a special elementwise multiplication of two vectors as defined by Equation 4, (x, y) is a direction vector between two points x and y as defined by Equation 5, and ||x||2 is the l2 vector norm, which is defined as ||x||2 = \u221a\u2211k=1x2i\n(s \u2299 v)i= {si \u2217vo, if voi > 0, else\nUsing the elementwise multiplication \u2299 as defined by Equation 4, ensures that the heuristic value monotonically increases for a goal g when only facts for which Pf(f \u2208 \u222aa\u2208O add(a)|so, g) > 0 holds are observed.\n(x, y) = y - x\nComputing the direction between (so \u2299 vo) and vo results in a vector that encodes what has to be changed from the initial state to reach a valid goal state as approximated by P. This is because the resulting vector becomes zero at all points where both so and vo have values greater than zero. Similarly to the first direction vector, computing the direction between (st \u2299 vo) and vo results in a vector that encodes what has to be changed from the currently observed relaxed state st to reach a valid goal state. Hence, the heuristic computed by FPV estimates the probability of a goal g being the true hidden goal g* of the observed agent by computing the distance already covered by the observation sequence between the initial state and a valid goal state for g as estimated by the fact observation probabilities. In addition, the heuristic punishes goals for which Pf assigns a probability of zero to facts that were actually observed. This is because, in this case, the vector resulting from (st \u2299 vo, vo) will have a value of minus one at all positions that correspond to such facts. Consequently, ||(st \u2299 vo, vo)||2 increases, which results in an overall decreased heuristic value.\nThe algorithm used to recognize goals based on the previously defined vector mappings is summarized in Algorithm 1. As a first step, vo has to be determined for each goal g \u2208 G (lines 4-6). As a second step, the vector representation st of the currently observed relaxed state is calculated (lines 7-9). Afterward, Algorithm 1 calculates the heuristic value for all goals (lines 10-12). As a last step, from the calculated heuristic values, Algorithm 1 selects the goals that have the maximum value (lines 13-19):\nExample. This paragraph illustrates FPV based on a simple example. For this example, we consider the grid environment in Figure 1. In 1, the agent is initially located in cell c23 and wants to reach one of the two goals, G1 or G2. The agent can move through the environment using moves in all directions except diagonal moves to all cells that are not colored black; each move has a cost of 1. We"}, {"title": "Estimating Fact Observation Probabilities", "content": "In practice, the fact observation probabilities are usually not given. Hence, this section proposes a method to estimate the fact observation probabilities from the given planning domain. To estimate fact observation probabilities, FPV first determines sets of supporter actions for each possible goal. Supporter actions were introduced by Hoffmann et al. [8], and they can be sampled very efficiently from a Relaxed Planning Graph (RPG)1 [8]. The idea behind supporter actions was to define a methodology for determining the actions in a given planning domain that are relevant to solving a given planning problem. More precisely, an action a is called a supporter for a fact f if f \u2208 add(a). The following subsection describes the exact algorithm that FPV uses to sample supporter actions. In total, FPV generates N different sets of supporter actions for each goal to ensure that the sets of supporter actions cover several possible paths. Based on the N sets of supporter actions for each goal, FPV calculates for each action a how probable it is that a occurs in one of the N sets. These probabilities can be interpreted as action observation probabilities from which the fact observation probabilities can be derived. Given these probabilities, FPV derives the observation probability for each f \u2208 F (i.e., P) by calculating the probability of observing any of the actions for which f \u2208 add(a) holds."}, {"title": "Sampling Supporter Actions", "content": "To generate the N sets of supporter actions for a goal description g, FPV first generates N sets of supporter actions for each subgoal gi \u2208 g. Algorithm 2 describes the exact algorithm that FPV uses to generate the N sets of supporters for each subgoal gi. The input of Algorithm 2 consists of four elements: f is the planning fact for which the sets of supporter actions are sampled, RPG is an RPG for the given planning problem, so is the initial state, and N is the number of supporter action sets that are generated. In lines 2 and 3, the count map and samples list are initialized. The count map has actions as keys and maps each action key to the number of times that action has already been selected as a supporter action. The samples is the final return variable of the algorithm and stores the generated sets of supporter actions. The for loop that starts in line 4 repeats the following sampling procedure N times. For the sampling process, first the elements C, found, and sups are initialized (lines 5-7). C is the set of facts for which the algorithm samples supporting actions, found is a set that keeps track of the facts already supported, and sups is the set of supporting actions already selected for the current sample. The search for supporting actions for a fact p occurs in lines 13-22. For this search, the algorithm starts at the first action level of the RPG, which ensures that supporters closer to so are found first. All potential supporter actions for p from the currently searched RPG action level are added to the psups set. When psups contains at least one element after searching the current RPG action level, no further levels are searched. In Line 23, the algorithm selects the potential supporter action that has been selected the minimum number of times (i.e., it has the minimal count value compared to all elements of psups). This selection procedure ensures that when different options exist for selecting supporting actions, all options are picked with roughly the same probability. When there are several actions with the minimum count value, the algorithm randomly selects one of those actions (Line 24). Afterward, the found, C, sups, and count variables are updated accordingly (lines 25-28). In lines 29-33, the algorithm iterates over the preconditions of the selected action and adds all facts that are not already supported or part of so to newC. Following this, in lines 34-37, the algorithm removes all add effects of the selected action from C and newC as the selected action already supports them. As a result, Algorithm 2 returns a list of sampled sets of relevant actions for reaching the subgoal gi from so.\nFrom these sets of supporter actions for each subgoal, FPV then generates N sets of supporter actions for g. FPV does this by using Algorithm 3. The input of Algorithm 3 is composed of three elements: The first element, S, is a map that maps all subgoals gi to their respective list of supporter action sets sampled previously by Algorithm 2. The second input element, N, is the number of supporter action sets generated. The third input element, g, is the goal description. In the beginning, Algorithm 3 initializes the two sets SG and SGi in lines 2 and 3. SG stores the generated sets of supporter actions for g, and SGi is used as a temporary set to unify the sets of supporter actions from each subgoal gi. In lines 4 to 12, the algorithm iterates N times to generate N sets of supporter actions from g. This is done by randomly picking one set of supporter actions for each subgoal gi from S (Line 6) and unifying it with SGi (Line 7). Removing all already-picked sets of supporter actions from S ensures that each set is only picked once (Line 8). After the for loop, which ends in Line 9, SGi holds a complete supporter action set for g that the algorithm then adds to SG. As a result, Algorithm 3 returns a set of N supporter action sets for g."}, {"title": "Evaluation", "content": "We empirically evaluated the FPV method on 15 benchmark domains commonly used to evaluate goal recognition approaches. The empirical evaluation aims to achieve the following goals:\n1. Evaluate the goal recognition performance of the FPV method compared to four existing goal recognition methods.\n2. Investigate how efficient the FPV method is regarding computation time compared to four existing goal recognition methods.\nAll experiments consider the online goal recognition problem, as defined by Definition 7.\nDatasets. We evaluated the approaches on 15 commonly used benchmark domains (e.g., Pereira et al. [11]).\nGoal Recognition Methods. To compare the performance of the FPV method to the performance of existing goal recognition methods, we implemented a planning-landmark-based approach introduced by Pereira et al. [11] (PLR), an approximate approach proposed by Ram\u00edrez and Geffner [13] (RG09), an LP Based approach introduced by Santos et al. [16] (LP), and an approach by Masters and Sardina [9] (MS). FPV, PLR, and MS were implemented in Java using the PPMAJAL2 library as a basis for PDDL related functionalities. For the RG09 and LP approaches, we used the original code that is publicly available. We chose the PLR, LP, and MS approaches because they are recent methods for the two major types of methods that implement the PRAP principle, i.e., approaches that derive a probability distribution over the possible goals by a comparison among the costs of different plans that are calculated for each goal (MS) and approaches that use heuristic scores to rank all possible goals g \u2208 G (PLR and LP). In addition, we compare FPV to the RG09 approach because it is related to the FPV method through the use of concepts from relaxed planning. For the MS approach, we used the MetricFF planner [7] with B = 1. For the FPV method, we sample N = 10 relevant action sets for each g and, as FPV involves some random selections, we report the results averaged over 20 repeated runs. We also tested larger values for N, which did not significantly increase recognition performance.\nExperimental Design. All experiments in this evaluation were carried out on machines with 24 cores, 2.60GHz, and at least 386GB RAM. We used the mean goal recognition precision to assess the performance of the different methods. We calculate the precision similar to existing works (e.g., [2]). Furthermore, as we consider online goal recognition problems R(t) in this evaluation, we calculated the mean precision for different fractions \u03bb = t/T of total observations used for goal recognition. Here, we used relative numbers because the lengths of the involved observation sequences substantially differ. Hence, the mean precision Prec for a fraction \u03bb \u2208 [0, 1] is calculated as follows:\nPrec(A, D) = \u2211R\u2208D [R\u2208R([\u03bbTR])] /D,\nHere, D is a set of online goal recognition problems R, g*R denotes the correct goal of goal recognition problem R, TR is the maximum value of t for online goal recognition problem R (i.e., length of observation sequence that is associated with R), and [g*R \u2208 GR(t)] equals one if g*R \u2208 GR(t) and 0 otherwise, where GR(t) is the set of recognized goals for R(t). In other words, the precision quantifies the probability of picking the correct goal from a predicted set of goals G\u0302 by chance."}, {"title": "Experimental Results and Discussion", "content": "Goal Recognition Performance. Table 2 shows the experimental results for all evaluated approaches when applied to the 15 benchmark domains. The average precision is reported for different \u03bb values. As mentioned earlier in this section, we report the averaged precision over 20 repeated runs for FPV. We also analyzed the standard deviation of FPV's recognition precision over the 20 runs and found that it is between 0 and 0.077, with an average of 0.016 for all benchmark domains. Hence, as the standard deviation is very low, we omitted it from Table 2 for better readability. Column S reports the average spread (size of the set of goals recognized as the true goal) for each domain. The larger this value is, the smaller the value of the precision tends to be, as predicting a larger number of goals to be the true goal generally decreases the precision measure (cf., Equation 6). The results show that the FPV method, on average, achieves the best goal recognition precision for all values of \u03bb. Especially when dealing with low observability (i.e., 0.3 \u2264 \u03bb \u2264 0.9), FPV significantly outperforms all other approaches. Nevertheless, it is also interesting to note that although FPV outperforms all other approaches on average, no single approach outperforms all other approaches in all domains. For example, the MS approach achieves superior performance in the driverlog dataset, the PLR approach achieves superior performance in the dwr dataset.\nComputational Efficiency. Table 3 reports the extrapolated average cumulated computation time for all evaluated approaches, averaged over all 15 benchmark domains. The computation time is reported in seconds for different potential sizes of observation sequences (i.e., |O|) and sets of possible goals (i.e., |G|). The values in this table are calculated from average computation time values per observation and goal for each approach and are averaged over all experiments. The table's primary purpose is to visualize how well the different recognition approaches scale regarding computation time when the size of the considered goal recognition problem is altered. The results show that the FPV approach is the most computationally efficient among all evaluated approaches. Table 3 visualizes the advantages of FPV regarding computational efficiency compared to the other evaluated approaches very well, especially when the size of the goal recognition problems is scaled. One can see that the computation time that FPV requires does not increase with an increasing number of observations. This is because once FPV has determined fact observation probabilities for each goal, FPV only has to compute the heuristic values for each goal for each new observation. The same can be observed for the PLR approach for a similar reason as for FPV. PLR only has to compute some heuristics based on the landmarks extracted upfront. The main computation time that PLR requires is due to the landmark extraction process, which is required once per online goal recognition problem, similar to the fact observation probability estimation of FPV. Nevertheless, the results show that FPV's extraction process is much more efficient than that of PLR, which results in FPV being roughly four times faster than PLR, the second fastest evaluated approach. As expected, the MS approach is the least computationally efficient, as it requires computing an entire non-relaxed plan per goal per new observation step. Interestingly, although RG09 uses relaxed plans, which can be computed more efficiently than non-relaxed plans, it also does not scale well with increasing observations. The main reason for this is most probably that the RG09 approach also recomputes a relaxed plan for each goal for each new observation step. In addition, it requires the planner to include all observations already observed in the computed plan. This results in a much more complex planning problem.\nThe results also show that when |G| increases, the FPV and PLR approaches scale less well than when |O| increases. This is because both FPV and PLR require the most computation time to compute fact observation probabilities or landmarks. With an increasing number of potential goals, this process, of course, requires more time. Nevertheless, one can see that the FPV approach still scales much better than the PLR approach and also much better than the MS, LP and RG09 approaches, for which the scaling is even worse than for PLR."}, {"title": "Related Work", "content": "Since the idea of Plan Recognition as Planning was introduced by Ram\u00edrez and Geffner [13], many approaches have adopted this paradigm [14, 15, 20, 18, 17, 9, 11, 4]. It was recognized relatively soon that the initial PRAP approaches are computationally demanding, as they require computing entire plans. Since then, this problem has been addressed by many studies with the approach by Pereira et al. [11] being a recent example. This method also belongs to a recent type of PRAP method (to which FPV also belongs), which does not derive probability distributions over the set of possible goals by analyzing cost differences but ranks the possible goals by calculating heuristic values. Additional approaches from this area include a variant that was suggested as an approximation for their main approach by Ram\u00edrez and Geffner [13] and the Linear Programming approach, which was introduced by Santos et al. [16]."}, {"title": "Conclusion", "content": "In conclusion, we presented FPV, a new goal recognition method based on comparing fact observation probabilities with the set of actually observed facts. We empirically evaluated the FPV and showed that it achieves better goal recognition precision than four state-of-the-art goal recognition methods, especially early in an observation sequence. FPV is also four times more efficient regarding computation time than the second most efficient approach, PLR, and roughly 2000 times faster than the least efficient approach, MS. FPV also scales much better in terms of observation sequence length compared to most of the evaluated approaches. FPV's advantage regarding computational efficiency is mainly due to the fact that FPV computes the fact observation probabilities offline, which leaves very little computational workload during online recognition. In future work, it would be interesting to consider evaluation scenarios based on actual human behavior. Another exciting path would be exploring different approaches to estimating the fact observation probabilities."}]}