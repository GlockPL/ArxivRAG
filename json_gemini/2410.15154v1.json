{"title": "MCCoder: Streamlining Motion Control with LLM-Assisted Code Generation and Rigorous Verification", "authors": ["Yin Li", "Liangwei Wang", "Shiyuan Piao", "Boo-Ho Yang", "Ziyue Li", "Wei Zeng", "Fugee Tsung"], "abstract": "Large Language Models (LLMs) have shown considerable promise in code generation. However, the automation sector, especially in motion control, continues to rely heavily on manual programming due to the complexity of tasks and critical safety considerations. In this domain, incorrect code execution can pose risks to both machinery and personnel, necessitating specialized expertise. To address these challenges, we introduce MCCoder, an LLM-powered system designed to generate code that addresses complex motion control tasks, with integrated soft-motion data verification. MCCoder enhances code generation through multitask decomposition, hybrid retrieval-augmented generation (RAG), and self-correction with a private motion library. Moreover, it supports data verification by logging detailed trajectory data and providing simulations and plots, allowing users to assess the accuracy of the generated code and bolstering confidence in LLM-based programming. To ensure robust validation, we propose MCEVAL, an evaluation dataset with metrics tailored to motion control tasks of varying difficulties. Experiments indicate that MCCoder improves performance by 11.61% overall and by 66.12% on complex tasks in MCEVAL dataset compared with base models with naive RAG. This system and dataset aim to facilitate the application of code generation in automation settings with strict safety requirements. MCCoder is publicly available at https://github.com/MCCodeAI/MCCoder.", "sections": [{"title": "I. INTRODUCTION", "content": "Motion control, a cornerstone of automation, has significantly advanced industrial processes since the advent of NC machines in the 1950s. By precisely controlling actuators, it has revolutionized manufacturing, from early CNC machines to today's sophisticated AI-driven robots and semiconductor equipment. While motion control encompasses various programming methods (e.g., CAD/CAM, PLC, robot teaching), this paper focuses on the complex domain of motion API invocation. Today, software engineers in this industry still manually program due to its complexity and safety concerns. Some private motion libraries have over 1,000 APIs with various arguments, creating a steep learning curve and making reliance on automatic programming tools difficult. Additionally, engineers must code and iteratively test with electrical and mechanical components to ensure optimal motion performance and eliminate potential hazards for humans and machines during execution."}, {"title": "II. PRELIMINARY", "content": "MCCoder utilizes a soft-motion architecture to generate Python code for motion control by invoking APIs from a private library."}, {"title": "A. Soft-motion", "content": "The soft-motion controller has a flexible and lightweight architecture that runs on a general PC without the need for specialized hardware. It uses a dedicated CPU core with a real-time OS and software motion algorithms to deliver performance comparable to high-end hardware controllers. MCCoder sends control code to the soft-motion engine through a DLL. Soft-motion executes the code in a simulation engine first, allowing for data logging and verification before deployment. Additionally, its PC-based software architecture makes soft-motion ideal for integrating new functions, including LLMs and other AI technologies."}, {"title": "B. Control Code and Private Library", "content": "Unlike standardized PLC programming like IEC 61131-3, control codes programming in Python are more flexible and difficult to apply a fixed template. In this research, we use a private motion library in the soft-motion controller with more than 1,000 APIs capable of controlling up to 128 axes and 256 independent task channels. These APIs encompass most motion control functionalities, such as fieldbus communication, digital and analog IO control, and various trajectory and position/velocity/current control for servo axes. The private library also includes hundreds of sample codes."}, {"title": "III. METHODOLOGY", "content": "We developed the MCCoder system to tackle the code generation challenges in motion control. Figure 2 shows an overview of the MCCoder system. It involves six modules: task decomposition, retrieval, code generation, soft-motion, self-correction and data verification."}, {"title": "A. Task Decomposition", "content": "The MCCoder system receives user questions as a motion task. It could be a single task or a multitask composing with many sub-tasks. This module will read the task and decompose them to be a list of sub-tasks."}, {"title": "B. Retrieval", "content": "The retrieval module extracts APIs and expert-created sample codes from the private library, each accompanied by clear instructions. Retrieval methods are commonly employed in code generation to provide relevant samples and contexts. Various Retrieval-Augmented Generation (RAG) techniques are available, including native RAG, advanced RAG, and modular RAG [4]. Given that the library contains uniformly formatted sample codes and APIs, we employ Sparse Retrieval (BM25), Dense Retrieval (embedding-based VectorStores), and a re-ranker to match user queries with the most relevant content, setting the top-k as 10."}, {"title": "C. Code Generation", "content": "The code generation model processes sub-tasks and retrieved sample codes to generate sub-codes based on prompts, repeating this for m tasks. It then combines the user question and sub-codes to generate the final code. A critical but often overlooked aspect is prompt engineering. To consistently refine the prompts, we iterated and established 8 guidelines to help models avoid common mistakes."}, {"title": "D. Soft-Motion", "content": "Soft-motion includes a real-time engine for operating actual machines and a simulation engine. MCCoder sends the final code to the soft-motion controller, which runs the code in the simulation engine and returns execution feedback."}, {"title": "E. Self-Correction", "content": "If the soft-motion engine returns a syntax or API error, MCCoder uses the error to retrieve relevant documents, regenerate code and run it again for self-correction iteratively. If no error is detected, soft-motion logs the entire trajectory data for subsequent verification."}, {"title": "F. Data Verification", "content": "We can design a 3D model of a machine and import the model file into the 3D simulator, which displays the motion in real-time, providing users with an intuitive verification when it runs. Additionally, logging data is recorded every 1ms by the soft-motion engine as the process data. These features are essential to build user trust in an AI-driven system through visual and data-driven verification."}, {"title": "IV. EVALUATION DATASET", "content": "Existing code generation datasets, like HumanEval [5] and MBPP EvalPlus [6], focus on general-purpose code evaluated via unit tests. The generation of control code requires execution in motion controllers to record the endpoint and trajectory data. MCEVAL evaluation dataset fills this gap for the automation industry. We meticulously constructed MCEVAL, verifying each task both manually and through soft-motion simulations to ensure that the programming tasks are well-formed."}, {"title": "A. Construction", "content": "MCEVAL comprises 116 tasks that were chosen to provide a diverse but manageable set of motion control program-ming challenges. It offers substantial diversity, covering most common used motion control functions in the library. These functions include point-to-point motion, linear, circular, and helical interpolation, splines, short-segment look-ahead, compensation, and event-driven interactions of axes and IOs under various profiles, as well as their combinations. Each task includes a TaskId, Instruction, CanonicalCode, and Difficulty. The properties of these tasks are detailed in Table I.\nTo mimic a human engineer's problem-solving capabilities, we structured the tasks into three difficulty levels. These levels are differentiated by sample code, primarily demonstrating single API function calls and not covering all argument combinations (e.g., there are 19 acceleration profiles). The examples are shown in Examples 2 and 3. Difficulty Level 1 involves invoking similar APIs as in the sample codes but with different numerical arguments, which should be easy for most LLMs if the retrieved sample codes are correct. Difficulty Level 2 uses arguments not shown in the sample codes, making it more challenging. LLMs might make mistakes but should utilize error information or re-trieved content for self-correction. Difficulty Level 3 involves combining up to 8 different motion tasks. Consolidating more than 8 tasks into a single instruction may exceed input or output token limits for most models. These levels emulate real-world scenarios from easy to complex motion tasks."}, {"title": "B. Metrics", "content": "To comprehensively assess the generation of code on the evaluation dataset, the following metrics are chosen based on control expertise.\n1) First Time Pass Rate (FTPR): FTPR is calculated as Equation (1) representing the proportion of codes that pass the test on the first attempt [7]. Pass@k is commonly used in code generation metrics and repeats many times to eliminate bias, but practical users prefer code that works correctly on the first try. We chose this straightforward measure to align with real-world use cases for control code.\n$FTPR=\\frac{Npassed}{NTotal}*100$ (1)\n2) MatchEndPoints and DTW: The soft-motion engine generates log data every 1ms during code execution. Log files from the canonical code in MCEVAL dataset and the generated code are compared. MatchEndPoints compares only the end points, while Dynamic Time Warping (DTW) compares all trajectory points, measuring the similarity of time series data. In motion control, it depends on specific scenario whether to check end points or trajectories, for instance, interpolation motions should match trajectories, while point-to-point motions only need endpoint matching. We will measure both FTPR (MatchEndPoints) and FTPR (DTW)."}, {"title": "V. EXPERIMENTS", "content": "We access closed-source models through APIs and open-source one on a local server with eight NVIDIA GeForce RTX 3090 GPUs and Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz. A soft-motion controller and 3D simulator were installed on a Windows 11 PC, ready for code execution and simulation."}, {"title": "A. Experimental Settings", "content": "We access closed-source models through APIs and open-source one on a local server with eight NVIDIA GeForce RTX 3090 GPUs and Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz. A soft-motion controller and 3D simulator were installed on a Windows 11 PC, ready for code execution and simulation."}, {"title": "B. Base Models", "content": "LLMs evolve rapidly, continuously setting new state-of-the-art benchmarks. For this study, we selected the latest closed-source and open-source LLMs as base models. Given the average token length of 1,424 in the sample codes, we opted for models with relatively large context windows and output lengths, as detailed in Table II. GPT-40, GPT-40-mini, Lama-3.1-70b, and DeepSeek-Coder-v2 were selected for their proven performance in code generation benchmarks."}, {"title": "C. Baseline", "content": "Since the private library is not in pre-trained LLM data, zero-shot evaluation is inapplicable. We established a base-line using the base model with naive RAG and top-k = 10, comparing it to the base models integrated into the MCCoder system."}, {"title": "D. Experimental Results", "content": "We compared naive RAG and MCCoder using FTPR (MatchEndPoints) and FTPR (DTW) metrics. The overall pass rates and three difficulty levels were also evaluated for each model, as shown in Table III. We will present results with research questions.\n1) RQ1. Does MCCoder imporve the code generation per-formance?: MCCoder yielded an overall FTPR improvement of 11.61%. While this might appear modest at first glance, a deeper analysis reveals a more substantial impact at higher difficulty levels. At Level 3, the improvement increases to 66. 12%, demonstrating MCCoder's exceptional ability to tackle complex real-world challenges.\n2) RQ2. Which base model behaves the best with MC-Coder?: GPT-40 demonstrated the best overall performance, particularly with MCCoder. However, DeepSeek-Coder-v2 exhibited very strong performance comparable to GPT-40 and even outperformed it in naive RAG, which is surprising given its 24 billion active parameters. In the HumanEval evaluation, DeepSeek-Coder-v2 scored 90.2, close to GPT-40's 91.0 [8]. Our experiments show similar results. GPT-40-mini and Lama-3.1-70B performed slightly weaker."}, {"title": "E. Error Analysis", "content": "During the evaluation of the control codes, the soft-motion engine returns some errors. We analyze the errors in code generation with gpt-40 and their causes in detail.\n1) API errors (5.2%): These errors indicate incorrect API function invoking. But the API error rate is relatively small, showing the effectiveness of the current retrieval method for relevant sample codes reference.\n2) Argument errors (79%): The most prevalent error type involves correct API invocations with incorrect arguments, frequently stemming from hallucinations or disregard for prompt instructions. Self-correction can sometimes address and correct these errors. Explicit error messages like \"type object ProfileType has no attribute scurve. Did you mean: SCurve?\u201d significantly enhance the likelihood of successful correction in sub-sequent attempts. In contrast, implicit error messages hinder accurate identification and correction.\n3) Syntax errors (15.8%): These errors predominantly arise from hallucinations or the invention of nonex-"}, {"title": "F. Ablation Study", "content": "We conducted an ablation study focusing on prompt engineering, task decomposition, and self-correction modules with the base models GPT-40 and GPT-40-mini. As shown in Figure 4, prompt engineering plays a significant role in the MCCoder system, causing a 20.51% performance drop in FTPR (DTW) for GPT-4o and a 15.79% drop for GPT-40-mini, with a larger impact on the larger model. Task decomposition resulted in a performance drop of around 13% for both models, indicating that more than half of the difficulty level 3 tasks failed without this module. The self-correction module had a relatively smaller impact, with a 1-5% performance difference. However, as we improve the"}, {"title": "VI. DISCUSSION", "content": "We presented the system to a group of control engineers, ranging from novices to experts, and collected their feedback. Both groups highly regarded MCCoder for facilitating learning new functions and improving coding efficiency. They highlighted the necessity of human-involved verification to address safety concerns, an issue they encountered when using general LLMs for control code generation. Moreover, they suggested enhancing code interpretability, such as through visual flowcharts or explanations of API usage, to assist in better understanding and debugging when minor errors occur. This feedback has provided valuable insights for future improvements and deployment."}, {"title": "A. Engineers' feedback", "content": "We presented the system to a group of control engineers, ranging from novices to experts, and collected their feedback. Both groups highly regarded MCCoder for facilitating learning new functions and improving coding efficiency. They highlighted the necessity of human-involved verification to address safety concerns, an issue they encountered when using general LLMs for control code generation. Moreover, they suggested enhancing code interpretability, such as through visual flowcharts or explanations of API usage, to assist in better understanding and debugging when minor errors occur. This feedback has provided valuable insights for future improvements and deployment."}, {"title": "B. Limitations", "content": "The current MCEVAL dataset may not comprehensively encompass the full spectrum of motion control tasks, ne-cessitating further testing across a broader range of mo-tion scenarios to enhance the dataset. Additionally, the error messages returned by the soft-motion controller are occasionally ambiguous, which impedes MCCoder's self-correction capabilities. While the system's effectiveness has been demonstrated through the four existing model evalua-tions, conducting additional model tests could yield further insights for refinement and improvement."}, {"title": "VII. RELATED WORK", "content": "Recent advancements in LLMs, such as OpenAI's Codex, GitHub's Copilot, Google's Gemini, and Meta's CodeLlama, have significantly enhanced code generation, improving both productivity and code quality [9]. Trained on diverse code-bases with advanced NLP techniques, these models excel in code completion, refinement, and debugging [10]. Bench-marks like HumanEval, MBPP, and CodeXGLUE reveal that these models are nearing or exceeding human-level"}, {"title": "A. Code Generation with LLMs and Strategies", "content": "Recent advancements in LLMs, such as OpenAI's Codex, GitHub's Copilot, Google's Gemini, and Meta's CodeLlama, have significantly enhanced code generation, improving both productivity and code quality [9]. Trained on diverse code-bases with advanced NLP techniques, these models excel in code completion, refinement, and debugging [10]. Bench-marks like HumanEval, MBPP, and CodeXGLUE reveal that these models are nearing or exceeding human-level"}, {"title": "B. Code Generation in Motion Control", "content": "Experts in industrial automation have explored methods to automate programming tasks, including model-driven development environments [18] and rule-based systems like automated Matlab-to-C++ translators [19]. 13 code generation methods for control logic has been classified since 2004 [20], but these traditional methods were limited to small-scale applications. Large Language Models (LLMs) offer signifi-cant advantages for large-scale applications. Researchers are now using LLMs for innovative code generation methods. For example, a retrieval-augmented approach was proposed for generating IEC 61131-3 Structured Text programs using GPT-4, LangChain, and OpenPLC, validated through expert simulations [2]. Similarly, LLM4PLC was developed as a user-guided iterative pipeline using syntax checkers and LLM fine-tuning for PLC programming [3]. These methods represent a promising start for automating control code generation."}, {"title": "VIII. CONCLUSION", "content": "In this paper, we introduce MCCoder to leverage LLMs for generating motion control code. MCCoder addresses the complexities and safety-critical aspects of motion con-trol programming by integrating prompt engineering, task decomposition, retrieval, code generation, soft-motion and verification through simulation and data logging. Validated using the MCEVAL dataset, MCCoder significantly improves code generation performance, especially for the most chal-lenging tasks. These advancement provide valuable insights and guidelines for future deployment and research in indus-trial automation and control code generation. Future work will focus on expanding the MCEVAL dataset, improving error message clarity from the soft-motion controller, and exploring fine-tuning with larger models to further enhance MCCoder's capabilities."}]}