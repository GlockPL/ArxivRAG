{"title": "How Certain are Uncertainty Estimates? Three Novel Earth Observation Datasets for Benchmarking Uncertainty Quantification in Machine Learning", "authors": ["Yuanyuan Wang", "Qian Song", "Dawood Wasif", "Muhammad Shahzad", "Christoph Koller", "Jonathan Bamber", "and Xiao Xiang Zhu"], "abstract": "Uncertainty quantification (UQ) is essential for assessing the reliability of Earth observation (EO) products. However, the extensive use of machine learning models in EO introduces an additional layer of complexity, as those models themselves are inherently uncertain. While various UQ methods do exist for machine learning models, their performance on EO datasets remains largely unevaluated. A key challenge in the community is the absence of the ground truth for uncertainty, i.e. how certain the uncertainty estimates are, apart from the labels for the image/signal.This article fills this gap by introducing three benchmark datasets specifically designed for UQ in EO machine learning models. These datasets address three common problem types in EO: regression, image segmentation, and scene classification. They enable a transparent comparison of different UQ methods for EO machine learning models. We describe the creation and characteristics of each dataset, including data sources, preprocessing steps, and label generation, with a particular focus on calculating the reference uncertainty. We also showcase baseline performance of several machine learning models on each dataset, highlighting the utility of these benchmarks for model development and comparison. Overall, this article offers a valuable resource for researchers and practitioners working in artificial intelligence for EO, promoting a more accurate and reliable quality measure of the outputs of machine learning models.", "sections": [{"title": "I. INTRODUCTION", "content": "Researchers face a wide range of challenges in Earth observation (EO) downstream tasks, including tasks like regression (e.g., predicting crop yield), image segmentation (e.g., delineating building boundaries), and image classification (e.g., identifying land use land cover types). Due to the inherent complexity of many problems, traditional physical models often do not exist. This has led the scientific community to increasingly rely on machine learning models to extract meaningful insights from the vast amount of EO data. Deep learning techniques are at the forefront of development in data-intensive science in EO [1]. Deep learning, especially convolutional neural networks (CNNs) and recurrent neural networks (RNNs), has proven to be very successful in various EO tasks, such as image recognition, object detection, semantic segmntation, action recognition, image captioning, and many more [1], [2]. Recently, the field has witnessed the rise of transformers and variants like vision transformers [3], [4], a deep learning architecture inspired by how language models understand context. Unlike CNNs and RNNs, transformers don't rely on a specific order for processing data. Their potential extends beyond analyzing static snapshots of our planet, but also into Earth system science. Here, transformers emerges as a promising tool. One particularly noteworthy application of transformers in Earth system science is EarthFormer [5]. This model tackles a specific challenge: Earth system forecasting, which traditionally relies on complex physical models. EarthFormer leverages a transformers ability to capture long-range dependencies to analyze spatio-temporal EO data, potentially offering a data-driven alternative for forecasting tasks.\nEO data are increasingly being relied upon for a diverse range of problems in Earth system science including numerical weather prediction, climate services and in testing and tuning Earth System Model (ESM) projections [6]. For example, the World Meteorological Organization has established, through its Global Climate Observing System, 55 Essential Climate Variables (ECVs). These are defined as physical, chemical or biological variables that are critical for defining Earth's climate and state. Around 60% of these ECVs can be observed using EO data. In addition, EO data are providing unique insights into Earth surface processes and climate dynamics that are being used to test, tune and parameterize ESMs [7], [8]. At the same time, we are experiencing an unprecedented growth in EO capabilities and the number of satellites in orbit, largely driven by the availability of more affordable commercial launcher platforms and satellites [6]. This increase in volume and complexity of EO data necessitates new and efficient methods for information extraction, compression and dimensional reduction. Machine learning methods have been used for some time to achieve this [9] but with little attention placed on the robustness and uncertainties in the models used and the resulting outputs.\nAs EO products are employed in various important activities, uncertainty quantification (UQ) has always been a crucial topic in EO. This is especially true when machine learning models are employed in information retrieval. EO involves collecting data from diverse sources, including satellites, airborne platforms, and ground-based sensors, that are subject to various sources of uncertainty. These uncertainties can arise from multiple factors such as varying sensor types, sensor calibration, spatial and temporal changes (e.g., changing illumination, weather conditions and different seasons), atmospheric interference, geometric distortions, and limitations in the measurement process. Those data are the inputs to the training and inference of machine learning models. The uncertainty originating from the input data is known as the data uncertainty, or aleatoric uncertainty in machine learning [10]. Apart from that, machine learning models are inherently uncertain. The model architectures themselves are not guaranteed to capture the real physics or processes. This is also known as model uncertainty, or epistemic uncertainty. Summarized in Fig. 1, the predictive uncertainty in \u0177 depends on three elements: 1. the uncertainty in the observations x, 2. the structural uncertainty in the model F, and 3. the uncertainty in the training data D."}, {"title": "B. Related work", "content": "1) Uncertainty analysis in AI & EO: As mentioned earlier, UQ in AI is categorized into two main types: aleatoric (or data) uncertainty and epistemic (or model) uncertainty [10]. The former captures the inherent randomness or variability in the data. It is often modeled by considering the noise or errors in the observed data. Epistemic uncertainty, on the other hand, captures the uncertainty resulting from limited/incomplete data or the assumptions made by the AI model. Typically, these two uncertainties are modeled separately using well-known UQ methods including Bayesian inference [11], [12], [13], [14] which assigns probability distributions to the network weights, ensemble methods [15] [16] which average predictions from multiple models, test time augmentations [17] which enable exploration of different views via augmented test samples, and single deterministic networks [18], [19] that capture only a single point estimate rather than a full probability distribution. UQ plays an important role in AI by providing insights into the reliability and robustness of the AI predictions. Without UQ, a model prediction is, effectively, uninterpretable: the uncertainty could be larger than the signal. Since these AI models often deal with complex and noisy data, their predictions are affected by uncertainties caused by various factors [10], e.g., variability in real world situations, errors inherent to the measurement systems, incorrect training procedure, misspecification of the model architecture, or errors caused by unknown data. UQ aims to estimate and characterize these uncertainties on the predictions (also referred to as predictive uncertainty) to make informed decisions and improve the trustworthiness of AI systems. Moreover, reliable estimates of uncertainty along with robust geo-variables derived from EO data may be embedded into process models (e.g., ocean, hydrological, weather, climate, etc.) to derive information vital for drawing meaningful conclusions in a wide range of applications. However, while there are numerous publications relating to Al for EO, the literature addressing UQ in EO is limited [20], [21], [22], [23], [24], [25].\n2) Benchmark datasets in EO: Benchmark datasets serve as valuable resources for researchers in AI. Datasets like DeepGlobe Land Cover Classification [26], SpaceNet [27], DOTA [28] and many more contains high quality reference labels. They enable fair comparisons and evaluations of different models and methodologies, facilitating the advancement of this field.\nOne of the major hurdles in UQ for EO is the lack of dedicated benchmarking datasets. A recent publication [29] comprehensively reviewed over 500 datasets in AI for EO, containing nearly all available resources. However, none were specifically designed for UQ. Only a handful of them mention the topic at all. To our knowledge there is no EO dataset tailored for benchmarking uncertainties of AI models. This gap exists because creating quality EO datasets with accurate labels is already a labor intensive process. The additional challenge of incorporating high quality uncertainty labels further increases the complexity and effort involved. Nevertheless, we present several existing EO datasets below. Although not specifically designed for UQ, they are still relevant and worth mentioning."}, {"title": "C. Contribution of this paper", "content": "This paper presents three novel EO datasets specifically designed for benchmarking uncertainty estimates from machine learning models. These datasets address three common problems in EO: regression, segmentation, and classification. We chose biomass regression, building footprint segmentation, and local climate zones (LCZs) classification as examples to create our benchmark datasets, considering their current popularity and high relevance within the EO community. The design of these datasets prioritizes aleatoric uncertainty over epistemic uncertainty. We believe that in big data regimes, modeling aleatoric uncertainty is more fundamental to real world problems themselves [33]. Aleatoric uncertainty is inherent to the data and cannot be reduced, whereas epistemic uncertainty can often be mitigated with larger amounts of data.\nThe three proposed datasets are as follows."}, {"title": "II. REGRESSIONUQ: SIMULATED BIOMASS DATASET FOR UQ IN REGRESSION", "content": "Above-ground biomass (hereafter termed biomass), defined as the dry weight of the trees in a unit forest, is an important indicator for monitoring and evaluating the forests. Recent missions, such as Biomass [34] and the Global Ecosystem Dynamics Investigation (GEDI) [35], are tailored for producing the biomass maps on a global scale to gain more knowledge of carbon cycle on our planet Earth. But due to the high cost of acquiring ground biomass measurements and the possible imperfect biomass retrieval model resulting from the scarcity of the data, UQ is crucial. For GEDI mission, it is required that the error of 80% of the biomass estimates shall be below 20 Mg/ha or 20% of the estimates.\nIn single-tree biomass estimation, allometric equations are accepted as the state of the art [36]. The model was obtained by fitting observed tree measurements tree height H, trunk diameter at breast height D (hereafter simply diameter), wood density p, and tree biomass B to a linear model in log scale $In B = a + \\beta ln(\\rho D^2H)$, where a and \u1e9e are two parameters to fitted by the data. Apparently, such empirical formulas are inherently uncertain due to its over simplification and limited training data. However, biomass datasets generated from allometric equations are still scars and are regarded as the best available datasets due to the high expense of data collection for the model training and testing. Acknowledging this fact, we simplify the problem in our dataset by setting the allometric equation to be the ground truth physical model, for two reasons: 1. allometric equations are accepted in the community as the best possible solution, and 2. a ground truth biomass value could be known and the ground truth predictive uncertainty could be calculated."}, {"title": "B. Dataset generation", "content": "1) General setup: The most widely used allometric equation for a single tree in tropical forests proposed by Chave et al. [36] was employed as the ground-truth physical model in the simulation. The equation is shown in Eqn. 1, where B is the biomass, D the diameter, H the tree height, and p the wood density. We generate a simulated biomass uncertainty dataset with pseudo data uncertainty ground truth.\n$B = f(\\rho, D, H) = 0.0673 \\times (\\rho D^2H)^{0.976}$"}, {"title": "3) Calculation of reference aleatoric uncertainty", "content": "With a defined ground truth physical model, it is straightforward to propagate uncertainties from the input tree height and diameter to the output biomass. Ideally, the variance on the output can be analytically calculated via random variable transformation. However, a data-driven model like a neural network is trained with the pooled training data, which is a mixture of noise distributions. Therefore, a realistic reference aleatoric uncertainty from a neural network shall also be calculated from the pooled distribution of the training data. This is explained in Fig. 3 using a one-dimensional model $\\hat{B} = f(H)$. With a clearly defined physical model, the noise at $H_0$ shown as the black Gaussian curve below the x-axis transforms to the black Gaussian curve along the y-axis through the equation. In contrast, for a data-driven model trained from a wide range of data shown as a combined distribution of black and green Gaussians below the x-axis, the output uncertainty is also a mixture of all the distributions. The realistic aleatoric uncertainty shall be estimated from the pooled output distribution.\nWe employed the approach of Monte Carlo simulation by dense sampling on the input data noise and estimating the variance of the noisy output of the equation. The noise was set to be Gaussian with zero mean at each input point of tree diameter and height, and variance depending on the SNR. 20,000\u00b2 noisy input points were simulated for each SNR setting. The aleatoric uncertainty was estimated by calculating the variance of the equation output around its true value using 800 neighboring points. Although the estimate using 800 points is already highly accurate, a smoothing postprocessing step by fitting a parametric curve between the estimated uncertainty and inputs was followed to further increase the precision of the reference aleatoric uncertainty."}, {"title": "C. Demonstration of the dataset", "content": "The dataset is demonstrated by evaluating two UQ methods [33], [37]. We employed a 4-layer fully-connection neural network g(.) as the baseline model. The network has two outputs predicting the biomass and its corresponding uncertainty denoted as $\\hat{B}$ and $\\hat{\\sigma}_B$, respectively. The numbers of the neurons of the hidden layers are set as 16, 32, and 32 respectively. Two UQ methods [33], [37] were evaluated in the experiment. The former one predicts the aleatoric uncertainty with an additional network output and the log-likelihood loss (Eqn. 2). The latter one [37] uses a similar approach, but adds the variance of observations as an additional input and replaces all the layers with Assumed Density Filtering (ADF) [38] based layers. Because of the additional input of the observation variance, the latter UQ method is able to address the case when the noise in training and test sets belongs to different distributions. The epistemic uncertainty of both methods were estimated using Monte Carlo dropout. In essence, the two methods follow the same principle.\n$\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{2\\hat{\\sigma}_{B_i}^2} (B_i - \\hat{B_i})^2 + \\log \\hat{\\sigma}_{B_i}^2$ (2)\nDuring the training stage, 10% of the neurons at all except the output layer are randomly dropped out. This is to prevent overfitting. At the test stage, Monte Carlo dropout was performed by sampling 90% of the neurons of these layers T times to derive multiple outputs. The variance of the outputs indicates the epistemic part of the uncertainty. The aleatoric $\\hat{\\sigma}^A_B$ and epistemic $\\hat{\\sigma}^E_B$ uncertainty were calculated as follows.\n$\\hat{\\sigma}_A^2 = \\frac{1}{N} \\sum_i \\hat{\\sigma}_{B_i}^2$      $\\hat{\\sigma}_E^2 = \\frac{1}{N} \\sum_i \\hat{B_i} - \\frac{1}{T} (\\sum_j \\hat{B}_{ij})^2$ (3)"}, {"title": "III. SEGMENTATIONUQ: RENDERED DATASET FOR UQ IN IMAGE SEGMENTATION", "content": "We selected building segmentation as the example application in the segmentation dataset, as it is a common task in EO. Building segmentation is the pixel-wise classification of imagery to delineate building footprints from the surrounding environment, which holds significant importance in various applications. Several benchmark datasets have been introduced for this task [29]. These datasets are often corrupted by noise, i.e., the real-world nature of most of these datasets increases their propensity to introduce inherent image noise, such as sensor noise or quantization error. Moreover, as the annotations are often derived from manual processes or crowd-sourced techniques, the risk of label noise being included in the dataset increases.\nDespite several advancements in deep architectures of image segmentation, the sensor or label noise will always be propagated to the network's output. This introduces uncertainty in the prediction, compromising the reliability of the semantic segmentation models. Similar to the regression task, the community calls for an effective quantitative characterization of those uncertainties from reliable UQ methods. However, previous datasets lack the ground truth uncertainty vital to benchmarking existing UQ methods. Here we introduce a novel synthetic dataset, following the similar idea in the RegressionUQ dataset of employing Monte Carlo simulation of input noise and propagate to the model output.\nSince noise in aerial images can be due to various reasons, such as thermal noise and camera positioning error, we employed Blender, a comprehensive open-source 3D modeling and rendering software, high-quality 3D mesh models, and aerial images to simulate images with different conditions, such as varying camera viewing angles, illumination, and noise. These variations provide us with a set of noisy input images that can be used to calculate a reference uncertainty.\nWe made use of the 3D mesh models and LoD2 building models of Berlin, Germany to render the synthetic aerial images to the corresponding 2D building masks. 10,000 image patches of different areas were rendered from the baseline \u201cnoise-free\" setting. For each of the 10,000 patches, we simulated in total 3 types of noise, 4 different noise level, with 50 random samples for each noise configuration. This amounts to total 6 million different image patches in the whole dataset.\nSince the segmentation network outputs categorical predictions, it is not straightforward to generate a metric such as RMSE in the continuous domain. We then developed a novel strategy that enables quantitative comparison of UQ methods to the calculated reference uncertainty."}, {"title": "B. Dataset Generation", "content": "We first present the generation of the \u201cnoise-free\" baseline dataset. Subsequently, we introduce variations of the dataset created by adding noise of different distributions into the baseline dataset, in order to investigate the effectiveness of UQ methods in quantifying noise of different distributions. In particular, we elucidate the process of calculating the reference aleatoric uncertainty for benchmarking different UQ methods.\n1) Baseline dataset: We generated a synthetic dataset consisting of simulated 2D aerial images and building segmentation masks. The images were rendered from 3D mesh models of Berlin, Germany using the software Blender. This allows us to simulate images with different conditions, such as camera viewing angle and position, sun illumination, and noise. The building masks were extracted from precise LoD-2 building models of the same area. The 3D models were acquired from Business Location Center Berlin download portal [40].\n2) Dataset variations: To create a dataset with different aleatoric uncertainty, we simulated variants by adding two types of typical image noise, Gaussian and Poisson, as well as by changing the camera viewing angle. These image noise types are typical image uncertainties in segmentation problems in EO. For both categories, 4 different levels of noise were simulated.\n3) Reference uncertainty calculation: We employ a similar MC sampling approach to the regression problem to generate the output noise distribution. One important requirement to rule out epistemic uncertainty is a ground truth physical model, which is not fulfilled for a data-driven model like a neural network. Therefore, in our implementation, we trained a baseline U-Net model $M_e$ using large amounts of clean \"noise-free\" data. The learning rate and batch size was set to 0.001 and 16, respectively. We believe the epistemic uncertainty of such model is minimized, as the model is well trained and fitted to the specific training data.\nFor the regression problem, it is obvious to take the second order moment (variance) of the prediction as the uncertainty measure, as the prediction was modeled as a Gaussian random variable. An image segmentation network, however, outputs binary categorical values. Measuring uncertainty on a binary random variable requires modeling it as a Bernoulli distribution, which could result in loss of important information of the predicted softmax probability. Therefore, we choose the comparison to be on the distribution of the softmax probabilities. Ideally, one shall compare the predicted distribution of the softmax probabilities with a reference distribution using a metric such as the Kullback-Leibler (KL) divergence. In our binary segmentation case, the distribution can be modeled as Beta distribution, and estimated from multiple MC outputs. But instead of estimating a reference Beta distribution, we took a non-parametric approach of uncertainty measure. Inspired by [43], we use the Shannon entropy (Eqn. 5) of the distribution of the softmax probabilities as the uncertainty measure. We refer to this as the aleatoric entropy hereafter. This is a measure of the randomness in the distribution of the softmax probability. It is defined on a continuous domain, so that the predicted aleatoric entropy can be compared with the reference.\n$H = - \\sum_{x \\in X} p(x) \\log p(x)$. (5)\nIn order to estimate the reference aleatoric entropy given an input data distribution, we adapted the technique proposed in [43], which represents the jth logit $z_j$ before the softmax layer as a Gaussian distribution $z_j \\sim \\mathcal{N}(\\mu_j, \\sigma_j^2)$. The Gaussian representation encapsulates uncertainties that can arise from sources like noisy training data or ambiguous input features. The moments of the Gaussian distribution can be estimated by feeding a set of simulated noisy input data to a \"ground-truth\" model, and estimating the distribution of the logits layer.\nFor example, for images with 8\u00b0 camera view variation, we feed the 50 simulated samples to the network, and estimate the mean and variance of each logits. This probabilistic representation was employed to resample the logit many times, denoted as $z_i^1, z_i^2, ..., z_i^N$, for calculating the entropy. In our methodology, we extract a sufficient number of Monte Carlo samples, ensuring a wide coverage of the logits' distribution. Each of these samples pass through the standard softmax function, resulting in a set of probability distributions, defined by:\n$p_i^j = \\text{softmax}(z_i^j) j \\in [1, N]$, (6)\nwhere N is the number of Monte Carlo Samples. Since we have a binary segmentation problem, j is only from 1 to 2. The second element of the softmax probabilities is always 1 minus the first element. The entropy were calculated on each logit with the N probabilities. For a image patch, we obtain a single aleatoric entropy value by averaging the entropies of all the pixels and the two classes. The algorithm is illustrated in Tab. V."}, {"title": "C. Comparison of UQ methods", "content": "To demonstrate the usage of the reference uncertainty, we compare the predicted uncertainties from two UQ methods including Bayesian neural network (BNN) [33] and Test Time Augmentation (TTA) [44] with our reference aleatoric entropy. In order for the BNN methods to generate a distribution of the softmax probability, we let the network outputs the variance of the logits, and go through step 4 and 5 in Tab. V. Tab. VI shows those aleatoric entropies for three different noise types and five different noise levels. We use the R-square score (R\u00b2) and RMSE as the quality metrics to evaluate the general performance at all noise level. R\u00b2 is used to assess the correlation of the prediction and the reference at different noise levels, whereas the RMSE indicates the magnitude of the errors between the ground truth and the estimated aleatoric uncertainty. In statistical terms, a lower RMSE implies a higher degree of goodness-of-fit, indicating the superior performance of the UQ method in terms of magnitude alignment with the ground truth uncertainty. Tab. VII provides these metrics for both the UQ methods against the reference uncertainty estimate.\nOut of the two UQ methods, BNN outperforms TTA for all types of noise except Gaussian noise in terms of R\u00b2. This suggests that BNN might be more adept at capturing the underlying patterns associated with aleatoric uncertainty across diverse noise conditions. Similarly, for the RMSE, BNN also demonstrates a better approximation to the reference compared to TTA for all three types of image noise. Hence, BNN outperforms TTA in this aspect as well as a lower RMSE is indicative of a model's superior precision. Moreover, we discovered that sample size increment also critically enhances the UQ method's accuracy in predicting the inherent stochastic variations. This finding aligns with the conclusion in the RegressionUQ dataset."}, {"title": "IV. CLASSIFICATIONUQ: MULTIPLE LABEL VOTES FOR UQ IN SCENE CLASSIFICATION", "content": "Remote sensing offers a wide range of applications regarding image-level classification. In the context of land use land cover classification, a popular classification scheme, the Local Climate Zones (LCZs) scheme, was introduced in [45]. Initially created to study urban heat islands, the scheme was quickly adapted to downstream application fields in urban planning [46] or city mapping [47]. The scheme consists of 17 classes, which consists of 10 urban and 7 non-urban classes. Although initial studies have focused predominantly on urban areas such as cities and small villages, global LCZ maps have been generated recently [48] [49].\nWe choose LCZs classification to creat the ClassificationUQ dataset, as we had experience in creating a large-scale LCZs training dataset So2Sat LCZ42 [50]. The So2Sat LCZ42 dataset features labeled Sentinel-1 and Sentinel-2 image pairs from carefully selected 42 urban agglomerations plus 10 smaller areas across all the continents (except Antarctica). The dataset were created by identifying and labeling homogeneous areas as polygons in each city, from which Sentinel-1 and -2 image patches of 32 by 32 pixels corresponding to an area of 320 by 320 meters were cropped out."}, {"title": "B. Description of dataset", "content": "Unlike other EO datasets, a rigorous, quantitative evaluation of the labeling quality was performed in the So2Sat LCZ42 dataset. We selected 10 European cities and let a group of remote sensing experts cast 10 independent votes on each labeled polygon, to identify possible errors and assess human labeling accuracy. The \"human confusion matrix\u201d shows our human labels achieve 85% confidence. In this work, we publish this evaluation dataset, and describes a method to turn the multiple human votes into a distributional label, instead of the typical one-hot label. It can not only serve as a quality measure of the labeling, but also inspire new UQ algorithm for noisy labels.\nThe disagreement resulting from the labeling approach is attributed to the ambiguity of the classification scheme and the human uncertainty of the accompanying labeling process. The labeling uncertainty can be seen as part of the aleatoric uncertainty of the dataset. Although the measurement of the label uncertainty could be sharpened by including more remote sensing experts, the label uncertainty stays irreducible as it displays the ambiguity of the classes and the labeling process related to the dataset. This irreducibility motivates the differentiation of the human label uncertainty from the often discussed label noise. With the aid of crowd sourcing experiments or label correction algorithms, the amount of label noise is meant to be lowered, whereas the human label uncertainty should be explicitly targeted and taken into account. The following section demonstrates the design of a new classification algorithm the using this dataset."}, {"title": "C. Learning with Human Label Uncertainty", "content": "We now briefly present the approach of [22], which sets up a simple yet effective framework for deep learning with human label uncertainty. Typically, when having multiple votes to an image, one uses majority vote, and uses one-hot encoding as the training label. This method make use of the multiple vote to create a distributional encoding, instead of one-hot, as the training label.\nTypical one-hot encoding of LCZ is a vector of 17*1, with only one element being 1, and rest being 0, e.g. y = [1,0,0,..., 0] for class 1. With multiple votes of a image exist, we can form the distributional label $y_{distr} = \\frac{Y}{M}$, where Y is the vote counts of each of the 17 classes, and M is the total count of votes. Following that, the KL divergence was proposed as a loss function in the training, since the objective to be minimized is a parametric probability function."}, {"title": "V. CONCLUSION AND OUTLOOK", "content": "This article presents three datasets for benchmarking uncertainty estimates from machine learning models in EO. The three datasets covers three typical problems in EO, which are regression, segmentation and classification. Our datasets not only provide the reference labels like other EO datasets do, but also provides the reference aleatoric uncertainty of the predictions (for regression and segmentation datasets) or the label uncertainty (classification dataset). This allows users to benchmark the performance of different UQ methods.\nTo produce the reference aleatoric uncertainty we employ the approach of Monte Carlo simulation of the noisy input data, and propagate them through a reference model. In our work, the aleatoric uncertainty of the regression problem is measured by the variance of the prediction, whereas in the segmentation problem it is measured by the Shannon entropy of the distribution of the softmax probability. We developed a workflow to derive a reference distribution of the softmax probability given a input data distribution, based on which entropy can be computed. Although we only demonstrated on the segmentation dataset, the approach can be easily extended to a classification problem. Based on those reference uncertainties, we benchmarked popular UQ methods applied to our regression and segmentation dataset. For the classification dataset, we demonstrated a method of employing the provided label uncertainty in the training, in order to reduce the calibration error of the model.\nHowever, there are also limitations in our approach. One needs a ground truth model through which noisy inputs can be passed, so that the aleatoric uncertainty can be isolated. This may be feasible for problems with a well understood (i.e. perfect) physical model, yet impossible to achieve for problems like classification and segmentation. An inaccurate model causes the conflation of aleatoric and epistemic uncertainties, which was observed in both the experiments for the regression and segmentation datasets. Our mitigation strategy is to employ more training samples. In terms of the classification dataset, the limitation is the high labor cost for casting multiple votes to each image, which prevents the generation of a large dataset."}]}