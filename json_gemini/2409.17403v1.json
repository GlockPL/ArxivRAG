{"title": "Transient Adversarial 3D Projection Attacks on Object Detection in Autonomous Driving", "authors": ["Ce Zhou", "Qiben Yan", "Sijia Liu"], "abstract": "Object detection is a crucial task in autonomous driving. While existing research has proposed various attacks on object detection, such as those using adversarial patches or stickers, the exploration of projection attacks on 3D surfaces remains largely unexplored. Compared to adversarial patches or stickers, which have fixed adversarial patterns, projection attacks allow for transient modifications to these patterns, enabling a more flexible attack. In this paper, we introduce an adversarial 3D projection attack specifically targeting object detection in autonomous driving scenarios. We frame the attack formulation as an optimization problem, utilizing a combination of color mapping and geometric transformation models. Our results demonstrate the effectiveness of the proposed attack in deceiving YOLOv3 and Mask R-CNN in physical settings. Evaluations conducted in an indoor environment show an attack success rate of up to 100% under low ambient light conditions, highlighting the potential damage of our attack in real-world driving scenarios.", "sections": [{"title": "1 Introduction", "content": "Object detection is a crucial task of autonomous driving systems, playing a pivotal role in ensuring the safety of human life. Accurate and reliable object detection enables autonomous vehicles (AVs) to perceive and respond to their environment, and to recognize and track objects such as pedestrians, vehicles, and obstacles. This capability is essential for making informed decisions and taking appropriate actions to navigate through complex and dynamic traffic scenarios.\nHowever, it is a well-recognized challenge that Deep Neural Networks (DNNs) based object detectors are vulnerable to adversarial perturbations. Previous research has designed various adversarial examples (AEs) within the digital domain to attack object detection models. To ensure physical realizability, existing studies create adversarial patches or stickers [6,23,22] as AEs that take real-world"}, {"title": "2 Background", "content": "We first introduce the background knowledge of projector technology and object detection. Then, we present existing literature related to physically-realizable AEs."}, {"title": "2.1 LCD Projector Technology", "content": "Liquid Crystal Display (LCD) projectors, widely employed in business presenta- tions, classrooms, and home entertainment, utilize liquid crystal technology to project images. The fundamental concept involves splitting white light emitted by a lamp into its red, green, and blue components through dichroic mirrors. These mirrors reflect specific wavelengths while allowing others to pass through. Consequently, the white light transforms into the primary colors of red, green, and blue, forming the basis for deriving all other colors [28].\nLumens of the projector measure the brightness of a projector, indicating the total amount of light it emits. A higher lumen rating in LCD projectors enhances image visibility, especially in well-lit surroundings. Indoor projectors typically fall within the 2,000 to 3,000 lumens range for emitted light, whereas some outdoor projectors can achieve super-high lumens.\nThe throwing ratio of an LCD projector is defined as the distance between the projector and the screen relative to the width of the projected image. Lux is the unit for one lumen per square meter. Therefore, for a determined projecting distance, the throwing ratio influences the size and brightness of the projected image.\nWhen we project a 2D image onto a 3D surface using a projector, the ap- pearance of the image can be affected by the shape and texture of the surface. On smooth surfaces, the image is likely to be more uniform. However, any ir- regularities in the surface might cause slight distortions or uneven brightness. Textured surfaces can cause the projected image to look uneven, as the texture may interfere with the clarity of the image. If the surface is flat, the projected image will appear as it does on a flat screen, whereas, on curved 3D surfaces, the image may appear distorted, especially towards the edges. The amount of distortion depends on the degree and nature of the curvature. In a real-world attack scenario, both vehicles and road objects, such as traffic cones, have 3D"}, {"title": "2.2 Object Detection", "content": "Object detection, a computer technology within the domain of computer vision and image processing, is centered on identifying instances of semantic objects be- longing to specific classes, such as pedestrians and cars, in images and videos [27]. In this paper, we focus on Mask-RCNN [21] and Yolov3 [20], as they are repre- sentative object detectors widely used in computer vision research.\nFaster-RCNN is an evolution of the initial R-CNN object detector network [8]. Utilizing a two-stage detection method, the first stage is to generate region pro- posals, and the second stage is to predict labels for the proposals. Additionally, Mask-RCNN [11] expands Faster-RCNN to incorporate instance segmentation, providing precise pixel-level masks alongside accurate object detection, making it ideal for applications requiring detailed object boundaries. Its flexible architec- ture and strong performance on benchmark datasets make it a versatile choice.\nYolo object detectors are designed to detect objects in real-time, making them highly suitable for various applications [13]. Yolov3 is a one-stage detec- tor utilizing a single convolutional neural network (CNN), which incorporates a backbone network to compute feature maps for each square-grid cell in the input image. It employs three different grid sizes to enhance the accuracy of detecting smaller objects. Its unified architecture simplifies implementation, and its ability to perform multi-scale detection ensures robustness across various ob- ject sizes. Both Yolov3 and Mask-RCNN employ non-maximum suppression in post-processing to eliminate highly overlapped redundant boxes."}, {"title": "2.3 Physical Adversarial Examples", "content": "Athalye et al. [2] introduce the EOT framework, focusing on crafting adversarial perturbations resilient to random linear transformations. Although this approach uses a 3D printer to print out the 3D object with the AE, which can successfully deceive the image classifiers, it is not flexible in the context of dynamic driving scenarios.\nOther studies [6,23,30] design robust physical perturbations, like stickers or patches, for traffic signs. These perturbations are proven resilient to changes when reproduced in the physical world, such as alterations in distance and view- ing angle. Sharif et al. [22] showcase the feasibility of physical AEs for face recognition using colored eye-glass frames and enhance the perturbation real- izability in the presence of input noise. Xu et al. [29,7] design a patch on a non-rigid object (i.e., T-shirt) to make the person undetected in the object de- tector. However, such patch-based attacks also lack the flexibility in a complex autonomous driving scenario.\nPatches have drawbacks that can be mitigated by the projectors, particularly due to the short-lived and dynamic nature of projections. This allows attackers"}, {"title": "3 Threat Model", "content": "Our proposed attack targets an autonomous driving scenario, in which the decision-making relies exclusively on camera sensors for the AV, such as Tesla vehicles with Full Self-Driving features and Active Safety Features [26]. The AV utilizes these cameras to identify and monitor 3D objects within the driving scene, including other moving vehicles.\nAttack Goal. The adversaries aim to cause a traffic accident by projecting a transient adversarial patch onto a 3D object, making it undetectable by a DNN-based object detector model that processes the camera feeds within the AV. For instance, the lack of detection may cause the surrounding vehicles to collide with the victim AV without engaging the brakes.\nAttacker's Capabilities. Our attack is a white-box, hiding attack. We con- sider a scenario where an attacker seeks to promptly generate and project an attack patch onto an existing real-world 3D object to hide it from an object de- tection system employing a DNN model. During this timeframe, every captured frame of the scene is anticipated to yield a deceptive object detection outcome. We assume the attacker possesses comprehensive knowledge of the target DNN, including the model parameters and architecture, while remaining unaware of the technical specifications of the cameras. Additionally, the attacker is assumed to have physical proximity to the target 3D objects, such as other vehicles. This proximity enables the attacker to capture images of the target 3D objects for adversarial patch training and facilitates the projection of the attack patch into the real world. We also assume that the attacker has the capability to purchase or locate an identical object as a target for the projection attack, thereby gath- ering all the required data for the attack preparation (i.e., adversarial patch generation)."}, {"title": "4 Attack Design", "content": "We first present an overview of the attack pipeline, and then we illustrate the de- sign details regarding the color mapping model, geometric transformation model, adversarial patch generation, and training data enhancement."}, {"title": "4.1 Attack Overview", "content": "Fig. 2 shows the adversarial patch generation pipeline. The goal is to optimize the adversarial patch using a geometric transformation model and a color mapping model to minimize the target object detection score on a specific DNN-based object detector. The adversarial patch undergoes geometric transformation, and then the color mapping model is applied to simulate the patch's appearance on the target object. The geometric transformation model simulates the distortion on the 3D object surface, applying the transformation on the patch based on the 3D structure of the target object. The color mapping model simulates the pro- jected color on the target surface. The transformed patch and the benign target object are combined through the color projection model to generate the patched final object. We then perform training data enhancement to place the patched target object in different backgrounds to enhance robustness in real-world driv- ing scenarios. Finally, the optimization process is conducted iteratively across the enhanced training dataset to generate the adversarial patch (for projection) by minimizing the loss function."}, {"title": "4.2 Color Mapping Model", "content": "To create physically-realizable perturbations, prior studies [22,6,23] typically leverage the non-printability score (NPS) to characterize printable colors by printers. In our attack scenarios, projecting different colors onto the target 3D object surface yields results influenced by multiple factors, such as projection strength, ambient light, projection surface, etc. Some previous investigations on the projection-based physical attacks [16,9] propose color mapping models to simulate the color overlays. The attainable range of colors is significantly re- duced in comparison to the spectrum accessible to printed patches, primarily because of the color and materials of the target surface.\nTherefore, we formulate our color mapping model $P_c$ as follows:\n$P_c(\u0398_p, S, P) = O,$\nwhere $\u0398_p$ is the model parameters, $S$ is the 3D target surface, $P$ is the projected image, and $O$ is the color overlays captured by the camera on the target 3D surface.\nSubsequently, we collect projected data to train the color mapping model, with the goal of approximating the resulting output color for specific projected images and target 3D surfaces. The fundamental approach involves projecting various colors with RGB values ranging from 0 to 255 onto the target object surface, capturing images each time to document the outcomes. The distinct color mappings are denoted as $(S, P_i) \u21d2 O_i$, where i represents each color index.\nTo train the color mapping model, we utilize triples $(S, P_i, O_i)$ to fit a neural network consisting of two hidden layers with ReLU activation. Then, Eq. (1) is transformed into an optimization problem with the following loss function:\n$Loss_{p_c} = argmin_{\u0398_\u03c1} \u2211_i ||P_c(S, P_i) \u2013 O_i||_1,$\nwhere $||.||_1$ denotes the L1 norm. We optimize the network parameters $\u0398_p$ using gradient descent and the Adam optimizer to derive the color mapping model."}, {"title": "4.3 Geometric Transformation Model", "content": "The EOT process [2] involves using images with various transformations, en- compassing scaling, translation, rotation, brightness, additive Gaussian noise, etc. This collection aids in creating a single perturbation effective in deceiving the target neural network across diverse views. However, this approach may not adequately capture the projected image warping on a 3D object surface, partic- ularly when viewed from different angles. For instance, in Fig. 3, the projected patch, i.e., the color board, on the 3D surface is unevenly distorted on the car's surface. Thus, it is imperative to consider this viewing angle aspect when mod- eling the transformation.\nDifferent viewing angles or relative movements between the projected 3D surface and the projector can result in varying distortions in each video frame. To address this, we incorporate TPS [3] to model the projection distortion on the 3D surface. TPS has been widely employed as a non-rigid transformation model in prior studies [12,29,7]. TPS transformation is a mathematical model used for non-linear spatial transformations, often applied in image warping and deformation. It is particularly useful in morphing one image into another while preserving local structures. TPS transformation is defined as a combination of affine, rigid, and non-rigid components. Our exploration will reveal that the non-rigid warping aspect of TPS proves to be an effective means of modeling projection distortion on 3D surfaces in learning adversarial patterns.\nTPS involves learning a parametric deformation mapping that describes the displacement of each pixel from an original image $x$ to a target image $z$ using a set of control points with predefined positions. Suppose that the control points in the source image are $(x_i, y_i)$ and their corresponding positions in the target image are $(z_i, w_i)$. The TPS transformation $P_g$ is defined as:\n$P_g(x,y) = f(x,y) + \u2211_{i=1}^N W_i\u00b7 \u03c6(r_i),$\nwhere $f(x, y)$ is an affine function, and $\u03c6(r_i)$ is a radial basis function defined as $\u03c6(r) = r^2\u00b7log(r)$. The Euclidean distance $r_i$ between the point $(x, y)$ and the control point $(x_i, y_i)$ is given by:\n$r_i = \\sqrt{(x \u2212 x_i)^2 + (y \u2013 y_i)^2}.$\nWith the determined transformation parameters, it also allows the reverse transformation from the target image $z$ back to the original image $x$. The reverse transformation is given by:\n$P_g^{-1}(z) = \u2211_{i=1}^N W_i\u00b7 \u03c6(r_i).$\nPlease note that Eq. (3) does not explicitly define $z_i$. Instead, it uses the notation $w_i\u00b7 \u03c6(r_i)$ to represent the non-rigid component of the transformation. $z_i$ is implicitly part of the control points $(z_i, w_i)$ in the target image but does not appear directly in the equation. In other words, $\u2211_{i=1}^N W_i\u00b7 \u03c6(r_i)$ already incorporates the influence of these control points in the transformation.\nImplementing TPS to design a 3D adversarial projection patch is challenging due to the difficulty in determining the control points on both the projected and captured target surface images. To tackle this challenge, we project a checker- board onto the target object and identify the intersection points between adja- cent grid regions as the control points by manually marking them. We then scale the projected image and mark the control points using the same procedure."}, {"title": "4.4 Adversarial Patch Generation", "content": "Our approach for generating the adversarial projection pattern involves combin- ing the color projection model and the geometric transformation model with the target network. We employ gradient descent along both dimensions to optimize the projected image. Our objective function is defined as:\n$argmin_\u03b4J(f_\u03b8(b + P(x, \u03b4))), s.t. 0 \u2264 \u03b4 \u2264 1,$\nwhere d is the projected image, x is the target object image, J is the detection loss, $f_\u03b8$ is the target object detector, and b is the input image background. P is the projection model which is defined as:\n$P(x, \u03b4) = (1 \u2212 M) \u00b7 x + P_c(\u041c \u00b7 x, \u03b4_\u0434),$\nwhere M is the mask of the patch on the target object image, and $\u03b4_\u0434$ is the patch on the target object image. Here, $M = P_g(S_\u03b4)$, and $\u03b4_\u0434 = P_g(\u03b4)$, respectively. $S_\u03b4$ denotes the shape of the projected image. We then rewrite Eq. (5) as:\n$P(x, \u03b4) = (1 \u2013 P_g(S_\u03b4)) \u00b7 x + P_c(P_g(S_\u03b4) \u00b7 x, P_g(S_\u03b4)).$\nWe predefine the projected patch shape. The projected patch goes through the geometric transformation model and color mapping model before it is applied to the input image. We mask the attack area on the object to ensure the perturba- tions only apply to the region of interest (ROI) on the 3D object surface.\nTo improve the practical feasibility of the projection, we impose a constant grid granularity of n \u00d7 n cells on the projection, similar to the procedure in [16]. This guarantees that each cell consists of pixels with identical colors, promoting uniform projections across various viewing distances of the target object. Ad- ditionally, we integrate the total variation in the loss function. This inclusion is intended to alleviate the effects of camera smoothing and/or blurring on the overall projection [17].\nTo simplify the flowing of gradients when backpropagating, we substitute d with a new variable w such that:\n$\u03b4 = \\frac{tanh u}{2}+0.5.$\nSince \u03b4 is bounded in [0,1], therefore, u is bounded in [-1,1], which leads to faster convergence in the optimization [16].\nNow, we can update our loss function Eq. (4) to constrain the amount of perturbations as:\n$argmin_\u03b4J(f_\u03b8(b + P(x, u))) + \u03bb||P(x, u) \u2013 x||_p + TV(u) s.t. - 1 < u \u2264 1,$\nwhere $\u03bb$ is a parameter utilized to regulate the significance of the p-norm $||.||_p$ and TV represents the total variation. The loss J is based on bounding boxes"}, {"title": "4.5 Training Data Enhancement", "content": "Generating AEs that effectively function in autonomous driving scenarios neces- sitates the consideration of various environmental conditions, such as different viewing angles, distances, rotation, and brightness. To craft a robust adversarial patch, the optimization process needs to incorporate different input transforma- tions. We adopt EOT to generate a set of training images synthetically. Our final loss is formulated as follows:\n$argmin_\u03b4x_{q~X}E_{b_i~B, m_j~M}J(f_\u03b8(b+m_j \u00b7 P(x_q, u))) + \u03bb||P(x_q, u) \u2013 x_q||_p + TV(u), s.t. - 1 < u \u2264 1.$\nwhere $P(x_q, \u03b4) = (1 \u2212 P_g(S_\u03b4)) \u00b7 x_q + P_c(P_g(S_\u03b4) \u00b7 x_q, P_g(S_\u03b4))$. X represents a collection of input images with various viewing angles, B is a set of background images, and M denotes a number of linear transformations to the target object with the patch. $P_\u0434$ denotes the geometric transformation model corresponding to the input object image $x_q$."}, {"title": "4.6 Overall Attack Process", "content": "We summarize the detailed attack process of our attack step by step as follows:\n\u2022 Step 1: The attackers determine the patch shape and obtain the parame- ters for the geometric transformation model. First, the attackers design a patch shape, such as a rectangle, to be used as input to the projector. They use a color board with the same shape as the patch to project it onto the surface of the target 3D object. They collect images of the object both with and without the projection. Finally, they select the control points for the TPS and derive the parameters for the geometric transformation model based on these control points.\n\u2022 Step 2: The attackers collect data for the color projection model and train the model. They project different colors onto the target object and capture images of the object with these projections. This data is then fed into the color projection model for training.\n\u2022 Step 3: The attackers generate the patch using the enhanced training dataset. The transformed adversarial patch and collected benign images are pro- cessed through the color mapping model and applied to the target objects. They then enhance the training dataset by incorporating EOT, placing the target objects with the adversarial patches in different backgrounds."}, {"title": "5 Evaluation", "content": "To evaluate the proposed attack, we conduct experiments on 3D projection at- tacks in the physical world. We investigate our attack under different environ- ment settings to verify its feasibility and effectiveness."}, {"title": "5.1 Experimental Setup", "content": "Attack Devices. Fig. 4 illustrates the experimental setup. The target object is a 1/10 scale RC car [1]. For the projection, we use a PowerLite 1771W WXGA 3LCD Projector [5], an indoor projector priced around $740, which offers a maxi- mum brightness of 3,000 lumens and a maximum resolution of 1, 280 \u00d7 800 pixels. The projector has a throw ratio range of 1.04-1.26. The initial experiments were conducted in a lecture room, with the projector positioned 1.5 to 2.5 meters away from the target object. Given the 1/10 scale of the RC car, this setup simulates an attack distance of 15 to 25 meters in a real-world driving scenario.\nTo capture images and videos of the target object, we use the default camera on an iPhone 12 Pro Max. The iPhone is mounted on a phone slider [24] to capture videos from viewing angles ranging from -20\u00b0 to 20\u00b0. These videos are recorded at various distances and angles while the projection is active, under ambient light conditions of 100 lux, 200 lux, and 500 lux.\nAdditionally, the data for the color mapping model are collected using the built-in webcam on an Alienware m15 R3 laptop [25]. The data for the geometric transformation model is collected by iPhone. The evaluation of the attack is conducted on the images and videos captured with the iPhone.\nTarget Attack Models. In our experiments, we evaluate our attack on YOLOv3 and Mask R-CNN object detectors. For YOLOv3, we utilize the Darknet-53 backbone [20]. For Mask R-CNN, we use ResNet-101 as a backbone along with a feature pyramid network for the region proposals [14]. Both object detectors produce a set of bounding boxes with associated confidence scores for each out- put class. We establish the detection threshold at 0.6, meaning that an object is detected as a \"car\" if the confidence score is higher than 0.6.\nEvaluation Metrics. We feed each frame from the videos into the target at- tack models and count the instances where a \"car\" is detected at the output."}, {"title": "5.2 Visualization of the Geometric Transformation", "content": "Fig. 5 illustrates an example of geometric transformation using TPS. Figs. 5(a)(d) are used to get source and target control points for TPS, respectively. We can then build up the geometric transformation model and obtain its correspond- ing parameters. Figs. 5(b)(c) are the simulated projected patch after geometric transformation, and the simulated projected patch on the vehicle. It can be seen that the simulated projected patch on the vehicle (Fig. 5(c)) and the data col- lected in real life (Fig. 5(d)) match very well. We utilize the parameters in the geometric transformation model and the color mapping model to convert the adversarial patch in the projector (Fig. 5(e)) to the adversarial patch on the vehicle (Fig. 5(f)). We add the transformed adversarial patch (Fig. 5(f)) onto the benign object image (Fig. 5(g)) to generate the final projected patch on the vehicle, which becomes the input to the object detector (Fig. 5(h)).\nWe also show visualization examples of attacks on Yolov3 and Mask R-CNN in Fig. 6. The left column on both Fig. 6(a) and Fig. 6(b) is the well-trained projected attack patch, the simulated patch after geometric transformation, and the simulated patch on the vehicle from top to bottom. The right columns are"}, {"title": "5.3 Attack Performance under Different Settings", "content": "We evaluate the 3D projection attack under three ambient light conditions with varying distances and angles for Yolov3 as shown in Fig. 7. The first row shows the baseline results of Yolov3. When there is no attack present, OMDR is usually very close to 0. Only when the ambient light is 500 lux and the viewing angle is between -20\u00b0 to 7.5\u00b0 at the attack distance of 2 meters, OMDR is relatively higher. The results might be attributed to the fact that the back view of the vehicle becomes harder to detect in Yolov3 under strong ambient light.\nFor the scenarios with the attacks (Figs. 7(d)(e)(f)), it shows that OMDR significantly increases. With lower ambient light, the attack is usually more suc- cessful. This is because, as ambient light increases, the range of achievable colors diminishes due to the reduced impact of the projector-emitted light on the ve- hicle's appearance. Besides, when the attack distance is between 1.5 to 2 meters and the attack angle is between -7.5\u00b0 to 7.5\u00b0, the average OMDR is around 96%. Notably, when the ambient light is 100lux, we can achieve 100% OMDR in this attack setting. A video demo of this attack can be viewed through the link: https://youtu.be/8RbDpAAmsjs.\nMoreover, with the attack angle between -20\u00b0 to 7.5\u00b0, it is usually easy to make the vehicle disappear in the detector. It is because that when the attack angle is between 7.5\u00b0 to 20\u00b0, part of the attack patch on the backside of the vehicle has less portion of the view as shown in Fig. 3(c). On the other hand, the attack patch has a better view in the rest of the viewing angles in our attack setting (Fig. 3(b)).\nWe also examine the performance of our attack on different object detec- tors under varying ambient light conditions at an attack distance of 1.5 meters. The average OMDR results for YOLOv3 and Mask R-CNN are summarized in"}, {"title": "6 Discussion", "content": "We mainly discuss the feasibility and practicality of the 3D projection attack.\nAttack Feasibility and Practicality. Unlike patch attacks, which involve placing physical stickers or patches on objects, projection attacks can be tempo- rary. The projections can be turned on and off quickly, making them harder to detect and track over time. Once the projection is turned off, there are no phys- ical traces left behind, unlike patches or stickers that can be discovered upon inspection.\nProjection attacks can change patterns dynamically, adapting to different conditions and object surfaces. This flexibility makes it more challenging for"}, {"title": "7 Conclusion", "content": "We propose a transient adversarial 3D projection attack targeting object detec- tion in autonomous driving scenarios, which projects a well-crafted adversarial patch onto a 3D surface. The 3D projection attack is formulated as an optimiza- tion problem, combining a color mapping model and a geometric transformation model. We enhance the robustness of our attack by considering various environ- mental factors. We conduct experiments to evaluate the proposed attack against YOLOv3 and Mask R-CNN object detectors in physical attack scenarios. Our evaluation results show an attack success rate of up to 100% under low ambi- ent light conditions. This research underscores the need for defense strategies to mitigate transient projection attacks on AI-driven autonomous vehicles. Fu- ture work should focus on creating adaptive and resilient countermeasures that can detect and neutralize such attacks in real time, thereby safeguarding both passengers and pedestrians in diverse driving conditions."}]}