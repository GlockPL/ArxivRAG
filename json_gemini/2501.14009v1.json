{"title": "Scalable and Explainable Verification of Image-based Neural Network Controllers for Autonomous Vehicles", "authors": ["Aditya Parameshwaran", "Yue Wang"], "abstract": "Existing formal verification methods for image-based neural network controllers in autonomous vehicles often struggle with high-dimensional inputs, computational inefficiency, and a lack of explainability. These challenges make it difficult to ensure safety and reliability, as processing high-dimensional image data is computationally intensive and neural networks are typically treated as black boxes. To address these issues, we propose SEVIN (Scalable and Explainable Verification of Image-Based Neural Network Controllers), a framework that leverages a Variational Autoencoders (VAE) to encode high-dimensional images into a lower-dimensional, explainable latent space. By annotating latent variables with corresponding control actions, we generate convex polytopes that serve as structured input spaces for verification, significantly reducing computational complexity and enhancing scalability. Integrating the VAE's decoder with the neural network controller allows for formal and robustness verification using these explainable polytopes. Our approach also incorporates robustness verification under real-world perturbations by augmenting the dataset and retraining the VAE to capture environmental variations. Experimental results demonstrate that SEVIN achieves efficient and scalable verification while providing explainable insights into controller behavior, bridging the gap between formal verification techniques and practical applications in safety-critical systems.", "sections": [{"title": "1 INTRODUCTION", "content": "Ensuring the safety and reliability of image-based neural network controllers in autonomous vehicles (AVs) is paramount. These controllers process high-dimensional inputs, such as images from front cameras, to make real-time control decisions. However, existing formal verification methods [10, 16, 28] face significant challenges due to the high dimensionality and complexity of image inputs, leading to computational inefficiency and scalability issues [5, 27]. Moreover, these methods often treat neural networks as black boxes, offering limited explainability and making it difficult to understand how specific inputs influence outputs-an essential aspect for safety-critical applications like AVs.\nRecent efforts have employed abstraction-based methods [10, 28] and reachability analysis [25] to approximate neural network behaviors. Specification languages grounded in temporal logic [22, 32] and Satisfiability Modulo Theory (SMT) solvers [9] have been used to formalize and verify properties. However, these approaches often struggle with scalability and explainability when applied to high-dimensional input spaces inherent to image-based controllers.\nFurthermore, robustness verification under real-world input perturbations remains challenging. Modeling and analyzing such variations efficiently is difficult due to the complexity of image data and environmental factors affecting AVs. Consequently, current approaches lack methods that:\n\u2022 Reduce Computational Complexity: Effectively handle the high dimensionality of image inputs without compromising verification thoroughness.\n\u2022 Enhance Interpretability: Provide insights into how input features influence control actions, facilitating better understanding and trust.\n\u2022 Improve Scalability: Scale to larger datasets and more complex controllers, especially when considering robustness against real-world perturbations.\nTo address these limitations, we propose SEVIN (Scalable and Explainable Verification of Image-Based Neural Network Controllers), a novel approach that leverages unsupervised learning with a Variational Autoencoder (VAE) [19] to learn a structured latent representation of the controller's input space. By encoding high-dimensional image data into a lower-dimensional, explainable latent space, we significantly reduce the computational complexity of the verification process, making it more scalable.\nOur method involves training a VAE on a dataset of image-action pairs collected from a driving simulator. The latent space is partitioned into convex polytopes corresponding to different control"}, {"title": "1.1 Summary of Contributions", "content": "(1) An explainable latent space is developed for a neural network controller dataset by employing a Gaussian Mixture-VAE model. The encoded variables are annotated according to the control actions correlated with their high-dimensional inputs, enabling the derivation of convex polytopes as defined input spaces for the verification process.\n(2) A streamlined and scalable framework is then constructed to integrate the VAE's decoder network with the neural network controller, facilitating formal and robustness verification of the controller by utilizing the explainable convex polytopes as structured input spaces.\n(3) Finally, symbolic specifications are synthesized to encapsulate the safety and performance properties of two image-based neural network controllers. Leveraging these specifications in conjunction with the \u03b1-\u03b2-CROWN neural network verification tool [2], formal and robustness verification of the controllers is effectively conducted."}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 Variational Autoencoder (VAE)", "content": "VAEs are generative models that compress input data (x) into a latent space and then reconstruct the input from this latent representation (x) [19, 24]. A VAE V(x), consists of an encoder E(x) and a decoder D(z), where z is the latent variable capturing the compressed representation of the input data.\nIn variational inference, the true posterior distribution p(z|x) is often intractable to compute directly and hence an approximate posterior q(z|x) is introduced[19]. The encoder maps the input data to a latent distribution q4 (z|x), parameterized by 6, while the decoder reconstructs the input data from the latent variable using pe (xz), parameterized by 0. Instead of directly calculating for the intractable marginal likelihood p(x), VAEs maximize the Evidence Lower Bound (ELBO) to provide a tractable lower bound to log p(x) [19]:\nELBO = Ez~qq (z|x) [log pe (x | z)] \u2013 DKL [q\u00a2(z | x) || p(z)] \nReconstruction Term\nKL Divergence Term"}, {"title": "2.2 Neural Network Verification", "content": "Neural network verification tools are designed to rigorously analyze and prove properties of neural networks, ensuring that they meet specified input-output requirements under varying conditions [20]. Consider an L-layer neural network representing the function F(x) for which the verification tools can determine the validity of the property as:\nx \u2208 X \u21d2 F(x) \u2208 A\nwhere X and A are the convex input and output sets, respectively [17]. The neural network verification ensures that for all inputs in a specified set X, the outputs of the neural network F(x) satisfy certain properties defined by a set A. The weights and biases for F(x) are represented as W(i) \u2208 Rdxd-1) and b(i) \u2208 Rd), where d) is the dimensionality for layer i \u2208 {1, ..., L} for the L-layered neural network. The neural network function F(x) = h(L) (x):\nh(i) (x) = W(i) \u0125(i-1) (x) + b(i),\n\u0125(i) (x) = \u03c3 (hi) (x)),\n\u0125(0) (x) = x\nwhere o denotes the activation function. When the ReLU activation function is used, the neural network verification problem (2) becomes a constrained optimization problem with the objective function as shown below [29],\nFmin = min F(x), Fmax = max F(x)\ns.t.\nXEX\nXEX\nFmin, Fmax \u2208 A\nwhere F(x) is defined as the set of piecewise-linear functions from Equation (3). Since the ReLU activation functions are piecewise linear, allowing the neural network to be represented as a combination of linear functions over different regions of the input space, the verification problem can be formulated as an optimization problem solvable by techniques such as Mixed-Integer Linear Programming (MILP) [29] and SMT solvers [15].\nFurthermore, robust formal verification is a specific aspect of neural network verification that focuses on the network's resilience to small perturbations in the input data [14]. The robustness verification problem can be formalized as:\n\u03c7\u03b5 \u0392(\u03a7\u03bf, \u03b4) \u21d2 F(x) \u2208 A\nwhere B(x0, 8) = {x|||x - xo|| \u2264 8} represents a norm-bounded perturbation around a nominal input x0, and 8 > 0 is the perturbation limit. Alternatively, robustness verification can also be formulated as a constrained optimization problem:\nFmin = min F(x), Fmax = max F(x)\n\u03a7\u03b5\u0392(\u03a7\u03bf,\u03b4)\n\u03a7\u03b5\u0392(\u03a7\u03bf,\u03b4)\ns.t. Fmin, Fmax \u2208 A"}, {"title": "2.3 Symbolic Specification Language", "content": "The symbolic specification language defines properties for neural network verification, integrating principles from Linear Temporal Logic (LTL) [22] to express dynamic, time-dependent behaviors essential for cyber-physical systems like AV.\nLTL formulas are defined recursively as:\n\u03a6 ::= true|a|\u03c61\u2228 \u03c62|\u00ac\u03c6| \u039f\u03c6|\u03c61 U \u03c62\nwhere\n\u2022 true denotes the Boolean constant True.\n\u2022 a is an atomic proposition, typically about network inputs or outputs.\n\u2022 V,,, and U represent disjunction, negation, next, and until operators.\nUsing the above LTL formulas, other operators like \"always\" (\u25a1\u03c6) and \"eventually\" (\u25ca\u03c6) can be defined \u25a1q = \u00ac\u25ca\u00ac\u03c6, \u25ca\u03c6 = true U\u03c6.\nThese temporal operators allow precise specification of properties over time, such as safety (q) and liveness (0) conditions. Specification methods based on LTL [22, 32], Signal Temporal Logic (STL) [1], Satisfiability Modulo Theories (SMT) [9], and other formal techniques provide the basis for rigorous neural network verification, enabling precise and reliable analysis of temporal behaviors in dynamic environments.\nEXAMPLE 1. Consider an image based neural network controller F(x) that is trained to predict steering action values (a). We expect that for all images in the subset of left-turn images Xleft, the controller should predict negative action values corresponding to turning left, i.e.,\nx \u2208 Xleft F(x) \u2208 Aleft\nFormal verification of the neural network F(x) thus corresponds to solving the following optimization problem:\nFmin = min F(x), Fmax = max F(x)\nxeXleft\nXEXleft\ns.t. Fmin, Fmax \u2208 Aleft\nWe can thus formally define the input specification to the neural network verification tool using the symbolic specification language operators as:\n\u03c6 := \u25a1( {Fmin, Fmax} \u2208 Aleft)\nIf the verification tool can show that the specification o stands true for the given input set Xleft, then the specification is satisfied (SAT), or else the specification is unsatisfied (UNSAT)."}, {"title": "3 PROBLEM FORMULATION", "content": "Ensuring the safety and reliability of image-based neural network controllers in AVs is paramount. Existing verification methods, however, face significant challenges due to the high dimensionality and complexity of image inputs, leading to computational inefficiency [5, 10, 27]. These methods often lack explainability, treating neural networks as black boxes with limited insight into how inputs"}, {"title": "3.1 Our Solution", "content": "We begin by collecting images and control actions from a driving simulator, forming a dataset of image-action pairs (x, a), where X = {x}N=1 consists of N front camera images and A = {aj}N=1 consists of the corresponding control actions. A VAE V (x) is trained to learn the latent representation Z of this dataset, encoding high-dimensional image data into a lower-dimensional latent space (see Section 4.1). This encoding reduces the computational complexity of the verification problem, making it more scalable.\nOnce the VAE is trained, we label the latent variables (z) based on their corresponding control actions (a). This labeling allows us to partition the latent space Z into convex polytopes Ci, such that C = Viel Ci, as elaborated in Section 4.3. Each polytope Ci corresponds to a specific control action set Ai, with the action space expressed as A = Uiel Ai. This partitioning generates an explainable input space for the formal verification process, enhancing the understanding of how inputs influence outputs.\nWe then split the trained VAE into encoder E(x) and decoder D(z) networks and concatenate the decoder with the controller network F(x). The combined network H(z) = F(D(z)) maps variables directly from the latent space to control actions. By operating in the latent space instead of the high-dimensional image space, we significantly reduce the input dimensionality and computational complexity of the verification problem, making the process more scalable and computationally efficient.\nFormal specifications q are defined using the symbolic specification language described in Section 2.3, capturing the safety and performance properties of the neural network controller F(x). The combined network H(z) and the specifications o are provided to a neural network verification tool, such as a-\u03b2-CROWN [2], which uses bound propagation and linear relaxation techniques to certify properties of neural networks. The verification tool checks whether H(z) satisfies the specifications o over the input convex polytopes in the latent space. The equivalence between verifying F(x) and H(z) is established in Theorem 1.\nTo address robustness verification under input perturbations common in real-world scenarios, we extend our approach by training the VAE on a dataset of both clean and augmented images (see Section 4.5). The augmented dataset X is generated by applying quantifiable perturbations-such as changes in brightness, rotations, translations, and motion blurring-to the original images (see Figure 4). The corresponding latent representation set Z is used by SEVIN to generate augmented latent space convex polytopes \u010ci for robustness analysis. The overall formal verification process aims to assess the neural network controller's performance under two distinct conditions:\nVanilla formal verification: for a clean input space Ci, drawn from the subset Xi \u2286 X, assumed to consist of unperturbed, front-camera-captured images."}, {"title": "4 SCALABLE AND EXPLAINABLE\nVERIFICATION OF IMAGE-BASED NEURAL\nNETWORKS (SEVIN)", "content": ""}, {"title": "4.1 Latent Representation Learning of the\nDataset", "content": "To develop a scalable and explainable framework for neural network controller verification, we first train a VAE on the dataset of images used in the neural network controller training process. The VAE is tasked with reconstructing front-camera images captured by an AV while learning latent representations that capture the underlying structure and variability in the data-such as different driving conditions, environments, and vehicle behaviors-in a compressed and informative form.\nLet V(x) : Rhxw \u2192 Rhxw represent a Gaussian Mixture Variational Autoencoder (GM-VAE) trained over a dataset of images X = {x}M CRhxw to learn a structured latent space representation z \u2208 Z C Rdz and reconstruct images x = V(x) \u2208 \u0176 cRhxw. Here, h \u00d7 w denotes the height and width of the images. We assume a Gaussian mixture prior over the latent variables z, defined as:\np(z) = \\sum_{k=1}^{K} Sk \u039d(2/\u03bc\u03ba, \u03a3\u03ba)\nwhere \u00b5k and Ek represent the mean and covariance matrix of the k-th Gaussian component in the latent space, and sk represents the mixture weight for each Gaussian, satisfying \u2211k=1 sk = 1. The"}, {"title": "4.2 Explainable Latent Space Encoding", "content": "The encoder E(x) learns a mapping from high-dimensional images x to low-dimensional latent representations z. Images with similar features-such as lane markings, traffic signs, or attributes like brightness and blur-are mapped close together in the latent space because the encoder learns to associate these common attributes with nearby regions. The continuity of the latent space enforced by the KL divergence ensures that similar inputs have similar latent representations. The decoder D(z) regenerates the front-camera images x from the latent variables z \u2208 Z.\nIn our approach, each image x is associated with a corresponding control action a, such as steering angle or linear velocity, which acts as a label for the image for training purposes. The control action is the action to be predicted by the neural network controller F(x) based on the image x. The dataset X collected from driving simulations is randomly split 70/30 for training and validating the VAE V(x) and the neural network controller F(x) respectively. The datasets are generated and labeled automatically during simulation, where the vehicle's control actions are recorded alongside the images captured. Further details on the data collection process are provided in Section 5.1. The proof for the Lemma 1 is provided in the Appendix A.1\nLEMMA 1. Let X = {x}M\u2081 be a dataset of images, and let A \u2282 Rm be a set of action values, where each image xi is associated with a control action a\u00a1 \u2208 A that the neural network controller F(x) should predict.\nUsing the encoder E(x), the images are mapped to latent variables z\u2081 = E(xi), resulting in the set of latent variables Z = {zi}M. The latent variables are assumed to follow the GM prior distribution described in (6). Then, for each action value a \u2208 A, the set of latent variables corresponding to a has positive probability under p(z). Specifically, the probability of sampling a latent variable z such that there exists a pair (z, a) in Z \u00d7 A is greater than zero:\nVa\u2208 A, p (\u2203z \u2208 Z s.t. (z, a) \u2208 Z \u00d7 A) > 0"}, {"title": "4.3 Latent Space Convex Polytope Formulation", "content": "For a collection of continuous action subsets {Ai}iel, where I is an index set and A = Uiel A\u00a1 covers the entire action space, we construct corresponding convex polytopes C\u00a1 C Rdz in the latent space. These polytopes approximate the regions associated with each action subset Ai. To generate Ci for a given action subset Ai, we perform Monte Carlo sampling of the latent variables z from the GM prior p(z), focusing on samples corresponding to Ai. Specifically, for each A\u00a1, we consider the set of images Xi C X such that each image x \u2208 X\u00a1 is associated with an action a \u2208 A\u00a1, i.e.,\nX\u2081 = {x \u2208 X | F(x) \u2208 A\u00a1}\nUsing the encoder E(x), we map the images x \u2208 Xi to their latent representations z, resulting in latent variables z \u2208 Zi, where Zi denotes the set corresponding to Ai. This establishes the correspondence between image-action pairs (x, a) and latent-action pairs (z, a). To construct Ci, we draw n independent samples of the latent variable z from the GM prior p(z), ensuring that each sample belongs to Zi corresponding to A\u00a1 and is within 2 standard deviations from the mean \u03bc\u2081\u2081 of p(z|z \u2208 Z\u012f). The convex polytope C\u012f is then defined as the convex hull of these sampled latent variables:\nC\u2081 = conv ({zj|zj = E(xj), xj \u2208 Xi, j = 1, ..., n})\nwhere conv() denotes the convex hull operation [11]. Constructing Ci in this manner provides an under-approximation of the latent space region corresponding to A\u00a1, as it is based on finite samples that are only within 2 standard deviations from the mean (\u00b5A\u2081). Increasing n improves the approximation and coverage. For thorough formal verification, it is often preferred to include potential variations and edge cases in the process as well. Hence, we enlarge the convex polytope Ci uniformly by applying a Minkowski sum [26] with a ball R(0, \u2208) centered at the origin with radius \u20ac > 0:\n\u010c\u2081 = C\u00a1 \u2295 R(0, \u20ac)\nwhere \u2295 denotes the Minkowski sum and the ball R(0, \u2208) is defined as:\nR(0, \u20ac) = {r \u2208 Rdz | ||r||2 \u2264 \u20ac}"}, {"title": "4.4 Formal Verification using Latent Space\nConvex Polytopes", "content": "The convex polytopes Ci defined in Section 4.3 facilitates the development of an explainable and scalable framework for verifying image-based neural network controllers. Consider a neural network controller F(x) trained on the same dataset X as the VAE from Section 4.1. Traditional formal verification processes for image-based neural networks involve solving optimization problems as expressed in Equation (4). However, the input space for such problems becomes significantly large, contingent on the dimensionality of the input image x \u2208 Rh\u00d7w. Moreover, the input space subset Xi used for verification is limited to a finite and discrete selection of images, which constrains both explainability and scalability.\nIn contrast, our approach leverages the convex polytope Ci to define an explainable and continuous input space in the latent domain. Specifically, Ci encompasses a continuous region of latent variables zi \u2208 Ci, where each z\u012f is associated with a control action a\u00a1 \u2208 A\u00a1."}, {"title": "4.5 Augmented Latent Spaces for Robustness\nVerification", "content": "In prior sections, we developed the SEVIN framework, which leverages latent representations to formally verify neural network controllers. These representations are derived from the dataset X, consisting solely of unperturbed images captured by the front camera of an AV. Consequently, both the reconstructed dataset X and the set of convex polytopes C correspond exclusively to clean data. In this section, we will utilize the SEVIN framework to also conduct robustness verification of the neural network controller."}, {"title": "4.6 Designing Symbolic Formal Specifications", "content": "The combined network H(z) undergoes verification against a set of SAFETY and PERFORMANCE specifications within both general formal verification and robustness verification frameworks. These specifications adhere to the Verification of Neural Network Library (VNN-LIB) standard, which is widely recognized for neural network verification benchmarks [12]. The VNN-LIB specification standard builds upon the Open Neural Network Exchange (ONNX) format for model description and the Satisfiability Modulo Theory Library (SMT-LIB) format for property specification, ensuring compatibility and interoperability across various verification tools and platforms. The VNN-LIB standard allows designers to specify bounds on each input and output parameter of the neural network under verification, providing a highly expressive framework for defining verification constraints. The specifications are meticulously crafted to align with the driving scenarios outlined in Section 5.1\nSAFETY specifications are designed to guarantee that the neural network controller does not produce unsafe action values within"}, {"title": "5 EXPERIMENTS", "content": "To test our proposed approach on an image based neural network controller, we choose an autonomous driving scenario where an AV drives itself around a track in a simulator. One of the goals of our experiments is to test and see if we can generate an explainable latent representation of the image dataset collected by the front camera images. Once we do that, we want to make sure that we can configure the convex polytopes in the latent space as inputs to the verification problem. Finally, we aim to evaluate the controller's performance by conducting both vanilla formal and robust formal verifications, and compare the performance metrics of our approach to a general image-based neural network robustness verification problem (see more in Section 5.3.5). The VAE's and neural network controllers are trained on 2x NVIDIA A100 GPU's and the formal verification is carried out on a NVIDIA RTX 3090 GPU with 24GB of VRAM."}, {"title": "5.1 Driving Scenarios", "content": "The driving simulator collects RGB images (x) from the AV's front-facing camera along with the steering control actions (a). The AV drives on a custom, single-lane track created in RoadRunner by MathWorks and simulated in the CARLA environment. The simulator's autopilot mode autonomously drives the vehicle, collecting control data, including the steering angle, as the control action"}, {"title": "5.2 Network Architectures", "content": "The GM-VAE used in our approach consists of an encoder and a decoder network, with convolutional and transposed convolutional layers, respectively, to process and reconstruct data. The encoder progressively increases channel sizes, while the decoder reduces them in reverse order. Each layer integrates batch normalization, ReLU activations, and dropout to prevent overfitting. The encoder begins with a linear layer, followed by three convolutional layers with increasing channel sizes: from 1 to 64, then 128, 256, and finally 512 channels. We employ K = 16 Gaussians in the mixture model to enhance the expressiveness of the latent representation. A Sigmoid activation function is applied at the output layer to ensure that the generated pixel values are within the range [0, 1]."}, {"title": "5.3 Results", "content": "We evaluated our proposed method using the neural network verification tool a \u2013 \u03b2 \u2013 CROWN [2], offering certified bounds on model outputs under specified perturbations. This tool is suitable for verifying the safety and reliability of neural network controllers in autonomous systems. To assess both vanilla formal and robust formal verification methods (Section 4.5), we employed two SAFETY specifications and three PERFORMANCE specifications as described in Section 5.2."}, {"title": "5.3.1 Image Augmentations", "content": "For the robust formal verifications, we applied different levels (\u03b41,2,3) of augmentations to the image dataset to generate X for training the VAEs. The types and quantifications of the image augmentations are as follows:\n\u2022 Brightness: Datasets were generated by randomly varying image brightness levels within specified ranges di:\n\u03b41: 80% to 120% of original brightness.\n\u03b42: 60% to 140% of original brightness.\n\u03b43: 50% to 150% of original brightness.\n\u2022 Vertical Motion Blur: Datasets were generated by varying the degree of vertical motion blur kernels within the ranges:\n\u03b41: Kernel sizes of 1 and 2 pixels.\n\u03b42: Kernel sizes of 3 and 4 pixels.\n\u03b43: Kernel sizes of 5 and 6 pixels."}, {"title": "5.3.2 Specifications", "content": "The SAFETY specifications used to verify the controller are defined as:\n\\p_{SAFETY}^{1} := \u2200z \u2208 C[-ve], H(z) \u2264 0.0,\n\\p_{SAFETY}^{2} := Vz \u2208 C[+ve], H(z) \u2265 0.0,\nwhere C[-ve] and C[+ve] denote the latent space regions corresponding to negative and positive control actions, respectively.\nThe PERFORMANCE specifications are defined as:\n\\p_{PERFORM}^{1} := \u2200z \u2208 C[-0.4,-0.1], -0.4 \u2264 H(z) \u2264 -0.1,\n\\p_{PERFORM}^{2} := Vz \u2208 C[ -0.1,0.1], -0.1 \u2264 H(z) \u2264 0.1,\n\\p_{PERFORM}^{3} := Vz \u2208 C[0.1,0.4], 0.1 \u2264 H(z) \u2264 0.4,\nwhere C[a,b] represents the latent space regions corresponding to control actions between a and b.\nThese specifications were used for both vanilla formal and robust formal verification methods. The verification results, along with the input formal specifications, applied image augmentations, and evaluated neural network controllers, are summarized in Table 3 and Table 1."}, {"title": "5.3.3 Vanilla Formal Verification", "content": "From the results, we can infer that both the vanilla and robust formal verification processes take < 1 second to conduct verification. This is due to the reduction in computational complexity of the processes as we introduce lower dimensional input spaces in the form of convex polytopes (C). Additionally, we also note that for the vanilla formal verification, the neural network controllers satisfy all of the specifications provided. This indicates that the controllers provide formal guarantees with"}, {"title": "5.3.4 Robust Formal Verification", "content": "For the robust formal verification process, we can notice specifications 3 and 45 to be unsatisfactory for the NvidiaNet architecture, for both levels and types of augmentations, \u03b4\u2081 and \u03b42. The NvidiaNet architecture thus seems to be more susceptible to image augmentations and fairs poorly during the robustness verification process. In contrast, the ResNet18 architecture tends to perform fairly better than NvidiaNet for the \u03b41 and \u03b42 levels of augmentations for both vertical motion blur and brightness variation. It starts providing unsatisfactory verification results once the 83 level of augmentations are applied to both types of image augmentations. This shows that the ResNet18 architecture is more robust than NvidiaNet in handling image perturbations for the case of autonomous driving scenarios."}, {"title": "5.3.5 Scalability Comparison", "content": "We conduct general robustness verification on the neural network controller F(x) using the \u03b1 -\u03b2-CROWN toolbox and compare these results with those obtained via the SEVIN framework. We employ the same set of SAFETY specifications as described in Equation (11), and we separate the brightness-augmented dataset X into two subsets, X[\u2212ve] and X[+ve], based on their corresponding negative and positive control action sets, A[-ve] and A[+ve]. By converting F(x) into the ONNX format and defining perturbation bounds for each dimension (i.e., pixel) of x, we leverage the neural network verification tool to perform robustness verification directly on the original high-dimensional image inputs. The upper and lower bounds are calculated for all images x \u2208 X[\u2212ve] and x \u2208 X[+ve]. The results are shown in Table 4. When comparing the results between Table 1 and 4, we observe that all the specifications are SAT for both methods. However, a significant difference lies in the time taken by the general method, which is almost ten times longer than that of the SEVIN method. This substantial difference is due to the fact that the input dimensionality of the verification problem using the SEVIN framework is approximately 600 times smaller than that of the general framework, making the problem computationally less complex and, consequently, more scalable to larger networks. Although for SEVIN, the combined network (H) to be verified has many more layers than the neural network controller (F)."}, {"title": "6 CONCLUSION", "content": "We provide a framework for scalable and explainable formal verification of image based neural network controllers (SEVIN). Our approach involves developing a trained latent space representation for the image dataset used by the neural network controller. By using control action values as labels, we classify the latent variables and generate convex polytopes as input spaces for the verification process. We concatenate the decoder network of the Variational Autoencoder (VAE) with the neural network controller, allowing us to directly map lower-dimensional latent variables to higher-dimensional control action values. Once the verification input space is made explainable, we construct specifications using symbolic languages such as Linear Temporal Logic (LTL). We then provide the neural network verification tool with the combined network and the specifications for verification. To enhance the formal robustness verification process, we generate an augmented dataset and retrain the VAEs to produce new latent representations. Finally, we test the SEVIN framework on two neural network controllers in an autonomous driving scenario, using two sets of specifications-SAFETY and PERFORMANCE-for both the standard formal verification and robustness verification processes. In the future, we aim to conduct reachability analysis using the SEVIN framework for neural network controllers with their specified plant model."}, {"title": "A PROOFS FOR LEMMAS AND THEOREMS", "content": ""}, {"title": "A.1 Proof of Lemma 1", "content": "PROOF. For each action value a \u2208 A, there exists at least one image x \u2208 X such that the associated control action is a. By applying the encoder to any such image x, we obtain the latent variable z = E(x). Thus, the pair (z, a) exists in Z X A.\nSince the latent variables z are generated from images x via the encoder E(x), and the latent space Z follows the GM prior p(z), this prior p(z) serves as the probability density function over Z. Therefore, p(z) assigns a positive probability to all latent variables in Z that correspond to images in X under the mapping E(x).\nFormally, for any a \u2208 A, we define the set of latent variables corresponding to a as:"}]}