{"title": "Eliciting In-context Retrieval and Reasoning for Long-context Large Language Models", "authors": ["Yifu Qiu", "Varun Embar", "Yizhe Zhang", "Navdeep Jaitly", "Shay B. Cohen", "Benjamin Han"], "abstract": "Recent advancements in long-context language models (LCLMs) promise to transform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With their expanded context windows, LCLMs can process entire knowledge bases and perform retrieval and reasoning directly a capability we define as In-Context Retrieval and Reasoning (ICR2). However, existing benchmarks like LOFT often overestimate LCLM performance by providing overly simplified contexts. To address this, we introduce ICR2, a benchmark that evaluates LCLMs in more realistic scenarios by including confounding passages retrieved with strong retrievers. We then propose three methods to enhance LCLM performance: (1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, which uses attention heads to filter and de-noise long contexts during decoding, and (3) joint retrieval head training alongside the generation head. Our evaluation of five well-known LCLMs on LOFT and ICR2 demonstrates significant gains with our best approach applied to Mistral-7B: +17 and +15 points by Exact Match on LOFT, and +13 and +2 points on ICR2, compared to vanilla RAG and supervised fine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasks despite being a much smaller model.", "sections": [{"title": "1 Introduction", "content": "The ability of large language models to process long contexts has significantly expanded their applicability across various domains, including book-level information retrieval (Ding et al., 2024; Jin et al., 2024), summarization (Kim et al., 2024; Saxena and Keller, 2024; Qiu et al., 2023), and question answering (Liu et al., 2024; Wang et al., 2024a). They also enable more complex tasks, such as agent trajectory modeling and planning (Zhao et al., 2024; Zhang et al., 2024), video captioning (Xue et al., 2024; Zhang et al., 2023), and text-to-video generation (Wang et al., 2024b; Lin et al., 2023). Recent advancements in long-context language models (LCLMs) hold particular promise for reshaping the Retrieval-Augmented Generation (RAG) paradigm (Lee et al., 2024; Li et al., 2024). With their expanded context windows, LCLMs reduce reliance on complex pipelines required by previous context-length limitations and simplify knowledge updates by allowing context modifications. For instance, LCLMs can accommodate entire knowledge bases within their context windows, effectively serving as working memory to be used for new queries.\nAchieving this goal requires LCLMs to effectively retrieve and reason within their \u201ccontextual knowledge base,\" a capability we define as In-Context Retrieval and Reasoning (ICR\u00b2). However, existing benchmarks often fail to accurately evaluate this capability. For example, Needle-in-a-Haystack (NIAH; Kamradt 2023) is a popular test to determine whether a model can retrieve a \"needle\" (a specific fact or statement) randomly inserted into a \"haystack\" (a corpus). Yet, the semantic discontinuity between the needle and the haystack can unintentionally reveal the needle's location, making the task overly simple. LOFT (Lee et al., 2024), the first large-scale benchmark for evaluating retrieval and reasoning within contextual knowledge bases, uses human-annotated relevant documents as the needle, and fills the haystack with randomly sampled documents from an external knowledge base. However, this random sampling results in a context with virtually no confounding information that is relevant but misleading, causing LOFT to significantly overestimate LCLM performance.\nTo bridge this evaluation gap, we introduce ICR2, a novel and challenging benchmark designed to assess LCLMs under more realistic conditions. ICR2 builds upon KILT (Petroni et al., 2021), a comprehensive knowledge base sourced from Wikipedia."}, {"title": "2 Related Work", "content": "Long-context large language models (LCLMs) have garnered significant attention for their ability to process extended sequences. Models like Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020) introduced sparse attention mechanisms to efficiently handle long context. Scaling efforts, exemplified by GPT-4 (Achiam et al., 2023), underscore the importance of expanding context windows for the long-context tasks. Additionally, approaches like Memorizing Transformers (Wu et al., 2022) have explored memory-augmented architectures. Data engineering methods (Fu et al., 2024; Xiong et al., 2024; Jin et al., 2024), expanding positional encoding (Ding et al., 2024), and the parameter-efficient fine-tuning (Chen et al., 2024) have also been proven to be effective.\nLCLMs' advanced capabilities hold the promise in reshaping RAG (Achiam et al. 2023; Jiang et al. 2023; Yang et al. 2024; Abdin et al. 2024). By replacing static knowledge bases with the contextual one inside LCLMs, this new paradigm simplifies the deployment by discarding intermediate components, e.g., re-rankers, while enabling the direct updates on LCLM's knowledge (Lee et al., 2024).\nUnlike the exisiting works, we systematically evaluate the use of LCLMs for scenarios where knowledge bases are directly placed as the context. Additionally, we demonstrate that with targeted enhancements, small-scale LCLMs can achieve performance comparable to state-of-the-art models."}, {"title": "3 Are LCLMs Competent for RAG?", "content": "3.1 LLMs Are Sensitive to the Confounders\nLOFT (Lee et al., 2024) introduces a Corpus-in-Context (CiC) approach for retrieval-augmented generation, integrating a large-scale external knowledge base directly into the LLM's context. However, LOFT operates under the assumption that the context is free of confounders. In practice, a corpus often contains confounders \u2013 documents related to the query but potentially leading to incorrect answers. For example, given the query \u201cWho is the 44th U.S. President?\u201d, the corpus might include documents about the other presidents, such as \u201cThe 45th U.S. President is Donald Trump\", which could mislead the LLM. Consequently, LOFT's design reduces the complexities of using real-world corpora for RAG, potentially overestimating the performance of LCLMs.\nWe show that LCLMs are indeed sensitive to the confounders missed in LOFT. We construct multiple test sets with varying confounding ratios  {0,25%, 50%, 75%, 100%}  and evaluate the LCLMs under zero-shot settings. The confounding ratio p denotes the proportion of confounding context that are selected by retrievers while filtering out the gold provenance, with the remaining, i.e., (100% \u2013 p), randomly sampled from an external knowledge base. At p = 0, all confounding passages in the contextual knowledge base are randomly sampled, equivalent to the setup in LOFT (Lee et al., 2024). Conversely, at p = 1, all confounders are selected by the retrievers.\nWe evaluate five LCLMs with a context length of at least 32K tokens: Phi-3-7B (Abdin et al., 2024), Qwen-2-1.5B/7B (Yang et al., 2024), Mistral-003-7B (Jiang et al., 2023), and GPT-4-Turbo (Achiam et al., 2023). Our findings in Figure 1, demonstrates that LCLM performance is highly sensitive to confounders, with performance generally degrading as the confounding ratio increases. This indicates that confounders in P\u00af obtained via retrievers, as in ICR2, pose greater challenges for the models\n3.2 ICR2 Benchmark\nTo address this limitation, we propose an alternative benchmark, ICR2, which leverages strong retrievers to identify and incorporate these confounders into the contextual corpus used in CiC, providing a more realistic and challenging evaluation framework. We choose KILT (Petroni et al., 2021), a comprehensive suite of benchmarks designed for knowledge-intensive NLP tasks, to be our external knowledge base (Figure 2). KILT covers tasks such as question answering (Kwiatkowski et al., 2019; Yang et al., 2018), fact verification (Thorne et al., 2018), and dialogue completion (Dinan et al., 2018), all paired with a single Wikipedia snapshot. Each KILT instance <q, a, P+> consists of a query q, the reference answer a, and its provenances P+ = {P1,...,Pm}, which is a set of relevant Wikipedia pages and specific locations within them that support the answer.\nBuilding on KILT, each ICR2 instance is rep-\""}, {"title": "4 Eliciting ICR\u00b2 for LLMs", "content": "4.1 Retrieve-then-generate Fine-tuning\nCompared to the standard supervised fine-tuning where a model is asked to generate a Direct Answer (DA) to a given query, our first proposal, retrieve-then-generate, is a two-step process: the model first retrieves relevant passages from context and then generates the final answer based on the context and retrieved passages, all in one decoding pass. Formally, we train an LCLM to optimize the objective,\np(y|q,c) = \u2211 p(y|q, c, zi)p(zi | q, c), (1)\nZEZ\nwhere q, y is the query and target, respectively, c is the contextual knowledge base, and Z is the collection of all relevant passages necessary for answering q. This objective can be easily integrated into the next-token prediction task trained with the maximum likelihood estimation, just by sequentially executing the retrieval and generation in a single forward pass. The overall loss is given as\nN\n1\nL =\n\u2211(log p(Zqi, ci) + log py*Z*, qi, ci)), (2)\ni=1\nwhere N is the number of training samples, and  Z= {1,2,,|z|} is the collection of all relevant passages for the query qi and contextual knowledge base ci.\nWe implement two variants of retrieve-then-generate fine-tuning, both using the special tokens <RETRIEVAL> and </RETRIEVAL> to delineate the retrieval step. In the first, Retrieve-Then-Answer (RTA), the model copies relevant passages verbatim during retrieval. In the second, Cite-Context-ID (CCI), the model generates only the IDs of the relevant passages.\n4.2 Retrieval Attention Probing\nOur second proposal, Retrieval Attention Probing (RAP), is an inference-time approach compatible with LCLMs without requiring re-training. Building on Wu et al.'s findings that specific attention heads are highly active during retrieval tasks (e.g., NIAH), RAP utilizes these retrieval-focused attention heads for context filtering before generating responses. For each attention head h and query q, we track the Top-M attention scores AM(q).\nCM (q), representing the M passages correspond-\n4.3 Joint Retrieval Head Training\nOur final proposal introduces a dedicated retrieval head to the LCLM model architecture. During inference, the model first uses the head to identify relevant passages, after which the generation head decodes a response conditioned on the retrieved content. During training, the retrieval and generation heads are jointly optimized using the Gumbel-TopK trick (Kool et al., 2019), which mitigates the non-differentiability of the retrieval process.\nFigure 4 illustrates the modified model architecture. The retrieval head generates a binary mask, M\u2208 {0,1}|C, indicating which passages to select (1) or ignore (0). The selected passages are then"}, {"title": "5 Experiment", "content": "We present our experiments and results to validate the effectiveness of the three proposals outlined in Sec. 4. We first describe the experimental setup in Sec. 5.1. We then discuss the outcomes of the retrieve-then-generate variants and the joint modeling of retrieval and generation in Sec. 5.2. Finally, we analyze the results of retrieval attention probing in Sec. 5.3."}, {"title": "5.1 Setup", "content": "Benchmarks. We use LOFT (Lee et al., 2024) and our ICR2 benchmarks to evaluate LCLMs' in-context retrieval and reasoning capabilities. LOFT tests retrieval, single- and multi-hop question answering, and reasoning using NaturalQuestions (Kwiatkowski et al., 2019), HotpotQA (Yang et al., 2018), and MuSiQue (Trivedi et al., 2022). ICR2 uses NaturalQuestions and HotpotQA, and additionally includes FEVER (Thorne et al., 2018) for fact verification and WoW (Dinan et al., 2018) for dialogue completion. Similar to LOFT, we report average scores across 100 test cases per task using the 32K context length versions, which is the maximum supported by all tested LCLMs.\nMetrics. For the question answering and fact verification tasks, we use the exact match in (Lee et al., 2024; Adlakha et al., 2024). We use ROUGE (Lin, 2004) to assess on dialogue completion.\nTraining Details. We use ICR2's training set to fine-tune all models. Specifically, we randomly sample 7500, 7500, 5000, and 5000 instances for NaturalQuestions, HotpotQA, FEVER, and WoW, respectively. To verify the effectiveness of our proposed methods, we focus our experiments on Mistral-Instruct-7B model (Jiang et al., 2023).\nBaselines. We compare our proposed approaches with the baselines outlined in Sec. 3.3: Vanilla RAG, Closed-book, and Oracle RAG. Our methods as described in Sec. 4 include three retrieve-then-generate supervised fine-tuning variants Direct Answer (SFT-DA), Retrieve-then-Answer (SFT-RTA), and Cite-Context-ID (SFT-CCI) as well as the joint retrieval head training (RetHead) and retrieval attention probing (RAP)."}, {"title": "5.2 Main Results", "content": "As shown in Table 4, all SFT variants outperform the Vanilla RAG on both benchmarks, indicating that LCLMs struggle to effectively leverage context as a knowledge base for RAG tasks. Furthermore, the gap between SFT-DA and the Oracle RAG highlights that supervised fine-tuning alone is insufficient to achieve optimal results.\nAmong the SFT variants, SFT-RTA in general outperforms the others with an average improvement of 2%. Specifically on LOFT benchmark, both SFT-RTA and SFT-CCI outperform SFT-DA with 6% and 2% improvement on average, respectively, indicating the retrieve-then-generate strategy helps. On ICR2, however, all SFT variants perform the same. This demonstrates that ICR2 is a more discriminative benchmark than LOFT.\nWe apply RAP to all SFT models. On average, RAP enhances the models significantly, with SFT-DA + RAP improving by 6% and SFT-RTA + RAP by 8%. The best-performing approach, SFT-RTA + RAP, achieves notable gains on the challenging ICR2 benchmark, with improvements of 3%, 4%, 1%, and 1% on NaturalQuestions, HotpotQA, FEVER, and WoW, respectively. It also achieves top performance on 5 out of the 7 tasks, demonstrating its superiority.Remarkably, it achieves comparable performance with the state-of-the-art GPT-4-Turbo on LOFT and ICR2 while using a much smaller model. Finally, RAP decoding is more effective for SFT models than with the original model, as SFT better activates retrieval-specific attention heads for the approach (see Sec. 6.4).\nFor the joint retrieval head training (RetHead), we performed experiments on training only the generation head (w/Lgen), only the retreieval head"}, {"title": "6 Discussion", "content": "6.1 Effect of Retrieval Delineation\nAs outlined in Section 4.1, our retrieve-then-generate variants, SFT-RTA and SFT-CCI, utilize"}, {"title": "4 Eliciting ICR\u00b2 for LLMs", "content": "6.2 Scaling the Supervised Fine-tuning\nWe are also interested in how the performance of the proposed SFT variants scale with the training set size. We train the three variants, SFT-DA, SFT-RTA, and SFT-CCI, with the same 10K, 15K and 25K examples from ICR2's training set, and report their performance on each task in the ICR2 benchmark, as shown in Fig 5. We observe that an increased training set size in general leads to an improved model performance. In particular, a smaller amount of training data fares worse with the retrieve-then-generate approaches, as they are by nature more challenging to learn compared to the SFT-DA approach, where answer is directly generated without an explicit retrieval step.\n6.3 Blocking Context Attention in Retrieve-then-generate Model\nTo verify if models actually learn to generate the final responses only from the retrieval predictions in our retrieve-then-generate methods (Sec. 4.1),"}, {"title": "4 Eliciting ICR\u00b2 for LLMs", "content": "p(y | q, c) = \u2211 p(y|q, zi)p(zi | q, c), (6)\nZiEZ"}, {"title": "4 Eliciting ICR\u00b2 for LLMs", "content": "6.4 Effect on Attention Heads of Retrieve-then-Generate Fine-tuning\nTo understand if fine-tuning sharpens attention heads' focus on relevant passages, we compare the hit rates (Sec. 4.2) achieved by the attention heads between Vanilla RAG and all of our SFT variants (Sec. 4.1), and the results are shown in Figure 6. Similar to (Wu et al., 2024), we find that a small group of attention heads can obtain higher hit rates than the others. However, unlike Vanilla RAG, SFT methods produce more retrieval-focused attention heads, and achieve higher peak hit rates. This"}, {"title": "6.5 Retrieval Attention Probing", "content": "Our inference-time method RAP (Sec. 4.2) uses two hyperparameters: Q is the number of the attention heads we recruit for retrieval, and M is the number of passages each head retrieves. In this section, we apply different value settings when deploying the SFT-RTA + RAP combined approach to explore their effect on model performance."}, {"title": "6.6 Decoding Speed", "content": "Our final analysis is on the efficiency of RAP. Based on the SFT-DA variant, we report its latency with and without the RAP enhancement in Table 8. We find that the RAP decoding does not increase latency significantly, despite it adds one additional decoding step to the base method. This can be attributed to the much shorter context retrieved by the attention heads, thus avoiding expensive long-context computation as in the baseline. This increase can be further reduced with approaches such as KV caching, which we leave for future work.\nRecall when applying RAP decoding, the base method (SFT-DA in this case) needs to first exercise the attention heads by generating a single token conditioned on the entire contextual knowledge base. A second decoding step is then performed on the query and the passages identified by the designated attention heads. Compared to the base method without RAP decoding where only one decoding step is required but is conditioned on the entire contextual knowledge base, the additional RAP decoding step is conditioned on a filtered and much shorter context, therefore it does not take much more time by comparison."}, {"title": "7 Conclusion", "content": "In this paper, we introduce ICR2, a new benchmark designed as a more realistic and discriminative benchmark for evaluating LCLM's abilities in in-context retrieval and reasoning. Our findings highlight the limitations for the current models. Additionally, we propose three methods \u2014retrieve-then-generate fine-tuning, retrieval attention probing, and joint retrieval head training\u2014 to enhance models, achieving the results comparable to GPT-4, but with a smaller model footprint.\nFuture work could prioritize developing adaptive in-context retrieval strategies that enable models to accurately retrieve and reason over context without extensive task-specific fine-tuning. Additionally, creating more efficient in-context methods for generating faithful responses conditioned solely on filtered context would enhance robustness against confounding information."}, {"title": "Ethical Considerations", "content": "We do not expect any ethical concerns to be raised with respect to this work."}, {"title": "Limitations", "content": "We acknowledge several limitations in this work. First, most experiments were conducted with a context length of 32K tokens. Our findings indicate that while many LCLMs claim to support longer contexts, their performance on tasks with 32K tokens remains suboptimal. Future work could focus on extending ICR2 and the proposed approaches to effectively support scenarios with longer context lengths.\nSecond, while Joint Retrieval Head Training demonstrates improved performance compared to Vanilla RAG, it still falls short of the performance achieved by the SFT variant. Future research could explore improved architectural designs to better integrate the supervision signals from both the retrieval and generation tasks.\nFinally, our evaluation primarily utilizes the Mistral-7B model due to computational constraints. Extending the proposed methods to other LCLMs would provide a broader assessment of their generalization capabilities and effectiveness across different model architectures."}, {"title": "A Prompt Template for theRetrieval-augmented Generation", "content": "We show the Corpus-in-Context (CiC; Lee et al.2024) prompt template used in our experiments inTable 9."}, {"title": "C.1Training Details", "content": "For all models, we set the base value of the rotary position embedding to 1e6 following Su et al.(2024). Training is conducted with a batch size of1 per Nvidia A100-40G GPU, and gradients are accumulated every 4 steps. Each model is trained forup to 10,000 steps (approximately 2 epochs), withthe best-performing checkpoint on the validationset selected as part of an early-stopping strategy.The learning rate is set to le 5, and the maxi-mum sequence length during training is capped at32,768 tokens, discarding any sequences exceedingthis threshold."}, {"title": "C.2 Inference Parameters", "content": "During inference, we use greedy decoding for allmodels, allowing a maximum generated sequencelength of 1,024 tokens. For RAP decoding, 100 ran-dom instances from the validation set are used toprobe the attention heads responsible for retrieval.For SFT-DA, retrieval is performed using the designated retrieval attention heads based on the first andonly decoded token. For SFT-RTA, retrieval is per-formed using all tokens generated in the retrievalstep to ensure complete context coverage."}, {"title": "C.2.1 RAP Hyperparameter Settings", "content": "In this section, we detail the hyperparameters usedin applying RAP decoding to SFT-DA and SFT-RTA on ICR2 and LOFT.For SFT-DA on ICR2:\u2022 NaturalQuestions: Q = 4, M = 1\u2022 HotpotQA: Q = 8, M = 1\u2022 FEVER: Q = 4, M = 4\u2022 WoW: Q = 4, M = 4For SFT-DA on LOFT:"}, {"title": "4 Eliciting ICR\u00b2 for LLMs", "content": "Dialogue CompletionFact VerificationQA[INST] Please answer the following question given the following passages:{Corpus}Question: {Query}Answer:[/INST][INST] According to the following passages, please verify the given claimand predict your judgment on its factuality as TRUE or FALSE:{Corpus}Claim: {Query}Judgement: [/INST][INST] According to the given passages, please provide a single response tocomplete the following conversation by role-playing as either Person A orPerson B. Your response should be as knowledgeable and coherent with theconversation history as possible:{Corpus}Conversation: {Query}[/INST]Table 9: Prompt template used for RAG tasks in our experiments. {Corpus} refers to the provided contextualknowledge base, and {Query} refers to a query in ICR2 or LOFT. A {Query} can be a question in the questionanswering tasks (Kwiatkowski et al., 2019; Yang et al., 2018), a claim to be verified in the fact verification task(Thorne et al., 2018), or a conversation history in the dialogue completion task (Dinan et al., 2018).\u2022 NaturalQuestions: Q = 4, M = 4\u2022 HotpotQA: Q = 4, M = 4\u2022 WoW: Q = 8, M = 2For SFT-RTA on ICR2:\u2022 NaturalQuestions: Q = 2, M = 1\u2022 HotpotQA: Q = 2, M = 1\u2022 FEVER: Q = 2, M = 8\u2022 WoW: Q = 4, M = 8For SFT-RTA on LOFT:\u2022 NaturalQuestions: Q = 8, M = 4\u2022 HotpotQA: Q = 4, M = 2\u2022 WoW: Q = 8, M = 2"}]}