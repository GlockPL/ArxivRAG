{"title": "Towards Assuring EU AI Act Compliance\nand Adversarial Robustness of LLMS", "authors": ["Tomas Bueno Momcilovic", "Beat Buesser", "Giulio Zizzo", "Mark Purcell", "Dian Balta"], "abstract": "Large language models are prone to misuse and vulnerable to security\nthreats, raising significant safety and security concerns. The European Union's\nArtificial Intelligence Act seeks to enforce AI robustness in certain contexts, but\nfaces implementation challenges due to the lack of standards, complexity of LLMs\nand emerging security vulnerabilities. Our research introduces a framework using\nontologies, assurance cases, and factsheets to support engineers and stakeholders\nin understanding and documenting AI system compliance and security regarding\nadversarial robustness. This approach aims to ensure that LLMs adhere to regula-\ntory standards and are equipped to counter potential threats.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have shown great results in generating content from the\ndata they were trained or fine-tuned on, when prompted in natural language (Kojima et al.\n2022). However, recent work shows that training, fine-tuning, prompting and generating\ncan be vulnerable to malicious or accidental misuse (Yao et al. 2024), as the models\nthemselves are brittle to adversarial attacks (Zou et al. 2023). By exploiting unknown\nproperties of LLMs, attacks can negatively impact the privacy and fundamental rights of\nEU citizens by leaking information or generating toxic content (European Parliament &\nCouncil of the European Union 2016). In combination with advanced capabilities (e.g.,\nrobotic control; Vemprala et al. 2023), applications (e.g., autonomous decision-making;\nWang et al. 2024) or contexts (e.g., medical diagnosis; Thirunavukarasu et al. 2023),\ncompromised LLMs can also have safety implications.\nThe recently adopted EU Artificial Intelligence Act (further: EUAIA; European\nParliament & Council of the European Union 2024) aims to mitigate the negative\nimpact of \"high-risk\" AI systems by imposing demands on providers and deployers\nin designated contexts. Two foreseeable issues will make implementation of the Act\nconsiderably challenging if such systems have LLM components. First, the standards that\noperationalize the legal language into technical requirements are yet to be established,\nand rapid pace of development could render some parts obsolete. Second, the architecture"}, {"title": "2 Background", "content": "The EUAIA (European Parliament & Council of the European Union 2024) is a law\ncovering particular aspects of AI usage in the EU, which was proposed in April 2021 and\nadopted in March 2024\u00b9. It is expected to enter into force in 2026, whereby technical\nstandards and guidelines that interpret the Act will be available at the earliest in mid-2025\n(CEN-CENELEC 2024), or no later than 2028 (Art. 6 Para. 5; Art. 15 Para. 2; EUAIA).\nThe core of EUAIA are duties placed upon the providers\u00b2 of any AI system that will\nbe used in high-risk domains (Art. 6-49; Annex I Section B; Annex III) or within regu-\nlated products (Annex I Section A). Other duties include: responsibilities of other stake-\nholders; prohibitions of using AI systems in particular domains (Art. 5); transparency-\nrelevant duties for providers of user-oriented and generative AI systems (Art. 50); and\nprovisions for structuring the regulatory administration (Art. 57-100). Although most\nduties are model-agnostic, providers of general-purpose AI models\u00b3 have specific obli-\ngations regardless of the domain (Art. 51-56).\nWhile LLMs are not inherently classified as high-risk, EUAIA duties may apply in\nat least three scenarios. First, stakeholders in the regulated contexts may find the general\ncapabilities and user-friendliness of LLM-based chatbots to be worth the compliance\neffort. Second, as first of its kind globally, the EUAIA may become the standard frame-\nwork for how to structure voluntary risk management. Third, regular reviews by the\nlegislators (Chapter IX & Art. 112) and any detected incidents (Art. 73) may result in\nthe risk classification, domain coverage or model-specific duties being amended."}, {"title": "3 Methodology", "content": "Our research methodology centers on knowledge representation from three parallel\nstreams. First, we perform a simple legal analysis (Hohfeld et al. 2001, van Engers\n& van Doesburg 2015) of the EUAIA to identify relevant duties and stakeholders.\nSecond, we elicit information about adversarial attacks and defenses in unstructured\nexpert interviews and literature review (cf. Bueno Momcilovic et al. 2024). Third, we\nuse the Goal Structuring Notation (GSN; Assurance Case Working Group (ACWG)\n2021) to express the confidence about EUAIA compliance and adversarial robustness in\nan exemplary assurance argument, comprising claims and evidence about appropriate\ndefenses. We then combine and formalize this information in an ontology 5 using the\nWeb Ontology Language (World Wide Web Consortium (W3C) 2012), and display it as\na human-readable narrative FactSheet report (Arnold et al. 2019)."}, {"title": "4 Proposed Framework", "content": "We identify 23 duties in EUAIA (cf. Table 1) that directly refer to safety, cybersecurity\nor robustness, or proximate terms such as incident, risk or misuse. Providers of high-risk\nAl systems need to satisfy fifteen of those duties, and providers of general-purpose AI"}, {"title": "5 Conclusion", "content": "The EU Artificial Intelligence Act aims to mitigate risks of AI systems by imposing\nobligations on the robustness of various properties. However, for systems with LLM\ncomponents, the implementation of these duties will be significantly challenging due\nto the inherent complexity and opacity of LLMs, alongside the continuous emergence\nof new security threats. Our proposed framework seeks to make the process of ensur-\ning compliance and robustness effective, by allowing engineers (i.e., providers and\ndeplyers) to more easily represent and reason about LLM defenses through ontologies\nand assurance cases. The framework allows legal stakeholders and users to audit these\nsystems with a complete, accurate and up-to-date snapshot.\nNonetheless, we recognize that this approach currently relies on manual work in\ncreating arguments. This limits its usefulness for documenting and evaluating changes to\nlaw, system or attack vectors. Our future research centers on integrating the framework\nwith techniques and tools that would allow arguments, concepts and relations to be\nexpressed automatically, and evaluating it experimentally."}]}