{"title": "Overview of the Amphion Toolkit (v0.2)", "authors": ["Jiaqi Li", "Xueyao Zhang", "Yuancheng Wang", "Haorui He", "Chaoren Wang", "Li Wang", "Huan Liao", "Junyi Ao", "Zeyu Xie", "Yiqiao Huang", "Junan Zhang", "Zhizheng Wu"], "abstract": "Amphion is an open-source toolkit for Audio, Music, and Speech Generation, targeting to ease the way for junior researchers and engineers into these fields. Amphion is released in December 2023 where the rapid advancements in generative models have revolutionized the fields of audio, music, and speech processing. Amphion incorporates these most up-to-date generative technology, and is designed to be a unified framework that includes diverse generation tasks and models, with the added bonus of being easily extendable for new incorporation. The toolkit is designed with beginner-friendly workflows and pre-trained models, allowing both beginners and seasoned researchers to kick-start their projects with relative ease. Since its initial launch in December 2023, Amphion has grown rapidly and received more than 8K stars on GitHub. Its dataset has become the most monthly downloaded audio dataset on the HuggingFace platform.", "sections": [{"title": "Introduction to the Amphion Toolkit", "content": "Amphion is an open-source toolkit for Audio, Music, and Speech Generation, targeting to ease the way for junior researchers and engineers into these fields. Amphion is released in December 2023 where the rapid advancements in generative models have revolutionized the fields of audio, music, and speech processing. Amphion incorporates these most up-to-date generative technology, and is designed to be a unified framework that includes diverse generation tasks and models, with the added bonus of being easily extendable for new incorporation. The toolkit is designed with beginner-friendly workflows and pre-trained models, allowing both beginners and seasoned researchers to kick-start their projects with relative ease. Since its initial launch in December 2023, Amphion has grown rapidly and received more than 8K stars on GitHub. Its dataset has become the most monthly downloaded audio dataset on the HuggingFace platform."}, {"title": "What's new in Amphion v0.2", "content": "Amphion v0.2 builds upon its initial release, significantly broadening its scope and utility. Amphion v0.2 expands on both datasets and models, covering a wider range of tasks in the fields of audio, music, and speech generation. The following table summarizes the technology supported in Amphion v0.2. Models introduced in this version are highlighted in bold fonts, which are also exclusively available in Amphion."}, {"title": "The Fundamentals of Amphion", "content": null}, {"title": "Neural Audio Codecs", "content": "Audio representations. A waveform represents the raw audio signal as a function of amplitude over time. In the analog domain, it is a continuous signal, but it becomes discrete when sampled at a fixed rate, such as 44.1 kHz or 16 kHz. Waveforms offer the highest temporal resolution, capturing all the fine-grained details of an audio signal. The earliest deep neural network for audio generation, WaveNet [113], directly modeled raw audio waveforms. However, this approach is computationally inefficient due to the high dimensionality of waveforms and is highly sensitive to noise.\nIn contrast, the Mel spectrogram is a time-frequency representation derived from the Short-Time Fourier Transform (STFT). It maps the linear frequency scale to the perceptually motivated Mel scale, which emphasizes frequencies crucial to human hearing. Represented as a 2D matrix, the horizontal axis corresponds to time frames, while the vertical axis represents Mel frequency bins [28]. Unlike raw waveforms, Mel spectrograms provide a more compact, continuous-valued representation. Many audio generative models predict Mel Spectrograms [26, 55, 67]. They can be inverted back into waveforms using neural vocoders [51, 57, 98], which enable high-quality reconstruction of the original audio signal.\nAudio Codecs. A Codec is a combination of the words coder/decoder. It is a computer algorithm designed to compress and decompress digital audio signals, enabling efficient storage and transmission of audio data. Established signal processing-based codecs like MP3, FLAC(Free Lossless Audio Codec), Opus [111] rely on hand-engineered features and psychoacoustic principles to remove redundant or imperceptible components of audio. Compared to continuous-valued Mel Spectrograms, audio codecs output a discrete sequence often with a small vocabulary, allowing the discrete generative modeling of audio such as using large language models. Recent advancements in deep learning have given rise to neural audio codecs, which learn to compress and reconstruct audio data directly from large-scale datasets. Neural audio codecs today are used to extract discrete or continuous representations for speech, audio and music generation tasks [17].\nNeural Audio Codecs. Neural audio codecs use machine learning models, often based on Vector Quantized VAEs (VQ-VAEs) [112], to compress audio. By leveraging neural networks, these models deliver higher audio quality than signal processing-based codecs with higher compression rates. Figure 1 presents a high-level architecture of a neural audio codec [62]. It has an convolutional encoder that downsamples waveforms to a much lower sampling rate (e.g., 50Hz), a residual vector quantization (RVQ) module that discretizes latent features, and a decoder that reconstructs audios from discrete tokens. Neural audio codecs are used not only for audio transmission, but also for extracting useful discrete and continuous audio representations for generative models [124]."}, {"title": "Speech Language Models", "content": null}, {"title": "Autoregressive Language Models", "content": "Autoregressive language models form a foundational approach in speech generation, leveraging the sequential nature of speech data. These models generate output tokens one by one, conditioning each token on all previously generated tokens. Formally, given a sequence of tokens x = [y1, y2,..., yn], ARLMs the joint probability of the sequence as a product of conditional probabilities:\n$P_@ (x) = \\prod_{i=1}^{n} P_@ (y_i | y_1, y_2, ..., y_{i-1}, c)$,\nwhere c represents optional conditioning information, such as semantic, linguistic, or acoustic features. The model parameters @ are typically optimized using maximum likelihood estimation (MLE) to minimize the negative log-likelihood of the observed sequences in the training data:\n$L_{AR} = -E_{x,c} \\sum_{i=1}^{n} log P_@ (y_i | y_1, y_2, ..., y_{i-1}, c)$.\nIn the context of speech generation, auto-regressive language models(ARLMs) are commonly em-ployed to model either discrete representations of speech (e.g., linguistic tokens or acoustic tokens derived from neural audio codecs) or directly raw waveforms. At the inference stage, ARLMs generate speech tokens autoregressively, where the generation of each token depends on all previously generated tokens, ensuring high fidelity in sequential dependencies but introducing latency challenges.\nSeveral works have successfully applied ARLMs to speech generation tasks. AudioLM [9] demon-strates the use of ARLMs to generate semantic and acoustic tokens for high-quality speech generation. VALL-E [116] extends autoregressive modeling to zero-shot TTS, achieving remarkable speaker fidelity. UniAudio [132] further generalizes ARLMs to unified audio modeling tasks, including speech, music, and sound, by leveraging a shared autoregressive architecture."}, {"title": "Masked Generative Models", "content": "Masked generative models (MGMs) are another class of speech language models. In this section, we provide a brief introduction to masked generative models [12]. Consider a discrete sequence x =\n[y1, y2,..., yn], where n denotes the length of the sequence. We define xt = x \\m_t as the operation\nof masking a subset of tokens in x using the corresponding binary mask mt = [mt,1,mt,2,...,mt,n].\nSpecifically, this operation involves replacing xi with a special [MASK] token if mt,i = 1, and otherwise\nleaving xi unmasked if mt,i = 0. Here, each mt,i is independently and identically distributed according\nto a Bernoulli distribution with parameter \\gamma(t), where \\gamma(t) \\in (0,1] represents a mask schedule\nfunction (for example, \\gamma(t) = sin( \\pi t/2T ),t \\in (0,T]). We denote x = x0. The masked generative models\nare trained to predict the complete sequence (masked tokens) based on the observed tokens (unmasked\ntokens) and the condition c, which can be modeled as p@(x0 | xt, c), and the model parameters @ are\ntrained to optimize the sum of the marginal cross-entropies for each unobserved token:\n$L_{mask} = -E_{x,t,m} \\sum_{i=1}^{n} m_{t,i} .log p_@ (y_i | x_t, c)$ (1)"}, {"title": "Diffusion and Flow-Matching Models", "content": null}, {"title": "Denoising Diffusion Probabilistic Models", "content": "The remarkable advancements in generative models have greatly expanded the capabilities of content creation. Behind many generative tools, there is a specialized sampling mechanism: diffusion models. Denoising Diffusion Probabilistic Models (DDPM) [39, 99] try to model a data distribution p@(x0) by two processes:\n(1) The forward process progressively transforms the data distribution into a standard Gaussian distribution using a predefined noise schedule 0 < \u03b21 < \u03b22 < \uff65\uff65\uff65 < \u03b2n < \u2026 < \u03b21 < 1. At each time step n \u2208 [1,..., N], the transition probability are defined as:\nq(xn | xn\u22121) = N(xn;\\sqrt{1 \u2212 \u03b2n}xn\u22121,\u03b2nI)\n(2)\nBy leveraging the stepwise transition probabilities q(xn | xn\u22121), the direct probability distribution q(xn | x0) can be derived as:\nq(xn | x0) = N(xn;\\sqrt{\u03b1n}x0,(1\u2212\u03b1n)\n(3)\nwhere \u2208 ~ N(0,I) is standard Gaussian noise, \u03b1n is reparameterized as 1 \u2013 \u03b2n, and \u03b1n = \u03a0 s=1 \u03b1s represents the cumulative noise level at step n.\n(2) A reverse denoising process reconstructs x0 using the reweighted noise estimation \u0109e. The optimization objective can be simplified to:\n$L_{DM} (0) = E_{x0, \\epsilon~N (0,1),n} ||\\epsilon \u2212 \\epsilon_0 (x_n, n) ||^2$ \n(4)\nwhere \u20ac\u03b8 (xn,n) represents a sequence of denoising autoencoders trained to iteratively predict noise based on input xn and step n."}, {"title": "Latent Diffusion Models", "content": "Unlike traditional diffusion models, Latent Diffusion Models (LDM) conducts the diffusion process in the latent representation space, reducing computational complexity in DDPM. LDM [93] aiming to approximate the true conditional data distribution q(zn | c) with a model distribution p@(zo | c), where zo is the prior of an audio sample in latent space.\nLDM [93] introduces conditioning mechanisms into the diffusion process, enabling multimodal training through cross-attention and enabling conditional generation tasks. Specifically, it trains a conditional denoising autoencoder \u03b5\u03b8(Zn,n,To (y)) to guide the generation process via the provided conditions y. The condition y can represent various modalities (e.g., text, phoneme), is mapped to an intermediate representation Te (y) by the encoder \u03c4\u03bf. The final model incorporates the control conditions into the UNet's intermediate layers through a cross-attention mapping. The optimization objective is:\nLLDM (0) = E\u025b(x),y,\u20ac~N (0,1),n ||\u2208 \u2212 \u20ac0 (Zn, N, \u0442\u04e9(y))||2\n(5)"}, {"title": "Flow-Matching Models", "content": "Considering that diffusion models limit the choice of sampling probability paths, leading to long training times and inefficient sampling, Flow-Matching Models[65] (FM) is a novel generative framework that transforms a simple prior distribution p (usually standard Gaussian distribution) to a target distribution q with learning a time-dependent flow mapping $ : [0, 1] \u00d7 Rd \u2192\nRd, defined via the ordinary differential equation (ODE):\ndot(x)\ndt = vt (\u03c6t (x)); \u03c60(x) = x\n(6)\nHere, v\u2081 represents a vector field that is learned by optimizing the Flow Matching (FM) objective. Given a vector field ut (x) and its generated conditional probability path pt (x), the objective is try to align the model's vector field v\u2081 (x;0) with the target vector field ut (x) and is expressed as:\nLFM(0) = Et,pt(x) ||Vt (x; 0) \u2212 ut (x) ||2\n(7)\nwhere t ~ U[0, 1] is flow step, pt (x) is a target probability density path and u\u2081 (x) is the corresponding vector field. Since both pt(x) and u\u2081 (x) are unknown, given a data sample z, they can be derived using the marginal probability from the conditional probability:\n$p_t(x) = \\int p_t(x|z)q(z) dz$ (8)\n$U_t(x | z) = \\frac{p_t(x|z)q(z)}{p_t(x)}$ (9)\nConditional Flow Matching (CFM) regress v\u2081(x) on the conditional vector field u\u2081 (x | z) and its generated conditional probabilistic path pt (x | z). This approach transforms the intractable marginal flow matching problem into a computable conditional problem, reformulating it into the Conditional Flow Matching (CFM) objective:\n$L_{CFM}(0) = E_{t,q(x_1),p_t (x|x_1)} ||V_t (X) \u2212 U_t (x | x_1) ||^2$(10)\nCosyVoice[26] employs optimal-transport conditional flow matching (OT-CFM) [65] to model the transition from the prior distribution po(x) to the target Mel spectrogram distribution q(x). The OT-CFM flow loss is:\n$L_{OT-CFM} = E_{t,q(x_1), p_0(x_0)} ||w_\\tau(\\varphi^\\tau(x_0,x_1) | x_1) \u2212 v_\\tau(\\varphi^\\tau (x_0,x_1) | \\theta)||\nwhere \u0444\u0442 (x0,x1) represents the flow from xo to x1, defined as:\n\u0444\u0442 (x0,x1) = (1 \u2212 (1 \u2212 \u03c3)t)xo + tx1\n(12)\nwhich represents the linear interpolation between x0 and x1 over time t \u2208 [0,1]. Each target sample x1 is matched to a random sample xo ~ N(0, 1). The gradient vector field along this flow is defined as:\nwt (\u03c6 (x0,x1) | x1) = x1 \u2212 ( 1 \u2212 \u03c3)xo\n(13)\nwhich serves as the learning target for the neural network. This vector field is linear, time-invariant, and depends only on x0 and x1. The speaker embedding v, speech tokens {\u03bcl}1:L, and masked mel-spectrogram X\u2081 are used as conditional inputs to guide the neural network in approximating the vector field v\u2081 (\u0422 (x0,x1) | \u03b8):\n$V_t(U_t(\\varphi_\\tau(x_0,x_1) | \\theta) = NN_\\theta (U_t(\\varphi_\\tau(x_0,x_1) | \\theta))$\n(14)"}, {"title": "Deepfake Detection", "content": "The rapid development of voice synthesis technology has raised significant concerns about the security risks associated with its misuse. State-of-the-art text-to-speech (TTS) models [119] are now capable of cloning any individual's voice using only a few seconds of audio. The term \"deepfake\" is commonly used by the media and public to describe any audio or video content in which key attributes have been digitally modified or replaced using artificial intelligence (AI) techniques. To mitigate these risks, researchers have introduced the Deepfake Detection task, which aims to distinguish AI-generated samples from genuine ones.\nWith the continuous advancements in deep learning, researchers have increasingly adopted deep learning-based methods for deepfake detection. [101] employed Graph Attention Networks (GAT) to model time segments and frequency subbands separately, and utilized GAT for feature fusion. [47] recognized the heterogeneity of temporal and spectral information in spectrograms, and therefore enhanced the graph neural network's ability to model the heterogeneity of time-frequency features us-ing heterogeneous attention mechanisms (AASIST). Building upon AASIST, [103] further improved the performance by incorporating the pre-trained model Wav2Vec [6] and the enhancement method RawBoost [102].\nIt is undeniable that the development of speech synthesis technology has significantly enhanced convenience in various domains, such as in-car navigation systems, e-readers, and intelligent robots. However, while current research primarily focuses on distinguishing machine-generated speech from human speech, a more pressing challenge lies in detecting misinformation embedded within spoken content. This task requires a comprehensive analysis of factors such as speaker identity, topic, and contextual consistency. To address this challenge, we introduce the open-source dataset SpMis [70] (Introduced in Section 3.1.1 and conduct an initial investigation into misinformation detection in synthesized speech. The SpMis dataset comprises speech samples generated by state-of-the-art text-to-speech (TTS) systems, covering five common topics and involving over 1,000 speakers, providing a valuable resource for advancing research in this domain."}, {"title": "Speech Generation Datasets", "content": "Speech generation datasets are essential resources for training models to produce natural-sounding speech. These datasets typically consist of paired audio and text data, enabling the learning of mapping text to speech.\nIn Table 2, we summarize existing speech generation datasets. Early datasets, such as LJSpeech [45] and VCTK [130], focus on audiobook or studio recordings and are limited to less than 100 hours of data. While larger datasets like Libri-Light [48] and MLS [86] have scaled to tens of thousands of hours, they predominantly consist of audiobook data, which may not generalize well to more spontaneous speech scenarios.\nRecent efforts, such as AutoPrepWild [135] and GigaSpeech [13], have aimed to bridge this gap by collecting in-the-wild data from diverse sources, introducing more naturalistic and spontaneous speech. However, these datasets still fall short in terms of total duration, multilingual coverage, and extendability. Extendable datasets allow researchers to incorporate new data, languages and speakers by running the data pre-processing pipeline.\nAmphion v0.2's Emilia [36] addresses these challenges by providing 101K hours of in-the-wild data across six languages (English, Chinese, German, French, Japanese, and Korean) at a sampling rate of 24kHz. Emilia stands out not only for its scale and multilingual coverage but also for its extendability, by open-sourcing a pipeline for dataset expansion, supporting the growing demand for diverse and robust speech generation models."}, {"title": "Speech LLMs", "content": "LLMs have shown remarkable flexibility [1, 106, 107, 109, 131], acting as a universal interface capable of offering assistance across a wide spectrum of tasks and domains. Initially, their strengths are most evident in text-based applications, such as language understanding, content generation, and complex question answering. Recently, their abilities have expanded beyond textual input to accommodate multi-modal data, such as speech [15, 16, 20, 23, 30, 32, 41, 52, 81, 105, 127]. This development greatly enhances the range of tasks they can tackle, allowing them to support richer and more dynamic interactions with users.\nBuilding on these expanded abilities, multi-modal LLMs have demonstrated impressive performance on tasks that involve not only understanding written language but also interpreting spoken language cues, such as speech recognition and speech emotion recognition. They generally centre on interpret-ing input signals and carrying out tasks directed by text-based instructions [16, 23, 32, 52, 105].\nMeanwhile, in pursuit of more seamless human-computer interaction, many recent studies have focused on enhancing voice-based communication with LLMs [20, 30, 127]. This line of research aims to reduce the latency and complexity that can arise from switching between spoken and textual inputs, thereby providing a more intuitive user experience. Most of these methods utilize speech tokenization techniques to model speech signals. By representing audio in a manner compatible with existing text-centric architectures, these methods enable the powerful text-driven reasoning capabilities of LLMs to be applied to spoken language tasks."}, {"title": "Technologies in Amphion", "content": "This section provides an overview of the specific models in Amphion. Section 3.1.1 describes our data pre-processing pipeline technology. Section 3.2 highlights the latest advancements in Amphion's text-to-speech technologies. In Section ??, we detail our neural audio codec technologies. Our voice conversion technology is presented in Section 3.4."}, {"title": "Datasets", "content": null}, {"title": "Emilia Dataset", "content": "Because of scarcity of diverse high-quality, large-scale speech datasets, we release the Emilia dataset [36] in Amphion v0.2. It offers over 100K hours of multilingual speech data spanning six languages, capturing diverse speech styles and real-world scenarios. Additionally, Emilia-Pipe, an open-source data preparation pipeline used to construct Emilia, enables efficient processing of raw audio data for model training.\nWe first describe Amphion's data preprocessing pipeline, Emilia-Pipe. Emilia-Pipe is designed to transform any audio into 3-30s speech suitable for training speech generation systems. As illustrated in Fig. 2, Emilia-Pipe includes six steps, i.e., Standardization, Source Separation, Speaker Diarization, Fine-grained Segmentation by VAD, ASR, and Filtering. Emilia-Pipe is implemented in Amphion v0.21.."}, {"title": "SpMis Dataset", "content": "SpMis dataset for spoken misinformation detection. The SpMis\u00b9\u00b9 Dataset is an open-source collection designed specifically for detecting misinformation in synthesized speech. It is structured around the concept of misinformation in synthetic audio, focusing on detecting and classifying speech that is artificially generated to mislead. The dataset includes textual data from five key domains-finance, medicine, politics, law, and education\u2014as well as other miscellaneous topics, all sourced from authoritative datasets in these fields. For speech generation, over a thousand distinct speakers were selected from the Libri-Light dataset, whose voices were synthesized using two open-source text-to-speech systems: Amphion and OpenVoice v2. The dataset's statistics are presented in Table 3.\nSynthetic Spoken Misinformation. Synthetic misinformation refers to information created using synthesis techniques that mislead the public into biased decisions. We define two scenarios:\n\u2022 Case 1: Speeches from ordinary people, whether synthetic or not, are not considered misinformation.\n\u2022 Case 2: Recordings of celebrities on a specific topic are valid, but synthesized recordings of celebrities are misinformation.\nHere, \"celebrity\" refers to shortlisted identities, while \"ordinary people\" are non-shortlisted identities.\nText Data. We generate speeches on five common topics, which include politics, medicine, education, laws, and finance. Each topic uses different corpora:\n\u2022 Finance. We use financial phrases from English news about listed companies in OMX Helsinki [76].\n\u2022 Medicine. We use medical abstracts [95], covering diseases and pathological conditions.\n\u2022 Politics. We use UK parliamentary speeches [80], ranging from 1979 to 2021.\n\u2022 Laws. We select the Super-SCOTUS dataset [29], which includes oral arguments and summaries from the US Supreme Court.\n\u2022 Education. We use the NCTE Transcripts dataset [22], focusing on classroom discourses from 4th and 5th grade mathematics classrooms.\nSpeech Data. We use the Libri-Light [48] dataset for reference speakers. The dataset provides thousands of speakers from open-source audiobooks, with characteristics extracted to generate speech based on the above text data."}, {"title": "SD-Eval Dataset", "content": "SD-Eval dataset for spoken dialogue understanding benchmark. This task of spoken dialogue understanding benchmark addresses the evaluation of multi-modal Large Language Models (LLMs) in processing and responding to complex spoken interactions. Speech encompasses linguistic, paralin-guistic, and environmental information, all of which play a critical role in effective communication. In the LLM context, as shown in Figure 4, ideally, the LLM-based spoken dialogues should be impacted by the rich information carried in speech (e.g. emotion, accent, age, environment). While LLMs have demonstrated proficiency in recognizing speech, they often fall short in generating appropriate responses due to the lack of speech-specific benchmarks and evaluation principles."}, {"title": "Debatts-Data Dataset", "content": "Debatts-Data for Domain-Specific TTS in Debates: The Debatts-Data [43] \u00b9\u00b3 is an extension to the Emilia dataset with the the data from Mandarin public debates. This contribution is important for domain-specific TTS in debates, because there are unique characteristics of debate-style speech, such as rhetorical emphasis, dynamic intonation, and varied pacing. General-purpose speech datasets like Emilia do not capture these characteristics in debates.\nWe propose a 110 hours Mandarin debating dataset called Debatts-Data to support the development of debating TTS models. We note that different from the traditional TTS dataset, the rebuttal in debating has an opponent and a speaker. Typically, the rebuttal session begins with a competition moderator introducing the process.\nPipeline: We developed a pipeline to process the raw recordings. The pipeline consists of five steps as shown in Fig. 5, including moderator detection, rebuttal session extraction, speaker diarization, overlap deletion and merging and speech enhancement and metadata extraction. In particular,\n\u2022 Moderator Detection: First, we utilized Paraformer zh [3] to transcribe texts and extract global speaker labels and then identified the moderator's speaker ID as the anchor ID.\n\u2022 Rebuttal Session Extraction: Then, we located the rebuttal session segments by extracting keywords from the moderators. This approach enables automated and precise extraction of specific segments from full competition recordings.\n\u2022 Speaker Diarization: After that, we employed a speaker diarization toolkit [11, 85] to identify speakers within the rebuttals and extract single-speaker speech pairs for training.\n\u2022 Overlap Detection and Merging: Next, we eliminated overlaps, which are common in rebuttals. The detection of overlaps is achieved by analyzing the overlapping timestamps in the diarization data. After deleting the overlaps, speech segments from the same speaker are concatenated.\n\u2022 Speech Enhancement and Metadata Extraction: Finally, we performed speech enhancement and metadata extraction. Speech enhancement is mainly to reduce background noise and remove speech segments with severe noise. Metadata extraction is to extract transcriptions, speaker labels, timestamps and style vectors [73]."}, {"title": "Text to Speech", "content": null}, {"title": "MaskGCT", "content": "MaskGCT is a fully non-autoregressive model for text-to-speech synthesis that uses masked generative transformers without requiring text-speech alignment supervision and phone-level duration prediction. MaskGCT is a two-stage system, both stages are trained using the mask-and-predict learning paradigm. The first stage, the text-to-semantic (T2S) model, predicts masked semantic tokens with in-context learning, using text token sequences and prompt speech semantic token sequences as the prefix, without explicit duration prediction. The second stage, the semantic-to-acoustic (S2A) model, utilizes semantic tokens to predict masked acoustic tokens extracted from an RVQ-based speech codec with prompt acoustic tokens. During inference, MaskGCT can generate semantic tokens of various specified lengths with a few iteration steps given a sequence of text. The overview of MaskGCT is shown in Figure 6.\nText-to-Semantic Model During training, we randomly extract a portion of the prefix of the semantic token sequence as the prompt, denoted as SP. We then concatenate the text token sequence P with SP to form the condition. We simply add (P,SP) as the prefix sequence to the input masked semantic token sequence St to leverage the in-context learning ability of language models. We use a Llama-style [109] transformer as the backbone of our model, incorporating gated linear units with GELU [38] activation, rotation position encoding [100], etc., but replacing causal attention with bidirectional attention. We also use adaptive RMSNorm [140], which accepts the time step t as the condition. During inference, we generate the target semantic token sequence of any specified length conditioned on the text and the prompt semantic token sequence.\nSemantic-to-Acoustic Model We also train a semantic-to-acoustic (S2A) model using a masked generative codec transformer conditioned on the semantic tokens. Our semantic-to-acoustic model is based on SoundStorm [8], which generates multi-layer acoustic token sequences. Given N layers of the acoustic token sequence A1:N, during training, we select one layer j from 1 to N. We denote the jth layer of the acoustic token sequence as A\u0134. Following the previous discussion, we mask Aj at the timestep t to get At. The model is then trained to predict A conditioned on the prompt AP, the corresponding semantic token sequence S, and all the layers smaller than j of the acoustic tokens. This can be formulated as p0S2A (A|A, (AP, S, A1:-1)). We sample j according to a linear schedule p(j) = 1- N(N+1) For the input of the S2A model, since the number of frames in the semantic token sequence is equal to the sum of the frames in the prompt acoustic sequence and the target acoustic sequence, we simply sum the embeddings of the semantic tokens and the embeddings of the acoustic tokens from layer 1 to j. During inference, we generate tokens for each layer from coarse to fine, using iterative parallel decoding within each layer. Figure 7 shows a simplified training diagram of the T2S and S2A models."}, {"title": "Vevo", "content": "Vevo is a versatile zero-shot voice imitation framework with controllable timbre and style [145], available via this link \u00b9\u2074. It can serve as a unified framework for a wide range of zero-shot speech generation tasks. As Figure 8 shows, Vevo consists of two core stages: (1) Content-Style Modeling (Content to Content-Style): Given a speech prompt as style reference, Vevo generates content-style tokens from the input content tokens (or the input text). Vevo employs the decoder-only autoregressive transformer [109, 114], leveraging its powerful capability of continued generation to model style. (2) Acoustic Modeling (Content-Style to Acoustic): Given a speech prompt as timbre reference, Vevo generates acoustic representations (such as Mel spectrograms) from the input of content-style tokens. Vevo uses a flow-matching transformer [65, 83], which has been verified to excel in in-context learning and reconstructing high-quality audio [26, 35, 55, 115], to achieve timbre-controllable generation.\nVevo Tokenizers To obtain the content and content-style tokens of speech, Vevo designs a self-supervised method to decouple the timbre, style, and linguistic content gradually, which is similar to a progressive information filtering: (1) Vevo firstly investigate the commonly used self-supervised speech pre-trained model, HuBERT [40]. Vevo find that its continuous hidden features contain rich information about timbre, style, and linguistic content, making it a suitable initial stage for information filtering. (2) Inspired by existing works for disentangling speaker-agnostic representations [46, 108, 112, 123], Vevo employ VQ-VAE [112] as a tokenizer for HuBERT to filter out timbre, resulting in content-style tokens. (3) Furthermore, Vevo propose that the vocabulary size of the VQ-VAE codebook can function as the \"width\" of the information bottleneck [87]. By reducing the vocabulary size, Vevo can narrow the bottleneck and filter out not only timbre but also significant style information, thereby obtaining content tokens. Besides, Vevo propose to reduce the consecutive duplicate units [56] of the content tokens, called duration reduction, to further remove some style patterns such as unit-level duration."}, {"title": "Debatts", "content": "Debatts is built on top of the zero-shot TTS framework with inspirations from the previous work [3", "119": ".", "109": "as the backbone. During pretraining"}, {"119": "resulting in a discrete semantic sequence Sspk. Meanwhile", "8": ".", "A1": "N from the prompt as input. The acoustic token prediction A consists of N layers. The acoustic tokens sequence of speaker prompt speech is extracted by a speech acoustic codec same as [119"}, {"A1": "N are equal, we simply add their embeddings and use them as the model input. During training, the model predict masked acoustic tokens based on"}]}