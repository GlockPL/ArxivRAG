{"title": "The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for Test Case Generation", "authors": ["Shuzheng Gao", "Chaozheng Wang", "Cuiyun Gao", "Xiaoqian Jiao", "Chun Yong Chong", "Shan Gao", "Michael R. Lyu"], "abstract": "Test cases are essential for validating the reliability and quality of software applications. Recent studies have demonstrated the capability of Large Language Models (LLMs) to generate useful test cases for given source code. However, the existing work primarily relies on human-written plain prompts, which often leads to suboptimal results since the performance of LLMs can be highly influenced by the prompts. Moreover, these approaches use the same prompt for all LLMs, overlooking the fact that different LLMs might be best suited to different prompts. Given the wide variety of possible prompt formulations, automatically discovering the optimal prompt for each LLM presents a significant challenge. Although there are methods on automated prompt optimization in the natural language processing field, they are hard to produce effective prompts for the test case generation task. First, the methods iteratively optimize prompts by simply combining and mutating existing ones without proper guidance, resulting in prompts that lack diversity and tend to repeat the same errors in the generated test cases. Second, the prompts are generally lack of domain contextual knowledge, limiting LLMs' performance in the task.\nIn this paper, we introduce MAPS, an LLM-tAilored Prompt generation method for test case generation. MAPS comprises three main modules: Diversity-guided Prompt Generation, Failure-driven Rule Induction, and Domain Contextual Knowledge Extraction. Specifically, in the Diversity-Guided Prompt Generation module, MAPS creates varied prompts by exploring diverse modification paths during the optimization process. It prevents the optimization process from converging to local optima. The Failure-driven Rule Induction module aims at identifying promising optimization direction by reflecting common failures in generated test cases, in which the reflection outputs are softly integrated into prompts based on a rule transformation method. The Domain Contextual Knowledge Extraction module aims at enriching the prompts with related domain knowledge by incorporating both in-file and cross-file context information. To evaluate the effectiveness of MAPS, we compare it with four state-of-the-art prompt optimization methods across three popular LLMs. The experimental results demonstrate that our method outperforms baseline methods by a large margin, achieving a 6.19% higher line coverage rate and a 5.03% higher branch coverage rate on average. Moreover, experiments on different LLMs show that our method can effectively find the most suitable prompt for each LLM.", "sections": [{"title": "I. INTRODUCTION", "content": "Test cases play a crucial role in validating the reliability and quality of software applications [1], [2]. By allowing developers to identify and rectify bugs and defects at the early development stage, it remarkably enhances the overall stability of the software [3]. However, manually writing test cases is a challenging and time-consuming task. Consequently, the task of test case generation, which aims at creating high-quality test cases automatically, has attracted both developers' and researchers' attention in recent years [4]\u2013[6].\nTraditional test case generation methods such as Evo- suite [7] and Randoop [4] mainly employ search-based and constraint-based techniques to craft test suites. Recent ad- vancements in deep learning have introduced many learning- based test generation approaches. For instance, AthenaTest [5] fine-tunes BART [8] on a dataset designed for test generation. A3Test [9] further incorporates assertion knowledge and a test signature verification mechanism for achieving better results. These models aim at leveraging general program- ming knowledge acquired from extensive developer-written code corpora to generate more comprehensive and meaningful tests. Recently, Large Language Models (LLMs), such as ChatGPT [10], have gained widespread adoption in various Software Engineering (SE) tasks, including test case genera- tion, and show promising results. Due to their powerful zero- shot capabilities, LLMs can be directly deployed for down- stream tasks through prompt engineering without requiring fine-tuning [11]. For example, ChatUniTest [12] harnesses the capabilities of LLMs and employs a generation-validation- repair mechanism to rectify errors in generated test cases. Yuan et al. [13] evaluate ChatGPT's performance in test case generation and enhance it through an iterative test refinement process.\nHowever, the existing LLM-based work primarily relies on human-written plain prompts, which often leads to suboptimal results since the performance of LLMs can be highly influ- enced by the prompts [14], [15]. Additionally, different LLMS"}, {"title": "II. BACKGROUND AND MOTIVATING EXAMPLE", "content": "In this work, we concentrate on black-box LLM-based Automatic Prompt Optimization (APO) [18], [24], given the widespread adoption and powerful capabilities of black-box LLMs. APO utilizes LLMs to optimize prompts by iteratively searching for the most effective ones within the discrete space of natural language. Formally, for a task, we work with a black-box model M, a small development set $D_{dev}$, a test set $D_{test}$, and a scoring function s(.). APO aims at discovering an advanced prompt p based on $D_{dev}$ from the natural language space that maximizes the performance of M on the test set $D_{test}$. The prompt p is expected to guide the model directly generate high-quality responses instead of time-consuming multi-iteration generation during test time. A typical APO framework operates as follows. First, it begins with a set of seed prompts which can be obtained either manually or through automatic techniques. Then the seed prompts are used to generate responses for $D_{dev}$ via M and the responses are evaluated using the scoring function s(.), such as the line coverage rate in test case generation. Prompts that perform well are retained, while those that do not are discarded. Using the retained prompts, the APO methods query M to generate new prompts. For example, a representative method OPRO [25] generates new prompts by prompting LLMs with the prompt \"Generate an instruction that is different from all the instructions and has a higher score than all the instructions above\". The newly generated prompts will be integrated with the retained prompts for next iteration optimization. After several iterations, the best prompt on $D_{dev}$ will be used as the final optimized prompt for $D_{test}$.\nWe first conduct a preliminary study by applying existing APO methods to real-world test case generation on Defects4J and find that it struggles to produce well-performing prompts. By analyzing its optimized prompts and generated test cases, we identify three main problems of current APO methods."}, {"title": "III. PROPOSED APPROACH", "content": "We provide an overview of MAPS's workflow in Fig. 1. MAPS starts with a set of seed prompts and augments the focal methods with both in-file and cross-file context information through the \u2460 domain contextual knowledge extraction module. In each iteration, MAPS first evaluates the performance of the current prompts on the small de- velopment set. The \u2461 diversity-guided prompt generation module then selects the top-performing prompts and infers diverse modification methods, which are used to help generate creates varied prompts. In the \u2462 failure-driven rule induction module, MAPS aggregates and selects representative failure information from failed test cases and induces concise rules to avoid such failures using a reflection-validation method. As shown in Algorithm 1, this iterative optimization process continues until reaching the maximum iteration number I.\nThe domain contextual knowledge extraction module aims to provide LLMs with related project-level context infor- mation, enabling them to generate accurate test cases. As illustrated in Fig. 2, the contextual knowledge is divided into two categories: in-file contextual knowledge and cross- file contextual knowledge."}, {"title": "B. Domain Contextual Knowledge Extraction", "content": "The domain contextual knowledge extraction module aims to provide LLMs with related project-level context infor- mation, enabling them to generate accurate test cases. As illustrated in Fig. 2, the contextual knowledge is divided into two categories: in-file contextual knowledge and cross- file contextual knowledge.\n\u2022 In-file Contextual Knowledge contains the class sig- nature, focal method, and the signatures of member methods. The class signature includes the type and name of the class containing the focal method, which could help LLMs avoid direct initialization of abstract or private classes. The focal method is the specific method to gen- erate test cases. Following previous research [5], [9], we"}, {"title": "C. Diversity-guided Prompt Generation", "content": "The diversity-guided prompt generation module aims at producing diverse prompts to foster a more comprehensive exploration of the prompt space by enforcing them to use different modification methods. As illustrated in Fig. 1\u2461 and Algorithm 2, after evaluating the performance of current prompts on the evaluation set, MAPS selects the top-K prompts with the highest average line coverage and branch coverage. Using these selected samples, MAPS first leverages the LLM M to generate N distinct modification methods for the current prompts based on a modification prompt template shown in Fig. 3 (a) (Lines 4), where $N = SIZE(P) \u2013 K$ and $SIZE(P)$ indicates the number of seed prompts, to maintain a constant prompt number following previous work [18]. These modification methods serve as diverse exploration directions within the discrete natural language search space. MAPS then leverages LLM M to generate new prompts based on each modification method sequentially (Lines 5-7). Finally, the selected prompts and the newly generated prompts are combined to serve as the new prompts for the next iteration of optimization."}, {"title": "D. Failure-driven Rule Induction", "content": "The failure-driven rule induction module aims at identifying promising optimization direction by avoiding LLMs to make recurring errors. It leverages common failures in generated test cases to identify the parts where existing prompts most need improvement and induces rules to optimize the prompt using a reflection-validation method. As shown in Fig. 13, this process contains three phases: failure information selection, error reflection, and rule validation. The details are illustrated in Algorithm 2.\n1) Failure Information Selection: To identify shortcomings in current prompts, we propose to delve into the failed test cases generated by current prompts and select their common errors. Specifically, MAPS first collects the failed test cases generated by the selected prompts SP associated with the corresponding focal method and error messages. Then, MAPS aggregates those failure information F based on the typical DBSCAN [26] clustering algorithm (Lines 8). To determine which failures to address in each iteration, we employ a weighted sampling method. The weight of each cluster is based on two factors: its size and the similarity of its failure information to handled failures H in previous iterations. A larger cluster size indicates a higher probability of the failure type, so we assign a larger weight to it. As for the similarity"}, {"title": "IV. EXPERIMENTAL SETUP", "content": "In the evaluation, we focus on the following four research questions:\nRQ1: How effective is MAPS compared with existing prompt optimization methods?\nRQ2: Is MAPS able to generate tailored prompts for dif- ferent LLMs?\nRQ3: What is the impact of each module on the perfor- mance of MAPS?\nRQ4: How does MAPS's performance vary under different experimental settings?\nTo study RQ1, we conduct a comprehensive evaluation of MAPS by comparing it with four representative prompt optimization methods on three popular LLMs. For RQ2, we assess MAPS's ability to generate LLM-tailored prompts for different LLMs by evaluating the performance of optimized prompts produced by MAPS and manually designed prompts"}, {"title": "V. EXPERIMENTAL RESULTS", "content": "To evaluate the effectiveness of MAPS in test case gen- eration, we compare it with four representative prompt op- timization methods across three popular LLMs. Tables IV- VI present the performance of MAPS along with baseline methods on Defects4J. For each method, we provide the average performance across all bugs, as well as detailed average results for each project. Based on these results, we derive the following findings.\nExisting prompt optimization methods struggle to pro- duce effective prompts for test case generation. By com- paring the performance of the basic prompt and four baseline methods, we can observe that existing methods struggle to pro- duce effective prompts for test case generation. Specifically, as shown in Table IV, the best-performing baseline, EVOPROMPT (GA), can only achieve 1.07% and 1.64% improvements over the basic prompt in line coverage and branch coverage, respec- tively. Moreover, methods like APE and OPRO even perform worse than the basic prompt in terms of line coverage, with decreases of 0.98% and 0.43%, respectively. This suggests that simply combining and mutating existing prompts is difficult to produce effective prompts for test case generation."}, {"title": "B. RQ2: LLM-Tailored Prompt Generation Evaluation", "content": "In this RQ, we study whether MAPS could generate tailored prompts for different LLMs. To achieve this, we evaluate the performance of the three final prompts obtained by three models on each model. Additionally, we also compare the prompt used in [13] to validate whether the prompt built by MAPS could outperform the manually-designed prompt. The experimental results are depicted in Table VII. The"}, {"title": "C. RQ3: Ablation Study", "content": "We conduct ablation studies to validate the effectiveness of each module in our method, i.e. domain contextual knowledge extraction, diversity-guided prompt generation, and failure- driven rule induction. The average results for each method are presented in Table VIII, with detailed results for each project available in our replication package [29].\nDomain contextual knowledge extraction. We conduct this experiment by removing the cross-file context information in the final prompt. As can be seen in Table VIII, excluding the cross-file context information dramatically degrades per- formance across all LLMs. Specifically, the branch coverage rate drops by 8.53%, 4.77%, and 2.98% on ChatGPT, Llama- 3.1, and Qwen2, respectively. These results demonstrate the effectiveness of integrating project context information to help LLMs generate accurate test cases.\nDiversity-guided prompt generation. To validate the effec- tiveness of diversity-guided prompt generation, we experiment by replacing the optimized instruction part of the final prompt with the one produced by the best baseline method. As shown in Table VIII, removing the diversity-guided prompt genera- tion leads to a consistent drop in all tasks and metrics. For example, the line coverage rate decreases by 8.21%, 1.01%, and 1.70% on ChatGPT, Llama-3.1, and Qwen2, respectively,"}, {"title": "D. RQ4: Parameter Analysis", "content": "In this section, we study how different experimental set- tings affect the performance of MAPS and baseline methods, including the number of seed prompts, the number of gener- ated prompts N, and the maximum iteration number I. As these parameters primarily influence the prompt optimization process, we report their performance on the development set in this section. In each study, we vary only the parameter under"}, {"title": "VI. DISCUSSION", "content": "To better understand how MAPS improves test case gen- eration, we present two examples of the final prompts cre- ated by MAPS and the resulting test cases from these final prompts. First, Fig 6 (a) shows the final prompt for Llama-3.1, along with the generated test case based on the focal method in Listing 1. We can find that by following the second induced rule, Llama-3.1 correctly generates a test case that uses the \u201ctry {...} catch (IllegalArgumentException e)\u201d in the test method. Second, Fig 6 (b) illustrates another example using Chat- GPT, where the focal method is taken from Listing 2. Com- pared to the incorrect test case generated by the baseline"}, {"title": "VII. RELATED WORK", "content": "Automatically discovering optimal prompts has emerged as an important challenge in the era of LLMs [19], [33]. Most ex- isting methods follow an iterative prompt optimization process. They start with a set of seed prompts and iteratively synthesize new prompt candidates, evaluating their performance to select the top ones for the next iteration. For example, APE [19] is a typical prompt optimization method that directly asks LLMs to generate variants of current prompts while maintaining their semantic meanings in each iteration. OPRO [25] further incorporates the performance information and lets the LLM generate new prompts that can enhance the test accuracy based on existing prompts and their performance. EvoPROMPT [18] is the state-of-the-art prompt optimization method that gen- erates new prompts based on evolutionary operators. It has two versions: EvOPROMPT (GA) and EvoPROMPT (DE), which use the Genetic Algorithm, and Differential Evolution, respectively. Different from those works, this paper focuses on LLM-tailored prompt optimization for test case generation and investigates improving the exploration of the search process."}, {"title": "B. Test Case Generation", "content": "Traditional methods like Randoop [4] utilize random fuzzing on unit APIs to construct prefixes that lead the unit into noteworthy states. Evosuite [7] is a search-based test generation strategy that employs evolutionary algorithms to autonomously craft test suites for Java classes aimed at improving coverage rate. A series of recent studies [5], [34], [35] have employed deep learning techniques by training models to convert target methods into their corresponding test cases or assertions. A series of recent studies [5], [36] have employed deep learning techniques for test case generation by formulating the test case generation as a neural machine translation task and train models to convert target methods into their corresponding test cases or assertions. For example, AthenaTest [5] fine-tunes BART [8] on a dataset designed for test generation. A3Test [9] further incorporates assertion knowledge and a test signature verification mechanism for achieving better results. Recently, leveraging advancements in LLMs, test case generation approaches based on LLMs have also been proposed and shown promising results. For example, CodaMOSA [37] leverages LLMs to provide example test cases for under-covered functions when search-based testing hits a coverage stall. ChatTESTER [13] incorporates ChatGPT along with an iterative test refiner to generate tests. Different from those works, our method serves as the first LLM- tailored prompt generation method for test case generation and can be further combined with existing methods to enhance their performance. Besides, our method aims at to directly avoid generating low-quality test cases with an optimized prompt instead of time-consuming multi-iteration generation and fixing during test time."}, {"title": "C. LLMs for Software Engineering", "content": "Large Language Models have recently been widely adopted for various software engineering tasks due to their impressive performance in both code generation and understanding [38]- [41]. For example, Yuan et al. [13] evaluate the performance of ChatGPT for test case generation and improve it by iterative test refiner. Gao et al. [14] investigate how to set the in-context demonstration for ChatGPT for code summarization and code generation tasks. CHATRepair [42] iteratively evaluates pro- grams on test cases and feeds the error messages to LLMs for further patch generation. Self-edit [43] utilizes compiler error messages to enhance the correctness of code generation. Li et al. [44] investigates the feasibility of slicing commer- cial black-box LLMs using medium-sized backbone models. SBLLM [45] combines search-based methods and LLMs to iteratively improve code efficiency. DeepSeek-Coder [46] is an open-source Mixture-of-Experts (MoE) code language model that achieves state-of-the-art performance across various code intelligence tasks. StarCoder 2 [47] is an advanced LLM trained in 600+ programming languages. It is trained on the Stack2 [47] dataset and natural language text from Wikipedia, Arxiv, and GitHub issues. Magicoder [48] is a recent model trained on synthetic instruction data enhanced with open- source code snippets. It proposes OSS-INSTRUCT which produces diverse and realistic instruction tuning data from open-source code snippets to address the biases typically found in synthetic data generated by LLMs."}, {"title": "VIII. CONCLUSION", "content": "In this paper, we introduced a novel automatic LLM-tailored prompt generation method MAPS for test case generation. During the optimization process, MAPS generates diverse candidate prompts to facilitate the exploration of the prompt search space and induces rules from failure cases to avoid recurring errors. Additionally, MAPS integrates various do- main contextual knowledge for generating correct test cases in practical projects. Extensive experiments on Defects4J show that MAPS outperforms existing prompt optimization meth- ods. The replicate package of this work is publicly available at https://zenodo.org/records/14287744."}]}