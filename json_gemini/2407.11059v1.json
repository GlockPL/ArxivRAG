{"title": "Was it Slander? Towards Exact Inversion of Generative Language Models", "authors": ["Adrians Skapars", "Edoardo Manino", "Youcheng Sun", "Lucas C. Cordeiro"], "abstract": "Training large language models (LLMs) requires a substantial investment of time and money. To get a good return on investment, the developers spend considerable effort ensuring that the model never produces harmful and offensive outputs. However, bad-faith actors may still try to slander the reputation of an LLM by publicly reporting a forged output. In this paper, we show that defending against such slander attacks requires reconstructing the input of the forged output or proving that it does not exist. To do so, we propose and evaluate a search based approach for targeted adversarial attacks for LLMs. Our experiments show that we are rarely able to reconstruct the exact input of an arbitrary output, thus demonstrating that LLMs are still vulnerable to slander attacks.", "sections": [{"title": "Introduction", "content": "State-of-the-art large language models (LLMs) require millions of dollars to train [Li, 2020]. Given this steep financial cost, there are strong incentives for developers to protect the reputation of their model and establish a track record of safe and trustworthy operation. Failure to do so, especially regarding harmful and offensive content generation, often results in public backlash [Milmo and Hern, 2024].\nAgainst this background, much research effort has been put in identifying the vulnerabilities of LLMs. On the one hand, adversarial inputs [Zou et al., 2023] and jailbreaks [Chao et al., 2023] may trigger unwanted output behaviours in a model. In general, generating adversarial attacks for language models is not trivial due to the discrete nature of the textual input and the large dimension of the search space [Song and Raghunathan, 2020]. For this reason, state-of-the-art methods such as ARCA are white-box in nature and rely on a heuristic search that approximates the input gradients [Guo et al., 2021; Jones et al., 2023]. Note that similar techniques are also used for benign purposes, i.e., improving the performance of large language models by optimising their prompts [Shin et al., 2020; Deng et al., 2022; Wen et al., 2024].\nOn the other hand, membership inference attacks are able to reconstruct the training set of a model by searching for high-confidence inputs [Shokri et al., 2017]. While this process might require a very large number of queries to the model and specific assumptions on the behaviour of the model on training data [Carlini et al., 2021; Mireshghallah et al., 2022], it poses a crucial threat for models trained on private data [Choquette-Choo et al., 2021]. More importantly, it shows that it is sometimes possible to reconstruct unknown inputs by optimising a surrogate metric [Zhang et al., 2022].\nIn this paper, we take a different perspective and consider direct attacks on the reputation of a LLM. For instance, let us imagine a fictitious scenario where we are the developer of TriviaLLM, a model specialising in answering quiz-like questions. After its use in some popular TV shows, the number of downloads of TriviaLLM skyrockets. However, our social media manager discovers a trend of concerned users reporting strange behaviours. As an example, a user may have the following complaint:\nOur problem as developers is that we cannot reproduce this behaviour. Why is User58 only sharing the LLM output? What was the original input? Is User58 telling the truth or engaging in an act of product defamation?\nIn general, a slander attack can be described as follows (see Figure 1). A user has access to our LLM f and can run it in inference mode for any input x yielding its corresponding output y = f(x), but cannot modify f as they do not have the technical skills or interest to do so. Whenever users encounter a problematic output y, they will likely complain publicly without revealing the input x they used. The developers are interested in reconstructing the secret input x given the public output y and the LLM f or proving that no such input exists.\nUnfortunately, reconstructing the input of an LLM from its textual output alone is not a trivial task. Indeed, a recent paper [Morris et al., 2023] claims that this form of exact inversion is only possible in the presence of additional information, namely the full probability distribution of the first output token p(y1). With such information, the authors can train an inverse model that approximates the input x = f-1(p(y1)) with moderate success. In contrast, training a text-to-text model on input-output pairs (x, y) yields a zero success rate.\nAt the same time, our main objective is to find ways to reproduce the problematic output y. As such, it is valuable to discover the presence of any input x' that triggers the output y with high probability. That way, we can validate whether our LLM f shows evidence of harmful behaviour or if the user's report was spurious. We call this more general objective weak inversion as it does not require recovering the secret input x. More specifically, our contributions are the following:\n\u2022 We identify exact inversion as a defence against slander attacks.\n\u2022 We propose weak inversion as a surrogate objective for exact inversion.\n\u2022 We solve weak inversion by searching for adversarial examples in both text space and embedding space.\n\u2022 We demonstrate empirically that searching for weak inversions does not substantially improve our ability to solve exact inversion."}, {"title": "Problem Setting", "content": "Define x = X_1X_2...X_n as the input sequence obtained by concatenating n symbols (characters, token, words) from a given alphabet x_i \u2208 A. Similarly, call y = f(x) the output sequence generated by the LLM f, with y = Y_1Y_2...Y_m consisting of symbols from the same alphabet y_i \u2208 A. Note that we assume that the LLM is deterministic here, even though they might generate different outputs given the same input x under specific temperature settings [Vaswani et al., 2017]. More specifically, we assume that f is trained to predict the likelihood $P_f(Y_i|XY_1 ... Y_{i-1})$ of the next symbol $y_i$ in the sequence. Thus, the likelihood of the full output y given the input prompt x is:\n$P_f(y|x) = \\prod_{i=1}^{m} P_f(Y_i|XY_1\u00b7\u00b7\u00b7 Y_{i-1})$\nPopular LLMs maximise the probability of y with top-k beam search or other similar heuristics [Meister et al., 2020].\nDefinition 1 (Exact Inversion). Given an input-output pair x,y with y = f(x), reconstruct the input sequence x from the language model f and the output y alone.\nPrevious work [Morris et al., 2023] claims that exact inversion is impossible without additional information. However, they only attempt to train an explicit inverted model approximating $f^{-1}$. To explore the potential benefits of using search algorithms instead, let us introduce a weaker form of inversion."}, {"title": "Illustrative Examples", "content": "The main problem with solving either inversion problems in Definitions 1 and 2 is that the probability of observing a specific y is extremely low, if not zero, for the majority of inputs x. Furthermore, many adversarial inputs x that yield high output probability $P_f(y|x)$ contain unusual sequences of symbols [Jones et al., 2023].\nFor example, consider the output sequence y =\"Your face is ugly\". Table 1 reports the GPT-2 model scores for a few candidate inputs. Note how a direct request to be insulted is less likely to produce the output than making the opposite statement \"Your face is pretty\". Furthermore, a random-looking sequence of English words and Japanese characters (last row), produced by our search algorithm, yields the highest probability of output."}, {"title": "Generating Adversarial Inputs", "content": "In this paper, we evaluate whether searching for adversarial examples yields input close to what a human user could have used to produce a given output y. Previous research on adversarial examples for language models favours white-box methods for efficiency reasons [Jones et al., 2023]. Unfortunately, those methods do not scale well to arbitrarily-long inputs. As such, we turn to more general search strategies:\n\u2022 Text-Based GA. Genetic algorithms (GA) searches over the input space by mutating a population of sequences X. Specifically, we perform probabilistic replacements, insertions, deletions and positional swaps of sequence symbols to generate new sequences x \u2208 X.\n\u2022 Embedding-Based PSO. Particle swarm optimisation (PSO) searches over the input space by perturbing sentence embeddings $emb(x) \u2208 R^d$, instead of raw sequences. In this way, we can explore a d-dimensional semantic space and rely on an embedding model to translate to and from the sequence input. In our experiments, we use the embeddings produced by a T5 autoencoder."}, {"title": "Progressive Search", "content": "While the search algorithms in Section 3 allow us to reconstruct inputs of any length, they may require a very large numbers of calls to the language model f to converge to a good"}, {"title": "Search Initialisation", "content": "As the search space for adversarial inputs is infinitely large, the choice of initialisation for both GA and PSO is crucial. Here, we focus on three main initialisation strategies:\n\u2022 Random. As a baseline, we experiment with random initialisation strategies for the population X.\n\u2022 Output Copy. Existing work on jailbreaks, shows that it is sometimes possible to get a language models to repeat an input sequence [Zou et al., 2023]. For this reason, we explore initialisation strategies that set x \u2248 y for all elements x \u2208 X of the population.\n\u2022 Inverted Model. The work of [Morris et al., 2023] trains an explicit inverted model $f^{-1}$ based on the T5 architecture. Accordingly, we initialise all x \u2208 X by sampling $x = f^{-1}(y)$."}, {"title": "Preliminary Experiments", "content": "In this section, we present our empirical evidence. Here, we want to answer the following research questions:\n\u2022 RQ1. What is the most efficient search algorithm?\n\u2022 RQ2. What is the impact of the initialisation strategy?\n\u2022 RQ3. What is the relationship between weak and exact inversion?"}, {"title": "Experimental Setup", "content": "The code to replicate our experiments is available at: https://zenodo.org/doi/10.5281/zenodo.11069036\nComputational Infrastructure. We use an NVIDIA's T4 16GB GPU for the experiments in Section 4.3 and an NVIDIA's Quadro RTX 6000 24GB GPU for the rest."}, {"title": "Search Algorithm Comparison", "content": "In Table 2, we compare text-based GA and embedding-based PSO search algorithms with both progressive and full objectives, under random initialisation. Given the vast difference in computational efficiency of these search algorithms, we terminate all of them after a given timeout of 350 minutes.\nAs expected, searching for adversarial examples improves the number of weak inversions. At the same time, text-based GA runs beats its embedding-based PSO counterpart by up to 10 absolute points. Furthermore, switching to progressive search allows both GA and PSO to explore a larger portion of the search space, albeit with an approximate objective function, thus slightly improving their weak inversion capabilities."}, {"title": "Initialisation Comparison", "content": "In Table 3, we compare our search algorithms under a variety of different initialisation strategies. For further details on strategies and additional results, see Appendix B and C. This set of experiments was run with a timeout of 200 minutes.\nOn the one hand, initialisation has a very large impact on the ability of GA and PSO to solve weak inversion. Interestingly, the most successful strategies involve copying the target output y as the input x, either verbatim (Output) or via some form of perturbation (Output synonym, Output paraphrase). Manual inspection of the generated inputs x \u2208 X show that they retain most of the target output text y. Indeed, they differ only by the insertion of additional text around y. Though the additional text is often uninterpretable, we speculate that it is optimised to prompt the model to repeat the input, thus acting as a jailbreak.\nOn the other hand, we get the best exact inversion scores by using the explicit inverted model $f^{-1}$ from [Morris et al., 2023] to initialise X. In particular, the strategy of sampling many candidate inputs from f-1 (Inversion sample) seems to improve scores relative to when greedily sampling from f-1 only once (Inversion). At the same time, any amount of search, even by the best-performing text-based GA, makes the exact inversion metrics worse. Together, these two facts suggest that the weak and exact inversion objectives are indeed correlated, but not enough to act as surrogate objective functions. We comment further on this in Section 4.4."}, {"title": "Language Model Comparison", "content": "In Figure 2, we compare the effectiveness of our search on two different LLMs over a longer time frame. We test both a small (GPT-2) and a large (LLAMA-2-Chat) LLM. Furthermore, we show the performance gain of searching with the optimal hyper-parameters (text-based progressive GA with Inversion sample initialisation) over the baseline parameters (GA with Random initialisation and full objective)."}, {"title": "Conclusions and Future Work", "content": "In this paper, we show that searching for adversarial inputs for a specific target output does not improve our ability to reconstruct the original input that cause said output. Even though the two objectives of adversarial (weak) inversion and exact inversion seem to be mildly correlated, weak inversion cannot be used as a surrogate objective in a search algorithm. In the future, we plan to define a more effective surrogate objective, which might shed light on what the minimal amount of information that is required for exact inversion."}, {"title": "GA and PSO Parameters", "content": "We made use of the 'eaSimple' implementation of the genetic/ evolutionary algorithm from the DEAP library, rather than the more specialised 'eaMuPlusLambda' or 'eaMuCommaLambda'. A single individual in the population would be represented by a variable-length list of numbers that range from 0 to the size of the target model's token vocabulary. We use the 'cxUniform' implementation of mating strategy with independent probability set to 0.3. We use a custom implementation of mutation strategy with independent probability set to 0.1. In this case, 0.1 represents the probability of each token/ number in an individual's list receiving one mutation. The available mutations are changing the token value to a random value, inserting a random token to the left of the token, deleting the token or swapping the positions of this token with another in the text. Each mutation is equally likely to be chosen except for when only one token remains in the string (in which case you cannot delete nor swap). We use the 'selTournament' implementation of the selection strategy with the explore-exploit variable of tournament size set to 15. The population size is set to 1000."}, {"title": "Initialisations", "content": "Descriptions of the strategies presented in Table 3:\n\u2022 Random. refers to randomly sampling from a uniform distribution to get a variably-long list of token IDs or a fixed-length embedding vector;\n\u2022 Output. refers to simply having the whole population start as the target output;\n\u2022 Output synonym. refers to starting with the target output after each word has been randomly replaced by one of its synonyms (which is likely to be different for each individual in the population) as provided by the WordNet corpus;\n\u2022 Output paraphrase. refers to instead getting many variations of the target output by using a T5 model fine-tuned for paraphrasing - temperature being set to 1.5, top_p being set to 0.99 and top_k being set to 500;\n\u2022 Inversion. refers to giving the target output to the Morris et al. baseline inversion model and using a single\ngreedy sample from it for the whole population - temperature being set to 0.0;\n\u2022 Inversion sample. is similar to the previous, except you repeatedly sample from the model (something which the authors did not do themselves) to get variety among the population - temperature being set to 1.0, top_p being set to 0.99 and top_k being set to 500.\nDescriptions of the strategies presented in Table 4:\n\u2022 Random dataset. refers to randomly sampling from an out-of-distribution dataset, specifically a collection of tweets made in February 2024;\n\u2022 Random fluent. refers to randomly choosing a single token and then using the target model to generate the rest of each input;\n\u2022 Random output. is similar but you instead start with the target output sequence to encourage the following text to be of a similar theme;\nNote that PSO requires an additional step of converting the described initialisation text to an embedding."}, {"title": "Additional Results", "content": "In Table 4, we compare our search algorithms under a few additional initialisation strategies (also detailed in Appendix B). Here, we explore how important it is for input text to be sampled from a distribution of syntactically-correct English, which is separate from it's semantic relevance to the target input. Text from Random dataset is 'correct' in terms of it being accepted by English readers while text from Random fluent is 'correct' in terms of it being accepted by our own LLM (i.e. it producing the text itself means that the text has a low perplexity score). Both of these perform slightly better than Random but not by much and they are equivalent in terms of input similarity metrics. However, there is a significant improvement over Random for Random output, which reaffirms previous conclusions that relevance to the target's semantics is much more important than other factors. The difference is not as significant for PSO as it is for GA, but this is also in line with previous results which showed that PSO can at most produce a few percent gain in weak inversion scores for initialisations that are not random (i.e. the worst performing ones). Either way, Random output scores still do not beat the simple Output initialisation,"}]}