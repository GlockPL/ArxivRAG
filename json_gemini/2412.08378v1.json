{"title": "HyViLM: Enhancing Fine-Grained Recognition with a Hybrid Encoder for Vision-Language Models", "authors": ["Shiding Zhu", "Wenhui Dong", "Jun Song", "Yingbo Wang", "Yanan Guo", "Bo Zheng"], "abstract": "Recently, there has been growing interest in the capability of multimodal large language models (MLLMs) to process high-resolution images. A common approach currently involves dynamically cropping the original high-resolution image into smaller sub-images, which are then fed into a vision encoder that was pre-trained on lower-resolution images. However, this cropping approach often truncates objects and connected areas in the original image, causing semantic breaks. To address this limitation, we introduce HyViLM, designed to process images of any resolution while retaining the overall context during encoding. Specifically, we: (i) Design a new visual encoder called Hybrid Encoder that not only encodes individual sub-images but also interacts with detailed global visual features, significantly improving the model's ability to encode high-resolution images. (ii) Propose an optimal feature fusion strategy for the dynamic cropping approach, effectively leveraging information from different layers of the vision encoder. Compared with the state-of-the-art MLLMs under the same setting, our HyViLM outperforms existing MLLMs in nine out of ten tasks. Specifically, HyViLM achieves a 9.6% improvement in performance on the TextVQA task and a 6.9% enhancement on the DocVQA task.", "sections": [{"title": "1. Introduction", "content": "Inspired by the remarkable achievements of large language models (LLMs), the development of multimodal large language models (MLLMs) is advancing at a rapid pace. Researchers have directed substantial efforts towards broadening the capabilities of LLMs to encompass additional modalities, resulting in significant breakthroughs in assorted vision-language tasks. Nevertheless, the challenge of fine-grained visual recognition is not adequately addressed. This shortcoming arises partly from"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Large Vision-Language Model", "content": "Driven by the tremendous success of large language models (LLMs), there has been growing interest in building end-to-end multimodal large language models. Specifically, most existing MLLMs adopt a modular structure, utiliz-"}, {"title": "2.2. High-resolution MLLMS", "content": "Directly inputting high-resolution images into visual encoders leads to high computational costs, primarily due to the quadratic complexity associated with the Transformer architecture and the substantial increase in the number of visual tokens. To alleviate this issue, existing high-resolution MLLMs can be divided into two main types. One type adopts a dynamic cropping method, cutting images into patches that are then separately input into Vision Transformers (ViTs) trained on low-resolution images. Although this method is simple and efficient, it can lead to the problem of image fragmentation, altering the original context, and disrupting positional relationships. In particular, if key information is located at the boundaries of the cropped images, it becomes difficult to encode it accurately. The other type uses a dual-encoder approach, introducing a high-resolution branch to supplement the low-resolution branch. For example, Vary and Deepseek-VL utilize the Segment Anything Model (SAM) in the high-resolution visual encoder to better capture detailed information, while MiniGemini and LLaVA-HR adopt ConvNeXt. However, the main visual encoder in these models is still CLIP-ViT, whose capabilities are limited by its low resolution. In our work, we designed a visual encoder called Hybrid Encoder specifically for the dynamic cropping strategy. Hybrid Encoder expands the ability of CLIP-ViT to process high-resolution images, solving the problem of image fragmentation and improving the model's capacity to handle high-resolution images."}, {"title": "3. Methodology", "content": "Figure 3 provides an overview of HyViLM. Initially, the image is dynamically segmented, resulting in several sub-"}, {"title": "3.1. Model Architecture", "content": "We employ a dynamic segmentation strategy to partition the input image into smaller blocks while preserving the original aspect ratio. A set of predefined high resolutions is established as follows: 336 \u00d7 672, 672 \u00d7 336, 672 \u00d7 672, 1008 \u00d7 336, and 336 \u00d7 1008. The original image $I_t$ is resized and padded to the most suitable predefined resolution, ensuring the aspect ratio is maintained, and subsequently segmented into $n_w \\times n_h$ patches, each of size 336 \u00d7 336.\nTo determine the optimal resolution, we compute the effective resolution ($Res_{eff}$) and the wasted resolution ($Res_{wasted}$) of the input image relative to each predefined resolution. The selection criterion prioritizes maximizing $Res_{eff}$ and, in cases where multiple resolutions yield identical $Res_{eff}$ values, minimizing $Res_{wasted}$. The computations are defined as follows:\n$Scale = min(\\frac{W_h}{W_i}, \\frac{H_h}{H_i}),$ (1)\n$W_s, H_s = W_i \\times Scale, H_i \\times Scale,$ (2)\n$Res_{eff} = min (W_s \\times H_s, W_i \\times H_i),$ (3)\n$Res_{wasted} = (W_h \\times H_h) - Res_{eff}.$ (4)\nHere, $W_h$ and $H_h$ represent the dimensions of the predefined high resolutions, while $W_i$ and $H_i$ denote the width and height of the input image, respectively. The resizing factor $Scale$ is calculated to determine the scaling proportion for the input image. This facilitates the derivation of the resized dimensions $W_s$ and $H_s$. Subsequently, $Res_{eff}$ is computed to ensure it does not surpass the original image's resolution, and $Res_{wasted}$ quantifies the unused resolution space. For each predefined resolution, the corresponding $Res_{eff}$ and $Res_{wasted}$ are evaluated. The predefined resolution that yields the highest $Res_{eff}$ is selected. In scenarios where multiple resolutions produce the same $Res_{eff}$, the resolution with the lowest $Res_{wasted}$ is chosen. The image is ultimately resized to the selected predefined resolution $(W_h, H_h)$, maintaining the aspect ratio, with any residual areas being padded as necessary.\nIn addition to the segmented patches, a global view of each input image is generated by resizing the image to a"}, {"title": "3.2. Hybrid Encoder", "content": "The architecture of the proposed Hybrid Encoder is depicted in Figure 3c. This architecture integrates ConvNeXt, CLIP-ViT, and CVFM modules. Both the ConvNeXt and ViT networks are divided into four stages. The final layer of each stage serves as an interaction layer, combining its output with that of the corresponding ConvNeXt stage via the CVFM module. This design enables the acquisition of fine-grained global information and effectively mitigates image fragmentation.\nIn the dynamic cropping strategy, an optimal high-resolution setting $H_h \\times W_h$ is selected. The original image $I_t$ is resized and padded to a predefined resolution while preserving its aspect ratio. As a result, a low-resolution global image and a series of sub-images $[I_{global}, I_{loc_1}, I_{loc_2},...,I_{loc_{n_w \\times n_h}}]$ are obtained for subsequent processing by the CLIP-ViT. In parallel, the original image $I_t$ is also resized and padded to a higher resolution of $(\\frac{1}{32} \\times H_h \\times \\frac{1}{32} \\times W_h)$. This high-resolution image $I_h$ is directly encoded by ConvNeXt, yielding a set of multi-level high-resolution features $F_{vh} = [F_{vh}^1, F_{vh}^2, F_{vh}^3, F_{vh}^4]$. The choice of the $(\\frac{1}{14} \\times H_h \\times \\frac{1}{14} \\times W_h)$ resolution ensures"}, {"title": "3.3. ConvNeXt-ViT Deep Fusion Module (CVFM)", "content": "To enhance the interaction between ConvNeXt and CLIP-ViT features, we propose the ConvNeXt-ViT Deep Fusion Module (CVFM). The CVFM is designed to crop the overall features extracted by ConvNeXt to match the corresponding features of CLIP-ViT, concatenate them along the channel dimension, and integrate features from various stages of ConvNeXt into the encoding process of CLIP-ViT.\nInitially, the features from each stage of ConvNeXt are resized to align with the feature dimensions of the hidden layers in CLIP-ViT. Consistent with the outcomes of dynamic cropping, the features from each stage, denoted as"}, {"title": "3.4. Training Paradigm", "content": "As shown in Figure 3, we adopted a two-stage approach in training HyViLM, which includes low-resolution pre-training and high-resolution visual instruction fine-tuning. The goal is to align visual features with the language model during the pre-training stage, followed by fine-tuning the language model in the instruction tuning stage. Specifically, during the pre-training phase, we do not use a dynamic cropping strategy; instead, we directly resize the images to 336 \u00d7 336 for $I_t$ and 768 \u00d7 768 for $I_h$. We enable CVFM and the visual projector to train while keep-"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Implementation", "content": "Datasets To achieve effective cross-modality alignment and instruction finetuning, we have gathered high-quality datasets from publicly accessible sources. Initially, we focus on pretraining the projector and interaction layers by amassing approximately 1.2M image captions. This includes 558K image-caption pairs extracted from the LLaVA-filtered CC3M dataset and 695K GPT-4V-generated captions from the ALLaVA dataset. For instruction finetuning, diverse datasets are utilized: 643K single- and multi-turn conversations (minus 21K TextCaps data) from the LLaVA dataset, in addition to 100K QA pairs from ShareGPT4V, 10K LAION-GPT-4V captions, 6K text-only multi-turn dialogues from LIMA and OpenAssistant2, and 700K GPT-4V-constructed instruction pairs from the ALLaVA dataset. To enhance OCR-related competencies, we further incorporate 28K QA pairs comprised of 10K DocVQA, 4K ChartQA, 10K DVQA, and 4K AI2D data. Overall, our datasets support about 1.5M instruction-related conversations aimed at improving image comprehension.\nTraining Settings In this study, we use the ViT-L pretrained by CLIP and ConvNeXt-L pretrained by LAION as the high-resolution visual encoder and the main architecture of Hybrid Encoder, respectively. For the language model, we utilize LLaMA3-8B-Instruct. We optimize the model for 1 epoch using the AdamW optimizer with a cosine learning rate schedule and a warmup ratio of 0.03. During the pretraining stage, we use a peak learning rate of 1e-3 and a batch size of 256, training the projection and interaction layers. In the instruction fine-tuning phase, our main peak learning rate is 1e-5 with a batch size of 64. The learning rate for the visual encoder is reduced to 2e-6 to ensure stability, while the interaction layer remains at 1e-5."}, {"title": "4.2. Evaluations", "content": "Benchmarks Following prior work, we evaluated our HyViLM on ten benchmarks. These include five general multimodal understanding benchmarks: MMBench, MMMU, MME, MathVista, and HallusionBench, as well as five document understanding-related benchmarks: TextVQA, DocVQA, AI2D, InfoVQA, and OCRBench. This selection was made to demonstrate that our model not only possesses strong fine-grained recognition abilities but also exhibits excellent general capabilities."}, {"title": "MLLMS", "content": "We compared our model with some state-of-the-art multi-language large models (MLLMs). (1) Normal Resolution MLLMs, such as LLaVA1.5, InstructBLIP, Qwen-VL, and mPLUG-Owl2. (2) High Resolution MLLMs, such as MiniGemini, CogAgent, Monkey, TextMonkey, HiRes-LLaVA-336px, LLaVA-UHD, DocOwl-1.5-Chat, DeepSeek-VL, MiniGemini-HD, and LLaVA-NeXT."}, {"title": "4.3. Main Results", "content": ""}, {"title": "General Multimodal Understanding", "content": "We evaluated HyViLM in terms of general multimodal understanding,"}, {"title": "Document Understanding", "content": "We have achieved outstanding performance on document-related leaderboards, making significant advancements in five document understanding tasks. Compared to the state-of-the-art models, HyViLM outperforms by 4.2% on TextVQA, 3.5% on DocVQA, 6.8% on AI2D, 7.3% on InfoVQA, and 6.2% on OCR-Bench. Additionally, our input tokens to the LLM remain consistent with LLaVA-NeXT, resulting in a substantial performance boost while maintaining computational complexity. It is noteworthy that MiniGemini-HD is a variant of our model that only enables shallow interactions of different visual features, highlighting the importance of deep visual feature interactions under a dynamic cropping strategy. Therefore, we can conclude that our designed Hybrid Encoder significantly enhances the model's fine-"}, {"title": "4.4. Ablation studies", "content": "In this section, we conduct ablation experiments on our model to fully validate its effectiveness. We use TextVQA, InfoVQA, MME for the ablation study."}, {"title": "Impact of Image Fragmentation", "content": "To validate our point that the dynamic slicing strategy can lead to image fragmentation, we compared our method with several alternatives: simply using dynamic slicing to improve model resolution, and a variant of our model that combines dynamic slicing with feature interaction between CLIP-ViT and ConvNeXt at the last layer. Interaction methods include channel-wise concatenation, local cross-attention, and global cross-attention. Local cross-attention means that each feature of CLIP-ViT only interacts with its corresponding feature from ConvNeXt, whereas global cross-attention means that each feature of CLIP-ViT interacts with all features from ConvNeXt. We trained using the same complete dataset. As shown in Table 3, our model significantly outperforms the dynamic slicing approach. Compared to dynamic slicing with last-layer interaction, our model also shows a noticeable advantage, which rules out the impact of ConvNeXt on model performance. It also indicates that the issue of image fragmentation occurs during the encoding process of CLIP-ViT, and that performing feature interaction after encoding offers limited performance improvement. These experiments clearly demonstrate that it's necessary to alter the internal structure of CLIP-ViT, so it becomes an integrated whole capable of encoding high-resolution images holistically."}, {"title": "The Optimal Interaction Structure", "content": "In Table 4, we conducted ablation experiments to explore the optimal interaction structure. As shown in Table 3, using only the output from the last layer of ConvNeXt for interaction features provides limited additional information and proves to be suboptimal. Therefore, we designed a multi-layer to multi-layer"}, {"title": "Ablation Study of Interaction Methods", "content": "The above ablation study demonstrates that utilizing multi-level information from ConvNeXt to interact with CLIP-ViT yields the best results. The following ablation experiments investigate different fusion methods. We designed the experiments as follows: in the CVFM, we used channel concatenation, local cross attention, global cross attention, and feature addition as fusion methods. It can be seen that the channel concatenation method produced the best results. This method requires resizing the features from different levels of ConvNeXt to match the feature dimensions of CLIP-ViT. We compared interpolation and convolution as resizing methods and found that interpolation achieves slightly better performance than convolution."}, {"title": "4.5. Qualitative Analysis", "content": "In Figure 4, we compare the prediction results of HyViLM with LLaVA-NeXT, where LLaVA-NeXT is trained with"}, {"title": "5. Conclution", "content": "In this article, we introduce a novel visual encoder, Hybrid Encoder, under a dynamic partitioning strategy. Hybrid Encoder addresses the fragmentation issue in images effectively by altering the structure of CLIP-ViT and incorporating global and fine-grained features using the CVMF approach. The model HyViLM, which applies Hybrid Encoder, achieves state-of-the-art performance in extensive experiments, including both document-related benchmarks and various general benchmarks. Notably, HyViLM is the first study to explore the collaboration of two high-resolution branches, offering a new perspective for further enhancing the fine-grained recognition capabilities of MLLMs."}]}