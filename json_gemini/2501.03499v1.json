{"title": "Can Deep Learning Trigger Alerts from Mobile-Captured Images?", "authors": ["Pritisha Sarkar", "Duranta Durbaar", "Vishal Saha", "Mousumi Saha"], "abstract": "Our research presents a comprehensive approach to leveraging mobile camera image data for real-time air quality assessment and recommendation. We develop a regression-based Convolutional Neural Network model and tailor it explicitly for air quality prediction by exploiting the inherent relationship between output parameters. As a result, the Mean Squared Error of 0.0077 and 0.0112 obtained for 2 and 5 pollutants respectively outperforms existing models. Furthermore, we aim to verify the common practice of augmenting the original dataset with a view to introducing more variation in the training phase. It is one of our most significant contributions that our experimental results demonstrate minimal accuracy differences between the original and augmented datasets. Finally, a real-time, user-friendly dashboard is implemented which dynamically displays the Air Quality Index and pollutant values derived from captured mobile camera images. Users' health conditions are considered to recommend whether a location is suitable based on current air quality metrics. Overall, this research contributes to verification of data augmentation techniques, CNN-based regression modelling for air quality prediction, and user-centric air quality monitoring through mobile technology. The proposed system offers practical solutions for individuals to make informed environmental health and well-being decisions.", "sections": [{"title": "Introduction", "content": "Addressing air pollution is essential for improving public health and preventing mil-lions of premature deaths worldwide. The rise of global air pollution due to rapidurbanization and industrialization has become a critical concern, with pollutants likeCO, CO2, NO2, and PM2.5 posing severe health risks, including respiratory diseasesand reduced lung function. With urbanization projected to reach 70% by 2050, efficient air quality monitoring is crucial. Current methods, relying on costly stationsand sensors, are inadequate. North America and Australia have lower pollution levelsthan India and Asia, while South America and Africa face higher yet underreportedpollution.Today, numerous devices and platforms are available to monitor current air pollu-tant levels which may provide route recommendations. However, there is no existingsolution that offers real-time pollutant level assessment tailored to an individual'sspecific health symptoms, identifying which pollutants present in the air are harm-ful. Our contribution addresses this gap by introducing a novel approach that usesimages captured by mobile cameras to determine real-time pollutant levels and assesstheir impact based on personal health conditions. This innovative method representsa significant advancement in environmental health monitoring.We propose using mobile camera images as a cost-effective alternative to air pollu-tion sensors to address individual air pollution-related health concerns. Utilizing datafrom the most polluted cities, we developed a predictive model to recommend whethera current location is suitable for a person's health. Our previous published research [1]is to demonstrate the effectiveness of a Modified CNN model in simultaneously predict-ing AQI along with pollutant concentrations across three metropolitan cities. Unliketraditional methods relying on historical sensor data, our current approach addresseschallenges like class imbalance and noisy data, resulting in a more robust and accuratemodel. Sensors are expensive and require high maintenance, so our model's ability torecommend suitability based on a person's symptoms is highly beneficial. Our modelpredicts pollutants in real time using image data. With the increasing availability ofportable cameras and smartphones, analyzing images for air quality metrics like PM,NO2, SO2, and CO, along with AQI, offers an efficient and affordable way to monitorair quality. This approach is gaining momentum, with growing literature on computervision and deep learning for analyzing real time image.Existing research includes a CNN-based image classifier integrated with a regres-sion model [2]. Zhou et al. [3] introduced a probabilistic dynamic causal model forPM2.5 dynamics. Deleawe et al. [4] explored machine learning for urban CO2 levelprediction. Another study [5] classified images into three types using CNN, focusingon PM2.5 concentration. A novel approach [6] used CNN to predict air pollution fromcamera images. [7] discussed two methods for image classification: one using KNNand random forest, the other using CNN. Transfer learning, effective in remote sens-ing, was shown in [8]. So the problem statement is: Can mobile camera image databe feasibly used to recommend health concerns through a single model? Our majorcontributions are:"}, {"title": "Literature Survey", "content": "Air quality forecasting has a rich history, predominantly relying on statistical andshallow machine-learning techniques [9] such as Regression [10], ARIMA [11], HMM[12], and ANN [3]. Transfer learning works well for remote sensing classification, asdemonstrated in [8], where an effective strategy involves unsupervised pre-training.This ability is validated in other research, [13], where CNNs excel in operational emer-gency scenarios for automated building damage assessment. Transfer learning workswell in aerial scene classification studies [14, 15], with fine-tuned neural networksused for feature extraction and SVM-based remote sensing image classification usinglinear and RBF kernels. Transfer learning struggles with class-imbalanced datasets,as demonstrated in [16], where a pre-trained network performed poorly in detect-ing invasive blueberry species in wetland aerial images. In her research [17], Athiraemployed deep learning models, specifically RNN, LSTM, and GRU, to predict futurePM10 levels using AirNet pollution and weather time series data, with GRU yield-ing the best results. In their study [18], Chau proposed Weather Normalized Models(WNM) based on deep learning techniques, including CNN, LSTM, RNN, Bi-RNN,and GRU, to assess air quality variations during the partial COVID-19 shutdownin Quito, Ecuador. Prediction of air quality using multimodal deep learning modelcombining a Long Short Term Memory (LSTM) and Convolutional Neural Network(CNN) in Seoul [19]. Another research work, [20], used two deep neural networks: one-dimensional CNNs and Bi-directional LSTM. This paper. Salman [21] introduced anLSTM-based weather prediction model for the Indonesian airport area, incorporatingintermediate variable signals within LSTM memory blocks, and demonstrating thatthe use of the pressure variable, enhanced predictive performance, with the multilayerLSTM model emerging as the most effective. Bekkar [22] used a hybrid CNN-LSTM"}, {"title": "System Overview", "content": "Our research paper aims to leverage historical datasets from three major regions ofBengaluru, Delhi and Tamil Nadu to predict air quality. Our approach involves asystematic series of steps represented in Figure 1:1. Data collection: We collected historical datasets for the aforementioned cities,which include various air quality parameters such as PM2.5, PM10, SO2, O3, NO2,and CO, along with corresponding timestamps.2. Image data preprocessing: The RGB images obtained were preprocessedusing statistical mechanisms and converted to numpy arrays ready to be fed as inputsto a neural network. Data augmentation was also implemented using horizontal andvertical transformations so as to analyse their effect on the obtained results.3. Modified CNN model: We employed a modified CNN model, 'HealthCam-CNN' to extract key features from the 3D arrays. This CNN model is customizedwith specific parameter values and internal mechanisms tailored for our air qualityprediction task.4. Accuracy assessment: We evaluated the performance models separately togain key insights.In the first part of our method, we used a CNN model to predict two pollutantsPM2.5 and PM10. Then we applied it to predict the remaining five pollutants fromreal-time polluted images separately. In our modified CNN model(in Figure 1 purpleboundaries part), 'HealthCamCNN' model we first predict two pollutants from images,and then predict five additional pollutants. This modified version is implemented in adashboard system, allowing users to easily assess their exposure to pollutants based ontheir symptoms and receive recommendations on whether the current environmentalconditions are suitable for them.This evaluation includes measuring the accuracy of each model in predicting airquality levels. We performed validation exercises to ensure the robustness of our modelsand predictions. Validation helps confirm the reliability of the results obtained fromboth the image and binary data models.Through this comprehensive approach, we gained insights into the effectivenessof using image data for predicting air quality levels. By comparing the accuracy ofour models, we determined the potential of image data as a valuable source for airquality predictions, in addition to traditional numerical datasets. This system overviewhighlights our research's key steps and methodologies to achieve accurate air qualitypredictions for multiple cities."}, {"title": "Data Description", "content": "We collected a dataset comprising 5455 daytime images from Bengaluru, Delhi, andTamil Nadu, paired with corresponding PM2.5 and PM10 values from 2023. Cap-tured under diverse weather conditions, the images included significant portions ofthe sky and objects at various depths, thereby facilitating air quality assessment. Thedata were categorized into 6 classes, namely Good, Moderate, Unhealthy for SensitiveGroups, Unhealthy, Very Unhealthy and Severe based on PM2.5 levels, together with"}, {"title": "Data Augmentation", "content": "Data augmentation is performed to increase the diversity and size of training datasets,which helps improve the robustness and generalization ability of machine learningmodels by exposing them to a wider range of variations and scenarios present inreal-world data."}, {"title": "Methodology", "content": "Convolution, a generalization of dot product for higher dimensions, has proved usefulfor extracting key features from images. The kernel scans the image horizontally fromleft to right and the output of a convolution operation is always a single real number.Therefore, when applied to a greyscale image, the output will always be a 2D array.Because our inputs are 3-channel RGB images, each kernel used will produce a separate2D convolution output, all of which are concatenated to obtain the Feature Mapfor the first layer. For every non-overlapping sub-sample of a predefined size of the"}, {"title": "Steps of our proposed method :", "content": "As per the Algorithm 2 (with Notation Table 3), the following steps are followed inour methodology;1. Data Compilation: We have sorted the dataset such that images of a given AQIclass are grouped. Created a structured dataset by pairing each image array withits corresponding label.2. Feature-Label Separation: Extracted features (image pixel representations) andlabels from the dataset: - Features (X): 3D arrays representing the images. - Labels(Y): Corresponding classification labels.3. Neural Network Configuration: Utilized the Keras library for building asequential neural network.4. Layer Importation: Imported essential layers such as Conv2D, MaxPooling2D,Dense, and Flatten.5. Convolutional Layers: Implemented 3 Convolutional layers, each comprising 1Conv2D layer followed by 1 MaxPooling2D layer, with 32 filters for the first layer& 64 for the later ones, each of size 3*3. We used LeakyReLU activation functionto alleviate the problem of terminating ReLU [35].6. Model branching: As shown in Figure 1 in segment 1 predict two pollutants andin segment 2 predict 5 pollutants.7. Flattening: Flattened the output from the convolutional layers,in segment 3 pre-dict two pollutants from images and then predict another 5 pollutants from thesetwo pollutant.The image processing begins with convolution operations on the input images, eachfollowed by a LeakyReLU activation layer to introduce non-linearity. Subsequently,maxpooling is applied to retain the most significant features extracted by each con-volutional kernel. This convolution and maxpooling process is iterated three times,progressively increasing the number of filters to capture more complex patterns inthe images. After convolution, the resulting kernels are flattened into vectors to facil-itate input into subsequent multilayer perceptrons (MLPs). The model architecturebifurcates into two segments: the first segment predicts two pollutants, PM2.5 andPM10, while the second segment predicts the remaining five parameters. In the thirdsegment in our modified CNN method we predict 2 pollutant from images firstly thepredict 5 pollutant from that 2 pollutant. This methodology describes the detailedsteps, including data pre-processing and CNN model training, in a more concise andorganized manner."}, {"title": "Results and Discussion", "content": "We implement a Convolutional Neural Network equipped with Max Pooling betweentwo Convolution layers and Regression layers at the end to predict the output param-eters. Given all images are RGB of size 224*224, we use 32 filters in the first layer& 64 for the following two layers, all of size 3*3, as shown in figure 1. A pool size of2*2 is used for max-pooling without any stride or padding. Adam optimizer is uti-lized to compile the model & both Mean Absolute Error (MAE) & Mean SquaredError (MSE) were used to assess the performance of the model. Each of the modelswere run for 50 epochs. The model was implemented using Tensorflow & Python andtrained on a Windows 10 machine with Intel core i5 CPU and 16gb RAM. Finally, wedeployed our model to a real-time dashboard that can display all the output parame-ters upon uploading a new image. The dashboard thus makes our work user-friendlyand practical as it facilitates air quality prediction solely from captured images withoutnecessitating the use of sensors.We explored two separate neural network models exploiting their inherent architec-tures to obtain better results for the respective parameters. The first model includedtwo segments with PM2.5 and PM10 being predicted in the first stage preceded bythe convolution block. The predicted values obtained thereby are then used as inputsto a multilayer perceptron which predicts the values of the remaining 5 parameters.The results obtained thus outperforms the second model which branches the 2 groupsof parameters after the convolution block. The resultant model not only has less over-head but also performs significantly better than the one simultaneously predicting allthe 7 parameters.The testing error values for the two-stage model have been noted in Table 4 and thebranching model in Table 5,6 respectively. It is obvious that the latter outperforms theformer, thereby establishing the branched CNN model as the superior architecture.As a result, all the output parameters range between [0,1] during training and arethen unnormalized using the inverse transform to compare the predicted values on thetest set."}, {"title": "Data augmentation results", "content": "The Mean Absolute Error obtained with our model with and without data augmenta-tion is included in Figure 2. Here, Horizontal Data Augmentation includes a horizontalreflection of the input images, thereby allowing the sky to feature in the bottom,whereas Vertical Data Augmentation does not. First of all, we note that it barelymakes any difference whether or not we include the horizontally reflected images inour training set. Therefore the assumption made in [5] to skip this method of data aug-mentation is invalid. Secondly, we make the remarkable observation that even though the original images without data augmentation always start with a much higher errorcompared to the augmented ones, the eventual value upon reaching a plateau is almostindistinguishable. Therefore we can henceforth let go of the common practice in CNNmodels of augmenting the datasets safe in the knowledge that it makes little difference to the end result. We establish this as one of the most significant results obtained in our work."}, {"title": "HealthCamCNN: A Real-Time Health Recommendation System", "content": "We have developed an innovative dashboard (the snapshots are represented in Figure 3for better understanding) that utilizes image recognition technology to assess current"}, {"title": "Conclusion", "content": "This research has successfully developed and validated a robust framework for utiliz-ing mobile camera image data in real-time air quality assessment and recommendationsystems. Key contributions include introducing an effective data augmentation methodthat expands input data size without compromising prediction accuracy. Experi-mental results demonstrate minimal differences in accuracy between original andaugmented datasets, affirming the reliability of the augmentation approach. More-over, a regression-based CNN model was specifically engineered to accurately predictpollutant levels from image data. The CNN model outperformed existing methods,showcasing its efficacy in providing precise environmental assessments. A pivotalaspect of this study is the implementation of a user-friendly, real-time dashboardthat dynamically presents the Air Quality Index and pollutant values derived frommobile camera images. Tailored recommendations on location suitability based on cur-rent air quality metrics and user health profiles enhance decision-making capabilities.Looking forward, further advancements can be explored in several areas. Incorporateuser feedback mechanisms to continually refine and improve recommendation accu-racy based on real-time user experiences. These future directions aim to advance thecapabilities of mobile-based air quality monitoring systems, ultimately contributingto improved environmental health management and decision support for individualsand communities."}, {"title": "Declarations", "content": "All authors have read, understood, and have complied as applicable with the statementon \"Ethical responsibilities of Authors\" as found in the Instructions for Authors\n\u2022 Ethics approval - NA\n\u2022 Consent to participate Informed consent was obtained from all individualparticipants included in the study.\n\u2022 Funding - No funding was obtained for this study\n\u2022 Consent for publication - All participants/Authors have consented to the submissionof the case report to the journal.\n\u2022 Availability of data - NA"}]}