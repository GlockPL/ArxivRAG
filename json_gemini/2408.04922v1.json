{"title": "UAV-Enhanced Combination to Application:\nComprehensive Analysis and Benchmarking of a\nHuman Detection Dataset for Disaster Scenarios", "authors": ["Ragib Amin Nihal", "Benjamin Yen", "Katsutoshi Itoyama", "Kazuhiro Nakadai"], "abstract": "Unmanned aerial vehicles (UAVs) have revolutionized search and res-\ncue (SAR) operations, but the lack of specialized human detection datasets for\ntraining machine learning models poses a significant challenge. To address this\ngap, this paper introduces the Combination to Application (C2A) dataset, syn-\nthesized by overlaying human poses onto UAV-captured disaster scenes. Through\nextensive experimentation with state-of-the-art detection models, we demonstrate\nthat models fine-tuned on the C2A dataset exhibit substantial performance im-\nprovements compared to those pre-trained on generic aerial datasets. Further-\nmore, we highlight the importance of combining the C2A dataset with general\nhuman datasets to achieve optimal performance and generalization across vari-\nous scenarios. This points out the crucial need for a tailored dataset to enhance\nthe effectiveness of SAR operations. Our contributions also include developing\ndataset creation pipeline and integrating diverse human poses and disaster scenes\ninformation to assess the severity of disaster scenarios. Our findings advocate for\nfuture developments, to ensure that SAR operations benefit from the most realis-\ntic and effective AI-assisted interventions possible.", "sections": [{"title": "Introduction", "content": "The advancement of UAVs, colloquially known as drones, has signaled a new era in the\nfield of emergency response and disaster management. With their unparalleled agility\nand ability to provide an aerial perspective, drones have rapidly become indispensable\nassets in the arsenal of SAR operations worldwide. These technological marvels signifi-\ncantly improve the efficiency and effectiveness of missions aimed at locating and aiding\npeople in disaster-hit areas [13]. Drones can have a significant impact on minimizing\nthe aftermath of disasters through time efficiency, making a difference in survival and\nfatality rates.\nDespite these advancements, a major shortcoming exists in the deployment of drone\ntechnologies-particularly in the area of object detection via drone vision. Existing\ncomputer vision or drone vision systems significantly depend on datasets to train detec-\ntion algorithms. However, these datasets are primarily designed for general situations\nand do not adequately address the specialized and intricate requirements of disaster con-\ntexts. The shortage of disaster detection datasets is mostly owing to logistical and ethical\nobstacles in capturing and annotating real events, which need substantial resources and\noften involve sensitive circumstances. The ethical dilemmas of capturing vulnerable"}, {"title": "Literature Review", "content": "Robust datasets are fundamental to developing and training precise machine learning\nmodels. The importance of datasets tailored to specific application domains is well es-\ntablished in the literature. Studies such as those by [1] and [27] emphasize the need for\ndatasets that encompass the intricacies of various disaster scenarios. These resources\nare pivotal for calibrating UAV-operated detection systems to recognize human subjects\nunder a multitude of conditions. Yet, there remains a scarcity of datasets that accurately\nmirror the complexities of disaster-hit environments.\nDebris and destruction frequently obscure human subjects in disaster zones, biasing\nUAV detection. According to the literature, existing datasets and models are progress-\ning, but they fall short of providing the granular detail required for reliable detection\nin such complex circumstances [19,18]. A significant body of research, including work\nby [11], emphasizes the pressing need for advancing UAV technology to navigate these\nobstacles adeptly. Yet, the development of datasets that reflect the reality of partial oc-\nclusions in disaster contexts is still in its early stages, indicating a pivotal area for future\nresearch.\nThe evolution of machine learning and computer vision has been enhancing to UAV\ncapabilities. These advancements have paved the way for more nuanced data analysis,\ncrucial for discerning human presence within complex terrains. Even with these im-\nprovements, the research shows that algorithms and models still need to be improved,\nespecially to make human detection more reliable [12].\nOne notable effort in this direction is the Search and Rescue Drone (SARD) dataset [25],\nwhich focuses on human detection in search and rescue operations using drone imagery.\nThe SARD dataset includes images of people in various poses simulating exhausted or"}, {"title": "Dataset Creation Pipeline", "content": "We developed a systematic pipeline to produce a comprehensive set of images for train-\ning machine learning models to detect humans in disaster scenarios. The dataset com-\nbines parts of the Aerial Image Dataset for Emergency Response Applications (AIDER)\nand the LSP/MPII-MPHB dataset. It shows a variety of human poses on a range of dis-\naster backgrounds."}, {"title": "Data Sources and Composition", "content": "AIDER (Aerial Image Dataset for Emergency Response Applications): The AIDER\ndataset [14] serves as the foundation for the disaster scene backgrounds. It comprises\nimages from four major disaster types: Fire/Smoke (320 images), Flood (370 images),\nCollapsed Building/Rubble (320 images), and Traffic Accidents (335 images). These\nauthentic disaster images offer a realistic portrayal of the chaotic and unpredictable\nconditions typical in emergency scenarios. We did not utilize the 1,200 normal case\nimages to keep the focus on emergency situations. This dataset offers a glimpse into the\nchaotic and unpredictable environments that characterize disaster scenes, making it an\nideal choice for our purposes.\nLSP/MPII-MPHB (Multiple Poses Human Body): For the human subjects, we sourced\nimages from the LSP/MPII-MPHB dataset [3,10], which contains 26,675 images fea-\nturing 29,732 instances of human bodies in various poses. This dataset is specifically\ndesigned to capture a wide range of human body positions, including bent, kneeling,\nsitting, upright, and lying, providing the necessary diversity to train models for detect-\ning humans under different conditions. The detailed annotations of human poses in this\ndataset are critical for training models to recognize human figures in complex disaster\nenvironments."}, {"title": "Pipeline Steps", "content": "Background Removal and Image Preparation: Using the U2Net segmentation\nmodel [23], we isolated human figures from the LSP/MPII-MPHB dataset by remov-\ning the background. The U2-Net, short for \"U-Squared Net,\" is a deep neural network\nknown for its powerful performance in salient object detection and image segmentation\ntasks. It employs a nested U-structure that enhances the learning of local and global\nfeatures within images, enabling precise segmentation of objects, including human fig-\nures, from their backgrounds. This process involved saving each figure with its respec-\ntive pose in a separate folder, ensuring that the focus remained on the human subject\nwithout any background distractions."}, {"title": "Properties of C2A Dataset", "content": "The C2A (Combination to Application) dataset is a curated collection specifically de-\nsigned for advancing human detection disaster scenarios by combining AIDER dataset\nimages (disaster scene backgrounds) and diverse human poses from the LSP/MPII-\nMPHB dataset. Comparison\nof various datasets is shown In this section, we present a comprehensive\nanalysis of the dataset's properties."}, {"title": "Number of Images and Image Size", "content": "In the C2A dataset, the total number of images is 10, 215, encompassing over 360,000\nobjects for human detection within disaster scenarios. The original size of the images\nspans a wide range from approximately 123 \u00d7 152 pixels to high-resolution images of\n5184 x 3456 pixels. This range is significantly broader than what is commonly found in\nstandard datasets like PASCAL VOC or MSCOCO, where the image dimensions gener-\nally do not exceed 1000 \u00d7 1000 pixels. The wide range of resolution in the C2A dataset\nensures the inclusion of various granular details necessary for the precise detection of\nhumans in diverse and challenging disaster environments. Furthermore, the most com-\nmon image width range within the C2A dataset is between 322 and 600 pixels, with\nover 50.32% of images falling within this range. The median image width is noted at\n428 pixels, indicative of the dataset's central tendency toward mid-range resolutions.\nThe dataset preserves the integrity of the scenes and avoids potential complications that\nmay arise from segmenting an instance across multiple image pieces."}, {"title": "Objects Size", "content": "In our C2A dataset, the pixel size of objects is distributed across a broad spectrum,\naccommodating the real-world variability in human sizes from an aerial perspective.\nSpecifically, we observe that a substantial 47% of instances are under 10 pixels, indica-\ntive of individuals who appear extremely small due to the altitude of the imagery. This\nreflects realistic scenarios where people are often tiny and challenging to detect. The\ndataset also contains 52% of instances in the range of 10-50 pixels and a minimal 1%\nwithin the 50-300 pixel bracket. There are no instances above 300 pixels, reinforcing\nthe dataset's focus on detecting smaller objects. It is challenging for the models to\ndetect the objects that are in tiny size."}, {"title": "Aspect Ratio of Objects", "content": "The aspect ratio (AR) is a critical parameter in anchor-based detection models, influenc-\ning the design and effectiveness of detectors like Faster R-CNN and YOLO series. In the\nC2A dataset, we analyze the AR of the minimally circumscribed horizontal bounding\nboxes encompassing each object. The distribution is skewed towards smaller ARs, with the majority\nof objects having an AR less than 1. This suggests that most bounding boxes are wider\nthan they are tall, a likely scenario when dealing with collapsed individuals or those in\nhorizontal positions in disaster scenarios. A noticeable amount of instances have ARs\nbetween 1 and 2, aligning with natural human proportions when standing or sitting.\nVery few instances possess a high AR, which is expected as elongated bounding boxes\nwould be less common unless representing individuals in highly unusual orientations\nor in motion."}, {"title": "Object Density of Images", "content": "Aerial image datasets often exhibit a far greater number of objects per image when com-\npared to datasets composed of natural images. Typical datasets, such as ImageNet, have\nan average of 2 objects per image, while MSCOCO averages 7.7. In stark contrast, our\nC2A dataset showcases a higher object density, reflective of the real-world complexity\nfound in disaster-stricken environments.\nThe histogram outlines the frequency of object instances per im-\nage within our dataset. The distribution peaks significantly around 20 to 40 objects,\nwith a notable extension towards images containing up to 100 instances. This dense\ndistribution is a testament to the C2A dataset's capacity for providing a challenging and\nenriched learning context for object detection algorithms, pushing the envelope of their\ndetection and discrimination capabilities."}, {"title": "Human Pose and Disaster Scene Information", "content": "In the pursuit of advancing SAR operations through machine learning, our C2A dataset\noffers more than object detection; it integrates critical contextual data by providing"}, {"title": "Evaluation", "content": null}, {"title": "Evaluation Metrics", "content": "The evaluation of object detection models was conducted using the mean Average\nPrecision (mAP) [22], a prevalent metric that integrates both precision and recall as-\npects of the predictions. Precision, defined as $Precision = \\frac{TP}{TP+FP}$, measures the\ncorrectness of the predictions, while recall quantifies the model's ability to identify\nall relevant instances. The mAP is the mean of Average Precision (AP) across all\nclasses, computed for varying Intersection over Union (IoU) thresholds, typically rang-\ning from 0.5 to 0.95. The AP at a specific IoU threshold is the area under the precision-\nrecall curve. The mAP at IoU threshold of 0.5, denoted as mAP@.50, is represented\nas $mAP@.50 = \\sum_{i=1}^{N}AP_{i}|_{IOU=0.5}$, highlighting a model's proficiency in detecting\nobjects with a moderate overlap with the ground truth."}, {"title": "Training Options", "content": "The evaluation of the models on the C2A dataset was conducted using NVIDIA A100\nGPUs, with a uniform batch size of 24 and an image resolution of 640x640 pixels across\n50 epochs. The ADAM optimizer was chosen for its efficiency in handling large datasets\nand complex image structures. Basic data augmentation techniques, such as flipping\nand resizing, were employed to enhance model robustness and prevent overfitting. The\nexperiments were facilitated by popular deep learning frameworks, specifically mmDe-\ntection [6], Detectron2 [32], and Ultralytics [9], known for their high performance in\nobject detection tasks. These frameworks provide extensive support for custom dataset\ntraining, enabling the effective application of state-of-the-art detection models to our\nspecialized dataset."}, {"title": "Benchmarking", "content": "The C2A dataset was subjected to a rigorous evaluation process using a suite of state-\nof-the-art object detection models. These evaluations aimed to benchmark the dataset's"}, {"title": "Result Analysis", "content": "The evaluation results demonstrate a range of performance metrics across different\nmodels, reflecting the diverse strengths of each approach. YOLOv9-e outperformed\nother models with the highest mAP (mean Average Precision) score, indicating its su-\nperior ability to detect objects with a high degree of accuracy across varying Intersec-\ntion over Union (IoU) thresholds. This suggests that the architectural improvements in\nYOLOv9, particularly for detecting small and partially occluded objects, are beneficial\nin the context of disaster scenarios.\nOn the other hand, Faster R-CNN and RetinaNet, while offering competitive perfor-\nmance, particularly at the AP50 metric, fell short of the YOLO models. Dino and\nCascade R-CNN showed substantial performance, with Cascade R-CNN achieving the\nsecond-highest mAP score, indicating its effectiveness in handling complex object re-\nlationships, likely due to its multi-stage detection process.\nThe analysis of AP50 scores, which are based on a lower IoU threshold, reveals that\nmost models perform significantly better when the requirement for the overlap between\npredicted and ground truth bounding boxes is relaxed. This discrepancy suggests that\nwhile the models are capable of identifying the presence of objects, refining the ac-\ncuracy of bounding box predictions remains a challenge and an area for potential im-\nprovement in future research iterations."}, {"title": "Discussion", "content": null}, {"title": "Model Optimization for Complex Disaster Scenarios", "content": "To investigate the impact of domain-specific training on model performance in com-\nplex disaster scenarios, we conducted a comparative analysis using several datasets and\na model [20]: C2A (synthetic disaster scenes), SARD (real-world search and rescue\nimages), and \"General Human Detection\" (a combination of crowd human [26], tiny\nperson [34], and VisDrone [7] datasets). By training models on these datasets and eval-\nuating their performance across different validation sets, we aimed to identify the most\neffective approach for detecting humans in challenging disaster environments."}, {"title": "Object Size and Detection Confidence", "content": "In-depth analysis of detection performance reveals a notable size bias where smaller ob-\njects (less than 20 pixels) are detected with less frequency and lower confidence scores.\nThis trend, observable in Fig. 3(a), points to a potential size-dependent limitation in-\nherent in current detection algorithms. Conversely, larger objects demonstrate higher\ndetection confidence, as seen in Fig. 3(b), where the mean confidence score, repre-\nsented by red points, scales with object size. This size-detection relationship suggests\nan avenue for model improvement\u2014specifically, enhancing the sensitivity of detection\nalgorithms to smaller objects could significantly improve performance in complex dis-\naster environments, where small-scale features can be critical."}, {"title": "Dataset Limitations and Prospects for Improvement", "content": "The C2A dataset, while effective, encounters limitations due to its synthetic nature.\nThe overlay of human figures from the LSP/MPII-MPHB dataset onto disaster scenes\ncan sometimes result in unrealistic scaling and positioning, potentially compromising\nthe model's ability to generalize to real-world scenarios. Interestingly, this element of\nunrealism could also serve as a form of data augmentation, introducing variability that\nmay help in training more robust and generalized models. Despite this, it is better to\nhave context-aware adaptive scaling and improved spatial algorithms to enhance the\nrealism of the training images. Moreover, transitioning to dynamic 3D models could\nmore accurately depict human movement, overcoming the static nature of 2D images.\nAnother limitation of the C2A dataset is that it consists of single images, whereas in\nmost actual disaster scenarios, the input data could be in the form of video footage.\nThis discrepancy between the training data and real-world application data may impact"}, {"title": "Conclusion", "content": "Our research presents the C2A dataset as a pivotal resource for enhancing machine\nlearning models' performance in UAV-assisted SAR operations within disaster-stricken\nenvironments. Despite its synthetic origins, the dataset serves as a critical step toward\ndeveloping more accurate and robust detection systems. We have demonstrated that\ncombining the C2A dataset with general human datasets leads to improved performance\nand generalization across various scenarios, highlighting the importance of integrating\ndomain-specific and general data for effective SAR operations. The comparative anal-\nysis of model performance across different datasets underscores the value of the C2A\ndataset in enhancing the operational efficacy of search and rescue missions. However,\nwe also acknowledge the limitations of relying solely on synthetic data and the need for\nincorporating real-world examples to increase confidence in the model's real-world ap-\nplicability. The limitations identified through our study provide a roadmap for future im-\nprovements, particularly in terms of scaling and positioning to mimic the unpredictable\nnature of real-world scenarios. We foresee the incorporation of actual disaster footage\nand advanced contextual understanding as key avenues for the next phase of dataset de-\nvelopment. As the field progresses, the ultimate goal remains clear: to refine these tools\nand datasets to ensure that they can be relied upon when they are most needed to save\nlives and navigate the complexities of disaster response with unparalleled precision and\nreliability."}]}