{"title": "SYMBOLIC STATE PARTITION FOR REINFORCEMENT LEARNING", "authors": ["Mohsen Ghaffari", "Einar Broch Johnsen", "Mahsa Varshosaz", "Andrzej W\u0105sowski"], "abstract": "Tabular reinforcement learning methods cannot operate directly on continuous state spaces. One\nsolution for this problem is to partition the state space. A good partitioning enables generalization\nduring learning and more efficient exploitation of prior experiences. Consequently, the learning\nprocess becomes faster and produces more reliable policies. However, partitioning introduces\napproximation, which is particularly harmful in the presence of nonlinear relations between state\ncomponents. An ideal partition should be as coarse as possible, while capturing the key structure of\nthe state space for the given problem. This work extracts partitions from the environment dynamics\nby symbolic execution. We show that symbolic partitioning improves state space coverage with\nrespect to environmental behavior and allows reinforcement learning to perform better for sparse\nrewards. We evaluate symbolic state space partitioning with respect to precision, scalability, learning\nagent performance and state space coverage for the learnt policies.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning is a form of active learning, where an agent learns to make decisions to maximize a reward\nsignal. The agent interacts with an environment and takes actions based on its current state. The environment rewards\nthe agent, which uses the reward value to update its decision-making policy. Reinforcement learning has applications\nin many domains: robotics [1], gaming [2], electronic [3], healthcare [4]. This method can automatically synthesize\ncontrollers for many challenging control problems [5], however dedicated approximation techniques, hereunder deep\nlearning, are needed for continuous state spaces. Unfortunately, despite many spectacular success with continuous\nproblems, Deep Reinforcement Learning suffers from low explainability and lack of convergence guarantees. At the\nsame time discrete (tabular) learning methods have been shown to be more explainable [6, 7, 8, 9] and to yield policies\nfor which it is easier to assure safety [10, 11, 12]; for instance using formal verification [13, 14, 15]. Thus, finding\na good state-space representation for discrete learning remains an active research area [16, 17, 18] [19, 20, 21, 22].\nTo adapt a continuous state space for discrete learning, one exploits partial observability, and merges regions of the state\nspace into discrete partitions, each representing a subset of the states of the agent. Ideally, all states in a partition should\ncapture meaningful aspects of the environment-best if they ensure the Markov property for the environment dynamics.\nConsequently, a good partitioning depends on the problem at hand. For instance, in safety critical environments,\nit is essential to identify small \u201csingularities\u201d\u2014regions that require special handling\u2014even if they are very small.\nOtherwise, if such regions are included in larger partitions, the control policy will not be able to distinguish them from\nthe surrounding, leading to high variance at operation time and slow convergence of learning."}, {"title": "2 Background", "content": "Reinforcement Learning. A Partially Observable Markov Decision Process is a tuple $\\mathcal{M} = (\\mathcal{S},\\mathcal{S}_0,\\mathcal{A},\\mathcal{S},\\mathcal{O},\\mathcal{T},\\mathcal{R},\\mathcal{F})$, where $\\mathcal{S}$ is a set of states, $\\mathcal{S}_0 \\in \\text{pdf } \\mathcal{S}$ is a probability density function for initial states, $\\mathcal{A}$ is a finite\nset of actions, $\\mathcal{S}$ is a finite set of observable states, $\\mathcal{O} \\in \\mathcal{S} \\rightarrow \\mathcal{S}$ is a total observation function, $\\mathcal{T} \\in \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\text{pdf } \\mathcal{S}$\nis the transition probability function, $\\mathcal{R} \\in \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is the reward function, and $\\mathcal{F} \\in \\mathcal{S} \\rightarrow \\{0,1\\}$ is a predicate\ndefining final states. The task is to find a policy $\\pi : \\mathcal{S} \\rightarrow \\text{Dist}(\\mathcal{A})$ that maximizes the expected accumulated reward [5].\nExample 1. Consider a room with dimensions $W\\times H$, a mouse in its bottom-left corner, a mousetrap in the bottom-right\ncorner, and a piece of cheese next to the mousetrap (Fig. 1). The mouse moves with a fixed velocity in four directions:\nup, down, left, right. Its goal is to find the cheese but avoid the trap. The states $\\mathcal{S}$ are ordered pairs representing the\nmouse's position in the room. The set of initial states $\\mathcal{S}_0$ is fixed to (1, 1), a Dirac distribution. We define the actions"}, {"title": "3 Partitioning Using Symbolic Execution", "content": "We present the idea using a single agent with the environment modeled as a computer program. The program (Env) is\nimplementing a single step-transition (T) in the environment with the corresponding reward (R). We use symbolic\nexecution to analyze the environment program Env, then partition the state space using the obtained path conditions.\nThe partitioning serves as the observation function O. The entire process is automatic and generic\u2014we follow the same\nprocedure for all problems.\nExample 2. Figure 3(a) shows the environment program for the 10 \u00d7 10 navigation problem (Example 1). For simplicity,\nwe assume the agent can move one unit in each direction, so $V = \\{1\\}$ and $\\mathcal{A} = \\{U, D, R, L\\} \\times V$. The path conditions\nin Fig. 3(b) are obtained by symbolically executing the step and reward functions using symbolic inputs x and y and\na concrete input from A. Using path conditions in partitioning requires a translation from the symbolic executor syntax\ninto the programming language used for implementing partitioning, as the executor will generate abstract value names.\nA good partitioning maintains the Markov property, so that the same action is optimal for all unobservable states\nabstracted by the same partition. Unfortunately, this means that a good partitioning can be selected only once we know a\ngood policy-after learning. To overcome this, SymPar heuristically bundles states into the same partition if they induce\nthe same execution path in the environment program. We use an off-the-shelf symbolic executor to extract all possible\npath conditions from Env, by using $\\mathcal{S}$ as symbolic input and actions from $\\mathcal{A}$ as concrete input. The result is a set PC of\npath conditions for each concrete action: $\\text{PC} = \\{\\text{PC}_{a_0},\\text{PC}_{a_1},...,\\text{PC}_{a_m }\\}$, where $\\text{PC}^{a}_{j} = \\{\\text{PC}^{a_j}_0,\\text{PC}^{a_j}_1,..., \\text{PC}^{a_j}_{k_a}\\}$.\nThe set PC contains the path conditions computed for action a, and $k_a$ is the number of all path conditions obtained\nby running Env symbolically, for a concrete action a.\nRunning the environment program for any concrete state satisfying a condition $\\text{PC}^{a}_{j}$ with action a will execute the same\nprogram path. However, the partitioning for reinforcement learning needs to be action independent (the same for all\nactions). So the path conditions cannot be used as partitions, as they are. Consider $\\text{PC}^{a_1}_{j_1} \\in \\text{PC}^{a_1}$ and $\\text{PC}^{a_2}_{j_2} \\in \\text{PC}^{a_2}$,\narbitrary path conditions for some actions $a_1, a_2$. To make sure that the same program path will be taken from a\nconcrete state for both actions, we need to create a partition that corresponds to the intersection of both path conditions:\n$\\text{PC}^{a_1}_{j_1} \\cap \\text{PC}^{a_2}_{j_2}$. In general, each set in PC defines a partitioning of the state space for a different actions. To make them\ncompatible, we need to compute the unique coarsest partitioning finer than those induced by the path conditions for"}, {"title": "4 Evaluation Setup", "content": "The partitioning of the state space faces a trade-off: on one hand, the granularity of the partitioning should be fine\nenough to distinguish crucial differences between states in the state space. On the other hand, this granularity should\nbe chosen to avoid a combinatorial explosion, where the number of possible partitions becomes unmanageably large.\nAchieving this balance is essential for efficient and effective learning. In this section, we explore this trade-off and\nevaluate the performance of our implementation in SymPar empirically. To this aim, we address the following research\nquestions:\nRQ1 To what extent does SymPar produce smaller partitionings than other methods and how do\naffect the performance of learning?\nRQ2 How does the granularity of the partitioning affect the learning performance?\nRQ3 How does SymPar scale with increasing state space sizes?\nthese partitionings\nWe compare SymPar with CAT-RL [22] (online partitioning) and with tile coding techniques (offline partitioning)\nfor different examples [5]. Tile coding is a classic technique for partitioning. It splits each feature of the state into\nrectangular tiles of the same size. Although there are new problem specific versions, we opt for the classic version\ndue to its generality.\nTo answer RQ1, we measure (a) the size of partitioning, (b) the failure and success rates and (c) the accumulated\nreward during learning. Being offline, our approach is hard to compare with online methods, since the different\nparameters may affect the results. Therefore, we separate the comparison with offline and online algorithms. For\noffline algorithms, we first find the number of abstract states using SymPar and partition the state space using tile\ncoding accordingly (i.e., the number of tiles is set to the smallest square number greater than the number of partitions\nfrom SymPar). Then, we use standard Q-learning for these partitionings, and compare their performance. For online\nalgorithms, we compute the running time for SymPar and its learning process, run CAT-RL for the same amount"}, {"title": "5 Results", "content": "RQ1. Tables 1 and 2 show that SymPar consistently outperforms both tile coding (offline) and CAT-RL (online)\non discrete state space in terms of success and failure rates, and reduced number of timeouts (Tout) during learning.\nAlso, the agents using SymPar partitionings obtain maximum reward more often than with tiling (Opt), cf. Tbl. 1.\nNote that in Tbl. 1, the size of partitionings is substantially biased in favour of tiling. Nevertheless, SymPar enables\nbetter learning. In Tbl. 2, CAT-RL obtains smaller partitionings in the first and third cases in the same amount of time\nas SymPar but the quality of learning is worse. For the other test problems, SymPar is better than CAT-RL in both\nthe size and learning performance.\nFor randomly selected states, the three top\nplots in Fig. 6, show that the agents trained by\nSymPar obtain a better normalized cumulative\nreward and subsequently converge faster\nto a better policy than the best competing\napproaches (other results, cf. Appendix, Fig. 9).\nThe three bottom plots in the figure show the ac-\ncumulated reward when starting from unlikely\nstates (small partitions) for the best competing"}, {"title": "6 Appendix / Supplementary Material", "content": "6.1 Technical Details.\nThe implementation of SymPar uses Symbolic PathFinder\u00b2 [25] as its symbolic executor, Z33 [46] as its main SMT-Solver and the SMT-solver DReal\u2074 [47] to handle non-linear functions such as trigonometric functions.\n6.2 Specification of SymPar\nHere, we explain how to create an example in our setting."}]}