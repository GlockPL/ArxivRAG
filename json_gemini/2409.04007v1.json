{"title": "Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition", "authors": ["Byunggun Kim", "Younghun Kwon"], "abstract": "Speech emotion recognition (SER) classifies human emotions in speech with a computer model. Recently, performance in SER has steadily increased as deep learning techniques have adapted. However, unlike many domains that use speech data, data for training in the SER model is insufficient. This causes overfitting of training of the neural network, resulting in performance degradation. In fact, successful emotion recognition requires an effective preprocessing method and a model structure that efficiently uses the number of weight parameters. In this study, we propose using eight dataset versions with different frequency-time resolutions to search for an effective emotional speech preprocessing method. We propose a 6-layer convolutional neural network (CNN) model with efficient channel attention (ECA) to pursue an efficient model structure. In particular, the well-positioned ECA blocks can improve channel feature representation with only a few parameters. With the interactive emotional dyadic motion capture (IEMOCAP) dataset, increasing the frequency resolution in preprocessing emotional speech can improve emotion recognition performance. Also, ECA after the deep convolution layer can effectively increase channel feature representation. Consequently, the best result (79.37UA 79.68WA) can be obtained, exceeding the performance of previous SER models. Furthermore, to compensate for the lack of emotional speech data, we experiment with multiple preprocessing data methods that augment trainable data preprocessed with all different settings from one sample. In the experiment, we can achieve the highest result (80.28UA 80.46WA).", "sections": [{"title": "I. INTRODUCTION", "content": "SPEECH emotion recognition (SER) is the technique in which a computer can recognize the inherent emotional features of a human's speech signal [1]. In particular, interest in the human-computer interaction (HCI) systems has arisen [2], and SER has noticed that some applications, such as psychotherapy [3] and consultation calls [4], require emotional labor. However, it is hard to understand how human speech emotions can be represented in terms of the exact values using common standards. This is because each person's method of recognizing emotions differs depending on their personality and culture [5]. The ambiguity of human emotions is one of the main challenges in developing an accurate SER model [6].\nTherefore, many studies have attempted to use the deep learning-based models [7]-[11], that can be trained directly from the emotional speech data. Among them, in recent, the convolutional neural network (CNN) based models trained with speech spectral features are proposed [12]\u2013[18]. A CNN-based model has two advantages. First, owing to the convolutional layer's weight sharing, a relatively small number of trainable parameters can be used. Second, a deep CNN layer model makes it possible to learn global context features using filters of only a small size [19]\u2013[21]. For the SER, it is essential to learn the linguistic features and the paralinguistic features of speech [22]. Therefore, the CNN model, which can learn the context of emotional speech utterances, performs better than the other structures.\nHowever, to learn the overall context of the input data with a CNN-based model, a sufficient number of deep layers must be stacked, or a larger filter kernel must be used. Therefore, attention modules have been proposed to cover the CNN layer's weakness for the SER [23]-[28]. Xu et al. [25] proposed a multiscale area attention, which applies the transformer-type attention mechanism [29] to the CNN-based model. This significantly improves the recognition per- formance by dividing the time-frequency spatial information into granular perspectives. Guo et al. [27] proposed spectral temporal channel attention, which is a modified version of bottleneck attention module (BAM) [30]-[32]. Therefore, it used not only focus on spatial features but also attention to channel features. In addition, it has an independent attention learning structure in all the axes of the input features. However, channel attention requires more learning parameters than spa- tial attention because of the two multi-layer perceptron (MLP) layers.\nMore trainable parameters are required when examining the attention structure and considering the more diverse aggre- gated input features [33]. However, an increase in trainable parameters causes overfitting problems when trainable samples are leaked, such as in SER [34]. Therefore, in this study, we search for an efficient attention structure that can improve emotional feature expression with only a few learning param- eters while maintaining a deep CNN-based model. Through experiments, we observe that CNN's channel features are essential for emotion classification performance. Therefore, the efficient channel attention (ECA) [35] that can learn how to focus on the important channel features is applied"}, {"title": "II. RELATED WORKS", "content": "Many different attention methods have been proposed. In this section, we present an overview of the development flow of CNN-based models using several attention methods. We divide contents whether the recurrent neural network (RNN) is used or not in the CNN-based model."}, {"title": "A. CNN-RNN Models with Attention Mechanism", "content": "The CNN can effectively learn local spatial features from the data. It is also possible to learn the global spatial features, such as the context of the data when stacking multiple CNNs. However, if the model stacks more layers, its complexity in- creases. In the SER problem, increasing the model complexity is critical. Therefore, to train the emotional context from the speech signals while sustaining the model complexity, most studies have proposed a combination model a CNN and RNN [12]-[14]. Although it is possible to learn the temporal features of speech using RNN, there are limitations to learning long sequences. Therefore, to compensate for the limitations, many models that combine attention mechanisms with RNN have been proposed.\nM. Chen et al. [15] proposed a CNN-LSTM-Attention model to aggregate the hidden states in each time step. This enables effective learning even for long sequences. In addition, for more comprehensive context learning, ADRNN [16], which uses residual connections and dilated convolution layers together, and ASRNN [17], which compensates for the shortcomings of RNN through a sliding RNN method, have been proposed.\nHowever, models using CNN-RNN-Attention layers force- fully learn the spatio-temporal features together. However, models consisting of CNN and RNN models in parallel are proposed to independently learn spatio-temporal features [38]. Zhao et al. [39] proposed a structure that separates LSTM and CNN in parallel and applies independent attention. Fur- thermore, Z. Chen et al. [28] proposed AMSnet, which is a parallel model that effectively synthesizes features through a connection attention mechanism."}, {"title": "B. CNN-based Models with Attention Mechanism", "content": "Recently, there has been a trend to use only CNN and attention layers without an RNN. Because RNN requires many more computational and training parameters than others, they focus on developing attention mechanism methods that help learn the context of the spatial features of speech spectrogram. Xu et al. [23] demonstrated effective emotional feature learn- ing only through self-attention after the CNN layer for the spatial context learning.\nAnother attention mechanism method was proposed to enhance the feature learning of the CNN layers. Li et al. [24] demonstrated the importance of frequency features that use frequential attention, in addition to spatial attention. Xu et al. [26] proposed the ATDA, which applied independent self-attention to all feature axes (temporal, frequential, and channel) to compensate for the weakness of temporal feature learning owing to the lack of an RNN. Guo et al. [27] proposed STC, which is a more efficient attention method for all feature axes of the CNN structure.\nWe further explore this trend and attempt to find an attention structure that is efficient and effective in learning emotional features while using a deep-layer CNN structure. In particular, for efficient attention, we focused on learning the channel features that contained the context information of the input data in the CNN layer. Therefore, we applied the ECA module to the SER problem and obtained improved emotion classification performance than before."}, {"title": "III. PREPROCESSING METHOD", "content": "In this section, we explain the preprocessing method used to extract crucial emotional speech signal features. Speech preprocessing suitable for a specific purpose is necessary to ef- fectively learn a neural network model that may provide better performance. It is not yet known which speech preprocessing method is the best for emotion recognition in speech.\nTherefore, we need to determine which speech preprocess- ing method is the best way to recognize the emotion of speech. For this purpose, we selected the log-Mel Spectrogram, which is frequently used for speech recognition. Then, we check the suitable windowing and overlap times in the log-Mel Spectro- gram to obtain the best result for the emotional recognition of speech."}, {"title": "A. STFT", "content": "The STFT is a method used to obtain the feature in frequency features by dividing a signal into short time periods. Even though STFT is used in speech processing, the setting in STFT that is the most suitable for the emotional recognition of speech has yet to be discovered. Therefore, we want to determine the value of the best setting when a neural network based on a CNN is applied to speech emotion recognition. The STFT can be described as follows:\n$X_t(f) = \\sum_{\\tau=-\\infty}^{\\infty} x(\\tau)w(t - ts)e^{2\\pi if\\tau}$ (1)\n$s=l-o$ (2)\nThe output $X_t(f)$ is obtained by applying a windowing function (w) to a signal x(7) in an interval of the windowing function. The output $X_t(f)$ then becomes a feature of the frequency. The windowing function moves according to stride (s), which is determined by the windowing length (l) and overlapping length (o).\nNote that the output $X_t(f)$ has a resolution limit. If the windowing length is longer, the frequency resolution increases; however, the resolution in time decreases. If the windowing length is shorter, the frequency resolution decreases, however, the time resolution increases. Therefore, we need to deter- mine which features are more important in terms of time or frequency. For this purpose, we performed our experiment by using eight different settings during preprocessing."}, {"title": "B. Speech Emotion Discrimination with Log-Mel Spectrogram", "content": "Because the amount of data for emotion recognition in speech is limited, it is advantageous for the size of the features to become small. The log-Mel spectrogram is effective for emotion recognition in speech because it can reduce the size of the features expressed in frequency. In addition, the log-Mel spectrogram displays speech characteristics in 2D images.\nHowever, recognizing the differences between different emotions is challenging. In addition, some images are ambigu- ous when characterizing the corresponding emotions. There- fore, a deep neural network model based on a CNN must be introduced to recognize the emotions of speech. In this study, we suggest the structure of a CNN model based on efficient channel attention, which is effective and efficient in emotion recognition of speech."}, {"title": "IV. MODEL ARCHITECTURE", "content": "Because there is little training data for speech-emotional recognition, we need to use as few parameters as possible to improve the successful recognition of emotions in speech. Therefore, we consider a neural network model for images based on a CNN. In addition, to effectively learn speech emotions in context, we set up a model to elevate the learning ability in the channel. Therefore, we apply efficient channel attention (ECA) to our neural network model-based CNN."}, {"title": "A. Convolution Layer", "content": "The Convolution layer learns the input features included in the local region by using various filters. The 2D Convo- lution layer learns the spatial information of the 2D images. When a 2D convolution layer is applied to a spectrogram for recognition for recognizing speech emotions, it can learn the relationship between time and frequency.\n$(X' *W)_{ij} = \\sum_{tfc}\\sum_{tfc}X^{1+1}(t, f, c) - W(i-t, j - f, c)$ (3)\n$2DConv(X^{1+1}) = [(X' * W_1), ..., (X^2 * W_c)]$ (4)\nEquations (3) and (4) show the method for calculating the convolution when a spectrogram is used as the input. The input $X^l \\in R^{T \\times F \\times C}$ is convoluted with weight filter $W^l = [W_1, ..., W_c] \\in R^{K \\times K \\times C \\times C'}$ to transform the C' size of channel dimension. In this calculation, the weight filters are used with kernel size (K \u00d7 K) of the local spatial information.\nWe must set an appropriate number of the weight filters and kernel sizes to better train with this convolution layer. Therefore, in this study, we explored the trainable paramet- ric efficiency in a CNN-based model for speech emotion recognition. We mainly focus on some weight filters called \"channel size\". The channel size in the convolution layer is a hyperparameter related to the information capacity of the neural network. If we use many weight filters, spatial information can be sensitively extracted from the input data. However, this leads to a more robust overfitting of the training dataset. We design a 6-layer CNN architecture to determine an adequate channel size for speech emotion recognition. This is explained in detail in the following section."}, {"title": "B. CNN-Based Architecture", "content": "We proposed a deep CNN-based model as a baseline for developing an effective SER model. The structure of the CNN- based model was based on a previously proposed model for SER. Using this model, we focused on CNN's channel features of the CNN, which effectively trained the speech emotion features."}, {"title": "C. The Design of ECA Module", "content": "The convolution layer can extract the local spatial features using the number of trainable filters from the input data. However, when more filters were used in the model, the representation capacity of the filters weakened. To overcome this situation, we adopt the ECA [35] in the CNN-based model, which can effectively improve the representation of the filters. The ECA is a channel attention architecture proposed by Wang et al.. Learning the relationship between different filters and focusing on the important ones is possible with fewer trainable parameters. We experimented with the application of an ECA suitable for a CNN-based SER model. As a result, increasing the filter's representation in the convolution layer helps to extract emotional features.\nThe ECA is a type of layer that applies a self-attention mechanism [29]. The self-attention mechanism comprises three components: query (Q), key (K), and value (V). These components are used to attend to the crucial information from input features. Each component's role is as follows. A query is a \"question\" about what is essential in the input features. The key is \"hint,\" which uses how similar the query is. This helps to find the most appropriate information in the input features. The value is the \"real answer\" that exists in pairs with keys and is used as the output features of self-attention.\n$Attention(Q, K, V) = Softmax(\\frac{QK}{\\sqrt{n}})V$ (5)\nThe most common self-attention mechanism in (5) is the inner-product attention. In the first step, each query is mul- tiplied with the multiple keys to obtain the relevant score matrix. In the second step, the score matrix is represented by a probability distribution within each query using the Softmax function. In the final step, the output is represented by a linear combination of all the values with a probability distribution. In summary, the output is the weighted value obtained that is most closely related to the query indirectly through the keys.\n$ECA(Q, V) = \\sigma(1DConv(Q)) \\otimes V$ (6)\n$Q = GAP(X)$ (7)\nECA is simplified by omitting the key from the existing self-attention structure. Equation (6) shows the progress of the ECA. The most significant difference from self-attention in (5) is that there is no key; however, the importance of the value is judged by the score learned from the query itself. This simplified attention mechanism is suitable for application in the SER.\nThe ECA structure can be divided into three steps. The first involves the prepara- tion of a query. The query represents the channel features in each input. We used global average pooling, which can contain temporal-frequency features in each channel without trainable parameters.\nThe next step is to make the score that represents the importance of each channel feature. We trained the channel feature's relation with a 1-D convolution layer to obtain the score. Specifically, the number of neighboring channel queries that train the relation with the target channel query is determined by the kernel size of the 1-D convolution layer. Moreover, with the sigmoid function, we score within 0 to 1.\nNext, a key difference from the original ECA is the omission of batch normalization. This change is made because batch normalization tends to extract global features of speech data rather than individual emotional features.\nThe final step is the re-representation of the input features. An element-wise product is performed on the input features and the attention channel score.\nECA can efficiently improve the channel feature of the convolution layer, which is essential for classifying speech emotions."}, {"title": "D. Weighted Focal Loss", "content": "Most speech emotion datasets have an unbalanced distribu- tion, depending on the emotion labels. Therefore, when we use cross entropy as a loss function, the model can be focused on a relatively large number of specific emotion labels (neutral and happiness). In addition, emotion classification is complex, depending on the label. For example, \u201chappiness\u201d and \u201cangry\u201d are often misclassified. Therefore, we used weighted focal loss to deal with those specific situations. The weighted focal loss can be expressed as follows.\n$WeightedFocal(p_i, w_i) = - \\sum_{i=1}^{N_{class}}w_i(1 - p_i)^{\\gamma} log(p_i)$ (8)\nIt has two properties. First, the focal loss [40] achieves flexible learning rates in different emotional classes ($n_{class}$). The focal loss function is a slightly more generalized function of the cross entropy weighted by the predicted probability of each emotion class ($p_i$). For emotion classes with low proba- bility values, the loss increases. This causes relatively more learning in backward updates. Conversely, emotion classes with high probability values resulted in relatively less learning. The hyperparameter gamma (\u03b3) controls how dramatically the loss value changes. In our experiment, we set \u03b3 = 1.\nSecond, to avoid imbalanced learning owing to the number of emotion classes, the loss function is multiplied by the learning rate weight ($w_i$) based on the number of labels in the training dataset. The learning rate weight was obtained as the reciprocal ratio of the number of data for each label."}, {"title": "V. EXPERIMENT SETTING AND RESULTS", "content": "The IEMOCAP [36] is the most popular dataset used in the SER problem. In this dataset, ten individual actors (five men and five women) recorded their voices, facial movements, and overall behavior for five sessions to understand human emotions in various situations. The IEMOCAP's data are divided into an improvised set, which contains improvised acting, and a script, which acts through dialogue. Three or more annotators manually classified the emotional speeches, and the final decision was made through a majority vote.\nThe data samples that we used in the experiment are as follows. We selected only the Improvised set, considering sit- uations in which human emotions can appear naturally. Next, we choose five emotional classes corresponding to \"angry\u201d, \"sadness\", \"happiness\u201d, \u201cneutral\u201d and \u201cexcited\". These classes are commonly used in various experiments. Moreover, to com- pensate for the data sample's number of \"happiness,\u201d \u201cexcited\" is considered as \u201chappiness\u201d. The experiments were conducted using 2943 speech data samples containing four emotional classes (angry:289, sadness:608, happiness:947, neutral:1099)."}, {"title": "B. Data Preprocessing", "content": "To efficiently represent the input data, we extracted the log- Mel spectrogram features from the speech signal. The data preprocessing steps are as follows.\nFirst, signal segmentation is performed to equalize the length of the input data. The IEMOCAP dataset speech samples had an average length of 4.5 s and varied from short (~ 0.5s) to long (~ 30s) speech. Therefore, the lengths of the speech samples were consistent at 6 s, which was slightly longer than the average. Specifically, if the speech data were shorter than 6 s, zero padding was performed at the beginning and end of the signal with the same length for the signal position in the center. If the speech was longer than 6 s, both the beginning and ends of the signal were cut to the same size to contain as long an utterance as possible.\nNext, we prepared the different versions of the datasets to search for more effective preprocessing settings with different window sizes and overlaps in the STFT. Therefore, an interval was set based on previous studies. Based on this, we chose eight different window sizes at 5ms intervals within a slightly wider range of 15 ms to 50 ms. The overlap size was adjusted to obtain the same size of input data.\nIn the final step, log-Mel filters were applied to effectively decrease the input data size Specifically, we use 64-number Mel filters to increase frequency resolution. All preprocessing steps were conducted using MATLAB."}, {"title": "C. Experimental Setup and Evaluation", "content": "We use 5-fold cross-validation for the entire dataset to ensure general SER performance. Samples from all the sets were randomly selected. To evaluate the model performances, we used the unweighted average accuracy (UA) and weighted average accuracy (WA). These two metrics are commonly used in the SER. Additionally, to analyze the balanced evaluation, we used the mean of UA and WA (ACC) values.\nThe PyTorch framework [43], a deep learning framework, was implemented in all the experiments in this study. The specific hyperparameters of the models were as follows: All weight parameters were initialized with He initialization [44]. For the model's optimization, we use the Adam optimizer [45] with $10^{-4}$ initial learning rates and $10^{-6}$ decay rates. The batch size was set to 32, and the focal loss parameter was set to 1. Finally, the models were trained for 150 epochs."}, {"title": "D. Searching the Proper Channel Size for CNN-Based Model Architecture", "content": "To extract emotional features from speech data more ef- fectively, we first experimented with the number of channel sizes used in a CNN-based model. The channel size is the most critical hyperparameter in the convolution layer. Also, the proper channel size is crucial to efficiently reduce the number of trainable weight parameters. The optimal number of channels can improve the classification performance of the SER model.\nFor this experiment, we individually trained and evaluated the eight different CNN-based models with variant channel sizes."}, {"title": "E. Using Original ECA Method in CNN-Based Models", "content": "To efficiently increase the channel feature representation, we use the ECA blocks in a CNN-based model. We first experi- mented with the original ECA block. For the experiments, we selected the CNN-based model (n = 4) that achieved the best performance in a channel size search experiment in section V-D. Moreover, the original ECA blocks in this model were applied. The original ECA is positioned after each convolution layer. Therefore six ECA blocks were added to the CNN-based model. Next, the ECA's kernel sizes (k) were set based on the channel size of each layer."}, {"title": "F. Searching the Proper ECA Block Usage with Different Version of Datasets", "content": "To effectively use ECA blocks, we experimented with how the model performance changes when using the ECA blocks in different positions on a CNN-based model with different kernel sizes (k). A CNN-based model has a structure in which the channel size increases as the layer deepens; therefore, the complexity of the channel features in deep layers is relatively high. Based on that situation, we want to determine where the ECA block can be more effective in helping to train the channel features. Specifically, the experiment was conducted by sequentially adding the ECA blocks starting from the sixth convolution layer, which was the deepest layer in the model. Subsequently, we changed the kernel size of the 1-d con- volution layer of the ECA block. The kernel size is the length of the local region in which the relationships between neighboring channel features are learned. In our experiments, four different kernel sizes (3, 5, 7, and 9) were used to verify the change in performance according to the kernel size. Moreover, we excluded the cases in which the kernel size was larger than nine because of poor performance. However, to determine the best kernel size, we must train (4+1)6 times to consider all case sizes according to the ECA block. Therefore, we skipped some cases of the experiments owing to limitations in computational resources. In all the experiments, we used the dataset of version 8."}, {"title": "G. Augmentation Method with Different Version of STFT Datasets", "content": "To overcome the limitations of representing speech emo- tional features obtained using only one preprocessing method, multiple preprocessing data augmentation experiments were performed. For this purpose, only the training dataset was added from the eight different versions of the datasets obtained by setting listed in Table I. Because each of the eight prepro- cessing methods has a different window size and overlap size, the model can train with richer emotional features.\nWe conducted two different experiments depending on the dataset selection methods to determine out the effect of multiple preprocessing data augmentation on SER. In the first case, we selected version 1 as the test set and collected training data samples from version 2 to version 8 in ascending order. Second, in contrast to the first, we selected dataset version 8 as the test set and collected the training data samples in descending order from version 7 to version 1. The models used in the experiment are CNN-based models with ECA blocks and models without an ECA block.\nFig. 10 shows the augmentation experiments in ascending order. In most cases, the results were higher than those in the cases where the augmentation method was not applied to either model. Specifically, the best results in the CNN- based model were obtained using all the preprocessing datasets"}, {"title": "H. Comparison with Other Attention Models", "content": "Next, we compare with other proposed models' performance that used attention methods. For this, we chose our best results models that contained ECA blocks and STFT data augmenta- tion. All models compared with our method use spectrogram data and attention methods used for feature aggregation or extrac- tion in the independent axis of the data."}, {"title": "I. Analysis of the Ablation Studies", "content": "For a detailed analysis of the proposed methods, the re- sults of the ablation models were compared. From the overall results, we can observe that the emotion classification perfor- mance is improved when applying our proposed ECA block.\nIn particular, the effect of the ECA block can significantly improve performance when used together with the STFT data augmentation method.\nSubsequently, we compared the classification performance for each emotion according to the ablation models. As shown in Fig. 12 and Fig. 13, you can see that in most models, the classification performance of angry and sadness was high; however, the classification performance for angry, happiness, and neutral tended to be low. Compared with Fig. 12(a), Fig. 13(b) shows that the classification performance of all emotions improved when the ECA and STFT augmentation methods were used.\nSpecifically, in Fig. 12(a) and (b), the happiness classifica- tion accuracy improved by 3.5%, and the neutral classification accuracy decreased by 2% using the ECA. It means that the ambiguity of classifying happiness and neutral was resolved through the ECA block. As shown in Fig. 13, the neutral classification accuracy improved by approximately 2% ~ 4%. In particular, as shown in Fig. 13(b), both angry and neutral classification accuracies were significantly improved.\nThe results show that channel feature extraction with the ECA blocks is effective for SER. Subsequently, to understand how the ECA block works for classifying each emotion, we checked the channel weights for each emotion. To plot the channel weights, the weights from the test set were averaged.\nAs shown in Fig. 14(a), the ECA weights of the fifth layer are not related to any emotion. However, as shown in Fig. 14(b), the ECA weights of the last layer are noticeably different. In particular, channel weights were distinguishable between angry (blue line) and sadness (orange line). In addi- tion, neutral (red line) is distinct from angry (blue line) and sadness (orange line). However, it is slightly different from the neutral (red line) to happiness (green line), because it is difficult to distinguish between them.\nIn summary, it is difficult to distinguish between all emo- tions, especially angry, happiness, and neutral emotions. How- ever, the proposed method can increase the accuracy of all emotions. Therefore, adopting channel attention in the CNN- based model can effectively extract the emotional context."}, {"title": "VI. CONCLUSION", "content": "This study proposes a promising and more effective pre- processing method and an ECA module for SER, offering the potential for significant advancements in the field. Our exper- iment, conducted with eight different preprocessing datasets from the IEMOCAP corpus, revealed a significant finding: a spectrogram with a higher frequency resolution is more effective in training emotional features, providing valuable insight for future research in the field. Our study is the first study to apply an ECA to the SER. We achieved significantly better results than previous models by applying ECA to our CNN-based model with an effective preprocessing method.\nConsidering these results, correctly understanding the rela- tionship between the channel features in the CNN structure"}]}