{"title": "PATCH-LEVEL TRAINING FOR LARGE LANGUAGE MODELS", "authors": ["Chenze Shao", "Fandong Meng", "Jie Zhou"], "abstract": "As Large Language Models (LLMs) achieve remarkable progress in language understanding and generation, their training efficiency has become a critical concern. Traditionally, LLMs are trained to predict the next token in a sequence. Despite the success of token-level training, it suffers from considerable computational costs due to the need to process an extensive number of tokens. To mitigate this issue, this paper introduces patch-level training for LLMs, which reduces the sequence length by compressing multiple tokens into a single patch. During patch-level training, we feed the language model shorter sequences of patches and train it to predict the next patch, thereby processing the majority of the training data at a significantly reduced computational cost. Following this, the model continues token-level training on the remaining training data to align with the inference mode. Experiments on a diverse range of models (370M-2.7B parameters) demonstrate that patch-level training can reduce overall computational costs to 0.5\u00d7, without compromising the model performance compared to token-level training. Source code: https://github.com/shaochenze/PatchTrain.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs, Achiam et al., 2023; Touvron et al., 2023a;b; Team et al., 2023; Bai\net al., 2023) have achieved remarkable progress in language understanding and generation, which\nare primarily attributed to their unprecedented model capacity and the corresponding growth in the\nvolume of training data they require (Kaplan et al., 2020; Hoffmann et al., 2022). However, this\nscaling up comes with a substantial rise in computational costs, making the training efficiency of\nLLMs a critical concern. Despite the ongoing efforts on efficient LLMs (Wan et al., 2023), it remains\na formidable challenge to reduce training costs without compromising the model performance.\nThe conventional approach for training LLMs is next token prediction, i.e., predicting the next token\nof a sequence. While this approach has achieved notable success, it represents an inefficient way for\nLLMs to acquire knowledge, as each token must be processed individually by the entire model. Even\nwhen disregarding the overhead of attention computation, each token still consumes approximately\nthe same FLOPs as the number of model parameters, resulting in considerable computational costs\nwhen dealing with an extensive number of tokens in the training data.\nFor other data modalities like images and audio, it has become increasingly popular of transforming\nthem into sequences for processing with sequence models (Chen et al., 2020; Zhu et al., 2024; Duan\net al., 2024). They also encountered the efficiency issue of sequence modeling, as directly converting\nraw inputs into sequences can produce excessively long sequences. For instance, unfolding a 256x256\nimage results in a pixel sequence of length 65,536, while a 10kHz audio signal translates into 10,000\nsamples per second. To improve the computational efficiency when dealing with such data, it is\ntypical to reduce the sequence length by compressing segments of a specific length into patches. For\ninstance, a 16x16 pixel segment can be represented as an image patch (Dosovitskiy et al., 2021), and\na 20ms slice of audio can be encapsulated in a hidden state representation (Baevski et al., 2020; Hsu\net al., 2021). However, for textual data, attempts to compress sequence length are less prevalent and\nnecessitate specialized model structures (Yu et al., 2024; Ho et al., 2024) or application scenarios"}, {"title": "2 PATCH-LEVEL TRAINING", "content": "In this section, we outline the patch-level training approach for large language models, as illustrated\nin Figure 3. Initially, the token sequence is transformed into a patch sequence by compressing every\nK consecutive tokens into a single patch. This patch sequence is then fed into the sequence model,\nand the model is trained to predict all tokens in the next patch. The knowledge acquired during\npatch-level training is subsequently transferred to the token-level model. Specifically, we use the\nparameters obtained from the patch-level model to initialize the token-level model, and then proceed\nwith token-level training on the remaining data.\nWhile formulating the patch-level model structure, our goal is to minimize the discrepancy between\npatch-level and token-level models, thereby ensuring that the knowledge gained during patch-level\ntraining can be smoothly transferred to the token-level model. Given the context length T for token-\nlevel training, we set the context length for patch-level training as KT, which is then compressed to\na patch sequence of length T to maintain consistency with the subsequent token-level training. To\navoid introducing unnecessary parameters during token-to-patch compression, we represent the patch\nembedding as the average of its associated token embeddings. Let pi be the i-th patch, XiK+k be the\nk-th token in the i-th patch, and E be the embedding function. The patch embedding is:\n\nE(p_i) = \\frac{1}{K} \\sum_{k=0}^{K-1} E(X_{iK+k}).\n\n(1)\nThe patch-level model is trained through next patch prediction, i.e., predicting the K tokens in\nthe next patch. The simultaneous prediction of multiple tokens has been explored in speculative\ndecoding, which typically employs multiple output heads and each head is responsible for predicting\na distinct token (Cai et al., 2024; Lin et al., 2024). However, this approach would also entail additional\nparameters that may be unfavorable for the subsequent knowledge transfer. Instead, we maintain\na single output head and make its prediction cover all tokens in the next patch. Specifically, we\ncalculate the cross-entropy loss for all the subsequent K tokens based on the same patch prediction\nPo (P<i), resulting in the following loss function:\n\nL_{patch} = -\\sum_{i=1}^{T} \\sum_{k=0}^{K-1} log p_{\\theta}(X_{iK+k} | P_{<i}).\n\n(2)"}, {"title": "3 EXPERIMENTS", "content": "We conduct extensive experiments to uncover the properties of patch-level training. First, with the\nhyperparameters set at K = 4 and \u03bb = 2/3, we show that patch-level training can reduce overall\ntraining costs to 0.5\u00d7, without compromising model performance compared to token-level training.\nFollowing this, we study the scaling properties of patch-level training in Section 3.3, and examine the\neffects of hyperparameters K and \u03bb respectively in Sections 3.4 and 3.5. Finally, Section 3.6 presents\na quantitative explanation for the efficiency of patch-level training."}, {"title": "3.1 SETUP", "content": "Datasets. We evaluate our approach on standard language modeling tasks, using the Pile dataset (Gao\net al., 2020) containing 360B tokens for training 1. We assess the performance of LLMs from multiple\naspects, including their perplexity, zero-shot accuracy, and instruction-following ability. Perplexity is\ncalculated on the WikiText-103 test set (Merity et al., 2017). We evaluate the zero-shot capabilities\nof language models on 6 NLP benchmarks, including MMLU (Hendrycks et al., 2021), HellaSwag\n(Zellers et al., 2019), PIQA (Bisk et al., 2020), WinoGrande (Sakaguchi et al., 2020), ARC-E, and\nARC-C (Clark et al., 2018) 2. For the pre-trained LLMs, we conduct instruction fine-tuning using the\nAlpaca dataset by GPT4 (Taori et al., 2023), and then evaluate their instruction-following abilities on\nMT-Bench (Zheng et al., 2024).\nModels. We use the Transformer backbone (Vaswani et al., 2017) and adopt most of the\narchitecture designs from LLaMA (Touvron et al., 2023a). We apply pre-normalization us-\ning RMSNorm (Zhang & Sennrich, 2019), use the SwiGLU activation function (Shazeer,\n2020), and rotary positional embeddings (Su et al., 2021). We also apply FlashAttention-\n2 (Dao, 2024) to accelerate attention computation. We scale the model demension and\nobtain 4 different sizes of Transformers: Transformer-370M (hidden_size=1024, intermedi-\nate_size=2752, hidden_layers=24, attention_heads=16), Transformer-780M (hidden_size=1536, inter-\nmediate_size=4128, hidden_layers=24, attention_heads=16), Transformer-1.3B (hidden_size=2048, in-\ntermediate_size=5504, hidden_layers=24, attention_heads=16), Transformer-2.7B (hidden_size=2560,\nintermediate_size=6880, hidden_layers=32, attention_heads=32).\nImplementation Details. Unless otherwise specified, the patch size K is 4. The context length for\ntoken-level training 2048. For patch-level training, the context length is the patch size K * 2048. The\nglobal batch size is 2M tokens, and the total number of training steps is N = 180000. For patch-level\ntraining, the number of training steps is NA, and then the model proceeds with token-level training\nfor N(1 \u2013 \u03bb) steps. After patch-level training, only the obtained model parameters are used for\ninitialization, and all other states like the optimizer and learning rate scheduler are reset. We use the\ntokenizer of LLaMA2, whose vocabulary size is 32000. Our models are optimized by the AdamW\noptimizer (Loshchilov & Hutter, 2019) with \u03b2\u2081 = 0.9, \u03b22 = 0.95, \u0454 = 1e \u2013 8. The learning rate is\n3e-4 and the cosine learning rate schedule is applied with warmup of 2000 steps. We use a weight\ndecay of 0.1 and gradient clipping of 1.0, and no dropout is applied during training."}, {"title": "3.2 MAIN RESULTS", "content": "We train a series of LLMs of varying sizes (370M-2.7B parameters) on the Pile dataset. To halve the\ntraining costs, we employ patch-level training with the settings of K = 4, \u03bb = 2/3, and compare\nits performance with the conventional token-level training. For the Transformer-370M, we also\nexplore other choices of \u03bb to evaluate its impact. Table 1 presents the performance comparison of the\nresulting models. Remarkably, our approach consumes only half of the computational resources and\nincurs almost no performance loss. It matches the baseline model in terms of perplexity and even\ndemonstrates a consistent gain in zero-shot evaluations, raising the average accuracy by approximately\n0.5%. The model performance is also influenced by the choice of \u03bb. Within the range of values\nwe set, a smaller \u03bb leads to better model performance but also entails larger training costs. A more\ndetailed study on the hyperparameter \u03bb will be presented in Section 3.5.\nWe further conduct instruction fine-tuning using the Alpaca dataset by GPT4 to examine the impact\nof patch-level training on the model's instruction-following ability. We evaluate our models using\nMT-Bench, a multi-turn question set, and present the experimental results in Figure 4. As can be\nseen, our approach maintains a similar instruction-following ability to the original models, with\nsome experiencing a score decrease (Transformer-370M, Transformer-1.3B) and others showing an\nimprovement (Transformer-780M, Transformer-2.7B), which can be viewed as regular variations.\nNext, we evaluate our approach in multi-epoch training. We randomly extract a subset of 60B tokens\nfrom the Pile dataset and increase the training epochs to 6. The results are shown in Table 2. To our\nsurprise, patch-level training continues to show superior training efficiency and even outperforms\nmodels trained on the full dataset in Table 1. We speculate that this is because combining patch-level\nand token-level training on the same data contributes to better model regularization. It also suggests\nthat our approach can be data-efficient by initializing the model with patch-level training for one or\nmultiple epochs, offering a promising direction for boosting model performance."}, {"title": "3.3 SCALING PROPERTIES", "content": "In the above, we have validated the effectiveness of patch-level training across several model sizes\n(370M-2.7B), using a training set of 360B tokens. However, state-of-the-art LLMs are generally\ntrained on model sizes and datasets that are at least an order of magnitude larger than our settings.\nTherefore, it is crucial to know the scaling properties of patch-level training, i.e., how it performs\nwhen applied to larger training datasets and models.\nIn Table 1, we notice a trend of perplexity related to the model size: the performance advantage of\npatch-level training appears to decrease as the model size increases. Table 3 describes this trend more\nprecisely, indicating that the model with patch-level training experiences smaller performance gains\nfrom the increase in model size. On the other hand, Table 4 presents the perplexity changes when\nmaintaining a constant model size and varying the size of the training data. As the data size increases,\nthe performance of patch-level training improves at a faster rate compared to the baseline model."}, {"title": "3.4 EFFECT OF PATCH SIZE K", "content": "We investigate the effect of patch size under the settings of 90B training data, 370M model parameters,\na batch size of 512K, and \u03bb = 1/2. The results are shown in Figure 5. Across different patch sizes,\nthe loss curves for patch sizes K = 2 and K = 4 are nearly indistinguishable, while further increasing\nthe patch size to 8 or 16 results in a certain performance decline. Despite this, these models still\nexhibit significant performance improvements when compared to the model trained from scratch,\nwhich does not benefit from the initialization of patch-level training.\nOverall, the patch size of K = 4 strikes a favorable trade-off between training efficiency and\nperformance. Considering that larger patch sizes can process more data at the same cost, we also\nexperiment with patch-level training using K = 8 on 90B tokens, which costs similar computational\nresources as K = 4 on 45B tokens. Following this, both models proceed with token-level training on\n45B tokens, and coincidentally, their loss curves are nearly identical. In this context, the advantage of\nK = 4 lies in its data efficiency, as it achieves similar performance while consuming less data."}, {"title": "3.5 EFFECT OF A", "content": "The hyperparameter \u03bb allocates the ratio of training data between patch-level and token-level training.\nA larger \u03bb results in more tokens being compressed into patches, leading to a higher acceleration\nrate, but it may also leave insufficient data to adjust the model to the token-level. In this section, we\ninvestigate the effect of \u03bb under the settings of 370M model parameters, a batch size of 512K, and a\npatch size of K = 4. We consider two scenarios:"}, {"title": "3.6 NEURON ACTIVATION", "content": "In this section, we quantitatively explain why patch-level training leads to better learning efficiency\nfrom the perspective of neuron activation. The training of LLMs is essentially a process of embedding\nknowledge from the training set into the model's parameters. During this process, the model employs\nall of its parameters to encode every token and updates the relevant parameters based on the gradient\nfeedback. We argue that this is an inefficient process for large models, as the knowledge encapsulated\nin each token is only associated with a small subset of model parameters, resulting in a limited\nnumber of effectively activated and updated parameters.\nWe substantiate this by measuring the percentage of activated neurons for models of different patch\nsizes, as depicted in Figure 8. In the token-level model (K=1), only a small proportion of neurons are\nactivated, particularly in the lower layers. By grouping multiple tokens into a patch, the information\ndensity processed at each step is increased, which is manifested as increased neuron activation rates.\nTherefore, patch-level training exhibits a better learning efficiency compared to token-level training."}, {"title": "4 RELATED WORK", "content": "Model Growth. Our approach draws inspiration from transfer learning, reducing training costs\nby transferring knowledge acquired at a lower training cost (patch-level) to a model with a higher\ntraining cost (token-level). A similar strategy has been employed in studies of model growth, which\ntrain large models at a relatively lower cost by progressively increasing the model size during training.\nFor example, Gong et al. (2019); Yang et al. (2020) improve the training efficiency by transferring\nknowledge from a shallow model to a deep model, where model layers are progressively stacked\nduring training. Gu et al. (2021) further proposes progressive compound growth, where the model\ngrows at multiple dimensions during training, including the context length, model width, and the\nnumber of layers. Subsequent studies primarily focus on the initialization problem during the model\ngrowth process, i.e., how to expand the small model into a large one. Chen et al. (2022); Yao et al."}, {"title": "5 CONCLUSION", "content": "This paper introduces patch-level training, an efficient training approach for large language models, in\nwhich models read training data in patches and learn to predict the next patch. Following this, a small\namount of training data is utilized to adjust the model to the token-level. Experimental results show\nthat this method can cut LLM training costs by 50% while maintaining comparable performance.\nYet, our exploration of patch-level training is still in its infancy, and advancements in the following\ndirections could further enhance this methodology: assessing the scalability of patch-level training by\nevaluating its performance on larger models and datasets; establishing an empirical scaling law for\npatch-level training, ideally incorporating both K and \u03bb; developing advanced training techniques to\naccommodate larger K and \u03bb, thereby pushing acceleration rates to a higher level; further investigating\nthe potential of patch-level training in multi-epoch training; exploring the applicability of patch-level\ntraining to other data modalities, such as images, speech, and video."}]}