{"title": "Native Fortran Implementation of TensorFlow-Trained Deep and Bayesian Neural Networks", "authors": ["Aidan Furlong", "Xingang Zhao", "Robert Salko", "Xu Wu"], "abstract": "This work presents a public framework that enables the direct implementation of TensorFlow-trained models within Fortran using the HDF5 file format to store the model. This framework provides a fully native solution for deep neural networks (DNNs) and Bayesian neural networks (BNNs) within Fortran without the dependence on Python runtime environments, TensorFlow's C API, or ONNX conversion.", "sections": [{"title": "INTRODUCTION", "content": "Over the past decade, the investigation of machine learning (ML) within the field of nuclear engineering has grown significantly. Nuclear engineering applications for ML range from the prediction of fuel lattice parameters to anomaly detection in power plant equipment. Most of the previous ML-related work investigating these solutions has been performed on a research scale with carefully designed ML experiments. Many of the methods developed from this ML research are approaching maturity [1], so studies will shift toward practical application. This next phase of investigation will help determine the feasibility and usefulness of ML model implementation in a production setting.\nBefore these ML models are implemented within a nuclear code, a basic assessment of direct compatibility is first required. Several of the codes used for reactor design and assessment-including MCNP [2], CASMO/SIMULATE [3], RELAP5 [4], and CTF [5], among others\u2014are primarily written in the Fortran language. Conversely, most ML model design, training, testing, and manipulation is performed in Python using either TensorFlow or PyTorch. The trained models are saved in a user-defined file format, such as HDF5 or ONNX, to allow for rapid loading in other ML frameworks. A custom framework is necessary to read these files and effectively \"load\" a model's architecture within Fortran.\nThis work presents a public framework that enables the direct implementation of TensorFlow-trained models within Fortran using the HDF5 file format to store the model. This framework provides a fully native solution for deep neural networks (DNNs) [6] and Bayesian neural networks (BNNs) [7] within Fortran without the dependence on Python runtime environments, TensorFlow's C API, or ONNX conversion.\nThis simple and lightweight implementation is suitable for applications that require a large volume of predictions (e.g., within iterative solvers). The computational efficiency aspect also provides the opportunity to effectively implement DNN ensembles, which support uncertainty quantification (UQ) in addition to the BNN outputs' inherent UQ capabilities. This allows for confidence information to be provided for every prediction, better informing on their qualities. Although this work was originally developed to be used in CTF, it is completely code agnostic and can be used in any Fortran deployment.\nIn this summary, the framework is described in detail, and this description is followed by verification comparing the Fortran predictions with those made in Python with TensorFlow. This verification was accomplished with two demonstration problems: (1) a noisy sinusoid and (2) the prediction of critical heat flux (CHF) values."}, {"title": "IMPLEMENTATION", "content": "This framework is designed for simplicity in its implementation within the intended application. In this spirit, all components related to model loading, data preprocessing, and prediction are located within a single module-dnn_module.f90 in the case of DNNs, or bnn_module.f90 in the case of BNNs. In terms of files, the TensorFlow output HDF5 model file is required. Because the training data was standardized in most cases, the inputs used to create a prediction must also be standardized. The information associated with standardization (x_mean, x_std, y_mean, and y_std) can easily be placed in a metadata.h5 file from the TensorFlow script used to train the model. Making a prediction involves four steps:\n\u2022 Define the model structure (layers, neurons, activations)\n\u2022 Load the weights and biases from the model file\n\u2022 Load the standardization parameters from the metadata file\n\u2022 Standardize the data and make a prediction\nIn the section of the target script that will call dnn_module.f90 to make predictions, the user defines critical aspects of the model and directs the flow of data through the prediction process. The model initializer is first called to define the model's architecture, initialize([N^{(l)}, N^{(l+1)}, ..., N^{(L)}]), where N denotes the number of neurons for each layer starting at the input layer I and ending at the output layer L. The actual weights and biases are then loaded by calling load_weights(model_path). The final step of configuring the model is simply assigning the activation functions. This step is accomplished by associating a procedure pointer within the derived type for activations, as in layer_activation(l)%func => relu_fn. The relu_fn is one of five pre-included activation functions, along with elu_fn, selu_fn, softplus_fn, and tanh_fn. Other activation functions may be added at the discretion of the user.\nIf standardization is necessary, then the metadata file is loaded by calling load_metadata(metadata_path). Once the metadata file has been loaded, standardize(x_data, x_mean, x_std) may be called for each input value (or vector if the user is making multiple predictions from a single call instead of calling for each prediction). This function returns x_data after performing the standardization. Finally, the prediction may be obtained by calling y_pred = predict(x_data). Depending on whether the model was trained with a standardized target, calling unstandardize(y_data, y_mean, y_std) may be necessary. Listing 1 is an example of what this workflow could look like."}, {"title": "VERIFICATION", "content": "Verification is necessary to ensure that the predictions made by the Fortran implementation accurately match those produced in the Python TensorFlow environment. In this study, verification was conducted using two reasonably complex test cases followed by a nuclear engineering application. The first test case involved a transformed sinusoidal function with random Gaussian noise, taking two inputs and predicting the corresponding y-value. Another test case featured a highly nonlinear function with three inputs, again using the y-value as the target. The nonlinear regression case is included in the public repository but is omitted here for space considerations. These cases were specifically designed to introduce sufficient complexity, ensuring that agreement between the Fortran and TensorFlow predictions is not merely due to simplicity (e.g., trivially predicting a linear function)."}, {"title": "Test Case 1: Noisy Sinusoid", "content": "In the case of the noisy sinusoid, a dataset composed of 5,000 points was generated with 80% used for training and 20% used for testing. Although the goal of these test cases is to assess the agreement between the Fortran and TensorFlow implementations (which could be achieved regardless of the accuracy of the model itself in predicting the test set), the architectures were still optimized prior to training to achieve reasonable error metrics. This was done using a traditional random search space with 1000 prospective configurations. The DNN model used two hidden layers, the BNN model used three hidden layers, and both were trained to 500 epochs."}, {"title": "Deep Neural Networks", "content": "The results from the DNN model were first considered by comparing 9 relevant error metrics between the implementations, along with the inference timing statistics. Because 20% of the generated dataset was allocated for testing, predictions were made for a total of 1000 input combinations. The Fortran implementation was able to accomplish this in 5.1% of the time that the TensorFlow counterpart used. This difference represents a speedup factor of 19.6. A complete listing of the 9 performance metrics is provided in Table I. Both implementations show good agreement with only small differences. AE denotes the absolute error, APE denotes the absolute percentage error, rRMSE denotes the relative root-mean-square error, and Ferror > 10% denotes the fraction of relative error values above 10%.\nThe residuals between the Fortran- and TensorFlow-based predictions were then collected and plotted via histogram and kernel density estimation (KDE), as shown in Fig. 1. Both are centered close to zero, suggesting that systematic bias is unlikely; in the absence of systematic bias, residuals are often expected to exhibit a symmetric distribution around zero. The spread is also relatively tight; nearly all residuals are enclosed by \u00b1 1.0 \u00d7 10-4 on a test set with bounds of 273.6 and 626.3."}, {"title": "Bayesian Neural Networks", "content": "Comparing the BNN model implementations is less straightforward than comparing the DNN cases because sampling of the posterior distribution is required. This sampling requires randomness, meaning that a reliable random number generator (RNG) is needed. Although RNGs themselves are not conceptually complicated, TensorFlow's default RNG implementation is more complex because of its underlying algorithm and seeding mechanisms. For this reason, obtaining identical values from the custom Fortran and TensorFlow RNGs is not feasible, which can lead to differences between predictions. To mitigate the effect of RNG variability and to ensure a well-approximated posterior distribution, 20,000 samples were taken for each prediction. Totaling 20 million samples, the Fortran implementation took 2.53 min, whereas the TensorFlow counterpart took 20.26 min, meaning that the speedup factor is 8.0. In practice, predictions could be well converged at just 100 samples. In this case, the time elapsed per sample-mean prediction would be 0.76 ms.\nThe same set of performance metrics from the DNN model's case is provided for the BNN model predictions in Table II. Although the difference values are larger than those of the DNNs, this is likely due to the different RNGs used, despite the large volume of samples taken. Nonetheless, differences are small and do not indicate any fundamental issues within the backend mathematics."}, {"title": "Test Case 2: Critical Heat Flux", "content": "The second test considers CHF, the point in a boiling system at which there is a regime change from nucleate to transition boiling. CHF is a safety-related parameter in nuclear systems. This test was chosen as a final verification prior to the deployment of this framework within the CTF thermal-hydraulics code to support study of CHF surrogate modeling. To predict a CHF value, five inputs describing a tube channel are used: diameter, heated length, pressure, mass flux, and inlet subcooling. The dataset used for this experiment was a filtered version of the Nuclear Regulatory Commission CHF database used to construct the 2006 Groeneveld lookup tables [8] to only contain instances of dryout. This database was previously shown to yield satisfactory results when used to train a DNN [9].\nBoth DNN and BNN models were trained in the TensorFlow environment, which achieved favorable error metrics on the test set: 2.18% and 3.05% relative errors with less than 3% of all values greater than 10% error. Both of these models are more complex than those used for the noisy sinusoid case. The DNN is structured with seven hidden layers, and the BNN is structured with four. Time per prediction is increased simply because of a larger number of calculations needed with increasing complexity. Once these models were confirmed to be well trained, they were exported and implemented with the Fortran framework to make predictions on the same test set (10% of the filtered dataset). This implementation was configured identically to that which was later put into an experimental branch of CTF. As with the noisy sinusoid test case, the differences between the Fortran-based predictions and the TensorFlow-based predictions were taken in 9 key metrics, which are presented in Table III."}, {"title": "CONCLUSIONS", "content": "This study presents a framework for implementing DNN and BNN models in Fortran, allowing for native execution without TensorFlow's C API, Python runtime, or ONNX conversion. Designed for ease of use and computational efficiency, the framework can be implemented in any Fortran code, supporting iterative solvers and UQ via ensembles or BNNs.\nVerification was performed using a two-input, one-output test case composed of a noisy sinusoid to compare Fortran-based predictions to those from TensorFlow. The DNN predictions showed negligible differences and achieved a 19.6\u00d7 speedup, whereas the BNN predictions were observed with minor disagreement, plausibly because of differences in random number generation. An 8.0\u00d7 speedup was noted for BNN inference. The approach was then further verified on a nuclear-relevant problem predicting CHF, which demonstrated similar behavior along with significant computational gains. Discussion regarding the framework's successful integration into the CTF thermal-hydraulics code is also included, outlining its practical usefulness.\nOverall, this framework was shown to be effective at implementing both DNN and BNN model inference within Fortran, allowing for the continued study of ML-based methods in real-world nuclear applications. Future work will include expanding support for Gaussian processes, enhancing UQ, and improving automation and compatibility with additional model formats."}]}