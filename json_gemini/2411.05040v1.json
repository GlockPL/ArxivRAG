{"title": "Bottom-Up and Top-Down Analysis of Values, Agendas, and Observations in Corpora and LLMs", "authors": ["Scott E. Friedman", "Noam Benkler", "Drisana Iverson", "Jeffrey Rye", "Sonja Schmer-Galunder", "Micah Goldwater", "Matthew McLure", "Ruta Wheelock", "Jeremy Gottlieb", "Robert P. Goldman", "Christopher Miller"], "abstract": "Large language models (LLMs) generate diverse, situated, persuasive texts from a plurality of potential perspectives, influenced heavily by their prompts and training data. As part of LLM adoption, we seek to characterize-and ideally, manage-the socio-cultural values that they express, for reasons of safety, accuracy, inclusion, and cultural fidelity. We present a validated approach to automatically (1) extracting heterogeneous latent value propositions from texts, (2) assessing resonance and conflict of values with texts, and (3) combining these operations to characterize the pluralistic value alignment of human-sourced and LLM-sourced textual data.", "sections": [{"title": "Introduction", "content": "As large language models (LLMs) are adopted across healthcare, humanities, and defense sciences, it's increasingly important to measure and manage the values that appear in their outputs. Measuring values may help us characterize whether model's behavior is consistent with universalism (i.e., reflecting a singular or dominant value system), pluralism (i.e., attending to a plurality of potentially- conflicting value systems), or something in-between. This paper presents an approach to analyzing LLMs and datasets to (1) surface a plurality of values from the bottom-up (\u00a73), (2) measure novel and user-provided values from the top-down (\u00a72), and (3) summarize the value dominance and pluralism in a dataset or LLM's output (\u00a75). Informed by prior work, it's important to characterize language models' value systems at at least two levels: (1) the values reflected in [samples of] their training data, and (2) the values reflected in their outputs to diverse prompts and stimuli:\n1. Values in the data (inputs). AI models acquired by large-scale machine learning contain systemic biases rooted in their training corpora [1], and research has shown that these models' biases correlate meaningfully with their training data's temporal origin (e.g., with respect to stereotypes [2]) and cultural origin (e.g., with respect respect to cultural values [3]). Consequently, assessing values in training data can help us explain, predict, and manage the values reflected in an LLM's output.\n2. Values in the generation (outputs). LLMs do not \u201chold\u201d values in the same psychological sense that people do, but their outputs may nevertheless consistently support or contradict human values. Recent work has evaluated the values of LLMs in top-down fashion by (1) using human-based value inventories (e.g., [4]) to directly survey LLMs [5, 6] or (2) prompting LLMs and then measuring their output's implicit resonance [7] against cross-cultural value statements [8].\nMost of the above approaches utilize (a) pre-existing, vetted value inventories to directly survey LLMs [4, 8, 5, 6] or (b) pre-defined value enumerations to characterize latent values in LLM texts"}, {"title": "Approach: Top-Down Analysis of Values & Themes", "content": "Our top-down analyses build on prior pair-wise NLP for stance analysis and value analysis, all based on the broader NLP problem settings of recognizing textual entailment [10] and natural language inference [11]. Existing pair-wise approaches of this shape take in a (premise, hypothesis) pair and then accurately (and relatively rapidly) predict whether the premise text holds a stance that is {positive, neutral, negative} with respect to various stance-topic hypotheses [12, 13], or analogously, predict whether the premise text takes a position that is {resonating, neutral, contradicting} with respect to the various value-hypotheses [9, 7].\nWe use the {resonating, neutral, contradicting} analysis of values and themes in this work, and we apply it more broadly: given a novel text t, we can assess it against value of interest vi by running (t, vi), and we can also analyze inter-value resonance by running (vi, Vi+1) to compute a directed network of resonance and contradiction over a potentially diverse plurality of values.\nNLP Dataset, Architecture, & Training: To build a combined value- and stance-based dataset, we in- corporate training and testing data from the World Value Corpus dataset [9] and the VAST dataset [13], respectively. We also utilize our bottom-up theme extraction dataset described below (\u00a73), including all (text, theme) pairs created by the bottom-up human annotators. The combined training dataset size is 19,804 (premise, hypothesis) pairs with varied labels: 7,943 resonance, 6,410 contradiction, and 5,451 neutral. We trained the DeBERTa-v3 variant DeBERTa-v3-base-mnli-fever-anli on our training data for 4 iterations with learning rate 2e-5."}, {"title": "Approach: Bottom-Up Extraction of Values & Themes", "content": "Value inventories [4, 8] are useful for psychological and anthropological analyses spanning geography and decades, having been vetted and translated for cross-cultural relevance. They are also, by nature, incomplete with respect to current events. For instance, the World Value Survey (WVS) contains agree-disagree prompts regarding reliability of online news sources or trust in the World Health Organization, but it does not\u2014and should not probe for participants' trust in every social institution, technology, and organization.\nOur bottom-up theme extraction uses fine-tuned LLMs (validated in Sec. 4) to extract three different types of themes. We exemplify each type from our maternal health value domain (\u00a75).\n1. Observations: Events or relationships in the world. These may be true or false, but the text expresses them as facts. Example: \"Colostrum boosts newborns' immune systems.\"\n2. Evaluations: Judgements on specific topics, including attributions of quality, trust, etiquette, or other dimension of regard. Example: \"Hospitals in urban areas are corrupt.\"\n3. Agendas: Statements of \u201cshould [not],\u201d promoting or justifying principles, norms, and [un]desired world states. Example: \"Mothers should feed colostrum to their newborns.\"\nWe developed these categories after analyzing the WVS: many of its probes are agenda-like, others are evaluation-like, all can be stated as propositions, and probes range from general (e.g., about \"family\") to very specific (e.g., about the WHO). All three types of themes are in proposition-form, so they could be the subject of agree/disagree survey prompts like the WVS. This means that the output of bottom-up theme extraction can feed into the existing top-down value assessment (described above), and we can thereby compute a network of bottom-up and top-down themes to help characterize the value alignment and pluralism of a model or corpus.\nNLP Dataset, Architecture, & Training: No previous dataset exists for this theme/value extraction task, so we developed one for this purpose, using an annotator-in-the-loop approach [14]. Three human annotators received annotation guidelines that described the three categories of themes (above) and included 12 fully-worked example paragraphs. Annotators wrote all of the distinct themes that"}, {"title": "Validation", "content": "Top-down evaluation. We ran our fine-tuned DeBERTa-v3 model on the World Value Corpus test set [9] and the SemEval 2016 Task 6 Twitter stance detection test sets [12]. For WVC, our model achieved 0.97 micro-F1 (0.92 for resonance, 0.98 for neutral, and 0.97 for contradiction), tied with the state-of-the-art ROBERTa model [9]. For SemEval 2016, our model achieved 0.78 micro-F1 for Task A (0.8 for resonance, 0.68 for neutral, 0.8 for contradiction) and 0.71 micro-F1 for Task B (0.65 for resonance, 0.78 for neutral, and 0.65 for contradiction), out-performing the SemEval 2016 competition winners for both Task A (F1=0.67) and Task B (F1=0.56). This is evidence that our top-down value-resonance strategy (\u00a72) produces high-quality predictions.\nBottom-up evaluation. Two human judges who were not involved with the bottom-up annotation process (\u00a73) received the annotation guidelines and used a web interface to make quality judgments on the themes extracted by humans and machines from a held-out test set. The two judges were blinded with respect to who or what extracted the themes (i.e., whether it was a human or a machine). The judges rated sets of extracted themes on a scale of 1 (poor) to 5 (excellent) along dimensions of completeness (how well the results captured all the themes in the text) and concision (how well the results minimize unnecessary content). In addition to evaluating the full extractions, they also rated each individual theme extracted on 1-to-5 quality scales. Results are shown in Fig. 1, comparing quality judgments across dimensions of the work of the two human raters (H1 and H2), our two fine-tuned models (Llama3 and Phi2), and 12-example few-shot results from GPT4. The only statistically significant results are that (1) GPT4 performed significantly worse on concision and (2) H1 produced higher-quality agendas and total themes compared to other humans and machines. Finally, the human judges predicted whether a human or a machine extracted the themes, and their accuracy at human-machine prediction was no better than chance (F1=0.52). These combined results suggest that our bottom-up theme and value strategy (\u00a72) produces high-quality extractions."}, {"title": "Example Applications", "content": "We combine top-down (\u00a72) and bottom-up (\u00a73) analyses of themes and pluralism, on two levels analysis: (1) analyzing datasets from ethnographic interviews with human participants and (2) analyzing argumentative LLM outputs focused at a specific topic. Each of our datasets includes \"pro\" and \"anti\u201d positions for a given topic, so we perform a comparative analysis of the plurality (or universality) of machine-extracted values for each position. For each analysis, we perform (1) bottom-up extraction of themes from texts and then (2) top-down characterization of how themes relate (i.e., resonance, contradiction, neutrality) to both \"pro\" and \"anti\" positions. All results are shown in Figs. 2, 3, and 4, listing the most relevant (i.e., non-neutral) observations (\u201cObs\u201d), evaluations (\"Val\u201d), and agendas (\u201cAgn\") that were extracted by our system. For each theme, we plot\""}, {"title": "Conclusion", "content": "This paper presented a novel approach to bottom-up value discovery from text (\u00a73) that achieves human-level value extraction (\u00a74) and a state-of-the-art top-down assessment of value resonance (\u00a72). We apply our approach on three domains-one human dataset and two LLM-generated corpora-to perform fully-automated value analyses. These analyses confirmed the values we expected (i.e., when we prompted the LLM to take a stance) and they also surfaced additional, unexpected values that were expressed in the texts. This permits us to analyze LLMs' value alignment and pluralism with respect to (1) a priori value hypotheses from human operators and (2) latent values from the text, in support of comparative read-outs. For future work, we plan to apply this approach in diverse domains and at larger scales, and to help characterize how the plurality of values expressed in LLM training data (or in prompts) impact the values expressed in LLMs."}, {"title": "Appendix / supplemental material", "content": "Ethical Considerations\nThe values extracted from texts in our approach are not necessarily those intended by the human or machine that authored the text. Consequently, values extracted or predicted by the machine to resonate with an author's texts should not be attributed to the authors; rather, they should be regarded as plausible value implications. This approach should not be applied to make moral judgments, ascribe ethics to authors, or classify individuals based on their socio-cultural commitments, since this is outside the bounds of our validation.\nLimitations\nOur models operate on unstructured texts alone, and not imagery, audio, video, or structured data. Further, our models are trained on texts of paragraph-size, so they do not have the larger context of a full news article, journal article, or fictional work when making extractive or associative value judgments. It also does not have access to an overview of current events, so texts concerning topics that change rapidly-such as geopolitical events and pop culture-may be inaccurately or incompletely characterized by our approach. Our model has been validated on a hold-out set of 32 examples that spans multiple domains, but we plan to validate it on additional domains and input formats in future work.\nBottom-Up Theme Extraction Prompt for Fine-Tuned LLMs\nWe used the following prompt for bottom-up theme extraction with LLMs, populating the input_text. To encode categorized themes, we used a newline-delimited sequence of themes, associating each theme_text with its theme_category (Observation, Evaluation, or Agenda) and the attribution of the theme (i.e., whether it's held by the author or by another entity mentioned in the text. During training, we specified the themes in this fashion, and during prediction/validation, we parse the LLM-generated themes from this format into structured data.\nInstruct: List themes from the text from the perspective of the author and others. For each theme, label the type (Observation, Evaluation, or Agenda) and the perspective in parentheses. No duplicates.\nInput: <input_text>\nOutput:\n<theme_text_1> (<theme_category_1> by <attribution_1>)\n<theme_text_n> (<theme_category_n> by <attribution_n>)\nDocument Generation Prompt for GPT-4\nWe used the following prompt for document generation with GPT-4, populating source_article, target_evaluation, and target_agenda with the values in table 1.\nWrite me five varied comments in response to an online news article with the headline '<article>' that (1) <agenda> and (2) <evaluation>.\nMake these as casual as possible.\nEach unique prompt was run 5 times for a total of 25 comments per unique prompt, using the following model settings:\nmodel: gpt-4"}]}