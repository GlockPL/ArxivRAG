{"title": "PLANTS\nA Novel Problem and Dataset for Summarization of\nPlanning-Like (PL) Tasks", "authors": ["Vishal Pallagani", "Biplav Srivastava", "Nitin Gupta"], "abstract": "Text summarization is a well-studied problem that deals with deriving insights\nfrom unstructured text consumed by humans, and it has found extensive business\napplications. However, many real-life tasks involve generating a series of actions\nto achieve specific goals, such as workflows, recipes, dialogs, and travel plans.\nWe refer to them as planning-like (PL) tasks noting that the main commonality\nthey share is control flow information. which may be partially specified. Their\nstructure presents an opportunity to create more practical summaries to help users\nmake quick decisions. We investigate this observation by introducing a novel plan\nsummarization problem, presenting a dataset, and providing a baseline method for\ngenerating PL summaries. Using quantitative metrics and qualitative user studies\nto establish baselines, we evaluate the plan summaries from our method and large\nlanguage models. We believe the novel problem and dataset can reinvigorate\nresearch in summarization, which some consider as a solved problem.", "sections": [{"title": "Introduction", "content": "Text summarization is a crucial task in natural language processing (NLP) that focuses on condensing\nlarge volumes of unstructured text into concise and informative summaries [Luhn, 1958]. This task\nhas significant applications in various domains such as news aggregation, document summarization,\nand content recommendation systems [El-Kassas et al., 2021]. Traditional summarization techniques\ncan be broadly categorized into extractive [Gupta and Lehal, 2010] and abstractive methods [Gupta\nand Gupta, 2019]. Extractive summarization selects key sentences or phrases from the original text,\nwhereas abstractive summarization generates new sentences that capture the essence of the text.\nRecently, large language models (LLMs) have demonstrated remarkable capabilities, outperforming\nhuman summaries [Pu et al., 2023] on several datasets such as Multi-News [Fabbri et al., 2019] and\nMediaSum [Zhu et al., 2021].\nDespite its extensive applications, text summarization has primarily concentrated on static documents,\noverlooking dynamic tasks that involve sequences of actions aimed at achieving specific goals. We\nrefer to these tasks as planning-like (PL) tasks[Srivastava and Pallagani, 2024]. Examples of PL tasks\ninclude workflows, recipes, dialogs, and travel plans, which often contain control flow information\ncritical for execution. For instance, consider the task of cooking a cheese sandwich. Numerous\nrecipes exist for making a cheese sandwich, each with varying ingredients and steps. A summary for\nthis PL task aims to condense these multiple recipes into a single, coherent summary. This summary\nwould allow a knowledgeable user to quickly make a cheese sandwich based on the brief summary or\nhelp a user decide which recipe best suits their needs based on the ingredients they have available.\nThis approach can be considered similar to multi-document summarization on a high level, where"}, {"title": "2 Planning-like Tasks", "content": "Planning-like tasks involve a series of actions required to achieve specific goals. These tasks\nare defined and explored in Srivastava and Pallagani [2024]. In this paper, we focus on three\nprimary domains of PL tasks: automated plans, recipes, and travel routes. Each of these domains\ninvolves unique challenges and characteristics that necessitate effective summarization for better user\ncomprehension and decision-making."}, {"title": "Automated Plans", "content": "Automated planning [Ghallab et al., 2004] involves creating action sequences for intelligent agents\nto achieve specified goals. In automated planning, a problem is typically represented as a tuple\nconsisting of states, actions, and goals. The objective is to generate an automated plan that transitions\nthe system from the initial state to the goal state while satisfying certain constraints. The semantics\nof automated plans require them to be sound and feasible, meaning each action must be executable in\nthe given context, and the sequence must logically lead to the achievement of the goal. Summarizing\nautomated plans helps in quickly understanding the essential steps and ensuring all actions are\nexecutable."}, {"title": "Recipes", "content": "In the domain of culinary arts, recipes are structured sequences of actions aimed at preparing specific\ndishes. Each recipe includes a list of ingredients and step-by-step instructions for combining them.\nGiven the multitude of recipes available for a single dish, there can be significant variation in\ningredients and preparation methods. This diversity makes it challenging for users to quickly identify\nthe essential components and steps needed to prepare a dish. Summarizing recipes allows users to\nidentify must-have ingredients and critical steps, making it easier to choose or adapt a recipe based\non available ingredients."}, {"title": "Travel Routes", "content": "Travel planning involves creating efficient paths from a starting location to a destination. This process\nincludes determining the optimal route, considering factors such as distance, travel time, and road\nconditions. Travel routes are complex, often involving multiple possible paths and decisions about\nwhich roads or highways to take. Summarizing travel routes provides a clear overview of the main\npaths, travel times, and distances, aiding in quick decision-making and efficient route planning.\nThese PL tasks, as summarized in Table 1, highlight the different characteristics and requirements\nacross domains. Summarizing these tasks enhances usability and accessibility, providing users with\nconcise, actionable insights for efficient decision-making and task execution."}, {"title": "3 Planning Task Summarization", "content": "Planning task summarization involves generating a concise summary of multiple plans that achieve\nthe same goal. In various domains, such as travel planning, recipe generation, and automated\nplanning, it is common to have multiple possible plans to reach a desired outcome. Each plan\nmay differ in the sequence and number of actions required. Inspired by early work on process\nsummarization [Srivastava, 2010], our approach aims to enhance user comprehension and facilitate\nbetter decision-making by providing a summary that consolidates these multiple plans into a single,\ncoherent overview, highlighting the key actions and considerations for achieving the goal."}, {"title": "4 PLANTS Dataset", "content": "In this section, we introduce the PLANTS dataset, specifically designed for planning task summariza-\ntion. The dataset encompasses three distinct planning-like tasks: automated plans, recipes, and travel\nroutes. For each task, we have curated 10 different problems/goals. Each goal has 5 different plans\nfor automated plans and recipes, and 3 different plans for travel routes, resulting in a total of 130\ndiverse plans in the dataset (see Figure 2).\nAutomated Plans: For generating automated plans, we utilized five classical planning domains from\nthe downward-benchmarks[Basel, 2024]: blocks, driverlog, mprime, openstacks-strips,\nand queen-split. These domains are released as part of the International Planning Competition\n(IPC) [ICAPS, 2022]. The downward-benchmarks repository includes both the domains and their\ncorresponding problems, where the goals are defined. We selected two distinct problems (i.e., goals)\nfrom each planning domain, resulting in a total of ten unique goals. Each problem was solved using\nSymK [Speck et al., 2020], a state-of-the-art classical optimal and top-k planner based on symbolic\nsearch that extends Fast Downward [Helmert, 2006]. We set k to 5, generating five different plans for\neach problem. This approach ensures that our dataset contains a variety of viable solutions for each\nplanning problem, providing a robust basis for summarization.\nRecipes: For the recipes, we manually selected ten distinct and commonly made dishes such as cheese\nsandwich, guacamole and omelette from the Recipe1M+ dataset [Mar\u0131n et al., 2021]. Recipe1M+\nis a large-scale dataset containing over one million recipes with associated images and instructions.\nAssumption: To ensure diversity in preparation methods, we assume that distinct ingredient lists will\nresult in different preparation steps. Based on this assumption, we extracted five different recipes\nfor each dish by calculating the Jaccard similarity between the ingredient lists and selecting recipes\nwith low similarity scores. This method ensures that the chosen recipes have varied ingredients,\nleading to diverse preparation steps. Specifically, we only extracted the ingredients and step-by-step\n4"}, {"title": "5 Experimental Settings", "content": "In this section, we describe the different models used for plan summary generation and also discuss\nthe user study settings. The constraints applied to these models and the prompt templates used for\nGPT-40 are detailed in Supplementary Material (Section 3)."}, {"title": "Models", "content": "For each task, we use GPT-4o as the representative of LLMs and an abstractive technique for obtaining\nplan summaries. For extractive summarization, we use TextRank. Additionally, we developed a new\nfrequency-based baseline method for extractive plan summarization. Each approach receives as input\na set of plans to generate a summary. For automated plans and recipes, each set contains 5 plans, and\nfor travel routes, each set contains 3 plans.\nAlgorithm 1 outlines our baseline method, which involves parsing the plans to extract actions and\ncreating a structured representation of the data. This structured data is then analyzed in two views:\ntext view and plan view. The text view analysis identifies common items and n-grams by counting\nthe frequency of individual actions and sequences of actions. The plan view analysis examines the\nstructure and sequence of actions, identifying the most common actions, secondary mentions (such\nas objects or ingredients), the shortest plan, and the most common action sequences. The results from\nthese analyses are combined to generate a plan summary."}, {"title": "User Study", "content": "To assess the ease of understanding, clarity for action, and overall preference for the summaries, we\nconducted a human evaluation involving ten annotators. The annotators were students (undergraduate\nand graduate students) and faculty staff, all with an understanding of the three PL tasks: automated\nplans, recipes, and travel routes. For each PL task, we provided the annotators with the actual\nplans and presented them with summaries generated by three different methods: GPT-4 (abstractive),\nTextRank (extractive), and our frequency-based baseline method (extractive). To ensure the reliability\nof our results, we calculated the overall inter-annotator agreement using Cohen's kappa coefficient\n[Cohen, 1968]. We found that the agreement among annotators was acceptable, with a coefficient of\n0.72."}, {"title": "6 Experimental Results", "content": "Experiment 1: Comparing the number of tokens across the summaries\nFigure 3 shows the boxplot comparing the token counts across three summarization methods: baseline,\nTextRank, and GPT-40. The median token count for baseline is around 53, indicating consistent\nsummary lengths with minimal variability. TextRank exhibits significant variability, with a median\ntoken count lower than baseline, reflecting diverse summary lengths. GPT-40 displays the highest\nmedian token count at approximately 176.5, indicating longer and more detailed summaries, with\na wider interquartile range. This analysis highlights the differences in summary lengths, providing\ninsights into the summarization characteristics of each method.\nExperiment 2: Comparing the information-richness of the summaries\nIn this experiment, we measure the lexical density of summaries generated by baseline, TextRank,\nand GPT-4o to evaluate their information richness. Lexical density is calculated as the proportion of\ncontent words\u2014nouns, verbs, adjectives, and adverbs-to the total number of words in a summary.\nFigure 4 shows the lexical density of the three summary methods across 30 planning summarization\ntasks in the benchmark dataset. GPT-40 consistently achieves the highest lexical density, indicating it"}, {"title": "Experiment 3: Comparing the ease of understanding of the summaries", "content": "From the user studies, we obtained results on how easy it is to understand a summary to take an\naction. Each summary was rated on a scale from 1 to 5, with 1 being very difficult to understand and\n5 being very easy to understand. The average ease of understanding scores are presented in Table 2.\nGPT-40 received the highest ease of understanding scores across the three PL tasks. For automated\nplans, the baseline approach ranked second, while TextRank was rated second for recipes and travel\nroutes."}, {"title": "Experiment 4: User preference of the summaries", "content": "The user study was also used to rank the summaries based on preferences. The aggregate preferences\nfor each summary choice were then analyzed. For automated plans, GPT-40 was the first preference\nfor 76% of users, followed by the baseline approach as the second preference for 44%, and TextRank"}, {"title": "7 Conclusion", "content": "In this work, we introduced the novel problem of planning task summarization. To address this\nproblem, we developed the PLANTS dataset, encompassing three distinct PL tasks: automated plans,\nrecipes, and travel routes. Alongside the dataset, we also presented a frequency-based baseline\nmethod for plan summarization. We evaluated both abstractive and extractive summarization methods\nfor planning task summarization through user studies and empirical analysis. Our findings indicate\nthat while GPT-40 is the preferred approach for generating plan summaries due to its detailed and\ninformation-rich outputs, further evaluation is needed to verify if these summaries maintain the\nexecutional semantics of PL tasks. The issue of hallucination in abstractive methods remains a\nsignificant challenge that warrants further investigation. Additionally, there is a need to develop\nevaluation metrics specifically tailored for PL task summaries to ensure their effectiveness and\nreliability.\nWe believe this work represents an initial effort towards advancing research in planning task summa-\nrization. The broader impact of this research could influence various domains, including robotics,\ndialog agents, and planning agents. We hope our contributions will inspire further advancements and\nexploration in this field, ultimately leading to more robust and efficient summarization techniques,\ndatasets, and evaluation metrics for the problem of planning task summarization."}, {"title": "8 Limitations", "content": "Size of the Dataset: While the PLANTS dataset provides a valuable starting point for planning task\nsummarization, it includes only 10 problems per domain, with 5 plans each for automated plans and\nrecipes, and 3 plans each for travel routes. This limited size may not fully capture the variability and\ncomplexity of real-world planning tasks. Additionally, the dataset does not include gold summaries,\nas it is challenging to obtain authoritative summaries for PL tasks due to their inherent variability and\nsubjective nature. However, to facilitate future research, we release the generators used to create this\ndataset, allowing for the development of larger and more diverse datasets across these domains."}]}