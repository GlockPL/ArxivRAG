{"title": "ADD-IT: TRAINING-FREE OBJECT INSERTION IN IMAGES WITH PRETRAINED DIFFUSION MODELS", "authors": ["Yoad Tewel", "Rinon Gal", "Dvir Samuel", "Yuval Atzmon", "Lior Wolf", "Gal Chechik"], "abstract": "Adding Object into images based on text instructions is a challenging task in semantic image editing, requiring a balance between preserving the original scene and seamlessly integrating the new object in a fitting location. Despite extensive efforts, existing models often struggle with this balance, particularly with finding a natural location for adding an object in complex scenes. We introduce Add-it, a training-free approach that extends diffusion models' attention mechanisms to incorporate information from three key sources: the scene image, the text prompt, and the generated image itself. Our weighted extended-attention mechanism maintains structural consistency and fine details while ensuring natural object placement. Without task-specific fine-tuning, Add-it achieves state-of-the-art results on both real and generated image insertion benchmarks, including our newly constructed \"Additing Affordance Benchmark\" for evaluating object placement plausibility, outperforming supervised methods. Human evaluations show that Add-it is preferred in over 80% of cases, and it also demonstrates improvements in various automated metrics. Our code and data will be available at: https://research.nvidia.com/labs/par/addit/", "sections": [{"title": "1 INTRODUCTION", "content": "Adding objects to images based on textual instructions is a challenging task in image editing, with numerous applications in computer graphics, content creation and synthetic data generation. A creator may want to use text-to-image models to iteratively build a complex visual scene, while autonomous driving researchers may wish to draw pedestrians in new scenarios for training their car-perception system. Despite considerable recent research efforts on text-based editing, this particular task remains a challenge. When adding objects, one needs to preserve the appearance and structure of the original scene as closely as possible, while inserting the novel objects in a way that appears natural. To do so, one must first understand affordance-the deep semantic knowledge of how people and objects interact, in order to position an object in a reasonable location. For brevity, we call this task Image Additing.\nSeveral studies (Hertz et al., 2022; Meng et al., 2022) tried addressing this task by leveraging modern text-to-image diffusion models. This is a natural choice since these models embody substantial knowledge about arrangements of objects in scenes and support open-world conditioning on text. While these methods perform well for various editing tasks, their success rate for adding objects is disappointingly low, failing to align with both the source image and the text prompt. In response, another set of methods took a more direct learning approach (Brooks et al., 2023; Zhang et al., 2023; Canberk et al., 2024). They trained deep models on large image editing datasets, pairing images with and without an object to add. However, these often struggle with generalization beyond their training data, falling short of the general nature of the original diffusion model itself. This typically manifests as a failure to insert the new object, the creation of visual artifacts, or more commonly \u2013 failing to insert the object in the correct place, i.e. struggling with affordances. Indeed, we remain far from achieving open-world object insertions from text instructions.\nHere we describe an open-world, training-free method that can successfully leverage the knowledge stored in text-to-image foundation models, to naturally add objects into images. As a guiding principle, we propose that addressing the affordance challenge requires methods to carefully balance between the context of the existing scene and the instructions provided in the prompt. We achieve this by: first, extending the multi-modal attention mechanism (Esser et al., 2024) of recent T2I diffusion models to also consider tokens from a source image; and second, controlling the influence of each multi-modal attention component: the source image, the target image and the text prompt. A main contribution of this paper is a mechanism to balance these three sources of attention during generation. We also apply a structure transfer step and introduce a novel subject-guided latent blending mechanism to preserve the fine details of the source image while enabling necessary adjustments, such as shadows or reflections. We name our method Add-it.\nImage Additing methods typically face three main failure modes: neglect, appearance, and affordance. While current CLIP-based evaluation protocols can partially assess neglect and appearance, there is a lack of reliable methods for evaluating affordance. To address this gap, we introduce the \"Additing Affordance Benchmark,\" where we manually annotate suitable areas for object insertion in images and propose a new protocol specifically designed to evaluate the plausibility of object placement. Additionally, we introduce a metric to capture object neglect. Add-it outperforms all baselines, improving affordance from 47% to 83%. We also evaluate our method on an existing benchmark (Sheynin et al., 2023) with real images, as well as our newly proposed Additing Benchmark for generated images. Add-it consistently surpasses previous methods, as reflected by CLIP-based metrics, our object inclusion metric, and human preference, where our method is favored in over 80% of cases, even against methods specifically trained for this task.\nOur contributions are as follows: (i) We propose a training-free method that achieves state-of-the-art results on the task of object insertion, significantly outperforming previous methods, including supervised ones trained for this task. (ii) We analyze the components of attention in a modern diffusion model and introduce a novel mechanism to control their contribution, along with novel"}, {"title": "2 RELATED WORK", "content": "Object Placement and Insertion. Inserting objects into images remains a core challenge in image editing. Traditional computer graphics methods often depend on manual object placement (C. Wang, 2014) or utilize synthetic data-driven approaches (Fisher et al., 2012). Early computer vision techniques employed contextual cues to predict possible object positions (Choi et al., 2012; Lin et al., 2013; Zhao et al., 2011). With advancements in deep learning, generative models have been trained to learn object placements. For example, Compositing GAN (Azadi et al., 2020) generates object composites by refining geometry and appearance, while RelaxedPlacement (Lee et al., 2022) optimizes object placement and sizing based on relationships depicted in scene graphs. OBJect3DIT (Michel et al., 2024) explores 3D-aware object insertion guided by language instructions, primarily using synthetic data. Despite their effectiveness, these methods often struggle with the complexities of real-world placement scenarios.\nEditing with Text-to-Image Diffusion Models. The emergence of high-performing text-to-image diffusion models (Rombach et al., 2022; Saharia et al., 2022; Ramesh et al., 2022; Balaji et al., 2022; Esser et al., 2024) has paved the way for effective text-based image editing techniques. Methods like Prompt-to-Prompt (Hertz et al., 2022) modify attention maps by injecting the input caption's attention into the target caption's attention, while SDEdit (Meng et al., 2022) uses a stochastic differential equation to iteratively denoise and enhance the realism of user-provided pixel edits. For editing real images, inversion techniques (Mokady et al., 2023; Wallace et al., 2022; Pan et al., 2023; Samuel et al., 2023; Deutch et al., 2024; Huberman-Spiegelglas et al., 2023; Tsaban & Passos, 2023; Brack et al., 2024; Garibi et al., 2024) first invert an input image to its latent noise representation using a given caption, enabling edits via methods like SDEdit or Prompt2Prompt. Cao et al. (2023) further improves real image editing using a mutual extended self-attention mechanism, an idea later extended to an array of generation (Tewel et al., 2024) and editing tasks like style- and appearance-transfer (Alaluf et al., 2024; Hertz et al., 2024) or object-dragging (Avrahami et al., 2024). Despite their effectiveness in various tasks, these methods struggle with object addition, often failing to align new objects with both the original image and the text prompt.\nTo improve editing performance, several methods proposed to directly fine-tune diffusion models. Imagic (Kawar et al., 2023) fine-tunes text embeddings (Gal et al., 2022a) and the diffusion U-Net (Ronneberger et al., 2015) to handle complex textual instructions, whereas Text2LIVE (Bar-Tal et al., 2022) and Blended Diffusion (Avrahami et al., 2022) blend edited regions throughout the generation. InstructPix2Pix (Brooks et al., 2023) introduced an instructable image editing model trained on a large synthetic dataset for instruction-based edits, while MagicBrush (Zhang et al., 2023) enhances this approach by fine-tuning InstructPix2Pix on a manually annotated dataset collected through an online editing tool. EmuEdit (Sheynin et al., 2023) trains a diffusion model on a large synthetic dataset to perform different editing tasks given a task embedding. Erase-Draw (Canberk et al., 2024) leverages inpainting models to automatically generate high-quality training data for learning object insertion. They show that one can train models to realistically insert diverse objects into images based on language instructions.\nDespite advancements in instruction-based image editing, we demonstrate that current methods still face significant challenges in accurately interpreting and executing object addition within images. In this paper, we propose a novel approach addressing the challenging task of object insertion. We show that by controlling the various attention components in the diffusion model, one can add new objects to existing images without further training or fine-tuning of the diffusion model."}, {"title": "3 METHOD", "content": "Our goal is to insert an object into a real or generated image using a simple textual prompt, ensuring the result appears natural and consistent with the source image. To achieve this, we leverage a pretrained diffusion model without any additional training or optimization. Our solution consists of three core components: (1) a weighted extended self-attention mechanism that balances information from the source image, text prompt, and target image, (2) a noising approach that preserves the source image's structure, and (3) a novel Subject-Guided Latent Blending mechanism to retain fine background details. For real images, we also introduce an inversion step, detailed below.\n3.1 PRELIMINARIES: ATTENTION IN MM-DiT BLOCKS\nModern Diffusion Transformers (DiTs) models, such as SD3 (Esser et al., 2024) and FLUX (Black-Forest, 2024), process concatenated sequences of textual-prompt and image-patch tokens through unified multi-modal self-attention blocks (MM-DiT blocks). Specifically, FLUX has two types of attention blocks: Multi-stream blocks which use separate projection matrices ($W_K$, $W_v$, $W_Q$) for text and image tokens, and Single-stream blocks where the same projection matrices are used for both. Both block types compute attention on the concatenated tokens as follows:\n$A = softmax([Q_p, Q_{img}][K_p, K_{img}]/\\sqrt{d_k}), h = A. [V_p, V_{img}]$\nwhere $Q_p$, $Q_{img}$ are the textual-prompt and the image-patch queries, respectively. The same applies to K and V. Notably, Flux is composed of a series of Multi-stream blocks followed by a series of Single-stream blocks.\n3.2 WEIGHTED EXTENDED SELF-ATTENTION\nOur approach builds on top of the attention mechanism in MM-DiT blocks. In this attention mechanism, tokens are drawn from two sources: the image patches $X_{image}$ and the textual prompt P. In prior attention-based diffusion architectures, it was shown that the appearance of a source image can be transferred to a target through an extended self-attention mechanism, where the new image can attend to the tokens of the source. We propose a similar extension here, by allowing the multi-modal attention to include another source the tokens of the input image we wish to edit. More formally, we define the three sources of information as: the source image $X_{source}$, the generated image $X_{target}$ and the textual prompt describing the edit $P_{target}$. To compute the source image tokens, we simply denoise it in parallel to the target image, and concatenate its keys and values to the self-attention blocks, extending eq. (1):\n$A = softmax([Q_p, Q_{target}][K_{source}, K_p, K_{target}] /\\sqrt{d_k}), h = A. [V_{source}, V_p, V_{target}]$\nwhere $K_{source}$ and $V_{source}$ are the keys and value extracted from the source image, and $K_p$, $V_p$, $K_{target}$, $V_{target}$ are the keys and values from the prompt and target image respectively. When $X_{source}$ is a generated image, denoising it in parallel is trivial - we simply need to start denoising from the same seed that created $X_{source}$. Dealing with a real image is more complicated, and we will describe our solution in the inversion section below.\nHowever, we notice that simply appending the keys and values of the source image to the attention blocks leads to the source image controlling the attention, which in turn leads to neglect of the edit prompt, with the final generated image being a simple copy of the source image. We explore the dynamics of this phenomenon in detail in section 5. To avoid this effect, we can re-balance the contribution of different attention components by weighting their keys. Indeed, by reducing the weight of the source image tokens, we can achieve better balance and allow for more changes. However, if this is not done carefully, then we risk upsetting the balance in the opposite fashion and seeing alignment with the source image completely ignored. Hence, we can introduce a weighting term to each source of information, giving us the following multi-modal attention equation:\n$A = softmax([Q_p, Q_{target}][\\gamma_s\\cdot K_{source}, \\gamma_p\\cdot K_p, \\gamma_t\\cdot K_{target}]/\\sqrt{d_k})$\n$h = A \\cdot [V_{source}, V_p, V_{target}]$\nwhere $\\gamma_s$, $\\gamma_p$, $\\gamma_t$ represent the weighting terms for the source image, the prompt, and the target image, respectively. In section 5 we explore the dynamics of the attention distribution across these three sources. In practice, we find that it is necessary to balance two key terms: the first is the attention distributed over the source image $A_{source} = \\frac{exp(Q_p K_{source})}{Z}$ and the second is the attention distributed over the target image, $A_{target} = \\frac{exp(\\gamma_p K_{target})}{Z}$, where Z is the softmax normalization term. To determine $\\gamma$ we define the function $f(x) = A_{source} - A_{target}$ and use a root-solver algorithm to find $\\gamma$ such that f(x)=0.\n3.3 STRUCTURE TRANSFER\nThe weighted extended-attention mechanism allows to balance between information from the source image and the prompt, but the added objects do not always adhere to the image context (e.g. dog is too big for the chair). We attribute this issue to different seeds dictating specific structures in the generated image, which do not always align with the source image. We show that effect in fig. 8, where images generated with the same seed produce similar objects with or without the extended attention mechanism. To address this problem, we propose to \"choose\" seeds with a structural similarity to the source image. We do so by noising the source latent $X_{source}$ to a very high noise level $t_{struct}$ with randomly sampled noise $ \\epsilon \\sim N(0, I)$ following the recitified flow denoising formula $X_t = (1 - \\sigma_t^2)x_0 + \\sigma_t\\epsilon$. When $t_{struct}$ is high enough, starting the denoising process from $X_{t_{struct}}$ will result in an image with similar global structure to the source image, while still allowing for changes to image content as demonstrated in fig. 8.\n3.4 SUBJECT GUIDED LATENT BLENDING\nThe combination of structure transfer and the weighted attention mechanism ensures that the target image remains consistent with the structure and appearance of the source image, though some fine details, such as textures and small background objects, may still change. Our goal is to preserve all elements of the source image not affected by the added object. To achieve this, we propose Latent Blending; A naive approach would involve identifying the pixels unaffected by the object insertion and keeping them identical to those in the source image. However, two challenges arise: First, a perfect mask is needed to separate the object from the background to avoid artifacts. Second, we aim to preserve collateral effects from the object insertion, such as shadows and reflections. To address these issues, we propose generating a rough mask of the object, which is then refined using SAM-2 (Ravi et al., 2024) to obtain a final mask M. We then blend (Avrahami et al., 2022) the source and target noisy latents at timestep $T_{blend}$ based on this mask.\nTo extract the rough object mask, we gather the self-attention maps corresponding to the token representing the object. We achieve this by multiplying the queries from the target image patches, $Q_{target}$, with the key associated with the added object token, $k_{object}$. These maps are then aggregated across specific timesteps and layers that we identified as generating the most accurate results (further details can be found in the appendix A.1. We then apply a dynamic threshold to the attention maps using the Otsu method (Otsu, 1979) to obtain a rough object mask, $M_r$. Finally, we refine"}, {"title": "3.5 Additing REAL IMAGES AND STEP-BY-STEP GENERATION", "content": "In the previous sections, we described our method for generating an edited image by drawing information from a source image within the same batch. When editing a generated image, this process is straightforward: one can save the source noise, $\\epsilon_{source}$, that generated the source image and create an input batch containing both $\\epsilon_{source}$ and a random noise, $\\epsilon_{target}$, used to generate the target image. However, when editing an existing image, $X_{source}$, we do not have access to its original noise. A common approach would be to use an inversion method to recover the original noise, $\\epsilon_{source}$, that generated $X_{source}$. However, in our experiments, popular inversion methods, such as DDIM inversion (Song et al., 2020), do not adequately reconstruct the image using FLUX. We propose a simple solution: instead of recovering the original noise $\\epsilon_{source}$, we sample a random noise $\\epsilon$. At each denoising step t, we produce a noisy source latent, $X_t^{source} = (1 \u2013 \\sigma_t^2)X_{source} + \\sigma_t\\epsilon$. We then apply our method as usual, using the input batch at timestep t, $[X_t^{source}, X_t^{target}]$, where the target image draws information from the source image. This simple technique ensures perfect reconstruction of the source image, since $\\sigma_0 = 0$ and therefore $X_0^{source} = X_{source}$.\nOur method, applicable to both generated and real images, can be extended for step-by-step generation. Users can start with an initial image from a textual prompt and iteratively modify it with additional prompts, progressively adding elements or changes to the scene. Examples of step-by-step generation are shown in fig. 1 and fig. 11."}, {"title": "4 EXPERIMENTS", "content": "Evaluation Baselines We compare our method with two classes of baselines: (1) Training-Free methods that leverage the existing capabilities of text-to-image models: Prompt-to-Prompt (Hertz et al., 2022), a method which injects the attention map of the source image into the target image to preserve its structure, and SDEdit (Meng et al., 2022), a method that adds partial noise to an existing image and then denoises it. Both methods were re-implemented on the FLUX.1-dev model for fair comparison. (2) Pretrained Instruction following models, specifically trained to edit and add objects to existing images: InstructPix2Pix (Brooks et al., 2023) an instruction following model trained on a large scale of synthetic instruction data, Magicbrush (Zhang et al., 2023) a version of InstructPix2Pix fine-tuned on manually annotated editing dataset, and Erasedraw (Canberk et al., 2024) a model trained on large dataset constructed using an inpainting model. Add-it implementation details can be found in appendix A.1.\nMetrics We evaluate the results of our method and the baselines using automatic metrics and human evaluations for each source and target image-caption pair. Automatic Metrics: we start by adopting the CLIP (Radford et al., 2021) based metrics proposed in Emu-Edit (Sheynin et al., 2023): (i) CLIPdir (Gal et al., 2022b) measures the agreement between change in captions and the change in images. (ii) CLIPimg measures similarity between source and target images. (iii) CLIP out measures the target image and caption similarity. We propose two additional metrics: (iv) Inclusion measures the portions of cases the object was added to the image, evaluated automatically using the open-vocabulary detection model Grounding-DINO (Liu et al., 2023). (v) Affordance measures whether the object was added to a plausible location, utilizing Grounding-DINO and a manually annotated set of possible locations. Human Evaluations: we ask human raters to pick the best Additing output when faced with a source image, instruction and images generated by our method and a competing baseline. Further details in appendix A.5.\n4.1 EVALUATION RESULTS\nEmu-Edit Benchmark Following EraseDraw (Canberk et al., 2024) we evaluate our method on a subset of EmuEdit's (Sheynin et al., 2023) validation set with the task class of \"Add\", designed for insertion instructions. The benchmark consists of sets of images and prompts before and after an edit, and the corresponding instruction. Table 2 shows our model outperforms all previous approaches in the CLIPdir, CLIPout and the Inclusion metrics. In the CLIPim metric, which indicates how close the edited image is to the source image, we are second only to Erasedraw. This result is"}, {"title": "5 ANALYSIS", "content": "In this section, we analyze the attention distribution in the MM-DiT block and the key components of our method to better justify our design choices. In appendix A.3 we analyze the role of positional encoding in the extended-attention mechanism.\nMM-DiT Attention Distribution First, we analyze the different attention components in the extended MM-DiT blocks. Recall that in the extended-attention mechanism described in section 3.2 there are three token sets: the source image $X_{source}$, the target image $X_{target}$ and the prompt $P_{target}$. In our experiments, we notice that simply applying the extended attention mechanism results in the target image closely following the appearance of the source image while neglecting the prompt - meaning no object is added to the image. We attribute this problem to the way the attention is distributed across the three sets of tokens. In particular, we find empirically that the target prompt's attention $A_p \\propto exp(Q_p\\cdot[K_{source}, K_p, K_{target}])$ serves as an effective proxy for balancing the three sources of attention. A simple way to control the attention distribution is by introducing scale factor $\\gamma_p, \\gamma_{target}$ so that $A_p \\propto exp(Q_p\\cdot[K_{source}, \\gamma_p\\cdot K_p, \\gamma_{target}\\cdot K_{target}])$. In practice, we find that using $\\gamma = \\gamma_p = \\gamma_{target}$ is adequate. In fig. 7 (B) we visualize the prompt attention $A_p$ spread across the three token sets. In the standard extended attention case ($\\gamma$ = 1.0), the source image tokens (purple) receive more attention than the target image tokens (orange), preventing the generated image from incorporating the added object. On the other hand when scaling up too much ($\\gamma$ = 1.2), the target image tokens overwhelm the source image token, causing the output image to stray away from the source image structure. Finally, when the scaling value balances the attention between $X_{source}$ and $X_{target}$ ($\\gamma$ = Auto), the output image successfully incorporates the added object, while preserving the target image structure and taking into account its context when placing the object. These observations are qualitatively shown in fig. 7 (C) and are also reflected in fig. 7 (A), where the scale that balances the attention offers a good balance between affordance and Object Inclusion.\nAblation Study Next, we evaluate the impact of different components of our method. First, we demonstrate the effect of the weight scale, $\\gamma$. In fig. 7 (A) we present a graph showing affordance and Object Inclusion as functions of different weight scales. As the weight scale increases, the added object tends to appear more frequently in the image. However, beyond a certain threshold, the affordance score drops. This decline occurs when the target image ignores the structure of the source image, generating objects in unnatural locations, as illustrated in fig. 7 (C). Next, we explore the effect of latent blending. In fig. 9 we show output images with and without the latent blending step, along with the affordance map automatically extracted by our method. Notice how the blending step aligns the fine details of the source image without introducing artifacts. Finally, we examine the structure transfer component. In fig. 8 we illustrate the effect of applying the structure transfer step at different stages of the denoising process. When the structure transfer is applied too early, the affordance score is low, meaning the target image does not adhere to the structure of the source image. On the other hand, applying it later in the process results in a lower object inclusion metric, indicating that the target image neglects the object. Ultimately, when the structure transfer is applied at t = 933, we achieve a balance between object inclusion and affordance. A qualitative example is also provided in fig. 8."}, {"title": "6 LIMITATIONS", "content": "Add-it shows strong performance across various benchmarks, but it has some limitations. Since the method relies on pretrained diffusion models, it may inherit biases from the training data, which could affect object placement in unfamiliar or highly complex scenes. Additionally, because our method uses target prompts rather than explicit instructions, users may need to construct more detailed prompts to achieve the same edit. For instance, with an image of a dog, the prompt \"A dog\" won't add another dog to the scene, as seen in fig. 10. Instead, the user would need to provide alternative prompts, such as \"Two dogs sitting on the grass\". Lastly, we observe that Additing on real images is still not as effective as it is on generated images. We attribute this shortcoming to the current FLUX inversion algorithm and believe that a more advanced inversion algorithm could help bridge this gap. Additional failure cases of the model are presented in fig. 16."}, {"title": "7 CONCLUSION", "content": "We introduced Add-it, a training-free method for adding objects to images using simple text prompts. We analyzed the attention distribution in MM-DiT blocks and introduced novel mechanisms such as weighted extended-attention and Subject-Guided Latent Blending. Additionally, we addressed a critical gap in evaluation by creating the \"Additing Affordance Benchmark,\u201d which allows for an accurate assessment of object placement plausibility in image Additing methods. Add-it consistently outperforms previous approaches, improving affordance from 47% to 83% and achieving state-of-the-art results on both real and generated image benchmarks. Our work demonstrates that leveraging the knowledge in pretrained diffusion models is a promising direction for tackling challenging tasks like image Additing. As diffusion models continue to evolve, methods like Add-it have the potential to drive further advancements in semantic image editing and related applications."}, {"title": "ETHICS STATEMENT", "content": "In this work, we acknowledge the ethical considerations associated with image editing technologies. While our method enables advanced object insertion capabilities, it also has the potential for misuse, such as creating misleading or harmful visual content. We strongly encourage the responsible and ethical use of this technology, emphasizing transparency and consent in its applications. Additionally, biases present in pretrained models may affect generated outputs, and we recommend further research to mitigate such issues in future work. Human evaluations were conducted with informed consent."}, {"title": "A APPENDIX", "content": "A.1 IMPLEMENTATION DETAILS\nAdd-it When evaluating Add-it, we use $t_{struct}$ = 933 for generated images and $t_{struct}$ = 867 for real images and $t_{blend}$ = 500. For the scaling factor $\\gamma$, we use the root-finding solver described in section 3.2 on a set of validation images and set $\\gamma$ to 1.05, as it is close to the average result and performs well in practice. We generate the images with 30 denoising steps, building upon the diffusers implementation of the FLUX.1-dev model. We apply the extended attention mechanism until step t = 670 in the multi-stream blocks, and step t = 340 for the single-stream blocks.\nLatent Blending Localization To extract a refined object mask as part of the Subject Guided Latent Blending component, we begin by extracting subject attention maps. Empirically, we find that the best-performing layers for this task are: [\"transformer_blocks.13\",\"transformer_blocks.14\", \"transformer_blocks.18\", \"single_transformer_blocks.23\", \"single_transformer_blocks.33\"]. To refine the mask from these attention maps, we need to identify points to use as prompts for SAM-2. To extract points from the attention map, we first select the point with the highest attention value. Then, we exclude the area around the chosen point and select the next highest point. This process is repeated until we either identify 4 points or the current maximal point value falls below 0.35 $P_{max}$, where $P_{max}$ is the initial maximum attention value. Finally, we feed the points to the SAM-2 model to end up with a refined object mask.\nA.2 ADDITIONAL RESULTS\nIn fig. 11 we present step-by-step outputs generated with Add-it. Notice that the scene remains unchanged, while each prompt adds an additional \"layer\" to the final image, resulting in a more complex scene.\nIn fig. 12 we show additional results from the Additing Affordance benchmark. In each case, the object must be added to a specific location in the source image. Across all examples, Add-it successfully places the object in a plausible location, preserving the natural appearance of the image.\nHere, we examine the significance of positional encodings in the extended attention mechanism. fig. 15 demonstrates their role through a simple experiment: we applied our method to a source image where the positional encoding vectors were shifted down and to the right. This misalignment resulted in a mismatch between the positional encoding of the child's head in the source and target images. Consequently, instead of generating headsets at the actual position of the child's head, the model produced them in the area corresponding to the \"shifted head\" position. This outcome demonstrates that the model heavily relies on positional information to transfer features between the source and target images. Despite the target image containing \"laptop\" features instead of \"head\" features at the relevant location, the model chose to place the headphones there. This decision was based on the area having the same positional encoding as the \"head area\" in the source image, rather than on the actual content of the target image at that location. We believe further research on the role of positional encoding vectors is an interesting direction for future work in the context of DiT models.\nA.4 Additing AFFORDANCE BENCHMARK\nDataset Construction Here, we provide the details for constructing the Additing Affordance Benchmark dataset. First, we used ChatGPT-4 to generate a dataset of tuples, each consisting of a source prompt and a target prompt, representing an image before and after object insertion, along with an instruction for the transition and a subject token representing the object to be added. The exact prompt is shown in fig. 18. Next, we used FLUX.1-dev to generate the source images from the source prompts in each tuple. We manually filtered out images where the object had no plausible\nA.5 USER STUDY DETAILS\nWe evaluate the models through an Amazon Mechanical Turk user study using a two-alternative forced choice protocol. In the study, raters saw an instruction, a source image, and two edited images, each produced by a different approach. They chose the edit that best followed the instruction, taking into account: image quality and realism, instruction following and preservation of the source image. For the evaluation, each head-to-head example was rated by two raters. In fig. 19 we show an example of a single trial a rater has seen."}]}