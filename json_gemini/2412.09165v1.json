[{"title": "When Text Embedding Meets Large Language Model: A Comprehensive Survey", "authors": ["Zhijie Nie", "Zhangchi Feng", "Mingxin Li", "Cunwang Zhang", "Yanzhao Zhang", "Dingkun Long", "Richong Zhang"], "abstract": "Text embedding has become a foundational technology in natural language processing (NLP) during the deep learning era, driving advancements across a wide array of downstream tasks. While many natural language understanding challenges can now be modeled using generative paradigms and leverage the robust generative and comprehension capabilities of large language models (LLMs), numerous practical applications-such as semantic matching, clustering, and information retrieval-continue to rely on text embeddings for their efficiency and effectiveness. In this survey, we categorize the interplay between LLMs and text embeddings into three overarching themes: (1) LLM-augmented text embedding, enhancing traditional embedding methods with LLMs; (2) LLMs as text embedders, utilizing their innate capabilities for embedding generation; and (3) Text embedding understanding with LLMs, leveraging LLMs to analyze and interpret embeddings. By organizing these efforts based on interaction patterns rather than specific downstream applications, we offer a novel and systematic overview of contributions from various research and application domains in the era of LLMs. Furthermore, we highlight the unresolved challenges that persisted in the pre-LLM era with pre-trained language models (PLMs) and explore the emerging obstacles brought forth by LLMs. Building on this analysis, we outline prospective directions for the evolution of text embedding, addressing both theoretical and practical opportunities in the rapidly advancing landscape of NLP.", "sections": [{"title": "I. INTRODUCTION", "content": "Text embedding learning is a fundamental task in Natural Language Processing (NLP), which aims to extract features from text with the natural languages (including words, sentences, and documents). Formally, given the text X, text embedding learning aims at training a model $f : X \u00d7 0 \u2192 R^d$ on the dataset D C X. The continue vector $h = f_o(x)$ output by f is called as the embedding of the text x, which is expected to contain valid information in the input text and perform well on the downstream tasks.\nAdvanced large language models (LLMs) have recently demonstrated exceptional generalization and transfer capabilities in downstream tasks where generative paradigms are applicable, such as information extraction, text classification, and machine translation. However, not all tasks are suitable for modeling using generative paradigms; tasks such as dense retrieval, semantic text similarity, etc., still require text embedding for computing similarity. In this aspect, LLMs are not different from traditional encoder-only PLMs and do not directly produce high-quality embedding. Therefore, how to obtain high-quality text embeddings remains a key problem.\nFortunately, LLMs have comprehension and generation capabilities far beyond those of previous models, which opens up new opportunities for text representation learning. Specifically, LLMs change the existing landscape in two ways: (1) LLMs can act as data annotators for large-scale, high-quality, and fine-grained text datasets, and (2) LLMs can replace current PLMs as model pedestals for generating higher-quality textual representations. Many works have been devoted to introducing large models for obtaining high-quality text embedding. However, most works focus on a single task, and similar methods have been \"proposed\u201d repeatedly in different fields. This leads to a lack of comprehensive and objective understanding of the role played by LLMs in text embedding across various fields.\nAt the same time, the development of LLM as introduced a number of emerging tasks. Examples include long context compression developed to improve the reasoning efficiency of LLM, and embedding inversion tasks to study the information leakage in text. Although the two tasks are developed with different motivations, they share the similar framework and use LLM to understand the information in the text embedding.\nThis survey focuses on deep representation learning methods, which train the encoders to map texts to a high-dimensional space. We hope this survey will help researchers focusing on different tasks find commonalities in the problem, combining efforts to promote more rapid development in this area. Compared to the existing surveys [1, 2] in this field, this survey covers more comprehensive downstream tasks. They not only include the traditional downstream tasks that require text embeddings (such as semantic text similarity, information retrieval, and text clustering) but also contain the emerged tasks in the era of LLMs (such as long context compression and embedding inversion). In addition, this survey discusses the relationship between LLMs and text embedding for the first time, mainly including (1) LLM-augmented text embedding, (2) LLM as text embedders, (3) Text embedding understanding with LLMs. Fig. 1 shows the relationship between the text embedding model (which we call text embedder) and the LLM in each section.\nThe subsequent sections will unfold in the following order: Section II introduces the developed stage, the training and evaluation tasks for text embedding; Section III introduces the text embedding methods where LLMs are used for data augmentation and while another model is used to be the text embedder; Section IV presents the text embedding methods where LLMs are the backbones of text embedders; Section V show two novel text embedding-related tasks in the LLM era: long context compression and embedding inversion. Section VI discusses the remained challenge before the presentation"}, {"title": "II. PRILIMINARY", "content": "A. History of Text Embedding\n1) Era of Statistics Machine Learning: In the early days, researchers primarily relied on manually designed features to represent text. The development of the method is accompanied by a transformation of the output vector's form, which is from bag-of-words models (one-hot vectors) to TF-IDF [3] (sparse vectors), then to word embeddings (dense vectors). These methods require domain experts to select and design features manually, and the quality and quantity of features limit their effectiveness. As machine learning techniques advanced, researchers began exploring methods to learn text embeddings using statistical approaches such as Latent Semantic Analysis (LSA) [4], Latent Dirichlet Allocation (LDA) [5], and others. While these methods can automatically learn text embedding, they still have limitations in capturing complex semantic and syntactic structures.\n2) Era of Deep learning: With the emergence of deep learning techniques, word embedding has become a significant breakthrough in text embedding learning. Models such as Word2Vec [6], GloVe [7], and FastText [8] can map words into a low-dimensional continuous vector space to capture semantic and syntactic relationships between words. Over the past few years, pre-trained language models (PLMs) with millions of parameters (e.g., BERT [9] and ROBERTa [10]) have been proposed first. The \u201cpretraining then fine-tuning\" paradigms have performed well on various downstream tasks and played an important role in practical on-the-ground applications. However, the embedding spaces of these PLMs are proved to be anisotropic [11], leading to surprisingly high similarities computed for any two texts.\nIn this phase, the centre of text embedding learning shifts to fine-tuning PLMs for higher-quality text embeddings. Some supervised methods were explored first, followed by post-processing and unsupervised methods. Despite the tremendous progress, there is still a huge performance gap between supervised and unsupervised methods. Then contrastive learning is introduced to use the negative example fully, which focuses on constructing positive and negative examples for the samples in each dataset and training the model to differentiate between positive and negative examples. The common contrastive learning loss is InfoNCE [12], which can be expressed as\n$L_{cl} = -E_{x~D} log \\frac{exp (s(h, h^+))}{exp (s(h, h^+)) + \\Sigma_{j} exp (s(h,h_j))}$         (1)\nwhere $s: R^d x R^d \u2192 R$ is the distance function, while $x^+$ and $\\{x_j\\}_{j=1}^{N}$ is the positive example and negative examples for the text x, respectively. Improvements in contrastive learning have focused on (1) the form of the loss function [13, 14, 15, 16] and (2) better methods for constructing positive and negative samples [17, 18, 19, 20]."}, {"title": "B. Large Language Model", "content": "We use the term \"large language models\" or \"LLMs\" to exclude pre-trained language models based on the Transformer encoder (such as BERT and RoBERTa) and neural networks with typically smaller parameter counts (such as LSTM [21] and GRU [22]). We refer to the language models whose variants' parameter number is more than 1B as LLMs. Therefore, the LLMs we study include:"}, {"title": "C. Training Dataset", "content": "Text-embedded data involves multiple downstream applications, and advanced methods often integrate massive amounts of data from various domains in various languages involved in training."}, {"title": "D. Downstream Task for Evaluation", "content": "Text embedding learning often serves multiple downstream tasks and cannot be trained using label Y as in text classification tasks. Therefore, one research focus of text embedding learning mainly lies in designing pretext tasks and constructing corresponding datasets.\n1) Semantic Textual Similarity: The Semantic Textual Similarity (STS) task quantifies the semantic similarity degree between two texts.\na) Evaluation Dataset: The dataset for evaluating STS can be expressed as $D_{eval} = \\{(x_i, x'_i, c_i)\\}_{i=1}^{n}$, where $(x_i,x'_i)$ is a text pair and $c_i$ is the average similarity between the two texts. The popular evaluation protocol usually contains seven STS datasets for English setting, which contains STS12-16 [38, 39, 40, 41, 42], STS-B [43] and SICK-R [44] in SentEval Benchmark [45] and STS-17 [43] and STS-22 [46] for multi-lingual setting.\nb) Evaluation Metric: The model is expected to take two texts $(x,x')$ as input and produce a predicted score $\\hat{c_i}$ that represents the similarity of the two texts. Common evaluation metrics include the Pearson correlation coefficient and the Spearman correlation coefficient. The Pearson correlation coefficient primarily assesses the correlation between absolute values of $c_i$ and $\\hat{c_i}$, which is expressed as\n$r_{pearson} = \\frac{\\Sigma_{i=1}^{n}(c_i - \\bar{c_i})(\\hat{c_i} - \\bar{\\hat{c_i}})}{\\sqrt{\\Sigma_{i=1}^{n}(c_i - \\bar{c_i})^2} \\sqrt{\\Sigma_{i=1}^{n}(\\hat{c_i} - \\bar{\\hat{c_i}})^2}}$      (2)\nwhere $\\bar{c_i}$ and $\\bar{\\hat{c_i}}$ are the mean value of $\\{c_i\\}_{i=1}^{n}$ and $\\{\\hat{c_i}\\}_{i=1}^{n}$, respectively. And the Spearman correlation coefficient primarily assesses the correlation between the ranks of $c_i$ and $\\hat{c_i}$ in their respective lists, which is expressed as\n$\\tau_{spearman} = 1 - \\frac{6\\Sigma_{i=1}^{n}(r_i - r'_i)^2}{\u043f(\u043f^2 - 1)}$         (3)\nwhere $r_i$ and $r'_i$ are the rank of $c_i$ and $\\hat{c_i}$ in their respective lists, respectively.\n2) Information Retrieval: Information retrieval (IR) aims to retrieve documents relevant to a query, where their matching scores measure the relevance between the query and the documents. (In this survey, we focus on text retrieval.) Information retrieval systems can be roughly divided into two stages. The first stage (retriever) aims to reduce the candidate space by retrieving relevant candidate documents. The second stage (reranker) mainly focuses on reranking the candidate documents.\na) Evaluation Dataset: Generally, the dataset for training and evaluation can be written as $D = \\{q_i,d_i^+\\}_{i=1}^{Q}$, where $q_i$ is the query and $d_i^+$ are the positive documents that match $q_i$. The popular evaluation protocol usually use BEIR [47] for English setting, MIRACL [48] and Mr.TyDi [49] for multi-lingual setting.\nb) Evaluation Metric: Given queries and documents, the IR model can provide their matching scores, and the quality of the model can be evaluated through these scores. For the first stage, common evaluation metrics include Recall@k and Accuracy@k, while for the second stage, metrics like MAP, MRR and NDCG are generally used.\nRecall@k calculates a truncated Recall value at the k-th position of a retrieved list.\n$Recall@k = \\frac{1}{|Q|}\\Sigma_{q=1}^{|Q|} \\frac{R_{q,k}}{S_q}$     (4)\nwhere Q is the query set. $S_q$ denotes the total number of relevant texts for query q, and $R_{q,k}$ denotes the number of relevant texts retrieved in the top-k positions by the retrieval model.\nAccuracy@k calculates the proportion of queries for which the top-k retrieved texts contain the answers, defined as:\n$Accuracy@k = \\frac{1}{|Q|}\\Sigma_{q=1}^{|Q|} I(R_{q,k} > 0)$,   (5)\nwhere I() is an binary indicator function that only returns 1 when the case is true.\nFurthermore, Mean Average Precision (MAP) calculates the mean of Precision scores over a set of queries Q:\n$MAP = \\frac{1}{|Q|}\\Sigma_{q=1}^{|Q|}AP_q$        (6)\nwhere $AP_q = \\Sigma_{k=1}^{S_q}Precision@k \u00d7 I(q,k)$, and $Precision@k = \\frac{1}{k}\\Sigma_{i=1}^{k}\\frac{1_R_{qk}}{i}$\nMRR, averages the reciprocal of the rank of the first retrieved positive text over a set of queries Q .\n$MRR = \\frac{1}{|Q|}\\Sigma_{q=1}^{|Q|}\\frac{1}{rank_q}$     (7)\nwhere $rank_q$ is the position of the first retrieved positive text.\nIn addition to quantifying the frequency of positive texts, Normalized Discounted Cumulative Gain (DCG) also considers the position of a relevant text. It favors a ranking system that situates a pertinent text at a superior position.\n$DCG_q@k = \\Sigma_{i=1}^{k} \\frac{2^{r_i} -1}{log_2(i+1)}$  (8)\nwhere $r_i$ is the graded relevance score for the i-th retrieved text. According to the aforementioned definition of DCG, NDCG is the aggregate of the normalized DCG values at a specific rank position.\n$NDCG@k = \\frac{1}{|Q|}\\Sigma_{q=1}^{|Q|} \\frac{DCG_q@k}{IDCG_q@k}$     (9)\nwhere IDCG@k denote ideal discounted cumulative gain at a particular rank position k."}, {"title": "E. Emerging Related Tasks", "content": "1) Long Context Compression: Long Context Compression involves compressing long context tokens for large models without sacrificing essential information, accelerating inference while maintaining performance in scenarios with extensive token lengths.\na) Evaluation Dataset: The Question Answer (QA) dataset for Long Context Compression task can be formalized as follows: $D = \\{(C_i, q_i, a_i)\\}_{i=1}^{n}$ where $C_i$ represents the long context, $q_i$ represents the question, $a_i$ represents the correct answer to the question $q_i$, derived from the context $C_i$. The training aims to allow the model to extract the correct answers from both the original long context $C_i$ and the compressed context $C'_i$. Evaluation is performed by measuring the model's ability to provide the correct answer $a_i$ even when the context has been compressed.\nb) Evaluation Metric: In Long Context Compression, evaluations are generally divided into two approaches. The first is to assess compression efficiency using metrics like context compression rate [79] and average inference time [80]. The second is to directly evaluate the generated quality by applying downstream QA tasks. In QA tasks, the generated answers are typically compared to the reference answers using various metrics to calculate consistency scores. Common metrics include Perplexity [81], Exact Match [80], and ROUGE-L [80].\nContext Compression Rate indicates The ratio of the compressed context length to the original context length:\n$CompressionRate = \\frac{len(C')}{len(C)}$     (10)\nPerplexity measures how well the model predicts the next word in a sequence, indicating its confidence and fluency:\n$Perplexity = (\\frac{1}{n} \\Sigma_ {i=1}^{n}(Ip(W_i/W_1,..., W_{i-1}))$ (11)\nExact Match (EM) checks if the generated answer exactly matches the reference answer, rewarding only perfect matches:\n$EM = \\frac{1}{N} \\Sigma_{i= 1} \u2161(prediction = groundtruth)$ (12)\nROUGE-L evaluates the overlap between the generated answer and the reference answer by focusing on the longest common subsequence, allowing for partial correctness:\n$ROUGE-L = \\frac{LCS(X,Y)}{len(Y)}$         (13)\n$ROUGE-LR = \\frac{LCS(X,Y)}{len(X)}$      (14)\n$ROUGE-L = \\frac{(1 + \u03b2^2) ROUGE-Lp \u00b7 ROUGE-LR}{\u03b2^2 ROUGE-Lp + ROUGE-LR}$ (15)\n2) Embedding Inversion: Embedding inversion is a type of privacy leakage that predicts relevant information based on the embedding of the input text. It is generally regarded as an attacker, with targets including attribute information in the input text, the entire input text, or the text embedding model itself.\na) Evaluation Dataset: For different levels of the leak information, the datasets can be word-level or sentence-level. For attribute inference attack, the dataset has the form of (Input,Words), where the \"Input\" is text and \"words\" represents important information in the input text, for example name, ID number. For the purpose of reconstruct all the input text, the dataset just contains input text. It should be noted that, text embedding will not be contained in dataset directly, it is generated by the victim embedding model using the text in dataset as input.\nb) Evaluation Metric: The main metrics used for evaluating embedding inversion include BLEU, ROUGE, Exact-Match, cosine similarity, and Token F1.\nBLEU is used to measure n-gram similarity between reconstructed text and original input text. we can compute it following:\n$BLEU = BP \u00d7 exp\\Sigma_{n=1}^{N}w_nlog(p_n)$  (16)\nwhere BP is the brevity penality, $w_n$ is the weight for each n-gram precision and $p_n$ is the modified precision for n-th n-gram.\nROUGE represents the recall of overlapping words of reconstructed text and Exact-Match represents the percentage of exactly matching reconstructed text to the original input text. While the cosine similarity is computed in the embedding space between embeddings of reconstructed text and original input text. The \"Token F1\" metric is the multi-class F1 scores between tokens of predicted text and input text and It is word-level metric."}, {"title": "III. LLM-AUGMENTED TEXT EMBEDDING", "content": "One approach to adopting LLMs for text embeddings is through knowledge distillation. Specifically, LLMs can be used to generate training data for the embedding model in two ways: 1. Directly synthesizing training data (\u00a7 III-A); 2. Providing supervision signals for existing data (\u00a7 III-B). \nA detailed description of different methods is presented in Table III.\nA. Data Synthesis with LLMS\nCurrent text embedding models are typically trained using contrastive learning [12], where the training data commonly consist of three components: anchors, positive samples, and negative samples. The specific definitions of these components vary across tasks. For example: In Information Retrieval (IR), the anchor is the query, while the positive and negative samples are documents related and unrelated to the anchor, respectively. In Semantic Textual Similarity (STS) tasks, the anchor is a text, with positive and negative samples being semantically similar and dissimilar texts. Additionally, recent studies [104, 100] have begun integrating instruction-following capabilities into text embedding models, resulting in a need for instructions in the training data. In this section, we provide an overview of how LLMs can be used to synthesize the different components of training data.\n1) Instructions: Existing instructions in training data are typically constructed based on datasets. This involves manually collecting multi-task datasets and applying templates to generate different instructions for each dataset [104]. However, this approach lacks diversity, limiting the full potential of the model's instruction-following capabilities. Therefore, a promising solution is to generate instance-level instructions by using LLMs. One approach directly leverages LLMs to generate instructions: I3 [96] generates diverse instructions by manually setting different conditions, including topics, organizational formats of the retrieved text, and definitions of relevance; E5mistral [102] categorizes tasks into different types (e.g., symmetric and asymmetric retrieval tasks) and designs specific prompts for each category to generate instructions. Another approach generates instructions based on existing data: Gecko[101] prompts LLMs to generate instructions by conditioning on different documents; Promptriever[100] generates instructions that describe the relationship between the given queries and documents, and enhance instruction diversity by setting various conditions such as length and style of the generated instructions.\n2) Positive Samples: Based on the formal similarity between positive samples and anchors, positive samples can generally be classified into the following two categories.\na) Symmetric Positive Samples: A classic task where anchors and positive samples are symmetric is STS. In this area, numerous studies have proposed various methods to generate symmetric positive samples. Based on the analysis in S-BERT [105], text embedding models for STS are often trained using training data from the Natural Language Inference (NLI) task. Following this convention, a line of research explores how to use LLMs to generate NLI-style training data: Two studies [86, 92] directly utilize the in-context learning capabilities of LLMs by using NLI-formatted training data as demonstrations to guide the generation of entailment sentences as positive samples; Other studies [82, 84, 88] first fine-tune LLMs with NLI training data, enabling them to generate NLI-formatted positive samples. Notably, AdaptCL [88] employ a reinforcement learning approach to iteratively update both the data generation model and the text embedding model, achieving improved performance. In addition, some studies explore generating other types of semantically similar texts as positive samples: Two studies [83, 85] generate diverse semantically similar positive samples by setting different conditions (e.g., genre, topic) and leveraging the in-context learning capabilities of LLMs; SKICSE [89] constructs positive samples by extracting additional information about the anchor using the inherent knowledge of LLMs. SumCSE [87] generates positive samples by having LLMs summarize the anchor; CLAIF [91] creates positive samples by masking tokens in the anchor and completing the missing parts using LLMs; E5mistral [102] further subdivides symmetric retrieval tasks into STS tasks and bitext retrieval tasks, designing specific prompts to generate different types of positive samples for each.\nb) Asymmetric Positive Samples: In asymmetric cases, the anchor and positive sample are often referred to as the query and document, respectively. In various studies, either the query or the document can be the target for generation by LLMs. Many works [93, 94, 95, 97, 98, 101] use the vast amount of documents available on the internet to generate queries: Some of them [93, 94, 95, 97, 101] directly leverage the in-context learning capability of LLMs, constructing prompts to facilitate query generation. Notably, InPars [94, 95] introduces a \"Guided by Bad Questions (GBQ)\u201d template, which uses randomly sampled pseudo-queries to improve the quality of generated queries; Moreover, another study [98] employs soft prompts to fine-tune LLMs, enabling them to generate queries based on documents. In addition to generating queries from documents, there are other generation methods: I3 [96] and E5mistral [102] generate both queries and documents based on their self-generated instructions; FollowIR [103] and promptriever [100] guides document generation using queries and the generated instructions.\n3) Negative Samples: Negative samples are crucial in contrastive learning, and research [106] has shown that hard negatives, as opposed to randomly sampled negatives from training batches, can significantly improve model performance. Consequently, many studies have explored methods for generating hard negative samples using LLMs. When generating hard negatives, methods similar to those for generating positive samples (as discussed in \u00a7 III-A2) are often employed [82, 84, 86, 88, 92, 83, 85, 87, 100]. The generated hard negatives can generally be divided into two categories. 1. NLI-based negatives [82, 84, 86, 88, 92]: Texts with a logical relationship of contradiction are generated as hard negatives; 2. Contextually irrelevant negatives [83, 85, 87, 100]: Texts are generated using specific definitions of irrelevance as hard negatives. Notably, Promptriever [100] introduces a unique type of hard negative to enhance the model's instruction-following capability. These hard negatives are positive samples relative to the query itself but become negative samples when the query is combined with the instruction."}, {"title": "B. Data Annotation with LLMs", "content": "In many scenarios, lots of existing data are available, where leveraging LLMs to provide supervision signals for data annotation is a more effective and efficient approach. In this section, we introduce the use of supervision signals from LLMs in the following scenarios: Mining training data from existing corpus; Filtering incorrect supervision signals in existing training data; Providing supervision signals for clustering algorithms.\n1) Training Data Supervision: Compared to directly generating text from LLMs to construct training data, mining training data from large corpus is more effective in meeting the demand for diversity. As introduced in \u00a7 III-A3, hard negatives are critical for text embedding models. Therefore, some studies leverage LLMs for hard negative mining: NV-Retriever [99] uses LLMs as embedding model and employs proposed \"positive-aware mining methods\" to identify hard negatives; Gecko [101] first retrieves relevant texts using a embedding model, then estimates the similarity between texts using LLMs to mine hard negatives. In addition to traditional training data composed of positive and negative samples, the application of LLMs enables the construction of training data with finer-grained similarity patterns: CLAIF and CLHALF [91] use LLMs to assign similarity scores to generated and existing text pairs, then train models using Mean Squared Error (MSE) loss and a proposed soft InfoNCE loss; NGCSE [92] employs LLMs to construct text pairs with similarity levels ranging from high to low, which equals to labeling text pairs with similarity scores. These pairs are then used for training with a proposed Hierarchical Triplet loss.\n2) Training Data Filtering: Existing training data and newly constructed training data from LLMs may both contain incorrect supervision signals. Therefore, filtering or even correcting erroneous supervision signals is an important research direction. This section first introduces methods for filtering data constructed by LLMs, followed by methods using LLMs for data filtering.\na) Filtering Training Data From LLMs: Erroneous supervision signals can be categorized into two types: false negatives and false positives. Several studies focus on methods for filtering false negatives: MultiCSR [84] filters false negatives by using a high-performing embedding model to provide text similarity scores; Promptriever [100] employs a cross-encoder to compute text similarity for filtering false negatives. Other studies focus on excluding false positives that are not in the Top-K retrieved results: Promptagator [93] trains a retriever on generated data; InPars-v2 [95] trains a retriever using MS MARCO data; SPTAR [98] directly uses BM25 as the retriever. In addition, AdaptCL [88] utilizes an NLI classification model and FollowIR [103] utilizes an top-tier embedding model to filter both false negatives and false positives simultaneously.\nb) LLMs as Training Data Filter: Some studies use LLMs to filter false positives in generated data: InPars [94] filters false positives using perplexity as a metric during text generation; I3[96] designs prompts to filter out queries that do not meet specified conditions. Additionally, some studies employ LLMs to filter false positives and false negatives in generated data simultaneously: GenSE fine-tunes LLMs to identify incorrect entailment and contradiction; MultiCSR [84] prompts LLMs to filter out instances that fail to meet the conditions for entailment and contradiction.\n3) Clustering Supervision: Embedding models are often applied in clustering algorithms, and many recent studies leverage LLMs to provide supervision signals for clustering processes, enhancing task performance. KeyEvents [107] uses LLMs to assist in Key Event Discovery by aggregating clustering results and summarizing cluster centers to identify key events. Two studies employ LLMs to support Generalized Category Discovery: ALUP [108] introduces an uncertainty metric to identify error-prone samples in the clustering results and uses LLMs to reassign these samples to different cluster centers. LOOP [109] identifies boundary samples as error-prone and employs LLMs to determine which category boundary these samples are closer to. In LOOP, LLMs are also used to extract the semantic meaning of each cluster. IDAS [110] assists in the Intent Discovery task by using LLMs to label each sample, improving clustering performance. ClusterLLM [111] enhances clustering algorithms by using LLMs to generate triplet constraints on an initial clustering result, applying hierarchical clustering to derive the final result, and validating the correctness of cluster merges during the process with LLMs. Another study [112] integrates LLMs throughout the entire clustering process: Pre-clustering, LLMs extract sample keywords to improve feature representation; During clustering, LLMs provide pairwise constraints to enhance clustering quality; Post-clustering, LLMs reassign error-prone samples to the correct clusters."}, {"title": "IV. LLMS AS TEXT EMBEDDER", "content": "LLMs commonly adopt encoder-decoder architecture, e.g., T5 [140], or decoder-only architecture, e.g., GPT-3 [141]. After going through pre-training, instruction tuning, and alignment with human preferences [142], LLMs can perform a wide range of tasks. However, these steps do not support obtaining high-quality embeddings from LLMs. To address this issue, exploring methods to obtain representations from LLMs directly is a promising direction. \nSpecifically, we focus on their backbone selection, architectural improvement, training methods, and evaluation protocols."}, {"title": "A. Backbone Selection", "content": "The LLM-based embedders use numerous open-source LLMs as backbones. Among Encoder-Decoder backbones, T5 (GTR) has a dominant position; Among decoder-only backbones, the most popular is Mistral(18 / 32), followed by LLaMAc (15 / 32) and OPT (4 / 32). The dominance of T5 is not surprising, as T5 and its variants have been leading innovations in encoder-decoder PLMs. Meanwhile, the popularity of the Mistral is strongly connected to its excellent performance. LLM2Vec [130] find that Mistral's embedding performance will not drop significantly compared to the other LLMs when converting casual attention to bi-directional attention. We speculate that this characteristic should derive from its pre-training or instruction fine-tuning phase, but no information has been revealed."}, {"title": "B. Architecture Improvement", "content": "Denoted d is the dimension of hidden states", "f^{(1)}": "circ f^{(0)"}, 17], "f^{(0)}": "X \u2192 R^d$ is the input layer", "f^l": "R^d \u2192 R^d$ is the l-th transformer layer and $g : R^d \u2192 R^c$ is the decoding layer. For convenience", "Strategy": "As with traditional pre-trained language models", "Pooling": "Mean pooling ($a_i = \\frac{1"}, {"Pooling": "Last pooling (only $a_n = 1$) is a new pooling strategy emerging in the LLM era to ensure that embedding can contain information about the whole text. However", "word": " is introduced, where [X] is a placeholder. In practice, [X] is filled with the input text, and the last pooling"}]