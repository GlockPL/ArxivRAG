{"title": "When Text Embedding Meets Large Language Model: A Comprehensive Survey", "authors": ["Zhijie Nie", "Zhangchi Feng", "Mingxin Li", "Cunwang Zhang", "Yanzhao Zhang", "Dingkun Long", "Richong Zhang"], "abstract": "Text embedding has become a foundational technology in natural language processing (NLP) during the deep learning era, driving advancements across a wide array of downstream tasks. While many natural language understanding challenges can now be modeled using generative paradigms and leverage the robust generative and comprehension capabilities of large language models (LLMs), numerous practical applications-such as semantic matching, clustering, and information retrieval-continue to rely on text embeddings for their efficiency and effectiveness. In this survey, we categorize the interplay between LLMs and text embeddings into three overarching themes: (1) LLM-augmented text embedding, enhancing traditional embedding methods with LLMs; (2) LLMs as text embedders, utilizing their innate capabilities for embedding generation; and (3) Text embedding understanding with LLMs, leveraging LLMs to analyze and interpret embeddings. By organizing these efforts based on interaction patterns rather than specific downstream applications, we offer a novel and systematic overview of contributions from various research and application domains in the era of LLMs. Furthermore, we highlight the unresolved challenges that persisted in the pre-LLM era with pre-trained language models (PLMs) and explore the emerging obstacles brought forth by LLMs. Building on this analysis, we outline prospective directions for the evolution of text embedding, addressing both theoretical and practical opportunities in the rapidly advancing landscape of NLP.", "sections": [{"title": "I. INTRODUCTION", "content": "Text embedding learning is a fundamental task in Natural Language Processing (NLP), which aims to extract features from text with the natural languages (including words, sentences, and documents). Formally, given the text X, text embedding learning aims at training a model f : X \u00d7 0 \u2192 Rd on the dataset D C X. The continue vector h = fo(x) output by f is called as the embedding of the text x, which is expected to contain valid information in the input text and perform well on the downstream tasks.\nAdvanced large language models (LLMs) have recently demonstrated exceptional generalization and transfer capabilities in downstream tasks where generative paradigms are applicable, such as information extraction, text classification, and machine translation. However, not all tasks are suitable for modeling using generative paradigms; tasks such as dense retrieval, semantic text similarity, etc., still require text embedding for computing similarity. In this aspect, LLMs are not different from traditional encoder-only PLMs and do not directly produce high-quality embedding. Therefore, how to obtain high-quality text embeddings remains a key problem.\nFortunately, LLMs have comprehension and generation capabilities far beyond those of previous models, which opens up new opportunities for text representation learning. Specifically, LLMs change the existing landscape in two ways: (1) LLMs can act as data annotators for large-scale, high-quality, and fine-grained text datasets, and (2) LLMs can replace current PLMs as model pedestals for generating higher-quality textual representations. Many works have been devoted to introducing large models for obtaining high-quality text embedding. However, most works focus on a single task, and similar methods have been \"proposed\" repeatedly in different fields. This leads to a lack of comprehensive and objective understanding of the role played by LLMs in text embedding across various fields.\nAt the same time, the development of LLM as introduced a number of emerging tasks. Examples include long context compression developed to improve the reasoning efficiency of LLM, and embedding inversion tasks to study the information leakage in text. Although the two tasks are developed with different motivations, they share the similar framework and use LLM to understand the information in the text embedding.\nThis survey focuses on deep representation learning methods, which train the encoders to map texts to a high-dimensional space. We hope this survey will help researchers focusing on different tasks find commonalities in the problem, combining efforts to promote more rapid development in this area. Compared to the existing surveys [1, 2] in this field, this survey covers more comprehensive downstream tasks. They not only include the traditional downstream tasks that require text embeddings (such as semantic text similarity, information retrieval, and text clustering) but also contain the emerged tasks in the era of LLMs (such as long context compression and embedding inversion). In addition, this survey discusses the relationship between LLMs and text embedding for the first time, mainly including (1) LLM-augmented text embedding, (2) LLM as text embedders, (3) Text embedding understanding with LLMs. Fig. 1 shows the relationship between the text embedding model (which we call text embedder) and the LLM in each section.\nThe subsequent sections will unfold in the following order: Section II introduces the developed stage, the training and evaluation tasks for text embedding; Section III introduces the text embedding methods where LLMs are used for data augmentation and while another model is used to be the text embedder; Section IV presents the text embedding methods where LLMs are the backbones of text embedders; Section V show two novel text embedding-related tasks in the LLM era: long context compression and embedding inversion. Section VI discusses the remained challenge before the presentation"}, {"title": "II. PRILIMINARY", "content": "A. History of Text Embedding\n1) Era of Statistics Machine Learning: In the early days, researchers primarily relied on manually designed features to represent text. The development of the method is accompanied by a transformation of the output vector's form, which is from bag-of-words models (one-hot vectors) to TF-IDF [3] (sparse vectors), then to word embeddings (dense vectors). These methods require domain experts to select and design features manually, and the quality and quantity of features limit their effectiveness. As machine learning techniques advanced, researchers began exploring methods to learn text embeddings using statistical approaches such as Latent Semantic Analysis (LSA) [4], Latent Dirichlet Allocation (LDA) [5], and others. While these methods can automatically learn text embedding, they still have limitations in capturing complex semantic and syntactic structures.\n2) Era of Deep learning: With the emergence of deep learning techniques, word embedding has become a significant breakthrough in text embedding learning. Models such as Word2Vec [6], GloVe [7], and FastText [8] can map words into a low-dimensional continuous vector space to capture semantic and syntactic relationships between words. Over the past few years, pre-trained language models (PLMs) with millions of parameters (e.g., BERT [9] and ROBERTa [10]) have been proposed first. The \u201cpretraining then fine-tuning\" paradigms have performed well on various downstream tasks and played an important role in practical on-the-ground applications. However, the embedding spaces of these PLMs are proved to be anisotropic [11], leading to surprisingly high similarities computed for any two texts.\nIn this phase, the centre of text embedding learning shifts to fine-tuning PLMs for higher-quality text embeddings. Some supervised methods were explored first, followed by post-processing and unsupervised methods. Despite the tremendous progress, there is still a huge performance gap between supervised and unsupervised methods. Then contrastive learning is introduced to use the negative example fully, which focuses on"}, {"title": "B. Large Language Model", "content": "We use the term \"large language models\" or \"LLMs\" to exclude pre-trained language models based on the Transformer encoder (such as BERT and RoBERTa) and neural networks with typically smaller parameter counts (such as LSTM [21] and GRU [22]). We refer to the language models whose variants' parameter number is more than 1B as LLMs. Therefore, the LLMs we study include:"}, {"title": "C. Training Dataset", "content": "Text-embedded data involves multiple downstream applications, and advanced methods often integrate massive amounts of data from various domains in various languages involved in training. Table II shows non-synthetic datasets that are often used in existing work. Given the enormous amount of work and the large differences between methods, we will not present"}, {"title": "D. Downstream Task for Evaluation", "content": "Text embedding learning often serves multiple downstream tasks and cannot be trained using label Y as in text classification tasks. Therefore, one research focus of text embedding learning mainly lies in designing pretext tasks and constructing corresponding datasets.\n1) Semantic Textual Similarity: The Semantic Textual Similarity (STS) task quantifies the semantic similarity degree between two texts.\na) Evaluation Dataset: The dataset for evaluating STS can be expressed as Deval = {(x, x', ci)}=1, where (x,x') is a text pair and ci is the average similarity between the two texts. The popular evaluation protocol usually contains seven STS datasets for English setting, which contains STS12-16 [38, 39, 40, 41, 42], STS-B [43] and SICK-R [44] in SentEval Benchmark [45] and STS-17 [43] and STS-22 [46] for multi-lingual setting.\nb) Evaluation Metric: The model is expected to take two texts (x,x') as input and produce a predicted score \u0109i that represents the similarity of the two texts. Common evaluation metrics include the Pearson correlation coefficient and the Spearman correlation coefficient. The Pearson correlation coefficient primarily assesses the correlation between absolute values of ci and, which is expressed as\npearson = \u03a3i=1(ci - ci)(c'i - c'i)/\u221a \u03a3i=1(ci \u2013 Ci)^2 \u03a3i=1(c'i \u2013 c'i)^2\nwhere ci and are the mean value of {ci}=1 and {c'i}=1, respectively. And the Spearman correlation coefficient primarily assesses the correlation between the ranks of ci and in their respective lists, which is expressed as\nTspearman = 1 - 6 \u03a3i=1(ri - r'i)^2/ n(n^2 - 1)\nwhere ri and r'i are the rank of ci and in their respective lists, respectively.\n2) Information Retrieval: Information retrieval (IR) aims to retrieve documents relevant to a query, where their matching scores measure the relevance between the query and the documents. (In this survey, we focus on text retrieval.) Information retrieval systems can be roughly divided into two stages. The first stage (retriever) aims to reduce the candidate space by retrieving relevant candidate documents. The second stage (reranker) mainly focuses on reranking the candidate documents.\na) Evaluation Dataset: Generally, the dataset for training and evaluation can be written as D = {qi,d+}_1, where qi is the query and d+ are the positive documents that match qi. The popular evaluation protocol usually use BEIR [47] for English setting, MIRACL [48] and Mr.TyDi [49] for multi-lingual setting.\nb) Evaluation Metric: Given queries and documents, the IR model can provide their matching scores, and the quality of the model can be evaluated through these scores. For the first stage, common evaluation metrics include Recall@k and Accuracy@k, while for the second stage, metrics like MAP, MRR and NDCG are generally used.\nRecall@k calculates a truncated Recall value at the k-th position of a retrieved list.\nRecall@k = 1/|Q| \u03a3q=1 Rq,k/Sq\nwhere Q is the query set. Sq denotes the total number of relevant texts for query q, and Rq,k denotes the number of relevant texts retrieved in the top-k positions by the retrieval model.\nAccuracy@k calculates the proportion of queries for which the top-k retrieved texts contain the answers, defined as:\nAccuracy@k = 1/|Q| \u03a3q=1 I(Rq,k > 0),\nwhere I() is an binary indicator function that only returns 1 when the case is true.\nFurthermore, Mean Average Precision (MAP) calculates the mean of Precision scores over a set of queries Q:\nMAP = 1/|Q| \u03a3q=1 APq\nwhere APq = 1/Rq,k \u03a3k=1 Precision@k \u00d7 \u2161(q,k), and Precision@k = |Rq,k|/k\nMRR, averages the reciprocal of the rank of the first retrieved positive text over a set of queries Q .\nMRR = 1/|Q| \u03a3q=1 1/rankq\nwhere rank\u0105 is the position of the first retrieved positive text.\nIn addition to quantifying the frequency of positive texts, Normalized Discounted Cumulative Gain (DCG) also considers the position of a relevant text. It favors a ranking system that situates a pertinent text at a superior position.\nDCG @k = \u03a3i=1^k 2^ri -1/ log2(i + 1)\nwhere ri is the graded relevance score for the i-th retrieved text. According to the aforementioned definition of DCG, NDCG is the aggregate of the normalized DCG values at a specific rank position.\nNDCG@k = 1/|Q| \u03a3q=1 DCGq@k /IDCGq@k,\nwhere IDCG@k denote ideal discounted cumulative gain at a particular rank position k."}, {"title": "E. Emerging Related Tasks", "content": "1) Long Context Compression: Long Context Compression involves compressing long context tokens for large models without sacrificing essential information, accelerating inference while maintaining performance in scenarios with extensive token lengths.\na) Evaluation Dataset: The Question Answer (QA) dataset for Long Context Compression task can be formalized as follows: D = {(Ci, qi, ai)}=1 where Ci represents the long context, qi represents the question, a\u017c represents the correct answer to the question qi, derived from the context Ci. The"}, {"title": "2) Embedding Inversion", "content": "2) Embedding Inversion: Embedding inversion is a type of privacy leakage that predicts relevant information based on the embedding of the input text. It is generally regarded as an attacker, with targets including attribute information in the input text, the entire input text, or the text embedding model itself.\na) Evaluation Dataset: For different levels of the leak information, the datasets can be word-level or sentence-level. For attribute inference attack, the dataset has the form of (Input,Words), where the \"Input\" is text and \"words\" represents important information in the input text, for example name, ID number. For the purpose of reconstruct all the input text, the dataset just contains input text. It should be noted that, text embedding will not be contained in dataset directly, it is generated by the victim embedding model using the text in dataset as input.\nb) Evaluation Metric: The main metrics used for evaluating embedding inversion include BLEU, ROUGE, Exact-Match, cosine similarity, and Token F1.\nBLEU is used to measure n-gram similarity between reconstructed text and original input text. we can compute it following:\nBLEU = BP \u00d7 exp \u03a3n=1^N wnlog(pn)\nwhere BP is the brevity penality, wn is the weight for each n-gram precision and pn is the modified precision for n-th n-gram.\nROUGE represents the recall of overlapping words of reconstructed text and Exact-Match represents the percentage of exactly matching reconstructed text to the original input text. While the cosine similarity is computed in the embedding space between embeddings of reconstructed text and original input text. The \"Token F1\" metric is the multi-class F1 scores between tokens of predicted text and input text and It is word-level metric."}, {"title": "III. LLM-AUGMENTED TEXT EMBEDDING", "content": "One approach to adopting LLMs for text embeddings is through knowledge distillation. Specifically, LLMs can be used to generate training data for the embedding model in two ways: 1. Directly synthesizing training data (\u00a7 III-A); 2. Providing supervision signals for existing data (\u00a7 III-B). A detailed description of different methods is presented in Table III.\nA. Data Synthesis with LLMS\nCurrent text embedding models are typically trained using contrastive learning [12], where the training data commonly consist of three components: anchors, positive samples, and negative samples. The specific definitions of these components vary across tasks. For example: In Information Retrieval (IR), the anchor is the query, while the positive and negative samples are documents related and unrelated to the anchor, respectively. In Semantic Textual Similarity (STS) tasks, the anchor is a text, with positive and negative samples being semantically similar and dissimilar texts. Additionally, recent studies [104, 100] have begun integrating instruction-following capabilities into text embedding models, resulting in a need for instructions in the training data. In this section, we provide an overview of how LLMs can be used to synthesize the different components of training data.\n1) Instructions: Existing instructions in training data are typically constructed based on datasets. This involves manually collecting multi-task datasets and applying templates to generate different instructions for each dataset [104]. However, this approach lacks diversity, limiting the full potential of the model's instruction-following capabilities. Therefore, a promising solution is to generate instance-level instructions by using LLMs. One approach directly leverages LLMs to generate instructions: 13 [96] generates diverse instructions by manually setting different conditions, including topics, organizational formats of the retrieved text, and definitions of relevance; E5mistral [102] categorizes tasks into different types (e.g., symmetric and asymmetric retrieval tasks) and designs specific prompts for each category to generate instructions. Another approach generates instructions based on existing data: Gecko[101] prompts LLMs to generate instructions by conditioning on different documents; Promptriever[100] generates instructions that describe the relationship between the given queries and documents, and enhance instruction diversity by setting various conditions such as length and style of the generated instructions.\n2) Positive Samples: Based on the formal similarity between positive samples and anchors, positive samples can generally be classified into the following two categories.\na) Symmetric Positive Samples: A classic task where anchors and positive samples are symmetric is STS. In this area, numerous studies have proposed various methods to generate symmetric positive samples. Based on the analysis in S-BERT [105], text embedding models for STS are often trained using training data from the Natural Language Inference (NLI) task. Following this convention, a line of research explores how to use LLMs to generate NLI-style training data: Two studies [86, 92] directly utilize the in-context learning capabilities of LLMs by using NLI-formatted training data as demonstrations to guide the generation of entailment sentences as positive samples; Other studies [82, 84, 88] first fine-tune LLMs with NLI training data, enabling them to generate NLI-formatted positive samples. Notably, AdaptCL [88] employ a"}, {"title": "IV. LLMS AS TEXT EMBEDDER", "content": "LLMs commonly adopt encoder-decoder architecture, e.g., T5 [140], or decoder-only architecture, e.g., GPT-3 [141]. After going through pre-training, instruction tuning, and alignment with human preferences [142], LLMs can perform a wide range of tasks. However, these steps do not support obtaining high-quality embeddings from LLMs. To address this issue, exploring methods to obtain representations from LLMs directly is a promising direction. In Table IV, we list 35 LLM-based Embedder and their detailed information. Specifically, we focus on their backbone selection, architectural improvement, training methods, and evaluation protocols.\nA. Backbone Selection\nThe LLM-based embedders appearing in Table IV use numerous open-source LLMs as backbones. Among Encoder-Decoder backbones, T5 (GTR) has a dominant position; Among decoder-only backbones, the most popular is Mistral (18 / 32), followed by LLaMAc (15 / 32) and OPT (4 / 32). The dominance of T5 is not surprising, as T5 and its variants have been leading innovations in encoder-decoder PLMs. Meanwhile, the popularity of the Mistral is strongly connected to its excellent performance. LLM2Vec [130] find that Mistral's embedding performance will not drop significantly compared to the other LLMs when converting casual attention to bi-directional attention. We speculate that this characteristic should derive from its pre-training or instruction fine-tuning phase, but no information has been revealed.\nB. Architecture Improvement\nDenoted d is the dimension of hidden states, L is the number of transformer layers, and c is the vocabulary size. Without"}, {"title": "B. New challenges", "content": "1) Complex Instruction following: In recent times, numerous embedding approaches based on language models (LLMs) claim to have trained with instruction tuning approach. The aim of such training is to improve the models' ability to understand and perform specific tasks according to the provided instructions. However, despite these claims, current instruction models encounter significant difficulties when trying to effectively meet complex retrieval demands. Many researchers are attempting to develop new benchmarks to evaluate these models' capabilities in following complex instructions. For example, tasks involving the precise chronological order of events [241], or those related to elaborate narratives, such as creating a detailed summary of a long fictional story or extracting key plot elements in a specific order from a complex literary work [103]. Additionally, there are reasoning-based retrieval tasks, like finding the solution code for a LeetCode problem [242]. The current inability of instruction models to handle these complex retrieval situations well indicates another crucial research area. This area undoubtedly requires the careful attention of the academic and research communities. By concentrating on improving these aspects, it is possible to potentially enhance the overall performance and practical usefulness of these language models in a wide range of applications.\nAdditional, although substantial progress has been made in text representation methods based on Instruction Tuning, it is challenging for users to clearly and accurately describe their intent in practical applications. Therefore, automatically generating clear descriptions based on available information, such as the type of corpus, the user's query, and other background information, represents a direction that requires further exploration.\n2) Privacy Leakage from Text Embedding: As embedding technology continues to evolve, it has been realized that the rich information contained in an embedding may be a threat to privacy security. [217, 243]. In the era of LLMs, retrieval-augmented generation [244] has become a necessary technique in many LLM-based services. In addition, the need for larger-scale data and knowledge has led to commercial services like vector databases and text embedding API. These trends have led to an unprecedented focus on information security in text embedding.\nAs the embedding inversion task demonstrates, the text can be partially or even fully restored from its embedding without accessing the embedders' parameters. While appropriate noise was shown to be resistant to trained decoders [187], it was not possible to confirm that the fine-tuning of decoders on noisy data could overcome noise interference.\nIn fact, we can see the antagonism in the two tasks elaborated in Section V: The methods in long context compression aim to compress long text losslessly, while in embedding inversion, the works warn that the information embedded in the embedding can be easily restored and try to design some defenses.\n3) High-Dimensional Text Embedding: Scaling the model size but fixing the embedding dimension has been shown to be an effective way to improve the encoder's performance [118]. However, increasing the dimension of hidden states with layer and attention head number has been a generalized method when scaling LLMs' size. Therefore, LLMs' hidden states generally have a dimension over 2,048, which is 2x and 2.67x than that of BERT-Large and BERT-base, separately. When large models are used as encoders, the representation dimensions exacerbate the computational and storage overhead of the representations. Some post-processing approaches [245, 246, 247] have been proposed to reduce the dimensionality of the learned representations, while matryoshka representation learning (MRL) [172] use the special optimization objectives during training that direct the most critical information to the part dimensions of the representation. The former has not yet been applied to large models, while the latter has already landed in commercial services 12. It seems that low-dimensional embedding can obtain a no-bad performance on the MTEB Benchmark. However, there are many deeper issues to be explored. For example, (1) what is the theoretical lower bound on the dimensionality of lossless compression; (2) how to design more efficient compression algorithms to obtain embeddings with higher information density; and (3) how to derive sparse representations that are more friendly to long documents. However, dimension, as one of the most important measures of embedding quality, still has many deeper issues to be explored. For example, (1) what is the theoretical lower bound of dimensionality for information lossless compression of data; (2) how to realize the efficient conversion of low information density high-dimensional embeddings to high-information density low-dimensional embeddings; and (3) how to convert text embedding to sparse representations that are more friendly to long documents.\n4) Training & Inference Overhead for LLMs: LLMs usually have more than 1B parameter, which makes full parameter fine-tuning very difficult. Various parameter-efficient fine-tuning methods, such as Adapter [248], Soft Prompt [249], and LoRA [157], alleviate the computation and memory overhead to some extent. However, the time overhead is still huge, making fine-grained ablation experiments for hyperparameters difficult to accomplish. Some work has failed to explore the full potential of the method by training a fixed number of steps [130] or epochs [134]. In addition, how model compression techniques, including embedding sparsification [154], parameter pruning [250], and knowledge distillation [251], will affect the embedding quality has not been fully explored yet."}, {"title": "VII. FUTURE TRENDS", "content": "In fact, we can see the antagonism in the two tasks elaborated in Section V: The methods in long context compression aim to compress long text losslessly, while in embedding inversion, the works warn that the information embedded in the embedding can be easily restored and try to design some defenses.\nA. Methods for Cross-Lingual & Cross-Modal Domain\nAdvanced LLMs already support over a hundred languages [252] and can understand the information in multiple modalities such as vision [253], audio [254], and even physical fields [255]. The powerful understanding of LLMs has been leveraged to represent multi-lingual and multi-modal data."}, {"title": "1) Cross-Lingual Representation Learning with LLMs:", "content": "1) Cross-Lingual Representation Learning with LLMs: In cross-lingual scenarios, the downstream tasks, such as STS [43, 46] and retrieval [256, 257], have sufficient high-quality datasets for evaluation. With the help of multilingual PLMs models (e.g., mBERT [9] and XLM-R [9]), mDPR [258], mE5 [259], and M3-Embedding [58] have achieved better results on various types of downstream tasks.\nSome LLMs are pre-trained on large multi-lingual corpora, which leads us to expect higher-quality cross-language textual representations using LLMs. Some exploratory work has attempted to introduce LLM into cross-language scenarios and has shown remarkable potential. E5-Mistral [102] demonstrates the great potential of LLM for synthesizing data and being a backbone, while UDEVER [132] shows the powerful generalization capabilities of multilingual LLM, even when fine-tuned using only English text. In addition, both OpenAI and Cohere offer multilingual embedding services; Cohere blogged about the second phase of its model training using additional supervised signals generated by LLM 13; and OpenAI's currently disclosed tech line [131] and up to 3,072 embedding dimensions give us a reasonable suspicion that the LLM backs the service as an encoder."}, {"title": "2) Cross-Modal Representation Learning with LLMs:", "content": "2) Cross-Modal Representation Learning with LLMs: In the cross-modal domain, joint language-vision representation learning [260, 261, 262] has received the most attention. Its applications have gradually expanded from image-text retrieval [263, 264] to multiple tasks such as composed image retrieval [265, 266], multi-modal document retrieval [267] and multi-modal knowledge retrieval [268, 269]. VISTA [270] generates high-quality instructions and captions using LLMS and achieves substantial improvements on various cross-modal retrieval tasks, while E5-V [271] demonstrates the potential of MLLMs as a multi-modal unified encoder, showing strong performance on uni-modal (even visual modal) and multi-modal by training the model on text pairs alone."}, {"title": "B. Task-Specific Representation", "content": "For a long time, text representation learning has been expected to obtain a universal representation without making the downstream task visible. However, this setting is often difficult in practice, such as (1) In the unsupervised setting, contrastive learning brings the two views of data augmentation for the same instance closer together, i.e., learning an encoder that is insensitive to a particular data augmentation transform. However, this may result in valuable information for downstream tasks not being included in the representation [272]; (2) In the supervised setting, undifferentiated joint training of different tasks will hurt performance [273, 104]; (3) In evaluation, the two texts can be evaluated from various perspectives and show different semantic similarities [274, 275];\nTherefore, we can convert the hope from universal text representations to universal text encoders F, which can be expressed as\nh = F(x,t) \u2208 Rd.\nThe universal encoders accept the text input x and the downstream task t and represent x specific to task t, where"}, {"title": "1) Instruction-Following Representation Learning:", "content": "1) Instruction-Following Representation Learning: In instruction-following representation learning, F is divided into two parts: F = f opt, where pt is a prompt template related to task t. In the beginning, prompts are used to align the fine-tuning targets with the pre-trained targets of the PLMs [276]. Then, similar instruction tuning [277] is proposed to adjust LLMs to be more adaptable to the format of task-oriented conversations. Instruction-following representation learning follows the idea of instruction tuning but models all tasks into a contrastive paradigm. TART [278] firstly validate the feasibility of this idea to information retrieval, while Instructor [104] fine-tuning GTR on 330 NLP tasks and find that including instructions would alleviate conflicts between tasks compared to traditional multi-task learning. In the zero-shot evaluation, the model fine-tuned with instructions shows good generalization to new instructions and datasets. The findings related to this technique are still iterating: [130] add instruction only on evaluation and demonstrate its effectiveness; [133] demonstrate that question-and-answer pairs can significantly improve a model's ability to comply with instructions under a particular training method."}, {"title": "2) Equivariant Representation Learning:", "content": "2) Equivariant Representation Learning: In equivariant representation learning, F is divided into two parts: F = utof, where ut is a mapping related to task t in the representation space. Before introducing specific methods, we define \u201cequivariant mapping\u201d in mathematics:\nDefinition 1: (Equivariance Mapping) Let f : X \u2192 Rd is a mapping from data to representations. We call f is equivariant to the algebraic group T = (T,\u0970) if there exist a transformation g : T\u00d7X \u2192 X and a transformation G:T\u00d7Z \u2192 Z satisfying Gt(f(x)) = f(gt(x)) for any te Tand x \u2208 X.\nT is no longer restricted to algebraic groups in equivariant representation learning. In practice, t is viewed as a data augmentation strategy in the unsupervised setting or a mapping from data to data/label for a specific task in the supervised setting. Specifically, when Gt is the identity mapping, the condition in Definition 1 degenerates to f(x) = f(g(x)), i.e., the alignment in contrastive learning [146]. Instance-level equivariant representation learning has been fully explored in self-supervised visual representation learning [279, 280, 281, 282]. In text representation learning, equivariant representation learning based on PLMs is applied in both task-level [273] and instance-level [283, 284, 285]."}, {"title": "C. Interpretable Text Embedding", "content": "Current text representations are far superior at the performance level to those obtained using feature engineering; however, they severely lack interpretability. This leads to an inability to localize why they fail in some cases (e.g., bad cases in information retrieval) to the point where we can't target them for improvement. Some recent approaches attempt to enhance the interpretability of representations using LLMs."}, {"title": "4) Multi-Stage Training:", "content": "4) Multi-Stage Training: During the supervised learning phase, numerous studies have explored multi-stage contrastive learning training methods to enhance further the generalization and versatility of the final embedding models. For instance, influenced by earlier work from the BERT era, models like GTR [118], GTE-Qwen2 [135], and BMRetriever [122] have investigated two-stage training strategies that combine weakly supervised contrastive learning (WCL) with supervised contrastive learning (SCL). Weakly supervised contrastive learning primarily leverages a large amount of weakly supervised relevance data collected from public domains (e.g., neighboring text spans and question-answer pairs from the QA community) for training. Although this data may contain noise, the extensive domain coverage provided by typically over a billion data points significantly improves the model's domain generalization capabilities. Additionally, NV-Retrieval proposes a phased supervised training approach tailored to different tasks (e.g., retrieval and STS tasks). In the first stage, NV-Retriever [99] utilizes only retrieval-supervised data with in-batch negatives alongside mined hard negatives. In the second stage, it blends data for retrieval tasks with datasets from other tasks (e.g., classification, regression)."}, {"title": "D. Commercial Service", "content": "D. Commercial Service\nMany AI companies provide generic text embedding services to support traditional NLP tasks and RAG. In English text embedding service, it mainly includes OpenAI 5, Google 6, Amazon 7, Alibaba Cloud8, Voyage 9 and Cohere 10. Due to the trade secrets involved, only a few companies have revealed some of the technology behind their services. For example, OpenAI simultaneously provides text embeddings with different dimensional settings for different levels of time-storage sensitivity in practice, where matryoshka representation learning (MRL) [172] is considered to be the key to obtaining the embedding with flexible dimensions using only one encoder. Google demonstrates that its Gecko [101], which achieves superior performance on 1B scale and 786 dimensions, relies heavily on a two-stage synthetic data generation process with LLMs: the first step generates diverse tasks and queries based on the instruction and the second step generates positive and negative samples based on the obtained tasks and queries. In addition, there have been methods [173] that attempt to distill knowledge through embeddings obtained from commercial APIs, obtaining a local embedder with similar performance successfully."}, {"title": "V. TEXT EMBEDDING UNDERSTANDING WITH LLMS", "content": "V. TEXT EMBEDDING UNDERSTANDING WITH LLMS\nLLMs have a powerful paraphrase capability that can be quickly aligned with and interpreted in a variety of already learned embedding spaces of image [192", "194": "and concept [195", "categories": "soft prompt V-A1 and context distillation V-A2. It is worth noting that in natural language processing, handling long contexts has always been a significant issue. Some works [196, 197, 198, 199, 200, 201, 202"}]}