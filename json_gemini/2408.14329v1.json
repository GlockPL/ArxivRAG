{"title": "PHEVA: A Privacy-preserving Human-centric Video Anomaly Detection Dataset", "authors": ["Ghazal Alinezhad Noghre", "Shanle Yao", "Armin Danesh Pazho", "Babak Rahimi Ardabili", "Vinit Katariya", "Hamed Tabkhi"], "abstract": "Video Anomaly Detection (VAD) seeks to identify unusual events in videos that deviate from normal behavior. However, creating VAD datasets is challenging due to the diversity of human behaviors, the rarity of anomalies, and the ethical and privacy concerns associated with human-centric data. To step forward in addressing these challenges, this paper introduces PHEVA, a Privacy-preserving Human-centric Ethical Video Anomaly detection dataset. By removing pixel information and providing only de-identified human annotations, PHEVA safeguards personally identifiable information. The dataset includes seven indoor/outdoor scenes, featuring one novel, context-specific camera, and offers over 5\u00d7 the pose-annotated frames compared to the largest previous dataset. This study benchmarks state-of-the-art methods on PHEVA using a comprehensive set of metrics, including the 10% Error Rate (10ER), a metric used for anomaly detection for the first time providing insights relevant to real-world deployment. As the first of its kind, PHEVA bridges the gap between conventional training and real-world deployment by introducing continual learning benchmarks, with models outperforming traditional methods in 82.14% of cases.", "sections": [{"title": "Introduction", "content": "The field of video surveillance has witnessed significant advancements, driven by the rapid progress in computer vision (Perwaiz, Fraz, and Shahzad 2024; Ratthi and Yogameena 2024; Pazho et al. 2023a; Yao et al. 2023). Among these, anomaly detection in video streams has emerged as a critical area of research (Zhu, Bao, and Yu 2022; Wu et al. 2022; Li et al. 2022; Wang et al. 2022). Video Anomaly Detection (VAD) refers to identifying events or patterns in video streams that deviate from the expected behavior.\nDespite the growing interest in VAD, the availability of datasets in this field remains limited, largely due to the complexity of human behavior. The vast range of normal and abnormal behaviors (open-set) and the context-specific nature of these behaviors-where definitions vary depending on the environment (Noghre et al. 2023; Noghre, Pazho, and Tabkhi 2024; Pazho et al. 2023b)-complicate dataset creation. On top of that, anomalous behaviors are rare, and not easily accessible (Doshi and Yilmaz 2023; Danesh Pazho et al. 2023). All of these are added to practical concerns related to data collection, annotation, and privacy and ethical considerations (Danesh Pazho et al. 2023; Doshi and Yilmaz 2022; Rodrigues et al. 2020; Liu et al. 2018). To step forward in addressing such limitations, we introduce PHEVA, a privacy-preserving, human-centric, ethical VAD dataset.\nWhile access to more comprehensive datasets can help mitigate the challenges of context-specificity and the open-set nature of VAD, it doesn't fully resolve the issue. To address these challenges beyond the predominant use of unsupervised learning, techniques such as continual learning are increasingly being adopted to manage domain shifts on the algorithmic side. For these techniques to be truly effective, datasets must accurately reflect real-world surveillance footage across diverse environments with continuous recording, providing an adequate number of samples.\nPHEVA stands out as the largest continuously recorded VAD dataset, providing over five times the human-occupied training frames and four times the testing frames compared to previous datasets. Additionally, it is one of the first to support continual learning for VAD, alongside its unsupervised split (Yao et al. 2024; Saha and Roy 2023; Hsu et al. 2018). It maintains real-world properties by encompassing continuous video recordings across seven distinct scenes, including indoor and outdoor settings with real-world traffic. In the PHEVA dataset, six cameras (C0 to C5) observe typical environments such as hallways and parking lots distributed among various locations of a community space. Additionally, a seventh camera was designated to capture a highly context-specific scene focused on law enforcement and security personnel training. Thus, behaviors that are considered normal in this environment are not necessarily normal in other environments. The Context-Specific Camera (CSC) is intended to assess how such specialized scenarios impact model performance.\nMoving to the privacy and ethical considerations in VAD, it is crucial to acknowledge the sensitivity of collecting and publishing human-focused video datasets (Saheb 2023; Ardabili et al. 2023; Joshi et al. 2024; Kunchala, Bouroche, and Schoen-Phelan 2023). Furthermore, deploying VAD algorithms in real-world scenarios raises concerns about potential infringements on individual privacy and rights. This all adds to the fact that VAD algorithms may introduce appear-"}, {"title": "Related Works", "content": "Our research focuses on continuously recorded datasets that feature unsupervised split for anomaly detection. Several early datasets started the field of VAD. Notably, the Subway dataset (Adam et al. 2008) comprises 139 minutes of footage from a subway entrance and exit. The UCSD Pedestrian dataset (Mahadevan et al. 2010) includes 19,000 frames from a pedestrian walkway and the CUHK Avenue dataset (Lu, Shi, and Jia 2013) captures people's movements around a building entrance and exit. While pivotal for VAD's early development, these datasets are now considered relatively small and feature a limited variety of scenes.\nStreet Scene (Ramachandra and Jones 2020) is another anomaly detection dataset, distinguished by its near bird's eye view perspective of a street, offering over 200,000 frames. Compared to previously mentioned datasets, Street Scene is substantial in size and uniquely includes non-human anomalies such as illegally parked cars.\nThe ShanghaiTech Campus (SHT) dataset (Liu et al. 2018) stands as a primary benchmark within the realm of VAD, particularly for pose-based approaches comprising over 317,000 frames captured across 13 distinct campus settings. It includes anomalies such as chases and fights. A shortcoming of the dataset is that the frames per camera are limited, posing challenges for camera-wise evaluation.\nThe IITB dataset (Rodrigues et al. 2020) is introduced as a collection of videos capturing human activities within a corridor, recorded using a single fish-eye camera. Normal activities observed are typically walking and standing, while a range of abnormal activities were performed by volunteers, from individual to group anomalies.\nThe NOLA dataset (Doshi and Yilmaz 2022) with over 1.4 million frames is captured from a single moving camera on a renowned street. In addition to human-centric anomalies, it includes anomalies such as a vehicle moving in the wrong direction.\nThe CHAD dataset (Danesh Pazho et al. 2023) is a large-scale VAD dataset recorded within a parking lot setting. This dataset includes approximately 1.15 million frames with detailed annotations for human detection, tracking, and pose. CHAD features 22 anomaly classes and is captured using four high-resolution cameras at 30 FPS.\nThere are other variations of VAD datasets that are not specifically from continuous recording videos. The UBnormal dataset (Acsintoae et al. 2022) is exclusively composed of synthetically generated videos. Additionally, The UCF Crime (Sultani, Chen, and Shah 2018) and XD-Violence (Wu et al. 2020) datasets differ from traditional anomaly detection datasets due to their compilation from varied sources and contexts rather than continuous recordings. This collection method suggests a fundamental shift in problem formulation, potentially categorizing them under a distinct task (Ramachandra and Jones 2020). While these datasets are valuable resources, they cater to a different problem."}, {"title": "Privacy and Ethical Considerations", "content": "Integrating computer vision algorithms into various sectors of society has underscored the importance of responsibly developing these technologies, with a keen focus on privacy and ethical considerations (Ahmad et al. 2022). In VAD and Smart Video Surveillance (SVS), the implications of AI-driven technologies extend beyond technological advancements, touching upon critical ethical concerns such as bias, discrimination, and privacy violations (Ardabili et al. 2022). These concerns have practical implications for the performance and fairness of computer vision systems in real-world applications. Bias in VAD algorithms can lead to discriminatory practices, where specific demographics may be unfairly targeted or misrepresented (Raso et al. 2018; Noriega 2020). Privacy concerns are equally significant, as the pervasive monitoring and analysis of individuals without adequate safeguards can lead to unwarranted privacy invasions, raising ethical and legal issues (Ardabili et al. 2023).\nAddressing these privacy and ethical concerns necessitates a multifaceted approach. For instance, (Ardabili et al. 2023) proposed a four-tier system design to safeguard privacy in developing AI-driven models, aligning with privacy protection legislation. These tiers encompass the algorithm, system, model, and data levels. However, ethical considerations are particularly crucial during the dataset creation (Whang et al. 2023) and algorithm development. Adopting more abstract representation techniques, such as only focusing on human pose information, has been proposed as an intermediate solution to address some of the privacy, biases, and ethical concerns if not fully mitigating them (Ardabili et al. 2023). Our qualitative interaction with communities also proves that the public and stakeholders are more responsive and participatory to dataset collection and algorithm development when focusing solely on human pose information rather than actual pixels. However, transitioning to a more abstract approach (human pose information rather than deep features at the pixel level) raises questions regarding the potential compromise in model accuracy. Recent advancements challenge this notion. On the SHT dataset (Liu et al. 2018), pixel-based methods like SSMTL++v2 (Barbalau et al. 2023) and Jigsaw-VAD (Wang et al. 2022) achieve AUC-ROC scores of 83.80 and 84.30, while pose-based approaches like MoPRL (Yu et al. 2023) and STG-NF (Hirschorn and Avidan 2023) attain AUC-ROC scores of 83.35 and 85.90.\nOur objective is to initiate the trend of de-identifying PII at the data collection phase instead of algorithmic phase. This methodology not only addresses legal challenges but also promotes expedited data gathering and dissemination. Such an approach is anticipated to facilitate research and technological advancements more finely attuned to the complexities of human behavior and societal requirements."}, {"title": "Data Collection and Setup", "content": "PHEVA dataset is captured employing seven Closed-Circuit Television (CCTV) cameras, each with a resolution of 1280 x 720 pixels. This data acquisition spanned over approximately 5 days of recording from 6:00 AM to 6:00 PM. Tailored only for pose-based anomaly detection, the PHEVA dataset excludes frames with no human presence. PHEVA dataset encompasses a diverse array of scenes, including three outdoor parking areas, three distinct hallway configurations, and a single entrance/exit of a building as seen in Figure 1. Six cameras (CO to C5) overlook normal environments while a context-specific camera (CSC) focuses on an outdoor setup where security personnel and law enforcement training may take place.\nFor a more realistic dataset, we have incorporated behaviors that from a public safety perspective, warrant classification as anomalous. This inclusion aims to enhance the applicability and effectiveness of VAD in real-world scenarios, excluding activities such as jumping, previously considered anomalous in prior datasets. PHEVA's individual anomalies involve throwing, hands up, lying down, and falling. In group situations, anomalies include punching, kicking, pushing, pulling, hitting with an object, and strangling."}, {"title": "Annotation Methodology", "content": "The PHEVA dataset has undergone manual anomaly annotation to guarantee optimal precision. Each frame was labeled and reviewed by at least three different individuals, with random testing conducted to ensure label consistency and accuracy. This dataset provides frame-level anomaly labels. Anomalous behaviors defined in Section 4, are considered abnormal for CO to C5 cameras, while any other typical daily activities are classified as normal. For CSC, actions outlined in Section 4 are deemed normal if conducted during controlled training sessions and considered abnormal otherwise to accurately represent real-world scenarios.\nIn real-world applications, anomaly detection models rely on algorithmically extracted data and not manual person annotations. To simulate this, we used models to extract human annotations, incorporating model noise into the data for more realistic benchmarking.\nAnnotations are rectangular areas that identify and enclose the position of a detected object/human. The bounding boxes are included in the annotations and utilized in later stages for human pose estimation and tracking. PHEVA utilizes YOLOv8 (Jocher, Chaurasia, and Qiu 2023), a SotA deep learning model for object detection.\nAnnotations enables the temporal analysis necessary for detecting subtle or complex anomalies for VAD. PHEVA uses ByteTrack (Zhang et al. 2022), a tracking algorithm that employs byte cost and a Kalman filter to efficiently track objects, handling occlusions and preserving identities.\nrefers to the spatial configuration of human body parts, represented as a set of keypoints enabling the precise analysis of body movements. PHEVA utilizes HRNet (Sun et al. 2019) for human pose estimation. The extracted poses are in the COCO17 (Lin et al. 2014) keypoint format. To improve the quality of pose annotations, linear interpolation is used to fill in missing poses. A 15-frame window is used for data smoothing, and interpolated poses are marked with a null visibility field to allow for their optional exclusion from analyses."}, {"title": "Metrics", "content": "gauges a model's ability to distinguish between classes at different thresholds. It calculates the area under the Receiver Operating Characteristic (ROC) curve, which plots the True Positive Rate (TPR) against the False Positive Rate (FPR) across different thresholds. Though common in VAD, AUC-ROC has drawbacks: it doesn't account for False Negative Rate (FNR), is sensitive to data imbalance, and may obscure key trade-offs by aggregating performance.\n, a metric for binary classifiers that is more suited for imbalanced data. It quantifies the area under the Precision-Recall curve, summarizing classifier performance across thresholds. This metric is valuable for skewed datasets, especially when focusing on minority classes. While higher AUC-PR values signify better models, it's limited in providing detailed analysis of negative class predictions and overall error landscape.\nis another measure to understand the trade-off between FPR and FNR. It shows the point where the rate of"}, {"title": "Comparative Dataset Analysis: Statistical Insights", "content": "In line with the privacy-preserving approach discussed in Section 3, our dataset exclusively provides pose sequence data rather than raw pixel data. For a fair comparison, PHEVA is benchmarked against recent, large-scale, continuously recorded unsupervised human-centric datasets.\nPHEVA is the largest pose-based, continuously recorded VAD dataset to date, with over five million frames (~46 hours) of annotated pose sequences from CCTV cameras. As detailed in Table 1, PHEVA offers over five times more normal training frames than the next largest dataset and includes a substantial test set with over 700K frames (~ 6.5 hours),"}, {"title": "SotA Model Benchmarking", "content": "To demonstrate the effectiveness of the proposed dataset, we leverage SotA pose-based anomaly detection models with an available code repository. Specifically, we employ MPED-RNN (Morais et al. 2019), GEPC (Markovitz et al. 2020), STG-NF (Hirschorn and Avidan 2023) and TSGAD (Noghre, Pazho, and Tabkhi 2024) as discussed in Section 2. For the TSGAD model, we opted to use only the pose branch, aligning with this study's primary focus on pose-based anomaly detection.\nIn the unsupervised learning paradigm, models are trained by minimizing a specific objective to only learn the normal behavior and then evaluated once on a test set. This approach prevents feedback or information leakage between training and evaluation, preserving the integrity of the process. For MPED-RNN (Morais et al. 2019), we used a batch size of 256 over five epochs, with reconstruction and prediction lengths of 12 and 6, respectively, as per the original study. GEPC (Markovitz et al. 2020) was trained with a batch size of 512, a 0.3 dropout rate, and training durations of 10 epochs for the autoencoder and 25 for the deep clustering module. STG-NF (Hirschorn and Avidan 2023) was trained for 8 epochs with a batch size of 256 using the Adamax optimizer. TSGAD (Noghre, Pazho, and Tabkhi 2024) employed a variational autoencoder trained with a batch size of 256, a 0.3 dropout rate, and 20 epochs. The input pose window size was set to 30 frames (1 second) for all models. All models except STG-NF used the Adam optimizer. Detailed training parameters are available in the supplementary materials for replication and further investigation.\nTable 2 illustrates a comprehensive analysis of models benchmarked on the PHEVA dataset. MPED-RNN (Morais et al. 2019) consistently achieves the highest overall performance on the PHEVA dataset, both across all combined cameras and on individual cameras, with TSGAD (Noghre, Pazho, and Tabkhi 2024) consistently ranking second. STG-NF (Hirschorn and Avidan 2023) shows irregular performance, with the highest EER, lowest AUC-ROC, and highest AUC-PR across all experiments. While effective in minimizing false negatives, its simplistic latent space struggles with the complexity of larger datasets like PHEVA, leading to high FPR and poor scalability, emphasizing the need for large, complex benchmarks to expose model weaknesses.\nTable 2 also reveals that the CSC camera offering a parking lot view (see Figure 1) is the most challenging for VAD algorithms. As noted in Section 4, this camera captures context-specific behaviors such as security training, making it harder to distinguish anomalies in the test set, leading to lower performance. Figures 3 also reveal that CSC has the highest crowd density and occlusion, further complicating its challenge for VAD models. These characteristics highlight a significant limitation of pose-based anomaly detection models in capturing context, scene dynamics, and interactions among people.\nWhile EER indicates the lowest equal error rate and its threshold when false positives and negatives are equally costly, real-world VAD often prioritizes minimizing false negatives. Therefore, a model with a lower 10ER is preferred. Table 2 shows that a lower EER doesn't always align with a lower 10ER, which is more practical. Comparisons on C4 and CSC highlight that models with similar EERs can have different 10ERs, underscoring the importance of 10ER in model selection."}, {"title": "Continual Learning Benchmarking", "content": "To simulate distribution shifts, we pre-train models on the SHT dataset (Liu et al. 2018) as the static starting data and adjust PHEVA to mimic real-world streaming data for step-by-step continual training. Given that anomalies are rare in real-world settings, we start with the predefined train and test sets from the original data split (Table 1) and randomly incorporate anomalies into the training stream, ensuring that less than 1% of the training data is anomalous. The test set is edited to be balanced with an approximate 1:1 ratio of normal to anomalous frames to make metrics such as AUC-ROC and AUC-PR more informative. The removed normal frames are added to the streaming training set, which is then split into 9 segments for 9 continual learning steps. We evaluate the two most recent VAD models, STG-NF (Hirschorn and Avidan 2023) and TSGAD (pose branch) (Noghre, Pazho, and Tabkhi 2024), using this framework.\nThe continual training process begins using the pre-trained models on SHT, which is incrementally trained on the split continual train set. The model is tested on the continual test set after each step, with training conducted over 10 epochs and a learning rate of 0.005. More detailed parameters are provided in the supplementary material to ensure reproducibility. This structured approach allows the model to adapt gradually, improving its performance while managing the complexities of continual learning in VAD.\nThe results of continual learning benchmarking are illustrated in Figure 4. The baseline, represented by the dotted lines, involves models trained on SHT (Liu et al. 2018) and tested on a balanced PHEVA test set. The dashed lines indicate the outcomes of conventional training on PHEVA and testing on the balanced test set. Our findings reveal that continual training outperforms the baseline in 98.21% of cases, demonstrating significant model performance improvement. Furthermore, a comparison with normal training shows that continual training yields better results in 82.14% of cases, highlighting its advantages over traditional offline training. The PHEVA dataset has enabled rigorous benchmarking of continual learning, further validating these improvements."}, {"title": "Conclusion", "content": "This study introduces PHEVA, the largest continuously recorded VAD dataset, marking an advancement in the field. Prioritizing privacy and ethical considerations, PHEVA offers extensive annotations across diverse indoor and outdoor scenes, including a novel context-specific setting. Our"}]}