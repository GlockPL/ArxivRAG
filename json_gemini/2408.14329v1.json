{"title": "PHEVA: A Privacy-preserving Human-centric Video Anomaly Detection Dataset", "authors": ["Ghazal Alinezhad Noghre", "Shanle Yao", "Armin Danesh Pazho", "Babak Rahimi Ardabili", "Vinit Katariya", "Hamed Tabkhi"], "abstract": "Video Anomaly Detection (VAD) seeks to identify unusual events in videos that deviate from normal behavior. However, creating VAD datasets is challenging due to the diversity of human behaviors, the rarity of anomalies, and the ethical and privacy concerns associated with human-centric data. To step forward in addressing these challenges, this paper introduces PHEVA, a Privacy-preserving Human-centric Ethical Video Anomaly detection dataset. By removing pixel information and providing only de-identified human annotations, PHEVA safeguards personally identifiable information. The dataset includes seven indoor/outdoor scenes, featuring one novel, context-specific camera, and offers over 5\u00d7 the pose-annotated frames compared to the largest previous dataset. This study benchmarks state-of-the-art methods on PHEVA using a comprehensive set of metrics, including the 10% Error Rate (10ER), a metric used for anomaly detection for the first time providing insights relevant to real-world deployment. As the first of its kind, PHEVA bridges the gap between conventional training and real-world deployment by introducing continual learning benchmarks, with models outperforming traditional methods in 82.14% of cases.", "sections": [{"title": "Introduction", "content": "The field of video surveillance has witnessed significant advancements, driven by the rapid progress in computer vision. Among these, anomaly detection in video streams has emerged as a critical area of research. Video Anomaly Detection (VAD) refers to identifying events or patterns in video streams that deviate from the expected behavior.\nDespite the growing interest in VAD, the availability of datasets in this field remains limited, largely due to the complexity of human behavior. The vast range of normal and abnormal behaviors (open-set) and the context-specific nature of these behaviors-where definitions vary depending on the environment-complicate dataset creation. On top of that, anomalous behaviors are rare, and not easily accessible. All of these are added to practical concerns related to data collection, annotation, and privacy and ethical considerations. To step forward in addressing such limitations, we introduce PHEVA, a privacy-preserving, human-centric, ethical VAD dataset.\nWhile access to more comprehensive datasets can help mitigate the challenges of context-specificity and the open-set nature of VAD, it doesn't fully resolve the issue. To address these challenges beyond the predominant use of unsupervised learning, techniques such as continual learning are increasingly being adopted to manage domain shifts on the algorithmic side. For these techniques to be truly effective, datasets must accurately reflect real-world surveillance footage across diverse environments with continuous recording, providing an adequate number of samples.\nPHEVA stands out as the largest continuously recorded VAD dataset, providing over five times the human-occupied training frames and four times the testing frames compared to previous datasets. Additionally, it is one of the first to support continual learning for VAD, alongside its unsupervised split. It maintains real-world properties by encompassing continuous video recordings across seven distinct scenes, including indoor and outdoor settings with real-world traffic. In the PHEVA dataset, six cameras (C0 to C5) observe typical environments such as hallways and parking lots distributed among various locations of a community space. Additionally, a seventh camera was designated to capture a highly context-specific scene focused on law enforcement and security personnel training. Thus, behaviors that are considered normal in this environment are not necessarily normal in other environments. The Context-Specific Camera (CSC) is intended to assess how such specialized scenarios impact model performance.\nMoving to the privacy and ethical considerations in VAD, it is crucial to acknowledge the sensitivity of collecting and publishing human-focused video datasets. Furthermore, deploying VAD algorithms in real-world scenarios raises concerns about potential infringements on individual privacy and rights. This all adds to the fact that VAD algorithms may introduce appearance biases against minority groups, leading to unfair decisions. To tackle these challenges, algorithms leverage pose to anonymize individuals in VAD and to reduce biases found in traditional pixel-based methods. This abstraction enhances privacy by removing most Personally Identifiable Information (PII) and reducing the impact of demographic characteristics such as age, gender, and race. While this solution mitigates algorithmic issues, dataset publication remains challenging. Our research proposes shifting pose extraction from an algorithmic pre-process to a dataset post-process step, inherently anonymizing data and facilitating public release. The PHEVA dataset, therefore, publishes only extensive de-identified human annotations such as bounding boxes, tracking IDs, and human poses providing a unified benchmark for pose-based VAD models.\nIn this study, we conduct extensive experiments, not only statistically comparing PHEVA with its peers but also benchmarking State-of-the-Art (SotA) pose-based VAD algorithms to ensure the learnability of data. Unlike most prior works that rely on a single metric, we evaluate these algorithms on PHEVA using a comprehensive set of metrics: Area Under the Receiver Operating Characteristic Curve (AUC-ROC), Area Under the Precision-Recall Curve (AUC-PR), the less commonly used Equal Error Rate (EER), and the 10% Error Rate (10ER), a metric new to anomaly detection that considers the unequal costs of false positives and negatives in real-world scenarios. Additionally, recognizing PHEVA's potential for continual learning, we introduce a tailored continual learning benchmark, providing adaptable and unified training and evaluation, showing that continual learning outperforms conventional training in 82.14% of benchmarked cases.\nThe contributions of this paper are:\n\u2022 PHEVA is the largest privacy-preserving, human-centric VAD dataset, continuously recorded in real-world community spaces with comprehensive de-identified human annotations.\n\u2022 Providing a context-specific camera capturing law enforcement training activities to highlight the impacts of context awareness and social interaction in VAD.\n\u2022 A detailed statistical comparison with other VAD datasets and comprehensive benchmarking of pose-based algorithms on PHEVA, using key metrics and the newly introduced 10ER metric for anomaly detection\u00b9.\n\u2022 PHEVA is the first dataset to introduce benchmarks for continual learning of VAD algorithms filling the gap between conventional training and real-world deployment."}, {"title": "Related Works", "content": "Our research focuses on continuously recorded datasets that feature unsupervised split for anomaly detection. Several early datasets started the field of VAD. Notably, the Subway dataset comprises 139 minutes of footage from a subway entrance and exit. The UCSD Pedestrian dataset includes 19,000 frames from a pedestrian walkway and the CUHK Avenue dataset captures people's movements around a building entrance and exit. While pivotal for VAD's early development, these datasets are now considered relatively small and feature a limited variety of scenes.\nStreet Scene is another anomaly detection dataset, distinguished by its near bird's eye view perspective of a street, offering over 200,000 frames. Compared to previously mentioned datasets, Street Scene is substantial in size and uniquely includes non-human anomalies such as illegally parked cars.\nThe ShanghaiTech Campus (SHT) dataset stands as a primary benchmark within the realm of VAD, particularly for pose-based approaches comprising over 317,000 frames captured across 13 distinct campus settings. It includes anomalies such as chases and fights. A shortcoming of the dataset is that the frames per camera are limited, posing challenges for camera-wise evaluation.\nThe IITB dataset is introduced as a collection of videos capturing human activities within a corridor, recorded using a single fish-eye camera. Normal activities observed are typically walking and standing, while a range of abnormal activities were performed by volunteers, from individual to group anomalies.\nThe NOLA dataset with over 1.4 million frames is captured from a single moving camera on a renowned street. In addition to human-centric anomalies, it includes anomalies such as a vehicle moving in the wrong direction.\nThe CHAD dataset is a large-scale VAD dataset recorded within a parking lot setting. This dataset includes approximately 1.15 million frames with detailed annotations for human detection, tracking, and pose. CHAD features 22 anomaly classes and is captured using four high-resolution cameras at 30 FPS.\nThere are other variations of VAD datasets that are not specifically from continuous recording videos. The UBnormal dataset is exclusively composed of synthetically generated videos. Additionally, The UCF Crime and XD-Violence datasets differ from traditional anomaly detection datasets due to their compilation from varied sources and contexts rather than continuous recordings. This collection method suggests a fundamental shift in problem formulation, potentially categorizing them under a distinct task. While these datasets are valuable resources, they cater to a different problem."}, {"title": "Privacy and Ethical Considerations", "content": "Integrating computer vision algorithms into various sectors of society has underscored the importance of responsibly developing these technologies, with a keen focus on privacy and ethical considerations. In VAD and Smart Video Surveillance (SVS), the implications of AI-driven technologies extend beyond technological advancements, touching upon critical ethical concerns such as bias, discrimination, and privacy violations. These concerns have practical implications for the performance and fairness of computer vision systems in real-world applications. Bias in VAD algorithms can lead to discriminatory practices, where specific demographics may be unfairly targeted or misrepresented. Privacy concerns are equally significant, as the pervasive monitoring and analysis of individuals without adequate safeguards can lead to unwarranted privacy invasions, raising ethical and legal issues.\nAddressing these privacy and ethical concerns necessitates a multifaceted approach. For instance,  proposed a four-tier system design to safeguard privacy in developing AI-driven models, aligning with privacy protection legislation. These tiers encompass the algorithm, system, model, and data levels. However, ethical considerations are particularly crucial during the dataset creation and algorithm development. Adopting more abstract representation techniques, such as only focusing on human pose information, has been proposed as an intermediate solution to address some of the privacy, biases, and ethical concerns if not fully mitigating them. Our qualitative interaction with communities also proves that the public and stakeholders are more responsive and participatory to dataset collection and algorithm development when focusing solely on human pose information rather than actual pixels. However, transitioning to a more abstract approach (human pose information rather than deep features at the pixel level) raises questions regarding the potential compromise in model accuracy. Recent advancements challenge this notion. On the SHT dataset, pixel-based methods like SSMTL++v2 and Jigsaw-VAD achieve AUC-ROC scores of 83.80 and 84.30, while pose-based approaches like MoPRL and STG-NF attain AUC-ROC scores of 83.35 and 85.90.\nOur objective is to initiate the trend of de-identifying PII at the data collection phase instead of algorithmic phase. This methodology not only addresses legal challenges but also promotes expedited data gathering and dissemination. Such an approach is anticipated to facilitate research and technological advancements more finely attuned to the complexities of human behavior and societal requirements."}, {"title": "Data Collection and Setup", "content": "PHEVA dataset is captured employing seven Closed-Circuit Television (CCTV) cameras, each with a resolution of 1280 x 720 pixels. This data acquisition spanned over approximately 5 days of recording from 6:00 AM to 6:00 PM. Tailored only for pose-based anomaly detection, the PHEVA dataset excludes frames with no human presence. PHEVA dataset encompasses a diverse array of scenes, including three outdoor parking areas, three distinct hallway configurations, and a single entrance/exit of a building. Six cameras (CO to C5) overlook normal environments while a context-specific camera (CSC) focuses on an outdoor setup where security personnel and law enforcement training may take place.\nFor a more realistic dataset, we have incorporated behaviors that from a public safety perspective, warrant classification as anomalous. This inclusion aims to enhance the applicability and effectiveness of VAD in real-world scenarios, excluding activities such as jumping, previously considered anomalous in prior datasets. PHEVA's individual anomalies involve throwing, hands up, lying down, and falling. In group situations, anomalies include punching, kicking, pushing, pulling, hitting with an object, and strangling."}, {"title": "Annotation Methodology", "content": "Anomaly Annotation\nThe PHEVA dataset has undergone manual anomaly annotation to guarantee optimal precision. Each frame was labeled and reviewed by at least three different individuals, with random testing conducted to ensure label consistency and accuracy. This dataset provides frame-level anomaly labels. Anomalous behaviors defined in Section 4, are considered abnormal for CO to C5 cameras, while any other typical daily activities are classified as normal. For CSC, actions outlined in Section 4 are deemed normal if conducted during controlled training sessions and considered abnormal otherwise to accurately represent real-world scenarios.\nPerson Annotation\nIn real-world applications, anomaly detection models rely on algorithmically extracted data and not manual person annotations. To simulate this, we used models to extract human annotations, incorporating model noise into the data for more realistic benchmarking.\nBounding Boxes Annotations are rectangular areas that identify and enclose the position of a detected object/human. The bounding boxes are included in the annotations and utilized in later stages for human pose estimation and tracking. PHEVA utilizes YOLOv8, a SotA deep learning model for object detection.\nPerson ID Annotations enables the temporal analysis necessary for detecting subtle or complex anomalies for VAD. PHEVA uses ByteTrack, a tracking algorithm that employs byte cost and a Kalman filter to efficiently track objects, handling occlusions and preserving identities.\nHuman Pose Annotations refers to the spatial configuration of human body parts, represented as a set of keypoints enabling the precise analysis of body movements. PHEVA utilizes HRNet for human pose estimation. The extracted poses are in the COCO17 keypoint format. To improve the quality of pose annotations, linear interpolation is used to fill in missing poses. A 15-frame window is used for data smoothing, and interpolated poses are marked with a null visibility field to allow for their optional exclusion from analyses."}, {"title": "Metrics", "content": "AUC-ROC gauges a model's ability to distinguish between classes at different thresholds. It calculates the area under the Receiver Operating Characteristic (ROC) curve, which plots the True Positive Rate (TPR) against the False Positive Rate (FPR) across different thresholds. Though common in VAD, AUC-ROC has drawbacks: it doesn't account for False Negative Rate (FNR), is sensitive to data imbalance, and may obscure key trade-offs by aggregating performance.\nAUC-PR, a metric for binary classifiers that is more suited for imbalanced data. It quantifies the area under the Precision-Recall curve, summarizing classifier performance across thresholds. This metric is valuable for skewed datasets, especially when focusing on minority classes. While higher AUC-PR values signify better models, it's limited in providing detailed analysis of negative class predictions and overall error landscape.\nEER is another measure to understand the trade-off between FPR and FNR. It shows the point where the rate of"}, {"title": "Comparative Dataset Analysis: Statistical Insights", "content": "In line with the privacy-preserving approach discussed in Section 3, our dataset exclusively provides pose sequence data rather than raw pixel data. For a fair comparison, PHEVA is benchmarked against recent, large-scale, continuously recorded unsupervised human-centric datasets. PHEVA is the largest pose-based, continuously recorded VAD dataset to date, with over five million frames (~46 hours) of annotated pose sequences from CCTV cameras. As detailed in Table 1, PHEVA offers over five times more normal training frames than the next largest dataset and includes a substantial test set with over 700K frames (~ 6.5 hours), which is four times larger than the second largest dataset, with a 70/30 split between normal and anomalous frames. Beyond the large volume of frames, the dataset's quality is highlighted by the pose count, which exceeds 8 million in the training set and nearly 1.5 million in the test set. Figures compares pose counts across various datasets, segmented by different views, providing a more detailed metric than frame counts alone. The large volume of per-camera data points is intentionally designed to meet the demands of emerging evaluation methods, such as online and continual learning experiments as discussed in Section 1.\nFigures 3.A and 3.B depict the distribution of the highest Intersection over Union (IoU) ratio per frame, comparing this data across different datasets and among the various cameras within the PHEVA dataset. A high IoU among bounding boxes indicates increased scene occlusion and a more challenging environment. Figures highlights PHEVA's occlusion distribution compared to its peers. IITB and CHAD are characterized by a wide range of IoU values, which suggests variability in the level of scene occlusion. In contrast, the SHT dataset, much like PHEVA, maintains a smaller range of distribution. On the other hand, Figure 3.B shows that PHEVA includes various levels of occlusion across all cameras. CSC stands out as the most occluded and challenging camera with the highest median.\nIt is also shown in Figure 3.C and 3.D that most of the views have a substantial density from 3 to 14 people per"}, {"title": "SotA Model Benchmarking", "content": "frame except for CSC, which shows a higher crowd density due to its nature showcasing groups of people conducting security training. CHAD exhibits the lowest range of crowdedness and SHT the highest. PHEVA generally stands out for its number of frames for each level of crowdedness.\nTo demonstrate the effectiveness of the proposed dataset, we leverage SotA pose-based anomaly detection models with an available code repository. Specifically, we employ MPED-RNN , GEPC , STG-NF and TSGAD as discussed in Section 2. For the TSGAD model, we opted to use only the pose branch, aligning with this study's primary focus on pose-based anomaly detection.\nIn the unsupervised learning paradigm, models are trained by minimizing a specific objective to only learn the normal behavior and then evaluated once on a test set. This approach prevents feedback or information leakage between training and evaluation, preserving the integrity of the process. For MPED-RNN , we used a batch size of 256 over five epochs, with reconstruction and prediction lengths of 12 and 6, respectively, as per the original study. GEPC was trained with a batch size of 512, a 0.3 dropout rate, and training durations of 10 epochs for the autoencoder and 25 for the deep clustering module. STG-NF was trained for 8 epochs with a batch size of 256 using the Adamax optimizer. TSGAD employed a variational autoencoder trained with a batch size of 256, a 0.3 dropout rate, and 20 epochs. The input pose window size was set to 30 frames (1 second) for all models. All models except STG-NF used the Adam optimizer. Detailed training parameters are available in the supplementary materials for replication and further investigation.\nTable 2 illustrates a comprehensive analysis of models benchmarked on the PHEVA dataset. MPED-RNN consistently achieves the highest overall performance on the PHEVA dataset, both across all combined cameras and on individual cameras, with TSGAD consistently ranking second. STG-NF shows irregular performance, with the highest EER, lowest AUC-ROC, and highest AUC-PR across all experiments. While effective in minimizing false negatives, its simplistic latent space struggles with the complexity of larger datasets like PHEVA, leading to high FPR and poor scalability, emphasizing the need for large, complex benchmarks to expose model weaknesses.\nTable 2 also reveals that the CSC camera offering a parking lot view (see Figure 1) is the most challenging for VAD algorithms. As noted in Section 4, this camera captures context-specific behaviors such as security training, making it harder to distinguish anomalies in the test set, leading to lower performance. Figures 3 also reveal that CSC has the highest crowd density and occlusion, further complicating its challenge for VAD models. These characteristics highlight a significant limitation of pose-based anomaly detection models in capturing context, scene dynamics, and interactions among people.\nWhile EER indicates the lowest equal error rate and its threshold when false positives and negatives are equally costly, real-world VAD often prioritizes minimizing false negatives. Therefore, a model with a lower 10ER is preferred. Table 2 shows that a lower EER doesn't always align with a lower 10ER, which is more practical. Comparisons on C4 and CSC highlight that models with similar EERs can have different 10ERs, underscoring the importance of 10ER in model selection."}, {"title": "Continual Learning Benchmarking", "content": "To simulate distribution shifts, we pre-train models on the SHT dataset as the static starting data and adjust PHEVA to mimic real-world streaming data for step-by-step continual training. Given that anomalies are rare in real-world settings, we start with the predefined train and test sets from the original data split (Table 1) and randomly incorporate anomalies into the training stream, ensuring that less than 1% of the training data is anomalous. The test set is edited to be balanced with an approximate 1:1 ratio of normal to anomalous frames to make metrics such as AUC-ROC and AUC-PR more informative. The removed normal frames are added to the streaming training set, which is then split into 9 segments for 9 continual learning steps. We evaluate the two most recent VAD models, STG-NF and TSGAD (pose branch) using this framework.\nThe continual training process begins using the pre-trained models on SHT, which is incrementally trained on the split continual train set. The model is tested on the continual test set after each step, with training conducted over 10 epochs and a learning rate of 0.005. More detailed parameters are provided in the supplementary material to ensure reproducibility. This structured approach allows the model to adapt gradually, improving its performance while managing the complexities of continual learning in VAD.\nThe results of continual learning benchmarking are illustrated in Figure 4. The baseline, represented by the dotted lines, involves models trained on SHT and tested on a balanced PHEVA test set. The dashed lines indicate the outcomes of conventional training on PHEVA and testing on the balanced test set. Our findings reveal that continual training outperforms the baseline in 98.21% of cases, demonstrating significant model performance improvement. Furthermore, a comparison with normal training shows that continual training yields better results in 82.14% of cases, highlighting its advantages over traditional offline training. The PHEVA dataset has enabled rigorous benchmarking of continual learning, further validating these improvements."}, {"title": "Conclusion", "content": "This study introduces PHEVA, the largest continuously recorded VAD dataset, marking an advancement in the field. Prioritizing privacy and ethical considerations, PHEVA offers extensive annotations across diverse indoor and outdoor scenes, including a novel context-specific setting. Our"}]}