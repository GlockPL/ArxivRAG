{"title": "Tarsier2: Advancing Large Vision-Language Models from Detailed Video Description to Comprehensive Video Understanding", "authors": ["Liping Yuan", "Jiawei Wang", "Haomiao Sun", "Yuchen Zhang", "Yuan Lin"], "abstract": "We introduce Tarsier2, a state-of-the-art large vision-language model (LVLM) designed for generating detailed and accurate video descriptions, while also exhibiting superior general video understanding capabilities. Tarsier2 achieves significant advancements through three key upgrades: (1) Scaling pre-training data from 11M to 40M video-text pairs, enriching both volume and diversity; (2) Performing fine-grained temporal alignment during supervised fine-tuning; (3) Using model-based sampling to automatically construct preference data and applying DPO training for optimization. Extensive experiments show that Tarsier2-7B consistently outperforms leading proprietary models, including GPT-40 and Gemini 1.5 Pro, in detailed video description tasks. On the DREAM-1K benchmark, Tarsier2-7B improves F1 by 2.8% over GPT-40 and 5.8% over Gemini-1.5-Pro. In human side-by-side evaluations, Tarsier2-7B shows a +8.6% performance advantage over GPT-40 and +24.9% over Gemini-1.5-Pro. Tarsier2-7B also sets new state-of-the-art results across 15 public benchmarks, spanning tasks such as video question-answering, video grounding, hallucination test, and embodied question-answering, demonstrating its versatility as a robust generalist vision-language model.", "sections": [{"title": "1 Introduction", "content": "With the rapid advancements in large vision-language models (LVLM) [21, 56, 61, 62,\n105, 106], significant progress has also been made in video understanding. Leading pro-\nprietary models, such as GPT-40 [41] and Gemini-1.5-Pro [102], have achieved state-of-\nthe-art (SOTA) performance across a variety of video understanding tasks. Additionally,\nseveral open-source models [61, 114, 23, 52, 20, 53, 23] also demonstrate strong perfor-\nmance on several video understanding benchmarks [25, 57, 67, 109, 128], although they\nstill lag behind proprietary models, particularly in complex, open-ended generation tasks.\nDespite these advancements, current models remain behind human-level video understand-\ning [78, 86, 19], mainly due to persistent challenges such as accurately perceiving temporal\ndynamics, spatial-temporal reasoning, and model hallucinations.\nIn this paper, we introduce Tarsier2, a 7B-parameter LVLM model that can outperform\nboth GPT-40 and Gemini-1.5-Pro in generating detailed video descriptions, a fundamen-\ntal challenge in video understanding. Beyond video description generation, Tarsier2 also\nachieves SOTA performance across various video question-answering (VQA) benchmarks\nat the same model size, surpassing or closely matching the performance of proprietary\nmodels on these VQA benchmarks. Figure 1 provides a comprehensive comparison be-\ntween Tarsier2, GPT-40 and previous SOTA results for open-source LVLMs with the same\nscale. Figure 2 presents examples illustrating Tarsier2's video understanding capability\nacross different tasks.\nTarsier2 employs a simple model architecture consisting of a vision encoder, a vision\nadaptor, and a large language model (LLM). We meticulously design a three-stage training\nprocedure: pre-training, supervised fine-tuning (SFT), and reinforcement learning (RL). In\ncomparison with Tarsier [105], Tarsier2 features several key improvements that significantly\nenhance its performance:\n\u2022 We scale up the pre-training dataset from 11 million to 40 million video-text pairs,\naddressing the challenge posed by the scarcity of high-quality video-text data. To\nachieve this, we implement meticulous filtering and sourcing. Specifically, we collect\n11 million commentary videos, featuring explanations and analyses of movies and TV\nshows, providing rich contextual information to greatly enhance video understanding.\nOur experiments confirm that increasing the volume of pre-training data consistently\nimproves model performance.\n\u2022 We construct a video description dataset containing 150K instances, each including a\ndetailed video description along with the specific frames corresponding to each event\ndescribed. During the SFT stage, we involve this dataset to provide the model with\nsupervision on temporal fine-grained alignment. Experimental results show that,\ncompared with traditional video-caption alignment training, this approach signifi-\ncantly improves accuracy in video description and reduces the hallucinations."}, {"title": "2 Related Work", "content": "Video-LLMs Recently, research on Video LLMs has surged [56, 76, 75, 121, 61, 6, 104,\n114, 52, 62, 127, 106, 54, 27, 2, 72, 20, 130], with efforts focusing on model architectures\nand video-text data collection. On the architecture side, current studies emphasize visual\nrepresentation [114, 106, 130], visual token resampling [114, 20, 115, 58], and the integra-\ntion of Vision Transformers (ViT) with LLMs [106, 55, 65, 8]. Tarsier2 adopts a simple\narchitecture composed of a visual encoder, a visual adaptor, and an LLM. Despite its sim-\nplicity, we demonstrate that a meticulously designed training strategy enables Tarsier2 to\nachieve strong video understanding capabilities.\nIn terms of video-text data, while many efforts aim to collect datasets for training\nVideo LLMs, their quantity and quality remain limited. For example, LLaVA-Video [127]\nis trained on just 1.3 million video-text pairs, and several open-source models, such as In-\nternVL2.5 [20], Aria [54], and VILA-1.5 [62], are trained on fewer than 5 million pairs.\nAlthough larger datasets like HowTo100M [81], HD-VILA [116], Panda-70M [18], and\nInternVid-10M [108] exist, they either cover limited domains or contain overly simplis-\ntic or low-quality text. Furthermore, some studies do not disclose the volume of video data\nused [106, 130, 27, 54]."}, {"title": "3 Approach", "content": "We initialized Tarsier with Qwen2-VL [106] weights and employed a three-stage training\nstrategy. First, we pre-trained Tarsier2 on 40 million large-scale video-text pairs. Next,\nwe fine-tuned the model on moderate-sized, curated, human-annotated datasets in two\nphases: one targeting video descriptions with fine-grained grounding and the other focus-\ning on natural, instruction-following video descriptions. Finally, we applied Direct Pref-\nerence Optimization [89] using automatically generated preference data to further enhance\nthe quality of the video descriptions. The training process is detailed below; for a compre-\nhensive list of hyper-parameters, please refer to Appendix A."}, {"title": "3.1 Pre-training", "content": "The pre-training stage encompasses a variety of tasks, including video captioning, video\nquestion answering, action recognition, action grounding, (multi-)image understanding,\nand text generation. The training data consists of 20 million public datasets and 20 million\nnewly collected in-house datasets. Figure 3 illustrates the composition of the pre-training\ndata, with a detailed breakdown presented in Appendix B. Our findings indicate that\nthe in-house data significantly enhances model's performance, complementing the public\ndatasets. In the following, we describe the pipeline used for in-house data collection.\nWe collected a large group of videos from the Internet, spanning diverse genres such\nas animation, movies, TV series, short videos, stock footage, games and so on. The videos\nare categorized into three types:\n\u2022 Short videos with captions. This category consists of 2.4 million videos directly\nsourced from the Internet, preserving their original video-caption pairs.\n\u2022 Commentary videos for movies or TV shows. The videos were segmented\ninto single-shot clips using PySceneDetect\u00b9. A filtering model removed static or"}, {"title": "3.2 Supervised fine-tuning", "content": "During the SFT phase, our primary objectives are to further improve the model's accu-\nracy and comprehensiveness in video descriptions and ensure the outputs are human-like:\nwell-structured, appropriately detailed, and capable of generating accurate long-form de-\nscriptions. To achieve this, we collected 150K video clips and conducted SFT in two stages.\nIn the first stage, each video clip in the SFT dataset is annotated with a detailed\ndescription with fine-grained temporal grounding. As shown in Figure 4, the annotations\nspecify the frames corresponding to each event in the description. The annotation process\nis detailed in Appendix C. This fine-grained frame-event alignment enhances the model's\nability to accurately identify and describe events by focusing on temporal and visual cues,\ncomplementing traditional video-caption alignment. Our experiments demonstrate that\nthis approach mitigates the omission of key events in generated video descriptions."}, {"title": "3.3 Direct Preference Optimization", "content": "In this subsection, we introduce a novel automated method for collecting preference data for\nvideo description. By performing DPO [89] training on this data, we can further improve\nthe model's ability to generate high-quality, detailed video descriptions.\nNegative sampling Existing works often conduct multiple times sampling on the same\ninput (video and text prompt) to acquire preference pair candidates [111, 126, 100]. In"}, {"title": "4 Experiments", "content": "In this section, we first evaluate the model's performance on various video understanding\nbenchmarks, comparing it to several baselines. We highlight Tarsier2's advantages not only\nin video description but also across other video understanding tasks. We then present an\nablation study to examine key components of our approach."}, {"title": "4.1 Quantitative Results", "content": null}, {"title": "4.1.1 Video Captioning", "content": "We evaluate Tarsier2 on two video captioning benchmarks: DREAM-1K [105] and E.T.\nBench-Captioning[67]. DREAM-1K is a detailed video description benchmark featuring\ndynamic and diverse videos, assessing the model's ability to describe fine-grained actions\nand events. E.T Bench-Captioning is composed of four dense video captioning tasks,\nrequiring key event localization and summary generation for segments in long-form videos."}, {"title": "4.1.2 Short-Video Question Answering", "content": "We evaluate Tarsier2-7B on several short-video question answering benchmarks to assess its\nability to comprehend and reason about visual content. As shown in Table 3, Tarsier2-7B\noutperforms both proprietary and open-source models across various benchmarks, achiev-\ning state-of-the-art results. Tarsier2-7B exhibits exceptional performance in MVBench [57]\nand PerceptionTest [86], with scores of 71.5% and 71.6%, respectively.\nFurthermore, Tarsier2-7B demonstrates significant performance improvements on bench-\nmarks featuring temporal reasoning, such as TVBench [25], \u03a4OMATO [94], and Vinoground\n[123]. Tarsier2-7B achieves strong results with 54.7% on TVBench, 42.0% on TOMATO,"}, {"title": "4.1.3 Long-Video Question Answering", "content": "We evaluate Tarsier2 on long-video question answering benchmarks by uniformly sam-\npling 128 or 256 frames, depending on the video length. Comparison results with other\nproprietary and open-source models are presented in Table 4. Despite our training set not\nincluding many long video data, Tarsier2, compared with others under 10 billion param-\neters, still achieves SOTA on three benchmarks and competitive performance on several\nother benchmarks."}, {"title": "4.1.4 Hallucination", "content": "We evaluate Tarsier2 on two video hallucination benchmarks: VideoHallucer [109] and\nEventHallusion [122]. The results are summarized in Table 5. For VideoHallucer, Tarsier2-\n7B achieves an overall score of 67.0%, outperforming all comparable baselines of similar"}, {"title": "4.1.5 Video Grounding", "content": "We evaluate the video grounding capability of models on E.T. Bench-Grounding, which\ncombines various grounding tasks from multiple datasets, including QVHighlights [51],\nCharades-STA [32], THUMOS'14 [42], and Ego4D-NLQ [35], among others. The results,\nshown in Table 6, indicate that Tarsier2-7B achieves the highest mean F1 score of 35.5%,\noutperforming all baselines and highlighting its superior temporal perception capabilities."}, {"title": "4.1.6 Embodied Question Answering", "content": "We evaluate Tarsier2 on embodied question answering to assess its performance in real-\nworld robotic scenarios, using three benchmarks: EgoTaskQA [44], RoboVQA [93], and\nOpenEQA [77]. To align with the baselines, Tarsier2 is fine-tuned on the training sets for\nEgoTaskQA and RoboVQA, while for OpenEQA, it is evaluated in a zero-shot setting.\nThe results, presented in Table 7, include exact match accuracy for EgoTaskQA, BLEU\nscore for RoboVQA, and the correctness score evaluated by GPT-4-1106-preview [1] for\nOpenEQA. Tarsier2 achieves top-tier performance across all three benchmarks, outper-\nforming both generalist and specialist models. Notably, on EgoTaskQA, its performance"}, {"title": "4.2 Ablation Study", "content": "We conduct a comprehensive ablation study to evaluate key components at different stages\nof the training process. The study is based on three tasks: 1) Caption: This includes the\nDREAM-1K dataset, the caption generation task from TempCompass (TempCompass-cg),\nand the caption matching task from Vinoground (Vinoground-Text) to assess captioning\nperformance. 2) Video QA: This encompasses short-video QA, measured by the average\naccuracy on MVBench, TVBench, and TOMATO, and long-video QA, measured by the\naverage accuracy on Video-MME, LongVideoBench, and TemporalBench. It evaluates the\nmodel's video understanding capabilities. 3) Hallucination: We use the average score"}, {"title": "4.2.1 Pre-training", "content": "In this section, we evaluate the impact of several factors during pre-training, including\nthe base model, pre-training data and training steps. For the caption task, we report\nresults after the SFT stage, which aligns the model's responses with the desired style. For\nother tasks, we report results after pre-training stage.\nCompared to Tarsier1, two key improvements are made in the pre-training phase: up-\ngrading the base model to Qwen2-VL and expanding the training dataset from 13 million\nto 40 million samples. Table 8 illustrates the additive contributions for each improvement,\nshowing that both enhancements consistently and significantly boost the model's perfor-\nmance in caption generation, video QA, and hallucination reduction. Specifically, these\nenhancements lead to accuracy improvements of 9.7%, 17.8%, and 7.2% for short-video\nQA, long-video QA, and hallucination tests, respectively. For video description, the F1\nscore on the DREAM-1K dataset improves by 6.2%.\nTo better understand the effect of the number of training tokens on pre-training perfor-\nmance, we plot the model's performance as a function of token count during the pre-training\nstage, as shown in Figure 7. The results show that model performance improves with an\nincrease in the number of training tokens, reaching convergence after 160 billion tokens.\nThis suggests that a large volume of data is essential for optimal video understanding\nperformance."}, {"title": "4.2.2 SFT", "content": "The key factor in the SFT phase is fine-grained alignment. To investigate its impact,\nwe conduct an ablation study, with the results presented in Table 9. When the video\ndescription data, which includes fine-grained temporal grounding information, is excluded\n(i.e., without grounding), model performance significantly deteriorates. Specifically, the\nF1 score on DREAM-1K decreases by 3.4%, accuracy on TempCompass-cg drops by 9.9%,\naccuracy on long-video QA falls by 1.3%, and accuracy on the hallucination test declines\nby 3.3%.\nFurthermore, the SFT phase leads to substantial improvements, highlighting the im-\nportance of high-quality manually labeled data. It boosts the F1 score on DREAM-1K\nby 5.6%, accuracy on TempCompass-cg by 9.6%, accuracy on Vinoground-Text by 3.0%,"}, {"title": "4.2.3 DPO", "content": "We conduct ablation experiments to evaluate the DPO phase, negative sampling (NS) and\npreference data filtering (PF) strategies. Specifically, we test the following settings: 1)\nw/o DPO: SFT model without DPO training. 2) w/o NS: Preference pairs generated\nby sampling the same video twice, without negative sampling. 3) w/o PF: Responses from\nnegative sampling are treated as rejections, without utilizing AutoDQ Scorer to perform\npreference data filtering. For a fair comparison, the training data size and hyper-parameters\nfor the latter two settings are kept consistent with the default setting, as detailed in Ap-\npendix D.\nAs shown in Table 10, Tarsier2 benefits a lot from the DPO training phase with signif-\nicant improvement on caption tasks, especially TempCompass-cg (6.5%) and Vinoground-\nText (5.6%). The hallucination capability also drops by 2.1% without DPO, while the\nperformance on video QA is not obviously affected. When further ablating dataset con-\nstruction strategy of DPO, negative sampling plays an important role, without which the\nmodel results on most of the tasks are degraded to be almost the same as the SFT model\n(\u201cw/o DPO\u201d), and the hallucination capability drops by 1.1%. Additionally, preference\ndata filtering with AutoDQ scorer has a significant impact on maintaining the quality of\nDPO datasets. As shown in Table 10, \"w/o PF\" leads to degradation on more than a half\nof the tasks, and especially the DREAM-1K F1 score is even worse than the SFT model."}, {"title": "4.3 Video Recaptioning using Tarsier2", "content": "In this section, we utilize Tarsier2 as a captioner to generate detailed descriptions for a\ndiverse set of 1M videos sourced from public datasets, resulting in the recaptioning dataset\nTarsier2-Recap-585K5. Details of the dataset composition are provided in Appendix F."}, {"title": "5 Conclusion", "content": "In this paper, we introduce Tarsier2, a state-of-the-art large vision-language model that\noutperforms existing proprietary and open-source models in generating detailed and accu-\nrate video descriptions. Furthermore, Tarsier2 sets new benchmarks across a wide range\nof video understanding tasks. Our ablation studies demonstrate that Tarsier2 's advance-\nments are driven by scaling the volume and diversity of the training dataset, fine-grained\ntemporal alignment, and DPO training.\nLooking ahead, we outline several promising directions for future research. First, ex-\ntending Tarsier2 to handle longer video durations by developing more efficient model archi-\ntectures and expanding the training dataset. Second, enhancing real-time video processing\nto improve the model's ability to analyze and describe videos as they stream. Third, ex-\nploring richer interactions between video, audio, and text to create more comprehensive\nand context-aware video understanding systems."}, {"title": "ADQR\u2265 0 and ADQP > 0 and ADQR+ \u2206DQP \u2265 \u03b4,", "content": "\u0394DQR\u2265 0 and \u0394DQP > 0 and \u0394DQR+ \u0394DQP \u2265 \u03b4,"}, {"title": "LDPO= -E(x,yw,y1)~D logo", "content": "LDPO= -E(x,yw,y1)~D logo"}, {"title": "\u03c0\u03b8(Yw|x)Tref (Yw|x)\u03c0\u03b8(\u03b3\u03b9|x)Tref (Ylx)", "content": "\u03c0\u03b8(Yw|x)Tref (Yw|x)\u03c0\u03b8(\u03b3\u03b9|x)Tref (Ylx)"}]}