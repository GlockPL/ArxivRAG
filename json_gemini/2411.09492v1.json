{"title": "MM-Eval: A Hierarchical Benchmark for Modern Mongolian Evaluation in LLMs", "authors": ["Mengyuan Zhang", "Ruihui Wang", "Bo Xia", "Yuan Sun", "Xiaobing Zhao"], "abstract": "Large language models (LLMs) excel in high- resource languages but face notable challenges in low-resource languages like Mongolian. This paper addresses these challenges by cat- egorizing capabilities into language abilities (syntax and semantics) and cognitive abilities (knowledge and reasoning). To systematically evaluate these areas, we developed MM-Eval, a specialized dataset based on Modern Mongo- lian Language Textbook I and enriched with WebQSP and MGSM datasets.\nPreliminary experiments on models including Qwen2-7B-Instruct, GLM4-9b-chat, Llama3.1- 8B-Instruct, GPT-4, and DeepseekV2.5 re- vealed that: 1) all models performed better on syntactic tasks than semantic tasks, highlight- ing a gap in deeper language understanding; and 2) knowledge tasks showed a moderate decline, suggesting that models can transfer general knowledge from high-resource to low- resource contexts.\nThe release of MM-Eval-comprising 569 syn- tax, 677 semantics, 344 knowledge, and 250 reasoning tasks offers valuable insights for advancing NLP and LLMs in low-resource lan- guages like Mongolian. The dataset is available at https://github.com/joenahm/MM-Eval.", "sections": [{"title": "1 Introduction", "content": "In recent years, large language models (LLMs) have revolutionized natural language processing (NLP), demonstrating remarkable capabilities in understanding and generating human language, excelling in tasks such as context comprehen- sion(Jin et al., 2024), language generation(Malik et al., 2024), summarization(Song et al., 2024), question answering(Schimanski et al., 2024), and translation(Xu et al., 2024). Models like Chat- GPT(OpenAI, 2023) and Llama(Touvron et al., 2023) have set new benchmarks across a wide range of languages, primarily high-resource ones such as Chinese and English. However, the sup- port for low-resource languages like Mongolian remains largely unexplored.\nMongolian, spoken by millions across Mongo- lia and Inner Mongolia of China, presents unique linguistic challenges due to its complex grammar, script, and historical evolution. In Mongolia, mod- ern Mongolian is written using the Cyrillic script, based on the Russian alphabet, while in Inner Mon- golia, China, the traditional Mongolian script, de- rived from the Sogdian-Uyghur script, is used. This paper focuses on modern Mongolian written in the Cyrillic script. Despite some efforts to include Mongolian in NLP research, there is still a signif- icant gap in understanding how well LLMs can handle Mongolian across various linguistic dimen- sions.\nThis research aims to fill the gap in Mongolian language support by systematically evaluating mod- ern LLMs' capabilities in processing Mongolian. Unlike existing task-oriented datasets, this study fo- cuses on models proven effective in high-resource languages. For Mongolian, we adopt a linguistic perspective, constructing a dataset based on lan- guage proficiency levels and previous LLM per- formance. Our dataset is organized into four hi- erarchical levels: syntax, semantics, knowledge, and reasoning. This structure allows for a detailed evaluation of model performance at different profi- ciency levels, providing deeper insights into their strengths and limitations.\nBy uncovering both the strengths and weak- nesses of current models, we aim to provide a"}, {"title": "2 Related Work", "content": "As large-scale models continued to evolve, more comprehensive and diverse open test datasets, such as CValues(Xu et al., 2023), were intro- duced. These datasets covered specialized knowl- edge (MMLU(Hendrycks et al., 2021)), logical rea- soning (GPQA(Rein et al., 2023)), mathematical ability (GSM8K(Cobbe et al., 2021)), coding skills (HumanEval(Chen et al., 2021)), and instruction- following capabilities (LiveBench(White et al., 2024)). These evaluation datasets can effectively test the model's various abilities in high-resource languages.\nIn contrast, Mongolian language processing has historically focused on downstream tasks such as text classification(Yang et al., 2022) and named entity recognition(Cheng et al., 2020), with limited evaluation for large models. This work significantly fills that gap by introducing comprehensive eval- uations tailored for Mongolian language models, addressing the deficiencies observed in previous studies."}, {"title": "3 MM-Eval", "content": "MM-Eval consists of four components: Syntax, Semantics, Knowledge, and Reasoning. The Syn- tax section contains 569 multiple-choice questions, the Semantics section includes 677 multiple-choice questions, the Knowledge section comprises 344 multiple-choice questions, and the Reasoning sec- tion features 250 math problems that require nu- merical answers. Figure 1 illustrates the overall"}, {"title": "3.1 Dual Capability Framework", "content": "We developed a dataset with a Dual Capability Framework to evaluate LLMs by dividing their ca- pabilities into language abilities and cognitive abili- ties. The cognitive abilities of a model are reflected through its primary training language, while lan- guage abilities vary depending on the language in question. To address this, our dataset is structured with a focus on these dual capabilities. Specifically, within language abilities, we distinguish between syntax and semantics. In terms of cognitive abil- ities, we further differentiate between knowledge and reasoning.\nThe language abilities section of our dataset eval- uates the model's proficiency in Mongolian, a low- resource language, reflecting its mastery of Mongo- lian independent of other language training data. In contrast, the cognitive abilities section assesses the model's overall cognitive capacity, which is influ- enced by all its training data but applied to Mongo- lian. This section highlights the alignment between Mongolian and the model's primary training lan- guage, showcasing how cognitive capabilities are transferred and manifested in Mongolian."}, {"title": "3.2 Data Collection", "content": "The primary source of our data for the language abilities section is Modern Mongolian Language Textbook I (Hou et al., 2017). We selected sen- tences from the dialogues and texts within this book to construct datasets for both syntax and seman- tics parts of evaluation. For the cognitive abilities section, the knowledge data is derived from two sources: a portion comes from the WebQSP(Yih et al., 2016) dataset, which includes information re- lated to geography and country-specific knowledge, and the other portion is generated using heuris- tic rules with ChatGPT API, followed by manual proofreading for accuracy. The reasoning data is sourced from the MGSM(Shi et al., 2023) dataset, focusing on cognitive reasoning tasks."}, {"title": "3.3 Data Processing", "content": "The content from the textbook is initially obtained through OCR to create an electronic text version. Subsequently, we extract dialogues, texts, and vo- cabulary from each lesson and perform data clean-"}, {"title": "3.4 Syntax Eval", "content": "For the syntax evaluation dataset, we selected sen- tences with three or more words from the extracted dialogue content in the textbook. After deduplica- tion, the sentences were split by spaces, and their word order was shuffled to generate three incor- rect options. These options were manually verified to ensure syntactic errors. Each original sentence, along with the three syntactically incorrect options, formed a multiple-choice question with four op- tions."}, {"title": "3.5 Semantics Eval", "content": "For the semantic evaluation dataset, we utilized sentences from both the dialogues and texts in the textbook, as well as the vocabulary lists. Each vocabulary list provides key terms for each lesson, along with part-of-speech information, making it well-suited for constructing semantic knowledge questions. Given the rich morphological variation in Mongolian, particularly with verbs, we limited our selection to nouns, pronouns, adjectives, and adverbs to avoid potential issues stemming from complex inflectional forms that could compromise the accuracy of the questions themselves.\nFirst, each sentence was split by spaces, and the resulting tokens were matched against the vocab- ulary list. Sentences without any matched words were discarded. For each sentence with a match, one of the matched words was randomly selected as the correct answer and removed from the sen- tence. Based on the part of speech of the removed word, three distractor words were selected from the vocabulary list. Specifically, nouns and pronouns were used as distractors for each other, while ad- jectives and adverbs were used similarly. The key criterion was that the distractor words should be plausible yet definitively incorrect as the answer. This process resulted in the construction of seman- tic evaluation questions."}, {"title": "3.6 Knowledge Eval", "content": "For the Knowledge Evaluation dataset, one of our data sources is the WebQSP dataset. However, due"}, {"title": "3.7 Reasoning Eval", "content": "For the Reasoning Evaluation dataset, the sources are the English and Chinese versions of the MGSM dataset, which contain identical content and an- swers. This dataset comprises application-style mathematical problems, each accompanied by nu- meric answers. We adopted a translation approach to convert both the English and Chinese versions of the questions into Mongolian. Given the complex"}, {"title": "4 Experiments", "content": "4.1 Setup\nWe deployed and inferred a local open-source model on a NVIDIA Tesla V100 (32GB) device. The inference parameters are: temperature=0, top-p=0.1, frequency penalty=1. The closed-source models are all invoked using APIs. The system prompt used for inference is: \"You are an AI assis- tant proficient in Mongolian.\". There are different user prompts for four different tasks, namely: Syn- tax:\"Select the grammatically correct sentence that follows the rules of Mongolian expression from the options below, and return the corresponding letter of the option (such as A, B, C, or D), do not return anything else.\"; Semantic: \"Complete the sentence to make it grammatically correct and meaningful in Mongolian. Return only the letter of the cor- rect option (A, B, C, or D), do not return anything else.\"; Knowledge:\"Based on the following ques- tion, choose the correct answer.Return only the letter of the correct option (A, B, C, or D), do not return anything else.\"; Reasoning:\"Calculate the result: Perform the calculations based on the given mathematical problem.\""}, {"title": "4.2 Models", "content": "We selected current mainstream open-source and closed-source models as the test models for our experimental dataset. The open-source models are: Qwen2-7B-Instruct(Yang et al., 2024), GLM4- 9b-chat(Zeng et al., 2023), Llama3.1-8B-Instruct. The closed-source models are: GPT-4-Turbo-04- 09, DeepseekV2.5(Dai et al., 2024). The input data is the question from the dataset. The evaluation metric is Accuracy."}, {"title": "4.3 Results", "content": "Table 1 presents the corresponding results of differ- ent models in four evaluation directions. Figure 2 shows the specific performance of different mod- els in a particular evaluation direction, with the bolded numbers representing the best results in that evaluation direction.\nFigure 2 presents the corresponding results of different models in four evaluation directions. Ta- ble 1 shows the specific performance of different models in a particular evaluation direction, with the bolded numbers representing the best results in that evaluation direction. The results reveal that GPT-4- Turbo-04-09 performs best in syntax (90.69%) and knowledge (80.52%) evaluations, while Qwen2- 7B-Instruct performs well in semantics (72.53%). However, all models struggle in reasoning, with the highest accuracy being 29.6%. These findings highlight the strengths and weaknesses of current LLMs in Mongolian, providing insights for future research and development."}, {"title": "5 Discussion", "content": "Our Dual Capability Framework categorizes LLM abilities into linguistic and cognitive capabilities, divided into syntax, semantics, knowledge, and rea- soning levels. Most modern LLMs, whether open- or closed-source, are trained on multilingual cor- pora, granting them some degree of multilingual competence. Studying their multilingual perfor- mance and its sources is essential for advancing LLMs.\nThis study examines LLM performance in Mon- golian across different levels. Our experiments show that while models perform well in basic lin- guistic tasks, they struggle with semantic under- standing and complex reasoning. Aligning knowl- edge content with the models' main training lan- guage could improve their performance in low- resource languages.\nMultilingual capability remains a critical re- search area. Our study suggests that while mod- els possess basic skills, performance varies by lan- guage and task. Future work should explore the mechanisms behind multilingual competence and ways to improve it.\nMM-Eval evaluates LLMs hierarchically but is limited by its single content source, only multiple- choice questions, and a narrow scope of logical reasoning tasks. Expanding these areas will enable more comprehensive evaluations."}]}