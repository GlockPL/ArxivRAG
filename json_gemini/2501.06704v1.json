{"title": "Fine-tuning ChatGPT for Automatic Scoring of Written Scientific Explanations in Chinese", "authors": ["Jie Yang", "Ehsan Latif", "Yuze He", "Xiaoming Zhai"], "abstract": "The development of explanations for scientific phenomena is crucial in science assessment. However, the scoring of students' written explanations is a challenging and resource-intensive process. Large language models (LLMs) have demonstrated the potential to address these challenges, particularly when the explanations are written in English, an alphabetic language. It remains unknown whether this approach can be applied to other logographic languages. This study thus explores the potential of fine-tuning ChatGPT, one advanced LLM, to automatically score scientific explanations written in Chinese. We collected and automatically scored student responses to seven scientific explanation tasks in Chinese, and examined the relationship between scoring accuracy and reasoning complexity with Kendall correlation. Finally, a qualitative analysis was conducted to explore how linguistic features influence scoring accuracy. The results indicate that through domain-specific adaptation, the fine-tuned ChatGPT can accurately score students' written explanations in Chinese. However, scoring accuracy correlates with reasoning complexity, showing a negative correlation for lower-level responses and a positive one for higher-level responses. The model tends to overrate complex reasoning for low-level responses with complex sentence structures and underrate high-level responses, using generalizing, summarizing, or simple causal reasoning. These opposing correlations are linked to different linguistic features. The comprehensiveness of student responses is often in tension with the simplicity and clarity of language structure in terms of scoring accuracy. For lower-level responses, simplicity and clarity are prioritized, leading to more accurate scores for simpler and shorter responses. For higher-level responses, comprehensiveness is prioritized, resulting in more accurate scores for long and information-rich responses. These findings demonstrate the effectiveness of LLMs in automatic scoring within a Chinese context and highlight the importance of considering linguistic features and reasoning complexity in developing and fine-tuning automatic scoring models for educational assessments.", "sections": [{"title": "1 Introduction", "content": "Constructing scientific explanations for real-world phenomena is fundamental in scientific inquiry and crucial for educational practices. This practice goes beyond descriptions of natural patterns and requires students to explain how or why a phenomenon occurs by seeking evidence and using scientific knowledge (Driver, Newton, & Osborne, 2000; McNeill & Krajcik, 2008). Such activities reflect students' scientific literacy and deepen their understanding of science (Yao & Guo, 2018). Consequently, national and regional science curriculum standards from countries such as the United States, Canada, Australia, and China emphasize \"constructing scientific explanations\" as a key scientific practice for students. Educational research and international large-scale assessments, such as PISA and NAEP, also employ scientific explanation as a primary means to evaluate students' science literacy (OECD, 2019; Zhai & Pellegrino, 2023). These tasks typically present students with a natural phenomenon or experimental results, requiring them to write coherent explanations. While these tasks offer a relatively comprehensive and deep assessment of students' abilities, scoring written explanations presents significant challenges. The writing's complex structure and diverse connotations demand considerable time and effort to score. Additionally, the intricate scoring criteria and the high costs associated with them can impede their practical application in everyday teaching.\nWith the development of artificial intelligence (AI), automatic scoring and developing learning environments for the automated analysis of learning have emerged as one of the most dynamic and cutting-edge directions in science education (Kubsch et al., 2022; Mislevy, Yan, Gobert, & Sao Pedro, 2020; Zhai, 2024). This technology alleviates the considerable time and effort teachers traditionally dedicate to scoring, enabling the provision of immediate feedback, which facilitates precise instructional decision-making and personalized learning experiences (Lamb, Hand, & Kavner, 2021; Riordan et al., 2020). This integration of technology represents a transformative shift in education, highlighting the extensive potential of automated scoring systems. Recently"}, {"title": "2 Automatic Scoring of Written Scientific Explanations", "content": "Scientific explanation involves the construction and communication of valid claims based on evidence and reasoning, typically requiring multiple reasoning steps. For example, explaining the causes and effects of climate change using scientific evidence and models or predicting the outcome of a chemical reaction based on the conservation of mass and the periodic table, demands complex reasoning (Krell, Vorholzer, & Nehring, 2022). Recognized as a crucial skill in science education, scientific explanation supports cognitive, affective, and social aspects of science learning, and helps"}, {"title": "3 Students' Writing of Science in Chinese", "content": "The research has found that writing scientific explanations in both Chinese and English requires complex mental processes expressed with words and sentences, but these processes differ greatly (Liao, Chen-Huei, Kuo, & Pai, 2012), reflecting the languish features of Chinese and English. Chinese is a widely used language worldwide, but it exhibits significant differences from English in scientific writing. Chinese writing tends to be more circular, visual, and synthetic, whereas English writing tends to adopt linear, rational, and analytical style (Chen & Zhang, 2021; SUN & TIAN, 2017). For instance, in describing locations, Chinese narratives typically start with the largest geographical entity and proceed to smaller ones, while English is the opposite(Y. Wang & Chen, 2013). Another difference is that English relies more on sentence structure, whereas Chinese writing conveys meanings more through word selection. English sentences typically feature an explicit subject-predicate structure, making word recognition easier, while Chinese sentences do not always adhere to this pattern clearly, making their sentence structures less explicit (Yang, Zhang, & Liang, 2018). So, implicit expression is considered a more advanced way of expression in Chinese writing, while explicit expression is preferred in English writing.\nThese linguistic distinctions were also reflected in the vocabulary and thinking patterns. Chinese people prefer comprehensive thinking, using words that reflect holistic concepts, in contrast to the more segmented, logical vocabulary prevalent in English writing. For example, the Chinese character \u624b(hand) is a holistic concept, fingers (\u6307), wrists (\u624b\u8155), and fists (\u62f3\u5934) are all parts of the hand. This holistic perspective is evident in phrases like\u201c\u624b\u201d\u4e0a\u6234\u7684\u8868(watch on his \u201cwrist\u201d) and\u201c\u624b\u201d\u4e0a\u6234\u7684\u6212\u6307(ring on his \"finger\") (Y. Wang & Chen, 2013). Moreover, the Chinese written language, influenced by both classical and vernacular forms, features a wealth of unique synonyms. For instance, \u773cand \u76eeboth refer to the organ of sight, but they convey slightly different meanings and are used differently. Additionally, some synonyms in Chinese may not have direct equivalents in other languages. For example, the Chinese word \u7b11encompasses different manners of laughing, but in English, there are many different words to express all kinds of ways to laugh, such as giggle (\u54af\u54af\u51fa\u58f0\u7b11),sneer(\u51b7\u7b11),smile(\u5fae\u7b11),chortle(\u54c8\u54c8\u5927\u7b11), snigger(\u6697\u7b11), all of these words are translated into Chinese should use the form like this: \u201cmanner +\u7b11\u201d (Chen & Zhang, 2021).\nThus, the automated scoring of written responses in Chinese could be more difficult than in English, especially for complex reasoning responses, due to the richness in vocabulary and sentence patterns in Chinese(X. Li et al., 2019)."}, {"title": "4 Methods", "content": "4.1 Participants\nParticipants were recruited through a stratified sampling method from schools in Beijing, ensuring a diverse representation of different school types, including municipal-level demonstration schools, district-level demonstration schools, and regular schools. School administrators communicated the study details to students and their guardians, ensuring that participants were explicitly informed about the research and voluntarily agreed to participate. This study selected 1,593 students in grades 8-12 as participants, stratified by school level, including 872 high school students, accounting for 54.7%, and 721 middle school students, accounting for 45.3%. Of the total population, 830 are boys, accounting for 52.1%, and 763 are girls, accounting for 47.9%.\n4.2 Instruments\nWe selected seven scientific explanation items from our established assessment bank to construct a scientific explanation ability test instrument. The selected items demonstrate high reliability and validity, and they have been extensively employed in prior studies, such as Yao and Guo (2018) and Zhai (2022). Each item in the test was presented as open-ended questions, beginning with a detailed description of a problem scenario and phenomena to provide students with the necessary context for their explanations. Students were asked to explain observed phenomena or predict phenomena based on the given information. Students need to organize their language to construct explanations to answer the questions.\nThe instrument includes a mix of scenarios: four out of 7 items are led by daily life scenarios, while the remaining three involve scientific lab contexts. The seven items include both direct explanation and prediction plus explanation: four items explicitly present phenomena that students are required to explain, while the other three require students to predict and explain potential phenomena that could arise from the described scenarios.\n4.3 Human Scoring\nThe first stage is preprocessing the collected student responses. In this study, students completed their answers on paper, so we first eliminated unclear student responses, and unrecognizable fonts or blank questionnaires. To facilitate further analysis, we transcribed the remaining student responses from Chinese handwritten texts into print text. The final number of samples retained and transcribed for each item is shown in Table 2.\nThe second stage employed human raters to score the transcribed text according to the scoring criteria . The scoring criteria include two parts: Holistic scoring rubrics and Analytic scoring rubrics. The Holistic scoring rubrics are consistent\n4.4 Data Analysis\n4.4.1 Fine-tuning ChatGPT\nLLMs like ChatGPT have shown great potential because they can handle many different uses in general. However, it is important to fine-tune these models using specialized multi-lingual (e.g., Chinese) datasets for specific areas like education. This is especially true when computers automatically check how well students answer questions. In this study, we explain how to finetune ChatGPT to score complex written responses of students. Our dataset (see Table 2) consists of student-written responses and human-expert grading for holistic and analytical classes with 5 scoring perspectives. Before fine-tuning the ChatGPT using OpenAI service, it is crucial to process the data based on the OpenAI guidelines as follows:\nData Cleaning and Privacy Assurance: The first stage was going over the dataset in great detail to find and eliminate any information that wasn't directly related to the study's goals. More crucially, this step was essential to protecting the pupils' privacy and confidentiality. All personally identifying information, including names and unique identifiers, was painstakingly deleted. This action was taken to preserve ethical norms, protect the privacy of each individual whose data was used in this study, and ensure that the data was clean.\nTokenization and Language Adaptation: The students' textual responses had to be carefully converted into a format that the model could understand, as the language structures of Chinese are somewhat complicated. This was a complex process of translating the rich subtleties of the Chinese language into a digital format that preserves the breadth and context of the student's remarks rather than just a technical tokenization approach. Special care was taken to ensure that the intricacies inherent in the Chinese language, such as idioms and implicit reasoning, were not overlooked during the tokenization process.\nData Standardization and Uploading: The dataset was subjected to a standardization process following tokenization and cleaning. This puts the data in an OpenAI platform-compatible structured format, usually JSON. Then, using the file upload API, the standardized files were safely uploaded to the OpenAI server. This was an important step since it took our carefully prepared dataset and put it into the AI processing domain, which started the process of turning raw data into meaningful conclusions.\nAnonymization for Ethical Compliance: The data was further anonymized after identifying information was eliminated. Random codes were used in place of any possible identifiers during this procedure. This step maintained a high quality of ethical research practice by ensuring that even if someone were to acquire the data, it would be impossible to relate it back to specific students.\nQuality Checks and Balancing: To ensure the dataset was representative and intact, it went through several quality checks. This involved distributing the dataset among various student answer durations and reasoning complexity levels. The goal was to produce a clean, well-structured dataset that accurately represented various student skills and cognitive processes.\nGiven the unique challenges of scoring scientific explanations in Chinese, fine-tuning ChatGPT was conducted with specific considerations:\nLoss Function: A regression-based loss function was used for continuous scores and categorical scores, and a classification loss was used. The intricate organization of scientific explanations, where scores frequently fall into complex categories or necessitate a scale that indicates differing degrees of accuracy and completeness, impacted this decision.\nLearning Rate: A cautiously low learning rate was our starting point. Because Chinese has distinct syntactic and semantic features that set it apart from the primarily English corpus, the model was initially trained on, this was important. Periodic assessments implemented gradual modifications in the learning rate to provide the best possible adaptation while retaining the model's original strengths.\nEpochs: It was vital to use multiple epochs, particularly given the complexity of scientific reasoning in the student explanations. Because reasoning in Chinese texts is implicit and context-dependent, it was important to control validation loss to avoid overfitting carefully and instead learn sophisticated reasoning patterns.\nBatch Size: To meet the computational requirements of processing Chinese characters and the intricate structures of scientific explanations, the batch size was optimized. The selected size allowed for effective processing without sacrificing the caliber of the training.\nData Augmentation: More data augmentation methods were applied to address the problem of multi-step reasoning in Chinese texts. To improve the model's comprehension and scoring of such responses, artificially generated samples that imitate complicated reasoning processes were incorporated.\nPost-fine-tuning, the model was evaluated and validated using a set distinct from the training data. The evaluation involved:\nMetrics: We used metrics such as Mean Absolute Error (MAE) or classification accuracy to gauge the model's scoring precision against human raters. This step is essential to assess the effectiveness of the fine-tuning process.\nComparison with Baseline: The performance of the fine-tuned model was compared with the original GPT-3.5-turbo model. This comparison helps quantify the benefits of domain-specific fine-tuning and understanding the improvements in the model's ability to score student responses.\n4.4.2 Alignment with Research Objectives:\nThe fine-tuning process was meticulously aligned with the research objectives and the questions raised in the introduction:\nScoring Accuracy: The fine-tuning procedure was designed to consider the subtleties of scientific language and reasoning in student writings to answer the first research question about the model's accuracy in evaluating written scientific explanations in Chinese. The model's performance was continuously assessed against various explanations to guarantee high accuracy. Scoring accuracy refers to the agreement between machine scoring and human scoring(Zhai et al., 2021). For example, an accuracy of 80% indicates that the machine scores align with human scores for 80% of the student samples.\nReasoning Complexity: Special focus was paid to fine-tuning the model to distinguish different levels of reasoning difficulty in response to the second and third questions regarding the effect of reasoning complexity on scoring accuracy. As part of this, the model was trained to recognize implicit reasoning processes and context-dependent explanations prevalent in Chinese texts.\n4.4.3 Correlation Analysis\nCorrelation analysis is a statistical method used to evaluate the strength and direction of the relationship between two or more variables. Kendall correlation analysis is a technique suited for determining significant associations between two ranked variables (Abdi, 2007). A Kendall correlation coefficient measures the strength of the relationship between two variables, ranging from -1 to 1. Table 4 shows a conventional approach to interpreting a correlation coefficient. The correlation coefficient's absolute value closer to 1 indicates a stronger linear relationship between the variables. An absolute value close to 0 suggests negligible correlation. Specifically, a coefficient less than 0.39 indicates a weak correlation; from 0.40 to 0.69, a moderate correlation; from 0.70 to 0.89, a strong correlation; and from 0.90 to 1.00, a very strong correlation (Schober, Boer, & Schwarte, 2018). This study used Kendall correlation analysis to understand the extent to which the reasoning complexity of student writing in Chinese correlates with the scoring accuracy of Fine-tuned ChatGPT.\nFirstly, we coded the reasoning complexity of students' responses using the criteria adapted from Cabello, Moreira, and Morales (2021); Kwon and Lawson (2000). Responses with low reasoning complexity mainly involve generalizing, summarizing, or simple causal reasoning about entities or elements perceptible by the senses. Medium reasoning complexity responses include a few variables elements, or processes beyond the sensory experience, attempting to express causality, nonetheless, not yet at a level that uses the parts of a theory to represent causal processes or ongoing mechanisms. Responses with high reasoning complexity engage with theories or abstract concepts to explain non-visible or non-perceptible elements, processes, or mechanisms, reasoning at this level is at a more sophisticated stage than in the previous levels. It can clarify the covariation relationship among multiple variables, and apply hypothesis deduction, conservation thinking, proportional reasoning, variable control, probability reasoning, and correlation reasoning to construct causal chains or explain running mechanisms. Table 5 shows the criteria for reasoning complexity and Table 6 outlines the distribution of students' responses across various reasoning complexity levels in seven items.\nSecondly, we chose the Kendall correlation analysis to assess the correlation between the reasoning complexity of student writing in Chinese and the scoring accuracy of Fine-tuned ChatGPT. We controlled for students' explanation performance during this process, thereby isolating this confounding variable. To control students' performance, we created two performance groups: those scoring 0 were placed in the lower-level group, and those scoring 1 in the higher-level group. Then, we constructed contingency tables to represent the observed frequencies of each combination of reasoning complexity and scoring accuracy categories. Then, we calculated the number of concordant pairs(nc) and the number of discordant pairs (nd), the number of samples (n), and the minimum value between rows and columns. Following this, we calculated Kendall's Tau-c using the corresponding formula:\nTc = 2(nc-nd)/ sqrt(n2-m)\nTake Holistic scoring in item 1 as an example. Out of 242 testing samples, 97 scored 0, forming the lower-level group, and 145 scored 1, categorized as the higher-level\n4.4.4 Qualitative analysis of response characteristics\nAlthough the classification process of automatic scoring is usually not transparent, the scoring is based on all information provided. In this study, the input information came from students' writing responses. Theoretically, all response characteristics could be factors that account for automatic scoring accuracy. Thus, to answer Research Question 3, we employed a qualitative manual matching approach to identify characteristics that might account for scoring differences. We analyzed both machine-misscored responses and correctly scored responses, focusing on their distinct languish characteristics following three steps, seeing Figure 2.\nThe first step was categorization, during which researchers segregated the higher-level group and lower-level group into four subgroups based on scoring accuracy: \"Higher level and Misscored subgroup (HM subgroup),\" \"Higher level and Correctly scored subgroup (HC subgroup),\u201d\u201cLower level and Misscored subgroup (LM subgroup),\" and \"Lower level and Correctly scored subgroup (LC subgroup).\" We then selected all student responses from items where reasoning complexity significantly affected scoring accuracy and categorized them into four subgroups. The second"}, {"title": "5 Results", "content": "To evaluate the performance of the finetuned ChatGPT on a Chinese dataset, we conducted three sets of experiments to 1) assess the automatic scoring accuracy, 2) examine the correlation between scoring accuracy and reasoning complexity, and 3) analyze the characteristics of students' responses.\n5.1 Scoring Accuracy of Fine-tuned ChatGPT for Written Responses in Chinese\nThe accuracy of the finetuned ChatGPT models was examined for several datasets (Item 1 to Item 7; See Figure 3). The findings indicate that all fine-tuned models achieved scoring accuracy above 0.75 for all scoring categories, though varying by item. Specifically, with scores continuously above 90%, Items 6 and 7 showed the highest overall accuracy. Item 3 and Item 4, on the other hand, showed worse performance, with all of the scores being below 85%.\nWe first examined the model's capacity to accurately assign holistic scores across datasets and found a notable variance, with accuracies ranging from 80% (Items 3 and 4) to 94% (Item 6). Further analyses revealed that, for the Data element, most items scored in the mid-80s percentage accuracy, indicating a stable performance, except item 6, which had an accuracy of 98%. In contrast, the Theory element yielded an accuracy ranging from 81% (Item 4) to 98% (Item 6), implying a relative inconsistency of scoring. Different from the two prior elements, the element Reasoning yielded the greatest variation, with Item 4 receiving the lowest accuracy (75%) and Item 6 achieving the highest (96%). Among all the elements, the Phenomenon element achieved the highest average accuracy, especially for Item 5, which achieved a remarkable 97%.\nWe also found varying accuracy among the four elements for each item. For example, Theory element on Item 1 had a high accuracy of 94%, whilst Phenomenon element yielded an accuracy of 86%. Similarly, Item 4 received 75% accuracy for Reasoning element, which is significantly lower compared to accuracies on other elements.\nThis fluctuation highlights the model's inconsistent reaction to distinct elements of scientific explanations.\nThe findings show that although the finetuned ChatGPT models exhibit strong performance for certain tasks (e.g., Items 6 and 7), their accuracy varies depending on the scored element and dataset.\n5.2 Association between Reasoning Complexity and ChatGPT Scoring Accuracy\nA Kendall correlation coefficient was calculated to assess the relationship between the reasoning complexity of students' responses and the scoring accuracy for each scoring element across seven items. Considering that students' explanation performance is a potential confounding variable affecting both reasoning complexity and scoring accuracy, we controlled students' performance to more accurately isolate the correlation. Specifically, we grouped students into lower- and higher-level groups based on their overall performance and then analyzed the relationships by students' performance level.\n5.2.1 The lower-level group\nThe lower-level group received 0 for their responses, the correlation between reasoning complexity and scoring accuracy for the lower-level group responses is shown in Table 8, we calculated a total of 30 Kendall correlations to explore the relationships between reasoning complexity and scoring accuracy across different scoring elements for seven items. Among these correlations, 29 correlations were negative, with 28 being statistically significant (p <0.05 or p <0.1). The average correlation coefficient across all items and scores was -0.31 (SD = 0.17). These results indicate that for lower-level performing students, the higher the complexity of reasoning in their explanations, the lower the scoring accuracy yielded by ChatGPT models.\n5.2.2 The higher-level group\nThe higher-level group received 1 for their responses, we conducted the same total of 30 Kendall correlation analyses to assess the relationship between reasoning complexity and scoring accuracy for the higher-level group responses, Table 9 shows the results. Among these, 26 out of 30 correlations were positive, with 10 being significant (p <0.05 or p <0.1). The average value of these significant correlation coefficients was 0.19 (SD=0.11). These results indicate that for higher-level students, the higher the reasoning complexity is, the higher of scoring accuracy yielded by ChatGPT.\n5.3 The linguistic features of students' responses\nIn this section, we present student's writing responses' linguistic features both at the word and sentence levels, the features may account for the machine mislabels. Since the most basic written unit in Chinese is a character rather than a word, the frequency calculations in this study are based on the frequency of Chinese characters instead of the frequency of words.\n5.3.1 The characteristics of the lower-level group\nOverall, responses from the lower-level group received 0 for their responses and demonstrated a low frequency of misspellings, polysemous words, synonyms, and pronouns, tend to use academic terms, and describe phenomena directly at the word level, rarely use passive constructions at the sentence level. Additionally, we also obtain some distinct patterns between the correctly scored subgroup and the misscored subgroup.\nThe lower level and correctly scored subgroup. At the word level, the written responses from the Lower level and Correctly scored subgroup (LC subgroup) used fewer technical terms, with a higher proportion of everyday language, and averaged only 20 Chinese characters, significantly shorter than the 40 Chinese characters found in the Lower level and Misscored subgroup (LM subgroup). At the sentence level, the LC subgroup exhibited several distinct characteristics compared to the LM subgroup: (1) they contain fewer and shorter sentences, typically with simple structures like subject-verb-object; (2) the syntax was relatively straightforward, using more simple and compound sentences; (3) noun and verb phrases were more straightforward, and prepositional phrases were less frequent, making the descriptions more comprehensible.\nTake a typical LC subgroup response in item 7 as an example. The response was \"There are gaps between molecules that allow for transmission (No. 194).\" This response only describes one phenomenon and fails to incorporate theories related to molecular thermal motion. Therefore, it receives a human score of 0 in the Theory element. It acknowledges the interaction between molecules at the microscopic level but lacks reasoning chains to show why the gaps between molecules lead to transmission, resulting in a lower reasoning complexity.\nMoreover, this response is short, containing only ten Chinese characters, and uses everyday terms like \"gaps\" and \"transmission\". Its structure is simple, with just one subject-verb-object sentence, and lacks complex clauses or modifiers. The phrases, such as \u201cgaps between molecules\u201d were straightforward and clear, without complex technical terms or multilayered modifications. This direct and easily understandable structure and terms aided in conveying the information more clearly. Due to the simplicity and directness, the machine scoring system can more easily parse this type of response. The simple sentence structure reduces the likelihood of parsing errors and makes information extraction straightforward and efficient. This concise and clear expression style helps the machine algorithm accurately evaluate the student's response, allowing the machine scoring to achieve high accuracy in this context, as the scoring criteria align more easily with the straightforward information provided by the student.\nThe lower level and misscored subgroup. Unlike the LC subgroup, the written responses by the Lower level and Misscored subgroup (LM subgroup) tend to be longer, averaging about 40 Chinese characters. These responses frequently use technical terms and professional vocabulary but not always accurately, especially when describing physical processes. At the sentence level, LM subgroup responses typically contain multiple sentences, providing detailed explanations. Their sentence structures are complex, with varying lengths and multiple modifying elements. They frequently used complex syntactic structures, including multiple clauses and relative clauses. Additionally, these responses feature complex and technical noun phrases and verb phrases, and they use prepositional phrases frequently, particularly when describing physical locations and states. These linguistic characteristics often result in higher reasoning complexity. However, they may also complicate the understanding and parsing of these responses, potentially challenging the scoring accuracy of ChatGPT.\nFor instance, the response to item 7 is \"The principle of diffusion is that molecules are constantly in random motion. The higher the temperature, the greater the internal energy, the internal energy is converted into kinetic energy and released, causing vigorous molecular motion, increased gaps, reduced forces, and greater diffusion (No. 59).\" The response displays a misconception- \u201cinternal energy is converted into kinetic energy\", which led to a score of 0 in the Theory element. However, it surpasses sensory experience by using a microscopic molecular thermal motion model and forming an evidence-to-theory-to-phenomenon reasoning chain, indicating a high reasoning complexity level.\nThis response exceeds 60 Chinese characters and uses numerous technical terms such as \"kinetic energy\" \"internal energy\" and \"force\". Regarding sentence structure, the response contains a long sentence segmented by commas into various clauses and phrases and contains information equivalent to multiple shorter sentences. Each clause carries a complete idea. For instance, \"The higher the temperature, the greater the internal energy, the internal energy is converted into kinetic energy and released\" shows a progressive structure, adding complexity and information. Besides, the syntax is relatively complex, employing causal clauses and explanatory phrases. For example, \"The principle of diffusion is\" introduces a definition followed by several clauses and phrases attempting to explain the mechanism of diffusion. This use of multiple causal relationships and parallel structures enhances the complexity of the sentence. It also employs multiple complex noun and verb phrases, such as \u201cmolecules are constantly in random motion\u201d and \u201cinternal energy is converted into kinetic energy and released,\u201d which are both scientifically rigorous and descriptive of specific physical processes, demonstrating high technicality and professionalism in language use.\nThe sentence's structure is intricate, involving several physical processes and transformations, potentially making it difficult for the ChatGPT to accurately capture the logical relationships and scientific concepts within each clause. This complexity necessitates a scoring algorithm capable of high semantic understanding and precise handling of scientific terminology and logical structures, which may result in less accurate scoring and lead to a high risk of misscored, where students with lower levels of performance may receive higher scores.\n5.3.2 The characteristics of the Higher-level group\nCompared to lower-performing responses, higher-performing ones included scientific terms more frequently. They used polysemous words, pronouns, and synonyms less often. Additionally, distinctive patterns emerged between the Higher level and Correctly scored (HC) subgroup and the Higher level and Misscored (HM) subgroup.\nThe higher level and correctly scored subgroup. At the word level, responses from the HC subgroup averaged about 46 Chinese characters, which had 20 more characters compared with the HM subgroup. Additionally, the HC subgroup used mathematical symbols or formulas more frequently than the HM subgroup. The longer responses and the use of symbols/formulas offered more detailed and precise explanations. At the sentence level, the HC subgroup's responses exhibited several features distinct from those of the HM subgroup: (1) most responses of the HC subgroup included multiple sentences with complex syntactic structures, incorporating multiple clauses and modifiers; (2) They often employed numerous noun phrases to explain phenomena, such as \"rate of change of magnetic flux\"; (3) Verb structures were more complex and varied, such as \u201cpartially converted the mechanical energy of the magnet into thermal energy\"; (4) Each student response contained a higher number of sentences, typically ranging from 5 to 10; (5) Prepositional phrases were also more prevalent, such as \u201centering the coil\u201d and \u201cduring the free fall of the magnet\" clearly indicating action states.\nFor instance, the response to item 3, \"As the magnet enters the coil, due to the changing magnetic field inside the coil induces an electric current, heat is generated. According to the equation Ep = Ek + Q, as Q is generated, Ek decreases, resulting in a decrease in \u25b3V, which means the magnet's motion is hindered (No. 43),\" accurately applies the principle of energy conservation and correctly uses the equation, thus receiving a score of 1 on the Theory dimension. It effectively employs non-visible theories and equations to link multiple related variables, explaining the mechanism of the magnet's fall. Therefore, it demonstrates a higher reasoning complexity level.\nThis response exceeded 70 Chinese characters, used numerous technical terms, such as \"electric current\" and \"magnetic field\", and employed symbols like \"Q\", \"Ek\", and \"AV\", as well as related equations to precisely describe the physical phenomena and processes, reducing ambiguity. It also includes multiple prepositional phrases, such as \"due to the change in the magnetic field inside the coil\u201d and \u201cas Q is generated,\" providing temporal and causal background information for a more complete description. Although it is a long sentence, it uses commas to separate different clauses and phrases, maintaining a strict logical progression. Starting from the fact that \"the magnet enters the coil,\" it connects through the theory \"Ep = Ek + Q,\" and finally concludes with \"the magnet's motion is hindered,\u201d thoroughly explaining the phenomenon of the magnet's fall. This complete and precise expression style reduces the likelihood of parsing errors and helps the machine algorithm accurately evaluate the student's response, allowing the machine scoring to achieve high accuracy in this context, as the machine has more accurate information provided by the student to compare with the scoring criteria.\nThe Higher level and Misscored subgroup. Responses from the HM subgroup averaged about 26 Chinese characters. These responses seldom use mathematical symbols and formulas, relying mainly on textual explanations. The syntax was not complex, consisting mostly of simple and short sentences, which made them easy to understand. The noun phrases used were limited and simple, such as \"change in a magnetic field\" and \"The work done by the pump on the gas\". The verb structures are relatively straightforward, like \"induces an electric current\" and \"generate heat\" .\nCompared with the HC subgroup, each response in the HM subgroup contained fewer sentences, typically ranging from 1 to 3, and prepositional phrases were fewer and simpler, such as \"doing work on the gas\" resulting in an unclear representation of physical states.\nTaking the HM subgroup response to item 3 as an example, the response, \u201cThe coil will exert resistance on the magnet because of energy conservation (No. 101),\u201d employs the principle of energy conservation to provide a concise explanation of the phenomenon, earning a score of 1 on the theory element. However, it simply links energy conservation with the conclusion of resistance, offering some causal analyses but failing to explain the mechanism of the magnet's fall. Therefore, it demonstrates a medium reasoning complexity.\nThis response contains only 19 Chinese characters and uses mostly non-technical terms, such as", "repulsive force\" or": "epulsion,"}, {"title": "6 Discussions", "content": "6.1 Fine-tuned ChatGPT Can Automatic Score Student Written Explanations in Chinese, though Less Accurate than Those Written in English\nAutomatic scoring has long been regarded as essential for effective assessment practices in education. Various automatic scoring algorithms and strategies have been employed to achieve high scoring accuracy (Fiacco, Jiang, Adamson, & Ros\u00e9, 2022; Gombert et al., 2023; Zhai, Yin, Pellegrino, Haudek, & Shi, 2020) with varying degrees of success (Zhai et al., 2021). Prior research has demonstrated the potential of LLMs in automatically scoring student written responses (Liu, He, Liu, Liu, & Zhai, 2023; Riordan et al., 2020). In our study, we examined the accuracy of Fine-tuned ChatGPT in scoring scientific explanations in Chinese. The results revealed high agreements between human and machine scoring, consistent with findings from English contexts, such as Latif and Zhai (2024). Our study offers evidence for its usability across different linguistic and cultural environments, highlighting its potential for broader applications. We also found that although the finetuned ChatGPT models exhibit strong performance for certain tasks, their accuracy varies depending on the scored element and dataset. These differences may be due to the scoring criteria' intricacy, the student answers' characteristics in each dataset, or the model's intrinsic incapacity to comprehend and assess particular kinds of scientific explanations. More research into these areas might yield more insightful conclusions and possibly direct advancements in techniques for fine-tuning and training models.\nAdditionally, we observed that the accuracy in a Chinese context is not as high as in English contexts (Latif & Zhai, 2024), despite using similar scoring rubrics and sample sizes. This discrepancy is likely due to the unique linguistic characteristics of Chinese, such as its implicit, circular, visual, and synthetic reasoning styles, which are consistent with findings from (C. Wang et al., 2021). These identified characteristics pose challenges for current language models (Dougrez-Lewis et al., 2024; Wu et al., 2023). This study also found noticeable differences in accuracy among various scores for each item, even when inputting the same written responses. However, unlike previous research by C. Wang et al. (2021), we did not indicate higher accuracy in analytic scoring compared to holistic methods. We believe this is due to the clarity and specificity of our scoring rubrics, which minimized differences in scoring precision. This finding suggests that the primary determinant of scoring accuracy by machines is the quality of human judgment used in model training and rubric design, rather than the type of scoring rubric employed (Beggrow, Ha, Nehm, Pearl, & Boone, 2014).Our results demonstrate significant potential for practical application in classroom teaching. For teachers, the model can assist in automatically scoring students' written responses in Chinese and providing immediate feedback, thereby reducing grading workload and enabling more efficient assessment practices. This immediate feedback also helps teachers make informed instructional decisions, such as identifying common misconceptions and adjusting teaching strategies to meet students' needs better(Adiguzel, Kaya, & Cansu, 2023). For students, timely feedback on their written responses can support their personalized learning by reinforcing correct concepts and promptly addressing errors(Limna, Jakwatanatham, Siripipattanakul, Kaewpuang, & Sriboon"}]}