{"title": "Enhancing Reasoning to Adapt Large Language Models for Domain-Specific Applications", "authors": ["Bo Wen", "Xin Zhang"], "abstract": "This paper presents SOLOMON, a novel Neuro-inspired Large Language Model (LLM) Reasoning Network architecture that enhances the adaptability of foundation models for domain-specific applications. Through a case study in semiconductor layout design, we demonstrate how SOLOMON enables swift adaptation of general-purpose LLMs to specialized tasks by leveraging Prompt Engineering and In-Context Learning techniques. Our experiments reveal the challenges LLMs face in spatial reasoning and applying domain knowledge to practical problems. Results show that SOLOMON instances significantly outperform their baseline LLM counterparts and achieve performance comparable to state-of-the-art reasoning model, ol-preview. We discuss future research directions for developing more adaptive AI systems that can continually learn, adapt, and evolve in response to new information and changing requirements.", "sections": [{"title": "1 Introduction", "content": "The rapid advancements in large language models (LLMs) have revolutionized various aspects of artificial intelligence, enabling them to understand and generate human-like text with remarkable proficiency. However, adapting these general-purpose models to domain-specific tasks remains a significant challenge. In this paper, we introduce SOLOMON (System for Optimizing Language Outputs through Multi-agent Oversight Networks), a Neuro-inspired LLM Reasoning Network Architecture that leverages Prompt Engineering and In-Context Learning techniques, and demonstrate how SOLOMON can effectively adapt from its original design purpose in medical applications to a new domain: semiconductor layout design. Section 2 presents the SOLOMON architecture and highlights its design principles that contribute to enhanced adaptability.\nTo provide context for our experiment, we first examine how a designer might attempt to use ChatGPT (with GPT-40 mode) for a via connection design task in section 3. This exploration reveals a critical limitation: while LLMs can accurately recite textbook definitions of domain-specific concepts, they struggle to extract and apply expert knowledge to solve practical tasks. Human needs to translate high-level concepts into specific geometric requirements, which the LLM can then use to generate code for drawing shapes. This highlights the key challenge in adapting LLMs for domain-specific applications: their limited reasoning capabilities.\nIn section 4, we developed a set of 25 tasks ranging from basic geometric shapes to complex semiconductor structures, to evaluate our SOLOMON architecture against five different LLMs. These tasks assess spatial reasoning capabilities and adaptability across various complexity levels. Through these experiments, we demonstrate SOLOMON's superior performance compared to standalone LLMs, and reaching the level of state-of-the-art reasoning models like O1-preview."}, {"title": "2 Neuro-inspired LLM Reasoning Network Architecture", "content": "SOLOMON's architecture (Fig. 1) is inspired by two neuro-inspired theories: Brain-like AGI Byrnes [2022] and the Free Energy Principle (FEP) Parr et al. [2022]. The former inspired us to use a pool of thoughts from multiple LLMs to discover the best reasoning plan. From the latter, we applied the FEP's main claim, human attention focuses on minimizing the differences between goals and perceptions, to select relevant information and avoid common pitfalls. The key components of SOLOMON are:\nThought Generators: A diverse pool of LLMs generating thoughts for the target task. This component forms an efficient parallel search engine through the Tree-of-Thoughts Yao et al. [2023], Zhang et al. [2024], Besta et al. [2024a,b] and functions as an adaptive RAG system for the Thought Assessor. By pooling thoughts from multiple LLMs with distinct knowledge bases and reasoning abilities, it provides a more flexible and effective mechanism for sampling diverse ideas compared to common embedding-based RAG. This approach also mitigates biases inherent in single LLM knowledge bases. Noted that the individual LLMs in the Thought Generators can be further enhanced with proprietary knowledge through classic RAG techniques.\nThought Assessor: An LLM-based system that analyzes the proposed \"Thoughts\" to generate a refined output. It conducts in-context learning on the Thought Generators' output and follows the Free Energy Principle for goal-oriented assessments on consensus and differences. This approach enhances the LLM-as-a-Judge method Zheng et al. [2023], Lin and Chen [2023], enabling self-reflection Ji et al. [2023] and guarding against hallucinations Guerreiro et al. [2023], thus improving AI safety and reliability.\nSteering Subsystem: A human-operated component that controls the attention of the Thought Generators and Thought Assessor. It uses Prompt Engineering to modify the goals of other components, enabling swift adaptation to different domain requirements through goal-directed exploration of the search space. This enhances the system's versatility across various applications by simply adjusting the attention focus.\nThis architecture offers significant advantages over traditional fine-tuning approaches, eliminating the need for recurrent fine-tuning associated with upgrading underlying LLMs or updating domain-specific knowledge. Basing on Prompt Engineering techniques, SOLOMON enables building more flexible AI systems capable of addressing diverse specialized contexts."}, {"title": "3 Problem: Spatial Reasoning and Domain Knowledge Application", "content": "Layout design in semiconductor processes requires not only generating correct basic geometric shapes but also spatial reasoning to create proper \"layouts\" that meet specific requirements. Via connections, which create 3D electrical pathways between different chip layers, exemplify this challenge. While seemingly simple, typically consisting of circular vias and rectangular metal connections, they demand precise positioning and sizing to ensure no short or open circuits when building the 2D layout into a 3D structure.\nThis finding highlights a critical insight: to enhance the adaptability of LLM-based AI systems, simply increasing the model size to memorize more information is insufficient; instead, we should prioritize developing LLMs' reasoning capacity to effectively utilize their knowledge in practical problem-solving scenarios."}, {"title": "4 SOLOMON Performance and Comparison", "content": "To evaluate SOLOMON's effectiveness in enhancing spatial reasoning for semiconductor layout design, we created a dataset of 25 layout design tasks. These tasks were categorized into four groups: Basic Shapes 1 and 2 included simple geometric shapes such as circles, polygons and text, which serve as the building blocks for more complex layouts. The Advanced Shapes category involved more intricate designs, such as serpentine and spirals, to test the models' ability to handle complex geometries. Finally, the Complex Structures category included tasks that required the composition of multiple shapes to form functional layouts, such as a Dense Layer Diode (DLD) chip, MicrofluidicChip, and the ViaConnection test case. These tasks were designed to benchmark the AI systems' capability in generating layouts that are representative of real-world semiconductor design needs.\nWe provided task requirements with a system prompt asking the LLMs to use Chain-of-Thought to analyze the task and write Python code to create a GDSII output. The evaluation process involved running the generated code to produce GDSII files, which were then converted to PNG images. Human evaluators categorized the output into five categories: correct, scaling error, partially correct, shape error, and runtime error. Five LLMs (GPT-40, Claude-3.5-Sonnet, Llama-3.1-70B, Llama-3.1-405B, and o1-preview) were used for the baseline experiment, with each task run 5 times per model. (See Appendix A.2 for details of prompts and example outputs.)\nTo evaluate SOLOMON, we utilized 20 thoughts generated by GPT-40, Claude, and two Llama-3.1 models from the baseline experiment. We created four SOLOMON instances, each using one of these LLMs as a Thought Assessor, excluding o1-preview which served as our benchmark for state-of-the-art reasoning performance.\nThe results demonstrate that the SOLOMON architecture significantly improves the performance of all four LLMs compared to their baseline. The most notable improvements are observed in the reduction of runtime errors, which can be attributed to the Thought Assessor seeing the error log of previous generated code and knowing what to avoid. This aligns with the design principle of the hierarchical, self-reflection mechanism, which aims to mitigate individual LLM's hallucination and blind-spot.\nThe second most notable problem in the baseline is scaling errors. We intentionally requested basic shapes to be drawn in millimeters to challenge the LLMs: they need to recall that the default unit in the gdspy library is micrometers. Sometimes LLMs simply failed to notice this and produced incorrect results. Additionally, each LLM seems to have bias when they hallucinate the default unit: Llama models perfers millimeters, Claude models sometimes recalls nanometers, and GPT-40 occasionally used meters. This issue was particularly problematic for Llama-3 models, when sometimes it correctly recalled the micrometer default but would insist that the user was wrong to request millimeters and proceed to draw without scaling, justifying it with comments like \u201cnot mm, as the GDSII format is in micrometers.\" Such \"arrogant\" behavior and misalignment with simple instructions could be harmful for deploying LLMs as fully autonomous AI agents. A recent Nature paper Zhou et al. [2024] has also discussed similar observations.\nThe SOLOMON architecture improves performance across all models, including Llama-3. By incorporating diverse perspectives, it reduces stubbornness and increases accuracy. SOLOMON instances also show enhanced ability to handle shape errors and partial correctness issues, as the Thought Assessor can identify and correct errors related to arithmetic miscalculations or incorrect relative positioning of shapes.\nComparing SOLOMON instances with o1-preview, we find that SOLOMON achieves comparable or superior results. All SOLOMON instances outperformed 01-preview in Basic Shape 1 categories, with the Claude-based SOLOMON surpassing o1-preview in 3 categories overall.\nInterestingly, Llama-3 based SOLOMON instances also received significant performance boost, eventhough they don't receive the image inputs, suggesting that the thought assessment mechanism indeed works for more than just image understanding. Additionally, insufficient information linking images to corresponding code and error logs sometimes resulted in misinterpretation for GPT-40 and Claude.\nAnalysis of SOLOMON errors reveals that performance depends heavily on the quality and consensus of initial thoughts. Tasks with ambiguous requirements often leads to significant disagreement among initial thoughts, leading to confusion of Thought Assessor and degraded performance."}, {"title": "5 Conclusion and Future Work", "content": "The introduction of the SOLOMON architecture significantly improved performance in semiconductor layout design tasks, particularly in reducing runtime errors and enhancing spatial reasoning capabilities. Our experiments demonstrated that SOLOMON instances outperformed their baseline LLM counterparts across various task categories, with some instances even surpassing the state-of-the-art 01-preview model in certain areas. This improvement validates the effectiveness of our neuro-inspired approach in enhancing LLMs' adaptability to domain-specific applications.\nHowever, challenges remain in translating domain knowledge into practical design requirements. Our via connection experiment revealed that while LLMs can accurately recite textbook definitions of domain-specific concepts, they struggle to extract and apply expert knowledge to solve practical tasks. Investigating the potential of stacking multiple SOLOMON layers to form a hierarchical reasoning model capable of recalling and reasoning with domain knowledge for task-solving is one of our major future focus.\nOther future research directions include: (1) Developing more comprehensive benchmark datasets for evaluating AI systems in layout design tasks. (2) Improving the linking between multimodal inputs (images and corresponding code+error) in the thoughts to enhance the Thought Assessor's interpretation abilities. (3) Exploring SOLOMON's performance when initial thoughts are of lower quality, and developing goal-oriented iterative learning mechanisms to improve thought quality through feedback loops. (4) Applying the SOLOMON architecture to a broader range of domain-specific tasks, such as power grid design and financial modeling.\nIn conclusion, while our results demonstrate the promise of LLMs as layout design copilots, further advancements in reasoning capabilities and domain knowledge application are necessary for their effective integration into semiconductor design processes and other specialized domains. The SOLOMON architecture represents a significant step towards creating more adaptive and capable AI systems for complex, domain-specific applications."}, {"title": "A Appendix", "content": "A.1 Open Source Code and Dataset\nTo ensure reproducibility and facilitate further research, we release the complete benchmark dataset of 25 tasks and LLM-calling code under the Apache 2.0 license at https://github.com/wenboown/ generative-ai-for-semiconductor-physical-design. This repository includes 5 runs of results (LLM answers, Python code, error logs, and PNGs) for each task in the baseline experiment for reproducibility. While the SOLOMON code is proprietary, its output results are included.\nAll LLM experiments were conducted by calling APIs: GPT-40 via OpenAI, Claude via Anthropic, and Llama 3s via IBM Watsonx. Total API costs are about $50 including re-runs of failed tasks and iterative testings. The local code (for calling APIs, collecting responses and saving to disk, running the LLM-generated code, and analysis) was run on a virtual machine with RHEL 8.0, equipped with a 32-core CPU and 256GB of memory.\nThe complete dataset, including prompts, ground truths, and LLM outputs, is available in our repository. This contains all materials needed to reproduce our baseline experiments and conduct further research.\nA.2 Task Prompts and Baseline LLM Performance\nThe system prompt used for baseline experiment (thought generating) for all tasks was as follows:\nYou are an expert Python developer specialized in generating layout designs in GDS (GDSII) format. Your task is to assist the user in creating Python code that accurately draws layout designs while being mindful of the geometric relationships and layout accuracy.\nWrite down your thinking step by step before you start coding:\n1. Always start by understanding the overall design requirements provided by the user.\n2. Break down the design into smaller components and define each geometric shape with precise coordinates.\n3. Ensure that all shapes and elements maintain their correct geometric relationships, such as alignment, spacing, and proportional dimensions.\n4. Validate each step of the design process to avoid errors and maintain accuracy.\nUse the 'gdspy' library to generate the GDS layout:\n1. Parse the user's design specifications.\n2. Define the library and cell for the GDS layout.\n3. Create each geometric element (e.g., rectangles, polygons) with precise coordinates.\n4. Ensure elements are placed correctly and maintain their intended relationships.\n5. Save the design to a GDS file.\nBe meticulous in your approach, and always consider the geometric relationships and layout accuracy in every step of the design process.\nHere are the complete list of all 25 task prompts (questions):\nTo aid human evaluators, we organized task prompts, ground truth images, and LLM output images from different runs in a tabular format. This presentation offers a clear view of various models' performance in generating GDSII layouts, enabling easy comparison between expected results and actual outputs from different LLMs and the SOLOMON system.\nA.3 Errors in Baseline Experiment\nA.3.1 Scaling Errors\nThe default unit in the gdspy library is micrometers. We requested basic shapes to be drawn in millimeters to test whether LLMs could correctly handle this unit conversion. All LLMs struggled to various degrees:\n(a) Some LLMs failed to pay attention to the requested unit (millimeters) and did not perform the necessary scaling.\n(b) In some cases, LLMs paid attention to the requested unit but made incorrect assumptions about gdspy's default unit. We observed biased hallucinations: Llama models tended to assume millimeters,\nA.3.2 Shape Errors\nIncorrect shapes often resulted from LLMs making basic arithmetic errors. For instance, in the \"Hexagon\" task, Llama-3.1-405B once used an internal angle of 120 degrees, producing a triangle instead of a hexagon. However, in other runs, it correctly calculated the angle based on the number of edges. Many of these errors can be mitigated through Chain-of-Thought (CoT) prompting, which encourages the model to do calculations step-by-step.\nA.3.3 Runtime Errors\nThis section provides a detailed breakdown of the errors encountered during the baseline experiment for each LLM. The most frequent error across all models was AttributeError: module 'gdspy' has no attribute 'LayoutViewer', occurring 26 times (59.09%) with GPT-40 and 33 times (61.11%) with Claude-3.5-Sonnet. This error was less common in other models, appearing only once each for Llama-3.1-70B and o1-preview, and not at all for Llama-3.1-405B.\nThe prevalence of this error indicates that GPT-40 and Claude-3.5-Sonnet attempted to provide GUI output, which was unavailable in the runtime environment. However, this issue stems from a lack of specification about the runtime environment in the prompt, rather than being entirely the LLMs' fault.\nTo ensure a fair comparison, we re-ran all generated code with \u2018LayoutViewer' lines commented out. The analysis presented in Figure 3 and the following breakdown reflect these adjusted results.\nOther common errors included hallucinations of nonexistent 'gdspy' functions or methods, resulting in various 'AttributeErrors' (e.g., \"CrossSection\", \"Circular\", \"Ellipse' ') and 'TypeErrors'. Some errors were due to spelling mistakes, such as misspelling gdspy.Text as gdspy.text.\nThe subsequent analysis presents a detailed error breakdown for each LLM, ranked by ascending number of errors.\n01-preview Total errors: 12\nThe main errors for 01-preview included:\n\u2022 TypeError: GdsLibrary.write_gds() got an unexpected keyword argument 'unit' (16.67%)\n\u2022 SyntaxError: invalid syntax (16.67%)\n\u2022 Various TypeErrors and AttributeErrors related to unexpected keyword arguments or missing attributes (66.66%)\nGPT-40 Total errors: 18\nThe most common errors for GPT-40 were:\n\u2022 TypeError related to unexpected keyword arguments (27.78%)\n\u2022 SyntaxError: invalid syntax (16.67%)\n\u2022 TypeError: 'float' object cannot be interpreted as an integer (11.11%)\nOther errors included various AttributeErrors, IndexErrors, and ValueErrors, each occurring once or twice.\nClaude-3-5-sonnet Total errors: 21\nThe most frequent error for Claude-3-5-sonnet was:\n\u2022 TypeError: Text.__init__() got an unexpected keyword argument 'anchor' (38.10%)\nOther errors included:\n\u2022 TypeError: Path.__init__() got an unexpected keyword argument 'layer' (9.52%)\n\u2022 Various TypeErrors, ValueErrors, and AttributeErrors, each occurring once (52.38%)\nLlama-3-405b Total errors: 36\nThe most frequent errors for Llama-3-405b were:\n\u2022 TypeError: 'int' object is not subscriptable (8.33%)\n\u2022 SyntaxError: invalid syntax (8.33%)\n\u2022 Various TypeErrors related to unexpected keyword arguments or multiple values for argu- ments (16.68%)\nThis model also encountered several ValueErrors and AttributeErrors.\nLlama-3-70b Total errors: 68\nThe most prevalent error for Llama-3-70b was:\n\u2022 AttributeError: module 'gdspy' has no attribute 'Library'. Did you mean: 'library'? (36.76%)\nOther common errors included:\n\u2022 TypeError: GdsLibrary.write_gds() got an unexpected keyword argument 'unit' (7.35%)\n\u2022 SyntaxError related to assignment (5.88%)\n\u2022 Various TypeErrors and AttributeErrors related to unexpected keyword arguments or missing attributes (25.00%)\nThese error patterns suggest that all models struggled with correctly using the gdspy library, often attempting to use non-existent attributes or passing incorrect arguments to functions. Syntax errors were also common across models, indicating issues with code structure and Python syntax.\nA.3.4 Inefficient Code\nIn the DLDChip task, which involves creating a dense array of identical shapes, the Llama-3.1-405B model generated code that created a large number of objects and performed numerous boolean operations. This led to high memory usage and extended execution time, requiring the code to be terminated after approximately 15 minutes of runtime.\nA.3.5 Ambiguous Instructions\nIn some cases, we observed that the LLM results mainly fell into two categories. After inspecting the prompts, we found that the instructions could be interpreted in two ways. In these cases, we counted both types of results as correct. However, when implementing a copilot, the agent should ask for clarification if the instructions are ambiguous.\nA.4 Via Connection Test Cases\nA.4.1 Prompts for Via Connection Tests\nTest 1: \"I have a sketch idea that i want to draw in GDSII, generate the python code for this design. each color represents an individual layer. We want to use a metal to connect two vias and put a pad on top of each via\u201d\nTest 2: \"I have a sketch idea that i want to draw in GDSII, generate the python code for this design. each color represents an individual layer. We want to have two vias near each end on a piece of metal. And a pad on top of the metal.\"\nTest 3: \"I have a sketch idea that i want to draw in GDSII, generate the python code for this design. each color represents an individual layer. we want use to connect two vias using a piece of metal and put a circular padding on top of each via\"\nTest 4 (Generated by LLM based on the final output in Test 3):\nLayers and Colors:\nThe design consists of three layers: via layer (yellow),\nmetal layer (blue), and pad layer (red).\nDimensions:\nVia: The radius of each via is 10 units.\nPad: The radius of each pad is 30 units.\nMetal Connection: The width of the metal connection is 40 units,\nand the total length is 600 units.\nPositions:\nThe first via is positioned at coordinates (50, 150).\nThe second via is positioned at coordinates (550, 150).\nConnections and Coverage:\nThe metal connection should fully cover the vias, extending slightly beyond their edges.\nEnsure the metal connection is slightly wider than the via diameter to provide full coverage.\nSpacing and Margins:\nLeave a margin of 10 units between the edge of the metal and the pads.\nEnsure there is a space of 50 units between the vias and the edges of the metal connection.\nAdditional Requirements:\nThe metal connection should be shorter than the total length to fit beneath the covering area of the pads, leaving some space at the edges.\nBy providing detailed information like this, you can ensure that the design is accurately reproduced.\nIf you have any specific design rules or preferences, make sure to include those as well.\nTest 5: \"I have sketched a design for 3d packaging, where we have a metal connecting two TSVs, please generate the python code to draw a GDSII for this design.\"\nTest 6: \"I have sketched a design for 3d packaging, where we have a metal connecting two TSVS , please generate the python code to draw a GDSII based on the sketch. each color represents an individual layer. The metal connection should fully cover the vias, extending slightly beyond their edges. Ensure the metal connection is slightly wider than the via diameter to provide full coverage.\""}]}