{"title": "SureMap: Simultaneous mean estimation\nfor single-task and multi-task disaggregated evaluation", "authors": ["Mikhail Khodak", "Lester Mackey", "Alexandra Chouldechova", "Miroslav Dud\u00edk"], "abstract": "Disaggregated evaluation\u2014estimation of performance of a machine learning\nmodel on different subpopulations\u2014is a core task when assessing performance\nand group-fairness of AI systems. A key challenge is that evaluation data is scarce,\nand subpopulations arising from intersections of attributes (e.g., race, sex, age) are\noften tiny. Today, it is common for multiple clients to procure the same AI model\nfrom a model developer, and the task of disaggregated evaluation is faced by each\ncustomer individually. This gives rise to what we call the multi-task disaggregated\nevaluation problem, wherein multiple clients seek to conduct a disaggregated\nevaluation of a given model in their own data setting (task). In this work we develop\na disaggregated evaluation method called SureMap that has high estimation\naccuracy for both multi-task and single-task disaggregated evaluations of blackbox\nmodels. SureMap's efficiency gains come from (1) transforming the problem into\nstructured simultaneous Gaussian mean estimation and (2) incorporating external\ndata, e.g., from the AI system creator or from their other clients. Our method\ncombines maximum a posteriori (MAP) estimation using a well-chosen prior to-\ngether with cross-validation-free tuning via Stein's unbiased risk estimate (SURE).\nWe evaluate SureMap on disaggregated evaluation tasks in multiple domains,\nobserving significant accuracy improvements over several strong competitors.", "sections": [{"title": "1 Introduction", "content": "Evaluation is a key challenge in modern AI, with much effort spent deciding what metrics to measure,\nwith which methods, and on what data. This challenge is especially acute in fairness assessment,\nwhich requires not only high-quality data to run a model and score its outputs but also demographic in-\nformation for defining groups. Due to the high cost of obtaining high-quality evaluation data, the issue\nof sample complexity-sample size needed to get a good performance estimate-remains salient, espe-\ncially when we want to release not just one overall measure but instead to output a disaggregated eval-\nuation that captures variation among demographic subpopulations of the data [Barocas et al., 2021].\nFor instance, we might want to assess group fairness by examining the variation in performance across\ngroups of users defined by intersections of the demographic attributes age, race, and sex. The naive ap-\nproach of independently evaluating each group's performance on its own data can fail because the sam-\nple sizes of intersectional groups rapidly decrease as we consider more attributes [Herlihy et al., 2024].\nRecent work has shown how to improve upon naive methods by combining data from multiple sub-\npopulations to inform their individual performance estimates [Miller et al., 2021, Herlihy et al., 2024].\nIn today's technology landscape it is common for multiple clients to procure the same model (e.g., an\nautomated speech recognition or language model) from an AI developer, with each client performing\na disaggregated evaluation of the same model on their own data. We refer to this problem as the\nmulti-task disaggregated evaluation. We formalize and study this problem, showing that one can\nimprove the disaggregated evaluations of individual clients by using multi-task data in the form of"}, {"title": "1.1 Contributions", "content": "1. SureMap: We introduce a method that uses SURE to tune the parameters of a well-chosen\nGaussian prior before applying MAP estimation. The prior is motivated by its attainment of a\ngood efficiency-expressivity tradeoff, requiring only a linear (in the number of subpopulations)\nnumber of parameters to recover several natural baselines for disaggregated evaluation.\n2. Datasets: Disaggregated evaluation has few benchmarks [Herlihy et al., 2024], so we intro-\nduce new ones for both the single-task and multi-task settings, covering automated speech\nrecognition (ASR) and also tabular domains (with linear models and also in-context LLMs).\n3. Single-task: We find that SureMap is always competitive with strong baselines from prior work,\nwhile improving significantly in some settings with intersectional sensitive attributes.\n4. Multi-task: Incorporating data from multiple clients into SureMap yields significant improve-\nments across all evaluated settings. This multi-task approach is more accurate even with just one\nadditional task and is the only method to consistently outperform the naive and pooling baselines."}, {"title": "1.2 Related work", "content": "Disaggregated evaluation is a core task in the fairness assessment of AI systems [Barocas et al.,\n2021]. Past work has sought to improve estimation accuracy by combining information across\ndifferent groups, e.g., via Bayesian modeling [Miller et al., 2021], Gaussian process approximation\nof loss surfaces [Piratla et al., 2021], and structured regression [Herlihy et al., 2024]. The last work\nfound that classical James-Stein-type mean estimation [James and Stein, 1961, Bock, 1975] is\noften competitive, and so we adopt it as our first non-naive baseline. We also compare to structured\nregression itself, which turns out to have a tight mathematical connection to SureMap; indeed, apart\nfrom our use of Gaussian (ridge) rather than Laplace (lasso) priors (regularization)\u2014as well as our\nuse of a more flexible tuning based on SURE rather than cross-validation\u2014the method of Herlihy et al.\n[2024] can be viewed as the discriminative counterpart to our generative approach (see \u00a7E for details).\nWithin the disaggregated evaluation literature we are the first to formulate and study multi-task\ndisaggregated evaluation. This is an important direction because (a) model providers often have their\nown data or data from multiple clients that can inform the evaluation and (b) transferring information\nacross distributions is a key way to handle very low-sample regimes. We also contribute several\ndatasets that we hope will spur further development in disaggregated evaluation.\nSureMap relies on applying classical mean estimation tools to quantities modeled as Gaussian means.\nNotably, Miller et al. [2021] model scores via well-studied distributions-e.g., Gaussians\u2014but since\nscores are related non-linearly to metrics it is unclear if this can lead to similarly simple estimators. To\ntune parameters, we use SURE, a popular statistical approach [Li, 1985, Donoho and Johnstone, 1995].\nSpecifically, in the empirical Bayes tradition, we use it to set the MAP estimator of a hierarchical\nmodel. Using SURE to tune the scale of an isotropic Gaussian prior was shown to be asymptotically\n(in the dimension) optimal in the case of heteroskedastic data distributions [Xie et al., 2012]. Since\ndisaggregated evaluation data is highly heteroskedastic due to variation in group size, this is positive\nevidence for our approach, although our prior is non-isotropic and has many more variance parameters."}, {"title": "2 Setup", "content": "We first describe the disaggregated evaluation problem (\u00a72.1), recast it as a Gaussian mean estima-\ntion (\u00a72.2), and motivate a multi-task variant (\u00a72.3), all while introducing several baselines estimators."}, {"title": "2.1 Setting and baselines", "content": "We want to assess a predictive model $p : X \\rightarrow Y$ under some distribution $D$ over input space $X$ and\noutput space $Y$ using error measure $l : Y \\times Y \\rightarrow R$. For example, in image classification, $D$ is a dis-\ntribution over (image, label) pairs and $l$ is the 0-1 error. To simplify notation, we mainly deal with the\ncomposite function $f(z) = l(y, p(x))$ acting on points $z = (x, y)$ in the product space $Z = X \\times Y$."}, {"title": "2.2 A Gaussian model for disaggregated evaluation", "content": "We use a simple but natural model to aid in the design of disaggregated evaluation methods. Specif-\nically, denoting the naive estimator by $y = \\hat{\\mu}^{\\text{naive}}(S) \\in R^d$, we model group $g$'s entry $y_g$ as being\ndrawn from a Gaussian with (unknown) mean $\\mu_g$ and (known) variance $\\sigma^2/n_g$, where $\\sigma^2$ is shared\nacross groups. This reduces the problem of disaggregated evaluation, as defined in \u00a72.1, to that of es-\ntimating the mean of a multivariate Gaussian with known diagonal covariance $\\Sigma_{g,g} = \\sigma^2/n_g$ given a\nsingle sample $y \\sim N(\\mu, \\Sigma)$. Our model has many advantages in the disaggregated evaluation setting:\n1. By the central limit theorem, y is asymptotically normal with mean \u00b5 and diagonal covariance \u03a3\nfor many distributions D of interest, even when the underlying data is non-Gaussian. Furthermore,\nbecause the methods derived from our model only take y and \u03a3 as input, they can be applied even\nwhen the evaluated statistic is not the pointwise average assumed by the setup in \u00a72.1, so long\nas $y \\sim N(\\mu, \\Sigma)$ holds asymptotically. An example of this is when $y_g = \\hat{\\mu}^{\\text{naive}}(S')$ corresponds\nto the area under the ROC curve (AUC) computed over group g's data $S \\cap Z_g$ [Lehmann, 1951];\nwe demonstrate SureMap's applicability to AUC empirically in \u00a7G (Figures 10 & 12).\n2. While a shared variance is a strong assumption, it is perhaps the simplest way of incorporating the\ninductive bias that $\\mathbb{E}\\Sigma_{g,g}$ will be highly correlated with the inverse of $n_g$, the number of samples\nfrom group g. In practice, we set $\\sigma^2$ to be the pooled estimate $\\frac{1}{n d}\\sum_{g=1}^d \\sum_{z \\in S \\cap Z_g} (f(z) - y_g)^2$.\n3. Gaussian mean estimation is one of the best-studied problem in statistics, with numerous\nwell-tested baselines and approaches for developing new methods. In particular, we make\nsignificant use of the classic James-Stein approach [James and Stein, 1961, Bock, 1975],\nSURE [Stein, 1981], and empirical Bayesian estimation methods [?].\n4. In the multi-task setting, clients are likely to be unwilling to share their actual data but possibly\nmore willing to share group summary statistics. Thus methods developed for our Gaussian\nmodel-which only require the group means y, group counts n, and an estimate of $\\sigma^2$-will\nbe more broadly applicable than methods that act directly on the dataset $S \\subset Z$."}, {"title": "2.3 The multi-task setting", "content": "We can easily extend this model to study multi-task disaggregated evaluation, in which for each task\n$t = 1,..., T$ (e.g., a client of the model provider) we observe a set $S_t \\subset Z$ of $n_t$ samples from\nthe task distribution $D_t$. The goal is then to output $T$ vectors $\\hat{\\mu}_t$ that are close on-average to the\ntasks' subpopulation errors $\\mu_{t;g} = \\mathbb{E}_{z \\sim D_t} [f(z)|z \\in Z_g]$. Converting to our Gaussian model, we\nobserve $T$ vectors $y_t \\sim N(\\mu_t, \\Sigma_t)$\u2014where we set $\\Sigma_{t;g,g} = \\sigma^2/n_{t;g}$ for some globally shared $\\sigma^2$ and\ntask-specific group count vectors $n_t \\in \\mathbb{Z}_{\\geq 0}^d$\u2014and must output $T$ mean estimates $\\hat{\\mu}_t({y_t}_{t=1}^T) \\in R^d$.\nWe consider two natural multi-task baseline estimators. The first is the global naive estimator (or\nglobal estimator for short), which combines the data from all tasks, computes a single global vector\nof group averages, and uses it as the estimate for each task:"}, {"title": "3 Methods", "content": "In the last section we reduced the problem of disaggregated evaluation to that of estimating a mean\n$\\mu \\in R^d$ given a sample $y \\sim N(\\mu, \\Sigma)$, where $\\Sigma$ is known and diagonal. We now design a method,\nSureMap, for the latter problem. Our technical approach involves the following two steps:\n1. Choosing a parameterized mean estimator. We use the MAP estimator under a multivariate\nnormal prior that we design specifically for intersectional subpopulations.\n2. Tuning the estimator's hyperparameters. We use SURE to estimate the quality of our estimator,\nwhich we then optimize over the choice of hyperparameters using the L-BFGS-B algorithm."}, {"title": "3.1 Designing a parameterized estimator", "content": "As mean estimation is a vast area, we use three criteria for designing an estimator: it should\n(1) dominate baselines such as naive and pooled; (2) have relatively few hyperparameters; and\n(3) handle heteroskedasticity stemming from variation in group sizes. One natural source of candidates\nare James-Stein-type shrinkage estimators: the original James-Stein estimator famously dominates\n$\\hat{\\mu}^{\\text{naive}}$ in MSE and has no hyperparameters to tune [James and Stein, 1961], satisfying our first two\ndesiderata. Furthermore, while James and Stein [1961] assumed an isotropic $\\Sigma$, subsequent estimators\nsuch as the following variant of an estimator due to Bock [1975] do handle heteroskedastic $\\Sigma$:"}, {"title": "3.1.1 An additive intersectional effects prior", "content": "For brevity, we build up our estimator somewhat informally; a full description is in \u00a7C.1. We return\nto our simple example where each group $g \\in [d]$ corresponds to an intersection $(s, a) \\in [d_1] \\times [d_2]$\nof two attributes: a sex $s \\in [d_1]$ and an age $a \\in [d_2]$. A simple prior that additively incorporates\nindividual attribute effects into intersectional group means is the following:"}, {"title": "3.1.2 Efficiency and expressivity", "content": "As detailed in \u00a7C.1, this prior can be naturally extended to any number of attributes $k$ using a\ncovariance matrix $\\Lambda(\\tau) \\in R^{d \\times d}$ specified by a vector $\\tau \\in R^{2^k}_{\\geq 0}$ of $2^k$ hyperparameters. Since\n$k \\leq [log_2 d]$, the total number of hyperparameters (including $\\theta \\in R^d$) is $d + 2^k = O(d)$, which is\nmuch smaller than the $O(d^2)$ complexity of the general case. We can further reduce this by fixing the\nentries of $\\theta$, constraining them to be identical, or setting them using external (e.g., multi-task) data.\nDespite this reduction in hyperparameters, we can show that for a suitable choice of $\\tau$, the estimator\n$\\hat{\\mu}^{\\text{MAP}}_{(\\tau)}(\\cdot)$ recovers many estimators of interest, including the naive estimator and the (possibly offset)\npooled estimator (see \u00a7C.2). This means that MAP with our structured prior should be able to\noutperform all four baselines from the previous section, if appropriately tuned."}, {"title": "3.2 Tuning by minimizing expected risk", "content": "Having specified a parameterized estimator, there remains the question of setting its parameters $\\theta$\nand $\\tau$. One might want to treat this as a hyperparameter tuning problem and use a data-splitting\napproach; however, the dimensionality of the problem makes standard techniques either expensive\nor noisy, and data splitting introduces additional randomness and design decisions into an already\ndata-poor environment. We instead make continued use of our Gaussian assumption and turn to\nSURE, which given a differentiable estimator $\\hat{\\mu} : R^d \\rightarrow R^d$ returns an unbiased estimate of its\nweighted MSE $L^W_n$ using sample data $y \\sim N(\\mu, \\Sigma)$:"}, {"title": "3.2.1 Single-task SureMap", "content": "In the single-task setting, we fix $\\theta = 0_d$ and tune the variance parameters $\\tau \\in R^{2^k}_{\\geq 0}$. Letting\n$A(\\tau) = (\\Lambda^{-1}(\\tau) + \\Sigma^{-1})^{-1}\\Lambda^{-1}(\\tau)$, we define the single-task SureMap estimator as"}, {"title": "3.2.2 Multi-task SureMap", "content": "To generalize SureMap to the multi-task setting we propose to specify $\\hat{\\theta}$ and $\\hat{\\tau}$ by minimizing SURE\naggregated across tasks, i.e., $\\sum_{t=1}^T R_n^t(y_t)$. While setting both parameters via direct optimization\nof this objective is the most straightforward approach, we find that it performs worse than single-task\nSureMap when there are only a few tasks (T < 5) and rarely improves significantly above the\nmulti-task global and offset estimators. This can be explained by observing the few-task limit-i.e.\nT = 1\u2014in which case optimizing the aggregated SURE objective results in setting $\\hat{\\theta} = y_1$ and thus\nmakes the multi-task estimator equivalent to the naive estimator.\nWe find that a better approach is to treat the choice of $\\hat{\\theta}$ as its own simultaneous mean estimation\nproblem and apply the SureMap approach to it. In particular, our model $y_t \\sim N(\\mu_t, \\Sigma_t)$ and our prior\n$\\mu_t \\sim N(\\theta, \\Lambda)$ imply that the samples $y_t \\sim N(\\theta, \\Lambda+\\Sigma_t)$ have mean $\\Theta$ and known covariances (apart\nfrom tuning parameters). Therefore, the MAP estimator of $\\theta$ itself given a hyperprior $\\theta \\sim N(0_d, \\Gamma)$\nwith covariance $\\Gamma > 0$ will have the form $\\hat{\\theta} = (\\Gamma^{-1} + \\sum_{t=1}^T(\\Lambda + \\Sigma_t)^{-1})^{-1} \\sum_{t=1}^T(\\Lambda + \\Sigma_t)^{-1}y_t$.\nTo reduce the number of tuning parameters, we use a prior of the same form as before by specifying\n$\\Gamma = \\Lambda(v)$ for $v \\in R^{2^k}_{\\geq 0}$, i.e., the same structured covariance as described in \u00a73.1.1 but with separately\ntuned parameters (see \u00a73.1.1 and \u00a7C.1). Substituting the meta-level MAP estimator of $\\Theta$ into the\nMAP estimator of $\\mu_t$ and tuning the parameters $\\tau$ and $v$ by optimizing the sum of SUREs (Eq. 10)"}, {"title": "3.3 Limitations", "content": "Various modeling assumptions impact performance of SureMap. For instance, the Gaussian error\nassumption is less appropriate when errors are heavy-tailed. In \u00a7G, Figure 9, we consider MSE as an\nexample of a target metric with heavy-tailed observation errors. We find that SureMap still performs\nwell, but is no longer superior to previous approaches. One avenue for improvement would be to use\na variance-stabilizing transformation (e.g., Hawkins and Wixley [1986]) prior to applying SureMap.\nNote that SureMap achieves its improved accuracy by shrinking naive estimates towards a less granular\nestimator (e.g., a pooled mean). As a result the estimation is biased towards less disparity, which\ncould lead to overly optimistic conclusions about fairness. For this reason it is extremely important to\nexamine not just the point estimates, but also confidence intervals. These can be obtained, for example,\nby viewing SureMap as a regression approach (\u00a7E) and leveraging inference techniques for regression."}, {"title": "4 Datasets", "content": "We evaluate our approach in several representative settings for disaggregated evaluation, including\ntwo tabular settings appearing in previous works [Miller et al., 2021, Herlihy et al., 2024, Liu et al.,\n2024], and three new settings: a multi-task tabular setting based on state-level U.S. census data and\nboth a single-task and a multi-task ASR evaluation setting."}, {"title": "4.1 Tabular datasets", "content": "We consider three tabular datasets, two for the single-task and one for the multi-task setting, covering\ntwo important domains where fairness concerns can arise: healthcare records and demographic data.\nWhile we focus on the classification task and 0-1 error, in \u00a7G we also report results for regression\n(Figures 8 & 9) and for classification with AUC as the target metric (Figures 10 & 12).\nDiabetes. This is a tabular dataset of Strack et al. [2014], containing around 100K patient records with\nsix race, two sex, and three age categories. We evaluate a logistic regression classifier trained to predict\npatient disposition after a hospital stay (discharged or otherwise). The target metric is the 0-1 error.\nAdult. We use the classic Adult census dataset [Kohavi, 1996] to evaluate performance of an\nin-context LLM learner-specifically 11ama-3-70b-in predicting whether a person makes more\nor less than $50K after being provided with eight examples via a modification of the prompt template\nof Liu et al. [2024]. The target metric is the 0-1 error, disaggregation is by race, sex, and age.\nState-Level ACS (SLACS). This is a tabular dataset for multi-task setting derived from the census\ndata for all U.S. states and Puerto Rico assembled by Ding et al. [2021]. Each datapoint corresponds to\na person in one of nine race and two sex categories; we consider three age categories: below 25, 25\u201364,\nand over 64. The underlying task is to classify each person as earning either more or less than $50K.\nWe train a regularized logistic model on the data from California, and seek to evaluate its performance\non the other 50 states/territories, which comprise the tasks. The target metric is the 0-1 error."}, {"title": "4.2 ASR datasets", "content": "We also introduce both single-task and multi-task speech recognition datasets, based on applying\nthe popular Whisper ASR model [Radford et al., 2023]\u2014specifically whisper-tiny-on the En-\nglish part of the Common Voice (CV) dataset [Ardila et al., 2020], which contains utterances from\nindividuals in one of nine age and three sex categories."}, {"title": "5 Evaluation", "content": "Our main metric is MAE relative to a ground truth vector, which we take to be the mean of all\navailable data for each subpopulation $g \\in [d]$, except those with fewer than 40 samples. In our main\nresults we subsample with replacement from the entire dataset at different rates and track performance\nas a function of the sizes of the resulting datasets. To obtain 95% confidence intervals we conduct 200\nand 40 random trials at each subsampling rate in the single-task and multi-task settings, respectively."}, {"title": "5.1 Single-task", "content": "We compare SureMap to the naive (Eq. 2) and pooled (Eq. 3) baselines, as well as to the Bock\nestimator with shrinkage towards the pooled estimator (Eq. 17) and the structured regression\nestimator of Herlihy et al. [2024]. On both Diabetes and Adult, SureMap significantly outperforms\nall competitors (Figure 2, top & middle), the greatest improvement is on subpopulations with limited\ndata. In \u00a7G, we consider a regression variant of Diabetes and observe similar results (Figure 8).\nOn the Common Voice task, SureMap performs roughly similarly to Bock, while outperforming\nstructured regression at some subsampling rates (Figure 2, bottom); here again the gains are driven"}, {"title": "5.2 Multi-task", "content": "In the multi-task setting, we use all the single-task methods as baselines while adding multi-task (MT)\nones, including MT global (Eq. 4), MT offset (Eq. 5), and an MT extension of Bock (Eq. 6) in which\n$\\theta_g$ is set using the average across the group g data on all other tasks. We first consider the SLACS\ntask, for which Figure 3 (top left) shows that MT SureMap significantly outperforms other methods in\nthe low-data regime while matching the best one (MT Bock) in the high-data regime. At subsampling\nrate 0.1, Figure 3 (top right) also shows that using multi-task data leads to improvement on all but two\nof the fifty tasks, that the reduction in MAE over the naive estimator on a typical task is 2x, and that\nthis improvement only loosely correlates with the task's ground truth distance from the multi-task\nmedian. On the other hand, it also shows that while MT Bock's improvements are typically smaller,\non SLACS it improves performance for every state (including the two where MT SureMap is worse).\nOn the CVC task, the MT offset baseline is the most competitive, except at the lowest subsampling rate\nwhere pooling is better and at higher subsampling rates where it stops improving with additional data\n(Figure 3, bottom left). SureMap outperforms it and all other methods across all subsampling rates\nand its advantage is greatest in low-data regimes, where it even outperforms pooling. In the task-level\nevaluation in Figure 3 (bottom right) we see that on every task, MT SureMap attains an improvement\nof 2-3.5x over the naive baseline and almost always outperforms MT Bock. Furthermore, the latter\nperforms substantially worse on tasks whose ground truth vectors are far away from the multi-task\ncenter while MT SureMap is not affected."}, {"title": "5.3 Ablations", "content": "We next look at how the degree of included intersectional effects and multi-task structure affect\nperformance. In Figure 4 (left), we evaluate utility of including higher-order interactions in the struc-\ntured prior. We implement SureMap variants with up to lth-order interactions by setting $\\tau_A$ for $|A| \\in$\n${l+1,...,k-1}$ to zero (but not for $|A| = k$). We observe that including zeroth-order effects (l = 0)\nin single-task SureMap (i.e., shrinkage to pooling) improves upon the usual single-parameter Gaussian\nprior (l = -1) in low-data regimes. Adding first-order effects (l = 1) leads to substantial further im-"}, {"title": "6 Conclusion", "content": "We have introduced SureMap, a disaggregated evaluation approach, which combines MAP estima-\ntion under a structured Gaussian prior with hyperparameter tuning via SURE. SureMap achieves\nsubstantial empirical improvements over strong baselines in both single-task and multi-task settings.\nValuable future directions include improving robustness to heavy-tailed data and developing multi-\ntask methods that can handle client privacy concerns. More broadly, we hope our work will have\na positive impact by allowing model users to more accurately identify fairness-related harms, the\nfirst step towards mitigating them. However, using SureMap and any other disaggregated evaluation\napproach must be done with care, so as to not risk overconfidence in a model's fairness."}, {"title": "A Notation", "content": "\u2022 For $p \\in [1, \\infty]$ we use $||\\cdot||_p$ to denote the p-norm on $R^d$.\n\u2022 We use $||\\cdot||$ to denote the spectral norm on $[R^{m\\times n}$.\n\u2022 For any positive semi-definite matrix $A > 0^{d\\times d}$ we use the notation $||\\cdot||_A : R^d \\rightarrow R_{\\geq 0}$ to denote\nthe vector norm $||x||_A = \\sqrt{\\langle x, Ax \\rangle} = ||\\sqrt{A}x||_2$ on $x \\in R^d$.\n\u2022 For any positive integer k we use $[k]$ to denote the set ${1, ..., k}$.\n\u2022 For any set S we use $2^S$ to denote its powerset.\n\u2022 For any vector $a \\in R^d$ we use $a_i$ to denote its ith entry. If a is only defined as an expression then\nwe will abuse notation and use $(a)_i$ to refer to its ith entry.\n\u2022 For any vector $a \\in R^k$ and any subset $S \\subset [k]$ with elements $s_1 < ... < s_m$ we define\n$a_S = (a_{s_1}, ..., a_{s_m})$ to be the vector of the entries of a whose indices correspond to the\nelements of S sorted in ascending order.\n\u2022 For any subset $S \\subset [k]$ we assume $\\sum_{s\\in S}$ iterates over the elements in ascending order.\n\u2022 For any matrix $A \\in R^{m\\times n}$ we use $A_{i,j}$ to denote its (i, j)th entry and $A_{i,:}$ its ith row. If A is only\ndefined as an expression then we will abuse notation and use $(A)_{i,j}$ to refer to its (i, j)th entry.\n\u2022 For any k-tensor $Z \\in R^{\\times_{a=1}^k d_a}$ with dimensions $d_1, ..., d_k \\in Z_{>0}$ and any vector $c \\in \\times_{a=1}[d_a]$\nof indices we use $Z_c$ to refer the the $(c_1, ..., c_k)$th entry of Z.\n\u2022 We use $0_m, 1_m \\in R^m$ and $0_{m\\times n}, 1_{m\\times n} \\in R^{m\\times n}$ to refer to all-zero and all-one vectors and\nmatrices, and $I_n$ to refer to the $n \\times n$ identity matrix."}, {"title": "B Heteroskedastic James\u2013Stein-type shrinkage estimation", "content": "For reference we state a weighted version of Stein's unbiased risk estimate (SURE) [Stein, 1981]:\nLemma B.1. Suppose $y \\sim N(\\mu, \\Sigma)$ for mean $\\mu \\in R^d$ and diagonal p.s.d. covariance $\\Sigma \\in R^{d\\times d}$,\nand consider a function $\\hat{\\mu} : R^d \\rightarrow R^d$ s.t. for every $i \\in [d]$ the function $\\hat{\\mu}_i$ is almost differentiable\nin $y_i$ and we have $E||\\nabla_y \\cdot \\hat{\\mu}(y)||_1 < \\infty$. Then for any diagonal matrix $W \\in R^{d\\times d}$ we have\n$E||\\hat{\\mu}(y) - \\mu||_W = E||\\hat{\\mu}(y) - y||_W - Tr(W\\Sigma) + 2E[\\nabla_y \\cdot (W\\Sigma\\hat{\\mu}(y))]$.\nProof. Expanding the squared norm and applying Stein's Lemma [Stein, 1981, Lemma 2] to the last\nterm yields"}, {"title": "C SureMap estimator", "content": "In this section we describe the prior in full for any number of attributes, prove expressivity results\nreference in \u00a73.1.2, and describe how to compute the estimator using coordinate descent."}, {"title": "C.1 A linear prior", "content": "Suppose each group $g \\in [d]$ corresponds to an intersection $g \\in \\times_{a=1}^k [d_a]$ of $k$ attributes, with\nattribute a having $d_a$ possible classes. For example, the first attribute could be one of $d_1$ different\nage brackets and the second could be one of $d_2$ different racial categories. For each subset $A \\subset [k]$\nof attributes define a random tensor $Z_A \\in R^{\\times_{a \\in A} d_a}$ with i.i.d. entries $Z_{A;c} \\sim N(0, 1)$, where\n$c \\in \\times_{a \\in A} [d_a]$ is a specific class combination of the attribute A. Then the additive intersectional\neffects prior on the mean $\\mu_g$ of group g is the weighted sum"}, {"title": "C.2 Expressivity of the linear prior", "content": "Theorem C.1. Consider a disaggregated setting with k attributes with $d_1, . . ., d_k$ possible categories,\nrespectively, and total number of groups $d = \\Pi_{a=1}^k d_a$. If we use a diagonal covariance $\\Sigma \\in [R^{d\\times d}$\nsatisfying $\\Sigma_{h,h} = \\sigma^2/n_h \\forall h$ then the following holds $\\forall y \\in R^d$:"}, {"title": "C.3 How to set the multi-task center", "content": "In the single-task case we set the prior mean $\\theta = 0_d$; this can also be done in the multi-task setting.\nAlternatively, we can use multi-task data to construct estimator $\\hat{\\theta}$ of some true underlying multi-task\nmean $\\Theta$ and substitute the former for $\\theta$; for simplicity we will restrict to the ourselves to linear\nestimators $\\hat{\\theta} = \\sum_{t=1}^T M_t y_t$, where the matrices $M_1, ..., M_T$ are independent of $"}]}