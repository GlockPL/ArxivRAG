{"title": "A HITCHHIKER'S GUIDE TO SCALING LAW ESTIMATION", "authors": ["Leshem Choshen", "Yang Zhang", "Jacob Andreas"], "abstract": "Scaling laws predict the loss of a target machine learning model by extrapolating from easier-to-train models with fewer parameters or smaller training sets. This provides an efficient way for practitioners and researchers alike to compare pre-training decisions involving optimizers, datasets, and model architectures. Despite the widespread use of scaling laws to model the dynamics of language model training, there has been little work on understanding how to best estimate and interpret them. We collect (and release) a large-scale dataset containing losses and downstream evaluations for 485 previously published pretrained models. We use these to estimate more than 1000 scaling laws, then derive a set of best practices for estimating scaling laws in new model families. We find that fitting scaling laws to intermediate checkpoints of training runs (and not just their final losses) substantially improves accuracy, and that all else equal estimates of performance are generally most accurate when derived from other models of similar sizes. However, because there is a significant degree of variability across model seeds, training multiple small models is sometimes more useful than training a single large one. Moreover, while different model families differ scaling behavior, they are often similar enough that a target model's behavior can be predicted from a single model with the same architecture, along with scaling parameter estimates derived from other model families.", "sections": [{"title": "INTRODUCTION", "content": "Substantial effort and cost are required to train even a single large language model (LLM).\u00b9 There is thus an acute need for efficient decision-making aids that can evaluate the effectiveness of proposed changes to language models' architecture or training data without full-scale training runs. While there is a large body of work that motivates or evaluates these changes using small models (Warstadt et al., 2023; Hillier et al., 2024), synthetic tasks (Aky\u00fcrek et al., 2024; Wortsman et al., 2023) or theory (Jelassi et al., 2024), one of the most important tools for current practitioners is the estimation of scaling laws for LLMs (Ivgi et al., 2022; Dubey et al., 2024).\nA scaling law extrapolates the performance of a target model from the performance of a set of models with fewer parameters or smaller training sets. Typically, this extrapolation requires models to belong to the same model family, differing only in parameter count and training set size, but using the same architecture and training distribution. A high-quality scaling law accurately predicts the target model's test performance (Rosenfeld et al.; Kaplan et al., 2020; Hoffmann et al., 2022).\nMost past work describing and characterizing scaling laws has begun by exhaustively training models in a family across a full range of dataset sizes and parameter counts. One question that has received comparatively little attention is how, when training a new LLM, a practitioner with limited computational resources should choose which small-scale models to train in order to best estimate a target model's final performance. This paper offers a practical guide to when, and how, to use small models to efficiently obtain meaningful predictions about large models' behavior\u2014maximizing prediction reliability while minimizing the budget for preliminary experimentation, which necessarily involves tradeoffs between the number of preliminary models trained, the size of the largest preliminary model, and size of the dataset used to train it."}, {"title": "DEFINING A SCALING LAW", "content": "A scaling law estimates the loss of a costly model by training cheaper ones which share a pretraining procedure and differ by some hyperparameters, typically model size (#params) and number of tokens seen during training (#toks). A scaling law is a function that predicts a target model's loss on held-out data when setting the value of one hyperparameter (Kaplan et al., 2020) or both Comparing laws' predictions about different pretraining choices allows informed decisions about which large-scale model to train.\nA scaling law also enables finding the optimal choice of hyperparameters under computational constraints on pre-training or inference.\nFormally, we will call a model f any single concrete neural language model with a specific set of parameters. Different seeds, or even different checkpoints from the same training run, correspond to different models. We define a scaled model family f as a set of models, with each \\( f \\in F \\) differing only in size \\( \\#\\text{params}(f) \\) and number of tokens \\( \\#\\text{toks}(f) \\).\nThere are two specific subsets of scaled model families that will be useful in our experiments. First, the maximal parameter family \\( \\text{max}_{\\#\\text{params}}(F) \\) contains only models in \\( F \\) with the largest number of parameters. Formally, define \\( m = \\text{max}_{f\\in F} \\#\\text{params}(f) \\); then \\( \\text{max}_{\\# \\text{params}}(F) = {f \\in F : \\#\\text{params}(f) = m} \\). This family will generally contain the target model(s) whose behavior we wish to predict \\( t \\in F_{\\text{target}} \\). Second, the q-maximal token family \\( \\text{max}_{\\# \\text{toks}}(F, q) \\) contains all models in f trained on at least a q-sized fraction of the training set. Formally, define \\( t = q \\cdot (\\text{max}_{f \\in F} \\#\\text{toks}(f)) \\); then \\( \\text{max}_{\\#\\text{toks}}(F,q) = {f \\in F : \\#\\text{toks}(f) \\geq t} \\). Note that this definition does not distinguish between partially trained models on one hand, and models trained to convergence on a subset of the largest training set used in a family on the other. Throughout this paper, we will not in general distinguish between these two types of models, a decision evaluated in Section 6. Indeed, except where noted, \\( \\text{max}_{\\#\\text{toks}}(F, q) \\) should be thought of as containing the checkpoints from the last q% of a training run.\nA scaling law \\( \\hat{L}(f | F) \\) estimates the performance of a new model f given a model family F. (We will simply write \\( \\hat{L}(f) \\) when the family is clear from the context.) All experiments in this paper use the common functional form from the literature (Hoffmann et al., 2022):\n\\[\n\\hat{L}(f) := E + \\frac{A}{\\left(\\frac{\\#\\text{params}(f)}{C}\\right)^\\alpha} + \\frac{B}{\\left(\\frac{\\#\\text{toks}(f)}{D}\\right)^\\beta}\n\\]\nHere E is a baseline capturing the scaled family's general performance; A, \u03b1 and B, \u03b2 describe the scaling effect of \\( \\#\\text{params} \\) and \\( \\#\\text{toks} \\) respectively."}, {"title": "DATA FOR 1000+ SCALING LAWS AND MORE", "content": "As part of this work, we have collected and released the largest-scale public dataset describing scaling behavior across model families. This dataset aggregates information from a large number of LLM training efforts that have released information about the behavior of multiple models of different sizes or scales. While experiments in this paper focus on scaling laws that measure loss, the dataset also includes information about model performance on downstream evaluation benchmarks where available. We have focused on language models where the largest one is more than 3B parameters and where data was shared publicly or in private correspondence. Our repository accepts further contributions and requests for additions. In addition to those, we have manually extracted some data from papers that did not release models but reported losses in figures."}, {"title": "DATA SOURCES", "content": "For each model in this dataset, we report any downstream evaluation and loss that was measured during training, as well as calculated #toks for each, links to matching checkpoints when available, links to data sources, and information about computational cost (in FLOPs) and number of training epochs (i.e. passes over the training set). Each model is identified by a unique name, a type (e.g. llama), #toks, #params, architecture type (e.g. encoder-decoder), and seed.\nModels in this dataset include Pythia (Biderman et al., 2023, which provides the largest set of models and variations in a family), OPT collected thanks to Xia et al., 2023; Biderman et al., 2023), OLMO (Groeneveld et al., 2024), Amber (Liu et al., 2023), K2 (Team, 2024), Mamba RedPajamas 3 ModuleFormer mixture of experts (Shen et al., 2023), overtrained models (Gadre et al., 2024), Mamba, Llama and hybrid architecture variations from transformer architectures Bloom (Le Scao et al., 2023), T5-Pile Sutawika et al., 2024), Pandey (2024) models, GPT-family models with different data regimes (Muennighoff et al., 2024), Gopher and GPT3 (Brown et al., 2020).\nThe data consists of 1.9M steps of training evaluated on loss or perplexity, usually on multiple data sources belonging to 485 unique pretrained models, and more than 40 scaled families.\nWe hope this will provide a useful resource for the community and plan to extend it further as models get released and their training dynamics are shared. We see such a resource as a facilitator to more research on model development (e.g. A/B testing), scaling laws, downstream scaling laws (Gadre et al., 2024; Ruan et al., 2024; Owen, 2024; Isik et al., 2024), training dynamics (Choshen et al., 2022) and more."}, {"title": "SCALING LAW ESTIMATION", "content": "In the rest of the paper, we present findings from estimating hundreds of scaling laws as follows:\nFitting For each model family F, we identify the maximal parameter family \\( F_{\\text{max}} = \\text{max}_{\\#\\text{params}}(F) \\), and estimate a scaling law \\( \\hat{L} \\) using the remaining models \\( F_{\\text{train}} = F \\setminus F_{\\text{max}} \\). Estimation of scaling law parameters uses the curve_fit function in scikit-learn (Pedregosa et al., 2011). We additionally experimented with an L-BFGS-based solver but found it to be less stable. We only estimate scaling laws for model families that contain at least three models."}, {"title": "HOW WELL CAN I EXPECT A SCALING LAW TO PREDICT?", "content": "4% is the best ARE typically obtained; ARE up to 20% can still distinguish between many modeling choices.\nTo establish how accurate a scaling law must be to be useful to practitioners, we first assess what changes in model accuracy have been considered meaningful in past work. We have surveyed experiments in the literature where an A/B test was performed, i.e., two models were trained similarly, manipulating one attribute to see how it affects scores. Empirically, we found no widely adopted modeling changes that were motivated with less than a 4% relative difference between models. Additionally, reported variance across random restarts of the same model architecture reaches up to 3.5% (c.f.,\u00a78; Sellam et al., 2021). We take this to mean that this is approximately the minimal effect-size experimenters care about and possibly the minimal effect one can reliably measure. Accordingly, this bounds the best goodness of fit we should expect or require of scaling laws.\nTo offer several concrete points of comparison: Pythia 6.9B models fixed inconsistencies in their code and hence have two versions (c.f. App. B; Biderman et al., 2023) which differ in loss by 40%. They also provide data deduplication A/B test that had a minor effect on the loss of about 5%. Gadre et al. (2024) tested the effect of training 400M parameter models for different #toks. The most similar (double the training tokens) has approximately 4% change and can reach a 50% loss difference with 30 times more training. Training on a constant #toks but repeating the same data resulted in almost no changes for up to 4 repetitions (epochs), and later in about 8%, 50% on 14.44 repetitions of the data (Muennighoff et al., 2024). Instead of varying the amount of data or epochs, Ge et al. (2024) found that training on a different kind of data incurred ARE of approximately 10% and different data mixes led to 6% changes or less."}, {"title": "WHEN I TRAIN A NEW MODEL, DO I EVEN NEED A NEW SCALING LAW?", "content": "Different model families exhibit different scaling behavior, but performance can sometimes be estimated using a single model in a new family.\nScaling laws relate performance to scalar training parameters like model or dataset size. For discrete decisions (whether the choice of nonlinearity or data preprocessing scheme), it is not immediately obvious how to pool information across models that differ in these traits Clearly, different pretrained models with the same #params and #toks still show different loss, so these differences can be consequential. But how do discrete choices of architecture, training procedure, or dataset, affect the form of scaling laws?\nOne way to answer this question is to look at the parameter estimates for scaling law parameters E, \u03b1, \u0391, \u03b2 and B differ across model families. These results are shown in Fig. 3, where it can be seen that there are often dramatic differences in all five parameters across families. In this sense, even the rate at which additional data or parameters improve model performance depend on underlying architectural details, suggesting that understanding the behavior of a new model family may require a new scaling law.\nBut another way to answer this question is to ask how reliably we can predict final model accuracy when borrowing (or pooling) some parameters of scaling laws between families\u2014even if these result in poor parameter estimates, they may predict large-scale model behavior within the range of meaningful differences identified in Section 4. To do so, we set the #params scaling parameters (A, \u03b1) to fixed values reported in past work, and estimate remaining parameters for individual model families. We take the variable values found by Muennighoff et al. (2024). We find (see Fig. 6 in App. A) that in some cases only a single training run in a new model family is necessary to obtain accurate scaling law predictions. In the OLMO family, for example, we obtain less than 1% error estimating the accuracy of a 7B model from a collection of 1B model checkpoints.\nWe find that predictions generalize, and a constant #params scaling factor is enough for most models (except the encoder-decoder T5-Pile). However, error rates are larger than in the source family, and predictions for larger models are worse (most conspicuous in OPT's error of 37%, 25% and 15% when extrapolating from 8.7B, 13B and 30B to 175B)."}, {"title": "CAN I JUST TRAIN THE TARGET MODEL A BIT INSTEAD OF MANY SMALL MODELS?", "content": "Yes, but obtaining reliable estimates in this way requires up to 30% of the full training run.\nThe above results (last row of Fig. 6 in App. A) also suggest the possibility of predicting losses not with just smaller models, but with partially trained versions of the target model itself. When predicting inside the same #params family\u2014that is, estimating \\( \\hat{L}(f | F_{\\text{target}} \\setminus {f}) \\)\u2014 the #params term in Eq. (1) is constant, and extrapolation is only required for #toks. As seen in the figures, this form of estimation is informative if permitted by computational constraints. Beyond the immediate usefulness of this approach, it is a promising avenue for future research. Better adjusting the scaling laws for predicting through training might improve this efficiency."}, {"title": "ARE EVEN SIMPLER BASELINES ENOUGH?", "content": "Some extrapolation is necessary: scaling laws can produce accurate estimates even when the target model vastly outperforms any training model.\nTo provide another form of comparison for the predicted scaling laws, we compute two baselines. Both baselines adopt a pessimistic evaluation assuming that the target model is no better than the best model in the small model family used to estimate a scaling law. Specifically, the baselines are the best performance \\( \\hat{L}(\\cdot | F_{\\text{train}}) = \\text{min}_{f \\in F_{\\text{train}}} \\hat{L}(f) \\) and the performance of the most-trained model, consuming the most compute for training, i.e. \\( \\hat{L}(\\cdot | F_{\\text{train}}) = \\text{arg max}_{f \\in F_{\\text{train}}} \\#\\text{params}(f) \\times \\#\\text{toks}(f) \\). Those baselines might be the best one can expect without fitting a law to scaling.\nWe find (See App. 5.2) that out of the two, the best performance baseline is closer to \\( \\hat{L}(F_{\\text{target}}) \\), which is to be expected, as the target model performance is better than any other model in F and this is the better of the two. In both cases, even with the full F, the baselines suffer more than 15% error, mostly above 10%, almost never get below 5%, and 18% ARE on average across all scaled families we study."}, {"title": "I HAVE SOME DATA, WHAT PORTIONS SHOULD I USE?", "content": "Estimate scaling laws from intermediate checkpoints, not just fully trained models!\nMost past work on scaling behavior of language models (e.g., Gadre et al., 2024; Muennighoff et al., 2024) has trained a separate model for each value of #toks studied. This is based on the assumption that changes in the learning rate schedule, which depend on the size of the full dataset that will be used for training, render losses from intermediate checkpoints uninformative.\nHowever, some recent work has demonstrated the effectiveness of learning schedules that do not require prior access to the size of the training set (Hu et al., 2024), and some work has questioned whether careful choice of the learning rate decay is necessary for reliable scaling laws Together, these findings motivate revisiting the assumption that only a single useful datapoint may be obtained from each training run. In the final portion of \u00a75.1, we observed the value of intermediate checkpoints when only a single #params family is used to fit a scaling law. We now test whether this finding extends to larger families\u2014i.e. whether including intermediate checkpoints from all models in a model family reduces ARE.\nResults are shown in Fig. 4, which plots ARE for scaling laws estimated from data subsets of the form \\( \\text{max}_{\\#\\text{toks}}(F, q) \\) for varying q. We find that including full training curves in scaling law estimation can predict losses well. In fact, relying merely on the end of training produces significantly worse performance across the board. Our remaining experiments thus fit scaling laws using all these intermediate checkpoints, and not final performance alone."}, {"title": "SHOULD I USE ALL INTERMEDIATE CHECKPOINTS?", "content": "Almost all, but drop checkpoints from the beginning of training.\nIn Fig. 4, we plot the ARE for different q-maximal token families serving as F, i.e., when fitting only with the end of training runs. There is not a clear trend indicating whether we should use all data (as might be suggested by GPT-3 results alone) or only some of it. But it is rarely the case that best estimates are obtained from the end of training alone.\nThere is, however, a distinctly uninformative phase at the beginning of training, as can be seen in the loss curves (App. B) and noted in the literature (e.g., Chen et al.). We observe that this period is more likely to contain significant spikes or an increase in loss (worse performance) despite additional training. We hence hypothesize this part should always be removed from the scaling law."}, {"title": "HOW BIG A MODEL SHOULD I TRAIN?", "content": "Larger models are better, but not necessary. Mainly, beware of specific models that might give noisy results.\nIn Fig. 1 we compare scaling laws when controlling the amount, percentage, or size of the models (2 at a time). We find that choosing models closer in #params to the target model is generally effective but the effect is neither strong nor monotonic. For example, in all cases fitting on all F provides on of the lowest ARE. However, in GPT, Gopher and OPT, predicting with the smallest 4 models available is already enough to achieve less than 10% error. In Pythia, the smallest models are not predictive but the rest of the models provide a similar fit. While relying on a larger model is beneficial, predicting many scales up (e.g., the behavior of a 34\u00d7 larger model in Pythia) is still reliable, especially if accounting for other factors we discuss next.\nIn fact, training additional, larger models before fitting a scaling law may sometimes decrease accuracy due to increased variance in large model performance\u2014see, for example, Pythia 2.8B in Fig. 1. Unfortunately, it is difficult to identify whether a seed is exceptionally high or low-performing without additional information. For example, cross-validation on F fails to detect it (see App. D).\nInstead, this instability can be addressed by accounting for seed variability. A wasteful way to do so would be to train every model several times. A better alternative is to diversify and train each model on as differing hyperparameters (here, seed, #params, #toks) as possible and to maximize the information gained (a common practice in efficiency-coverage scenarios, e.g., Perlitz et al., 2024)."}, {"title": "HOW MANY MODELS ARE NEEDED FOR RELIABLE PREDICTIONS?", "content": "5 models is a safe bet, more would improve the results' robustness. These models can be small.\nWe have seen that predicting with larger models and hence extrapolating less yields better results. However, given compute constraints (and additional hardware constraints like memory), practitioners may generally wish to use smaller models when possible. Consider for example Fig. 1b where we compare fitting on 4 models but vary their size. We find that more models reduce ARE even without being bigger models. As discussed in \u00a77, adding a larger model to a current scaled family serves two goals, it increases the proximity to the predicted model, as well as increases the number of models seen.\nWe separate the contribution of size and number of models effect. In Fig. 1c, we predict with the largest model being held constant and add (at minimal cost) smaller models. We see again that larger models do benefit predictions. For example, the small models part (left) of the graph indicates large errors (bright). However, we also see again the unwanted effects a single model may have on the overall prediction. Consider for example the figure's diagonal in Pythia. Cells in a diagonal share a group of models and each row adds another one to F. Evidently this specific group hurts results, even when larger models are added to F. With enough models (bottom of diagonal), the negative decreases. Switching the model (next column) also removes the negative effect. Moreover, across all rows the tendency is never monotonic, implying larger models do not not ensure better predictions.\nBut in general, we see that increasing the number of models tends to improve prediction. For example, in GPT3 the best predictions are with many models. Perhaps intuitively, adding a larger model and improving both #params and number of models aspects improves quite consistently"}, {"title": "WHAT PARAMETERS DO I ACTUALLY NEED TO ESTIMATE?", "content": "Scaling laws might have fewer degrees of freedom than described in the literature.\nAssuming we do not try to account for aspects other than #toks and #params one might wonder if some of the observed errors come from model misspecification\u2014an incorrect functional form for \\( \\hat{L} \\), which (with a small number of exceptions including Caballero et al.) has generally gone uncontested since it was first proposed (Rosenfeld et al.; Hoffmann et al., 2022). Here we specifically evaluate whether scaling laws empirically exhibit fewer degrees of freedom than has been proposed. First, we compute the principal components of the 5 learned parameters and find that 3 components explain 99.49% of the variance between the 5 parameters. Inspection reveals that two of these components tightly couple the pairs of parameters dealing with the same training parameter (#params and #toks). Plotting values of A against \u03b1 and of B against \u03b2 we see a clear linear relationship between these variables despite ther non-linear interaction in Eq. 1. There are a few exceptions: the Encoder-Decoder model T5-Pile shows a different behavior from the rest of the scaled families, and four additional scaled families show a different relationship between B and \u03b2. In fact, all these families share the common feature that they were trained using multiple passes over a single training set The outlier point with \u03b2 > 4 is a 70m baseline of Pythia for a continual training intervention experiment Future work may consider different function forms tying some of the parameters or introducing other ones instead.\nAnother change for the function form that future work should consider is accounting for the learning rate schedule, as our experiments assumed it was negligible. A mismatch between the form and the real dependence might explain the inconsistencies in using the beginning of training."}, {"title": "RELATED WORK", "content": "This work builds on a large number of recent studies relating scaling law estimation and decision-making about model training. Among the aspects studied are total training costs including inference effects of sophisticated data selection training time transfer of learned skills behavior of models in other modalities mixtures of experts data mixing downstream performance vocabulary size and architecture comparisons including small models or other phenomena like finetuning and the loss in different positions in the training sequences Especially relevant to our context is that rely on multiple pretraining settings for creating scaling laws that generalize across models or kinds of losses.\nAnother line of works that can be seen as a scaling law discusses the relation between model width and hyperparameter choices rather than loss"}, {"title": "LIMITATIONS", "content": "Our use of ARE as a primary evaluation metric does not distinguish between over-estimation or under-estimation of performance. When using scaling laws to choose between candidate models to train, these error estimates may be unnecessarily conservative (e.g. if both families' laws are biased in the same direction).\nAnother major limitation in this study is the difficulty of aggregating information across model families. As most published families evaluate models of incomparable scales, often over incomparable ranges, we were unable to produce an informative version of Fig. 1 that aggregated information across all models available, and was thus able to give general recommendations about compute-optimal choice of preliminary experiments."}, {"title": "DISCUSSION", "content": "This paper provides a first study of open questions in the estimation of scaling laws and their relation to large-scale pretraining decisions. We expect that many of these conclusions could be sharpened or extended with the availability of additional information about model training, and we call on other leaders of large-scale training efforts to share training losses and evaluation results from multiple checkpoitns during pretraining-even in cases where model parameters themselves cannot be released.\nOur findings leave open many important questions, from performing efficient predictions by fitting on many model families to scaling laws of the deltas between a/b test for a change in attribute (e.g. optimizer) or generalize from one a/b test to another, and to other methods of efficiently compare architectures that do not rely on multiple models (e.g. continual learning). In addition, our results in \u00a79 suggest other scaling law parameterizations might better fit data."}, {"title": "SCALE UP WITH 1 MODEL", "content": "We bring errors of data from fitting from a single model on a given percentage of training to the largest model with full training. Scaling is constant and follows the literature and the largest model stands as target model (so the bottom line in each figure represents predicting from the beginning of training)."}, {"title": "LOSS CURVES AND PREDICTIONS", "content": "We provide in Fig. 7 graphs of the loss during training of the target models per originating source (e.g., a paper) together with the predictions by using different percentage of the training."}, {"title": "IS SCALING WORKING ONLY UPWARDS?", "content": "No. Small models usually show consistent and predicatable performance.\nUsually, one does not use a scaling law to extrapolate to a smaller model as one can just train the small model. However, under observational scaling laws, where one wants to research a phenomenon without scaling at all or when many models were trained and one wishes to create smaller models for various reasons scaling down might prove useful. Moreover, in the context of traditional scaling laws this may act as a baseline. Such an experiment may shed another light on the number of models |F| versus their size #params. If large models are better because they are more stable or otherwise fit laws more robustly, few models will be enough, if the number of models or scale down difference from the prediction, it will show similar behaviour to scaling up. See more in \u00a78.\nTo test this we reverse the order of models and predict with the largest models the loss on the smallest models. This means that for example in the case of 3 models, we predict the smallest model's loss and fit the scaling law relying on the 3 largest models. As before, we break the results by the percentage of training done and do not reverse it.\nAs shown in Fig. 8, the number of models plays an important role in fitting well and a minimum of 30-40% of the training is necessary for good fit, more than that often improves further."}, {"title": "CAN WE DETECT BAD MODELS TO FIT ON?", "content": "If so, not through cross validataion.\nIn \u00a77, we raise the issue of instability of scaling law predictions, with a single model vastly changing the results. We tried to see if, without knowing the ARE, we could remove bad models from the prediction. We hypothesized that models that we can't predict would mean models that would skew our predictions when fitted upon. We performed a cross-validation on the #params families in F each time setting the models with most #toks as target ans exclusing the #params family from F. Our hypothesis was found to be incorrect. Such cases of hard-to-predict models were found to indicate that the models left in F are bad predictors and not that the target is very dissimilar (a \"bad\" training). In 58% of the cases removing that model from the scaling law created the worst ARE possible on the actual target, more than removing any other model."}]}