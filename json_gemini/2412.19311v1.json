{"title": "XSRL: Safety-Aware Explainable Reinforcement Learning - Safety as a Product of Explainability", "authors": ["Risal Shahriar Shefin", "Md Asifur Rahman", "Thai Le", "Sarra Alqahtani"], "abstract": "Reinforcement learning (RL) has shown great promise in simulated environments, such as games, where failures have minimal consequences. However, the deployment of RL agents in real-world systems-such as autonomous vehicles, robotics, UAVs, and medical devices-demands a higher level of safety and transparency, particularly when facing adversarial threats. Safe RL algorithms aim to address these concerns by optimizing both task performance and safety constraints. However, errors are inevitable, and when they occur, it is essential that RL agents can explain their actions to human operators. This makes trust in the safety mechanisms of RL systems crucial for effective deployment. Explainability plays a key role in building this trust by providing clear, actionable insights into the agent's decision-making process, ensuring that safety-critical decisions are well understood. While machine learning (ML) has seen significant advances in interpretability and visualization, explainability methods for RL remain limited. Current tools fail to address the dynamic, sequential nature of RL and its need to balance task performance with safety constraints over time. The re-purposing of traditional ML methods, such as saliency maps, is inadequate for safety-critical RL applications where mistakes can result in severe consequences. To bridge this gap, we propose xSRL, a framework that integrates both local and global explanations to provide a comprehensive understanding of RL agents' behavior. In addition, xSRL enables developers to identify policy vulnerabilities through adversarial attacks, offering tools to debug and patch agents without retraining. Thus, xSRL enhances the RL safety as a byproduct of explainability and transparency. Our experiments and user studies demonstrate xSRL's effectiveness in increasing safety in RL systems, making them more reliable and trustworthy for real-world deployment.", "sections": [{"title": "1 INTRODUCTION", "content": "Reinforcement learning (RL) has demonstrated significant potential in simulated environments, such as games and simplified settings, where failures have minimal consequences. However, deploying RL agents in real-world systems such as autonomous vehicles, robotics, UAVs, and medical devices-introduces a much higher risk, as failures can lead to severe repercussions. Thus, training RL agents in these settings requires a greater emphasis on safety, especially under adversarial conditions. Addressing these concerns, the field of safe RL has emerged, developing algorithms that explicitly optimize both task performance and safety constraints [1, 5, 7, 16, 18, 22, 31, 33]. While these algorithms aim to enhance both safety and accuracy, errors are inevitable, and when they occur, it becomes crucial for the agent to explain its behavior to human practitioners. Practitioners must discern whether an agent is behaving correctly, or is malfunctioning and thus requires intervention. Therefore, user trust in these safety approaches is vital; without it, practitioners may disregard the systems, undermining their effectiveness. Explainability fosters this trust by allowing practitioners to query the reasoning behind safety-related decisions, alongside the agent's learned policy. This additional transparency helps make safety decisions more actionable and strengthens practitioners' confidence in the RL system. In this work, we hypothesize that safety in RL is intrinsically tied to explainability and transparency, which allow the users to gain a clear understanding of agent behavior, enable efficient debugging and address its safety concerns.\nThe need for explainable RL (XRL). In machine learning (ML), significant progress has been made in developing interpretation and visualization methods to help users understand a ML model's performance, track its metrics [27], generate data-flow graphs for its decision-making [39], and visualize representations it has learned [36]. However, interpretation tools designed specifically for RL agents are still limited, most of which only offer basic capabilities such as behavior summarization [3], contrastive explanations [37], or short video clips at critical time steps, but without providing any in-depth reasoning about the RL agents' behavior. While re-purposing ML interpretation methods (e.g., saliency maps [30]) might seem feasible, their design intent is insufficient for explaining RL agents, especially in safety-critical environments. RL agents must not only optimize task performance but also adhere to strict safety constraints, as any mistake can lead to significant harm to agents or human workers. Existing ML explainability tools focus on individual, static decisions, which fails to capture the sequential and dynamic nature of RL, where actions impact future states and they require balancing immediate rewards with long-term risks. Additionally, RL agents often operate under stochastic policies and delayed feedback, making it difficult to explain how they learn to manage the trade-off between exploration and exploitation while maintaining safety. Therefore, safety-aware explainability must address both task performance and the agent's adherence to safety constraints, ensuring that every action is evaluated in terms of both its short- and long-term benefits and risks-a complexity that re-purposed ML tools are not well-equipped to handle.\nDesiderata. Effective XRL methods for safety must meet key criteria to ensure usability and trustworthiness. First, they must provide global explanations of the agent's policy, offering insights into its behavior across the state space [10, 21, 35, 40], especially for end-users unfamiliar with RL. Second, they should deliver local explanations, detailing the agent's reasoning at specific timesteps. Third, explanations must be interpretable, preferably in natural language, with minimal user effort. Fourth, the method must ensure explanation fidelity, validated through tests [20] to confirm alignment with the agent's actual policy. Finally, XRL should include adversarial explainability, analyzing how decisions change under adversarial conditions to identify vulnerabilities and improve robustness. Beyond user trust, XRL should provide utilities for RL users and developers, such as debugging, testing, and patching policies to enhance safety and performance [6, 12, 24]. Currently, no tools or frameworks address all these needs.\nProposal. To address the desiderata, we propose xSRL, a novel framework integrating local and global explanations to enhance RL safety through transparency and explainability. xSRL explains safety violations by showing how task requirements or safety constraints influence decisions. xSRL introduces a novel local explanation method, training two critics, $Q_{task}$ and $Q_{risk}$, to estimate returns and risks for a state $s_t$, action $a_t$, and policy $\\pi$. These post-hoc Q-functions apply to any fixed policy without accessing its internal structure. For global explanations, xSRL extends our previous work, CAPS [21], enhancing nodes (abstract states) with $Q_{task}$ and $Q_{risk}$, and annotating edges (actions) with motivations rooted in task or safety requirements. xSRL also provides adversarial explainability, enabling developers to debug and patch policies by identifying vulnerabilities and analyzing behavior under risk. Developers can launch adversarial attacks and improve safety without retraining the policy. To our knowledge, xSRL is the first framework combining local and global explanations to address RL agent safety and the first to offer adversarial explanations for vulnerability analysis and policy patching.\nKey Contributions. We show that xSRL' explanations provide the users with both trust and utility to understand and protect RL agents in critical scenarios. Trust is measured computationally through fidelity tests and empirically via user studies. Fidelity tests assess the alignment between the explanation graphs and the agent's actual policy across two benchmark environments in safe RL [26, 33], using safety techniques [26, 31, 33] to patch vulnerable policies. Results indicate that xSRL generates accurate policy graphs with less than 33.5% error in risk estimation, even under adversarial attacks. To further validate xSRL's effectiveness, we compared xSRL against local and global explanations by evaluating users' comprehension of xSRL explanations for both vulnerable and safe policies under adversarial attacks in a user study with 9 distinct conditions. These conditions represented various combinations of local and global explanations with different safe RL patching methods. Both xSRL and global explanations (CAPS [21]) achieved comparable accuracy, demonstrating the superiority of global explanations (xSRL and CAPS) over local explanations in providing clearer insights in high-risk contexts. However, xSRL offers a distinct advantage over CAPS by facilitating debugging and patching for safety constraints. We also evaluated the impact of explanation-guided adversarial attacks on an agent trained with the soft-actor-critic (SAC) algorithm [9], where the attack reduced the agent's safety by approximately 72% under a 50% attack rate. After identifying vulnerabilities in the SAC agent's policy, we patched it with a safety shield [2] and a safe policy, significantly improving the agent's safety. Lastly, user studies assessed participants' ability to distinguish the safer agent between two alternatives, confirming xSRL's utility in enhancing safety and providing actionable insights. Overall, xSRL effectively increases the trustworthines and utility of RL agents for real-world, safety-critical applications by identifying and resolving policy vulnerabilities."}, {"title": "2 RELATED WORK", "content": "XRL methods have evolved to offer insights into the behavior of RL agents, with approaches generally categorized into local and global explanations [25]. Local explanations focus on specific actions taken by an agent at a point in time, often using post-hoc methods like saliency maps [8, 11, 14], which highlight key features that influence the decisions. Intrinsic methods, such as reward decomposition [15], build explainability into the agent's decision model itself by breaking down the Q-value into components that reveal the agent's motivations at each time step. These approaches help in understanding why specific decisions are made but do not provide insight into the satisfaction or violation of safety constraints explicitly.\nGlobal explanations attempt to describe an agent's overall strategy by summarizing its behavior across various states. Recent work introduced the concept of \"agent strategy summarization\" [3, 4], where an agent's behavior is demonstrated through its actions in a carefully chosen set of states. The key challenge in this approach is how to select the most crucial state-action pairs that effectively portray the agent's behavior, allowing users to anticipate how it may act in new scenarios. Techniques such as HIGHLIGHTS [3] identify important states based on the impact of decisions on the agent's utility, while other methods use machine learning to optimize state selection [13, 17]. These summaries reduce the effort required for humans to understand the agent's behavior while still providing comprehensive information about its capabilities.\nCombining local and global methods, such as integrating strategy summaries with saliency maps [14], has shown promise but lacks the depth needed for understanding safety-oriented decisions"}, {"title": "3 BACKGROUND", "content": "For the purpose of explaining safety of RL agents, we consider the standard Constrained Markov Decision Processes (CMDPs), $M = (S, A, \\mu, P(.|., .), R, \\gamma, C)$ where S and A denote the state and action space; $\\mu$ and $P : S \\times A \\times S \\rightarrow [0, 1]$ denote the initial state distribution and state transition dynamics, respectively. $R : S \\times A \\rightarrow R$ is the reward function; $\\gamma$ denotes discount factor; and $C = {c_i : S \\times A & R > 0; i = 1, 2, .., T}$ denotes the set of cost associated with safety constraint violations in any trajectory episode $t = {s_0, a_0, ..., a_{T-1}, s_T }$ with a maximum trajectory length of T. We assume that either accomplishing the task goal or violating a safety constraint in M leads to episode termination. The objective of the agent's policy, we call it task policy hereafter, task is to learn the optimal control to maximize the expected discounted reward at time t; $R_t^{atask} = E[\\sum_{i=t}^{T}\\gamma^{i-t} r_i]$. The task policy $\\pi_{task}$ is the solution to the CMDP.\nProblem Statement. Our ultimate goal is to provide a comprehensive explanation of an RL agent's behavior. Consider an RL task solved by an agent trained with either value-based algorithms such as DQN [23] and DDQN [38] or policy-based algorithms such as PPO [29] or TRPO [28]. This paper aims to explain this agent's policy by summarizing its overall strategy to solve the task. Formally, given N episodes $T = {X^{(i)}, r_i, c_i}_{i=1:N}$ of the target agent, $x^{(i)} = { (s_t^{(i)}, a_t^{(i)}, r_t^{(i)}), c_t^{(i)} }_{t=1:T}$ is the i-th episode of length T, where $s_t^{(i)}$ is the state, $a_t^{(i)}$ is the action, $r_t^{(i)}$ is the reward, and $c_t^{(i)}$ is the cost, at time t in episode i. Our goal is to generate a summary of these episodes as a graph accompanied by estimated values of Q-functions for the task reward and the cost of safety violations.\nBaseline. In this paper, we build on our previous method, CAPS [21], a recently introduced global explanation XRL method that has been successful in providing comprehensible Summaries of RL policies, as our main comparison baseline. CAPS collects natural language (NL) predicates from the user and gathers up to 500 timesteps from the RL agent's trajectories. To simplify the explanation process, it uses the CLTree [19] clustering algorithm to abstract the agent's states into clusters, forming a hierarchy. A heuristic optimization then selects the best cluster configuration based on state transition accuracy and user interpretability. CAPS constructs the agent's policy ($\\pi$) as a directed graph G = (V, E), where set of nodes V represent abstract states (clusters), and set of edges E show the agent's actions and transition probabilities. CAPS enriches the graph by labeling abstract states with English explanations based on user-defined predicates and boolean algebra. We chose CAPS [21] because it offers a comprehensive global explanation, revealing the agent's policy across the state space rather than focusing on specific states. It also outperforms other global XRL methods [35, 40] in fidelity and user comprehension.\nWhile CAPS [21] can explain an RL agent's policy, it cannot address the impact of safety violations on the agent's behavior, nor does it enable users to debug specific safety concerns. xSRL addresses these limitations by providing safety through enhanced explainability and transparency."}, {"title": "4 APPROACH: SAFETY-AWARE EXPLAINABLE RL METHOD", "content": "In this section, we present our approach, so-called xSRL, which uniquely combines a novel local explanation method with an extended version of the global explanation framework CAPS [21] to explain an RL agent's decision-making process at both individual states and its overall strategy to achieve a comprehensive consideration for both task and safety requirements."}, {"title": "4.1 Safety Interpretation via Integrating Local and Global Explanations", "content": "Local Explanation Method. In XRL, reward decomposition [15] is used to reveal the reasoning behind an agent's actions in specific states by decomposing the reward into different components. This method, which separates rewards into individual reward components values $R_c(s, a)$, highlights the factors influencing an agent's decisions at each timestep. However, such approach is not built to explain safety for RL agents, which can be optimized in a separate objective function through joint optimization [22, 31, 34] or provided through a separate policy from the task policy [26, 33]. To bridge this gap, we propose a local explanation method that can explicitly explain safety in RL. Our local explanation method consists of 2 components: task reward estimation and risk estimation at each (s, a) pair.\nTo estimate the future risk probability of a safety-constrained task policy $\\pi_{task}$, we train a separate risk-critic $Q_{risk}$ that evaluates the safety of a given state-action pair independently of task objectives. This risk-critic function $Q_{ttask}(s, a)$, learned via:\n$Q_{ttask}(s, a) = E_{\\tau \\sim \\pi_{task}(st)} [\\sum_{t'=t}^T \\gamma^{t'-t} c(s_{t'}, a_{t'})],$ (1)\nprovides a way to assess the safety constraints of actions taken by the task policy. Here, $c(s_t, a_t)$ denotes the cost associated with violating safety constraints when taking action $a_t$ in state $s_t$. In practice, we approximate $Q_{risk}^{task}$ parameterized by $\\phi$, using sampled transitions $(s_t, a_t, s_{t+1}, c_t)$. This is done by minimizing the following MSE loss with respect to the target (RHS of Eq. 1):\n$J_{risk} (s_t, a_t, s_{t+1}; \\phi) = \\frac{1}{2} ( (Q_{risk}(s_t, a_t) - E_{a_{t+1} \\sim \\pi(a|s_{t+1})} [\\gamma Q_{risk}(s_{t+1}, a_{t+1})] )^2,$ (2)\nTo estimate the future task reward for a task policy $\\pi_{task}$ at each state, we train a separate task-critic $Q_{task}$ similar to $Q_{risk}$ in Eq.1 but for the task reward r instead of safety cost c:\n$Q_{task}^\\pi(s, a) = E_{a_{t+1:T} \\sim \\pi(a|s_{t:T})} [\\sum_{t'=t}^T \\gamma^{t'-t} r(s_{t'}, a_{t'})],$ (3)\nSince value-based RL algorithms, in general, and actor-critic policy-based algorithms estimate Q-function for the task policy, we can directly utilize their $Q_{task}$ to explain their performance at each state in terms of task requirements.\nGlobal Explanation Method. To enhance the effectiveness of local explanations, we propose integrating both $Q_{task}$ and $Q_{risk}$ into the global explanation method, CAPS [21]. This approach allows users to understand how the agent balances task objectives with safety constraints while presenting its overall strategy across episodes. CAPS summarizes the agent's policy in a directed graph where nodes represent abstract states and edges represent actions along with their transition probabilities, as described in Section 3. We choose to present xSRL explanations in a directed graph format because it effectively illustrates the relationships and dependencies among states, actions, and outcomes; their causal and safety relationships; and the progression from one state to another based on specific actions. However, using CAPS alone as a global explanation tool did not provide sufficient insight for users to determine why the agent chose a particular action at a specific state, as demonstrated in our user studies (Section 5).\nOur method xSRL improves CAPS graph by: (1) incorporating local $Q_{task}$ and $Q_{risk}$ to show task and risk estimation at each abstract state, and (2) explicitly indicating whether each action is driven by task or safety considerations. We hypothesize that combining global policy summaries with Q-function decomposition will significantly enhance user understanding of agent safety, compared to relying solely on local or global explanations, as shown in our evaluation section. To compute $Q_{risk}(B, a)$ and $Q_{task}(B, a)$ for an abstract state B and action a, we average $Q_{risk}(s, a)$ and $Q_{task}(s, a)$ over all concrete states $s \\in B$ and all possible actions a from s:\n$Q_{task}(B, a) = \\frac{1}{n} \\sum_{i=1}^n E_{a \\sim \\pi(.|s)} Q_{task}(s, a),$ (4)\n$Q_{task}(B, a) = \\frac{1}{n} \\sum_{i=1}^n E_{a \\sim \\pi(.|s)} Q_{risk}(s, a),$ (5)\nwhere n represents the total number of concrete states within the abstract state B. These values are attached to the abstract state B in the directed graph, showing how the agent's policy $\\pi$ evaluates task satisfaction (Eq.4) and safety constraint violations (Eq.5) when taking action a from state B. The second enhancement we added to CAPS will be discussed in Section 4.3."}, {"title": "4.2 Safety Debugging via Adversarial Explanation", "content": "This section illustrates how xSRL can be used to provide so-called adversarial explanations to help users in discovering and debugging vulnerabilities in RL policies. Specifically, we demonstrate how users, using the information revealed by xSRL, can launch adversarial attacks to explore potential pitfalls of an RL agent. Through xSRL's graphs, users can also explain the agent's mistakes and its violations of safety constraints, allowing them to formulate a remediation policy that improves the agent's original behavior (discussed in Section 4.3).\nTo initiate an adversarial attack, we first collect 500 episodes (t) from the target agent and explain them using xSRL's graph G. Next, using G, we identify the top-K safety-critical states across all episodes, defined as follows:\nDefinition 1: Safety-Critical State. Given a policy $\\pi_{task}$, a state $s_c$ is a safety-critical state iff there is at least one action a chosen by $\\pi_{task}$ such that:\n$Q_{task}(s_c, a) > E_{safety},$ (6)\nwhere the set of all safety-critical states $C$ is $s_c \\in S$. Finally, we run the agent for another 500 episodes, forcing it to take adversarial actions at the common critical states C identified by G at varying rates (as we show in Section 5), while collecting its trajectories $T_A$. These adversarial actions are generated using the Alternative Adversarial Action (AAA) attack [32], employing a pre-trained adversarial policy $\\pi_{adv}$ from [26] to select alternative adversarial actions $a_{adv} \\sim \\pi_{adv}(s_t)$. We then use xSRL to generate explanation graph for the agent under attack $G_A$, based on $T_A$. By contrasting both graphs, G and $G_A$ without and with the adversarial attacks, respectively, users can pinpoint the safety vulnerabilities in the agent's policy by noticing the overall graphs and their attached $Q_{risk}$ and $Q_{task}$ values."}, {"title": "4.3 Patching Explanation-Based Discovered Vulnerabilities", "content": "This section demonstrates how xSRL can guide the patching process for vulnerabilities discovered in RL policies through adversarial explanations. To avoid retraining the task policy $\\pi_{task}$, we propose a simple approach using an auxiliary policy that optimizes only the safety violation cost function c. Specifically, we employ the safety policy $\\pi_{safety}$ from [26], which is trained by maximizing the KL-divergence from an adversarial policy that increases safety violation costs. We then adopt the post-posed shielding strategy from [2] during online execution to shield the states with higher $Q_{risk}$ from attacks. To implement the safety shield, we leverage the risk-critic $Q_{risk}$ trained for explainability in (Eq.1) as:\n$Shield(s_t, a_t): Q_{risk}(s_t, a_t) > T_{safety},$ (7)\nwhere $T_{safety}$ is a predefined threshold value such that at any state $s_t$ and for any action $a_t$ selected by $\\pi_{task}(s_t)$; if $Shield(s_t, a_t)$ is triggered, then the shield replaces the selected action $a_t$ by a safer action given by the safety policy $a_{safe} \\sim \\pi_{safety}(s_t)$. The value of $T_{safety}$ is environment-specific and can be chosen based on a sensitivity test for each environment."}, {"title": "5 EVALUATION", "content": "In this section, we evaluate xSRL based on two key objectives: trust and utility. Trust assesses whether the explanations generated by xSRL are comprehensible to end-users, while utility measures the effectiveness of these explanations in identifying and resolving vulnerabilities in the agent's policy.\nTasks: We conducted our experiments and user studies in two continuous MuJoCo CMDP environments [33]: (1) Navigation 2, and (2) Maze. In both environments, the state-actions spaces are continuous and the agent's objective is to reach a goal state while avoiding collisions with obstacles, walls, or boundaries. For each task, we used a well-trained SAC agent [9] as the target. We present our results for Navigation 2 in the main paper, with the results for Maze provided in Appendix D.\nExplanation Baselines: As discussed in Section 4.1, XRL methods can be classified into two broad categories: (1) local explanations and (2) global explanations. For comparison, we select two representative baselines. The first baseline is our local explanation method, which presents users with $Q_{task}$ and $Q_{safety}$ values at specific time steps. The second baseline is CAPS[21], which generates a directed graph summarizing the agent's overall policy.\nSafety Patching Baselines: We employed three different safe RL methods as patching techniques. First, we used the safety policy $\\pi_{safe}$ developed in AdvExRL [26] and RRL-MF [33], separately. These methods represent approaches that optimize safety separately from task performance. Second, we tested SQRL [31], which exemplifies joint optimization of both task performance and safety. Details of these methods are given in Appendix A."}, {"title": "5.1 Trustworthiness of xSRL's Explanations", "content": "We evaluate the trustworthiness of xSRL explanations using two approaches: computational fidelity scores and user studies."}, {"title": "5.1.1 Fidelity.", "content": "Overview. Fidelity measures how accurately the produced explanation reflect the true behavior of the RL agents. The higher the fidelity, the better alignment between the graph and the agent's behavior, the better the explanation. We calculate fidelity for four components of the agent that xSRL explanations capture: (i) action, (ii) policy selection, (iii) $Q_{risk}$, and (iv) $Q_{task}$. The action fidelity is measured by the proportion of actions taken by the RL agent that match the actions predicted by the produced explanation graph [20]. Similarly, policy selection fidelity measures the extent to which the graph correctly identifies the policy used by the agent to make decisions. $Q_{risk}$ and $Q_{task}$ fidelities measure how closely xSRL's local explanations match the values produced by the agent's policy. We use the weighted average of the Normalized Root Mean Squared Error (NRMSE) to compute the difference between the $Q_{risk}$ and $Q_{task}$ estimates generated by xSRL and those from the agent's policy. A lower NRMSE indicates a better explanation.\nTo compute all the fidelity scores, we simulate around 2,000 timesteps of agent-environment interactions for several agents: SAC [9] without attacks, SAC under a 50% attack, SAC patched with the safe policy $\\pi_{safety}$ from AdvExRL [26], the safe policy from RRL-MF [33], and the SQRL [31]-trained agent. We generated and averaged fidelity scores across five graphs for each agent."}, {"title": "5.1.2 User Studies.", "content": "Overview. To empirically assess users' trust in xSRL explanations, we conducted three user studies to evaluate the impact of combining local and global explanations in xSRL, as well as the effect of each of two levels of explanation individually. Each study used three different patched agents (AdvExRL, RRL-MF, and SQRL), resulting in a total of nine distinct conditions. As local explanations apply to specific states, we selected states with the highest $Q_{risk}$ to show users the agent's behavior in risky scenarios.\nProcedure. We recruited 270 participants (30 per study), consisting of sophomores from various majors at our institution and participants from Prolific, with IRB approval. We applied specific filters to ensure relevant participant backgrounds. Each participant was randomly assigned to one study to avoid crossover effects. Initially, participants were introduced to Navigation 2 environment, followed by an explanation of the safety constraints, potential attacks on the agent, Q-values, and key study terms (in layperson language). Participants were then shown three videos of the RL agent: one where the agent succeeds without an attack, one where the agent fails under attack, and one where the agent succeeds under attack with a safety patch. After each phase, participants answered four questions across two scenarios: one where the agent was under attack without a safety mechanism and one where the agent was patched with a safety mechanism. The first three questions 1 were: (i) Q1. What action will the agent likely take in a state with high risk? (ii) Q2. Why did the agent choose that action? (iii) Q3. Will the agent succeed (reaching the goal safely)? Q1 evaluates participants' basic comprehension of the explanations and the impact of Q-values ($Q_{task}, Q_{risk}$). Q2 assesses whether participants can identify the primary motivation behind the agent's action at a given state evaluating whether the decision was made to achieve the goal, avoid a risk area, respond to an adversarial attack. This evaluation provides insight into the participant's comprehension of the agent's motivations rather than an analysis of the action's overall effectiveness which is measured by Q3. Navigation 2 environment was selected because it provides a sufficient level of complexity that would allow participants to focus on understanding the safety aspects of the agent's behavior rather than being distracted by learning the environment itself. Participants were compensated with a base payment of $1.5, plus an additional bonus of 10 cents for each correct answer.\nHypotheses. We will test two hypotheses:\nH1: We hypothesized that using only local or global explanations would be less effective in helping users identify why the agent made certain decisions and whether or not it would succeed under attack, compared to xSRL's combined approach. In other words, participants presented with local or global explanation methods alone will struggle with Q1-Q3 for both safe and unsafe agents, while those presented with xSRL's explanation will perform better."}, {"title": "5.2 Utility of xSRL's Explanations", "content": "This section evaluates the utility of xSRL's safety explanations in identifying and resolving vulnerabilities in the agent's policy using both computational and empirical approach. The computational approach involves (1) assessing the impact of the explanation-guided attack developed in Section 4.2, and (2) measuring the effectiveness of patching techniques in improving the agent's safety under the same attack. The empirical approach involves analyzing the final question in our user studies to determine whether participants can distinguish between safe and unsafe agents."}, {"title": "5.2.1 Impact of Explanation-guided Attack and Patching Techniques.", "content": "The explanation-guided attack (AAA) is implemented by targeting abstract states that exhibit higher $Q_{risk}$ values in the xSRL graph of the SAC agent. We attacked 0, 25, 75 and 100% of the concrete states clustered within these high-risk abstract states. Figure 3b shows the behavior of the SAC agent under attack, without any safety mechanism. To measure the attack's impact, we used two metrics from [26], including $Safety(\\%)$, which quantifies the proportion of time the agent acts safely over its maximum episode length, and $Success-Safety(\\%)$, which indicates how close the agent is to reaching its goal. An agent is deemed successful if it finishes the episode within a predefined minimum distance from the goal. For the distance threshold, we used the same value specified in [26] for Navigation2 environment. Figure 2 presents the impact of the attack on SAC without a safety mechanism, as well as patched agents using AdvExRL, RRL-MF, and SQRL, all under 0-100% attack rates. The results clearly show that SAC suffers the most, as its vulnerability is evident in the xSRL graph for its behavior. In contrast, AdvExRL is the safest, as indicated by the minimal impact of the attack on its safety and success, which is also visually reinforced by its xSRL explanation graph."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduced xSRL, a framework that combines local and global explanations to address safety constraints in reinforcement learning (RL) agents. Our approach aims to improve the interpretability of RL policies, with a focus on safety-related behavior, offering developers insights to identify and address vulnerabilities in agent policies. By utilizing policy abstraction and safety-specific visualizations, xSRL provides a clearer understanding of agent decisions and helps users refine policies based on safety concerns. Through computational and empirical evaluations, we showed that xSRL enhances both trust and utility, offering a useful tool for RL policy testing and refinement in safety-critical environments. These results highlight the value of safety-aware explanations in supporting more effective RL system development and user interaction."}]}