{"title": "Generating novel experimental hypotheses from language models: A case study on cross-dative generalization", "authors": ["Kanishka Misra", "Najoung Kim"], "abstract": "Neural network-based language models (LMs) have been shown to successfully capture complex linguistic knowledge. However, their utility for understanding human language and more broadly, human cognition, is still debated. We contribute to this debate by presenting a case study where we use LMs as simulated learners to derive novel experimental hypotheses that may then be tested with human learners. We apply this paradigm to study productive generalization of novel verbs across two dative constructions (DO: she pilked me the ball; PP: she pilked the ball to me)\u2014acquisition of which is known to involve a large hypothesis space of contextual features\u2014using LMs trained on transcripts of child-directed speech. We specifically ask: \u201cwhat properties of the training exposure facilitate a novel verb's generalization to the (unmodeled) alternate construction?\u201d To answer this, we systematically vary the exposure context in which a novel dative verb occurs in terms of the properties of the theme and recipient, and then analyze the LMs' usage of the novel verb in the unmodeled dative construction. As a precondition to determine their utility for deriving novel hypotheses, we first test experimental conditions analogous to prior human studies, where we find LMs to replicate known patterns of children's cross-dative generalization. Our subsequent simulations reveal a nuanced role of the features of the novel verbs' exposure context on the LMs' cross-dative generalization. In particular, we find generalization to be facilitated when the first postverbal argument of the exposure context is pronominal, definite, short, and conforms to the prototypical animacy expectations of the exposure dative (i.e., inanimate theme/animate recipient). These patterns are characteristic of harmonic alignment in dative constructions observed in prior work, where the argument with features ranking higher on the discourse prominence scale tends to precede the other. This gives rise to a novel hypothesis that cross-dative generalization is facilitated or preempted based on the extent to which the features of the exposure context-in particular, its first postverbal argument are harmonically aligned. Based on these findings, we sketch out future experiments that can test this hypothesis in child learners.", "sections": [{"title": "1. Introduction", "content": "Recent advances in Artificial Intelligence powered by language models (LMs) trained at scale have generated a series of discussions about their role in (human) cognitive science (Ambridge, 2020; Piantadosi, 2023; Portelance and Jasbi, 2023; Kodner et al., 2023; McGrath et al., 2023, i.a.). A significant portion of such discussions are not specific to LMs as a training objective per se, and are more broadly applicable to the role of artificial neural networks (ANNs) (Pater, 2019; Baroni, 2020; Warstadt and Bowman, 2022). Overall, both the arguments for and against the utility of LMs or ANNs are heavily reminiscent of debates from the second wave of connectionism (McClelland, 1988; Massaro, 1988; Fodor and Pylyshyn, 1988; Smolensky, 1991; McCloskey, 1991; Hadley, 1997, i.a.). Questions about whether connectionist models/ANNs/LMs count as theory, whether their successful replication of human behavior has any implications for advancing our understanding of human cognition, and discussion about their bearings on learnability arguments, are some recurring themes in these debates. While there are disagreements about the utility of contemporary LMs for cognitive investigations along these dimensions and more, one general consensus is that cautions must be taken in accepting them directly as models of human cognition (Guest and Martin, 2023), despite their strong predictive capabilities of behavioral (Goodkind and Bicknell, 2018; Wilcox et al., 2020; Shain et al., 2024, i.a.) and neural data (Schrimpf et al., 2021; Goldstein et al., 2022, i.a.). This is mainly due to sparse linking hypotheses between components of the human cognitive capacity and components of widely adopted models such as architecture, data, training process, and input/output representations, leading to limitations in explanations that can be offered by studies of these models alone.\nNevertheless, following McCloskey (1991), we argue that treating these models as animal models is a promising way that black box models with high predictive capacity can contribute to cognitive science, and empirically explore this possibility. Specifically, we provide a concrete case study where we use LMs as simulated learners to derive novel experimental hypotheses that can in turn be tested with actual human learners. We expect this approach to be the most fruitful for domains in which large-scale human experiments are challenging, such as child language acquisition. While there are existing human experimental findings implicated by (e.g., predictions of Portelance et al. 2021 being corroborated by concurrent human study of Jara-Ettinger et al. 2022) and motivated by (e.g., findings of Kim and Linzen 2020 motivating the human experiments of Kim and Smolensky 2024) neural network simulation studies, limited work has been explicitly designed for the goal of novel hypothesis generation in the experimental linguistics space (with the rare exception of Lakretz et al. 2021). Furthermore, no such studies to the best of our knowledge exist in the language acquisition literature. This work attempts a proof-of-concept implementation of this idea using the acquisition of dative alternation as a case study."}, {"title": "1.1. Acquisition of Dative Alternation as a Case Study", "content": "The dative alternation in English refers to a phenomenon where both prepositional phrase (PP: 1a) and double object (DO: 1b) constructions are licensed for the same verb. One important difference between the two constructions is the linear ordering of the theme and recipient arguments: in PP, the theme precedes the recipient, and in DO, the recipient precedes the theme.\n(1) a. Najoung gave a treat to Cookie."}, {"title": "b. Najoung gave Cookie a treat.", "content": "However, not all dative verbs participate in this alternation; some only appear to be licensed in either the PP or the DO construction (*I donated the library the book, *He wished luck to me). Hence, learning which constructions a dative verb is licensed in is a problem that language learners face. It has been shown empirically that learners often generalize to the alternate construction even in the absence of direct evidence showing that the verb is licensed in the alternate construction (e.g., Gropen et al. 1989; Arunachalam 2017), rather than being fully conservative and restricting the use of the dative verb to only the observed construction. Then, how does a learner distinguish non-alternating verbs from alternating verbs that they happened to not have observed positive evidence for one of the licensed constructions? Two explanations are possible: first, there may be non-arbitrary criteria that can tease these cases apart (the criteria hypothesis: Gropen et al. 1989), and second, learners may have access to negative evidence that blocks overgeneralization. In this work, we focus on the first possibility and explore the criteria-based resolution to this learning problem\u2014an instance of Baker's paradox (Baker, 1979).\nA large body of prior work explores possible criteria that set apart alternating and non-alternating verbs, ranging from morphophonological to semantic factors (see Citko et al. 2017 for an overview). Many such factors are identified on the basis of distributional evidence (how often do we observe a dative verb with X/Y/Z properties in DO and PP constructions in a large corpus?) and adult acceptability judgments (is a dative verb with X/Y/Z properties acceptable in DO and PP constructions?). These studies are important in understanding which cues are available to the learners, since statistical cues in the input critically shape language acquisition (Saffran et al., 1996; Thompson and Newport, 2007; Romberg and Saffran, 2010). However, the mere availability of distinctive cues does not necessarily entail that they are used by the learners. In this regard, nonce word studies testing whether learners generalize in the absence of observing the verb in the alternate form provide the most direct answer for the causal question: What cues do learners actually use to distinguish alternating verbs from non-alternating verbs? In adult learners, Gropen et al. (1989) identified factors such as possessive semantics and the number of syllables to affect PP to DO generalization, and Coppock (2009) found an effect of number of syllables as well as null results on the effect of prosodic weight and etymology (Germanic vs. Latinate). In children, empirical evidence is sparser; only a handful of studies have investigated generalization to the alternate construction (Gropen et al., 1989; Conwell and Demuth, 2007; Arunachalam, 2017), in addition to several studies that investigate the comprehension of dative structures (Rowland and Noble, 2010; Conwell, 2019) that may speak to the preconditions to generalization.\nThe sparse empirical evidence in children's acquisition of the dative alternation is partly due to the large size of the hypothesis space (combination of a wide range of distributional cues available in the input, as identified in the literature) and the difficulty in recruiting target participants at scale. To this end, we propose to use LMs as simulated learners to systematically explore this hypothesis space to identify a small number of targeted hypotheses, which in turn can be tested with actual"}, {"title": "children. We specifically investigate the role of distributional cues (as opposed to cues from the form of the verb itself) in the LM learners' generalization of novel dative verbs encountered in only one of the alternate constructions. Features of the context that the verb appears in, such as theme/recipient animacy, definiteness and length, have been shown to predict the choice between DO or PP in adult and child production (Bresnan et al., 2007; De Marneffe et al., 2012) and have informed design decisions of nonce verb learning studies (Arunachalam, 2017). However, little is known about how these distributional cues get used by the learners to shape their inference about the alternation pattern of new verbs being learned. Our approach is to compile a list of distributional features discussed in the literature, run simulation studies that quantify the effect of these features on LM learners' generalization, and select interesting hypotheses based on the results with the ultimate goal of testing them with human subjects. One important precondition in order to take the results of the simulation studies as something worthy of analyzing is that we should be able to replicate known findings with the models and simulation methods we adopt. Therefore, we aim to replicate the findings of existing studies (Conwell and Demuth, 2007; Arunachalam, 2017) before exploring novel hypotheses.", "content": "1.2. LMs and Dative Alternation\nDative alternation has been frequently adopted as a testing ground for analyzing structural representations in LMs. For instance, LMs such as GPT-2 (Radford et al., 2019) and Llama-2 (Touvron et al., 2023) have been shown to exhibit effects analogous to structural priming (Bock, 1986) with respect to different dative structures (Sinclair et al., 2022; Jumelet et al., 2024; Zhou et al., 2024). In these studies, LMs assigned greater probabilities to sentences in a particular dative construction when adapted to instances of the same construction\u2014either via backpropagation or prefixes presented to the LM in context. Furthermore, Hawkins et al. (2020) showed that BERT (Devlin et al., 2019) and GPT-2 are sensitive to alternation patterns of known dative verbs (Levin, 1993), matching human alternation preferences. Analogous to humans, LMs also showed sensitivity to argument features such as length and definiteness which are known to predict speakers' dative choice during production (Bresnan et al., 2007; De Marneffe et al., 2012). Finally, a few studies have also investigated dative alternation in LMs from the perspective of novel word learning (Thrush et al., 2020; Petty et al., 2022)\u2014these are the most directly related to our work in terms of methodology. Specifically, Thrush et al. (2020) simulated novel verb learning in BERT, and showed that the model can use novel verbs in alternate constructions, having only been exposed to one of the constructions. This work masked out all content words during the course of BERT's finetuning, opting to instead only expose skeletons of alternating constructions (e.g., to expose BERT to the DO, they used an input like: The [MASK] daxed the [MASK] a [MASK]). As a result, the precise role of the features of the theme and recipient on the model's productive generalization of the novel verb (here, daxed) is unclear. Petty et al. (2022) adopted a similar novel word learning method, but the word being learned was not a verb-instead, it was either the theme or the recipient in a dative sentence. Among other findings, they showed that LMs such as BERT and ROBERTa (Liu et al., 2019) can learn to infer a novel word's argumenthood (theme vs. recipient) based on the position it occurs in a dative exposure, and that recipients are more likely to be animate than inanimate, echoing prior corpus analyses (Bresnan et al., 2007)."}, {"title": "The aforementioned studies show that Transformer-based (Vaswani et al., 2017) LMs are clearly sensitive to different dative structures and their arguments. However, these analyses were conducted on LMs trained on data multiple orders of magnitude larger than the upper bound of the input a child receives, only allowing for weaker implications about human learners. Additionally, existing studies, even those involving novel word learning, do not seek to shed light on the properties of the exposure that license or block cross-dative generalization as we pursue in this work. Finally, no prior work explicitly aims to extract novel hypotheses targeting human learners.", "content": "1.3. LMs and Child-directed Speech\nThis work contributes to a growing body of literature on the analysis of LMs trained on child-directed speech. A number of prior studies (Huebner and Willits, 2018, 2021; Wang et al., 2023; Qin et al., 2024) have trained Recurrent Neural Network (RNN; Elman 1990), Long Short-term Memory (LSTM; Hochreiter and Schmidhuber 1997), and Transformer LMs on transcripts of child-directed speech and found the models to recover clusters of simple semantic and syntactic categories as well as demonstrate sensitivities to various grammatical phenomena. A subset of these studies (Wang et al., 2023; Qin et al., 2024) reported that the aforementioned findings hold true even for LMs trained on text data that is sourced from a snapshot of the linguistic input received by a single child (derived from Sullivan et al. 2021's data). Concurrently, Yedetore et al. (2023) showed that training autoregressive LMs on child-directed speech does not impart them with a hierarchical bias akin to children\u2014instead, LMs preferred linear generalization. In addition, Portelance et al. (2023) used surprisals derived from LMs trained on CHILDES (MacWhinney, 2000) and found them to be better predictors of age of acquisition of children's vocabulary than surprisals extracted from simpler n-gram LMs.\nIn a similar vein, our experiments target the linguistic knowledge and behavior in LMs trained on child-directed speech. One key difference is that we study the acquisition of novel words, which will be affected not only by the linguistic knowledge from the LMs' pretraining\u2014which the aforementioned studies have characterized\u2014but also by the distributional context in which the novel words occur. By systematically varying these distributional contexts, we hope to better characterize the criteria that license productive generalization, and turn them into testable experimental hypotheses for human learners."}, {"title": "2. Methodology", "content": "Our LM learners are trained from scratch on transcripts of English child-directed speech (MacWhinney, 2000; Huebner and Willits, 2021). Below, we describe the data and the choice of the LM architecture used, followed by the method for simulating novel verb learning in the LMs. Then, we describe our hypothesis space and the pipeline used to construct the training and test stimuli."}, {"title": "2.1. Modeling", "content": "Data. We used AO-CHILDES (Huebner and Willits, 2021) as the training dataset for our LM learners. This corpus contains approximately 5M words from the American English portion of CHILDES (MacWhinney, 2000), including children from 0 to 6 years of age. It has been filtered to include only the child-directed portion, and is ordered temporally. Assuming that a soft upper-bound for the amount of linguistic input to an English speaking American child is around 1M words per month (Hart and Risley, 2003; Roy et al., 2015; Dupoux, 2018; Frank, 2023), the AO-CHILDES corpus consists of about 14% of the linguistic input to a 3 year old. We used the train/validation/test splits of AO-CHILDES provided by the BabyLM challenge (Warstadt et al., 2023), a competition for building LMs trained on developmentally plausible amounts of data. There are 4.21M words in the training set (11.7% of the soft upper bound of the linguistic input to an English speaking American child), 400K words in the validation set, and 340K words in the test set. All utterances in AO-CHILDES are in lowercase.\nLM architecture and training. The LMs used in this work are autoregressive, decoder-only Transformers (Radford et al., 2019) trained using the next-word prediction objective-specifically, the OPT architecture (Zhang et al., 2022). However, the main method (\u00a72.2) is agnostic to the model architecture. For our choice of hyperparameters, we primarily followed BabyBERTa (Huebner et al., 2021), an LM trained on AO-CHILDES that showed strong performance on tasks targeting the linguistic competence of English-learning children. We used 8 attention heads, 8 layers, a vocabulary size of 8192, an embedding size of 256, and a feedforward hidden dimension of 1024. This amounts to a total of 8.4M trainable parameters, which is comparable to the number of trainable parameters of the BabyBERTa models (8.5M). For the tokenizer, we again followed BabyBERTa and used a Byte-Pair Encoding-based tokenizer (Sennrich et al., 2016). We re-trained this tokenizer on our training set since the original BabyBERTa tokenizer was trained on a combination of corpora not limited to AO-CHILDES (see Huebner et al., 2021), leading to superfluous tokens not in our training corpus and therefore irrelevant to our investigation. We trained our LMs on the training set of the AO-CHILDES dataset for 10 epochs, and chose the best learning rate based on the validation set. To ensure that our results are not due to idiosyncrasies of a particular training run, we report results for 10 different random seeds\u2014i.e., 10 instances of our LM architecture trained on the same corpus using the same tokenizer, only differing by the initialization of the weights. More details about the architecture and the training configuration can be found in Table A.9."}, {"title": "2.2. Simulating Novel Verb Learning in LMs", "content": "Original method. We are interested in characterizing the generalization behavior of an LM learner after it has been exposed to a novel verb in a particular construction (DO vs. PP). The primary method we use for this investigation builds on the general paradigm first introduced by Kim and Smolensky (2021) to investigate lexical category inference in LMs. The method is inspired by developmental linguistic studies that probe for lexical category abstraction in children (H\u00f6hle et al., 2004) using a head-turn preference procedure (Nelson et al., 1995). The method consists of the following three steps. First, a novel token is introduced to the LM's vocabulary by adding a new randomly initialized vector to the embedding layer of the model. The embedding layer is responsible for mapping lexical items in the inputs to dense vectors that are passed onto higher layers which eventually lead to the LM's output probability distribution over its vocabulary items. Next, the model weights are updated\u2014using the same objective as the one used to train the base LM\u2014for a predefined number of steps on an exposure set that contains sentences with the novel token. After the final update, the state of the model that satisfies some selection criterion is chosen for evaluation; for instance, the state with the best validation loss or accuracy. Importantly, during learning, only the vector for the novel token is updated keeping all other parameters of the model constant, so as to preserve their effect on the novel token as it is being updated. The final selected model state is then evaluated on a held-out generalization set designed to assess the models' use of the novel token. In the original study of Kim and Smolensky, the generalization set consisted of contexts that are lexically disjoint from the exposure examples, with the lexical category (part of speech, the target of their original investigation) of each of the novel tokens completely disambiguated. That is, if wug was presented to the model in a noun-signaling context such as (2a) and dax in an adjective-signaling context such as (2b), then the test set would consist of sentences such as (3a) and (3b).\n(2) Exposure:\na. I saw a wug run (N-signaling; Category Inference: wug is a noun)\nb. the book was very dax. (Adj-signaling; Category Inference: dax is an adjective)\n(3) Generalization:\na. They can find the (N-expecting; should prefer wug)\nb. it is quite of you. (Adj-expecting; should prefer dax)\nAn LM's generalization behavior can be characterized based on the extent to which it prefers predicting wug in (3a) over (3b) and dax in (3b) over (3a), having being updated on items in (2). On running their experiments using the BERT-large whole word-masking model (Devlin et al., 2019), Kim and Smolensky found above-chance accuracies across all pairs of categories (chosen from NOUN/VERB/ADVERB/ADJECTIVE), demonstrating abstraction-compatible generalization behavior in the model. Misra and Kim (2023) further characterized this generalization behavior in terms of movement of the novel embedding in low-dimensional space towards the centroid of the space occupied by known category members over the course of training. That is, wug, when exposed to inputs like (2a) became more noun-like, with its final embedding state being close to embeddings of other known, unambiguous nouns. Similarly, dax (exposed to inputs such as (2b)) became more adjective-like, with its final embedding becoming closer to the embeddings of unambiguous adjectives. This method is easily applicable to study generalization behavior we are interested in-we can examine an LM's usage of a novel dative verb after it has been exposed to the verb in a particular dative construction to quantify the degree of model generalization."}, {"title": "Adaptation to dative alternation. We adapt this method and study the effect of systematically varying the exposure context in terms of the type of construction (DO vs. PP) a novel dative verb occurs in, and the properties of the theme and recipient sourced from prior work such as Bresnan et al. (2007) and De Marneffe et al. (2012) as discussed in \u00a71.1. Following laboratory-based studies of novel dative verb acquisition and cross-dative generalization (Conwell and Demuth, 2007; Arunachalam, 2017), we use single exposure contexts per learning trial in our experiments on LMs. This means that the model learns the novel verb based on a single distributional context containing the novel verb, shown to the model multiple times (i.e., number of gradient update steps). Given an LM's exposure to a sentence with a particular feature configuration, we can analyze its behavior on a held-out generalization set containing sentences with the novel verb in either dative construction. For instance, if we are interested in the role played by theme-pronominality in the generalization of a novel verb (pilked) from PP to DO, then our exposure sentences would consist of examples like (4):", "content": "(4) a. she pilked it to me.\nb. she pilked the ball to me.\nAfter exposing two identical LMs to (4a) and (4b), respectively (i.e., in independent training runs), we can test each LM's preference for using pilked in a large sample of DO-sentences, like (5):\n(5) a. he pilked her the block.\nb. they pilked us some books.\nc. mommy pilked lucy the orange.\nd. ...\nIf pronominal themes encourage PP to DO generalization, then an LM that is exposed to (4a) should find sentences in (5) on average more likely than an LM that was exposed to (4b). This comparison allows us to link an LM's cross-dative generalization behavior to the features of its exposure context. Similarly, if we are interested in the opposite direction of generalization, we can construct sentences analogous to those in (4) but this time in a DO construction, and similarly test behavior on a held-out generalization set consisting of sentences that use the verb in PP. The above examples (4) show only a single feature contrast (pronominal theme vs. nominal theme) instantiated with a single lexical item each (it vs. the ball) for ease of exposition. In our actual experiments however, we consider the full space of the features-of-interest and also sample several different exposure stimuli per feature configuration by varying the lexical items. This setup allows us to control for multiple features at a time while quantifying the effect of a given feature in a factorial design.\nApart from the composition of the exposure/generalization data and the target phenomenon, our adaptation of Kim and Smolensky's paradigm involved a few additional modifications tailored to our investigation. Since the models are learning a single novel dative verb at a time, each individual learning trial in our experiments involves only a single input (as opposed to a pair of"}, {"title": "inputs in the case of Kim and Smolensky). This input is either a single sentence or a sentence preceded by some discourse context when testing the effect of discourse givenness. Furthermore, adding a randomly initialized novel token to a pretrained language model and then training the model on sentences containing the newly added token can sometimes distort the output distribution of the model, as shown by Hewitt (2021). Therefore, we initialized the novel token embedding by sampling from a multivariate Gaussian distribution with mean and variance computed using the embedding layer of the LM, following the suggestion of Hewitt (2021).", "content": "Model selection. We used the LMs' judgments of the \u201cverbhood\u201d of the novel tokens as our model selection criterion. That is, we minimally required the LM to recognize the novel token as a verb as opposed to any other part of speech. To do this, we constructed a validation set containing 300 sentences from the AO-CHILDES validation and test sentences. Half of the sentences contained a verb with the VBD part-of-speech tag, which we replaced with the novel token. For the other half, we randomly selected a non-verb token which we also manually verified, and replaced it with the novel token. Table C.10 in the appendix shows five examples of verb and non-verb-expecting sentences used to compute these verbhood judgments. The criterion was implemented in practice as follows: the LM's average log probability for the 150 verb-expecting sentences where the novel token occupies the verb position should be maximally greater than that for the 150 non-verb-expecting sentences where the novel token occupies a non-verb position. We saved the model state after each epoch during the learning phase, and chose the state of the LM that best reflects the fact that the novel token being learned is a verb\u2014the state where the average log probability difference (\"verbhood \u2206\u201d) between the two validation subsets is maximal.\nGeneralization measure. Finally, to quantify the LM's generalization behavior, we measured its log probabilities per token on a held-out generalization set which consists of sentences with the novel verb in the DO and PP constructions. This is analogous to the experimental settings of existing human studies (Conwell and Demuth, 2007; Arunachalam, 2017), which is to measure the learner's behavior on generalization stimuli where the novel verb occurs in an unmodeled construction (i.e., a construction different from the one the learner was exposed to). Under this setting, exposure contexts that promote the usage of the novel verb in the unmodeled construction will be associated with greater log probabilities per token for the generalization set, whereas those that are preemptive will be associated with lower generalization set log probabilities per token.\nWe visually summarize our method in Figure 1. In a nutshell, Steps 1 and 2 specify our experimental conditions where exposure sentence stimuli containing the novel verb are created by sampling themes and recipients that correspond to a certain feature configuration (described in \u00a72.3). Steps 3 and 4 demonstrate a single learning trial of the novel token, where we apply our adapted version of Kim and Smolensky's method. These are followed by Steps 5 and 6 where we analyze the LM-assigned log probabilities on a given generalization set, and repeat this process for a different exposure stimulus. Towards the end of all our learning trials for a given LM, we end up"}, {"title": "Define feature space for Theme and Recipient", "content": "Sample exposure stimulus given a dative frame and feature configuration\nAdd novel verb to LM's vocabulary\nUpdate the novel verb's embedding in LM for exposure stimulus and freeze all LM representations\nInvestigate behavior of updated LM on generalization set\nReset LM and repeat with new inputs"}, {"title": "2.3. Stimuli Construction and the Hypothesis Space", "content": "2.3.1. Exposure Stimuli\nOur goal is to relate LM learners' cross-dative generalization of a novel dative verb to the features of the exposure context in which it was learned from. To this end, our experiments use stimuli that vary in terms of the feature configurations of the context in which a novel dative verb occurs. This allows us to derive concrete hypotheses about the role of the different features in the LMs' generalizations, controlling for the effects of the other features. Our choice of the features of the exposure contexts is motivated by prior work characterizing dative alternation in human learners via production and comprehension experiments (Conwell and Demuth, 2007; Rowland and Noble, 2010; Rowland et al., 2014; Stephens, 2015; Arunachalam, 2017; Conwell, 2019), as well as corpus analyses of child-directed speech and adult-adult conversations (Gropen et al., 1989; Bresnan et al., 2007; De Marneffe et al., 2012). These studies have primarily focused on the features of the theme and recipient, since a core difference between the alternate constructions is the ordering of these arguments. We explore a large space of the possible feature configurations of the theme and recipient by constructing sentences containing a novel dative verb (the learning target) these serve as exposure contexts in our simulation trials that the LMs learn the novel verb from. The specific set of features we consider are: pronominality, animacy, definiteness, length, and discourse givenness. Below we discuss each feature, along with a brief description of the representative lexical items that we used in our stimuli. Consistent with the formatting of AO-CHILDES, we use lowercased items throughout.\nPronominality. Pronominality (pronominal vs. nominal) has been shown to have a significant effect on relative acceptability and preference between dative constructions (Bresnan et al., 2007; De Marneffe et al., 2012). In novel dative verb learning studies, recent work has found pronominal recipients (especially me, a frequent recipient in child-directed speech) to facilitate DO comprehension (Conwell, 2019). In our stimuli, we included 8 different pronouns, accounting for variation in animacy (him/her vs. it) and definiteness (her vs. someone). For non-pronominal items, we used nouns that appear in AO-CHILDES (e.g., mommy, daddy, grandpa, the cat, the ball, the bear).\nAnimacy. The animacy of the theme and recipient has been centrally discussed in theoretical and experimental literature surrounding dative alternation (Gropen et al., 1989; Bresnan et al., 2007; Rappaport Hovav and Levin, 2008; Beavers, 2011; De Marneffe et al., 2012; Arunachalam, 2017, i.a.). In experimental work, the combination of animate theme and animate recipient in a DO construction has been found to be difficult to comprehend for children (Rowland and Noble, 2010; Arunachalam, 2017). In our stimuli, we included 31 different entries for animate and inanimate items, accounting for variation in pronominality (her vs. it; grandma vs. the cup), length (the ball vs. the ball in the room), and definiteness (a book vs. the book)."}, {"title": "Definiteness. Definiteness is inextricably linked to the discourse status of an item definite items (the ball) are often discourse given, while indefinite items (a ball) are often used to introduce new discourse entities. The discourse status of the arguments have been found to affect the choice between dative constructions, at least in adults (Bresnan et al., 2007). In particular, definite items (usually discourse given) tend to occur before indefinite items (Arnold et al., 2000; Wasow, 2002). This explains the preference for DO when the recipient is definite, and preference for PP when the theme is definite. Our stimuli accounts for definiteness via different pronouns (him, her, them, it vs. someone/something), proper nouns (mommy, daddy, elmo, bert), and determiners (the vs. some/a).", "content": "Length. The length of the arguments has also been shown to play a role in postverbal word order in English (Aissen", "alternation": "themes tend to be longer than recipients in DOs, while the opposite is true for PPs (Bresnan et al., 2007; De Marneffe et al., 2012). We measured length in terms of tokens, and coded length as a binary distinction between short and long based on a threshold of two\u2014i.e., short items have up to two tokens while longer ones have more than two tokens. We used proper nouns and pronouns, and NPs with two tokens (e.g., a ball, the cup, a dog) as short items, while for long items we added PP modification (e.g., the cup on the table, the toys in the room) or modifiers to the nouns (e.g., the red ball, the cute puppy).\nDiscourse givenness. The final feature we considered in our hypothesis space is discourse givenness; prior work has observed that given information is typically mentioned before new information (Clark and Clark, 1977; Gundel, 1988; Arnold et al., 2000). Aligned with this observation"}]}