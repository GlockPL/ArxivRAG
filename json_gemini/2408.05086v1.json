{"title": "Generating novel experimental hypotheses from language models: A case study on cross-dative generalization", "authors": ["Kanishka Misra", "Najoung Kim"], "abstract": "Neural network-based language models (LMs) have been shown to successfully capture complex linguistic knowledge. However, their utility for understanding human language and more broadly, human cognition, is still debated. We contribute to this debate by presenting a case study where we use LMs as simulated learners to derive novel experimental hypotheses that may then be tested with human learners. We apply this paradigm to study productive generalization of novel verbs across two dative constructions (DO: she pilked me the ball; PP: she pilked the ball to me)\u2014 acquisition of which is known to involve a large hypothesis space of contextual features\u2014using LMs trained on transcripts of child-directed speech. We specifically ask: \u201cwhat properties of the training exposure facilitate a novel verb's generalization to the (unmodeled) alternate construc- tion?\u201d To answer this, we systematically vary the exposure context in which a novel dative verb occurs in terms of the properties of the theme and recipient, and then analyze the LMs' usage of the novel verb in the unmodeled dative construction. As a precondition to determine their utility for deriving novel hypotheses, we first test experimental conditions analogous to prior human studies, where we find LMs to replicate known patterns of children's cross-dative generalization. Our sub- sequent simulations reveal a nuanced role of the features of the novel verbs' exposure context on the LMs' cross-dative generalization. In particular, we find generalization to be facilitated when the first postverbal argument of the exposure context is pronominal, definite, short, and conforms to the prototypical animacy expectations of the exposure dative (i.e., inanimate theme/animate re- cipient). These patterns are characteristic of harmonic alignment in dative constructions observed in prior work, where the argument with features ranking higher on the discourse prominence scale tends to precede the other. This gives rise to a novel hypothesis that cross-dative generalization is facilitated or preempted based on the extent to which the features of the exposure context\u2014in particular, its first postverbal argument are harmonically aligned. Based on these findings, we sketch out future experiments that can test this hypothesis in child learners.", "sections": [{"title": "1. Introduction", "content": "Recent advances in Artificial Intelligence powered by language models (LMs) trained at scale have generated a series of discussions about their role in (human) cognitive science (Ambridge, 2020; Piantadosi, 2023; Portelance and Jasbi, 2023; Kodner et al., 2023; McGrath et al., 2023, i.a.). A significant portion of such discussions are not specific to LMs as a training objective per se, and are more broadly applicable to the role of artificial neural networks (ANNs) (Pater, 2019; Baroni, 2020; Warstadt and Bowman, 2022). Overall, both the arguments for and against the util- ity of LMs or ANNs are heavily reminiscent of debates from the second wave of connectionism (McClelland, 1988; Massaro, 1988; Fodor and Pylyshyn, 1988; Smolensky, 1991; McCloskey, 1991; Hadley, 1997, i.a.). Questions about whether connectionist models/ANNs/LMs count as theory, whether their successful replication of human behavior has any implications for advancing our understanding of human cognition, and discussion about their bearings on learnability argu- ments, are some recurring themes in these debates. While there are disagreements about the utility of contemporary LMs for cognitive investigations along these dimensions and more, one general consensus is that cautions must be taken in accepting them directly as models of human cognition (Guest and Martin, 2023), despite their strong predictive capabilities of behavioral (Goodkind and Bicknell, 2018; Wilcox et al., 2020; Shain et al., 2024, i.a.) and neural data (Schrimpf et al., 2021; Goldstein et al., 2022, i.a.). This is mainly due to sparse linking hypotheses between components of the human cognitive capacity and components of widely adopted models such as architecture, data, training process, and input/output representations, leading to limitations in explanations that can be offered by studies of these models alone.\nNevertheless, following McCloskey (1991), we argue that treating these models as animal models is a promising way that black box models with high predictive capacity can contribute to cognitive science, and empirically explore this possibility. Specifically, we provide a concrete case study where we use LMs as simulated learners to derive novel experimental hypotheses that can in turn be tested with actual human learners. We expect this approach to be the most fruitful for domains in which large-scale human experiments are challenging, such as child language ac- quisition. While there are existing human experimental findings implicated by (e.g., predictions of Portelance et al. 2021 being corroborated by concurrent human study of Jara-Ettinger et al. 2022) and motivated by (e.g., findings of Kim and Linzen 2020 motivating the human experiments of Kim and Smolensky 2024) neural network simulation studies, limited work has been explicitly designed for the goal of novel hypothesis generation in the experimental linguistics space (with the rare exception of Lakretz et al. 2021). Furthermore, no such studies to the best of our knowledge exist in the language acquisition literature. This work attempts a proof-of-concept implementation of this idea using the acquisition of dative alternation as a case study."}, {"title": "1.1. Acquisition of Dative Alternation as a Case Study", "content": "The dative alternation in English refers to a phenomenon where both prepositional phrase (PP: 1a) and double object (DO: 1b) constructions are licensed for the same verb. One important dif- ference between the two constructions is the linear ordering of the theme and recipient arguments: in PP, the theme precedes the recipient, and in DO, the recipient precedes the theme.\n(1) a. Najoung gave a treat to Cookie.\nb. Najoung gave Cookie a treat.\nHowever, not all dative verbs participate in this alternation; some only appear to be licensed in either the PP or the DO construction (*I donated the library the book, *He wished luck to me). Hence, learning which constructions a dative verb is licensed in is a problem that language learners face. It has been shown empirically that learners often generalize to the alternate construction even in the absence of direct evidence showing that the verb is licensed in the alternate construction (e.g., Gropen et al. 1989; Arunachalam 2017), rather than being fully conservative and restricting the use of the dative verb to only the observed construction. Then, how does a learner distinguish non-alternating verbs from alternating verbs that they happened to not have observed positive evidence for one of the licensed constructions? Two explanations are possible: first, there may be non-arbitrary criteria that can tease these cases apart (the criteria hypothesis: Gropen et al. 1989), and second, learners may have access to negative evidence that blocks overgeneralization. In this work, we focus on the first possibility and explore the criteria-based resolution to this learning problem\u2014an instance of Baker's paradox (Baker, 1979).\nA large body of prior work explores possible criteria that set apart alternating and non-alternating verbs, ranging from morphophonological to semantic factors (see Citko et al. 2017 for an overview). Many such factors are identified on the basis of distributional evidence (how often do we observe a dative verb with X/Y/Z properties in DO and PP constructions in a large corpus?) and adult accept- ability judgments (is a dative verb with X/Y/Z properties acceptable in DO and PP constructions?). These studies are important in understanding which cues are available to the learners, since sta- tistical cues in the input critically shape language acquisition (Saffran et al., 1996; Thompson and Newport, 2007; Romberg and Saffran, 2010). However, the mere availability of distinctive cues does not necessarily entail that they are used by the learners. In this regard, nonce word stud- ies testing whether learners generalize in the absence of observing the verb in the alternate form provide the most direct answer for the causal question: What cues do learners actually use to distinguish alternating verbs from non-alternating verbs? In adult learners, Gropen et al. (1989) identified factors such as possessive semantics and the number of syllables to affect PP to DO generalization, and Coppock (2009) found an effect of number of syllables as well as null results on the effect of prosodic weight and etymology (Germanic vs. Latinate). In children, empirical evidence is sparser; only a handful of studies have investigated generalization to the alternate con- struction (Gropen et al., 1989; Conwell and Demuth, 2007; Arunachalam, 2017), in addition to several studies that investigate the comprehension of dative structures (Rowland and Noble, 2010; Conwell, 2019) that may speak to the preconditions to generalization.\nThe sparse empirical evidence in children's acquisition of the dative alternation is partly due to the large size of the hypothesis space (combination of a wide range of distributional cues available in the input, as identified in the literature) and the difficulty in recruiting target participants at scale. To this end, we propose to use LMs as simulated learners to systematically explore this hypothesis space to identify a small number of targeted hypotheses, which in turn can be tested with actual"}, {"title": "1.2. LMs and Dative Alternation", "content": "Dative alternation has been frequently adopted as a testing ground for analyzing structural representations in LMs. For instance, LMs such as GPT-2 (Radford et al., 2019) and Llama-2 (Touvron et al., 2023) have been shown to exhibit effects analogous to structural priming (Bock, 1986) with respect to different dative structures (Sinclair et al., 2022; Jumelet et al., 2024; Zhou et al., 2024). In these studies, LMs assigned greater probabilities to sentences in a particular dative construction when adapted to instances of the same construction\u2014either via backpropagation or prefixes presented to the LM in context. Furthermore, Hawkins et al. (2020) showed that BERT (Devlin et al., 2019) and GPT-2 are sensitive to alternation patterns of known dative verbs (Levin, 1993), matching human alternation preferences. Analogous to humans, LMs also showed sensi- tivity to argument features such as length and definiteness which are known to predict speakers' dative choice during production (Bresnan et al., 2007; De Marneffe et al., 2012). Finally, a few studies have also investigated dative alternation in LMs from the perspective of novel word learn- ing (Thrush et al., 2020; Petty et al., 2022)\u2014these are the most directly related to our work in terms of methodology. Specifically, Thrush et al. (2020) simulated novel verb learning in BERT, and showed that the model can use novel verbs in alternate constructions, having only been ex- posed to one of the constructions. This work masked out all content words during the course of BERT's finetuning, opting to instead only expose skeletons of alternating constructions (e.g., to expose BERT to the DO, they used an input like: The [MASK] daxed the [MASK] a [MASK]). As a result, the precise role of the features of the theme and recipient on the model's productive generalization of the novel verb (here, daxed) is unclear. Petty et al. (2022) adopted a similar novel word learning method, but the word being learned was not a verb-instead, it was either the theme or the recipient in a dative sentence. Among other findings, they showed that LMs such as BERT and ROBERTa (Liu et al., 2019) can learn to infer a novel word's argumenthood (theme vs. recipient) based on the position it occurs in a dative exposure, and that recipients are more likely to be animate than inanimate, echoing prior corpus analyses (Bresnan et al., 2007)."}, {"title": "1.3. LMs and Child-directed Speech", "content": "This work contributes to a growing body of literature on the analysis of LMs trained on child- directed speech. A number of prior studies (Huebner and Willits, 2018, 2021; Wang et al., 2023; Qin et al., 2024) have trained Recurrent Neural Network (RNN; Elman 1990), Long Short-term Memory (LSTM; Hochreiter and Schmidhuber 1997), and Transformer LMs on transcripts of child-directed speech and found the models to recover clusters of simple semantic and syntactic categories as well as demonstrate sensitivities to various grammatical phenomena. A subset of these studies (Wang et al., 2023; Qin et al., 2024) reported that the aforementioned findings hold true even for LMs trained on text data that is sourced from a snapshot of the linguistic input received by a single child (derived from Sullivan et al. 2021's data). Concurrently, Yedetore et al. (2023) showed that training autoregressive LMs on child-directed speech does not impart them with a hierarchical bias akin to children\u2014instead, LMs preferred linear generalization. In addition, Portelance et al. (2023) used surprisals derived from LMs trained on CHILDES (MacWhinney, 2000) and found them to be better predictors of age of acquisition of children's vocabulary than surprisals extracted from simpler n-gram LMs.\nIn a similar vein, our experiments target the linguistic knowledge and behavior in LMs trained on child-directed speech. One key difference is that we study the acquisition of novel words, which will be affected not only by the linguistic knowledge from the LMs' pretraining\u2014which the aforementioned studies have characterized\u2014but also by the distributional context in which the novel words occur. By systematically varying these distributional contexts, we hope to better char- acterize the criteria that license productive generalization, and turn them into testable experimental hypotheses for human learners."}, {"title": "2. Methodology", "content": "Our LM learners are trained from scratch on transcripts of English child-directed speech (MacWhinney, 2000; Huebner and Willits, 2021). Below, we describe the data and the choice of the LM architecture used, followed by the method for simulating novel verb learning in the LMs. Then, we describe our hypothesis space and the pipeline used to construct the training and test stimuli.\nData availability. We make our data and code available at https://github.com/kanishkamisra/ encouraging-exposures."}, {"title": "2.1. Modeling", "content": "Data. We used AO-CHILDES (Huebner and Willits, 2021) as the training dataset for our LM learners. This corpus contains approximately 5M words from the American English portion of CHILDES (MacWhinney, 2000), including children from 0 to 6 years of age. It has been filtered to include only the child-directed portion, and is ordered temporally. Assuming that a soft upper- bound for the amount of linguistic input to an English speaking American child is around 1M words per month (Hart and Risley, 2003; Roy et al., 2015; Dupoux, 2018; Frank, 2023), the AO- CHILDES corpus consists of about 14% of the linguistic input to a 3 year old. We used the train/validation/test splits of AO-CHILDES provided by the BabyLM challenge (Warstadt et al., 2023), a competition for building LMs trained on developmentally plausible amounts of data. There are 4.21M words in the training set (11.7% of the soft upper bound of the linguistic input to an English speaking American child), 400K words in the validation set, and 340K words in the test set. All utterances in AO-CHILDES are in lowercase.\nLM architecture and training. The LMs used in this work are autoregressive, decoder-only Trans- formers (Radford et al., 2019) trained using the next-word prediction objective\u2014specifically, the OPT architecture (Zhang et al., 2022). However, the main method (\u00a72.2) is agnostic to the model architecture. For our choice of hyperparameters, we primarily followed BabyBERTa (Huebner et al., 2021), an LM trained on AO-CHILDES that showed strong performance on tasks target- ing the linguistic competence of English-learning children. We used 8 attention heads, 8 layers, a vocabulary size of 8192, an embedding size of 256, and a feedforward hidden dimension of 1024. This amounts to a total of 8.4M trainable parameters, which is comparable to the number of trainable parameters of the BabyBERTa models (8.5M). For the tokenizer, we again followed BabyBERTa and used a Byte-Pair Encoding-based tokenizer (Sennrich et al., 2016). We re-trained this tokenizer on our training set since the original BabyBERTa tokenizer was trained on a combi- nation of corpora not limited to AO-CHILDES (see Huebner et al., 2021), leading to superfluous tokens not in our training corpus and therefore irrelevant to our investigation. We trained our LMs on the training set of the AO-CHILDES dataset for 10 epochs, and chose the best learning rate based on the validation set. To ensure that our results are not due to idiosyncrasies of a particular training run, we report results for 10 different random seeds\u2014i.e., 10 instances of our LM archi- tecture trained on the same corpus using the same tokenizer, only differing by the initialization of the weights. More details about the architecture and the training configuration can be found in Table A.9."}, {"title": "2.2. Simulating Novel Verb Learning in LMs", "content": "Original method. We are interested in characterizing the generalization behavior of an LM learner after it has been exposed to a novel verb in a particular construction (DO vs. PP). The primary method we use for this investigation builds on the general paradigm first introduced by Kim and Smolensky (2021) to investigate lexical category inference in LMs. The method is inspired by de- velopmental linguistic studies that probe for lexical category abstraction in children (H\u00f6hle et al., 2004) using a head-turn preference procedure (Nelson et al., 1995). The method consists of the"}, {"title": "Adaptation to dative alternation.", "content": "We adapt this method and study the effect of systematically varying the exposure context in terms of the type of construction (DO vs. PP) a novel dative verb occurs in, and the properties of the theme and recipient sourced from prior work such as Bresnan et al. (2007) and De Marneffe et al. (2012) as discussed in \u00a71.1. Following laboratory-based stud- ies of novel dative verb acquisition and cross-dative generalization (Conwell and Demuth, 2007; Arunachalam, 2017), we use single exposure contexts per learning trial in our experiments on LMs. This means that the model learns the novel verb based on a single distributional context con- taining the novel verb, shown to the model multiple times (i.e., number of gradient update steps). Given an LM's exposure to a sentence with a particular feature configuration, we can analyze its behavior on a held-out generalization set containing sentences with the novel verb in either dative construction. For instance, if we are interested in the role played by theme-pronominality in the generalization of a novel verb (pilked) from PP to DO, then our exposure sentences would consist of examples like (4):\n(4) a. she pilked it to me.\nb. she pilked the ball to me.\nAfter exposing two identical LMs to (4a) and (4b), respectively (i.e., in independent training runs), we can test each LM's preference for using pilked in a large sample of DO-sentences, like (5):\n(5) a. he pilked her the block.\nb. they pilked us some books.\nc. mommy pilked lucy the orange.\nd. ...\nIf pronominal themes encourage PP to DO generalization, then an LM that is exposed to (4a) should find sentences in (5) on average more likely than an LM that was exposed to (4b). This comparison allows us to link an LM's cross-dative generalization behavior to the features of its exposure context. Similarly, if we are interested in the opposite direction of generalization, we can construct sentences analogous to those in (4) but this time in a DO construction, and similarly test behavior on a held-out generalization set consisting of sentences that use the verb in PP. The above examples (4) show only a single feature contrast (pronominal theme vs. nominal theme) instantiated with a single lexical item each (it vs. the ball) for ease of exposition. In our actual experiments however, we consider the full space of the features-of-interest and also sample several different exposure stimuli per feature configuration by varying the lexical items. This setup allows us to control for multiple features at a time while quantifying the effect of a given feature in a factorial design.\nApart from the composition of the exposure/generalization data and the target phenomenon, our adaptation of Kim and Smolensky's paradigm involved a few additional modifications tailored to our investigation. Since the models are learning a single novel dative verb at a time, each individual learning trial in our experiments involves only a single input (as opposed to a pair of"}, {"title": "Model selection.", "content": "We used the LMs' judgments of the \u201cverbhood\u201d of the novel tokens as our model selection criterion. That is, we minimally required the LM to recognize the novel token as a verb as opposed to any other part of speech. To do this, we constructed a validation set containing 300 sentences from the AO-CHILDES validation and test sentences. Half of the sentences contained a verb with the VBD part-of-speech tag, which we replaced with the novel token. For the other half, we randomly selected a non-verb token which we also manually verified, and replaced it with the novel token. Table C.10 in the appendix shows five examples of verb and non-verb-expecting sentences used to compute these verbhood judgments. The criterion was implemented in practice as follows: the LM's average log probability for the 150 verb-expecting sentences where the novel token occupies the verb position should be maximally greater than that for the 150 non-verb- expecting sentences where the novel token occupies a non-verb position. We saved the model state after each epoch during the learning phase, and chose the state of the LM that best reflects the fact that the novel token being learned is a verb\u2014the state where the average log probability difference (\"verbhood \u2206\u201d) between the two validation subsets is maximal.\nGeneralization measure. Finally, to quantify the LM's generalization behavior, we measured its log probabilities per token on a held-out generalization set which consists of sentences with the novel verb in the DO and PP constructions. This is analogous to the experimental settings of existing human studies (Conwell and Demuth, 2007; Arunachalam, 2017), which is to measure the learner's behavior on generalization stimuli where the novel verb occurs in an unmodeled construction (i.e., a construction different from the one the learner was exposed to). Under this setting, exposure contexts that promote the usage of the novel verb in the unmodeled construction will be associated with greater log probabilities per token for the generalization set, whereas those that are preemptive will be associated with lower generalization set log probabilities per token.\nWe visually summarize our method in Figure 1. In a nutshell, Steps 1 and 2 specify our experimental conditions where exposure sentence stimuli containing the novel verb are created by sampling themes and recipients that correspond to a certain feature configuration (described in \u00a72.3). Steps 3 and 4 demonstrate a single learning trial of the novel token, where we apply our adapted version of Kim and Smolensky's method. These are followed by Steps 5 and 6 where we analyze the LM-assigned log probabilities on a given generalization set, and repeat this process for a different exposure stimulus. Towards the end of all our learning trials for a given LM, we end up"}, {"title": "2.3. Stimuli Construction and the Hypothesis Space", "content": "Our goal is to relate LM learners' cross-dative generalization of a novel dative verb to the features of the exposure context in which it was learned from. To this end, our experiments use stimuli that vary in terms of the feature configurations of the context in which a novel dative verb occurs. This allows us to derive concrete hypotheses about the role of the different features in the LMs' generalizations, controlling for the effects of the other features. Our choice of the features of the exposure contexts is motivated by prior work characterizing dative alternation in human learners via production and comprehension experiments (Conwell and Demuth, 2007; Rowland and Noble, 2010; Rowland et al., 2014; Stephens, 2015; Arunachalam, 2017; Conwell, 2019), as well as corpus analyses of child-directed speech and adult-adult conversations (Gropen et al., 1989; Bresnan et al., 2007; De Marneffe et al., 2012). These studies have primarily focused on the features of the theme and recipient, since a core difference between the alternate constructions is the ordering of these arguments. We explore a large space of the possible feature configurations of the theme and recipient by constructing sentences containing a novel dative verb (the learning target) these serve as exposure contexts in our simulation trials that the LMs learn the novel verb from. The specific set of features we consider are: pronominality, animacy, definiteness, length, and discourse givenness. Below we discuss each feature, along with a brief description of the representative lexical items that we used in our stimuli. Consistent with the formatting of AO-CHILDES, we use lowercased items throughout."}, {"title": "2.3.1. Exposure Stimuli", "content": "Pronominality. Pronominality (pronominal vs. nominal) has been shown to have a significant ef- fect on relative acceptability and preference between dative constructions (Bresnan et al., 2007; De Marneffe et al., 2012). In novel dative verb learning studies, recent work has found pronominal recipients (especially me, a frequent recipient in child-directed speech) to facilitate DO compre- hension (Conwell, 2019). In our stimuli, we included 8 different pronouns, accounting for varia- tion in animacy (him/her vs. it) and definiteness (her vs. someone). For non-pronominal items, we used nouns that appear in AO-CHILDES (e.g., mommy, daddy, grandpa, the cat, the ball, the bear).\nAnimacy. The animacy of the theme and recipient has been centrally discussed in theoretical and experimental literature surrounding dative alternation (Gropen et al., 1989; Bresnan et al., 2007; Rappaport Hovav and Levin, 2008; Beavers, 2011; De Marneffe et al., 2012; Arunachalam, 2017, i.a.). In experimental work, the combination of animate theme and animate recipient in a DO construction has been found to be difficult to comprehend for children (Rowland and Noble, 2010; Arunachalam, 2017). In our stimuli, we included 31 different entries for animate and inanimate items, accounting for variation in pronominality (her vs. it; grandma vs. the cup), length (the ball vs. the ball in the room), and definiteness (a book vs. the book)."}, {"title": "Definiteness.", "content": "Definiteness is inextricably linked to the discourse status of an item definite items (the ball) are often discourse given, while indefinite items (a ball) are often used to introduce new discourse entities. The discourse status of the arguments have been found to affect the choice between dative constructions, at least in adults (Bresnan et al., 2007). In particular, definite items (usually discourse given) tend to occur before indefinite items (Arnold et al., 2000; Wasow, 2002). This explains the preference for DO when the recipient is definite, and preference for PP when the theme is definite. Our stimuli accounts for definiteness via different pronouns (him, her, them, it vs. someone/something), proper nouns (mommy, daddy, elmo, bert), and determiners (the vs. some/a).\nLength. The length of the arguments has also been shown to play a role in postverbal word order in English (Aissen, 1999; Arnold et al., 2000; Wasow, 2002). Heavy (more complex, longer) phrases tend to occur after lighter (less complex, shorter) phrases, with this observation dating back to Behaghel (1909) (noted by Arnold et al. 2000). This pattern is also reflected in dative alternation: themes tend to be longer than recipients in DOs, while the opposite is true for PPs (Bresnan et al., 2007; De Marneffe et al., 2012). We measured length in terms of tokens, and coded length as a binary distinction between short and long based on a threshold of two\u2014i.e., short items have up to two tokens while longer ones have more than two tokens. We used proper nouns and pronouns, and NPs with two tokens (e.g., a ball, the cup, a dog) as short items, while for long items we added PP modification (e.g., the cup on the table, the toys in the room) or modifiers to the nouns (e.g., the red ball, the cute puppy).\nDiscourse givenness. The final feature we considered in our hypothesis space is discourse given- ness; prior work has observed that given information is typically mentioned before new informa- tion (Clark and Clark, 1977; Gundel, 1988; Arnold et al., 2000). Aligned with this observation, corpus analyses of child-directed speech as well as adult conversations show that the DO con- struction is preferred when the recipient is given, while PP is preferred when themes are given (Bresnan et al., 2007; De Marneffe et al., 2012). In our experiments, we varied givenness between the theme and recipient\u2014when the theme is given, the recipient is considered new, and vice-versa. That is, givenness is a single binary variable as opposed to two separate binary variables for theme and recipient, which was the case for all previous features. To specify givenness, we inserted a sentence before the main dative stimulus, which contains the agent and the information that is given. We considered three different variations for our givenness-specification templates, where {given-arg} can either be the theme or the recipient:"}, {"title": "Stimuli generation.", "content": "Next, we describe how we used the above features to create our exposure stimuli. We used the following two templates for DO and PP, where [pilked] is the novel verb to be learned:\n(6) Dative Templates:\na. DO: {agent} [pilked] {recipient} {theme}.\nb. PP: {agent} [pilked] {theme} to {recipient}.\nIn our stimuli generation pipeline, we first constructed the main dative contexts by slot-filling the above templates using the sampled agent, theme, and recipient triples, and then separately varied givenness by inserting the appropriate templates containing the agent and the given item as pre- viously described. We sampled themes and recipients by jointly considering their pronominality, animacy, definiteness, and length. Each feature configuration is associated with a predefined set of lexical items to sample from (e.g., pronominal, short, definite, and animate recipient \u2192 {him, her, them, us, me}, nominal, short, definite, and animate theme \u2192 {bert, daddy, elmo, the bear, mommy, ...}). Each stimuli item is associated with 8 features (4 features each for theme and recipi- ent), so there are 256 theoretically possible configurations. However, some of these configurations are impossible in practice for instance, there cannot be a pronominal theme or recipient that is also long according to our definition of long (> two tokens). Additionally, we included a con- straint where themes and recipients cannot have the same surface form, which discards sets where both the theme and recipient were: (1) definite, inanimate pronoun, it; (2) indefinite, inanimate pronoun, something; or (3) indefinite animate pronoun, someone. Only 135 out of all theoretically possible configurations had non-empty sets to sample items from.\nWe sampled 5 unique pairs of themes and recipients for each of the 135 feature configurations. The agents were sampled from a set of 7 animate nouns and pronouns that also occurred in the AO-CHILDES dataset ({he, she, sam, lucy, mommy, daddy, nonna}) again with the constraint that the agent has a different surface form than either of the themes and recipients. This resulted in 675 triples of agent, theme, and recipient, using which we then filled in the two dative templates (6), yielding 1350 sentences in total.\nTo vary the givenness of the themes and recipients, we sampled stimuli such that the item that is given is definite. This is because describing something in the discourse and having it be"}, {"title": "2.3.2. Generalization Stimuli", "content": "For our generalization set, we used naturalistic dative contexts that occur in the validation and test sets of the AO-CHILDES corpus. This was to capture the distribution of natural generalization problems that human learners might encounter during the course of development. To this end, we built a pipeline that extracts dative contexts from the AO-CHILDES validation and test sets, described in detail in Appendix B. This pipeline retrieved 556 PP contexts, and 1211 DO contexts. We then manually extracted contexts where the dative verb is acceptable in the past tense, matching the tense of the verb from our exposures. We additionally discarded any false positives due to parsing errors. This gave us 160 DO contexts and 95 PP contexts. In all contexts, we replaced the dative verb with the novel verb being learned in our experiments. This generation process led to 255 generalization set stimuli in total\u2014see Table 2 for examples.\nThe mismatch in the number of DOs and PPs is expected\u2014DOs are generally more likely to appear in child-directed speech than are PPs (Conwell et al., 2011). Experimentally, this is not of great concern because, unless specified otherwise, all subsequent analyses only make claims on the basis of comparisons made on a fixed generalization set that consists of the same dative construction. Therefore, the mismatch between the frequencies of PP and DO do not affect these analyses. There is one exception to this (see \u00a74.1) where explicit comparisons are made between"}, {"title": "3. Preliminary Experiment: LM Generalization of Known Verbs that do not Alternate in Training", "content": "Before analyzing our LM learners' generalization behavior on novel verbs, we begin by first confirming whether they adequately predict the alternation patterns of real verbs that have asym- metric distribution in their training data. The learning scenario we target here is as follows. The learner observes two types of verbs which only occur in a single dative construction (say, PP). One of these two types of verbs outside of the limited set of the learner's observations\u2014is in fact"}]}