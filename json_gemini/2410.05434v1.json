{"title": "BETTER THAN YOUR TEACHER: LLM AGENTS\nTHAT LEARN FROM PRIVILEGED AI FEEDBACK", "authors": ["Sanjiban Choudhury", "Paloma Sodhi"], "abstract": "While large language models (LLMs) show impressive decision-making abilities,\ncurrent methods lack a mechanism for automatic self-improvement from errors\nduring task execution. We propose LEAP, an iterative fine-tuning framework that\ncontinually improves LLM agents using feedback from AI expert teachers. Our key\ninsight is to equip the expert teachers with a privileged state \u2014information available\nduring training but hidden at test time. This allows even weak experts to provide\nprecise guidance, significantly improving the student agent's performance without\naccess to privileged information at test time. We evaluate LEAP on diverse decision-\nmaking benchmarks, including text-based games (ALFWorld), web navigation\n(WebShop), and interactive coding (Intercode Bash). Our experiments show that\nLEAP (1) outperforms behavior cloning and ReAct baselines (2) enables weak\nstudent models (e.g., Llama3-8B) to exceed the performance of strong teacher\nmodels (GPT4-0), and (3) allows weak models to self-improve using privileged\nversions of themselves. We also provide a theoretical analysis showing that LEAP's\nsuccess hinges on balancing privileged information with the student's realizability,\nwhich we empirically validate. Our code is available at https://leap-llm.\ngithub.io.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLM) show impressive decision-making abilities across domains (Yao et al.,\n2022b; Jimenez et al., 2023; Sodhi et al., 2024). However, they struggle to learn and recover from\nerrors encountered at test time. Prompting alone is insufficient, as it requires manually specifying\nexceptions, which is difficult to scale and leads to intractably long context lengths.\nWe address the problem of fine-tuning LLM agents to improve their decision-making using online\ninteraction data. Imitation learning offers a sample-efficient way to train such models, where a teacher\ncorrects the student agent's actions. However, finding an expert capable of both demonstrating the\ntask and correcting mistakes from any state the student visits is extremely challenging. The expert\nmust not only know how to complete the task but also how to recover from the student's errors, which\nis often more complex than simply performing the original task. For example, in a task like \"Heat the\nmug and put it in the cabinet,\" if the student places the wrong object in the microwave, the expert\nmust first backtrack and remove the incorrect object before proceeding with the task.\nOur key insight is to equip the expert teacher with access to a privileged state \u2014 additional\ninformation unavailable to the student during test time, but crucial to improve the expert's ability\nto offer corrective actions at train time. This may include hidden states of a partially observable\nenvironment or intermediate subgoals. For the task \"Heat the mug and put it in the cabinet\", the\nprivileged state reveals the mug's location and necessary steps for heating it (Fig. 1). With this extra\nknowledge, the expert can first clear the microwave of any incorrect items and then teach the student\neffective search strategies, like checking common mug locations. Over time, this enables the student\nto not only learn how to perform tasks correctly but also how to recover from mistakes, ultimately\nallowing the student to outperform the expert, even without access to the privileged state at test time."}, {"title": "2 PRELIMINARIES", "content": "Problem Formulation: Decision-making LLMs. We frame decision-making as a Partially Observ-\nable Markov Decision Process (POMDP) (Kaelbling et al., 1998) because real-world environments"}, {"title": "3 APPROACH", "content": "We present Learning from Experts with Access to Privilege (LEAP), a framework for iteratively\ntraining an LLM agent through online AI feedback from a privileged expert (Fig. 1). Central to our\napproach is the concept of a privileged expert teacher, which leverages access to privileged state\ninformation to guide the student's recovery from errors. Section 3.1 defines the privileged teacher,\nSection 3.2 details LEAP, and Section 3.3 presents the theoretical analysis of LEAP."}, {"title": "3.1 PRIVILEGED EXPERT TEACHER", "content": "An expert teacher is a policy that can be queried with student history ht of observations and ac-\ntions, along with the student's predicted reason action (pt, at) to generate a corrected reason action\n(\u1fe5t, \u1fb6t) ~ \u03c0\u0395 (.|pt, at, ht). For POMDPs, computing such corrections is intractable and strictly more\nchallenging than just solving the original task, often resulting in suboptimal feedback.\nWe define a privileged expert teacher as a teacher that has access to privileged state information,\nSt, which fully describes the environment at a given time. This access transforms the teacher's\ndecision-making into a fully observable Markov Decision Process (MDP), making it significantly\nmore tractable. We formally define the privileged expert teacher as:\n(\u1fe5t, \u1fb6\u03c4) ~ \u03c0\u0395\u00b7|pt, at, ht, St)\n(3)\nThe privileged state st can either be explicitly provided or is implicit in privileged text information\nprovided to the expert. For instance, in Alfworld (Shridhar et al., 2020a), it includes the exact location\nof objects and the goal necessary for the agent to navigate effectively. In WebShop (Yao et al., 2022a),\nit involves the specific name of the target product and its attributes, aiding precise search queries or\nrecommendations. Similarly, in InterCode (Yang et al., 2024), the privileged state consists of the\nground truth code and any unit tests the code must pass, providing a clear benchmark for the agent to\ngenerate or correct code. Note that the privileged information is available only at train time.\nA crucial requirement for the privileged expert teacher is that it must not reveal privileged information\nin its feedback to the learner. The prompt instructions specify that the teacher should provide general\nreason and action that avoid the direct use of privileged information. Using such information directly\nmight solve tasks quickly but makes the corrected actions (pt, \u0101t) unrealizable for the learner, who"}, {"title": "3.2 THE LEAP ALGORITHM", "content": "The LEAP algorithm iteratively trains the student policy \u03c0 through online interactions with the\nenvironment & and corrections from a privileged expert teacher \u03c0\u0395. The process begins by collecting\nan initial set of demonstrations of history, reason, and action from the teacher D \u2190 {h, pa}, and\ntraining a BC policy \u03c0\u03bf on this data using supervised fine-tuning (SFT).\nAt each iteration, the current learner policy \u03c0\u2081 is rolled out in the environment to generate new data\nDi\u2190 (ht, pt, at). For each timestep in Di, LEAP computes the privileged state st by leveraging\ninformation available only during training. The privileged teacher is then invoked on trajectories\nwhere the agent fails, generating corrected reasoning and actions, pt, \u1fb6t ~ \u03c0\u0395 (.|pt, at, ht, st). These\ncorrections are used to augment the dataset Di. The policy \u03c0\u2081 is subsequently updated using this\naugmented dataset through a no-regret learning method. This iterative process continues, refining the\npolicy over multiple iterations until the best-performing policy is selected based on validation.\nThe update step in LEAP should be designed to ensure no-regret learning, a property that guarantees\nthe policy's cumulative performance will converge to that of the best possible policy in hindsight.\nThe most common update is to aggregate data UDi, and train \u03c0\u2081 using supervised fine-tuning\n(SFT) on the aggregated dataset, as done in DAGGER (Ross et al., 2011). This is equivalent to\nFollow the Regularized Leader (FTRL), a no-regret update step. An alternate update is to treat the\nproblem as a preference optimization problem where the corrected reason action (pt, \u0101t) is preferred\nover the student reason action (pt, at). Preference optimizers like Direct Policy Optimization\n(DPO) (Rafailov et al., 2024) or Kahneman-Tversky Optimization (KTO) (Ethayarajh et al., 2024)\noptimize a preference loss while regularizing to the base policy, in this case the previous policy \u03c0i\u22121.\nThis is equivalent to an Online Mirror Descent (OMD) update, which is also a no-regret update. We\nuse SFT by default but compare it against preference optimization as an ablation in the results."}, {"title": "3.3 ANALYSIS", "content": "We briefly touch on the theoretical guarantees of LEAP, and refer the reader to Appendix. B for a full\nexposition. We define the realizability of an expert policy and show how it affects LEAP performance.\nDefinition 3.1 (Average Realizability Gap). The average realizability gap epsilon(\u03c0\u0395, \u03a4) between\nthe privileged expert \u03c0\u0395 and the best policy \u03c0* over time horizon T is:\n\u03b5(\u03c0\u0395,\u03a4) := sup 1 Est,ht~d7||\u03c0\u0395(at|St) \u2013 \u03c0*(at|ht))||1\nT \u03c0 t=1\n(4)\nwhere ||.||1 is the L1 distance, dr is the induced distribution over state and history of any policy \u03c0."}, {"title": "Theorem 3.2 (LEAP with Privileged Expert)", "content": "Running N iterations of LEAP with privileged expert\n\u03c0\u0395 yields at least one policy a such that the time-average performance +J(\u03c0) is bounded as:\n1 1J(\u03c0) \u2265 J(\u03c0\u0395) \u2013 \u0397(\u03a0\u0395) (\u03b5(\u03c0\u0395,\u03a4) + \u03b3(N))\n(5)\nwhere \u03c0\u0395 is the privileged expert, \u0397(\u03c0\u0395) is the recoverability coefficient of \u03c0\u0395, \u03b5(\u03c0\u0395,\u03a4) is the\naverage realizability gap, and y(N) is the average regret of the DAGGER update.\nThe equation shows a trade-off between performance of the privileged expert (first term) and the\nrealizability of the privileged expert (second term), which we show how to optimize in Appendix. B."}, {"title": "4 EXPERIMENTS", "content": "We evaluate LEAP across 3 decision-making domains: ALFWorld, a text-based game, WebShop, a\nweb navigation environment, and Intercode, an interactive coding environment. Below is a summary:"}, {"title": "1. LEAP improves upon base behavior cloning (BC) policy", "content": "LEAP significantly improves the\nbase BC policy \u03c0\u03bf over successive fine-tuning iterations, showing gains on AlfWorld (65.7% \u2192\n91.8%), WebShop (29.4% \u2192 61.8%), and InterCode (60.3% \u2192 71.8%). See Tables 1, 2, Fig. 3."}, {"title": "2. LEAP enables weaker student models to surpass stronger teacher models", "content": "Over successive\niterations, student models (e.g., Llama3-8B) outperform their stronger teachers (e.g., GPT-40),\ndespite starting with lower or comparable performance. For example, on ALFWorld, the student\nmodel (\u03c03 : 91.8%) surpasses the teacher (\u03c0\u0395 : 65.7%), with similar trends on WebShop\n(\u03c02 : 61.8% vs. \u03c0\u0395 : 58.4%) and InterCode (\u03c03 : 71.8% vs. \u03c0\u0395 : 71.3%). See Tables 1, 2, Fig. 3."}, {"title": "3. LEAP requires a balance between using privileged information and generating realizable\ncorrections", "content": "Experts using minimal privileged information (\u03c0\u0395 : 73.1%) generate more realizable\nactions, while those with excessive privileged feedback (\u03c0\u03be: 5.22%) produce unrealizable\ncorrections. \u03c0\u03be strikes a perfect balance achieving optimal performance (\u03c0\u03be: 91%). See Fig. 5."}, {"title": "4. LEAP enables LLM agents to self-improve by using their own privileged versions as experts", "content": "LLM agents can self-improve by using their privileged versions as teachers. For example, in\nALFWorld, Llama3-8B bootstraps itself through iterations, improving from 65.7% to 82.1% and\nfinally to 91.8%. See Fig. 6."}, {"title": "5. LEAP can be trained using both SFT or Preference Optimization", "content": "Both SFT and preference\noptimization improve over the base BC policy (\u03c0\u03bf), with SFT delivering stronger gains (68.6% \u2192\n96.4%) compared to DPO (68.6% \u2192 74.3%) and KTO (68.6% \u2192 87.1%). See Table 3."}, {"title": "4.2 DOMAIN 1: TEXT-BASED GAMES", "content": "Setup. ALFWorld (Shridhar et al., 2020b) is a text-based game where each task has a high-level\nobjective, e.g. \"heat mug and put it in cabinet\" which the agent must complete by navigating and\ninteracting with a virtual household through text commands, e.g. go to shelf 1, pick up mug 2, etc. To\nsolve it, the agent must plan subgoals, track progress, and search for objects efficiently (e.g. mugs\nare more likely to be on shelves or cabinets). Each task has 30 timesteps. There are 6 categories of\ntasks, training dataset of 3257 games and two evaluation datasets: 139 in-distribution games and 134\nout-of-distribution games. We compare against a prior work BUTLER (Shridhar et al., 2020b) and\nReAct (Yao et al., 2022b); the original ReAct paper used text-davinci-002 with task-dependent\nin-context examples. We add ReAct with stronger models such as gpt-40\u00b9, claude\u00b2, and gemini\u00b3,\nand a general instruction prompt. For LEAP, we LoRA (Hu et al., 2021) fine-tune Llama3-8B (Dubey\net al., 2024) model using gpt-40 as the privileged expert teacher for 4 iterations. The privileged\ninformation for each game contains the essential objects to solve the task, object locations, and the\noptimal actions. See Appendix. C.2, D.1 for full details and prompts.\nResults. Table 1 shows that LEAP significantly outperforms all baselines, with the best policy\nachieving 91.8% success rate on out-of-distribution tasks. Iteration 1 of LEAP has the biggest\nperformance gain (65.7% \u2192 91.0%), leading to a student policy \u03c0\u2081 that surpasses the teacher\ngpt-40 with higher success rate (91.8% > 65.7%) and lower actions (11.3 < 20.2). Note\nthat performance improvements are not monotonic over iterations of LEAP, (65.7% \u2192 91.0% \u2192"}, {"title": "4.3 DOMAIN 2: WEB AGENTS", "content": "Setup. WebShop (Yao et al., 2022a) is an online shopping environment where each task is a shopping\nrequest from a user (\u201cI need a gluten-free bacon, 4 oz pack of 2, priced under $50\") that the agent\nhas to solve through web interactions (search[\"gluten-free bacon\"] or click options). Success is\ndetermined by an overall score with 4 components: whether attributes match (ATT), whether options\nmatch (OPT), whether product types match (TYPE) and whether price matches (PRICE). Each task\nhas 30 timesteps. The training dataset has 12086 tasks, the test dataset has 500 tasks. We compare\nagainst ReAct (Yao et al., 2022b) with different base models like GPT-40 and GPT-40-mini. We also\ncompare against IL baseline in WebShop (Yao et al., 2022a), which trains two models, one for search\n(from a list of keywords) and one for clicking, thus simplifying the problem. We LoRA fine-tune\nLlama3-8B model using gpt-40 as the privileged expert teacher for 4 iterations. The privileged\ninformation contains product attributes, product option, price constraint, and an example product that\nwould satisfy all criteria. See Appendix. C.3, D.2 for full details and prompts.\nResults. Fig. 3 (a) shows that LEAP outperforms all baselines, with the best policy (\u03c02) achieving\n61.4 score. Iteration 1 has the biggest performance gain (29.4 \u2192 51.8) followed by iteration 2\n(51.8 61.8). This leads to a student policy \u03c0\u2082 that surpasses the teacher GPT-40 with a higher\nscore (61.8 > 58.4). Similar to ALFWorld, while performance improvements are not monotonic,\nboth \u03c02, \u03c03 remain above the teacher gpt-40 performance. Fig. 3 (b) shows how LEAP improves\non all 4 components of the score. The biggest improvements between LEAP and gpt-40 is in OPT\ncomponent (42.4 > 22.1), indicating gpt-40 fails to select a product with the right option often."}, {"title": "4.4 DOMAIN 3: INTERACTIVE CODING", "content": "Setup. Intercode Bash (Yang et al., 2024) is an interactive coding environment, where the agent\nreceives a task that involves operations on a filesystem, e.g. \"Find all text files and write their\nnames to a single file\", and must interact with Bash to solve it and submit a final command.\nIntercode uses NL2Bash (Lin et al., 2018) to create 4\ndatasets. We use the first 2 for training, and the next\n2 for testing. We limit time horizon to 10. We com-\npare against ReAct (Yao et al., 2022b) with gpt-40\nand gpt-40-mini. We also compare against the top\n3 entries on the Intercode leaderboard, which are the\nTryAgain baselines from (Yang et al., 2024) with\ngpt-4, gpt-3.5 and CodeLlama-34B-INST (Roziere\net al., 2023) respectively. We LoRA fine-tune both Llama-\n3.1-8B model and Llama-3.1-70B model, each for 2 itera-\ntions, using gpt-40 as the privileged expert teacher. The\nprivileged information is the ground truth Bash command.\nResults. Table 2 shows that LEAP trained with 70B pol-10 actions). Baselines from [1] (Yang et al.,\nicy outperforms all baselines, with policy \u03c0\u2081 achieving 2024) evaluated on *(train + test) data.\n71.8% success rate to match the gpt-40 expert teacher\n(71.8% > 71.3%). We find that on this coding benchmark, the gpt-40 expert is a very strong\nbaseline. LEAP trained with 8B models improve over the BC policy, but falls short of the gpt-40\nexpert teacher. These results show that if the expert teacher is already a strong baseline, even after\nleveraging privileged information, it might be hard for the student to outperform the teacher."}, {"title": "4.5 WHAT IS THE TRADE-OFF BETWEEN PRIVILEGED INFORMATION AND REALIZABILITY?", "content": "We study the tradeoff between how much privileged state the expert uses vs the realizability of\ntheir corrections on ALFworld. We create 5 different privileged experts \u03c0\u0463, ..., \u03c0\u266d by varing the\nprompt to instruct them to use increasing amounts of privileged state, e.g. for \u3160 it says \"Use this\ninformation sparingly for your correction,\u201d but for \u3160 it says \u201cfeel free to include information from\nthe privileged state\u201d. We run 1 iteration of LEAP to generate updated \u03c0\u00b9, \u03c05. Fig. 5 (a) shows\nthat success ess rate initially rises from \u03c0\u00b9 to \u03c0\u00b3, but then falls off sharply till \u03c05. Num actions has a\nsimilar trend. This validates the hypothesis that there is indeed an optimal tradeoff. Interestingly, \u03c05\nis much worse than \u03c0\u2081, showing that it's more important to be realizable than to provide optimal yet\nunrealizable corrections. Fig. 5 (b) shows the corrections from \u03c0\u03be, \u03c0\u03be, \u03c0\u03be. \u3160 generates reasoning\nthat is perfectly realizable to the student, but since it has no privileged state the actions are not optimal.\nIn contrast, \u3160 predicts the correct action, but blatantly reveals the privileged information in the\nreason, being unrealizable to the student. 5 strikes a perfect balance where it offers the correct\naction, but offers a reason that is very much realizable, i.e. kettles are likely to be in stoveburners."}, {"title": "4.6 CAN LEAP BE USED TO SELF-CORRECT A STUDENT?", "content": "We test the hypothesis that LEAP should enable a model to self-improve by using its privileged version\nas the expert. On ALFWorld, we fine-tune Llama3-8B initializing with the BC policy \u03c0\u03bf and running\n2 iterations of self-improvement. Fig. 6 shows that LEAP is able to significantly self-improve policies\nover iterations (65.7% \u2192 82.1% \u2192 91.8%), with the final policy matching the best policy when\nusing GPT-40 as a teacher. We see uniform improvements across all categories, with some being\nimproved more from \u03c0\u03bf \u2192 \u03c0\u2081 and others from \u03c0\u2081 \u2192 \u03c02. We conclude that for some environments,\nlike ALFworld, the privileged state does the heavy lifting, i.e. performance improvements happen at\na similar rate to using a strong teacher vs using the model itself as a teacher. We note that this need\nnot hold for all environments, e.g. for some a strong teacher is required to extract the optimal reason\nand action from the privileged state. Additionally, if the base policy \u03c0\u03bf is not strong, there maybe not\nlift off for the model and rate of improvement would be limited."}, {"title": "4.7 HOW DOES SFT COMPARE TO PREFERENCE OPTIMIZATION?", "content": "We study the effect of different update methods for LEAP:\nSFT vs Preference Optimization. Our initial hypothesis is\nthat KL regularized preference optimization should also\nresult in performance improvement without needing to ag-\ngregate datasets. We run 1 iteration of DPO with different\nregularization \u1e9e values. Table. 3 shows that while DPO\nimproves upon the base \u03c0\u03bf policy, the improvement is far\nsmaller than SFT for both \u03b2 = 0.1, 0.01. One explanation\nis that SFT is far more aggressive in using the correction\ndata compared to preference optimization, i.e., SFT deems\nthe corrected reason action to be better than any alterna-\ntive. To test this, we used a different preference optimizer\nKTO, that operates with unpaired preference data and set\nweight AD = 1.0, AD = 0.0, effectively emulating SFT-like behavior. KTO with these settings has a\nsignificantly high performance of 88.1% close to SFT (91.0%)."}, {"title": "5 RELATED WORK", "content": "Imitation Learning and Privileged Information. A powerful paradigm in machine learning is\nleveraging privileged information (Vapnik et al., 2015)-data available only during training and\ninaccessible at test time. This concept has been transformative in robotics, where a simulated\nexpert policy with full state supervises a learner that only perceives sensor observations, such as\nnavigation (Zhang et al., 2016; Uppal et al., 2024), self-driving (Chen et al., 2020), manipulation (Chen\net al., 2023a; Hu et al., 2024), and legged locomotion (Lee et al., 2020; Kumar et al., 2020).\nA fundamental challenge in utilizing privileged information is the realizability gap-where actions\nsuggested by an expert may be infeasible for the learner to predict. This gap often leads to spurious\ncorrelations, manifesting as the \"latching effect\u201d where learners repetitively predict the same action,\ncommonly observed in autonomous driving (Muller et al., 2005; Kuefler et al., 2017; Bansal et al.,\n2018; Codevilla et al., 2019) and language models (Bengio et al., 2015; Holtzman et al., 2019; Wang\n& Sennrich, 2020; Ortega et al., 2021). While off-policy methods (Wen et al., 2020; 2022) have been\nproposed, (Swamy et al., 2022) show that online interaction with the environment is necessary.\nInteractive imitation learning methods such as DAGGER (Ross et al., 2011), where experts provide\ncorrective actions during the learner's rollouts, both theoretically and empirically lead to very effective\npolicies in this regime (Choudhury et al., 2018). These methods operate under the assumption of\nasymptotic realizability (Swamy et al., 2022), where the learner's policy can eventually match the\nexpert's actions as the episode progresses. However, when this assumption fails often due to the\npartial observability of the task or model capacity-suboptimal performance ensues, particularly\nfor LLM agents, as demonstrated by our experiments (Sec. 4.5). While recent approaches resort to\nexploration or RL (Walsman et al., 2022; Tennenholtz et al., 2021; Nguyen et al., 2022; Weihs et al.,\n2021), these are intractable to extend to LLM agents. Our work provides a practical recipe for LLM\nagents that optimally balances the trade-off between using privileged information and maintaining\nrealizability, and further shows how such an expert can be used to enable policy self-improvement."}, {"title": "Learning from AI Feedback", "content": "A scalable method for aligning language model using AI feedback (Lee\net al., 2023). Recent works have explored different types of feedback, from self-generated corrections\nto those provided by external AI models or environments (Pan et al., 2023). One body of work focuses\non self-correction, where LLMs refine their responses based on feedback from their own outputs (Bai\net al., 2022; Madaan et al., 2024). This can occur either at train-time, where LLMs are fine-tuned to\ngenerate corrected responses (Bai et al., 2022; Ganguli et al., 2023), or at test-time, where the model\nimproves its response interactively (Madaan et al., 2024). However, while simple in principle, (Huang\net al., 2024) shows that without external sources of feedback, LLMs often struggle to accurately judge\nthe correctness of their reasoning, limiting the effectiveness of purely self-generated corrections.\nAnother cluster of work explores feedback from external sources. Some approaches use feedback\nfrom strong models like GPT-4 acting as judges (Zheng et al., 2023) or critics trained on high-quality\ncurated data (Wang et al., 2023; Paul et al., 2023). Others leverage environmental feedback or\nexternal tools to guide improvements. (Welleck et al., 2023) uses environmental feedback to train a\ncorrector model, (Chen et al., 2023b; Olausson et al., 2023) utilize execution feedback to improve\ncode generation, and (Gou et al., 2023) employs external tools. Notably, Reflexion (Shinn et al.,\n2023) leverages feedback from external environments to generate self-reflections for agents, although\nit assumes multiple attempts in the same environment, which our approach does not. Our work\nintroduces the use of privileged information in decision-making tasks for LLM agents, a technique\nnot yet fully explored in the literature. This also enables LLM agents to self-improve using privileged\ninformation, setting it apart from existing self-improvement methods that rely on iterative solution\ngeneration and evaluation (Zelikman et al., 2022; Pang et al., 2024; Hosseini et al., 2024)."}, {"title": "6 LIMITATIONS", "content": "We proposed LEAP, an iterative fine-tuning framework that improves LLM agent performance using\nprivileged AI feedback, enabling weaker models to surpass stronger expert teachers and allowing\nagents to self-improve. However, LEAP has notable limitations. First, generating interaction rollouts\nis time-consuming, particularly in complex environments requiring multi-step reasoning. Each rollout\nrequires simulating the environment and generating complete trajectories, that becomes computation-\nally expensive across multiple fine-tuning iterations. Second, corrective feedback from the expert\noften results in lengthy output tokens. As trajectory lengths grow, balancing the effectiveness of\nfeedback with its verbosity becomes challenging. Exploring methods to distill and summarize expert\nfeedback at critical points is an interesting direction for future work."}, {"title": "A BROADER IMPACTS", "content": "Technological Impact. By iteratively fine-tuning LLMs through privileged expert feedback, LEAP\ncan significantly enhance AI performance across diverse applications, such as virtual assistants\nand dialogue agents handling complex decision-making tasks. This approach allows LLMs to\nautonomously refine their decision-making, reducing the need for human intervention, streamlining\nautomation, and improving efficiency in both consumer and enterprise environments.\nAdditionally, LEAP 's ability to enable weaker models to outperform stronger ones democratizes AI\naccess, making state-of-the-art models more accessible to organizations with limited computational\nresources. This also opens new possibilities for improving model generalization in settings where\nperfect information is unavailable. However, the reliance on privileged feedback during training\nunderscores the importance of further research into model transparency and interpretability. As these\nmodels grow more autonomous, ensuring their decision-making remains understandable to users will\nbe critical for building trust and fostering widespread adoption.\nSocietal Impact. Equipping LLMs with iterative self-improvement capabilities unlocks a range of\nsocietal benefits. In sectors such as education, customer service, and software development, these\nAl systems can assist professionals in decision-making, automate routine tasks, and enhance overall\nproductivity. By continuously improving their performance, these models can reduce the cognitive\nload on workers, allowing them to focus on more complex, creative, and value-added activities.\nDespite these advantages, there are ethical concerns that must be addressed. As LLMs become\ncapable of self-improvement, ensuring their trustworthiness and accountability becomes critical.\nWithout proper safeguards, the autonomous refinement of AI could lead to unpredictable behaviors,\nmaking it harder to ensure alignment with human values. Furthermore, there is a risk that self-\nimproving Als could be misused for malicious purposes, such as automating harmful tasks or\nspreading disinformation. To mitigate these risks, strict ethical guidelines and robust oversight is\nnecessary to ensure deployment of such systems serves societal good and prevents misuse."}, {"title": "B ANALYSIS", "content": "B.1 TRADE-OFF BETWEEN REALIZABILITY AND PRIVILEGED INFORMATION\nWe now analyze the performance of a policy imitating an expert with access to privileged information.\nThe performance depends both on the performance of the expert policy and the realizability gap\nbetween the learner and the expert. We then derive a constrained privileged expert to better trade\noff these two terms and provide a practical approximation of this constrained expert. For notational\nsimplicity, we assume that at represents both reason and action. We follow the derivation in (Swamy\net al., 2022) but simplify for our setting.\nLet \u03c0\u2208 II be the student policy that we are training. We assume the class of policies II is rich enough\nto contain all history-based policies. Let J(\u03c0) be the performance, i.e. the cumulative reward of a\nlearner policy that selects actions based on the history at = \u03c0(ht). Let J(\u03c0\u03b5) be the performance\nof the expert policy that selects actions based on the privileged state at = \u03c0\u0395(st). Let \u03c4 ~ \u03c0be a\nprivileged rollout which is the sequence of observation, actions, and privileged state upon executing\nthe policy. We represent the rollout as r = {(ht, st, at)}. Note that such a privileged rollout is only\naccessible at train time.\nWe begin by defining the average imitation gap between an expert policy \u03c0\u0395 and learner \u03c0.\nDefinition B.1 (Average Imitaiton Gap). The average imitation gap of a learner \u03c0 over a horizon T:\nAIG(\u03c0, \u03a4) := 1 (J(\u03c0\u0395) - J(\u03c0)) = \u0395\u03c0\u0395r(St, at) - \u0395\u03c4~\u3160r(St, at) (6)\nT\nWe next define the realizability gap between the privileged expert and the best policy in policy class\nas the L1 distance (or equivalently Total-Variation (TV) distance) between the two policies on the\ndistribution of states and histories induced by any policy."}, {"title": "Definition B.2 (Average Realizability Gap)", "content": "The average realizability gap over time between the\nprivileged expert \u03c0\u0395 and the best policy in the policy class \u03c0* is:\n\u03b5(\u03c0\u0395", "\u03a4)": "sup 1 Est", "policy\n\u0397(\u03a0\u0395)": "max ||A\u00ae (s, a)||\ns,a\n(8)\nWe are now ready to state the main theorem of LEAP.\nTheorem B.4 (LEAP with"}]}