{"title": "Autonomous Alignment with Human Value on Altruism through Considerate Self-imagination and Theory of Mind", "authors": ["Haibo Tong", "Enmeng Lu", "Yinqian Sun", "Zhengqiang Han", "Chao Liu", "Feifei Zhao", "Yi Zeng"], "abstract": "With the widespread application of Artificial Intelligence (AI) in human society, enabling AI to autonomously align with human values has become a pressing issue to ensure its sustainable development and benefit to humanity. One of the most important aspects of aligning with human values is the necessity for agents to autonomously make altruistic, safe, and ethical decisions, considering and caring for human well-being. Current AI extremely pursues absolute superiority in certain tasks, remaining indifferent to the surrounding environment and other agents, which has led to numerous safety risks. Altruistic behavior in human society originates from humans' capacity for empathizing others, known as Theory of Mind (ToM), combined with predictive imaginative interactions before taking action to produce thoughtful and altruistic behaviors. Inspired by this, we are committed to endow agents with considerate self-imagination and ToM capabilities, driving them through implicit intrinsic motivations to autonomously align with human altruistic values. By integrating ToM within the imaginative space, agents keep an eye on the well-being of other agents in real time, proactively anticipate potential risks to themselves and others, and make thoughtful altruistic decisions that balance negative effects on the environment. The ancient Chinese story of Sima Guang Smashes the Vat illustrates the moral behavior of the young Sima Guang smashed a vat to save a child who had accidentally fallen into it, which is an excellent reference scenario for this paper. We design an experimental scenario similar to Sima Guang Smashes the Vat and its variants with different complexities, which reflects the trade-offs and comprehensive considerations between self-goals, altruistic rescue, and avoiding negative side effects. Comparative experimental results indicate that agents are capable of prioritizing altruistic rescue while minimizing irreversible damage to the environment and making more altruistic and thoughtful decisions. This work provides a preliminary exploration of agents' autonomous alignment with human altruistic values, laying the foundation for the subsequent realization of moral and ethical AI.", "sections": [{"title": "1 Introduction", "content": "With the rapid advancement of AI, it has already exposed potential safety and moral risks in multiple areas, including causing irreversible damage to the environment [1, 2], deceiving human in different situations [3-6], etc. How to ensure that agents autonomously align with human altruistic values is an urgent and important issue, as it determines whether AI can benefit to human society and contribute positively to humanity's well-being.\nThroughout history, human societies have consistently maintained the virtuous tradition of altruism as a fundamental moral value. For instance, in the ancient Chinese story Sima Guang Smashes the Vat, Sima Guang broke the vat to save the child who accidentally fell into a large water vat when playing. Such moral values have gradually been inherited into the present society where AI coexists with humans. We also hope"}, {"title": "2 Results", "content": ""}, {"title": "2.1 The Basic Smash Vat Environment", "content": "Inspired by the ancient Chinese story Sima Guang Smashes the Vat, we build this basic smash vat environment, as shown in Fig. 1. In the environment, the explicit rewarded task of the agent is to reach the target in the fewest possible steps. However, there exist other tasks implicit in our carefully designed environment: we want the agent to minimize negative environmental impacts and rescue others trapped in the vat by smashing the vat. Clearly, these tasks are contradictory to each other: To avoid negative side effects, it will require the agent to take more steps to reach the target, and the same goes for rescuing trapped human; In order to save people, agents must perform the act of smashing vat, which causes irreversible damage to the environment. Since"}, {"title": "2.2 Experimental Results and Analysis", "content": ""}, {"title": "2.2.1 Experimental Results under Different Environment Variants", "content": "Based on the fundamental smash vat environment, we also designed some variants of the task with different difficulty levels by altering the distribution of the elements, which are named as the BasicVatGoalEnv, BasicHumanVatGoalEnv, SideHumanVatGoalEnv,\nCShapeVatGoalEnv, CShapeHumanVatGoalEnv and SmashAndDetourEnv. These environments focus on different task conflicts, as shown in Table 1. We tested our algorithm in these environments and the motion trajectories are shown in the last row of Fig. 2."}, {"title": "2.2.2 Comparison with other methods", "content": "In order to further validate the effectiveness of our method, we compared it with traditional DQN [34] as a baseline, which is trained solely on external environmental reward functions. Additionally, we compared it with Empathy DQN [21], which introduced an empathy mechanism that uses agent's own strategy to speculate on the state"}, {"title": "2.2.3 Ablation Experiment", "content": "We also conducted ablation experiments to verify the respective effects of the penalty term for negative effects $R_{nse}(s_t, a_t)$ defined in Eq. 2 and the incentive term for empathizing with others $R_{emp}(s_t, a_t)$ defined in Eq. 3 in our proposed method. We compare the average vat remaining rate and human rescued rate in the last 100 training episode where the algorithm converges and the results tend to be stable, the relevant results are shown in Table 3."}, {"title": "2.2.4 Hyperparameter Analysis", "content": "We also tested the impact of hyperparameters \u03b1 and \u03b2 proposed in Eq. 4 on our algorithm, which are used to control the agent's tendency to avoid negative effects and empathetic altruism. How the relative weights of environmental rewards, negative effect penalties and empathy altruism incentive affect the behavior of the final trained agent is a question worth exploring. Thus, we conducted tests by selecting several typical values within the range of [1, 20] while keeping $\u03b1 = \u03b2$, and also tested two scenarios where \u03b1 and \u03b2 are not equal. The comparison result is plotted in Fig. 4 using a method similar to plotting Fig. 3.\nIn most environments, the result curves under different hyperparameter settings almost coincide, which indicate that the value of the hyperparameter within a reasonable range does not significantly affect the experimental results, the agent is capable of prioritizing rescue based on an empathetic mechanism and avoiding smashing the vat when no one is trapped inside. This shows the robustness of our proposed method. An"}, {"title": "2.2.5 The Compatibility on SNN and DNN", "content": "In order to test the compatibility of our proposed random imagination based method with different network models, inspired by some computational empathy models based on SNN that incorporate brain-inspired mechanisms, we have also considered integrating our approach with spiking neural networks [35] and testing its efficacy. We adopt a direct spike encoding strategy. By replacing the ReLU neurons [36] in our network with Leaky Integrate-and-Fire (LIF) neurons [37], we substituted the original DNN with an SNN without changing the network architecture [38]. We used the surrogate gradient backpropagation algorithm [39] to optimize the network during the training process. More detailed information about SNN can be found in Appendix B. The corresponding results have been depicted in the green lines of Fig. 3.\nThe experimental results show that our performs well when integrated with SNN. It is a natural outcome considering that our proposed method is essentially model independent. This indicates that the intrinsic motivation mechanism we proposed can be easily integrated with other existing deep reinforcement learning algorithms regardless of their specific network architectures, demonstrating its broad applicability and robustness."}, {"title": "3 Discussion", "content": "Drawing inspiration from the ancient Chinese story of Sima Guang Smashes the Vat and human cognitive ability, this paper proposes a unified computational framework of self-imagination integrated with ToM, empowering agents to autonomously align with human values on altruism. This framework enables agents to predict the potential impacts of their actions, particularly by using perspective-taking to forecast the effects of their decisions on others' interests, before making decisions, thus generating intrinsic motivations leading to safe, moral, and altruistic decision-making. We design an experimental scenario similar to Sima Guang Smashes the Vat and its variants with different complexities, where exist conflicts among agents' own task, rescuing others and avoiding negative effects. Experimental results show that the agent trained by our proposed method is able to balance the above contradictions, prioritize rescuing individuals while minimizing environmental negative impacts and completing their own tasks. Further experiments demonstrate the effectiveness of the proposed framework as well as its good robustness under different hyperparameter configurations and compatibility with different networks.\nOur proposed method differs from some existing related methods. Below is a brief description of the main differences between our method and existing ones, along with the advantages our method offers:\n1. Compared with existing pure RL methods, such as DQN [34], our method is capable of considering the impact of agent actions on the environment and others, thereby generating internal incentives to avoid negative effects and empathize with altruism without explicit specified reward function. This can prompt agents to make more ethical and safe decisions without external reward function guidance.\n2. In comparison to existing methods that only account for the negative impact of an agent on the environment, such as original AUP [19] and FTR [20], our approach additionally incorporates empathy towards others. This overcomes the limitation of agent being overly conservative that only avoid negative effects, enabling the agent to make proactive decisions to aid others, even at the cost of causing irreversible damage to the environment.\n3. Compared to existing methods that only consider empathy towards others, such as Empathy DQN [21], our approach avoids the negative effects of environmental destruction without the need for an explicitly defined reward function, especially in those environments where there are no empathizable subjects, thereby exhibiting greater universality and generalizability.\n4. Compared with the work of Alamdari et al. [23], which extended existing methods to avoid negative effects (FTR[18]) to enable agents to empathize with others and thus avoid harming others' interests, our method estimate others' status based on self-experience and does not require obtaining rewards from others, thus providing wider applicability.\nWe aspire to ultimately enable AI to comprehend human morality, so that it can better benefit human society. The significance of this work lies more in a preliminary exploration of agents autonomous alignment with human altruistic values, laying the foundation for the subsequent realization of moral and ethical AI. Nevertheless,"}, {"title": "4 Methods", "content": "When an agent performs a task without an explicit external reward, it shows indifference to the negative impact of its behavior on the environment or on the interests of other agents. To align with human altruistic values, and achieve safe and altruistic behavior that generalizes across different situations, especially when there is a conflict between the agent's task, environmental negative effects, and the interests of other agents, intrinsic incentive generation mechanisms are essential. Intrinsic safety and moral altruistic behavior arise from imagining the potential impact of actions on the environment and others based on the agent's own experience. Based on this, the model proposed in this paper consists of three main components: the agent's imaginary space updated based on the its self-experiences, intrinsic motivation to avoid negative effects and empathy towards others by perspective taking, along with the interaction and coordination between the self-imagination module and decision-making network. The overall framework of our proposed model can be seen as Fig. 1. The relevant implementation code can be found at https://github.com/BrainCog-X/Brain-Cog/tree/main/\nexamples/Social_Cognition/SmashVat."}, {"title": "4.1 Self-imagination Module", "content": "In everyday life, humans frequently anticipate the consequences of their decisions before acting. Considering a simple example, when a mother asks her child to sweep the floor, the child may consciously avoid areas such as a table with water bottles even without explicit instructions. This behavior arises from the child's ability to mentally simulate potential outcomes, such as accidentally knocking over the bottle, which could result in upsetting the mother or necessitate additional time and effort to clean up spilled water and broken bottle fragments. Although these imagined scenarios do not actually occur, they significantly influence the child's decision-making process. This observation inspires the idea of enabling agents to make rational decisions by endowing them with the capacity to imagine possible outcomes based on their prior experiences.\nSelf-imagination can be implemented through various specific approaches. Inspired by the work of AUP [19], we adopt a method based on random reward functions. This implementation offers several advantages: random number generation offers simplicity and efficiency at the algorithmic level compared to complex reward generation mechanisms while enabling coverage of diverse scenarios; and it eliminates the need for prior"}, {"title": "4.2 Avoid Negative Side Effects", "content": "With the estimated values of various states in imagined situations $Q_i$, we can utilize them to enable the agent anticipate the potential consequences of its decision-making actions before execution, thus avoiding negative environmental effects. An intuitive idea is that the agent should imagine the possible consequences of taking a certain action a at the current state s by examining the specific Q-value $Q_i(s, a)$ to determine whether the consequences are good or bad. However, the concept of good or bad is relative, and only becomes meaningful when there exists a reference for comparison. Hence, we consider introducing a baseline state to serve as a comparative standard.\nFig. 5 shows different choices of baselines. Like Krakovna [17] and Turner [18] et al., we choose the stepwise inaction state $S'$ as the baseline state. One natural choice of baseline is the starting state $S_0$, but this might cause a penalty on change of the environment that is not caused by the agent's action. To avoid this, the inaction baseline $S^{(0)}$ seems to be a more reasonable choice of baseline called, which is referred to the state that the environment would currently be in if the agent have never acted. But inaction baseline may cause other problems. Using this baseline state may lead to"}, {"title": "4.3 Self-experience based ToM", "content": "The key to achieving ToM lies in considering problems in the shoes of others. In the context of evaluating the potential impacts of decision-making actions on the environment, agents can extend their considerations further by accounting for how environmental changes may affect others. However, directly inputting the observed current state of others into the agent's own strategy network to estimate the impact of actions on them [21] implicitly assumes that the agent and others share similar tasks. Consequently, this approach lacks generalization in environments where the tasks of the agent and others are inconsistent. Obviously, if we can directly obtain rewards and estimated value of the current state from others and incorporate them into the agent's decision-making[23], it would be beneficial for the agent to make altruistic decisions. However, in the real environment, it is difficult for us to obtain task rewards and value estimates from others for the state. And using inverse reinforcement learning to estimate others' tasks and rewards [22] is too complex and computationally time-consuming.\nSince $Q_i$ are learned based on randomly generated reward, they are decoupled from the real task reward of the environment. Thus, we use $Q_i$ to estimate the value"}, {"title": "4.4 Integration of Self-imagination Module and Decision-making Network", "content": "The negative effects and empathy-related rewards generated within the imagined space serve directly as intrinsic rewards that influence the agent's decision-making. In other words, when making decisions in the real environment, the agent comprehensively considers both the actual environmental feedback $R_{env}$ and the intrinsic rewards predicted in its imagined space. Using the total reward function $R_{total}$, the DQN network is then optimized to adjust the decision-making strategy accordingly. Based on the definition of $R_{nse}$ given by Eq.2 and $R_{emp}$ given by Eq. 3, here we propose the complete reward function $R_{total}$ as follows:\n$R_{total}(s, a) := \\frac{R_{env}(s, a) - \u03b1R_{nse}(s, a) + \u03b2R_{emp}(s,a)}{(\u03b1 + \u03b2)/2}$ (4)\nwhere $R_{env}(s, a)$ refers to the original environmental reward function, \u03b1 and \u03b2 denote weight hyperparameters used to control the tendency of agents.\nThe imaginary space and the real environment interact in real time, forming a dynamic and positive loop. The state transitions $(s_t, a_t, s_{t+1})$ generated from the interaction between the agent and real environment are consistently used to update the imagined space, while the intrinsic motivation derived from this imagined space guides the decision-making module in executing safe and moral behavior. This positive real-time interaction facilitates synchronized online learning via a shared self-experience buffer.\nThe complete algorithm process is shown in pseudocode Algorithm 1."}, {"title": "Appendix A Experiment Details", "content": "Here we supplement additional details on the experimental setup that were not thoroughly described in the main text."}, {"title": "A.1 The Observation Space of the Smash Vat Environment", "content": "In the smash vat environment, the agent possesses global observation capabilities, which means the agent can observe every object in the environment, the location of every other agent (if there exist), as well as its own position.\nSpecifically, the observation space of the agent is a 3 \u00d7 7 \u00d7 5 image-like array, or tensor. The first channel of the array represents the distribution of various elements in the environment, using 0 to denote an empty grid and integers from 1 to 3 to represent other elements besides human. The second channel uses one-hot encoding to"}, {"title": "A.2 Network Architecture", "content": "The Architecture of the network used in our method is shown as Table A1."}, {"title": "A.3 Training Hyperparameter Settings", "content": "The relevant hyperparameters used in training are shown in Table A2."}, {"title": "Appendix B Spiking Neural Network", "content": "Here we provide a brief introduction to SNN.\nSpiking Neural Network, as the third generation of neural networks [35], is a more biologically plausible model of neural networks. Compared to traditional DNN, SNN emphasizes the use of spike sequences with precise firing times as the basic carriers of information."}, {"title": "B.1 LIF Neuron", "content": "The LIF neuron [37] is a simplified model that describes the generation and propagation mechanism of neuronal action potentials. It abstracts the cell membrane as an equivalent circuit containing a capacitor, resistor and power source, where the capacitor reflects the capacitance of the cell membrane, the resistor reflects the permeability of the leak channels, and the power source reflects the influence of external input currents and the resting potential.\nThe differential equation describing the LIF neuron is as Eq. B1:\n$\u03c4 \\frac{du}{dt} = -[u(t) - u_{rest}] + RI(t)$ (B1)\nwhere u(t) denotes membrane potential, $u_{rest}$ denotes the the resting potential, I(t) denotes the input currents, \u03c4 = RC denotes the time constant, and R and C denote the membrane resistance and capacitance, respectively."}, {"title": "B.2 Direct Spike Encoding Strategy", "content": "Information is transmitted between neurons in the form of spike sequences, thus requiring a specific encoding scheme to encode the input as a series of spike sequences. Direct spike encoding strategy duplicates the input multiple times, with each copy corresponding to a time step, and then inputs them into the network sequentially. Direct encoding can be viewed as applying a constant current stimulus to the neurons in the first layer [42]. These neurons will generate corresponding pulse sequences based on their own dynamic characteristics and synaptic weights, and transmit them to subsequent layers. In this case, the first layer acts as a learnable encoder that can adjust its parameters based on feedback from network training, thereby achieving optimal encoding of the analog input signal."}, {"title": "B.3 Surrogate Gradient Backpropagation", "content": "Because of the nondifferentiable nature of the spiking function, the gradient of a smoother function called the surrogate gradient is used as an alternative to the real gradient, enabling the back propagation algorithm to be successfully applied to the training of SNNs [39]. The surrogate gradient function used in our experiment to"}, {"title": "", "content": "replace the spiking function is defined in Eq. B2.\n$g(x) = \\begin{cases}0, & x < -1 \\\\ - \\frac{\\alpha}{2}|x|x + \\alpha x + \\frac{1}{2}, & |x| \\leq 1 \\\\ 1, & x > 1\\end{cases}$ (B2)"}]}