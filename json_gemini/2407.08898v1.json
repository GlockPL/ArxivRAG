{"title": "IDAT: A Multi-Modal Dataset and Toolkit for Building and Evaluating Interactive Task-Solving Agents", "authors": ["Shrestha Mohanty", "Negar Arabzadeh", "Andrea Tupini", "Yuxuan Sun", "Alexey Skrynnik", "Artem Zholus", "Marc-Alexandre C\u00f4t\u00e9", "Julia Kiseleva"], "abstract": "Seamless interaction between AI agents and humans using natural language remains a key goal in AI research. This paper addresses the challenges of developing interactive agents capable of understanding and executing grounded natural language instructions through the IGLU competition at NeurIPS. Despite advancements, challenges such as a scarcity of appropriate datasets and the need for effective evaluation platforms persist. We introduce a scalable data collection tool for gathering interactive grounded language instructions within a Minecraft-like environment, resulting in a Multi-Modal dataset with around 9,000 utterances and over 1,000 clarification questions. Additionally, we present a Human-in-the-Loop interactive evaluation platform for qualitative analysis and comparison of agent performance through multi-turn communication with human annotators. We offer to the community these assets referred to as IDAT (IGLU Dataset And Toolkit) which aim to advance the development of intelligent, interactive AI agents and provide essential resources for further research.", "sections": [{"title": "1 Introduction", "content": "One of the enduring goals of artificially intelligent (AI) agents [83] is to seamlessly interact with humans using natural language. This capability allows AI agents to learn new skills [55, 90, 79] or assist in solving tasks [68, 34, 44]. To achieve this, AI agents must be able to comprehend [50, 49] and respond to human language, executing instructions across various environments [69]. Over the years, researchers have developed numerous tasks to address this challenge, often focusing on scenarios where humans provide instructions to achieve specific goals [26, 68]. For example, in the blocks world task, the agent must understand human instructions to move blocks on a grid [83, 11]. Other setups use Minecraft [27, 24] for tasks such as moving objects [1], simulating human behavior [62], or performing household tasks [68, 82]. However, human instructions are often inherently ambiguous. To complete these tasks successfully, agents need to engage in conversations by asking clarifying questions [4, 67, 64], thereby creating a more user-friendly interface [56].\nTo advance and emphasize this objective of interaction-driven agent building, we organized the Inter-active Grounded Language Understanding (IGLU) competition at NeurIPS in 2021[36] and 2022[37]. The primary aim of this competition was to foster the development of interactive agents capable of comprehending and executing grounded natural language instructions, particularly emphasizing the nuances of natural language dialogues and clarifications. The overarching goal of IGLU is to equip researchers with the data, tools, and insights necessary to evaluate the efficacy of interactive multi-turn communication with humans. The first significant challenge hindering the exploration of building interactive agents is the scarcity of appropriate datasets. Moreover, the data collection process is time-consuming and difficult to set up, requiring scalable, flexible, and easily extendable"}, {"title": "2 Interactive Grounded Language Understanding (IGLU) Setup", "content": "The IGLU competitions in 2021 [36] and 2022 [37] address the challenge of developing interactive agents capable of learning to solve building tasks through grounded natural language instructions in a collaborative environment. An interactive agent is defined as one that accurately follows instructions, requests clarification when necessary, and swiftly adapts to newly acquired skills.\nTo approximate this scenario and simplify the study to obtain easily interpretable findings, allowing us to understand general principles, we propose the following simulated setup: The architect and builder communicate via a chat interface in 3D environment. The architect provides the builder with grounded instructions on constructing the target structure. The builder may either seek clarification if"}, {"title": "3 Data Collection Tool", "content": "We developed a scalable open-source data collection tool to facilitate the collection of multi-modal corpora (Sec. 4) for the collaborative building task [55, 31] using the setup described in Sec. 2 Unlike the data collection environment established by [55], which utilizes the Malmo platform and requires a Minecraft game server [32], our tool is entirely developed in JavaScript. This approach eliminates the need to set up a Minecraft game server, significantly simplifying the process. Additionally, our tool is highly scalable, allowing for efficient expansion and integration with crowdsourcing platforms such as Amazon MTurk. Our data collection tool can be used to easily collect more data.\nVoxel World Environment We harnessed a Minecraft-like game environment called CraftAssist voxel world [27, 73] for our data collection tool which provides an immersive platform for agents to learn from language instructions and engage in fundamental navigation and building tasks, driven by its unique physics characteristics and its 3D world representation. In the CraftAssist voxel grid world agents perform building actions within a 11 \u00d7 11 \u00d7 9 sized build region [55] that can be recorded as action states and retrieved for future sessions. The integrated CraftAssist library supports actions such as picking, placing, and removing blocks of different colors within the voxel world. Additionally, agents can jump to place blocks, enabling the creation of structures with varying complexity. This approach ensures scalability and facilitates extensive experimentation and development within the platform. Fig. 2b gives the visualization of the voxel world environment in our platform.\nTo reduce user friction in giving and comprehending instructions, we embedded a compass on the ground of the voxel world to aid users in understanding spatial orientations. Then in the architect task, we ask the builder to explicitly specify the view of the current structure on which the instruction is based from one of the five orientations: northward, southward, eastward, westward, or from top. Later in the builder task, we put the builder in the same orientation before providing the instructions from the architect. In this way, the architect and the builder are able to establish a shared understanding of the spatial attribute of the target structure in a multi-turn manner asynchronously. For each task, we record the following information: gameId, stepId, and avatarInfo. avatarInfo contains the agent's spatial coordinates (x, y, z) and its corresponding pitch and yaw angles. Additionally, for the builder agent, we record a tap of the agent's actions (movement, block placement) along with the world state changes discretely. We record the architect's instructions and the builder's clarification questions.\nData Collection Setup Our tool for collaborative building tasks is designed to be scalable and easily deployable to collect large datasets efficiently. It facilitates the collection of multi-modal collaborative building tasks, seamlessly integrating with crowd-sourcing platforms for efficient participant scaling. Furthermore, we enhance the data collection process by introducing asynchronous turn-taking. This means the tool no longer relies on having the same set of annotators online throughout the game. We have implemented checks to prevent a single annotator from taking on both architect and builder roles"}, {"title": "4 IDAT Dataset", "content": "The IDAT dataset is a comprehensive multi-modal dataset that includes instruction utterances, voxel world states at each action, and the corresponding images. Following the previously described methodology, we provide a two-part dataset: a seed dataset and the IGLU dataset 5.\n4.1 Seed Dataset\nThe seed dataset comprises multi-turn dialog sequences aimed at collaboratively building a target structure. A complete session of dialogues to achieve the target structure is referred to as a game as shown in Fig. 3. In each turn, an annotator assumes the role of either the architect or the builder. Architects are randomly assigned a target structure from a diverse set of structures. They provide the next step instruction for the Builder. The Builder starts from scratch at the beginning of a game or builds on intermediate results by executing the Architect's instructions. If the instruction is unclear, the Builder can pose a clarifying question."}, {"title": "4.2 IGLU-Dataset", "content": "The multi-turn data collection process described in the previous section is fairly complex and tricky to scale. We simplify the process to be a single turn where all required attributes are captured in one shot. We first remove the complexity of building a predefined target structure. Instead, annotators are asked to perform some free-form building actions within the voxel world, while providing instructions that should allow another annotator to rebuild the same structure. These single-turn task segments enable asynchronous collaboration between annotators. This process enables the data collection at a significantly faster pace, leading to a larger corpus comprising natural language instructions, corresponding actions performed based on those instructions, and a set of clarifying questions. We record and save actions performed by annotators in a key-value pair format that stores the movement of the agent and positional changes of blocks within the voxel world.\nWe utilized the Seed dataset to provide diverse starting canvases for annotators as follows:\n\u2022 An annotator is assigned a world state from the Multi-Turn dataset as the starting point for their building task (Fig. 2b: Ideation Stage).\n\u2022 The annotator is prompted to perform a sequence of actions for a duration of one minute.\n\u2022 Then, the annotator is required to describe their actions in the form of a natural language instruction.\n\u2022 Another annotator is shown the instructions and asked to perform the steps mentioned. If the instruction is unclear, the annotator specifies it as thus and asks clarification questions (Fig. 2b: Clarification Question Stage)."}, {"title": "5 IGLU Evaluation", "content": "While our focus in this paper is not on the solutions or baselines presented during the competition, we note them to underscore the need for the evaluation protocol we employed during the competition. This includes the development of an online interactive human evaluation platform which is a major contribution of this work. This evaluation platform serves as a crucial supplement to offline evaluation metrics, ensuring the robustness and validity of the evaluation process of interactive agents and allowing for deeper qualitative insights.\n5.1 Offline Evaluation\nInteraction Focused Task Evaluation:\nRQ1 When? It is evaluated as a binary classification problem: Does the provided instruction require a clarifying question? We use the macro average F\u2081 score to evaluate classifiers based on instructions marked as unclear in the corpus, ensuring a balanced measure of both precision and recall across the two classes.\nRQ2 What? It is evaluated based on the quality of selected clarifying questions for unclear cases.\nWe formulate the problem of ranking a pool of clarifying questions instead of generating the questions for several reasons. Generating clarifying questions in a collaborative environment is challenging, as shown in [36]. If clarifying questions already exist in a pool, finding the most appropriate ones becomes a more manageable task than generating them from scratch [4]. Additionally, the evaluation of classification and ranking tasks is much more well-established compared to generation tasks, as there may be multiple correct clarifying questions for any given scenario. Therefore, ranking a pool of clarifying questions allows for better evaluation and control over the output. We assess how well the model can rank a list of human-issued clarifying questions in the corpus for a given ambiguous instruction. The model's effectiveness is measured using Mean Reciprocal Rank (MRR). The average F1 score of the top three participants for RQ1 is 0.76. For ranking clarifying questions, the top three teams achieved an average MRR of 0.58. These results indicate that significant room for improvement remains, highlighting the challenges associated with these tasks."}, {"title": "5.2 Human-in-the-Loop Interactive Online Evaluation: Greenlands Platform", "content": "To facilitate the evaluation of the RL agents by human participants we developed the interactive evaluation platform. Greenlands' host agents on a Minecraft server, enabling human evaluators, sourced from a crowdsourcing platform (Amazon MTurk), to interact with and assess the agents' performance in a real time. Our findings suggest that while current RL agents exhibit a degree of functionality, they fall short of human expectations in terms of interactivity and reliability. Technical design of the platform's is provided in the appendix F. Our evaluation is focused on IGLU 2022 the top agents [37] (Brain Agent (B) and MHB-Pegasus (P)), and baseline model developed by IGLU team to serve as a control (MHB) [69], which archived the following F\u2081 scores in the offline evaluation: (1) \u0412 \u2014 0.254, (2) \u0420 \u2014 0.178, (3) MHB = 0.150.\nOur human evaluation protocol involved participants playing two separate games of interactive collaborative building task, each featuring a different agent in random order. After interacting with both agents, participants were asked to identify which agent they perceived as superior and to provide qualitative feedback on each agent's behavior. This comparative approach mitigates the inherent subjectivity by focusing on the relative performance. Participants were blinded to the identity of the agents, anonymized as Agent 1 and Agent 2. To ensure a fair comparison, both games assigned to a participant within a single MTurk hit involved the same task, with identical initial and target structures. These tasks were randomly selected from our test set."}, {"title": "5.2.1 Human Evaluation Results and Discussion", "content": "We recorded a total of 45 MTurk assignments. The human evaluations, summarized in Tab.3, suggest a correlation between human preferences and offline evaluation scores, with Brain Agent generally preferred over MHB-Pegasus. However, the generalizability of these results may be limited. Examples of human feedback on the performance of each agent are provided in appendix F.2.\nUpon reviewing the qualitative feedback, we consistently see that none of the agents met human expectations or completed the tasks. Through our analysis, we identified three predominant concerns across all agents, as reported by the participants: responsiveness to commands, precision in executing actions, and compliance with given instructions.\nAligning training scenarios with the complexities of the real world is a challenging problem for interactive agents. This difficulty is evident in both offline and online evaluations of the agents. Interestingly, despite the agents' generally poor performance, there was a discernible alignment between human preferences and the outcomes of offline evaluations. This suggests that even in the presence of task completion deficits, the behavioral patterns exhibited by agents can significantly influence human perceptions of their capabilities."}, {"title": "6 Related Work", "content": "Evolution of NLIs and ApplicationsEarly work in Natural Language Interfaces (NLIs) [84, 17, 29] laid the foundation for understanding and designing effective interfaces for human language communication with computers. In recent years, there has been a resurgence of interest in NLIs due to advances in language understanding capabilities driven by large-scale deep learning models [21, 47, 16, 2, 66, 12, 60, 15] and the increasing demand for various applications such as virtual assistants, dialog systems [40, 42, 13, 41, 43], and question answering systems [45, 46, 22, 91]. NLIs now extend beyond traditional databases to encompass knowledge bases [18, 10] to robots [77], personal assistants [35, 34], and other forms of interaction [25, 20, 88, 71]. Agent Interactivity and Learning The focus has shifted towards interactivity and continuous learning [54, 33], enabling agents to interact with users [85], learning new tasks from instructions [39, 50, 75], assessing their uncertainty [86], asking clarifying questions [3\u20136], and leveraging feedback from humans to correct mistakes [23, 58, 57, 51]. Currently, LLMs are also being studied to asses uncertainty and their own errors [64, 65]. Newer directions are studying ways of identifying possible multi-modal utility of agentic systems to [7, 8, 63]. Grounded Language Understanding This paper focuses on grounded language understanding-connecting natural language instructions with real-world or simulated environment context and taking corresponding actions [30, 52, 48]. This is crucial to enabling more effective communication between humans and intelligent agents. Our work focuses specifically on tackling grounded language understanding in the context of collaborative building tasks performed by agents [14, 53, 69].\nLeveraging Minecraft We select Minecraft for grounded language understanding due to its distinct advantages. Szlam et al. [74] highlights the benefits of an open interactive assistant in Minecraft. The game's 3D voxel grid world and adherence to simple physics rules provide ample research scenarios for reinforcement learning experimentation [30]. Minecraft's interactive nature, player interactions, and dialog exchanges offer diverse opportunities for grounded natural language understanding [87, 70, 55]. The game's immense popularity ensures enthusiastic player interaction, facilitating rich human-in-the-loop studies. Minecraft's advantage extends to the availability of the highly developed set of tools for logging agents interactions and deploying agents for evaluation with human-in-the-loop, including Malmo [32], Craftassist [27], TaskWorldMod [59], MC-Saar-Instruct [38] and IGLU GridWorld [92]. Among the Minecraft-based related works, MineDojo [24] is similar to IGLU in the sense that both are designed to develop intelligent agents within the expansive Minecraft environment. While MineDojo aims to build versatile agents capable of performing diverse tasks through an internet-scale knowledge base, IGLU seeks to enhance interactive agents that can understand and act on grounded natural language instructions, with a strong emphasis on natural language dialogue and clarification."}, {"title": "7 Conclusion", "content": "In conclusion, we introduce IDAT comprising the dataset, tools, and evaluation platform tailored for the development of interaction-driven agents. The dataset comprises approximately 9,000 instructions and over 1,000 clarifying questions, along with corresponding actions and grid world states for interactive building tasks in a Minecraft-like environment. The released data collection tool is scalable, supports our task setup, and can be seamlessly integrated with crowdsourcing platforms. This adaptable tool enables the collection of tailored data for specific use cases, and we recommend"}, {"title": "8 Limitations", "content": "This work focused on a single environment, Minecraft, which might not be an ideal representation of real-world environments. Although Minecraft does not perfectly replicate real-world environments, it serves as a valuable platform for training agents on fundamental tasks using natural language. This is particularly relevant given the current performance limitations observed in agent-building tasks. Some may find the scale of the dataset limiting. However, the developed data collection tool is designed to facilitate the efficient gathering of additional data, thereby addressing this limitation."}, {"title": "Acknowledgments and Disclosure of Funding", "content": "We would like to express our gratitude to the many individuals who made our work possible. Our amazing co-organizers of the competition\u2014Milagro Teruel, Arthur Szlam, Mikhail Burtsev, Mohammad Aliannejadi, Ziming Li, Zoya Volovikoa, Aleksandr Panov, and Kavya Srinet-provided invaluable assistance and contributions that were essential in building the evaluation platform, providing feedback on the data collection platform, and organizing the competition. We are grateful to Ahmed Awadallah for their guidance and support throughout this project, and to the AICrowd team for their support in hosting the competition. We extend our thanks to the team at Microsoft, including Lars Liden, Matt Mazzola, Swadheen Shukla, Qianqian Qi, Piali Choudhury, Curtis von Veh, Sam Yeh, and Jianfeng Gao, whose expertise and commitment were instrumental in the development of the Greenlands platform. Their collaborative spirit helped bring our vision to fruition. Our advisory board and previous co-organizers of the competition also deserve thanks for their input and advice. Finally, special thanks to Microsoft for their funding and overall support in making this project possible."}]}