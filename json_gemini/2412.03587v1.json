{"title": "Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient Fine-Tuning of Language Models", "authors": ["Hyegang Son", "Yonglak Son", "Changhoon Kim", "Young Geun Kim"], "abstract": "Transformer-based large-scale pre-trained models achieve great success, and fine-tuning, which tunes a pre-trained model on a task-specific dataset, is the standard practice to utilize these models for downstream tasks. Recent work has developed adapter-tuning, but these approaches either still require a relatively high resource usage. Through our investigation, we show that each adapter in adapter-tuning does not have the same impact on task performance and resource usage. Based on our findings, we propose SAFE, which gradually freezes less-important adapters that do not contribute to adaptation during the early training steps. In our experiments, SAFE reduces memory usage, computation amount, and training time by 42.85%, 34.59%, and 11.82%, respectively, while achieving comparable or better performance compared to the baseline. We also demonstrate that SAFE induces regularization effect, thereby smoothing the loss landscape.", "sections": [{"title": "Introduction", "content": "Large-scale pre-trained language models (PLMs) have manifested superior performance in various tasks (Kenton and Toutanova, 2019; Liu et al., 2019; Radford et al., 2019; Yang et al., 2019). However, training a PLMs from the scratch is usually time-consuming and resource-intensive. Common practice has been hence to fine-tune the large-scale pre-trained models by adapting all the parameters with the downstream tasks, i.e., full parameter fine-tuning (full-tuning).Recently, Parameter-Efficient Fine-Tuning (PEFT), which focuses on optimizing a small fraction of parameters for downstream tasks, is receiving much attention (Houlsby et al., 2019; Lester et al., 2021; Li and Liang, 2021; Liu et al., 2022, 2023). Among various PEFT strategies, adapter-tuning has emerged as a prevalent method. It integrates lightweight modules, termed adapters,"}, {"title": "Related Work", "content": "Parameter-Efficient Fine-Tuning: To efficiently adapt large-scale PLMs to downstream tasks, many adapter-tuning methods (Chen et al., 2023; He et al., 2023; Hu et al., 2021; Houlsby et al., 2019; Karimi Mahabadi et al., 2021; Liu et al., 2022) have been proposed. In general, adapter-tuning methods inject small, trainable, and task-specific adapter modules into each transformer layer of a pre-trained model. Given a pre-trained weight matrix $W_o \\in \\mathbb{R}^{d \\times k}$ and input $x \\in \\mathbb{R}^{k \\times 1}$, the weight update of adapter-tuning is expressed as $W_o+ \\Delta W$. During training, $W_o$ is frozen and does not receive gradient updates, while $\\Delta W$ contains trainable parameters. For $h = W_ox$, The modified forward pass in adapter-tuning yields:\n$h = W_ox + \\Delta Wx$.\nTo further improve parameter efficiency of adapter-tuning, AdaLoRA (Zhang et al., 2022) adaptively adjusts the number of trainable parameters among adapters according to their importance score it reduces the number of trainable parameters for less important adapters. However, the adapter-tuning methods still use a large amount of memory, as shown in Figure 1, since they do not reduce the activation memory which accounts for a large portion of memory usage.\nPruning LLM Model Parameters: To reduce the model memory of fine-tuning, two categories of pruning methods have been proposed (Liang et al., 2021): structured pruning and unstructured pruning. Structured pruning methods remove grouped parameters (e.g., channels, layers) from the LLM. However, they usually degrade the accuracy. Furthermore, they have a limitation in terms of the compression ratio because of the low flexibility."}, {"title": "Motivation", "content": "In this section, we present a pivotal research question for resource-efficient fine-tuning.\nRQ: Do all adapters contribute equally to the process of adaptation?\nTo answer this question, we analyze the impact of adapters injected into each transformer layer on accuracy and resource efficiency. We measure the accuracy and memory usage of BERTbase model on MNLI and QNLI dataset from GLUE (Wang et al., 2018), by attaching an adapter to each transformer layer one-by-one. Figure 2(a) and (b) show the measured accuracy and memory usage respectively the x-axis indicates the index of transformer layer that the adapter is injected into.\nAs shown in Figure 2(a), each adapter has different impact on the accuracy, and the importance of each adapter varies depending on the downstream task. In addition, despite uniform counts of trainable parameters, resource usage decreases for adapters closer to the output layer, as depicted in Figure 2(b). These observations point to the possibility that adapters in early layers contribute less to task adaptation, even though they require considerable resources. In other words, if we selectively deactivate less impactful adapters, it is possible to co-optimize the resource efficiency and accuracy.\nTo further analyze changes of the feature representations for each adapter throughout the training process, we quantify the representation similarity between adapters in each training step and those in the final model (which we obtained after the convergence of fine-tuning). We quantify the representational similarity using Centered Kernel Alignment (CKA) by referring to previous works (Li et al., 2022a). Figure 3 visualizes the representational similarity measured throughout the training"}, {"title": "Selective Adapter Freezing (SAFE)", "content": "In this section, we propose a selective adapter freezing method, SAFE. SAFE adaptively freezes less important adapters in the early training steps, in order to reduce unnecessary computation and memory usage without compromising the accuracy.\nFigure 4 shows the overview of SAFE. SAFE consists of two stages: warm-up stage and freezing stage. In the warm-up stage, SAFE performs several epochs of fine-tuning while monitoring the feature representation changes (i.e., importance score) of the adapters (Section 4.1). If the important score"}, {"title": "Importance Score", "content": "In the warm-up stage\u00b9, we identify less important adapters by monitoring the feature representation changes of the adapters. To capture the feature representation changes of the adapters, SAFE uses Centered Kernel Alignment (CKA), which is a representative metric for representation similarity similar practice has been used in previous works (Neyshabur et al., 2020; Raghu et al., 2021). It calculates CKA between the activation of a layer adapted with an adapter and that of the original layer as:\n$CKA (X_i, Y_i) = \\frac{||Y_i^T X_i||_F^2}{||X_i^T X_i||_F ||Y_i^T Y_i||_F}$.\nwhere $X$ and $Y$ are the activations of a layer that is adapted with an adapter and the original layer, respectively, i = Index of Layer, and $|| \\cdot ||_F^2$ represents the square of the Frobenius norm of a matrix. Higher CKA value indicates that the feature representation of a layer is still similar with that of the original one. To this end, SAFE calculates the importance score of an adapter as:\n$Imp(Adapter_i) = 1 - CKA(X_i, Y_i)$."}, {"title": "Adapter Freezing", "content": "In the freezing stage, SAFE gradually freezes adapters based on their importance score. At $t_w$-th epoch, SAFE compares the importance score of adapters with threshold $\\tau_T$. If the importance score of an adapter is lower than $\\tau_T$, SAFE identifies the adapter as a freezing candidate. After identifying freezing candidates, SAFE gradually freezes them based on a moving threshold $\\tau_T$ until $t_f$-th epoch it increases 0 to $\\tau_T^2$ between $t_w$-th and $t_f$-th epochs following a cubic schedule as (Zhang et al.,\nWe define the number of warm-up epochs as the epoch at which the importance score of all adapters change by less than 5% for consecutive epochs.\n\u00b2We empirically determine $T_T$ and final freezing epochs $t_f$ based on extensive experiments with various models and datasets."}, {"title": "Regularization Effect of SAFE", "content": "By selectively freezing less critical adapters, SAFE induces a regularization effect within the model. In transformer-based PLM $N_o$, each of the l transformer blocks $T_i$ is equipped with a distinct set of parameters $\\Theta_l$ for $l \\in \\{1, ..., n\\}$. To reduce the computational overhead of directly fine-tuning all parameters, lightweight adapters $\\Delta \\theta_l$ are introduced. To clarify how introducing adapters contributes to performance enhancements, Fu et al. (Fu et al., 2023) formalize the optimization function as follows:\n$min_{\\theta} L(\\theta) + ||(I \u2013 M)(\\theta \u2013 \\theta_o)||_2^2$,\nwhere $\\theta = \\theta_o + M\\Delta \\theta$ and $M \\in \\{0, 1\\}^{m \\times m}$, with $m = dim(\\theta)$, serves as a diagonal matrix for selective parameter adjustment. Each diagonal element $M_{ii} \\in \\{0, 1\\}$ indicates whether the corresponding parameter of $\\Delta \\theta_i$ is active (1) or inactive (0), with all off-diagonal elements $M_{ij}$ set to 0. The regularization term is crucial for explaining how parameter constraints introduced by adapters can enhance model performance on downstream tasks. The $rank(M)$ is bounded by m, reflecting full capacity for parameter adaptation within each transformer block. However, such an approach can lead to excessive computation. In contrast, our study explores the implications of constraining the $rank(M)$ to a reduced upper limit of by selectively activating $AW_l$ for $T_i$. This constraint not only optimizes computational efficiency but also preserves the adaptability essential for superior performance on downstream tasks, as evidenced by our empirical results detailed in Section 5.3."}, {"title": "Experiments", "content": "Experimental Setting\nModels: We assess the fine-tuning efficacy of SAFE using state-of-the-art transformer-based models, including BERTbase, BERTlarge (Kenton and Toutanova, 2019), ROBERTabase, ROBERTalarge (Liu et al., 2019), GPT-2medium, and GPT-2large (Radford et al., 2019).\nDatasets: The aforementioned models are evaluated across various tasks that span a broad spectrum of NLP applications, including Natural Language Understanding (NLU), Question Answering (QA), and Natural Language Generation (NLG). Initially, we utilize eight datasets from the General Language Understanding Evaluation (GLUE) (Wang et al., 2018) which comprises two single-sentence classification tasks, three similarity and paraphrase tasks, and four natural language inference tasks. Furthermore, we conduct experiments on the SQUAD dataset (Rajpurkar et al., 2016) with both BERT and ROBERTa model families. Decoder-only models such as GPT-2large are also tested to determine if SAFE maintains its effectiveness in"}, {"title": "Main results", "content": "Natural Language Understanding\nTable 1 shows the results of different methods on GLUE tasks. Since SAFE selectively freezes 51.04% of less important adapters early throughout the training process, SAFE significantly reduces memory usage by 40.47%, from 20.35GB (LORA) to 12.11GB, and decreases computation costs (FLOPs) by 35.15%. Even with such improvements in resource efficiency, SAFE improves the average GLUE score from 84.66 (LoRA) to 84.99 this is because SAFE induces a regularization effect on less-important adapters improving generalization performance of the model (see Section 5.3).\nCompared to AdapterDrop, SAFE provides up to 2.69% higher score (MRPC) while reducing memory usage by 49.47% (0.91% higher GLUE score and 40.47% reduced memory usage on average). This is because AdapterDrop reduces computation"}, {"title": "Question Answering", "content": "Table 2 shows the results of SQUAD dataset. SAFE consistently outperforms baseline under all settings. Notably, SAFE reduces memory usage and computation costs by up to 79.92% and 88.41% on ROBERTalarge by freezing 91.67% of the adapters, while improving the F1 score from 93.39 (LoRA) to 94.13. This result also demonstrates that the benefits and effectiveness of SAFE are not restricted to specific model sizes, making it a valuable strategy for enhancing adapter-tuning outcomes across models of varying scales."}, {"title": "Natural Language Generation", "content": "Table 3 shows that SAFE prevails on natural language generation task with GPT-2. SAFE achieves comparable performance to LoRA across all metrics while significantly reducing memory usage. This result also demonstrates that SAFE is effective not only for encoder models but also works well with decoder models.\nTo elucidate the underlying mechanisms behind SAFE's enhancements in model performance and memory efficiency, we conduct a detailed empirical analysis. We visualize and compare the loss landscapes of the baseline and SAFE. Additionally, we quantitatively evaluate the flatness of the loss surfaces by analyzing the spectrum of Hessian eigenvalues. This methodical approach allows us to substantiate the improvements attributed to SAFE, providing insights into its effectiveness in optimizing both performance and resource utilization."}, {"title": "Regularization Effect", "content": "Loss Landscape Analysis. The flatness of a loss landscape is a recognized indicator of the generalization ability of models (Jiang et al., 2020). Specifically, flatter landscapes are indicative of enhanced robustness to parameter perturbations (Xie et al.,"}, {"title": "Resource Efficiency", "content": "We evaluate the resource efficiency of SAFE in terms of memory usage, computation amount, and training time. Table 4 shows the average resource efficiency improvement for the main results on NLU, QA and NLG tasks. Overall, SAFE reduces memory usage, computation amount, and training time by 42.85%, 34.59%, and 11.82% compared to LoRA, respectively (on average). This means that SAFE can fine-tune twice as many downstream tasks under the same FLOPs budget and further enable on-device fine-tuning for personalization. For example, when fine-tuning a RoBERTalarge model with a question answering downstream task, SAFE reduces memory usage from 17.73GB to 3.56GB; 8GB is the usual memory size of the edge devices."}, {"title": "Expanded Experimental Results", "content": "Image Classification Task Evaluations. In Appendix A, we conduct comprehensive evaluations of SAFE on a variety of image classification tasks. These experiments consistently demonstrate the efficacy of SAFE, confirming its robust performance across diverse vision-related applications.\nCompatibility with Advanced Adapters. Further discussions on the integration of SAFE with various advanced adapter modules are presented in Appendix B. Our results highlight SAFE's versatility and compatibility with multiple adapter-tuning frameworks (Houlsby et al., 2019; Zaken et al., 2022). This adaptability ensures that SAFE's methodology remains effective, independent of specific adapter designs, thereby facilitating scalability"}, {"title": "Conclusion", "content": "In this paper, we propose SAFE, which selectively freezes adapters for enabling resource efficient fine-tuning of PLMs. We observe that not all adapters contribute equally to adaptation. Motivated by the observation, SAFE gradually freezes less-important adapters, which do not contribute to adaptation during the early training steps. In our evaluation on various models and datasets, SAFE significantly saves memory usage and computation and accelerating training time, with comparable (or even better) accuracy. We also demonstrate that SAFE induces regularization effect, thereby improving generalization performance and accuracy compared to the state-of-the-art PEFT methods. We believe that SAFE can enable resource-efficient fine-tuning of large-scale PLMs, and further pave the path forward to personalized fine-tuning on resource-constrained edge devices."}, {"title": "Limitations", "content": "We suggest the need for combination with prior research on memory-efficient training. These include low precision, microbatching, weight sharding, and gradient checkpointing techniques. Though we have not evaluated SAFE along with such memory-efficient training methods, SAFE can be complementarily used along with the methods since SAFE can be applied independently of the training method or weight precision. In particular, since the quantization-based compression technique is quite popular and effective in terms of both compression ratio and preservation of final accuracy, favorable results are expected from combining the proposed technique with the memory-efficient training methods (Han et al., 2015)."}, {"title": "Experimental Setup", "content": "Model\nWe conduct experiments using a pre-trained model deployed on HuggingFace (Wolf et al., 2019). For experiments on the NLU and QA benchmarks, we use bert-base-uncased and bert-large-uncased trained on BookCorpus, a dataset consisting of 11,038 unpublished books and English Wikipedia. We use roberta-base and roberta-large trained on 5 datasets (BookCorpus, English Wikipedia, CC-News, OpenWebText, and Stories) for the RoBERTa model. For experiments on the NLG benchmark, we use GPT-2 medium and GPT-2 large distributed by OpenAI. We use vit-base-patch16-224-in21k and vit-large-patch16-224-in21k distributed by Google for experiments in the ViT model. Finally, We use the swin-base-patch4-window7-224 and swin-large-patch4-window7-224 models distributed by Microsoft for experiments in the SWIN model.\nComputing Resources\nOur experimental setup leverages 2 RTX4090 with 24GB memory for NLU, QA, and NLG tasks and 1 RTX 4090 for CV downstream task.\nDataset Statistics\nWe present the dataset statistics of GLUE and SQUAD in following table."}, {"title": "Hyperparameter Settings", "content": "We explore 10% of all epochs for at least 5 learning rates. Hyperparameter settings, including learning rate, are made by referring to previous works (He et al., 2023; Houlsby et al., 2019; Hu et al., 2021; Zaken et al., 2022). We use the AdamW optimizer (Loshchilov and Hutter, 2018) and LinearLR learning rate scheduler and set weight decay to 0 in experiments. In our evaluation, we configure LoRA as follows: r = 4, alpha = 16, target modules = [\"query\", \"value\"], and LoRA dropout = 0.1."}]}