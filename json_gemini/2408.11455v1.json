{"title": "Using Part-based Representations for Explainable\nDeep Reinforcement Learning", "authors": ["Manos Kirtas", "Konstantinos Tsampazis", "Loukia Avramelou", "Nikolaos Passalis", "Anastasios Tefas"], "abstract": "Utilizing deep learning models to learn part-based represen-\ntations holds significant potential for interpretable-by-design approaches,\nas these models incorporate latent causes obtained from feature represen-\ntations through simple addition. However, training a part-based learning\nmodel presents challenges, particularly in enforcing non-negative con-\nstraints on the model's parameters, which can result in training diffi-\nculties such as instability and convergence issues. Moreover, applying\nsuch approaches in Deep Reinforcement Learning (RL) is even more de-\nmanding due to the inherent instabilities that impact many optimization\nmethods. In this paper, we propose a non-negative training approach for\nactor models in RL, enabling the extraction of part-based representations\nthat enhance interpretability while adhering to non-negative constraints.\nTo this end, we employ a non-negative initialization technique, as well\nas a modified sign-preserving training method, which can ensure bet-\nter gradient flow compared to existing approaches. We demonstrate the\neffectiveness of the proposed approach using the well-known Cartpole\nbenchmark.", "sections": [{"title": "1 Introduction", "content": "Deep Reinforcement Learning (RL) has achieved state-of-the-art performance in\nvarious applications, including robotics [1,2]. However, the use of RL agents in\ncritical environments, where safety is highly prioritized, is hindered due to the\nlimited transparency of the models. Extracting the rationale of a deep learn-\ning (DL) model in a human-interpretable way remains a challenging task, but\ndoing so would be highly useful for improving both the performance and trust-\nworthiness of the model, as well as preventing failures [3]. To this end, post-hoc\nexplanation methods have been extensively studied over the years, providing ra-\ntionales for the predictions of the model [4,5]. However, such approaches cannot\nalways provide a reliable explanation [6,7], with pre-hoc methods for explain-\nable AI gaining increasing attention recently [8]. The pre-hoc approaches aim"}, {"title": "2 Proposed Method", "content": "In this work, we focus on training non-negative agents using policy gradient-\nbased approaches, such as the PPO algorithm [17], but without loss of generality,\nsince the proposed method can also be directly applied to other RL methods as\nwell, such as Q-learning based approaches. More specifically, PPO utilizes actor-\ncritic networks, where the actor model decides which action should be taken, with\nits parameters denoted as 0. On the other hand, the critic network, equipped"}, {"title": "3 Experimental Evaluation", "content": "We experimentally evaluated the proposed method on the typical RL benchmark\nCartpole implemented using the Deepbots framework [21,22]. More specifically,\nthe simulated environment is composed of a four-wheeled cart that has a long\npole attached to it by a free hinge. On the top of the pole, there is a sensor to\nmeasure their vertical angle. The pole acts as an inverted pole pendulum and\nthe goal is to keep it vertical by moving the cart forward and backward.\nWe applied the PPO algorithm to all evaluated cases, with the actor network\ngetting the observations of the agent as input, consisting of two hidden layers of\n10 neurons each, and outputs the action of the agent. Similarly, the critic network\ngets the observations as input, and it outputs the advantage of each state. The\ncritic network also consists of two hidden layers of 10 neurons. On both networks,\nwe employed the ReLU activation function in the hidden layers, with the actor"}, {"title": "4 Conclusions", "content": "In this study, we have introduced a novel training approach that focuses on non-\nnegativity in deep RL using PPO. The proposed approach enables the extraction\nof part-based representations, which offers enhanced interpretability while fol-\nlowing non-negative constraints associated with human cognition. To achieve\nthis objective, the proposed method employs a non-negative initialization tech-\nnique, followed by a modified sign-preserving training method. More specifically,\nwe proposed employing an exponential distribution-based non-negative initial-\nization method for the actor model and then using an appropriately modified\nsign-preserving alternative to Stochastic Gradient Ascent (SGA) for training the\nactor model in a non-negative manner. By adopting the proposed method, we\ncan mitigate issues related to the reduction of the learning capacity of models\nand the vanishing gradients due to the use of clipping mechanisms involved in\nexisting approaches. This helps mitigate issues such as vanishing gradients and\nenhances training stability. Consequently, the proposed pipeline enables more ef-\nficient training of inherently explainable models based on the non-negative part-\nbased representation of the actor. To validate the effectiveness of the proposed\nmethod, we conducted experiments on the well-established Cartpole benchmark.\nThe results demonstrate the effectiveness of the proposed method in achieving su-"}], "equations": ["rt(0) = \\frac{\u03c0\u03b8(a|St)}{\u03c0\u03b8old(a|St)} \u2208 R,", "relip t (0) = clip(rt(0), 1 \u2013 \u03b5, 1 + \u03b5) \u2208 [1 \u2013 \u03b5, 1 + \u03b5],", "clip(x, m, M) = max(min(x, M), m) \u2208 [m, M].", "Lactor (st; 0, \u0113) = Et [min (relip t (0) At(0), rclip t (0) At(0))] \u2208 R,", "dt(0) = Rt + \u03b3\u03bbV (St+1) - V (st) \u2208 R,", "At(0) = \u03a3n-t i=0 \u03b3\u03b5\u03bb\u03ac\u03b4\u03b5+i(\u0113) \u2208 R,", "Leritic = Et [dt()2] \u2208 R", "0 ~ Exp(x) = \\frac{ln(U(0, 1))}{\u03bb} \u2208 R+,", "0 = Oold + Na\\frac{\\frac{\u2202Lactor}{\u22020old}}{|\\frac{\u2202Lactor}{\u22020old}|},", "\u03b8 = \u03b8old - Nc \\frac{\u2202Lcritic}{\u2202\u03b8old},", "\u03b8' = max (0,0 - \u03b7 \u2202J \u2202\u03b8)."]}