{"title": "Towards Physiologically Sensible Predictions via the Rule-based Reinforcement Learning Layer", "authors": ["Lingwei Zhu", "Zheng Chen", "Yukie Nagai", "Jimeng Sun"], "abstract": "This paper adds to the growing literature of reinforcement learning (RL) for healthcare by proposing a novel paradigm: augmenting any predictor with Rule-based RL Layer (RRLL) that corrects the model's physiologically impossible predictions. Specifically, RRLL takes as input states predicted labels and outputs corrected labels as actions. The reward of the state-action pair is evaluated by a set of general rules. RRLL is efficient, general and lightweight: it does not require heavy expert knowledge like prior work but only a set of impossible transitions. This set is much smaller than all possible transitions; yet it can effectively reduce physiologically impossible mistakes made by the state-of-the-art predictor models. We verify the utility of RRLL on a variety of important healthcare classification problems and observe significant improvements using the same setup, with only the domain-specific set of impossibility changed. In-depth analysis shows that RRLL indeed improves accuracy by effectively reducing the presence of physiologically impossible predictions.", "sections": [{"title": "1. Introduction", "content": "The healthcare community has witnessed a surging interest in reinforcement learning (RL) as a solution to a wide variety of clinical decision problems (Gottesman et al., 2019; Liu et al., 2020; Yu et al., 2021). Unlike supervised learning models that take as input abundance of IID data and outputs a mapping from data to true labels, RL operates under the sequential decision making framework that learns from trial-and-error interaction with the environment (Sutton & Barto, 2018). As noted by Yu et al. (2021), RL is desirable as it is capable of finding optimal policies using only previous experiences of treatment and outcomes. This makes RL more attractive than existing pharmacodynamics- or simulation-based studies where an accurate model is intractable due to heterogeneity of biological systems and their individual responses. So far, successful RL examples include dynamic adaptive treatment (Guez et al., 2008; Pineau et al., 2009; Vincent, 2014; Fatemi et al., 2021) to automatic medical diagnosis (Kao et al., 2018; Peng et al., 2018; Xu et al., 2019; Xia et al., 2020). These applications often take as input clinical observations and assessments of patients, with success heavily dwelling on expert knowledge such as clinical decision rules (Heerink et al., 2023; Yu et al., 2023) or dynamics of disease progression (Saboo et al., 2021; Hu et al., 2023). The expert knowledge is required to design an adequate Markov Decision Process (Puterman, 1994) such that the states/rewards/dynamics are faithful and informative; the actions are reflective of intended changes and thus the optimal policy can represent meaningful solutions.\nThis paper complements the growing body of RL for healthcare literature by focusing on another important but less studied problem: supporting physiologically sensible predictions. This problem naturally emerges from data-driven learning models that achieve high prediction accuracy but can make mistakes that violate the physiological rules. Take sleep for example, a human goes through five sleep stages, from wake to rapid eye movement (REM). While a sudden transition from REM to wake is possible, the reverse direction violates the physiological rules and is therefore impossible (Berry et al., 2013). However, these rules are implicit underlying training data, and the established techniques or data transformation can cause a discontinuity to the rules. As a result, the data-driven models on one hand can achieve high accuracy; on the other hand, they inevitably suffer from physiologically impossible predictions resulting from not respecting the rules during training, preventing them from deployment to real circumstances (Burrello et al., 2020; Boonyakitanont et al., 2021; Theodorou et al., 2024).\nWe propose to tackle this challenge with a new framework that augments the data-driven models with an additional Rule-based RL Layer (RRLL). Specifically, RRLL is simple and lightweight: it does not require heavy expert knowledge such as the knowledge of disease progression to characterize possible states, dynamics and reward functions (Fatemi et al., 2021; Hu et al., 2023). By contrast, it is sufficient for RRLL to function knowing only the rules of impossibility. This often creates a much smaller set compared to the rules of possibility, e.g. the wake stage can transition to many other stages that eventually lead to REM. With the rules of impossibility, the RL layer takes in predictions output by a classifier as states, and outputs as actions physiologically sensible, reassigned labels, evaluated by the rules as rewards. After training, the entire model can work in an end-to-end manner with RRLL simply acting as the last layer.\nThe contributions of the paper are threefold. Firstly, we present a new framework that augments any data-driven model with an additional rule-based RL layer (RRLL) to support clinically sensible decisions. We are unaware of any published results that address a similar idea. In Section 2 we discuss physiological considerations and existing methods in detail. Secondly, we show the efficacy of the proposed RL layer by extensive experiments with real physiological data for sleep stage prediction and seizure detection. We comprehensively discuss the MDP for the RL problem in Section 3. Lastly, we make public the datasets and to facilitate future research such as utilizing more sophisticated RL algorithms for the layer or developing other healthcare datasets into RL environments."}, {"title": "2. Related Works", "content": "RL for Healthcare. The literature of RL for healthcare has mainly evolved around dynamic adaptive treatment and automatic diagnosis (Yu et al., 2021). The former being more intuitive, often leverage disease assessments as states and dosage as actions, and the goal is to learn an optimal policy that can adaptively recommend dosage along disease progression (Guez et al., 2008; Pineau et al., 2009; Fatemi et al., 2021; Li et al., 2023). The latter typically abstracts symptoms as a binary state vector, with actions corresponding to queries for a certain symptoms in the state vector. The optimal policy is one that can identify the user's disease as soon as possible by making as few as possible queries (Peng et al., 2018; Xu et al., 2019; Zhong et al., 2022; Yu et al., 2023). Our work differs from them in that RRLL aims at supervising the base predictor model rather than recommending treatments or potential disease classification. RRLL can be seen as playing the role of the predictor by drawing reassigned labels, and hence it is really an additional layer to the predictor. At the same time, the reassigned label is drawn from an optimal policy that is learned from reinforcement signals characterized by rules, hence the name rule-based reinforcement learning.\nPhysiological Decision Making. Recent advances in the healthcare community have shown great promise in automating the laborious process of identifying human physiological states and based on which perform clinical decisions. Successful examples include whole-night sleep stage reconstruction (Hwang et al., 2021; Chen et al., 2022; Thapa et al., 2024), epileptic seizures (Tang et al., 2022; Kotoge et al., 2024) and arrhythmias (Golany et al., 2021; wei H. Lehman et al., 2023). The identification problem is typically cast as a classification framework, under which long-term assessment of patients (e.g. physiological signals) are segmented into raw states of short duration, learned by neural networks for state-relevant features in the hope for capturing long-term dependency of physiological states. However, beyond the capability of mapping input to physiological states, reliable models must adhere to task-specific rules for clinical decision making (Sutton et al., 2020; Theodorou et al., 2024). Physiological phenomena typically exhibit gradual transitions, e.g. the progression from wakefulness to deep sleep (Phan et al., 2022), or the stabilization of heart rates during homeostasis (Saul & Valenza, 2021). These sequential transitions are inherently tied to clinical diagnostic criteria (Berry et al., 2013) and must be carefully modeled to enhance the interpretability and physiologically plausibility.\nPhysiological Rules as Learning Constraints. Few efforts attempted to embed the physiological rules into the model by e.g. learning rule-conditioned features (Niroshana et al., 2019; Perslev et al., 2021; Al-Hussaini & Mitchell, 2022); incorporating constraints into the loss function (Phan et al., 2019; 2022; Chen et al., 2023); employing post-processing modules to rectify output constraint violations (Burrello et al., 2020; Li et al., 2021; Boonyakitanont et al., 2021) or integrating additional components to enforce alignment between output and the rules (Huang et al., 2022). Despite a certain degree of effect of these methods, a reliable, effective and general clinical decision support system is still missing in the literature. One important reason is that the rules are domain-specific. Existing methods were typically designed on a case-by-case basis, informed by heavy expert knowledge. For instance, sleep studies often incorporate rules into"}, {"title": "3. Proposed Method", "content": "Our proposed architecture augments a data-driven model with an additional RL layer. This layer corrects the physiological impossible predictions from the model by trial-and-error learning and outputs a reassigned label. This reassignment is evaluated by a simple rule-based reward function given by a known set of impossibility."}, {"title": "3.1. Problem Formulation", "content": "Assume the classification setting with a dataset $D = {x,y}$ and a data-driven model $f$. $x$ can be patient profiles or neurophysiological signals. The model maps input data $x \\in \\mathbb{R}^N$ to one-dimensional class label $f(x) = \\hat{y} \\in [K]$, where $K > 2$ is an integer and $[.]$ denotes the integer set ${1, 2, ...}$ of all possible labels. We use boldface to distinguish between a scalar predicted label $\\hat{y}$ from its one-hot vectorized counterpart $y$. We do not restrict the model class of $f$, though we use a neural network $f_\\theta$ in this paper. The parameters $\\theta$ are updated by a loss function that compares the difference between the predicted label $\\hat{y}$ to the true label $y$. In this work, we assume that $f_\\theta$ has been trained and $\\theta$ is frozen, so it does not interact with the RL part.\nFor each input sample $x$, we extract the feature vector from the second-to-last layer of $f_\\theta$, let us denote it by $z \\in \\mathbb{R}^M$, and typically $M < N$. The predicted label $\\hat{y}_t = f_\\theta(x)$, feature vector $z$ will be used as part of the input to the RL layer, whose goal is to improve prediction accuracy by minimizing the amount of impossible transitions."}, {"title": "3.2. Markov Decision Process Design", "content": "RL problems are typically formulated as a Markov Decision Process (MDP) defined by the tuple $(S, A, P, r, \\gamma)$, where $S$ denotes the state space, $A$ the action space. $P$ denotes the transition probability and $r$ the reward function. $\\gamma \\leq 1$ is the discounting factor that discounts future values. We assume the discounting factor to be 1 since in our setting future predictions are of equal importance.\nOur RL agent directly works on labels. Therefore, we define $A$ as the space of all possible $K$-dimensional one-hot labels. We distinguish between the one-hot vector $a$ and its scalar version $a \\in [K]$. To give sufficient information to the agent to give meaningful reassignment, we design our state space as follows: at each time step $t$, for input instance $x_t$, we pass it through the classification network to obtain its feature vector $z_t$ and the predicted label $f_\\theta(x_t)$. Because physiological rules typically depend on consecutive physiological stages, we also remember the previous action or reassigned one-hot label $a_{t-1}$. The three quantities are concatenated to yield a $2K + M$ dimensional state $s_t := [\\hat{y}_t, z_t, a_{t-1}]$. The agent then samples a new label from its softmax policy $\\pi(a_t|s_t)$ to be reassigned to $x_t$.\nTo evaluate the desirability of the state-action pair $(s_t, a_t)$, we need to define our reward function. The reward is defined based on the set of impossibility granted by prior knowledge. For example, we know that a human can never directly transition from wake to rapid eye movement. Therefore we can"}, {"title": "3.3. Rule-based Reinforcement Learning Layer", "content": "It is clear that the problem tackled by the RL layer depends on the quality of the preceding data-driven model. In the extreme case where the model is perfect with 100% accuracy, there is nothing can be learned by the RL layer and the highest possible cumulative return is simply 0. On the other hand, if the model is poor, it gives a large room for potentially higher cumulative reward. But in this case, since the predicted label is highly likely to be wrong, its feature vector $z$ may not be very useful, and the RL agent would rely on randomly selecting other labels until bumping into the correct one, which requires longer training horizon and/or stronger exploration mechanism. To compensate for this, we adopt the classic $\\epsilon$-greedy method to facilitate exploration.\nLet us define the cumulative return $\\mathcal{G}(s_t, a_t)$ summing over the prediction horizon $T$. Our goal is to learn an optimal policy that maximizes the expected return $G_t := \\mathbb{E} \\left[\\sum_{t'=t}^T r(s_{t'}, a_{t'})\\right]$. To train the layer, we can let $T$ denote the length of the trajectory input to the classifier network $f_\\theta(\\cdot)$. However, when the trajectory is long, it may be more preferable to manually segment the input e.g. by individuals so that $T$ becomes smaller.\nAmong the plethora of RL algorithms, the classic policy search algorithm REINFORCE (Williams, 1992) stands out for our setting due to its simplicity and interpretability. REINFORCE is an on-policy algorithm that optimizes the policy towards maximizing cumulative reward by sampling from the current policy (Sutton et al., 1999). This fits well with the healthcare setting where it is typical to iterate each individual sample to update the policy (Hartvigsen et al., 2019). Extending the framework to off-policy or even offline algorithms is an interesting future direction.\nLet us denote the policy network parametrized by $\\phi$. We optimize the network by minimizing the following loss:\n$\\mathcal{J}(\\phi):= -\\mathbb{E} \\left[\\sum_{t=0}^T \\log \\pi_\\phi (a_t | s_t) \\sum_{t'=t}^T r(s_{t'}, a_{t'}) - b_\\psi(s_t) \\right]$\nwhere $b_\\psi$ is a state-dependent baseline parametrized by $\\psi$ for better numerical stability. The baseline network is simply updated by regression to the summation of rewards\n$\\mathcal{J}(\\psi) := \\mathbb{E} \\left[ b_\\psi(s_{t'}) - \\sum_{t'=t}^T r(s_{t'}, a_{t'}) \\right]^2$\nIn physiological processes, it is typical for a status to remain unchanged for a period of time. To take this fact into account, we impose a penalty when the current action is different than the last action. In summary, RRLL optimizes the following objective:\n$\\mathcal{L} = \\mathcal{J}(\\phi) + \\mathcal{J}(\\psi) + \\alpha \\sum_{t=0}^T \\mathbb{1}\\{a_t \\neq a_{t-1}\\} \\log \\pi(a_t|s_t),$\nwhere $\\alpha > 0$ is a weighting coefficient for the penalty. Minimizing the likelihood is equivalent to minimizing the probability of an action that leads to a reassigned label. After the training of RRLL, the entire model can be used as a whole to perform end-to-end inference with RRLL acting simply as the last layer for predicting labels. Implementation details are provided in Appendix B."}, {"title": "3.4. Workflow of RRLL Training", "content": "Figure 1 illustrates the workflow of training RRLL. It is worth pointing out that the base predictor does not interact with RRLL in the sense that once its weights are frozen once it has been trained. Therefore, RRLL relies only on the predicted labels and feature vectors. This renders RRLL lightweight and general since only labels and features need to be stored but not the potentially complex model that could comprise billions of parameters. RRLL draws a new label as its action to the input instance and the input state and action will be evaluated by the set $\\zeta$. After training, the entire model (base predictor plus RRLL) can be used end-to-end on any prediction task same as the base predictor.\nRemark. On the technical level, our method bears some similarity to (Peng et al., 2018; Hartvigsen et al., 2019). Specifically, Peng et al. (2018) used binary vectors representing symptoms as input and outputs a binary vector indicating potential diseases. By contrast, we gather one-hot predicted labels, previous action and features to give sufficient information to the agent to output sensible relabeling, acting as a teacher to the classifier. Hartvigsen et al. (2019) also adopted REINFORCE with similar losses for learning an early-stopping seizure detection policy. Their setting is simpler as the agent faces a sequence of binary classification problems of stop or continue, and each trajectory yields either 0 or 1 reward. RRLL by contrast, needs to simultaneously discern physiologically impossible actions from a sequence of rewards defining different situations at each step and reassign a correct label."}, {"title": "4. Experiments", "content": "We verify the utility of RRLL by important healthcare classification problems that often see physiological impossible predictions. We take the state-of-the-art models as the base predictor and investigate how RRLL can improve prediction by reducing physiologically impossible predictions."}, {"title": "4.1. Proof-of-Concept: Sleep Staging", "content": "Human sleep goes through several sleep stages from light sleep to deep sleep. Automating the laborious manual sleep staging process has been a popular topic in the AI4Science literature. However, using RL for better stage prediction has not been studied before to the best of our knowledge. We use the largest public sleep database \u2013 the Sleep Heart Health Study that has 42,560 hours of EEG recordings from 5,793 subjects. Sleep stages are scored into 5 classes: Wake, N1, N2, N3 and REM. We use 85% of patients for training and 15% for testing.\nWe choose the fragment-wise FC-Attention model (Chen et al., 2023) as the base predictor. To evaluate RRLL along during the base predictor training, we store the features and predicted labels (z, \u0177) after a certain number of training epochs (1, 2, ... 100) and label these models by the number of epochs they have been trained, e.g. model 100 indicates the model trained after 100 epochs. RRLL is then applied"}, {"title": "Sleep Rules", "content": "The five distinctive sleep stages from wake to light stages (N1 and N2) and deep sleep (N3), culminating in rapid eye movement (REM) (Berry et al., 2013) represent gradual, transitioning processes, standardized into clinical rules to guide experts in labeling whole-night physiological recordings (e.g., EEGs) (Hobson & Pace-Schott, 2002). Though it may be rare but still possible that a human goes from REM to wake; going from wake to REM is impossible. Therefore, for sleep we can define the set of impossibility $\\zeta$:\n$\\left(\\begin{array}{ll}\\\\\nsleep := & \\begin{array}{ll}\\\\\nN1 & \\nrightarrow N3, \\text{REM} \\\\\\\\\nN2 & \\nrightarrow N3, \\text{REM} \\\\\\\\\nN3 & \\nrightarrow \\text{Wake} \\\\\\\\\n\\text{REM} & \\nrightarrow N1 \\\\\\\\\n\\text{Wake} & \\nrightarrow N1, N3. \\end{array}\n\\end{array}\\right)$\nThis set of rules is used in computing the reward Eq. (1). Whenever the agent tries to take an action $a_t$, it will be judged by the rules to see if it is a possible transition by $\\zeta_{\\text{sleep}} (a_{t-1})$. An impossible transition will be counted as a constraint violation. We visualize how RRLL improves accuracy by reducing constraint violation in the results."}, {"title": "Results", "content": "Table 1 summarizes the performance of RRLL on sleep. It can be seen that RRLL outperforms all baselines in both tasks by a noticeable margin. From the model learning perspective, RRLL contributes a new, state-of-the-art model that can be readily used for any prediction task.\nTo analyze why RRLL can achieve the superior performance, we conduct extensive visualization. Figure 3 shows the cumulative reward learning curves of RRLL on top of all base predictor models from 1 to 100. It is visible that RRLL is capable of quickly and robustly converging on all base models. Since the reward function Eq. (1) encodes the desirability of each state, a rise in cumulative rewards indicates an increase of correct assignments and a decrease of physiological impossible predictions. The increase of accuracy can be seen from figure 4 where the average improvement attained by RRLL is shown to be consistently better than the original model from the very beginning of learning, to the point where the model starts to overfit (brown bars of model 40 and 100). This shows RRLL is robust even in the extreme case even when the base predictor model is highly accurate with no much room for further improvement.\nFigure 5 visualizes how RRLL reduces the percentage of constraint violation per Eq. (3) in the entire dataset along with the model learning and RL progress. By observing model 100, another observation can be drawn: the predictor model improves accuracy not by minimizing physiological impossible predictions, which still account for a non-negligible portion in model 100 and therefore can still be effectively reduced by RRLL.\nFigure 6 LHS shows a Waffle plot (Li, 2024) of samples associated with different rewards, excluding the action maintain and correct, which accounts for the majority of samples. It is clear that along with learning, the totality of wrong assignments shrinks, and the proportion of RRLL correct reassignments increases. Figure 6 RHS shows the totality of RRLL correct assignments: the proportion of successful actions, no matter maintain or reassign, increases along the RL epochs (the action maintain is only show for the last epoch for uncluttered visualization). At the end of learning, they in total accounts for \u2248 97% accuracy.\nTo gain more intuition of the effectiveness of RRLL, we showcase with a whole-night sleep hypnogram of a selected patient. It can be seen that even on Model 1, RRLL can notably reduce impossible transitions displayed as the green peaks such as the N2-to-REM misclassifications. Even as the base predictor becomes more accurate, RRLL is still capable of removing constraint violations, as can be seen from Model 100, between step 250 and step 750."}, {"title": "4.2. Extra Validation: Seizure Onset Detection", "content": "Another important task that suffers from discontinuous and impossible predictions is seizure onset detection. Around 10% of people will experience a seizure during their lives (Devinsky et al., 2018). Accurate seizure onset prediction from electrophysiological changes is immensely helpful in providing timely intervention. However, accurate prediction is still challenging and existing results remain limited.\nWe use the CHB-MIT database2 that has 844 hours of 22-channel scalp EEG data from 22 patients, including 163 recorded seizure episodes. In this example, three stages are defined as Normal, Preictal (before the actual seizure onset) and Ictal (seizure onset). We opt for the one of the state-of-the-art models (Tang et al., 2022) as our base predictor. For the baselines, we opt for the two-stage post-processing methods that reassign labels to sequences of EEG fragments based on the outputs of prediction models (Burrello et al., 2020; Boonyakitanont et al., 2021). The first baseline called Post employs label voting within a sliding window. The second baseline ScoreNet utilizes LSTM to further score the labels and their corresponding latent vectors. We use the following well-established metrics for seizure onset detection to evaluate performance: normalized mutual information (NMI), adjusted Rand index (ARI), and accuracy (ACC)."}, {"title": "Seizure Rules", "content": "The rules for seizure progression are straightforward: patients transition from a normal state to a pre-ictal state and finally to an ictal (seizure) state. In the dataset we do not take ictal to normal into account:\n$\\left(\\begin{array}{ll}\\\\\n\\text{seizure} := & \\begin{array}{ll}\\\\\n\\text{Normal} & \\nrightarrow \\text{Ictal} \\\\\\\\\n\\text{Preictal} & \\nrightarrow \\text{Normal} \\\\\\\\\n\\text{Ictal} & \\nrightarrow \\text{Normal}, \\text{Preictal} \\end{array}\n\\end{array}\\right)$\nIt is worth noting that because the task is hard and the dataset is imbalanced, the predictor outputs wrong predictions that make RRLL learning slow. We find that a simplified reward function that removes the condition on the predicted label helps the agent better learn:\n$r(s_t, a_t) = \\begin{cases}\n1, & a_t = \\hat{y}_t \\neq y_t \\\\\n0, & a_t = \\hat{y}_t = y_t \\\\\n-1, & a_t \\neq y_t \\text{ and } a_t \\in \\zeta(a_{t-1}) \\\\\n-2, & a_t \\neq y_t \\text{ and } a_t \\notin \\zeta(a_{t-1})\n\\end{cases}$"}, {"title": "Results", "content": "Table 2 summarizes the performance of the baselines. It is clear that RRLL with the base predictor again attains a significant edge. Figure 8 examines the improvement more carefully by visualizing a selected patient going through going through the Normal-Preictal-Ictal process. Since in the considered dataset it is not possible for a patient to jump between Normal and Ictal, a transition between the two states is considered a constraint violation or impossible prediction. The task is known to be challenging, as can be see from the second window that the base predictor outputs highly oscillating predictions alternating between the three possible states. By contrast, from the third window it is evident RRLL is capable of removing most of the physiologically impossible predictions that jump from ictal to normal and vice versa. This can also be seen from the top window showing cumulative rewards. Since positive reward is only given to successful corrections, positive cumulative reward indicates the agent has learned to maintain correct predictions and minimize the impossible ones."}, {"title": "5. Conclusion", "content": "This paper proposed Rule-based Reinforcement Learning Layer (RRLL) that augments any base predictor with the capability of correcting physiologically impossible predictions. RRLL is a layer as it allows the entire model to predict still in an end-to-end manner. At the same time, it is learned in an RL fashion signaled by rules: it takes in as states predicted labels and outputs reassigned labels, evaluated by a reward function based on a small set of impossible transitions. RRLL complements the existing literature on RL for healthcare, and provides a workable Markov Decision Process design that has been shown to perform favorably with a classic policy search algorithm. Extensive experiments showed that RRLL was capable of improving prediction accuracy by effectively reducing impossible predictions, which was not possible by training the base predictor alone.\nOne interesting future possibility lies in trying more advanced RL algorithms such as the off-policy or even offline methods. Another promising direction is to design a more general Markov Decision Process such that the domain-specific set of impossibility can be removed."}, {"title": "Impact Statement", "content": "This paper focuses on improving prediction of machine learning models on physiological problems. The data used in the paper comes from public sources, and our model falls into the regular deep learning models. While our model has the potential to facilitate automated diagnosis/prediction, we feel the societal consequences are not specifically needed to be highlighted here."}, {"title": "Appendix", "content": "A. Experimental Setting"}, {"title": "A.1. Sleep Staging", "content": "Baselines. We selected four baselines: AttnSleep (Eldele et al., 2021), SleepFormer (Phan et al., 2022), FC-Attention (Chen et al., 2023), and FC-Attention+SleepFormer. Clinically, sleep staging is performed on 30-second EEG fragments. AttnSleep and FC-Attention are fragment-wise models that label each fragment individually. They focus on capturing stage-specific temporal or frequency characteristics (e.g., 12-16 Hz sigma waveforms in N2 (Adamantidis et al., 2019)) according to AASM rules, enabling more physiologically meaningful staging outcomes. SleepFormer follows a sequence-to-sequence paradigm (Sutskever et al., 2014), taking multiple EEG fragments as input to Transformer models. It explicitly learns sequential transition information and simultaneously labels a sequence of EEG fragments. (Chen et al., 2023) also evaluate FC-Attention+SleepFormer in their work, which combines the intra-fragment stage-specific characteristics of FC-Attention with inter-fragment transition learning using sequence-to-sequence approach.\nSetup and Metrics. We selected the fragment-wise FC-Attention model as the predictor for our experimental settings. FC-Attention, a Transformer-only general model, effectively captures intra-fragment characteristics but does not account for inter-fragment or stage transition information in its learning process. In our approach, we first trained the predictor to sequentially output labels for each 30-second EEG fragment in patient recordings. We then applied our RRLL framework to reassign these predicted labels based on rule learning. During predictor training, we recorded (z, \u0177) across different training epochs (e.g., 1, 2, and 20, with early stopping applied at the best performance) and applied RRLL to these models. This allowed us to evaluate the effectiveness of RRLL from an untrained state to a well-converged model. We use the well-established metric: precision (Pre), recall (Re) and F1-score to evaluate the effectiveness of baselines and RRLL on SHHS."}, {"title": "A.2. Seizure Onset Detection", "content": "Dataset. We use the CHB-MIT database that has 844 hours of 22-channel scalp EEG data from 22 patients, including 163 recorded seizure episodes. Following the work of (Kotoge et al., 2024), we defined the five-minute period before a seizure as the preictal phase. We divided each dataset into 70%/25%/5% for training, testing, and validation. Patient recordings were segmented into 1-second fragments, and the IDs of all fragments were stored to enable recall in long-term recordings for verifying RRLL.\nBaselines. We compared our proposed method with two post-processing seizure prediction baselines, both of which are two-stage methods that re-assign consistent state labels to sequences of EEG fragments based on the outputs of prediction models. (Burrello et al., 2020) used a sliding 5-second window to re-assign labels based on a patient-specific voting threshold. (Boonyakitanont et al., 2021) applied a weighting phase to score the probabilities of seizure onsets.\nSetup and Metrics. Since the two baselines primarily focus on second stage post-processing, for the predictor in both baseline methods and our proposed setting, we selected the state-of-the-art DCRNN model (Tang et al., 2022). DCRNN uses 12-second EEG fragments as input, effectively capturing brain state changes over time. We then applied the two post-processing baselines and our RRLL framework to correct the model-predicted labels. We evaluated performance of state partitioning using two clustering metrics: normalized mutual information (NMI) and adjusted Rand index (ARI), and accuracy (ACC) for effectiveness of seizure onset detection.\nRules. Approximately 40% of epileptic patients have drug-resistant epilepsy with recurrent seizures that cannot be controlled by available medications (Boonyakitanont et al., 2021). Clinically, many patients benefit from accurate state partitioning of EEG recordings (e.g., normal, pre-ictal, or ictal states) and seizure onset detection, as these help localize and surgically remove the onset zone in the brain, which exhibits the earliest electrophysiological changes during a seizure event. However, current fragment-wise classification models focus on accurately classifying individual fragments without explicitly modeling long-term dependencies or state transitions. This often results in abrupt misclassifications within otherwise state-consistent sequences. In general, the rules for seizure progression are relatively straightforward: patients transition from a normal state to a pre-ictal state and finally to an ictal (seizure) state, as follows:"}, {"title": "B. Implementation Details", "content": "RRLL. The hyperparameters for RRLL is summarized in Table B. Specifically", "classes": "pi(a|s) = \\frac{exp(q_i)}{\\sum_{i=1}^{K}exp(q_i)}$, where $q_i$ is the i-th logit and \u03b7 > 0 is the temperature coefficient. Since we use the \\epsilon-greedy method, at each step, we generate a random number that if larger than e, we sample from the policy \u03c0, otherwise uniformly random from [K"}]}