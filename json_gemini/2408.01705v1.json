{"title": "Downstream Transfer Attack: Adversarial Attacks on Downstream Models with Pre-trained Vision Transformers", "authors": ["Weijie zheng", "Xingjun Ma", "Hanxun Huang", "Jun Yin", "Tiehua Zhang", "Zuxuan Wu", "Yu-Gang Jiang"], "abstract": "With the advancement of vision transformers (ViTs) and self-supervised learning (SSL) techniques, pre-trained large ViTs have become the new foundation models for computer vision applications. However, studies have shown that, like convolutional neural networks (CNNs), ViTs are also susceptible to adversarial attacks, where subtle perturbations in the input can fool the model into making false predictions. This paper studies the transferability of such an adversarial vulnerability from a pre-trained ViT model to downstream tasks. We focus on sample-wise transfer attacks and propose a novel attack method termed Downstream Transfer Attack (DTA). For a given test image, DTA leverages a pre-trained ViT model to craft the adversarial example and then applies the adversarial example to attack a fine-tuned version of the model on a downstream dataset. During the attack, DTA identifies and exploits the most vulnerable layers of the pre-trained model guided by a cosine similarity loss to craft highly transferable attacks. Through extensive experiments with pre-trained ViTs by 3 distinct pre-training methods, 3 fine-tuning schemes, and across 10 diverse downstream datasets, we show that DTA achieves an average attack success rate (ASR) exceeding 90%, surpassing existing methods by a huge margin. When used with adversarial training, the adversarial examples generated by our DTA can significantly improve the model's robustness to different downstream transfer attacks.", "sections": [{"title": "1. Introduction", "content": "Due to the exceptional representation capability of large pre-trained models, it has become a common practice to fine-tune a large pre-trained model for a downstream task in both natural language processing (NLP) (Touvron et al., 2023; Brown et al., 2020) and computer vision (CV) (Caron et al., 2021; Steiner et al., 2021; He et al., 2022) applications. Following this pretraining-and-finetuning paradigm, both traditional supervised learning methods and the emerg-"}, {"title": "2. Related Work", "content": "Pre-training and Fine-tuning Pre-training a large model on a large-scale dataset equips the model with a foundational ability to extract all levels of features from the input. Existing pre-training methods can be roughly classified into supervised learning (Khosla et al., 2020; Krizhevsky et al., 2012; Steiner et al., 2021) and self-supervised learning (SSL) methods. SSL allows the model to learn directly from web-scale data without label annotations, thus becoming a popular and practical choice for large-scale pre-training. SSL methods can be further broadly categorized into contrastive methods (Garrido et al., 2022; Caron et al., 2021; Chen et al., 2020; He et al., 2020; Grill et al., 2020) and generative methods (He et al., 2022; Bao et al., 2021).\nFine-tuning adapts a pre-trained model to a specific downstream task by training on the downstream dataset. As the pre-trained models become larger and larger, traditional full fine-tune tends to suffer from efficiency and storage limitations. This motivates the proposal of parameter-efficient transfer learning (PETL) methods (Chen et al., 2022; Hu et al., 2021a; Jia et al., 2022b). These methods involve freezing the weights of the pre-trained model and introducing auxiliary fine-tunable modules, thus having the ability to achieve comparable performance with full fine-tuning while saving a lot of parameter updates and storage. For instance, LoRA leverages low-rank matrices to represent the updates of attention block parameters (Hu et al., 2021a), while AdaptFormer attaches parallel adapters to the fully connected layers (Chen et al., 2022).\nTransferable Adversarial Attacks Transferable adversarial attacks, or transfer attacks for short, are a form of black-box attacks that leverage the cross-model transferability of adversarial examples (Liu et al., 2016). Most of the existing works were focused on the cross-model transferability, where the adversary generates the attack using a surrogate model trained on the same training dataset as the target model (Dong et al., 2018; 2019; Xie et al., 2019; Wu et al., 2020; Zhang et al., 2023b; Wei et al., 2022; Ma"}, {"title": "3. Downstream Transfer Attack", "content": "Notations Let $f_{\\theta}$ be the pre-trained encoder and $f_{\\theta}^k(x)$ is its k-th layer feature map output for a given input image $x$. Let $f_{\\theta_f}$ be the model fine-tuned from $f_{\\theta}$ on a downstream dataset $D_d$. Let $x \\sim D_d$ be an image in dataset $D_d$, and $f_{\\theta_f}(x)$ be the probability output of classifier $f_{\\theta_f}$. Let $F(x) = \\arg \\max f_{\\theta_f}(x)$ be the final classification result.\n3.1. Threat Model\nIn our downstream transfer attack setting, the adversary aims to attack a target model $f_{\\theta_f}$ which was fine-tuned from a pre-trained model $f_{\\theta}$. The fine-tuning was done by either full fine-tuning or a PETL method on a downstream dataset that is different from the pre-training dataset. The adversary has no knowledge or access to the fine-tuning process and can only query the target model to mount the attack. However, the adversary has full access to the pre-trained model which is often assumed to be a large open source model. Given a test image $x$ of the downstream dataset, the adversary exploits the pre-trained model to generate the adversarial perturbation $\\delta$ with attack budget $|\\delta||_p \\leq \\epsilon$. The resulting adversarial example $x' = x + \\delta$ is then fed into the downstream model $f_{\\theta_f}$ to execute the attack.\nThe attacker's goal is to maximize the loss of the downstream model $f_{\\theta_f}$:\n$\\max L(f_{\\theta_f}(x + \\delta)), s.t.||\\delta||_p \\leq \\epsilon$,\nwhere $L$ denotes the loss of the downstream task. In the case of image classification task, the above adversarial objective can be defined as the misclassification error:\n$\\max \\mathbb{1}(F_{\\theta_f}(x + \\delta) \\neq y), s.t.||\\delta||_p \\leq \\epsilon$,\nwhere $\\mathbb{1}(\\cdot)$ denotes the indicator function and $y$ denotes the ground truth label.\nRelation to Existing Threat Models Our downstream transfer threat model is an extension of the threat model proposed in PAP (Ban & Dong, 2022) and AdvEncoder (Zhou et al., 2023b), which assume that the pre-trained model is known to the adversary. In our threat model, the adversary can access both the pre-trained model and the test samples that the attacker wishes to attack, but does not know the fine-tuned parameters or architecture of the downstream model. This allows us to generate sample-wise transfer attacks, while PAP and AdvEncoder can only generate UAPs (as the adversary has no knowledge of the downstream dataset).\nThe advantages of sample-wise transfer attacks over UAPs are twofold: 1) they are stronger than UAPs, revealing more severe threats; and 2) they can help train more robustness models when used for adversarial training. Our attack setting can also be viewed as a special case"}, {"title": "3.2. Methodology", "content": "Arguably, the most crucial part of designing a transferable attack is to find an appropriate indicator of transferability. This has been found to be challenging in the traditional cross-model transfer setting (Liu et al., 2016; Dong et al., 2018; 2019). This is because strong attacks generated on a source model tend to overfit the source model and thus transfer poorly to other (target) models, even if the target models are trained on the same dataset (Lu et al., 2020; Zhang et al., 2022a; 2023b). We find that this is also the case for downstream transfer attacks where the fine-tuned models often have substantial parameter changes caused by either full finetuning, self-attention adaptation (e.g., LoRA), or additional adapter layers (e.g., AdaptFormer). In this work, we propose a novel metric called Average Token Cosine Similarity (ATCS) as the indicator of downstream transfer-ability and also our adversarial objective. Based on ATCS, we further introduce a layer selection strategy to find the most vulnerable and transferable layer of a pre-trained ViT to downstream models.\nAverage Token Cosine Similarity Since pre-trained models do not necessarily have a classification head, the adversarial objective should be defined on the intermediate layer output (i.e., features) of the pre-trained model. A typical ViT model consists of a series of identical transformer layers (blocks). The output of a transformer layer is a sequence of tokens that have the same dimensions. As such, we propose to first compute the cosine similarity between the clean vs. adversarial feature tokens and then average this cosine similarity across the tokens to obtain the final ATCS. Formally, the ATCS loss is defined as:\n$\\mathcal{L}_{ATCS}^k(x, x') = \\frac{1}{|T|} \\sum_{t \\in T} cos(f_\\theta^k(x)_t, f_\\theta^k (x')_t)$,\nwhere $f_\\theta^k(x)$ is the feature map output of the $k$-th layer, $f_\\theta^k(x)_t$ is the $t$-th token of $f_\\theta^k(x)$, and $|T|$ is the total number of tokens.\nThe reason why we use ATCS over the traditional cosine similarity computed on flattened tokens is that the high dimensionality of the transformer layer output could cause"}, {"title": "4. Experiments", "content": "4.1. Experimental Setup\nModels and Datasets We consider models pre-trained by three representative pre-training methods: AugReg (Steiner et al., 2021) (a supervised pre-training method), DINO (Caron et al., 2021) (a contrastive learning method), and MAE (He et al., 2022) (a masked image modeling method). We utilize the public model weights pre-trained by the three methods on GitHub or Hugging Face. We consider three representative fine-tuning approaches: full fine-tune, LoRA (Hu et al., 2021a), and AdaptFormer (Chen et al., 2022). Following (Ban & Dong, 2022), we use 10 downstream datasets to evaluate the performance of DTA. These datasets contain 3 coarse-grained and 7 fine-grained datasets. The images are resized to 256 \u00d7 256 and then center-cropped to 224 x 224 before feeding into the network. The clean performances of the models are provided in the appendix.\nBaseline Methods We take PAP (Ban & Dong, 2022) and NRDM (Naseer et al., 2018) as our baselines. PAP, which generates image-agnostic perturbation, is the first method for attacking fine-tuned models. NRDM is a sample-wise attack initially introduced for black-box attacks on diverse tasks. Since NRDM operates on features, it can be directly applied to the downstream attack setting. We empirically set the attacking layer of NRDM to $k$ = 8 for ViT small and base, and $k$ = 16 for ViT-large, as they yield the best performance.\nAttack Setting Following previous studies (Zhou et al., 2023b; Moosavi-Dezfooli et al., 2017a; Zhang et al., 2022b),"}, {"title": "4.2. Main Results", "content": "For downstream transfer attacks, it is important to achieve good attack performance across different fine-tuning methods, pre-training methods, and downstream datasets. A quick glance at the results in Table 1, 2, and 3 reveals that our DTA surpasses the baselines by a huge margin across all pre-training methods and fine-tuning datasets. For full fine-tuning (Table 1), DTA achieves an average ASR of 93.11%, surpassing that of NRDM and PAP by more than 10% and 40%, respectively. A similar result is also observed for LoRA (Table 2) and AdaptFormer (Table 3), where DTA achieves an ASR above 95%. Comparing the results between full fine-tune and LoRA/AdaptFormer, we find that PETL fine-tuned models (by LoRA/AdaptFormer) are more vulnerable to downstream transfer attacks than full fine-tuned models. This is because the parameters of the pre-trained models are all fixed in PETL, leaving more feature vulnerability to the downstream models. This also confirms that PETL makes less feature change to the pre-trained model than full fine-tuning.\nAnother interesting observation is that the baselines exhibit a much higher variance when applied to different pre-trained models, and even fail in certain cases. For example, PAP works pretty well on MAE pre-trained models, yet fails badly on AugReg and DINO pre-trained models. It is quite the opposite for NRDM, which works worse on MAE pre-trained models. We conjecture this is because MAE focuses more on reconstruction, making shallow layer features fundamental for the gradual reconstruction at the deep layers. For pre-training methods that focus more on the clustering effect of the deep features, e.g., AugReg and DINO, they are more vulnerable at the middle or deep layers (see Figure 6 in the appendix)."}, {"title": "4.3. Evaluation on Large-Scale ViTs", "content": "Here, we evaluate the three attacks on pre-trained models with varying scales. All models were pre-trained on ImageNet-1k using AugReg. As the results in Table 4 show, our DTA beats the baseline methods by a considerable margin. Particularly, the average ASR of DTA is 85.52%, which is 10.92% and 71.46% higher than NRDM and PAP, respectively. On ViT-Large, DTA achieves an average ASR of 75.25% across the 4 datasets, which beats NRDM and PAP by 17.02% and 70.5%, respectively. Moreover, the sensitivity and instability of the baseline methods across different model scales are extremely high, which greatly limits their practicability when applied to diverse pre-trained models."}, {"title": "4.4. Evaluation on Adversarially Pre-trained ViTs", "content": "We also evaluate the attacks on adversarially pre-trained models, a common way to improve adversarial robustness. We take the XCiT model adversarially pre-trained (with $l_0$ budget $\\epsilon$ = 8/255) on ImageNet-1k by Edoardo et al. (Debenedetti et al., 2023) as our pre-trained model. Due to the poor performance of AdaptFormer, here we mainly analyze full fine-tune and LoRA. As shown in Figure 4, unsurprisingly, the ASR decreases drastically for all attacks, which is well below 30% and 45% when transferred to attack the full or LORA fine-tuned models, respectively. However, DTA is still comparably more effective than others. An-"}, {"title": "4.5. Attacking Object Detection and Segmentation", "content": "For object detection, we choose ViTDet (Li et al., 2022) as the downstream model, which was fine-tuned on the COCO2017 dataset from an MAE pre-trained ViT-base model. Object detection tasks often pad different-sized images to the same size, it is thus meaningless to generate adversarial examples for the padding par. So we crop a 448x448 region from the upper-left corner of the image to perform the attack. As indicated in Table 5, our DTA attack caused the model's mAPs to drop the most, from 51.5 to 35.6 on the COCO2017 validation set.\nFor the segmentation task, we take UPerNet (Xiao et al., 2018) as the downstream model, which was fine-tuned on the ADE20k dataset from an MAE pre-trained ViT-base model. We employ a 512x512 attack region here. As presented in Table 6, all attack methods result in a substantial decrease in mIoU, with our DTA being the most effective, reducing the mIoU to 15.54. This proves that our DTA can be generalized to attack different types of downstream tasks."}, {"title": "4.6. Improving Adversarial Training", "content": "Here, we show that DTA can help build better defenses against downstream transfer attacks. We follow the fine-tuning paradigm to finetune the downstream models on CIFAR10 using adversarial training (Madry et al., 2017). The $\\epsilon$ for adversarial finetuning is set to 4/255 to help maintain clean accuracy. But we use a larger $\\epsilon$ = 10/255 for testing. As shown in Table 7, models fine-tuned with PAP can only defend PAP attacks and fail badly on DTA attacks, for AugReg and DINO pre-trained models. By contrast, using our DTA can help achieve universal robustness against different"}, {"title": "4.7. Ablation Studies", "content": "We conduct 3 ablation studies on loss function, attack layers, and threshold \u03b3. Here, we only report the main conclusions and defer the detailed results and analyses to appendix B.\nLoss Function We test 4 alternative loss functions for ATCS including the vanilla cosine similarity. Overall, ATCS is better than the vanilla cosine similarity in most cases and is more effective than other losses.\nAttack Layer(s) We compare our DTA layer selection strategy with two alternative strategies and find that our strategy is better than attacking a fixed layer or all layers.\nThreshold \u03b3 We show that, as the threshold \u03b3 increases from 0 to 1, the ASR first increases and then decreases, reaching its peak performance at \u03b3 = 0.2. And the trend is consistent across different downstream datasets."}, {"title": "5. Conclusion", "content": "In this paper, we studied the problem of downstream transfer attacks (DTAs) and explored how an attacker can generate highly transferable adversarial examples using a pre-trained model to attack downstream models fine-tuned by different techniques. We proposed to use Average Token Cosine Similarity (ATCS) as the adversarial objective and revealed that the ATCS value obtained at different layers is a good indicator of downstream transferability. With ATCS, we further proposed a DTA attack that can find the most vulnerable layer and generate highly transferable adversarial examples. Extensive experiments demonstrate the effectiveness of our DTA attack and its superiority over existing attacks. We also found that emerging PETL methods like LORA are more susceptible to transfer attacks crafted on the pre-trained model. We also show that our DTA can help train more robust models resistant to downstream transfer attacks when applied with adversarial training."}, {"title": "A. Clean Accuracy of Downstream Models", "content": "Table 8 reports the clean accuracies of the victim downstream models. As can be observed, the finetuned models all perform well on the clean datasets, regardless of the finetuning method."}, {"title": "B. Ablation Study", "content": "Here, we analyze the impact of the adversarial loss function and the two hyper-parameters including layer k and threshold y to DTA. We fix the fine-tuning method to full fine-tune and set the attack step size to 0.03 and step number to 10."}, {"title": "Loss Function", "content": "Here, we test 4 alternative loss functions for our ATCS:\n$\\mathcal{L}_1 = -cos(f_\\theta^k(x), f_\\theta^k (x')) \\mathcal{L}_2 = || f_\\theta^k(x) \u2013 f_\\theta^k (x') ||$\n$\\mathcal{L}_3 = \u2212 f_\\theta^k (x) \\cdot f_\\theta^k (x') \\mathcal{L}_4 = || f_\\theta^k (x')||$\nAs illustrated in Figure 5, although $\\mathcal{L}_4$ has been proven to be effective in the UAP setting (Ban & Dong, 2022; Mopuri et al., 2017), it does not perform well (or at least not as well as other losses) in the downstream transfer setting, which is a sample-wise attack scenario. Overall, our ATCS achieves a slightly better performance than the vanilla cosine similarity in most cases and is more effective than other loss functions."}, {"title": "Attack Layer(s)", "content": "Here, we compare our DTA layer selection strategy with several alternative layer selection strategies, such as selecting a particular layer or all layers together. We report the average ASR of attacking 10 downstream models with different pre-train methods. As shown in Figure 6, our DTA achieves a higher ASR than attacking a particular layer or all layers. For MAE pre-trained models, attacking the shallow layer is better, while for DINO and AugReg pre-trained models, attacking the middle layer is better. Attacking all layers works well for DINO and AugReg pre-trained models with comparable performance to DTA, but is inferior to DTA for MAE pre-trained model. Overall, our DTA works the best among the three layer selection strategies."}, {"title": "C. More Understandings of DTA", "content": "Undoubtedly, representation/feature reuse is one key aspect of the pretraining-and-finetuning paradigm. The downstream model inherits the internal loss landscape and features of the pre-trained model (Neyshabur et al., 2020). This implies that the perturbations generated to distort the pre-trained features are to some extent also disruptive to the downstream model. To better understand this, we feed the clean and adversarial examples separately into the pre-trained vs. fine-tuned models and compare the obtained ATCS, i.e., $ATCS(f_\\theta^k(x'), f_\\theta^k (x'))$ and $ATCS(f_{\\theta_f}^k(x), f_{\\theta_f}^k (x))$. Here, we set the attack layer to k = 8. As shown in Figure 8, the ATCS of the adversarial examples is higher than that of the clean samples, which implies that the vulnerabilities of the pre-trained model explored by the adversarial samples are preserved in the fine-tuned model. Notably, when fine-tuned using PETL methods, the downstream model becomes more similar to the original model in the feature space. This similarity also predicts that PETL models are more susceptible to downstream transfer attacks.\nThere is also an interesting shift of focus on the attacking layers targeted in different works, i.e., earlier works often attack the middle layers (Lu et al., 2020; Naseer et al., 2020; Zhang et al., 2022b), while more recent works favor the shallow layers (Ban & Dong, 2022; Zhang et al., 2022b). Based on our analyses and empirical observations, this might be related to the trend that earlier models pre-trained using supervised learning are more vulnerable in the middle and deep layers, while more recent models pre-trained by self-supervised learning are more vulnerable in the shallow layers. Our DTA provides a simple but effective technique to explore the most vulnerable layers of the pre-trained model and thus can be effective for different pre-training methods."}, {"title": "Threshold", "content": "Here we study the effect of threshold \u03b3. Figure 7 shows that, for various downstream datasets, as the threshold value increases from 0 to 1, the ASR first increases and then decreases, reaching its peak performance at y = 0.2. And the trends are quite consistent on different downstream datasets. Arguably, the optimal y may vary in real-world scenarios, which can be carefully tuned if the attacker knows more information about the downstream task."}]}