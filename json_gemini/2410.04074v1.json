{"title": "On Eliciting Syntax from Language Models via Hashing", "authors": ["Yiran Wang", "Masao Utiyama"], "abstract": "Unsupervised parsing, also known as grammar induction, aims to infer syntactic structure from raw text. Recently, binary representation has exhibited remarkable information-preserving capabilities at both lexicon and syntax levels. In this paper, we explore the possibility of leveraging this capability to deduce parsing trees from raw text, relying solely on the implicitly induced grammars within models. To achieve this, we upgrade the bit-level CKY from zero-order to first-order to encode the lexicon and syntax in a unified binary representation space, switch training from supervised to unsupervised under the contrastive hashing framework, and introduce a novel loss function to impose stronger yet balanced alignment signals. Our model shows competitive performance on various datasets, therefore, we claim that our method is effective and efficient enough to acquire high-quality parsing trees from pre-trained language models at a low cost.", "sections": [{"title": "Introduction", "content": "Grammars form the backbone of languages, providing the essential framework that dictates how lexicons are arranged to convey meaning. Understanding and generating language heavily relies on grasping these latent structures. Unsupervised parsing, which aims to deduce sentence structure without relying on costly manually annotated treebanks, has been widely studied in academia. However, despite its importance, advancements have been slow due to the intrinsic complexity of this task. Nowadays, addressing these challenges becomes even more crucial for further exploring the capabilities of large language models.\nWord embedding and language model techniques (Mikolov et al., 2013a,b; Radford, 2018; Devlin et al., 2019) have shown that training models to predict tokens in specific contexts is remarkably effective in implicitly capturing lexical features. A well-known example is the captured lexical relationship of king - man + woman = queen. As one of the most widely accepted explanations for this phenomenon, the distributional hypothesis (Harris, 1954; Mikolov et al., 2013a,b) suggests this is because tokens appearing in similar contexts tend to be assigned similar meanings. Specifically, similar contexts achieve this by placing tokens in analogous syntactic structures. This phenomenon naturally prompts us to consider whether there is a representation learning method that can explicitly encode both lexical and syntactic information in a unified format, making it possible to capture syntactic structures as well as lexical relationships by training language models solely with conventional conditional token prediction procedures.\nFortunately, the recently proposed binary representation meets these requirements perfectly. Wang et al. (2023) proposed a binary representation that bridges the gap between the continuous nature of deep learning and the discrete intrinsic property of natural languages. Instead of directly applying contrastive learning on the high-dimensional continuous hidden states of pre-trained language models, Wang et al. (2023) project them as K-dimensional score vectors. These scores can easily be binarized into K-bit codes, and token-level contrastive learning is applied among these scores and their binarized codes. They demonstrate that lexical information can be properly preserved within only 24 bits. Following this, Wang and Utiyama (2024) additionally take spans on the target parsing trees into consideration. They use marginal probabilities to construct a novel similarity function that reflects not only lexical information but also the boundary of each span, and then perform contrastive hashing across spans rather than tokens. In their supervised parsing experiments, they show the effectiveness of the structured binary representation by achieving comparable performance to conventional parsers.\nAlthough in the supervised settings, Wang and Utiyama (2024) achieves satisfactory results, we found that for unsupervised settings, their model is insufficient to induce meaningful parsing trees. In this paper, we aim to elicit constituency parsers from pre-trained language models without training them on annotated treebanks. We analyze the existing issues of their structured binary representation and explore the possibility of further enhancing the unified information-preserving capability. To achieve this, we upgrade the bit-level CKY module from zero-order (\u00a72.1) to first-order (\u00a73.1) to integrate lexicon and syntax in a unified format, convert parsing from supervised (\u00a72.2) to unsupervised (\u00a73.2), and propose a novel objective function (\u00a73.3) to impose stronger yet balanced alignment signals. Besides, we also discuss how the learning objective of contrastive hashing aligns with the target of parsing. This provides an explanation (\u00a73.2) different from the distributional hypothesis and explains why our training leads to syntactic structures rather than other structures. Experiments show that our models achieve competitive performance and indicate that acquiring high-quality syntactic annotations at a low cost is becoming practicable. We refer to our parser as Parserker, following the original Parserker (Wang and Utiyama, 2024)."}, {"title": "Background", "content": "", "subsections": [{"title": "Zero-order Constituency Parsing", "content": "Given sequence W1,..., Wn, constituency parser returns the most probable binary-branching parsing tree t = {(li, ri, Yi)}\nwhich is represented as a list of labeled spans indicating constituents at different hierarchies. Where li and ri refer to the left and right boundaries of the i-th constituent, and Yi \u2208 Y stands for its assigned label. Previous models commonly employ encoders to transform inputs into hidden states h\u2081, . . ., hn first, use classifiers to predict span scores g(l, r, y) and tree scores g(t), and then normalize them among all valid trees to obtain tree probability p(t). Training and inference stages aim at maximizing the probabilities of target trees and searching trees with the maximal probabilities t, respectively.\n$g(t) = \\sum_{(l,r,y) \\in t} g(l,r,y)$\n$p(t) = \\frac{\\exp g(t)}{Z}$\n$Z = \\sum_{t'\\in T} \\exp g(t')$     (2)\n$\\hat{t} = \\{(l_i, r_i, Y_i)\\}_{i=1}^{2n-1} \\leftarrow \\underset{t \\in T}{\\arg \\max} p(t)$     (3)\nApart from being used to normalize probabilities of trees, the log partition term Z, which stands for the total scores of all valid constituency trees, can also be used to compute span marginal probabilities. As Eisner (2016) mentioned, computing the partial derivative of the log partition with respect to span scores yields marginal probabilities efficiently.\n$\\mu(l, r, y) = \\frac{\\partial \\log Z}{\\partial g(l, r, y)}$     (4)\nIntuitively speaking, marginal probability reflects the joint probability of selecting tokens \u03c9\u03b9, ..., \u03c9r as a constituent with label y assigned to it. If a span is not likely to be selected, its marginal probability will not be high regardless of its label. Therefore, similar to hidden states, marginal probabilities are considered a format containing not only lexical but also syntactic features. Unlike hidden states, these marginal probabilities explicitly correspond to the specific boundaries and labels of spans in parsing trees globally normalized under the CKY framework, whereas hidden states implicitly preserve this information in a high-dimensional, human-unreadable format."}, {"title": "Supervised Contrastive Hashing", "content": "Recently, Wang and Utiyama (2024) extended constituency parsers by replacing discrete labels y \u2208 Y with binary codes c \u2208 {\u22121,+1}K. In their approach, the code-level scores g(l,r,c) are obtained by summing up bit-level scores gk(l, r, ck).\n$g(t) = \\sum_{(l,r,c) \\in t} g(l,r,c)$    (5)\n$g(l, r, c) = \\sum_{k=1}^{K} g_k (l, r, c_k)$    (6)\nMoreover, to compute these bit-level scores, they retained the one-head-one-bit design of Wang et al. (2023) and employed a multi-head attention module to predict the score of being assigned +1.\n$g_k(l, r,+1) = \\frac{(W_q h_l:r)^T (W_k h_l:r)}{\\sqrt{d_k}}$    (7)\ngk(l, r, -1) = 0 (8)\nWhere Wq, WK\u2208Rd\u00d7d are the query and key matrices used to produce the k-th bit. They assign a score of 0 for the -1 case and extend the marginal probability and decoding to the bit level.\n$\\mu_k(l,r,c_k) = \\frac{\\partial \\log Z}{\\partial g_k(l, r, c_k)}$\n$\\hat{t} = \\{(l_i, r_i, c_i)\\}_{i=1}^{2n-1} \\leftarrow \\underset{t \\in T}{\\arg \\max} p(t)$   (10)"}]}, {"title": "Supervised Contrastive Hashing", "content": "To perform contrastive learning, Wang et al. (2023) and Wang and Utiyama (2024) define their similarity functions in a similar manner, both first binarize one input and then calculate the similarity between the continuous one and the binarized one. However, the former binarizes scores via taking their signs, while the latter leans bits towards the sides with larger marginal probabilities.\n$c = [c^1, ..., c^K] \\in \\{-1,+1\\}^K$     (11)\n$c^k =\\begin{cases} +1 & \\mu_k(l, r,+1) > \\mu_k(l, r, -1) \\\\ -1 & \\text{otherwise} \\end{cases}$\nAs mentioned above, marginal probabilities contain both label and structural information. To impose supervision on lexicon and syntax simultaneously by leveraging this property, they proposed defining the novel similarity as the average of bit-level marginal probabilities of the i-th constituent with the binary label of j-th constituent.\n$s(i, j) = \\frac{1}{K} \\sum_{k=1}^{K} \\mu_k (l_i, r_i, c_j^k)$     (12)"}, {"title": "Proposed Methods", "content": "", "subsections": [{"title": "First-order Constituency Parsing", "content": "Efficient computing requires batchifying the inside pass of the CKY algorithm for parallel dynamic programming on GPUs (Stern et al., 2017; Zhang et al., 2020). Within the CKY framework, Wang and Utiyama (2024) introduce a large tensor as the chart for dynamic programming, where G(l, r, c) refers to the total scores of all trees spanning from I to r with code c as the top label, while g(l, r, c) stands for a single constituent. The algorithm starts from single-word spans and incrementally computes larger spans by enumerating splitting positions and summing children with the top span.\nG(l, r, c) \u2190 \u2211 G(l,m,\u00b7) +\n\u2211 G(m + 1,r,\u00b7) + g(l,r,c) (15)\nm=l\nm=l\nThis procedure has been widely employed as a practical standard (Zhang et al., 2020; Wang et al., 2023). However, we notice that natively using it for unsupervised parsing is not sufficient. As shown in Figure 2, the crux is that even though Equation 15 enumerates all valid splitting positions, the span score g(l, r, c) does not take the splitting positions into consideration. According to Equation 7, this score depends only on the leftmost and rightmost tokens, regardless of the chosen splitting positions. In other words, different choices of splitting positions do not vary the code scores of top spans. Therefore, performing contrastive hashing by using such scores barely provides any effective information for unsupervised parsing. We refer to this kind of CKY as zero-order CKY.\nNaturally, the most intuitive solution is upgrading to first-order CKY by taking the splitting position m into consideration through introducing a novel span score function g(l, r, m, c).\nr-1\nG(l, r, c) = \u2211 G(l, m, \u00b7) +\nm=l\nG(m+1,r,\u00b7) + g(l,r,m,c)\nAnd instead of relying only on the leftmost and the rightmost hidden states, we use the averaged representation of the left and right children, respectively.\nK\ng(l, r, m, c) = \u2211gk(l, r, m, ck) (17)\nk=1\n$g_k(l,r,m,+1) = \\frac{(W_q h_{l:m})^T (W_k h_{m+1:r})}{\\sqrt{d_k}}$\nhl:m = mean hi\nl<i<m\nhm+1:r = mean hj\nm<j<r\nWhere hl:m and hm+1:r are the averaged representation of the left and right children, respectively. In this way, the splitting position influences the scores of binary codes through children hidden states.\nHowever, naively computing gk(l, r, m, +1) requires additional computational resources for averaging vectors and performing dot products in real-time, which heavily slows down training and inference. Fortunately, through simple derivation, we note that the new score can be obtained by merely averaging the old scores. Upgrading CKY from zero-order to first-order then introduces almost no additional delay by applying this trick.\ngk(l,r,m,+1) = mean gk(i, j, +1) (18)\nl<i<m<j<r\nAccording to this definition, the new scores can be interpreted as being obtained by averaging the left and right children, respectively, and then calculating the scores for construing a span across them. Different choices of splitting positions result in different representations of the left and right children, leading to different bit scores for the top span. Since scores reflect the substructure of spans, aligning and uniformalizing these scores in Hamming space using contrastive learning is equivalent to aligning and uniformalizing the subtrees in syntactic structure space. Hence, our method can also be considered relevant to syntactic distance (Shen et al., 2018a, 2019). Additionally, we also assign a score of 0 for the -1 case.\ngk(l, r, m, -1) = 0  (19)\nAnd extend the marginal probabilities as well.\n$\\mu_k(l, r, m, c_k) = \\frac{\\partial \\log Z}{\\partial g_k(l, r, m, c_k)}$ (20)"}, {"title": "Unsupervised Contrastive Hashing", "content": "We define our similarity in a manner similar to Wang and Utiyama (2024). As we have upgraded the bit-level CKY module from zero-order to first-order, we also upgrade the binarization procedure.\nc = [c\u00b9, ..., cK] \u2208 {\u22121,+1}K\nck = { +1 \u03bc\u03ba(l,r,m, +1) > \u03bc\u03ba(l, r, m, \u22121)\n-1 otherwise\nand the similarity function as follows.\ns(i, j) = 1 \u2211\u03bc\u03ba (li, ri, mi, cj)\nUnlike in the supervised settings of Wang and Utiyama (2024), we aim to obtain constituency parsers without training them on annotated treebanks, i.e., {(li, ri, Yi)}2n\u012b\u00b9. Therefore, it is difficult for us to constrain the search space as Equation 13 and to divide spans according to ground-truth labels as Equation 14. Thus, we unlock these restrictions and let parsers determine constituent boundaries and binary labels jointly through searching in an unconstrained space T[\u00b7, \u00b7, \u00b7].\n$\\hat{t} = \\{(l_i, r_i, \\hat{c_i})\\}_{i=1}^{2n-1} \\leftarrow \\underset{t \\in T[\\cdot, \\cdot, \\cdot]}{\\arg \\max} p(t)$   (23)\nAfter that, since we neither have access to the ground-truth labels yi, we turn to use the lexicons in spans w\u00ee, ..., w\u2081\u2081 as the labels to divide these selected spans. In this way, pulling or pushing spans is determined solely on surface textual features. Besides, since a portion of input tokens are masked out during the augmentation stage, our parsers can be considered a masked language model as well, except that they are trained with a contrastive objective at the span level.\nP = {j | Wli:ri  = Wlzj:rzj}\nN = {j | Wli:ri  \\neq Wlzj:rzj} (24)\nFrom the perspective of training, as Wang et al. (2023) mentioned, one of the most appealing properties of contrastive learning is that it can convert tasks from wh-questions to yes-no questions. Conventional classification approaches demand embedding vectors for all spans, but enumerating them all is clearly intractable. According to Appendix A, we note that even disregarding the sparsity, employing an embedding with millions of entries is barely practical due to its huge memory consumption. In contrast, our contrastive hashing only needs to know if spans are identical or not, allowing it to pull or push their representations directly without needing to introduce specific embeddings. This property makes previously intractable training feasible and efficient.\nFrom the perspective of representation learning, contrastive learning aims to maximize the distinguishability between instances. In our model, this corresponds to maximizing the distinguishability between subtrees. For parsing, choosing the splitting positions that minimize the internal differences within subtrees is equivalent to maximizing the differences across subtrees. In other words, parsing can be considered as a procedure of searching the minimum entropy tree formed by repeatedly merging the most similar contiguous subtrees, thus, it aligns with the learning objective of contrastive hashing. We believe this explains why such a contrastive hashing procedure results in syntactic trees rather than other structures, and this provides justification for our use of contrastive learning."}, {"title": "Instance Selection", "content": "Contrastive learning (Gao et al., 2021) learns informative representation through pulling together positive and pushing apart negative instances. Wang and Utiyama (2024) enumerate each instance i and compare it with all instances in the batch jet to compute the instance-level loss l(i, \u00b5, t), and then aggregate all these losses as the batch-level loss L. By using log \u2211exp as a approximation of max,\n$\\underset{X \\in X}{\\max} (x) \\approx \\log \\sum \\exp (x)$  (25)\nThey tweaked those commonly used contrastive objectives into unified formats as follows, where S = {i} is simply defined as the instance itself.\nlself \u2248 max s(i,j) \u2013 s(i,i) (26)\nlsup\u2248 max s(i, j) \u2013  mean s(i, j) (27)\nNUP\nPlhash\u2248 maxs(i,j) \u2014 s(i,i) (28)\nNlmax \u2248 max s(i, j) \u2013  max s(i, j) (29)\nNUS\nP\n(30)\n= max(max s(i, j), s(i,i)) \u2013 max s(i, j), s(i,i))\nObjective function lself is commonly utilized in scenarios involving only a single positive instance. Khosla et al. (2020) then extended it as lsup to handle multiple positive instances scenarios, and it was later surpassed by lhash and lmax. For more details, we refer readers to their original papers (Wang et al., 2023; Wang and Utiyama, 2024).\nBriefly speaking, objective lhash assumes there is only one true positive and excludes potential false negatives and positives from both terms, with P replaced with S. Moreover, lmax adopts a different approach to handling multiple positive instances. They still assume there is only one true positive instance among P, but they dynamically select the closest one as the true positive, instead of statically selecting S. By imposing such a weak alignment signal, they also avoid the geometric center issue of lsup. However, we found that for tasks with large label vocabularies, such as language models, this signal turns out to be too weak. Therefore, instead of pulling only the closest pairs, we propose to mainly focus on the farthest pairs,\nlmin \u2248 maxs(i, j) \u2013 mins(i, j)\n( = max max s(i, j), s(i,i)) \u2013 mins(i, j)\nNPlmin ~maxs(i, j) \u2013 min s(i, j)\n( = max max s(i, j), m\u0131n s(i, j))  \u2013 min s(i, j)"}, {"title": "Architecture", "content": "Following Wang and Utiyama (2024), our model also consists of a pre-trained language model, an attention hash layer, and a bit-level CKY module. The only difference is that we upgrad CKY from zero-order to first-order, which enhances its ability to unify the representation of lexicon and syntax.\nAlthough it is also a masked language model, our model does not require introducing a large embedding matrix for calculating token classification in the output layer. Since it relies on the attention hash layer to produce binary codes of spans, the number of parameters in the output layer is reduced from |Y| \u00d7 d to two K \u00d7"}, {"title": "Training and Inference", "content": "During the training stage, sentences are fed into the model twice to obtain two different views by being augmented with different dropout masks. We calculate the marginal probabilities \u03bc\u00b9 and \u03bc\u00b2, and then predict constituency trees t\u00b9 and t2 on these two versions, respectively. For each view, we select the corresponding span scores from the marginal probabilities of one view, according to the predicted tree of the other view, and then perform span-level contrastive hashing by using the objectives above and average them as the batch loss.\nL = mean l(i, \u00b5\u00b9, \u00ce\u00b2) + mean l(i, \u03bc\u00b2, \u00ce\u00b9) (34)\niet2\niet1\nSince unsupervised constituency parsing only aims at detecting the span boundaries without needing to predict labels, we do not need to build the code vocabulary as Wang and Utiyama (2024) did. During the inference stage, we simply search for the most probable constituency parsing trees in an unconstrained space with the Cocke-Kasami-Younger (CKY) algorithm (Kasami, 1966)."}]}, {"title": "Experiments", "content": "", "subsections": [{"title": "Settings", "content": "Experiments are conducted on the commonly used datasets Penn Treebank (PTB) (Marcus et al., 1993) and Chinese Treebank 5.1 (CTB) (Xue et al., 2005). Following previous settings (Shen et al., 2018b, 2019; Zhao and Titov, 2021), we use the same preprocessing pipeline to discard punctuation in all splits. Although this pipeline may not be the best choice for pre-trained language models and might result in some information loss, since language models are commonly trained with punctuated corpora, we follow this setting only to provide comparable results to previous work. Regarding the evaluation metric, we follow Kim et al. (2019a) to remove trivial spans, i.e., single-word and entire-sentence spans, calculate unlabeled sentence-level F1 scores, and take the average across all sentences.\nWe use the deep learning framework PyTorch (Paszke et al., 2019) to implement our models and download checkpoints of pre-trained languages from huggingface/transformers (Wolf et al., 2020). Different from some recent work (Yang et al., 2022; Liu et al., 2023), which require customizing CUDA kernels with Triton (Tillet et al., 2019), our model can be easily and efficiently implemented with pure PyTorch."}, {"title": "Main Results", "content": "On the English dataset PTB, as shown in Table 1, our model reaches its peak performance at 24 bits and 16 bits when using BERT and RoBERTa pre-trained language models, respectively. We consistently surpass all other implicit grammar models. Due to the relatively small size of PTB, the probing methods by Cao et al. (2020) and Maveli and Cohen (2022) utilized additional text data for training. Even without using such extra data, our model still achieves performance very close to theirs.\nOur model outperforms all existing models by a large margin on the Chinese dataset CTB, as shown in Table 2. Explicit grammar models that perform well on English datasets (Yang et al., 2022; Liu et al., 2023) do not achieve similar success on the Chinese dataset. Additionally, we notice that our model requires much more bits than on the English dataset, i.e., 36 and 44, to reach their full potential. We hypothesize that this is due to the relatively small size of the Chinese dataset, as shown in Appendix A, which prevents the models from being fully trained to encode lexicon and syntax features within only a few bits."}, {"title": "Ablation Studies", "content": "Table 3 reveals how the different combinations of negative and positive terms affect performance. First of all, we notice that once minp is employed, regardless of which negative terms are used along with it, the models consistently result in high scores in the MAX column. On the contrary, without employing minp, these scores dramatically drop. This confirms our statement that for tasks with large label vocabularies, positive terms require strong alignment signals to learn effective representations. Moreover, when it comes to the MEAN column, whether the term maxN\u222a{mimp} is employed determines whether the high scores of minp can be maintained. We also notice that maxwus consistently outperforms maxNup. This indicates that simply pushing away all instances of P indeed introduces the false negatives issue. As Wang et al. (2023); Wang and Utiyama (2024) claims, retaining only S mitigates this issue, but when P is introduced back to the positive term under a strong alignment, the lack of uniformity signals brings a new imbalance issue, and our solution Imin re-balances them by using minp in both terms."}, {"title": "Case Studies", "content": "Figure 3 shows an example of our parsing results, with more examples available in Appendix B. Relying on the implicitly induced grammar, our model provides remarkably accurate parsing results, with all constituents correctly selected. Additionally, the hashing results also demonstrate the impressive capability in discovering syntactic categories. For instance, both preterminal symbols like adjectives, e.g., quick (5400), brown (5E42), and lazy (5E03), and nonterminal symbols like noun phrases, e.g., the quick brown fox (7EBB) and the lazy dog (EEBB), are assigned similar and relevant binary codes to each other. This phenomenon can also be observed in sentences in Appendix B, indicating that our parser can accurately reveal both part-of-speech and constituent features at different hierarchical levels."}]}, {"title": "Related Work", "content": "Syntactic language models, as a historical and important field of language models, had been widely studied even before the deep learning era (Chelba and Jelinek, 2000; Charniak, 2001; Roark, 2001; Klein and Manning, 2002, 2004; Bod, 2006a,b). After that, Shen et al. (2018a,b, 2019) added syntactic inductive bias to LSTM by introducing master gates to control the information flow in hierarchical directions, thereby enabling the model to learn syntactic distance. Under this framework, by training recurrent language models in the usual way, they can obtain parsers that implicitly structure sentences according to the learned syntactic distances. They have also successfully applied this method to transformers (Shen et al., 2021).\nImplicit grammar models induce grammar during the training process by incrementally constructing larger span representations. Kim et al. (2019b) were the first to extend the recurrent neural network grammar (RNNG) (Dyer et al., 2016) from supervised to unsupervised settings. They build parsing trees through continuously shifting and reducing, without introducing explicit production rules. Additionally, DIORA (Drozdov et al., 2019b, 2020) construct span representations and update charts in both inside and outside passes, and then encourage consistency between them. Similarly, R2D2 (Hu et al., 2021, 2022) is trained in a similar manner, but with Gumbel-softmax (Jang et al., 2017) introduced during the tree construction.\nApart from implicit grammar models, explicitly inducing probabilistic context-free grammar (PCFG) is also widely focused. Kim et al. (2019a) first brought PCFG approaches back with a neural parameterization technique and trained language models to reconstruct entire input sentences token by token. Zhu et al. (2020) shows that additionally modeling lexical dependencies (Collins, 2003) is effective, and Jiang et al. (2016); Yang et al. (2021a) further confirmed extending to bilexical dependencies is even more beneficial. Besides, Yang et al. (2021b, 2022); Liu et al. (2023) claimed that the limited number of symbols is a bottleneck of PCFG induction, and proposed to introduce more symbols by applying tensor decomposition to overcome the cubic computational complexity.\nHow much syntactic knowledge is preserved within the ordinary language model is also a question worth considering. Mare\u010dek and Rosa (2018, 2019) noticed the value of attention scores first. They defined distance functions similar to our Equation 17 for probing. However, they did not consider splitting positions, and relied only on fixed attention heads without having them fine-tuned. As a result, their methods resulted in limited accuracy. Hewitt and Manning (2019); Wang et al. (2019); Kim et al. (2020); Li et al. (2020); Bai et al. (2021) then introduced parameterized functions to prob syntactic distances on hidden states and attention scores, and then fine-tuned the entire models. Cao et al. (2020); Maveli and Cohen (2022) fine-tuned pre-trained language models with an additional classifier to distinguish manually generated constituents and distituents, and utilized predictions from this classifier to determine splitting positions on parsing trees during the evaluation stage.\nIn various senses, our approach confirmed the conclusions of many previous works and further pushed their limits. First, by switching the language models from token-level to span-level, we confirmed that modeling lexical dependencies is beneficial and extending this modeling to all tokens in children is more effective. Additionally, by introducing binary representation, we confirmed that employing more symbols is advantageous and further scaling up to 2K can help parsers do further better. Third, we confirmed that constructing span representations and updating the chart is helpful, and unifying the representation of lexicon and syntax leads to more competitive results. Finally, we confirmed the multi-head attention scores already preserve syntactic information, and fine-tuning them can help probe for more insightful features."}, {"title": "Conclusions", "content": "In this paper, we confirmed that the information-preserving capability of binary representation is effective at both lexicon and syntax levels, and we demonstrated that it is feasible to elicit parsers from pre-trained language models by leveraging this capability. We achieved this by upgrading bit-level CKY from zero-order to first-order, extending contrastive hashing from supervised to unsupervised, and proposing a novel objective function to impose stronger yet balanced alignment signals. Experiments show our model achieves competitive performance, and also indicate that the technique for acquiring high-quality syntactic annotations at low cost has now reached a practical stage."}, {"title": "Limitations", "content": "We successfully obtain parsers in an unsupervised manner. Nonetheless, the number of bits remains a hyperparameter that needs to be tuned by testing them individually. Although, in practice, enumerating from 8 to 48 is sufficient for most cases, the relationship between the required number of bits and the specified task remains unclear. Therefore, we aim to explore this issue in future work. Moreover, we simply define the left and right span representations as the average of their token vectors, as shown in Equation 17. The reason for using such a naive definition is a compromise for the sake of speed. However, it is evident that this simple linear mapping may not efficiently preserve high-order information, and future work could explore more complex mechanisms."}]}