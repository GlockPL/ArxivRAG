{"title": "FASTLRNR AND SPARSE PHYSICS INFORMED\nBACKPROPAGATION", "authors": ["WOOJIN CHO", "KOOKJIN LEE", "NOSEONG PARK", "DONSUB RIM", "GERRIT WELPER"], "abstract": "We introduce Sparse Physics Informed Backpropagation (SPInProp), a new class of\nmethods for accelerating backpropagation for a specialized neural network architecture called Low\nRank Neural Representation (LRNR). The approach exploits the low rank structure within LRNR\nand constructs a reduced neural network approximation that is much smaller in size. We call the\nsmaller network FastLRNR. We show that backpropagation of FastLRNR can be substituted for that\nof LRNR, enabling a significant reduction in complexity. We apply SPInProp to a physics informed\nneural networks framework and demonstrate how the solution of parametrized partial differential\nequations is accelerated.", "sections": [{"title": "Introduction", "content": "Backpropagation is a key concept used in training deep learn-\ning models [12]. This paper concerns a technique for accelerating backpropagation\nvia dimensionality reduction in a specialized neural network (NN) architecture called\nLow Rank Neural Representation (LRNR) [5, 21]. LRNRs have a built-in low rank\nfactorization structure that grants them advantages in solving parametrized partial\ndifferential equations (pPDEs) [5], in particular for problems that proved challenging\nfor physics informed neural networks (PINNs) [20, 17]. Theoretically, LRNRS are\nable to approximate complicated nonlinear shock interactions while maintaining low\ndimensionality and regular dynamics [21].\nWe show that, thanks to the LRNRs' low rank structure, smaller NN approxima-\ntions we call FastLRNRs can be constructed. Backpropagation can be performed on\nthe FastLRNR efficiently, and the resulting derivatives can be used to approximate\nderivatives of the original LRNR. Here we focus on presenting the main ideas and\ncomputationally study the simplest possible version. We anticipate many variants\nexploiting the same structure, and we refer to this class of methods accelerating back-\npropagation as Sparse Physics Informed Backpropagation (SPInProp). Here we focus\non an accelerated solution of pPDEs, but SPInProp can potentially accelerate various\ncomputational tasks in deep learning models."}, {"title": "General approach to SPInProp", "content": "In this section, we set up key definitions\nand put forth the main approach. We first define the LRNR architecture in Section 2.1,\nand present the associated FastLRNR architecture in Section 2.2."}, {"title": "Low Rank Neural Representation (LRNR)", "content": "Given n\u2208 N, denote\n[n] := {1, ..., n} and [n]\u2080 = [n] \u222a {0}. A feedforward NN for given depth L\u2208 N and\nwidths (M\u2080, ..., M\u2097) \u2208 N\u1d38\u207a\u00b9 is a function f : \u211d\u1d39\u2070 \u2192 \u211d\u1d39\u1d38 defined as the sequence of\ncompositions\n(2.1)\nz\u02e1 = \u03c3(W\u02e1z\u02e1\u207b\u00b9 + b\u02e1), l\u2208 [L \u2212 1], z\u1d38 = W\u1d38z\u1d38\u207b\u00b9 + b\u1d38,\nwhere the weight matrices W\u02e1 \u2208 \u211d\u1d39\u02e1\u00d7\u1d39\u02e1\u207b\u00b9 and bias vectors b\u02e1\u2208 \u211d\u1d39\u02e1 for l\u2208 [L] are\ncalled the parameters of the NN, and the nonlinear activation function \u03c3: \u211d \u2192 \u211d\nacts entrywise. The input to the NN is z\u2070 \u2208 \u211d\u1d39\u2070 and the output is z\u1d38 \u2208 \u211d\u1d39\u1d38. We\ndenote the maximum width by M\u2098\u2090\u2093 := max\u2097\u2208[L]\u2080 M\u2097, the total width by M\u209c\u2092\u209c\u2090\u2097:=\n\u2211\u2097\u2208[L]\u2080 M\u2097, the collection of the weights and biases W := (W\u02e1)\u2097\u2208[L] and b := (b\u02e1)\u2097\u2208[L],\nrespectively.\nImplicit Neural Representation (INR) is a dense NN whose input dimension M\u2080 is\nthe spatio-temporal dimension and its output dimension M\u2097 is the vector dimension\nof physical variables (density, temperature, pressure, etc). So the dimensions M\u2080 and\nM\u2097 are relatively small. For example, an INR representing a solution to a scalar PDE\nin \u211d\u00b2 has M\u2080 = 2 and M\u2097 = 1.\nTo define LRNRs, we start by defining an INR u\u1d62\u2099\u1d63 : \u211d\u1d39\u2070 \u2192 \u211d\u1d39\u1d38 whose weight\nmatrix W\u02e1 is substituted by the product of three weight matrices\n(2.2)\nW\u02e1 = U\u02e1S\u02e1V\u02e1\u1d40, l\u2208 [L],\nwith the individual factors taking on the form U\u02e1 := [U\u2081\u02e1 | \u00b7\u00b7\u00b7 | U\u1d63\u2097\u02e1] \u2208 \u211d\u1d39\u02e1\u00d7\u1d63\u02e1, V\u02e1 :=\n[V\u2081\u02e1 | \u00b7\u00b7\u00b7 | V\u1d63\u2097\u02e1] \u2208 \u211d\u1d39\u02e1\u207b\u00b9\u00d7\u1d63\u02e1, S\u02e1 := diag(s\u02e1) \u2208 \u211d\u02b3\u02e1\u00d7\u1d63\u02e1. We denote s\u02e1 = (s\u2081,\u2026\u2026,s\u1d63\u2097)\u2208 \u211d\u02b3\u02e1\nand assume all entries in s\u02e1 to be non-negative. We let r := (r\u2097)\u2097\u2208[L], r\u2098\u2090\u2093:=\nmax\u2097\u2208[L] r\u2097, r\u2098\u1d62\u2099 := min\u2097\u2208[L] r\u2097 and r\u209c\u2092\u209c\u2090\u2097 := \u2211\u2097\u2208[L] r\u2097.\nThe factored re-formulation (2.2) of the weight matrices (2.1) resembles the sin-\ngular value decomposition (SVD) in its appearance [11], and if we assume r\u2097 \u00ab M\u2097 for\nall l\u2208 [L] the products (2.2) form low rank matrices. Note that the same treatment\nis possible for the bias terms b\u02e1, but we will not factor the bias here (however, see [21]\nwhere the bias is similarly factored and plays a central role).\nWe collect these matrices with the notations U := (U\u02e1)\u2097\u2208[L], V := (V\u02e1)\u2097\u2208[L], and\ns := (s\u02e1)\u2097\u2208[L]. We refer specifically to parameters in s as coefficient parameters, and to\nthe parameters in (U, V) as bases parameters; they will play distinct roles. Note that\ns has O(Lr\u2098\u2090\u2093) parameters, whereas U and V each have O(M\u2098\u2090\u2093r\u2098\u1d62\u2099) parameters.\nThe quadruple (U, V, b, s) contains all the parameters of u\u1d62\u2099\u1d63 so one can express its\nfull parametric dependence by writing\n(2.3)\nu\u1d62\u2099\u1d63(\u00b7) = u\u1d62\u2099\u1d63(\u00b7; U, V, b, s).\nThe meta-learning framework proposed in [5] has two different phases for train-\ning the parameters (U, V, b, s). In the first phase, or the meta-learning phase, the"}, {"title": "FastLRNRs", "content": "We turn our attention to the dimensionality reduction of\nLRNRs. The key idea is to construct an approximation to the s-independent parts of\nu\u2097\u1d63\u2099\u1d63 (2.4). Taking two contiguous layers, we write the part between the coefficient\nparameters s\u02e1's as p\u02e1(\u2022) := V\u207d\u02e1\u207a\u00b9\u207e\u1d40\u03c3(U\u02e1\u2022) and view it as a projected version of the\nnonlinear activation \u03c3 (we omit the biases for ease of exposition). It is known that\nwhen a scalar nonlinear function is applied to low rank vectors, the function values\nthemselves can be well-approximated by low rank bases in certain situations [7, 2, 4,\n3, 15]. A family of well-known and simple techniques can be applied to obtain an\napproximation to p\u02e1 of the form\n(2.6)\n=(\u2022) := (l+1)T=(\u00db.(.)), l\u2208 [L \u2212 1],\nin which the reduced bases parameters now have the dimensions \u00db\u02e1 \u2208 \u211d\u02b3\u02e1\u00d7\u1d63\u02e1 and\nV\u02e1\u207a\u00b9 \u2208 \u211d\u02b3\u02e1\u207a\u00b9\u00d7\u1d63\u02e1, and the reduced dimensions are small in the sense r\u2097 ~ r\u2097 \u00ab M\u2098\u2090\u2093.\nWe writer := (r\u02e1)\u2097\u2208[L\u22121] and r\u0302\u2098\u2090\u2093 := max\u2097\u2208[L\u22121] r\u2097.\nFor example, consider the use of the empirical interpolation method (EIM/DEIM\n[2, 4]). Writing out the EIM approximation, which uses a subsampling of the state\nvectors, we have\n(2.7)\n\u2248(\u2022) := Vl+1T=l(PlT=l)\u22121\u03c3(PetUl(.)), \u039e\u02e1\u2208 \u211d\u1d39\u02e1\u00d7\u1d63\u02e1, r\u2097\u2208N,\nwhere Pl\u2208 \u211d\u1d39\u02e1\u00d7\u1d63\u02e1 is a sampling matrix made up of subcolumns of a M\u2097 \u00d7 M\u2097\nidentity matrix, and \u2203\u02e1 \u2208 \u211d\u1d39\u02e1\u00d7\u1d63\u02e1 is a matrix with linearly independent columns\nwhose square subblock Pl\u2203\u02e1 \u2208 \u211d\u02b3\u02e1\u00d7\u1d63\u02e1 is invertible. Viewed in the general form\n(2.6), this approximation has \u00db\u02e1 = Pl\u1d40U\u02e1 and V\u02e1\u207a\u00b9 = V\u207d\u02e1\u207a\u00b9\u207e\u1d40\u039e\u02e1(Pe\u1d40\u039e\u02e1)\u207b\u00b9.\nInserting (2.6) into the LRNR architecture, one obtains z\u02e1 = =(diag(s\u02e1)z\u02e1\u207b\u00b9)\nwhere the multiplication by diag(s\u02e1) can be rewritten using the notation \u2299 for the\nHadamard (elementwise) product. Concisely,\n(2.8)\nz\u02e1 = U\u02e1(s\u02e1 \u2299 \u00bf\u02e1\u207b\u00b9), l\u2208 [L \u2212 1], \u00bf\u1d38 = U\u1d38(s\u1d38 \u2299 \u00bf\u1d38\u207b\u00b9).\nThen we have an approximation of the hidden states z\u02e1 in the original LRNR (2.1)\npurely in terms of the approximate projected/reduced states, that is\nz\u02e1 \u2248 V\u207d\u02e1\u207a\u00b9\u207e\u1d40z\u02e1, l\u2208 [L-1]\u2080, L\u2248 z\u1d38.\nNote that in the input and output layers, the reduction can be trivial.\nWe refer to the reduced form of LRNR (2.8), as a FastLRNR \u00db\ud835\udc39\ud835\udc34\ud835\udc46\ud835\udc47:\u211d\u1d39\u2070 \u2192 \u211d\u1d39\u1d38.\nDiagrams in Figure 2.1 compare the LRNR versus the FastLRNR architectures, and\nillustrate the subsampling strategy devised in EIM in this context. We remark that\nthe approximation still has the structure of NNs (2.1), except that the nonlinear\nactivation functions \u03c3 correspond to specialized functions \u03da\u02e1, which are generally\ndifferent functions depending on the layer (cf. [18]). In this analogy, the Hadamard\nproducts with s\u02e1 in (2.8) play the role of affine mappings in (2.1), and in this sense\nFastLRNRS (2.8) (and LRNRs) are said to be diagonalizeable."}, {"title": "SPInProp via backpropagation of FastLRNR", "content": "SPInProp complexity\ndepends only on L and the small dimension r\u0302\u2098\u2090\u2093. Comparing the backpropagation\ncomplexities: (1) For NNs (2.1) with parameters (W,b), O(LM\u2098\u2090\u2093\u00b2) operations; (2)\nFor LRNR u\u2097\u1d63\u2099\u1d63 (2.4) with parameters s, O(LM\u2098\u2090\u2093r\u2098\u2090\u2093\u00b2); (3) For FastLRNR \u00db\ud835\udc39\ud835\udc34\ud835\udc46\ud835\udc47\n(2.8) with parameters s, O(L\u1e5d\u2098\u2090\u2093\u00b2)."}, {"title": "Fast Low Rank PINNs (Fast-LR-PINNs)", "content": "We seek to demonstrate that\nSPInProp operations can be used to efficiently perform backpropagation operations\nwhen solving pPDEs. To this end, we devise computational experiments in the LR-\nPINNs meta-learning framework [5] with important additions. We discuss the meta-\nlearning phase in Section 3.1. A new phase using SPInProp, which we call the fast\nphase, is introduced in Section 3.2"}, {"title": "Meta-Learning with Sparsity Promoting Regularization", "content": "As dis-\ncussed in Section 2.1, during the meta-learning phase the bases and bias parameters\n(U, V, b) are trained. This is done using a hypernetwork representation of s(\u03bc) (see\n(2.5)). We define a hypernetwork f\u2095\u0443\u209a\u2091\u1d63 : D \u2192 \u211d\u1d57\u1d52\u1d57\u1d43\u2097 which is itself a NN (2.1) whose\nwidths satisfy M\u2095\u0443\u209a\u2091\u1d63,\u2097 ~ S\u209c\u2092\u209c\u2090\u2097 and has an extra ReLU applied so that the outputs\nare non-negative. We call its NN parameters \u0398 and write f(\u2022) = f(\u2022; \u0398).\nThe meta-network is used during this phase, defined as\n(3.1)\nu\u2098\u2091\u209c\u2090(\u00b7; \u03bc, U, V, b, \u04e8) := u\u1d62\u2099\u1d63(\u00b7 ; U, V, b, f\u2095\u0443\u209a\u2091\u1d63 (\u03bc ; \u04e8)).\nThis model results from a type of output-to-parameter concatenation between (a) u\u1d62\u2099\u1d63\nwith low rank structure in its parameters (2.3), and (b) f\u2095\u0443\u209a\u2091\u1d63 whose non-negative\noutputs feed into u\u1d62\u2099\u1d63 as coefficients s.\nThe PINNs loss for the meta-learning phase is given by\n(3.2)\nL\u2098\u2091\u209c\u2090 (U, V, b, \u0398) := E(\u06f0,\u00b5) [|N[U\u2098\u2091\u209c\u2090 ; \u03bc](\u00b7; \u03bc, U, V, b, \u04e8)|\u00b2]\n+ E(.,\u03bc) [[B[U\u2098\u2091\u209c\u2090 ; \u03bc](\u00b7; \u03bc, U, V, b, \u04e8)[\u00b2]\nwhere N[\u00b7; \u03bc] denotes a (nonlinear) differential operator, and B[;\u00b5] the initial-\nboundary operator. The expectation is taken over some probability measure over\nthe PDE domain \u03a9 and physical parameter domain D as specified in the pPDE prob-\nlem. Uniform measure is commonly used.\nWe define the orthogonality regularization term and the sparsity-promoting reg-\nularization term as\n(3.3)\nR\u2092\u1d63\u209c\u2095(U, V) := \u2211 {||UeTUe \u2013 Ie|| + ||Vetve \u2013 Ie||},\nle[L]\nRsparse(\u0398) := \u0395\u03bc [ \u2211 ||\u03c3(\u0393' fhyper (\u03bc; \u0398))||\u2081],\nle[L]\nwhere Il \u2208 Rrexre are identity matrices, ||\u2022|| F denotes the Frobenius norm, and where\nthe matrices Fl\u2208 R(re-1)\u00d7re are banded matrices with -1's on the diagonal and \u03b3 on\nthe 1st super-diagonal (\u03b3 \u2265 1 is a hyperparameter). The motivation for adding Rsparse\nis to promote the sparsity in the coefficient parameters s in u\u1d62\u2099\u1d63 above (3.1), thereby\nconstraining the meta-network umeta to become as low rank as possible. In u\u2098\u2091\u209c\u2090 the\ncoefficient parameters are coupled to the fhyper output variables, so we enforce the\nsparsity on the parameters \u0472 of fhyper. When Rsparse is zero the coefficient parameters\nse (which are output from fhyper) satisfy (1/\u03b3)se > se+1 so a geometric rate of decay is\nachieved. An apparently more intuitive [8] choice of taking the 1-norm || fhyper (\u03bc; \u0398)||\u2081\nwas less successful in promoting sparsity in our experiments.\nFinally, the meta-learning problem is given by\n(3.4)\nmin L\u2098\u2091\u209c\u2090 (U, V, b, O) + A\u2092\u1d63\u209c\u2095R\u2092\u1d63\u209c\u2095 (U, V) + Asparse Rsparse (\u0398).\nU,V,6,\n3.2. Fast phase. At the end of the meta-learning phase, we have learned the\nbases and bias parameters (U*, V*, b*), along with the hypernetwork parameters \u0398*."}, {"title": "Fast phase", "content": "This brings us to the fine-tuning phase, where one solves the PDE for any given\nphysical parameter \u03bc\u2208D by fine-tuning the u\u2097\u1d63\u2099\u1d63 coefficient parameters s (2.4)\nas informed by the differential equations. The coefficients s are initialized at the\nhypernetwork prediction value fhyper (\u03bc; \u0398*). During this phase, the learning problem\nis to minimize the physics-informed loss function over a small number of coefficient\nparameters s,\n(3.5)\nmin Ltune (s; \u03bc),\ns\nwhere Ltune (8; \u03bc) := E [[N[ULRNR; 4](\u2022; 8)[\u00b2] + E [|B[ULRNR ; \u03bc](\u2022; 8)|\u00b2].\nIn the previous experiments [5], the fine-tuning phase for Ulane was found to\nreliably yield improved PDE solutions. Now, if FastLRNR \u00dbFAST (2.8) is a type of an\napproximation of u\u2097\u1d63\u2099\u1d63, it is natural to attempt to update the coefficient parameters\ns using the smaller \u00fbFAST, and test if these updates result in improved solutions. If\nthey do, it means updates using \u00dbFast alone can lead to improved coefficient values\nSfast (\u03bc), at the computationally cheaper cost of SPInProp operations (Section 2.3).\nWe newly introduce the fast phase during which we solve a learning problem for\n\u00dbFAST. Following a subsampling strategy (Section 2.2), we define a sparsely sampled\nversion of the fine-tuning-phase loss Ltune,\n(3.6)\nLfast (s; \u03bc) := \u2211 N[\u00dbFAST; \u03bc](\u00b7; 8)| + \u2211 B[\u00dbFAST ; \u03bc](\u00b7; s)|,\n\u2022 \u2208\u03a7\u03a9\u03a9\n\u2022 \u2208\u03a7\u03a9\u0398\u03a9\nwhere X < \u03a9\u0398\u03a9 is a set of sparse sampling points. Here we have used the l\u2081 error,\nmotivated by the L\u2081 minimization formulation of convective problems [14, 13].\nA na\u00efve approach would be to substitute the higher-dimensional learning problem\n(3.5) by a lower-dimensional one, replacing the loss Ltune with a direct analogue Lfast.\nHowever, this approach did not automatically lead to improvements in the solution\nin our experiments. While the training loss decreases consistently, the FastLRNR\nsolution u\u2097\u1d63\u2099\u1d63(\u2022; s) eventually diverges away from the true solution despite improve-\nment during the initial epochs. This suggests the na\u00efve learning problem is affected\nby a type of generalization issue. We speculate there are two possible causes. First,\nFastLRNR performs sparse sampling of both the PDE domain and the hidden states,\npossibly incurring bias that leads to generalization errors. Second, FastLRNR per-\nforms projections at each layer and the effect of these projections on stability can be\nsignificant and complex, potentially causing this issue; related stability issues arise in\nsimpler linear models like reduced basis methods (see e.g. [22, 9, 10, 6]).\nTo deal with the generalization issue, we regularize the na\u00efve learning problem by\nadding a locality regularization term for the coefficient parameters s. We set the fast\nphase learning problem as\n(3.7)\nmin Lfast (s; \u00b5) + AlocRloc(s; \u03bc), Rloc(s; \u03bc) := \u2211 ||se \u2013 fhyper (\u03bc) ||\u2081.\ns\nle[L]\nOnce this optimization problem is solved, the solution Sfast (\u03bc) can be used in the\noriginal LRNR representation. Note that the problem (3.7) was informed by physics\nat the sampling points X, implying that \u00dbFAST is trained to be close to u\u2097\u1d63\u2099\u1d63 only"}, {"title": "Parametrized convection-diffusion-reaction problem", "content": "We consider a\nparametrized initial boundary value problem: Convection-diffusion-reaction on the\nspatio-temporal domain \u03a9 := (0,2\u03c0) \u00d7 (0,1) with periodic boundary conditions. We\nseek the solution u: \u03a9 \u2192 R satisfying the following PDE with non-negative physical\nparameters \u03bc = (\u03bc\u2081, \u03bc\u2082, \u03bc\u2083),\n\u039d[u; \u03bc] = 0, (x,t) \u2208\u03a9,\n(4.1)\nwhere N[u ; \u03bc] := \u03b4\u209cu + \u03bc\u2081\u03b4\u2093u \u2013 \u03bc\u2082\u2202\u2093\u2093u \u2013 \u03bc\u2083u(1 \u2013 \u0438),\nu(x, 0) = sin(x), \u03c7\u03b5 (0,2\u03c0), u(0,t) = u(2\u03c0,t), t\u2208 (0,1).\nWe consider two physical parameter domains: The first is a pure convection problem,\nand the second is the full convection-diffusion-reaction problem:\n(4.2)\nDconv = diag([\u00b5conv,min, \u00b5conv,max]), Dcdr = diag([\u00b5cdr,min, \u00b5cdr,max]),\nwhere we set conv,min = (5,0,0), \u00b5conv,max = (8,0,0), \u00b5cdr,min = (1,0,0), \u00b5cdr,max\n(3,2,2). The PDE is nonlinear and is highly convective when the diffusion term \u03bc\u2082"}]}