{"title": "DECODING COMPLEXITY: CHPDA \u2013 INTELLIGENT PATTERN\nEXPLORATION WITH A CONTEXT-AWARE HYBRID PATTERN\nDETECTION ALGORITHM", "authors": ["Lokesh Koli", "Shubham Kalra", "Karanpreet Singh"], "abstract": "Detecting sensitive data such as Personally Identifiable Information (PII) and Protected Health\nInformation (PHI) is critical for data security platforms.[1] This study evaluates regex-based pattern\nmatching algorithms and exact-match search techniques to optimize detection speed, accuracy,\nand scalability. Our benchmarking results indicate that Google RE2 provides the best balance of\nspeed (10-15 ms/MB), memory efficiency (8-16 MB), and accuracy (99.5%) among regex engines,\noutperforming PCRE while maintaining broader hardware compatibility than Hyperscan. For exact\nmatching, Aho-Corasick demonstrated superior performance (8 ms/MB) and scalability for large\ndatasets. Performance analysis revealed that regex processing time scales linearly with dataset size\nand pattern complexity. A hybrid AI + Regex approach achieved the highest F1 score (91. 6%) by\nimproving recall and minimizing false positives. Device benchmarking confirmed that our solution\nmaintains efficient CPU and memory usage on both high-performance and mid-range systems.\nDespite its effectiveness, challenges remain, such as limited multilingual support and the need for\nregular pattern updates. [2] Future work should focus on expanding language coverage, integrating\ndata security and privacy management (DSPM) with data loss prevention (DLP) tools, and enhancing\nregulatory compliance for broader global adoption.", "sections": [{"title": "1 Introduction", "content": "Efficient data management is essential for organizations to ensure that sensitive information such as Personally\nIdentifiable Information (PII), Protected Health Information (PHI) and financial records are systematically identified and\nprotected. Effective classification aids in compliance with regulations such as the General Data Protection Regulation\n(GDPR) and the Health Insurance Portability and Accountability Act (HIPAA), while mitigating security risks through\nreal-time threat detection[3] Automated tools improve operational efficiency by streamlining access and eliminating\nredundancies. Customized classification systems fulfill global compliance requirements, while centralized control\nmechanisms enhance governance through unified policy enforcement.[4] Strategic data classification is crucial to\nachieve security, compliance, and operational effectiveness in the digital environment of today.\n\nIdentifying PII and PHI across various data formats presents considerable challenges, particularly with unstructured data\nsets. Differences in encoding and file formats (e.g., PDFs, Word documents, databases, CSV, and other text files) and data\nstorage systems complicate the consistent extraction of sensitive information [5]. Moreover, international regulations\nsuch as GDPR, HIPAA, and the California Consumer Privacy Act (CCPA) impose varied compliance mandates, adding\nfurther complexity to detection efforts. Customizing detection mechanisms to align with region-specific regulations\nwhile ensuring accuracy across different content types is formidable. The necessity for real-time detection and\nthe reduction of false positives amplifies this challenge, necessitating advanced algorithms and comprehensive data\nmanagement strategies."}, {"title": "2 Related work", "content": "Detecting sensitive information such as Personally Identifiable Information (PII) and Protected Health Information\n(PHI) has been widely studied in the domains of data security, natural language processing (NLP), and pattern-matching\ntechniques. Existing approaches primarily fall into three categories: regex-based detection, AI-driven Named Entity\nRecognition (NER), and hybrid methods combining both."}, {"title": "2.1 Regex-Based Detection", "content": "Regular expressions (regex) have long been used for pattern-based text matching, forming the backbone of many\ndata loss prevention (DLP) systems [8]. Popular regex engines such as PCRE, Google RE2, and Hyperscan have\nbeen benchmarked for efficiency in large-scale text scanning [9]. While regex-based approaches offer deterministic\naccuracy and speed, they struggle with pattern generalization and require frequent updates to accommodate evolving\ndata structures. Furthermore, regex engines like PCRE suffer from backtracking issues, leading to unpredictable\nexecution times [10]."}, {"title": "2.2 AI-Driven Named Entity Recognition", "content": "Recent advancements in NLP have enabled deep learning-based NER models to identify sensitive entities beyond strict\npattern matching. Models such as BERT [11] and spaCy's NER [12] have demonstrated strong recall in detecting\ncomplex entities across diverse linguistic contexts. However, AI-based approaches introduce challenges such as higher\ncomputational costs, false positives, and the need for extensive labeled datasets [13]."}, {"title": "2.3 Hybrid AI + Regex Approaches", "content": "Several studies have explored hybrid methods that combine regex with machine learning for enhanced detection\naccuracy. Souza et al. [14] proposed an approach where regex serves as a pre-filtering mechanism, followed by an AI\nmodel to refine entity classification. Similarly, Friebely et al. [15] demonstrated that integrating regex with deep learning\nimproves precision while maintaining high recall, making such approaches more suitable for real-time applications."}, {"title": "2.4 Comparative Benchmarking and Efficiency", "content": "Prior research has also focused on benchmarking various detection techniques for performance and scalability. Hyper-\nscan has been identified as the fastest regex engine but comes with hardware constraints [16]. Meanwhile, RE2 has been\npraised for its balance between speed and memory efficiency, making it a practical choice for large-scale deployments\n[17]. AI-based solutions, while powerful, tend to be resource-intensive and less predictable in execution time [18]."}, {"title": "2.5 Contributions of This Work", "content": "While previous studies have explored regex, AI-based NER, and hybrid detection models separately, our work\nsystematically benchmarks these approaches under real-world conditions. We evaluate regex engines such as RE2,\nPCRE, and Hyperscan alongside AI-driven detection methods to identify an optimal balance between accuracy, speed,\nand scalability. By integrating regex with AI in a hybrid model, we achieve improved detection accuracy while\nmaintaining computational efficiency, making our approach well-suited for large-scale data security applications."}, {"title": "3 Materials and methods", "content": ""}, {"title": "3.1 Proposed system", "content": "The proposed system detects and manages sensitive information while minimizing resource consumption effectively. It\nuses a lightweight agent deployed on client systems, configured with a PII and PHI patterns glossary. The detection\nworkflow incorporates multiple advanced steps, starting with regex-based pattern recognition using Google RE2 to\nidentify predefined formats and patterns. The process optimizes contextual keyword matching by leveraging the\nAho-Corasick algorithm, which assigns confidence scores based on keyword proximity. Then, the system further refines\nthe detected data by filtering out low-confidence matches using threshold scores. Advanced Named Entity Recognition\n(NER) models powered by machine learning perform secondary scans to identify entities like names and phone numbers\nin context to enhance accuracy. Specialized validation algorithms like Luhn's verify specific data types, like credit card\nnumbers, reducing false positives. See Figure 1. The system ensures robust compliance with GDPR, HIPAA, and CCPA\nregulations, delivering reliable and secure data protection."}, {"title": "3.2 Regex", "content": "Google RE2 Pattern Recognition We chose Google RE2 as our primary regular expression detection library over\nalternatives such as PCRE, Oniguruma, and Boost. Regex is known for its efficiency and security in big data processing;\nunlike other libraries, RE2 offers linear time complexity for regular expression matching, and its SET mechanism\nprovides an effective solution for matching multiple regex patterns simultaneously in a single scan of the input text. For\nexample, consider the task of identifying Social Security Numbers (SSNs) in various formats, such as the standard\nformat $\\b\\d{3}-\\d{2}-\\d{4}$, the compact format $\\b\\d{9}$, and the masked format $\\bXXX-XX-\\d{4}$, within a large\nbody of text. Instead of scanning the text separately for each pattern, RE2 compiles these patterns into a single\ndeterministic finite automaton (DFA) merging their logic and assigning unique IDs to each pattern for efficient\nmatching.\n\nRE2 scans the text character by character during processing, updating the automaton state based on the encountered\ninput. For instance, given a paragraph like \"John's SSN is 123-45-6789. He also used the compact format 987654321\non some forms. For security reasons, his company sometimes masks it as XXX-XX-6789,\" RE2 matches 123-45-6789\nto the standard SSN pattern, 987654321 to the compact format, and XXX-XX-6789 to the masked format. The SET\nmechanism evaluates all patterns simultaneously, producing results in linear time with matched IDs corresponding to\ntheir respective patterns. [19]\n\nDetailed Explanation:\n1. Start State: The automaton begins in the initial state.\n2. Transitions:\n\u2022 The first branch handles digits ($\\d$) for both the standard SSN format (123-45-6789) and the compact\nSSN format (987654321).\nThe standard format transitions to a state expecting a hyphen (-) after three digits, followed by two\nmore numbers, another hyphen, and four final digits.\nIn the compact format, it directly reads nine consecutive digits and reaches an accepting state.\n\u2022 The second branch handles the masked SSN format (XXX-XX-6789). This branch transitions through\nX characters for the first three positions, followed by a hyphen (-), then two more X characters, another\nhyphen, and finally four digits. See Figure 2.\n3. Accepting States:\n\u2022 The automaton has multiple accepting states:\nOne for the standard SSN format after processing $\\d{3}-\\d{2}-\\d{4}$.\nOne for the compact SSN format after processing $\\d{9}$.\nOne for the masked SSN format after processing XXX-XX-\\d{4}.\n4. Efficiency: The shared transitions (e.g., digits or hyphens) between different patterns reduce redundancy,\nensuring the automaton processes the text efficiently in a single pass.\n5. Fail States: Suppose an invalid character is encountered (e.g., an extra letter or symbol not part of the expected\npattern). In that case, the automaton transitions to a fail state and stops further processing for that branch."}, {"title": "3.3 Rescan with AI Model", "content": "After the initial pattern detection phase, the system performs a secondary scan using AI-powered Named Entity\nRecognition (NER) models specifically designed to address the complexities of detecting sensitive information that\ntraditional regular expressions (regex) often miss. For example, while regex may effectively identify basic patterns\nlike email addresses or phone numbers, it struggles with nuanced data such as medical terminologies, legal clauses,\nfinancial records, or even names and addresses embedded in complex textual structures.AI models excel at analyzing\ncontext and semantics to identify entities that do not follow straightforward patterns.\n\nFor instance, NER models trained on datasets like PubMed and other medical literature detect Protected Health\nInformation (PHI), such as medication names or patient IDs, even when presented in varying formats or embedded\nwithin medical notes. Financial reports and transaction records datasets in finance allow models to identify sensitive\nfinancial details, such as account numbers and credit card information, that Experts in the legal domain customize NER\nmodels to recognize complex entities specific to contracts, such as clauses, sensitive terms, and involved parties. They\ntrain these models on custom datasets generated through generative AI, which creates diverse and realistic examples\nthat capture industry-specific nuances, edge cases, and rare entity patterns. This active approach ensures comprehensive\ndetection coverage and minimizes the risk of overlooking Personally Identifiable Information (PII), PHI, or financial\nrecords.\n\nThe system's scalability and modular architecture further enhance its utility, enabling the seamless integration of new\nindustry-specific models to address emerging regulatory or compliance requirements. By leveraging generative AI for\ndata diversity, the system produces robust models that handle various scenarios, significantly reducing false positives\nand improving overall detection accuracy."}, {"title": "3.4 Exact Match: Optimized with Aho-Corasick algorithm", "content": "The Aho-Corasick algorithm efficiently solves the problem of exact pattern matching in significant texts, especially\nwhen multiple patterns need simultaneous matching. It constructs a deterministic finite automaton (DFA) using a Trie\n(prefix tree) to represent the set of patterns. The algorithm inserts each pattern into the Trie and adds failure links to\nhandle mismatches by directing the algorithm to the longest matching suffix. This approach allows the algorithm to\ncontinue the search without restarting, significantly improving performance.[21] The Aho-Corasick algorithm processes\nthe text in a single pass, achieving a time complexity of O(N+M), where N is the length of the text, and M is the\ntotal length of all patterns. The algorithm is particularly valuable for virus scanning, real-time intrusion detection,\nand analyzing large amounts of text.[22] While it operates efficiently, it does face some challenges, including high\nmemory usage because of its Trie structure and the necessity of rebuilding the automaton when the patterns change.\nNevertheless, it excels at managing thousands of patterns simultaneously without needing to backtrack or restart the\nsearch process. This capability and its consistent performance make it an effective tool for multi-pattern matching,\nespecially in complex and time-sensitive situations.\n\nExample:\nTo better understand how the Aho-Corasick algorithm works, consider the following patterns and a sample text:\n\nPatterns:\n1. he"}, {"title": "3.5 Testing authentication functions", "content": "Various specialized validation functions apply to different data types to accurately detect Personally Identifiable\nInformation (PII) and Protected Health Information (PHI). For example, Luhn's algorithm validates credit card numbers\nby checking their checksum structure. Similarly, format checks, reserved area numbers, and geographical data validate\nSocial Security Numbers (SSNs). The SSN must follow the 9-digit format XXX-XX-XXX. Before 2011, the first three\ndigits (area number) were geographically assigned, and specific numbers (such as 000, 666, or 900-999) were invalid.\n\nTo validate phone numbers, we ensure they follow country-specific formats. (e.g., +1-XXX-XXX-XXX for U.S.\nnumbers or +44-XXXX-XXXX for U.K. numbers), including checks for valid country codes and region codes. Mobile\nnumbers are verified against known carriers or ranges to ensure authenticity and activity.\n\nEmail addresses undergo validation for correct formatting using regular expressions (regex) and domain checks. Medical\nrecord numbers (MRNs) follow predefined institutional formats for validation. Health insurance policy numbers adhere\nto established structures, while driver's license numbers comply with specific state or country validation rules, often\nincorporating checksum digits. Passport numbers require verification for format compliance and adherence to country-\nspecific regulations. Organizations must implement tailored validation techniques to ensure the accuracy of detected\ndata patterns, reduce false positives, and enhance the reliability of systems handling sensitive information. They must\nverify that bank account numbers, including International Bank Account Numbers (IBANs), adhere to the correct\nformat and checksum.\n\nAdditionally, they need to check postal codes for proper formatting and geographical validity while validating dates of\nbirth for correct formatting and reasonable age ranges. Tax Identification Numbers (TINs) should be tested against\ncountry-specific regulations. By applying these validation functions, organizations can maintain data integrity and\nensure compliance with regulatory standards such as GDPR and HIPAA."}, {"title": "3.6 Proximity Match Scoring Mechanism", "content": "This mechanism quantifies the relationship between detected patterns (e.g., sensitive data) and surrounding contextual\nkeywords by assigning a confidence score. The confidence score is determined based on the proximity between the\nkeyword and the pattern, as well as the validation of the pattern using a verification function. The closer the keyword\nis to the detected pattern, the higher the confidence score. Additionally, if the pattern passes the validation function\n(e.g., Luhn's algorithm for SSNs), a further boost in confidence is applied. This approach ensures robust context-aware\ndetection while reducing false positives."}, {"title": "3.6.1 Algorithm", "content": "Confidence Score Calculation\n1: Input: A string S, detected keyword K, detected pattern P, and a validation function V(P).\n2: Calculate Distance d\n3: Compute the distance between K and P:\n\n$d = |Position(K) \u2013 Position(P)|$\n\n4: Assign Proximity Score\n5: Define a maximum distance $D_{max}$ beyond which the proximity score is 0.\n\n$Proximity\\_Score(d) = max(0, \\alpha\\cdot (D_{max} - d))$\n\nwhere a is a scaling factor.\n6: Validate Pattern\n7: if V(P) = True then\n8: Add Validation_Score to Ctotal.\n9: else\n10: Validation_Score = 0.\n11: end if\n12: Calculate Total Confidence\n\n$C_{total} = Proximity\\_Score(d) + Validation\\_Score$\n\n13: Output: $C_{total}$"}, {"title": "3.6.2 Example", "content": "Input:\n\u2022 String: \"John's card number is 123-45-6789.\"\n\u2022 Keyword: K = \"card number.\"\n\u2022 Pattern: P = \"123-45-6789\"\n\u2022 Validation function: V(P) = Luhn's Algorithm\n\n1. Calculate Distance d:\n\nd = 7 (number of characters between \"card number\" and \"123-45-6789\").\n\n2. Assign Proximity Score:\n\nLet $D_{max}$ = 20, $\u03b1$ = 2.\n\n$Proximity\\_Score(d) = max(0, \u03b1\\cdot (D_{max} \u2013 d))$\n\n= max(0,2 (20 \u2013 7)) = 26.\n\n3. Validate Pattern:\n\nThe pattern P = \"123-45-6789\" passes Luhn's algorithm.\n\nValidation_Score = 30.\n\n4. Calculate Total Confidence:\n\n$C_{total} = Proximity\\_Score(d) + Validation\\_Score$\n\n$C_{total} = 26 + 30 = 56$.\n\nThe total confidence score for detecting the sensitive data P (\"123-45-6789\") near the keyword K (\"card number\") is\n56. Proximity contributes 26, and validation contributes 30.\n\nThis algorithm can be adapted to suit specific use cases by using different proximity weighting ($\\alpha$), distance thresholds\n($D_{max}$), and validation scores."}, {"title": "3.7 Data filtering based on threshold scores", "content": "Once the system assigns confidence scores, it applies a data filtering mechanism based on a user-defined threshold\nscore T. The system retains detected patterns with confidence scores C that satisfy the condition:\n\n$C > T$\n\nfor further processing, while it discards those with C < T. This filtering step ensures that only highly reliable matches\nare considered, effectively reducing noise and prioritizing actionable information.\n\nThreshold T Adjustment: The threshold T can be dynamically adjusted to balance precision and recall:"}, {"title": "4 Experimental setup", "content": "Setup details\nSoftware: The developers implemented the AI-based NER model using programming languages and libraries such as\nTensorFlow and PyTorch. They utilized Google's RE2 library for regular expression-based operations because of its\nefficiency in pattern recognition and processing large datasets.\nHardware: The team evaluated client-side processing on ARM-based devices equipped with 2 GB of RAM and a\nquad-core processor to ensure system efficiency in lightweight environments. They also tested multi-user scenarios\nusing multiple Windows systems with different test accounts.\n\nPerformance testing\n\nMeasuring speed and accuracy\n\n\u2022 The system achieved an average processing speed of 100 MB/s across data files up to 1 TB in various formats\n(e.g., PDF, CSV, and JSON files). Benchmark accuracy tests showed over 95% accuracy in detecting PII and\nPHI.\n\n\u2022 The team has tuned the detection accuracy to maintain a score of 94%, ensuring minimal missed sensitive data.\nFalse positive rate: The false positive rate remained below 3%, indicating the system's ability to distinguish\nsensitive data from irrelevant patterns."}, {"title": "5 Results and Discussion", "content": ""}, {"title": "5.1 Regex Pattern Matching Algorithms Comparison", "content": "Based on the results in Table 1.3, Google RE2 strikes an optimal balance between speed, memory consumption, and\naccuracy, making it the ideal choice for our solution. RE2 achieves a detection speed of 10-15 ms/MB, significantly\nfaster than PCRE's 50-80 ms/MB, while maintaining lower memory usage at 8-16 MB compared to PCRE's 12-24\nMB. Additionally, RE2 delivers a high accuracy rate of 99.5% with minimal false positives at 0.5%. Although\nHyperscan demonstrates superior performance with faster detection (2-5 ms/MB) and higher accuracy (99.9%), it\ncomes with substantial hardware restrictions and higher memory requirements (32-64 MB), making it impractical for\nbroad deployment in resource-constrained environments See Table 1. Consequently, we prioritized RE2's scalability,\nreliability, and adaptability across diverse platforms, ensuring consistent performance without needing specialized\nhardware[20]."}, {"title": "5.2 Exact Match Algorithms Performance", "content": "Based on the results in Table 1.4, Aho-Corasick stands out as the optimal choice for our solution due to its exceptional\nbalance of speed, scalability, and efficiency. Aho-Corasick achieves a search time of just 8 ms/MB, significantly\noutperforming Knuth-Morris-Pratt's 15 ms/MB and Boyer-Moore's 12 ms/MB. Additionally, Aho-Corasick excels in\nhandling large datasets, showing excellent scalability for texts over 100MB, making it ideal for applications requiring\nhigh performance on vast datasets. While Knuth-Morris-Pratt and Boyer-Moore offer efficient matching for smaller\ndatasets, their slower speeds and limited scalability make them less suitable for larger, more complex use cases. Given\nAho-Corasick's superior performance and efficiency, it is the preferred choice for tasks involving large-scale text\nanalysis and real-time pattern matching, ensuring optimal results across various data sizes and platforms See Table 2."}, {"title": "5.3 Performance Analysis", "content": "The performance results indicate a consistent rise in folder classification time as dataset size and regex complexity\nincrease. For example, with the 100-pattern regex set, processing times ranged from 4.85 seconds for a 100MB dataset\nto 530.87 seconds for 10GB. When the regex complexity increased to 150 or 172 patterns, processing times saw a slight\nuptick, particularly for larger datasets, as detailed in See Table 3. This data, further illustrated in Figure 4, underscores\nthe linear scaling of regex processing times with growing dataset size and pattern complexity."}, {"title": "5.4 Detection Accuracy", "content": "The detection accuracy results highlight the strengths and trade-offs of different methods. Regex alone demonstrated\nhigh precision for exact matches but struggled with recall, particularly for non-standard patterns. AI alone achieved\nbetter recall, successfully identifying patterns beyond regex limitations, but at the cost of a higher false favorable rate.\nIn contrast, AI + Regex integration provided the best balance between precision and recall, achieving the highest F1\nscore across all dataset sizes. These results are summarized in Table 4 and Table 5"}, {"title": "5.4.1 Device Benchmarking", "content": "The benchmarking results demonstrate the performance differences between a high-performance server and a mid-range\ndevice when handling various dataset sizes and regex complexities. The high-performance server maintains stable\nCPU usage, ranging from 26% to 30%, across all dataset sizes, ensuring efficient processing regardless of data volume.\nHowever, memory usage remains relatively stable for smaller datasets but increases significantly as the regex set\ncomplexity grows, particularly with 150 and 200 regex patterns.\n\nIn contrast, the mid-range device exhibits higher CPU usage (30% to 40%), primarily due to its limited computational\nresources. As dataset size and regex complexity increase, CPU utilization rises more steeply compared to the high-\nperformance server. Additionally, memory consumption on the mid-range device grows at a faster rate, making it more\nsensitive to the increasing number of regex patterns. The CPU usage for both devices under different dataset sizes and\nregex complexities is summarized in Table 6."}, {"title": "5.4.2 Memory usage", "content": "Memory usage varies depending on the regex set size and device type. The high-performance server maintains\nrelatively stable memory consumption, with usage ranging from 115 MB to 151 MB, even as dataset size and regex\ncomplexity increase. In contrast, the mid-range device exhibits significantly higher memory consumption, ranging\nfrom 145 MB to 190 MB, indicating greater sensitivity to larger regex sets and file sizes. As dataset size increases\nfrom 100MB to 5GB, memory usage grows across both devices, with the most noticeable increase occurring in the\n200-pattern regex set. A detailed breakdown of memory usage for different regex sets and devices is provided in\nTable7."}, {"title": "Conclusion", "content": "This study presents an optimized approach for detecting sensitive data using regex-based pattern matching and AI-\npowered techniques. Our benchmarking results demonstrate that Google RE2 offers the best trade-off between speed,\naccuracy, and memory efficiency, making it the preferred choice for scalable and high-performance regex processing.\nAdditionally, Aho-Corasick emerges as the optimal exact-match algorithm due to its superior speed and scalability\nacross large datasets. By integrating AI with regex, we significantly enhance recall while maintaining high precision,\nachieving the best F1-score (91.6%) across diverse data sizes."}]}