{"title": "MacLight: Multi-scene Aggregation Convolutional Learning for Traffic Signal Control", "authors": ["Sunbowen Lee", "Hongqin Lyu", "Yicheng Gong+", "Yingying Sun", "Chao Deng"], "abstract": "Reinforcement learning methods have proposed promising traffic signal control policy that can be trained on large road networks. Current SOTA methods model road networks as topological graph structures, incorporate graph attention into deep Q-learning, and merge local and global embeddings to improve policy. However, graph-based methods are difficult to parallelize, resulting in huge time overhead. Moreover, none of the current peer studies have deployed dynamic traffic systems for experiments, which is far from the actual situation.\nIn this context, we propose Multi-Scene Aggregation Convolutional Learning for traffic signal control (MacLight), which offers faster training speeds and more stable performance. Our approach consists of two main components. The first is the global representation, where we utilize variational autoencoders to compactly compress and extract the global representation. The second component employs the proximal policy optimization algorithm as the backbone, allowing value evaluation to consider both local features and global embedding representations. This backbone model significantly reduces time overhead and ensures stability in policy updates. We validated our method across multiple traffic scenarios under both static and dynamic traffic systems. Experimental results demonstrate that, compared to general and domian SOTA methods, our approach achieves superior stability, optimized convergence levels and the highest time efficiency. The code is under https://github.com/Aegis1863/MacLight.", "sections": [{"title": "1 INTRODUCTION", "content": "Traffic signal control (TSC) is an important issue in urban management. As the number of vehicles owned by residents increases, the deteriorating traffic conditions have a serious impact on social development. Traffic signal optimization is a low-cost means to alleviate traffic pressure.\nThe optimization of traffic light timing constitutes a complex nonlinear stochastic problem, as highlighted in [32]. Traditional intelligent control solutions often resort to assumptions or lack of flexibility, such as unlimited lane capacity [24], Christina Diakaki et al. [6] assumes that the traffic flow is uniform, or fail to adapt effectively to dynamic traffic flows [7]. Consequently, the performance may fall short of that achieved by a fixed timing plan meticulously crafted by human experts.\nAlthough mathematical modeling of real traffic systems is very difficult, the emergence of mature traffic simulators can provide interactive environments, which means that model-free methods can be applied. Reinforcement learning (RL) [21-23] provides SOTA solutions in the field of model-free control. Preliminary RL approaches, such as Q-learning [27] and its variants, have shown promising results in optimizing TSC. By iteratively learning from the environment, these algorithms can dynamically adjust signal to minimize congestion.\nIntelligent control methods for individual traffic lights are very mature [28], but they are inefficient for large road networks. Current research focuses on whether multiple traffic lights can effectively coordinate to achieve effects such as green wave roads. A common approach is to model the road network as a topology graph structure and introduce Graph Attention Networks (GAT) [25], enabling traffic signals to consider both local and neighboring features for comprehensive optimization decisions through feature aggregation. However, current GAT-based approaches [14, 15, 29] are almost used in Deep Q-learning (DQN) [17]. DQN, As an off-policy framework, despite being data-efficient, graph learning and batch learning consume significant time and computational resources. A more critical issue is they are prone to overfitting, leading to policy collapse.\nIn this case, we consider both local and global characteristics and propose a novel global scene aggregation approach. Our approach is motivated by two key points: firstly, the ability of decision-making and value evaluation of agents should to be separated. Thus, we utilize Proximal Policy Optimization (PPO) [19] as the backbone model, which has a value evaluation module and a policy improvement module to process different information respectively. Secondly, global scene aggregation does not necessarily require topological graph modeling. Research by Hua Wei et al. [29] indicates that in topological graph modeling scenarios, each agent considering only one-hop neighbors yields the best results, which limits the agent's understanding of broader states. Therefore, we aggregate the features of each agent (scene), using convolutional neural networks (CNN) [11] for a latent global representation. Another important reason for not using graph convolutional neural networks (GCNs) is that it is difficult to compute in parallel and apply to the more advanced Actor Critic RL framework. Consequently, our approach is called multi-scene aggregation convolutional learning (MacLight).\nFurthermore, we are the first construct dynamic traffic flow scenario by using the professional open-source simulator SUMO [13]. It can simulate the change of traffic flow distribution caused by emergency traffic incidents. We incorporate it as a challenging experimental scenario, alongside other general scenarios to test algorithms. Specifically, we can impose emergency speed limits or ban traffic on any road and reroute all vehicles. Vehicles will consider speed limits or prohibitions and choose new routes, leading to sudden changes in traffic distribution on other roads. It requires agents not only to cope with familiar traffic characteristics but also to have the ability to handle dynamically changing traffic flows. This greatly expands the scope of existing research.\nIn summary, the contributions of this paper are as follows:\n1. We construct a dynamic traffic flow simulation scheme to simulate any possible emergency traffic events, greatly expanding the current research space.\n2. We propose an online-trained variational autoencoder (VAE) based on CNN for global state representation, obtaining a compact and efficient representation from the latent space for downstream learning.\n3. We integrate global state representation into the value evaluation module of PPO, enabling the algorithm to balance local and macro characteristics, and demonstrating superior performance compared to both general and domain SOTA."}, {"title": "2 RELATED WORK", "content": "Customizable simulator. In the field of TSC, the Simulation of Urban Mobility (SUMO) simulator is widely used for urban planning and traffic flow simulation. The simulator allows researchers to define any desired traffic flow scenario. Ma and Wu [16] were"}, {"title": "3 NOTATION", "content": "We define the key concepts in RL for TSC before introducing our model, including the signal configuration and modeling TSC as a Partially Observable Markov Decision Process (POMDP).\nIntersection. Fig. 1 shows a general right-hand 2-way 6-lane intersection. We define the traffic light numbers starting from the north and proceeding clockwise. The left-turn lanes can only be used for left turns, while the right-turn lanes can be used for both going straight and turning right. Among these, traffic lights numbered 1, 5, 9, and 13 are for right-turn lanes and are default to green. However, when going straight in a right-turn lane, one must obey the green signal of going straight. Each time 3 or 4 green light signals are given to allow passing, a total of 8 signal combinations are defined in the scenario.\nPhase. Referring to the table on the right side of Fig. 1, we define an equitable signal configuration scheme that ensures no lane conflicts exist for any passing scenarios within a single cycle and each lane has two opportunities for passing within the cycle. This scheme is consistent with most real-world configurations, and the algorithm can adapt individually even if there are different configuration schemes.\nPOMDP Modeling for TSC. The traffic signal control problem is modeled as a POMDP. We consider each intersection as an independent agent that faces continuously changing traffic conditions and can only observe its own information completely, without grasping the global state. Another principle is that the next state is only affected by the current state and the current decision, and has nothing to do with the previous state. A POMDP can be described by a tuple $(S, O, A, P, R, \\pi, \\gamma, )$, and are introduced below.\nGlobal state space S & Partial state space O. The partial observation of agent i at time t is $o_t^i \\in O$, while global state $s_t \\in S$ and $o_t^i \\in s_t$. Partial observations are also called local observations in following context. Refer to [2], each local observation consists of four parts:\n1. The current action represented as a one-hot vector;\n2. A boolean value indicating whether the current signal allows switching. We specify that each action must remain in place for at least 10 seconds to meet real-world requirements;\n3. The vehicle density in each lane, calculated as the number of vehicles in the lane divided by the lane capacity;\n4. The density of waiting vehicles in each lane, calculated as the number of stopped vehicles divided by the lane capacity;\nThese components are encoded into a vector to represent the current state of each intersection.\nAction A. In the case of Fig. 1, the eight phases correspond to eight different action choices. At time t, the action of agent i is $a_t^i \\in A$. In the simulation, by default, we provide the corresponding yellow signal before switching the red signal.\nTransition probability P. Due to the Markov property, the probability transfer function is expressed as $P(s_{t+1}|s_t, a_t)$. The specific form of the function is unknown and is usually represented by reality or a simulator. We perform RL to capture the dynamic characteristics.\nReward R. Referring to the design of Alegre et al. [2], we first define the vehicle waiting time. At time t, the total waiting time of all vehicles stopped at intersection (agent) i is denoted as W. Then the reward of the agent is $r = -1 - W$. Our goal is to maximize the reward, which means that the agent should try to make the current waiting time shorter than the previous waiting time. The final reward is expected to converge to around 0, i.e., the system reaches a state of equilibrium. The advantage of considering waiting time as a reward is that the agent will not deliberately delay the release time of some lanes due to fewer cars there, but instead balanced take all vehicles into consideration.\nIn fact, there are many reward functions. We test all reward functions in \"ingolstadt21\" [3], and this scenario is completely different from ours. We adopt the same algorithm independent PPO (IPPO) for all experiments. In this case, we evaluate various indicators and determine that the aforementioned method is the best choice, with superior performance compared to other methods. The experimental results are shown in Table 1.\nPolicy \u03c0. The decision made by agent i in time t based on the current partial observation o is given by policy function $\\pi(a|o)$. The agent policy should maximize the total reward $\\Sigma_\\tau \\gamma r_\\tau $ where y is the discount factor, usually 0.98. This means that agents discount future reward, and care first about near-term reward."}, {"title": "4 METHODOLOGY", "content": "In this section, we will introduce the implementation of MacLight, including information aggregation, VAE feature compression, PPO, and method of dynamic traffic flow construction.\n4.1 Multi-scene aggregation matrix\nConsidering the geographical invariance of intersections, we organize the global information into a three-dimensional matrix. The global information is obtained by merging several local information, and each local information can be regarded as a scene. The local information of each intersection can be represented as a feature vector. Each vector is appropriately transposed and organized according to its location in geographic space, ultimately forming a high-dimensional global feature matrix as shown in the upper right of Fig. 2. The width and height of the matrix correspond to the geographical locations, and the number of channels is equal to the length of a single feature vector.\nClearly, the grid-based setting is the foundation for adopting CNN as the representation model. Although real-world road networks do not appear as regular as pixel grids, considering that most intersections are four-armed, the grid-like characteristics can still be observed when transforming them into a graph.\n4.2 Autoencoder\nFor feature extraction of three-dimensional matrices, we construct a VAE based on CNN. The structure refers to the VAE in Fig. 2. The encoder performs downsampling and finally outputs a compact compressed representation. The decoder restores the representation to the original matrix. Its training is carried out according to the method of Diederik P. Kingma and Max Welling [10].\nWe first give the process of upsampling, for a matrix x with a channel length of 33, the process is\n$h = Conv_{356}[ReLU(Conv_{28}(ReLU(Conv(x))))].$ (1)\nThe parameters of the Gaussian distribution represented in the latent space are calculated as\n$\\mu = W_\\mu h + b_\\mu,$\n$log \\sigma^2 = W_{logvar} h + b_{logvar},$ (2)\nwhere $W_\\mu, b_\\mu, W_{logvar}$ and $b_{logvar}$ correspond to weights and biases respectively. Then, an effective compact representation z is obtained by Gaussian distributions built on \u03bc and \u03c3:\n$z = \\mu + \\epsilon \\cdot \\sigma, \\ \\epsilon \\sim N(0, 1).$ (3)\nThe decoder uses transposed convolution models:\n$Z_{reshape} = Reshape(Linear(z)),$\n$\\hat{X}_{recon} = Sigmoid[ConvTrans_{33}[ReLU(ConvTrans_{34}( ReLU(ConvTrans_{328}(z_{reshape}))))]],$ (4)\nwhere we use sigmoid activation for output because the value range of the observation vector is between 0 and 1. Thus, The loss function is expressed as:\n$L_{vae} = L_{recon} + L_{kl},$ (5)\nwhere $L_{recon}$ and $L_{kl}$ are simply expressed as\n$L_{recon} = -log p(x|z),$\n$L_{kl} = - \\sum(1 + log \\sigma^2 - \\mu^2 - \\sigma^2).$ (6)\nIn short, the VAE can be trained online during the RL training process. Due to the efficient calculation of CNN on GPU, the overall algorithm can maintain its advantage in saving time. The global feature representation z will be concatenated with the local feature to be local-global aggregation representations sf and passed to the corresponding agent for PPO learning.\n4.3 PPO\nWe adopt the PPO algorithm with Generalized Advantage Estimation (GAE) trick as backbone model, refer to bottom right of Fig. 2. The core idea of PPO is to update the policy by maximizing a clipped objective function, which helps prevent large updates that could destabilize training.\nThe policy function for PPO can be expressed as:\n$L^{CLIP}(\\theta) = E_t[min(r_t(\\theta) \\hat{A}_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t)],$ (7)\nwhere $r_t(\\theta)$ is the probability ratio defined as $\\frac{\\pi_\\theta(a_t | o_t)}{\\pi_{\\theta_{old}}(a_t | o_t)}$. Here, $\\pi_\\theta$ denotes the policy parameterized by \u03b8, $a_t$ is the action taken, and $o_t$ is the local observation at time t. The term $\\hat{A}_t$ represents the estimated advantage, which quantifies how much better the taken action was compared to the expected action under the current policy.\nWe use GAE to compute the advantage estimate $\\hat{A}_t$. It considers not only the immediate reward but also the value of future states, allowing for a more accurate approximation of advantage. The advantage can be computed as follows:\n$\\hat{A}_t = \\sum_{i=0}^{\\infty} (\\gamma \\lambda)^i \\delta_{t+i},$ (8)\nwhere lambda is a discount factor to balance short-term and long-term advantages, and $\\delta_t$ is defined as:\n$\\delta_t = r_t + V_\\theta(s_{t+1}) - V_\\theta(s_t),$ (9)\nwhere $V_\\theta(s_t^f)$ represents the value function approximated by the neural network, $r_t$ is the immediate reward, $s_t^f$ is global-local aggregation representation introduced in the previous subsection, and \u03b3 is the discount factor that balances the importance of future rewards.\nIn addition, to updating the policy, the value function loss can be defined as:\n$L_V(\\theta) = E_t [(V_\\theta(s_t^f) - V_{target,t})^2],$ (10)\nwhere $V_{target,t}$ is typically the sum of the immediate reward and the discounted value of the next state:\n$V_{target,t} = r_t + \\gamma V_\\theta(s_{t+1}^f).$ (11)\nThrough this structured approach, PPO with GAE provides a robust mechanism for policy updates while maintaining stability in learning, allowing for effective exploration and improved sample efficiency in the task. MacLight pseudocode is summarized in Algorithm 1. Ultimately, the algorithm will try to maximize the total reward to achieve the overall goal.\n4.4 Dynamic traffic flow construction\nIn current TSC RL studies, the deployment of traffic flow is typically fixed, with all vehicles following predetermined routes. Our experiment, however, is the first to build a dynamic traffic flow environment that simulates emergency road events, which cause sudden changes in the distribution of traffic on other roads.\nAs illustrated in Fig. 3, when four central roads\u2014D3C3, D3D2, D2C2, and C3C2\u2014are blocked, all vehicles are asked to reroute. We conduct two experiments to analyze the effects. In Fig. 4, the gray curve shows the traffic flow distribution on a specific lane under normal conditions (without any interference), while the red curve shows the distribution after blocking a specific road. The lane blockage period is marked by a blue vertical dashed line. During this time,"}, {"title": "5 EXPERIMENTS", "content": "This section introduces the experimental environments, evaluation indicators, comparison algorithms, main experimental and ablation analysis. Table 3 can quickly check the experimental results.\n5.1 Environment and metrics\nEnvironment. In order to comprehensively evaluate algorithms, three different traffic scenarios are constructed on the same road network as shown in Fig. 3. The system is represented by a 4\u00d74 grid arranged horizontally and vertically, with a distance of 200 meters. The three scenarios are a normal-pressure scenario with regular traffic flow called Normal, a high-pressure scenario with extremely high traffic flow called Peak, and a dynamic traffic scenario with normal traffic flow but random emergency road blockage called Block. The randomly blocked lanes are indicated by yellow parts in Fig. 3. In terms of task difficulty, the minimum traffic pressure for our scenarios is much greater than all the other current studies, refer to Table 2. Benchmarking our experimental scenarios, when all three scenarios adopt a fixed phase switching time of every 45 seconds, the system simulation statistics are shown in Fig. 6. All simulations are performed on the SUMO [13] simulation platform.\nMetrics. In each scenario, we not only present the total reward results for all algorithms but also establish three objective metrics for comprehensive evaluation: the system's average waiting time, the queue length of waiting vehicles, and the average speed. These metrics take into account both temporal and spatial factors, enabling a more holistic assessment of the transportation system and preventing reward hacking [20]. The training seed range for all algorithms is set from 42 to 46, with details provided in the following subsection.\n5.2 Comparison methods\nMacLight will be compared with a variety of algorithms, including the traditional method of setting a fixed time switching phase and a variety of advanced algorithms based on RL. MacLight's model parameters refer to Appendix B.\nFixed time. Similar to the control method in reality, we configure the same fixed time switching method for all traffic lights: switching the phase every 45 seconds.\nIPPO. Refer to [5]. A separate agent with PPO algorithm is constructed for each intersection, and each agent only focuses on its own local information. IPPO can be regarded as the ablation object of MacLight.\nMAPPO. Refer to [31]. Similar to IPPO, but only one value evaluation network is used globally, whose input is the concatenation of local observations of all agents, while policy modules are as same as IPPO.\nIDQN. Similar to IPPO, but replaces the PPO with DQN. IDQN is the backbone model of CoLight and DuaLight.\nCoLight. Referring to [29], a strong algorithm for applying RL to TSC tasks using GAT, built on top of DQN.\nDuaLight. Reference [15], a SOTA based on CoLight, adds feature weight matrix and neighborhood weight matrix for different scenarios to the backbone network for Q learning, which shows better representation effect than CoLight. It is also based on DQN.\n5.3 Main results\nComparative experiments. Table 3 shows comprehensive comparison of experimental results. MacLight performs best in the Normal scene, with relatively good average performance and stability, followed by IPPO. In the high-pressure traffic environment Peak, the return and waiting time indicators are not as good as the Fixed method, because the indicators represent the average of the entire process, and if we check the final value, MacLight still has the best performance. In the dynamic traffic environment Block, indicators are inferior to IPPO. IDQN and the DQN-based CoLight and DuaLight methods perform poorly and are very prone to overfitting and policy collapse when faced with relatively sparse rewards and unstable data.\nTraining and testing. Fig. 7 shows the change of cumulative rewards during the entire training process, with the shadows indicating the maximum and minimum regions recorded for different seed experiments. On-policy approaches MacLight and IPPO, consistently demonstrates stable policy improvement across all scenarios. In contrast, off-policy methods such as IDQN and CoLight, while exhibiting robust initial performance, tend to collapse shortly thereafter. These methods are better suited for less challenging scenarios, leveraging the advantages of smaller models to avoid overfitting. However, they falter in high-difficulty, sparse-reward environments. For complete training diagrams of all indicators, refer to Appendix C. We show the test results of all algorithms in Table. 4, where the indicator is average return.\nAblation analysis. IPPO in Table 3 can be regarded as an ablation experiment of MacLight, because MacLight modifies the input of the value module from local features to local-global aggregate representation. On most indicators, MacLight shows advantages, while the second-best method is IPPO.\nTraining time on wall clock. Table 5 shows the training time of MacLight compared to other off-policy algorithms. We tested multiple random number seeds, each seed trained 80 episodes, and each episode contained 3600 seconds simulation. The times in the table are calculated as the average of the total length of 80 episodes on each seed. MacLight requires less than 1 hour to train, while off-policy algorithm IDQN needs at least 2 hours, Colight and DuaLight are even slower. This is because the GCN-based method cannot be massively parallelized, further slowing down the computational efficiency."}, {"title": "6 CONCLUSION", "content": "In this paper, we proposed the MacLight for TSC and construct both static and dynamic traffic flow for evaluation. The main contribution of MacLight is to construct a CNN-based VAE for global state feature extraction, and connect with the local state to form a local-global representation, which is used as the input of the value evaluation module to guide the policy improvement. MacLight uses the PPO algorithm as the backbone so that global and local information can be processed in parallel and improve each other. In addition, as an on-policy algorithm, MacLight provides high operating efficiency, taking only about one-third of the time of the off-policy method. Finally, the dynamic traffic simulation environment we constructed greatly expands the current research space and provides a basis for applying RL in emergency traffic scenarios.\nThere is still room for improvement in our work. Although CNN is more efficient than GCN-based methods, real road networks are usually not as regular as Manhattan roads and cannot be directly constructed as pixel matrices. We can use multiscale convolution to alleviate this problem in the future. In addition, there is room for improvement in the reward function, such as introducing both local and global indicators."}, {"title": "A DYNAMICAL TRAFFIC FLOW DISTRIBUTIONS", "content": "Referring to Fig. 1, we conducted two different simulations. The gray curve shows the traffic flow distribution in each lane without any interference to the traffic system, while the red curve shows the traffic flow distribution in each lane with blocking some roads. The blue titles are the numbers of the blocked lanes. The lane blocking time interval is marked with a blue vertical dashed line in the figure. There are cases with opposite signs such as A1B1 and B1A1, indicating two opposite directions of lanes on the same road."}, {"title": "B MODEL PARAMETERS", "content": ""}, {"title": "C COMPLETE INDICATORS TRAINING PROCESS", "content": ""}]}