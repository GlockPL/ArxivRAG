{"title": "STUDYING THE EFFECT OF AUDIO FILTERS IN PRE-TRAINED\nMODELS FOR ENVIRONMENTAL SOUND CLASSIFICATION", "authors": ["Aditya Dawn", "Wazib Ansar"], "abstract": "Environmental Sound Classification is an important problem of sound recognition and is more\ncomplicated than speech recognition problems as environmental sounds are not well structured\nwith respect to time and frequency. Researchers have used various CNN models to learn audio\nfeatures from different audio features like log mel spectrograms, gammatone spectral coefficients,\nmel-frequency spectral coefficients, generated from the audio files, over the past years. In this paper,\nwe propose a new methodology : Two-Level Classification; the Level 1 Classifier will be responsible\nto classify the audio signal into a broader class and the Level 2 Classifiers will be responsible to\nfind the actual class to which the audio belongs, based on the output of the Level 1 Classifier. We\nhave also shown the effects of different audio filters, among which a new method of Audio Crop is\nintroduced in this paper, which gave the highest accuracies in most of the cases. We have used the\nESC-50 dataset for our experiment and obtained a maximum accuracy of 78.75% in case of Level 1\nClassification and 98.04% in case of Level 2 Classifications.", "sections": [{"title": "1 Introduction", "content": "Environmental Sound Classification (ESC) has become a challenging job in recent times. The classification and\nidentification of environmental sounds, which include dog barking, birds chirping, knocking on the door, the sound\nof a vacuum cleaner, car horn, water drops and many other similar sounds, are necessary for developing smart-home\nappliances [1], security systems [2, 3], etc. to make human life more secure. For example, if mobile devices are able to\nrecognize the sound of car honking then road accidents and pedestrian accidents will decrease by a significant rate. Or,\nif the auto-pet care systems can identify the sounds of dogs or birds, they can provide the animals with food and water.\nOr if a smart security system can recognize door knocking sounds it will be able to notify the owner of the place about\nthe presence of a person behind the door. In other words, a well-featured and trained sound classifier will be able to\nrecognize the sound from human surroundings or environment and will be able to make human life safe and sound.\nTraditional Sound Processing is based on the sound features like log mel spectrograms with delta informations [4],\nGammatones [5] and Mel Frequency Cepstral Coefficients (MFCC) [6]. Various Machine Learning and Deep Learning\ntechniques are applied with these features to obtain high scoring results. Though the Machine Learning algorithms\nwere able to obtain good results, in recent years the breakthrough of Deep Neural Networks, mainly Convolutional\nNeural Networks has been significant. CNN's were used with the sound features to obtain accuracies like 83.9% [7],\n86.95% [8] to high as 97.57% [9].\nIn this paper, we propose a two-level classification method using CNN models of the classes VGG, ResNet and\nEfficientNet with audio modifiers like PCEN, Spectral Gating(Noise Removal), Audio Crop and Audio Filters like\nLow Pass Filter, High Pass Filter, Band Pass Filter and Band Stop Filter. The Level 1 classifier will classify the sounds\ninto broader groups of Animals, Birds, Nature, etc., while the Level 2 classification will pull the audio signal to the"}, {"title": "2 Related Works", "content": "In this section, we discuss the previous works by researchers on the ESC problem.\nPiczak extracted the log mel spectrograms for each frame of the audio files. Piczak used these log mel spectrograms\nand their delta informations in the CNN model proposed by him in [4]. He got an accuracy of 64.5% with this approach.\nThe goal of Piczak's paper was to evaluate the success of CNNs, when applied to ESC tasks.\nAgrawal et al. [5], used a TEO-based gammatone feature set for the problem. Firstly, they extracted the gammatone\nfilterbanks from the raw audio files and applied a bandpass filter. Then, they applied a half-wave rectifier on each of\nthe sub-bands and then Teager Energy Operator was applied again on each of the sub-bands. Finally, they applied\nshort-term averaging and short-term spectral features were obtained. The obtained spectrograms were given as input to\na CNN architecture similar to that used by Piczak [4] and obtained a score of 81.95%. However, they showed that the\nTEO-based Gammatone Spectral Coefficients failed to give better results with CNN.\nZhang et al. [10] showed the performance of Dilated Convolution Network with seven layers and two input channels.\nThey used log mel spectrograms and delta feature spectrograms in the proposed Dilated Convolution Network. They\nalso studied the classification accuracy obtained for ReLU-type activation functions. However, they got a classification\naccuracy of 68.1% on the ESC-50 dataset and it was also noted by them that the improvement of classification accuracy\ncost more of higher computational complexity and bigger storage.\nZhichao et al. [7] proposed a new CNN architecture inspired by VGG Net by using 1-D convolution filters in place of\n3 \u00d7 3 convolution filters, to learn local patterns across frequency and time. They extracted log mel spectrograms and\ngammatone spectrograms, which were used in the proposed CNN architecture, along with their delta information and\nachieved a classification accuracy of 83.9%.\nIn [8], Zhichao et al. adopted a convolutional RNN architecture for the problem. At first, they used CNN with channel\ntemporal attention mechanism in convolution layers, with log-gammatone spectrograms to extract high-level features\nfrom the spectrograms, which were further used in the bidirectional gated recurrent unit to analyse temporal correlations.\nThey showed a classification accuracy of 86.5% on the ESC-50 dataset.\nIn [11], Ullo et al. proposed a method that uses a hybrid structure made of OAS, STFT, CNN and different classification\ntechniques for the classification of the classes in the ESC-10 dataset and they achieved a classification accuracy of\n95.8%.\nMushtaq et al. [9] showed the performances of different data augmentations on the audio files and log mel spectrograms\nof the original audio files and augmented audio files with transfer learning and obtained an accuracy of 97.57% for\nESC-50 dataset. They also showed the performances of distinct pre-trained models, which included ResNet, DenseNet,\nAlexNet, SqueezeNet and VGG.\nAnsar et al. [12] proposed an EfficientNet ensemble with triple-layered approach to eliminate noise for classification of\naudio signals. They further validated a trade-off between model depth and number of parameters to obtain optimal\naccuracy through extensive evaluation on a bouquet of models.\nIt is clear from the previous works that, all the new methodologies and new models were built to classify the audio files\nto 50 classes directly. So in this study, a new approach is proposed, a two-level classification, with the spectrograms\nobtained after the application of audio modifiers and the pre-trained CNN models, which obtained a state of art accuracy\nscore compared to the works discussed above."}, {"title": "3 Methods", "content": "In this section, we discuss about the types of CNNs and audio modifiers used for our project."}, {"title": "3.1 Convolutional Neural Network (CNN)", "content": "CNNs are used in the application of image processing. CNNs can be trained well to understand the hidden features of\nthe images. This is because CNN applies different relevant filters and the architecture reduces the number of parameters\ninvolved and increases the re-usability of the weights. For this reason, the spatial and temporal dependencies of an\nimage are successfully captured by CNN. CNNs are mostly used for classification and computer vision tasks. Fig 11\nshows a general architecture of a CNN Model.\nCNN models consists mainly of three layers :\n1. Convolution Layer\n2. Pooling Layer\n3. Fully-connected Layer\nLeCun et al. [13] introduced the first CNN architecture, LeNet-5 for the recognition of handwritten digits (input images\nwere of dimension 32 \u00d7 32 \u00d7 1), using the MNIST dataset 2. LeNet-5 was a vary shallow CNN with alternating\nconvolution layers and pooling layers and had only about 60,000 parameters.\nAlexNet was then introduced by Krizhevsky et al. [14]. The network was similar to LeNet but instead of alternating\nconvolution layers and pooling layers, AlexNet had all the convolution layers stacked together. Also compared to\nLeNet-5, this network is much bigger and deeper.\nLater, VGGNet was introduced by Simonyan & Zisserman [15]. Earlier, models like AlexNet used high dimensional\nfilter in the initial layers, but VGG changed this by using 3 \u00d7 3 filters.\nHe et al. [16] have presented a residual learning framework where the layers learn residual functions with respect to\nthe inputs received instead of learning un-referenced functions. They were able to prove that this work is particularly\nuseful for training deeper networks since residual networks are easier to optimize and gain much accuracy. The main\ndrawback of this network is that it is much expensive to evaluate due to the huge number of parameters.\nVarious models were thus developed focusing either on performance or computational efficiency. Tan & V.Le introduced\nEfficientNet [17] model, which was able to solve both the problems. They proposed a common CNN architecture,\nwhich worked with three parameters width, depth and resolution. Width refers to the number of channels present in\nvarious layers, depth refers to the number of layers in the model and resolution refers to the input image size for the\nmodel. EfficientNet mainly helps in performing compound scaling with depth, width and resolution of the image.\nThe compound scaling method only enhances the predictive capacity of the networks by replicating base network's\nunderlying convolutional operations and network structure.\nIn this paper we have also shown the efficiency of 3 classes of pre-trained models, which are as follows :"}, {"title": "3.2 Spectrogram", "content": "A Spectrogram, usually depicted as a heatmap, is a visual representation, of a spectrum of frequencies of a signal as it\nvaries with time. Spectrograms of some audio files of each category of class based on Table 1 are shown in Fig 4. The\nspectrograms are passed into CNN Models to learn the audio features."}, {"title": "3.3 Understanding Audio Modifiers Used", "content": ""}, {"title": "3.3.1 Spectral Gating", "content": "Spectral Gating as explained by [18] is a technique, which is comprised of several steps. A Fourier transformation is\napplied on the noise-only portion of the audio signal to create a spectral \"fingerprint\", which is further used as a \"gate\"\nto filter the audio signal. The frequencies in the audio signals, which are above the gated value are passed, while those\nbelow the value are removed. Here, we have removed noise based on the principle of Spectral Gating. The result of\nSpectral Gating performed on an audio file is shown in Fig 3 with the audio form and spectrogram."}, {"title": "3.3.2 Per Channel Energy Normalization (PCEN)", "content": "[19] introduced PCEN as an alternative to the log-mel frontend. [20] discussed the working principle of PCEN. PCEN\nis the result of three-component operation :\n1. Temporal integration\n2. Adaptive Gain Control\n3. Dynamic Range Compression\nThe result of PCEN on two audio files is shown in Fig 5."}, {"title": "3.3.3 Audio Crop", "content": "The main idea behind introducing this feature in our work, is to repeat the non-zero portions of an audio sample over the\nmaximum time length of the audio files provided with the data. This method can be explained better with algorithms 1\nand 2. Algorithm 1 finds the maximum time length present among the audio files and algorithm 2 removes the silent\nportions of the audio files. The process of Audio Cropping is shown in Fig 6."}, {"title": "3.3.4 Audio Filters", "content": "Research and developments on Audio Filters have been done for audio modifications as mentioned in [21]. The filters\nthat we have used in our work modifies the audio signals based on frequencies.\n1. Low Pass Filter : Low Pass Filter allows the frequencies lower than a cut-off frequency to pass and attenuates\nthe frequencies higher than the cut-off frequency.\n2. High Pass Filter : High Pass Filter allows the frequencies higher than a cut-off frequency to pass and attenuates\nthe frequencies lower than the cut-off frequency."}, {"title": "4 Proposed Method", "content": "In this section, we discuss the design of the Two-Level Classification method, which we are proposing in this paper, as\nshown in Figure 2.\nStep 1 The audio files of ESC-50 dataset is taken as input.\nStep 2 In the pre-processing part, for our work, we have divided the dataset into 7 groups as shown in table 1. This\nnew divisions were made based on the origin or source of the sounds. For example, Dog, Sheep, Pig, etc are\nevidently animals. So, they are grouped in the \"Animal\" class. Rooster, Crow, Hen and Chirping Birds are\nplaced in the \"Bird\" class. Rain, Sea Waves, Wind, Pouring Water, Thunderstorms are observed in nature and\nso they are grouped in the \"Natural Soundscapes\" class. Sneezing, Clapping, Breathing, Drinking, etc are\nparts of human behavior. So, they are placed in the \"Human\" group. Mouse Click, Keyboard Typing, Washing\nMachine and Vacuum Cleaner are sound originating from some particular machines. So, they are grouped in\nthe \"Machine Sounds\" class. Door knock, Toilet flush, Clock alarm, Can opening, etc are found in domestic"}, {"title": "5 Implementation of the Method", "content": ""}, {"title": "5.1 Dataset: ESC-50", "content": "We have used the ESC-50 [22] dataset for our work, which is a collection of 2000 recordings with an average duration\nof 5 seconds and a sampling frequency rate of 44100 Hz. These recordings have been collected from Freesound.org 3.\nThe dataset consists of recordings of 40 audio files for each of the 50 categories."}, {"title": "5.2 Libraries", "content": "The work has been done using the Python Programming Language. The Python libraries used for this project are\ndiscussed below."}, {"title": "5.2.1 NumPy", "content": "The NumPy library 4 helps in performing complex mathematical operations with arrays and random number generations.\nIn this project we have used NumPy for random split of the dataset to make Training and Testing Sets to train and test\non the CNN Models, respectively."}, {"title": "5.2.2 Pandas", "content": "The Pandas library 5 helped in manipulating the dataframe and also performing some basic operations on the dataframe."}, {"title": "5.2.3 Matplotlib and Seaborn", "content": "Matplotlib and Seaborn 7 were used for plottings."}, {"title": "5.2.4 Librosa", "content": "Librosa 8 was used to work with the audio files. Librosa helped in extracting the audio files from their respective\nlocations. Librosa also has functions to extract mel spectrograms and functions for audio modifications which were\nused on the audio samples."}, {"title": "5.2.5 SciPy", "content": "SciPy is a library which has the functions to implement the audio filters on the audio samples provided in the data."}, {"title": "5.2.6 Noise Reduce", "content": "The noisereduce 10 is used for Noise Removal from the audio files provided with the dataset."}, {"title": "5.2.7 Tensorflow and Keras", "content": "Tensorflowll and Keras12 were used to implement the CNN models. The pre-trained models from Keras were used\nin this work."}, {"title": "5.2.8 Model Hyperparameters", "content": "After the pre-trained layers of the pre-trained models, we added a layer with global average pooling method. After\nthe global average layer, two dense layers were added each with 512 filters and activation function ReLU. The kernel\ninitializer of the first dense layer was set to glorot uniform. Stochastic gradient descent as the optimizer function during\ncompilation of the models.\nFig 4 shows that the intensity of sound is maximum within the range of 0 \u2013 512 Hz, while it decreases slightly to 2048\nHz and starts to fade after that in the cases of thunderstorm, vacuum cleaner, glass breaking, train sounds. Spectrograms\nof chirping birds, clapping show that the intensity is faded till 128 Hz. Again, in the cases of clapping and dog maximum\nintensity is visible mostly within 512 \u2013 4096 Hz. Besides this, the black portion indicates silence in the audio files of\ndog and glass breaking. Based o these observations, in case of the Audio Filters, we have used 512 Hz as the lower\nthreshold and 2048 Hz as the higher threshold frequencies."}, {"title": "5.3 Implementation Details", "content": "For our convenience, for each CNN Model, we first divided the data for the respective model in the ratio of 8:2 to create\nTraining Set and the Testing Set. Then we divide the Training set again in the ratio of 8:2, as shown in table 3. Also, we\nhave used a sampling rate of 44.1 KHz for the audio files."}, {"title": "6 Results", "content": "From table 3, it is clear that the number of testing samples is less except the case of Level 1 Classification. We are going\nto compare the performances of the classifiers and the models based on Classification Accuracy for Level 1 classifiers\n(table 4) only, but in the other cases we are going to judge based on Highest Validation Accuracy obtained as a single\nmiss-classification by the model will decrease the Classification Accuracy significantly, specifically in the cases of\nBirds and Machine Sounds classification, though we have provided both the Classification Accuracy and Validation\nAccuracy in the result tables of the classification models for Animals (table 5), Birds (table 6), Natural Soundscapes\n(table 7), Human (table 8), Machine Sounds (table 9),. Domestic (tal\nThese raw spectrograms shown in Fig 4\nif (table 10) and Outdoor noises (table\nIn Fig 4 are given as input to the N models examine mine their performances on the\naudio files, which are shown in the column \"No Filter\" in the tables showing the results of the classifications performed.\nThe spectrograms obtained with the different audio modifiers like Spectral Gating, PCEN and Audio Crop are shown in\nFig 3, 5 and 6, respectively. The CNN models when combined with these audio filters, use these generated spectrograms\nas input and the results are shown in the columns \"Spectral Gating\", \"PCEN\" and \"Audio Crop\" in the tables showing\nthe results of the classifications performed.\nBased on the observations obtained from the raw spectrograms, we have fixed the lower threshold and higher threshold\nfrequencies to 512 and 2048 Hz, respectively. The obtained spectrograms after using Low Pass Filter (threshold = 512\nHz), High Pass Filter (threshold = 2048 Hz), Band Pass Filter (lower threshold = 512 Hz and higher threshold = 2048\nHz) and Band Stop Filter (lower threshold = 512 Hz and higher threshold = 2048 Hz) are passed as input to the CNN\nmodels and the obtained results are shown in the columns Low Pass Filter, High Pass Filter, Band Pass Filter and Band\nStop Filter in the tables showing the results of the classifications performed."}, {"title": "6.1 Level 1 Classification", "content": "Now, from the results of the Level 1 Classification as shown in table 4 it can be seen that, Classification Accuracy\nwas 75.94% from the CNN model, EfficientNetB1 without any filtration; it increased to 74.75% from CNN model,\nEfficientNetB0 with Noise Removal, then it decreased to 50.50% with the application of PCEN; it further increased\nto 78.75% with the combination of EfficientNetB2 and Audio Crop, which is the maximum Classification Accuracy\nobtained in the Level 1 Classification. But then again it decreased to 48.50% with the application of Low Pass Filter,\nincreased to 60.25% with High Pass Filter. Before giving the final accuracy as 36.50% with Band Stop Filter, it showed\n55.25% in the case of Band Pass Filter."}, {"title": "6.2 Level 2 Classification", "content": ""}, {"title": "6.2.1 Animal", "content": "In case of Level 2 Classification of the Animal class as shown in table 5, the validation score started from 86.27% from\nResNet50 and ResNet152 with the raw spectrograms of the unfiltered audio files. The validation score then started\nto decrease to 64.71% with PCEN, after giving the accuarcy as 82.35% with the combination of Noise Removal and\nEfficientNetB3. But, again increased to 88.24% with Audio Crop and EfficientNetB2."}, {"title": "6.2.2 Bird", "content": "The results of Level 2 Classification of the Bird class from the table 6 show that here the highest validation accuracy\nwas obtained as 96.00% with the following combinations of audio modifier & CNN model.\n1. No Filter & VGG16\n2. No Filter & EfficientNetB2\n3. Noise Removal & EfficientNetB1\n4. Audio Crop & ResNet152\n5. Audio Crop & EfficientNetB1\n6. High Pass Filter & ResNet50\nThe accuracy was also obtained as 80.00% with Low Pass Filter and Band Pass Filter. The least accuracy score was\nobtained as 64.00% with Band Stop Filter."}, {"title": "6.2.3 Natural Soundscapes", "content": "Finally, from the results table 7 of the Level 2 Classifier of Nature class it is clear that, the highest validation accuracy\nobtained is 95.45% from the application combinations of No Filter & ResNet152, Noise Removal & ResNet152 and\nNoise Removal & EfficientNetB1. Here, the accuracy score was also obtained as high as 90.91% with Band Pass Filter\nand also 86.36% with the applications of Audio Crop and PCEN. But, in this case the minimum highest validation\naccuracy was obtained from the application of Low Pass Filter as 54.55%."}, {"title": "6.2.4 Human", "content": "93.75% is the highest validation accuracy obtained by the Level 2 Classifier of the Human class as shown in the results\nof table 8, with the combiation of Audio Crop and the CNN model EfficientNetB0. It also got validation score of\n87.50% with Noise Removal. But, like the Level 2 Classifiers of the previous classes it got minimum highest validation\naccuracy as 51.56% with the application of Band Stop Filter."}, {"title": "6.2.5 Machine Sounds", "content": "The classification results from the table 9 of the Level 2 Classification for Machine Sounds show that, the classifier got\nhighest validation accuracy as 92.00% with the following combinations of audio modifiers & CNN models as follows.\n1. No Filter & VGG16\n2. No Filter & ResNet152\n3. No Filter & EfficientNetB3\n4. Audio Crop & VGG19\n5. Audio Crop & ResNet101\n6. Audio Crop & ResNet152\n7. Audio Crop & EfficientNetB3\nThe minimum accuracy score obtained was 64.00% with Band Stop Filter."}, {"title": "6.2.6 Domestic", "content": "The results from table 10 of the Level 2 Classification of the Domestic class show that the highest validation accuracy as\n98.04% with Noise Removal and EfficientNetB1. It also showed the score of 96.08% without application of modifiers\nand with Audio Crop. But, similar to the previous results of the Level 2 Classification, the lowest score was obtained as\n56.86% with Band Stop Filter."}, {"title": "6.2.7 Outdoor", "content": "The Level 2 Classification of the Outdoor class got the highest validation accuracy as 93.75% with the combination of\nAudio Crop and EfficientNetB3 as can be seen from table 8. But it also showed the accuracy score high as 89.00%\nand 85.94% with the application of Noise Removal and from the spectrograms of the raw audio files with VGG16 and\nResNet152, respectively. The lowest validation accuracy obtaned in this case is 50.00% with Band Stop Filter.\nNow, comparing with the scores of previous works as shown in table 12, we have achieved quite a challenging accuracy\nscores compared to the previous works."}, {"title": "7 Discussion", "content": "In this section, we are going to discuss the results obtained and shown in Section 6. As from table 4 it can be seen that\nthe Level 1 Classification got its highest Classification Accuracy, 78.75% from the CNN Model EfficientNetB2 with\nAudio Crop.\nNow in cases of the other classifications, the best results have been shown in the table 13. Table 14b and Fig 7b shows\nthe number of times each CNN class achieved the highest validation scores and it is quite clear from table 14b and\nFig 7b that, EfficientNet worked best in most of the cases. Though it has got Highest Validation Accuracy in 10 cases\nbut, if we examine the tables 4, 5, 6, 7, 8, 9, 10 and 11 in Section 6, it will be clear that, in most of the cases the audio\nmodifiers have got their highest Classification Accuracies, in case of Level 1 Classification and Highest Validation\nAccuracies in case of the other classifications with a CNN model belonging to the EfficientNet."}, {"title": "8 Conclusion", "content": "The main objective of this paper is to propose a Two-Level Sound Classification method for the Environmental Sound\nClassification problem. Experiments on the ESC-50 dataset show that the classification accuracy of the Level 1\nClassification was obtained as high as 78.75%, while the highest validation score obtained by the Level 2 Classification\nis 98.04%. In addition, this paper also shows the efficiencies of different CNN models combined with different audio\nmodifiers and discussed their impact on the audio files. In future, we plan to optimize the hyperparameters more\naccurately and examine the performances of other threshold frequencies with the audio filters, though we have obtained\nthe highest accuracy with audio crop and further improve the performance of the proposed methodology. We hope our\nmethod and process of thinking will encourage future researchers to implement them in their research."}]}