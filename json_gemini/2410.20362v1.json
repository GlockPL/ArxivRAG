{"title": "Rethinking Data Synthesis: A Teacher Model Training Recipe with Interpretation", "authors": ["Yifang Chen", "David Zhu"], "abstract": "Recent advances in large language model (LLM) training have highlighted the need for diverse, high-quality instruction data. Recently, many works are exploring synthetic data generation using LLMs. However, they primarily focus on prompt engineering with standard supervised instruction-finetuned models, which contains a fundamental limitation: these models are optimized for general question-answering/problem-solving rather than data generation. We propose a paradigm shift named NOMAD by investigating how to specifically train models for data generation, demonstrating that this task differs significantly from training a classical LM. We identify two key factors: no-prompt-masked training and proper training set size selection. Our method, NOMAD, shows substantial improvements over baselines, achieving >4% gains in TriviaQA and >2% in GSM8K with limited training data. Finally, we offer new insights by interpreting synthetic data through the lenses of \"relevance\" and \"novelty\".", "sections": [{"title": "Introduction", "content": "Instruction design, exemplified by OpenAI's approach with real-world user data (Ouyang et al., 2022), has become a key data curation technique in LLM post-training. However, the traditional approach of collecting human-generated instructions faces substantial limitations due to labor costs.\nRecent approaches have explored synthetic data generation using powerful teacher LLM models, primarily focusing on prompt-engineering methodologies (Taori et al., 2023; Honovich et al., 2023; Xu et al., 2023; Wang et al., 2023; Lee et al., 2023; Xu et al., 2024). They usually begin with a small seed pool of example tasks, gradually generating, filtering and refining new prompts. However, these approaches typically rely on standard instruction-masked supervised fine-tuning (SFT) models designed for general question-answering. Therefore, we argue that current models have key limitations: they prioritize solving problems accurately over generating novel ones, lack question-generation-specific design, and can generate contextually in-complete questions in chat formats. This motivates our core investigation: Should we train a specialized model specifically for data synthesis instead of the current post-training recipe, and if so, how?\nThis paper addresses this question by investigating two critical aspects that differentiate data synthesis from standard language model training:\n1. The Role of Prompt Masking: We address a tiny yet long-ignored question in standard SFT: the impact of prompt masking. While traditional approaches mask prompts to improve response quality, we demonstrate that learning from prompts is crucial for generating better synthetic data.\n2. Training Data Optimization: We explore the counterintuitive finding that larger training sets don't always yield better results. Our research shows that carefully selecting a smaller subset of training data often produces more effective supplementary synthetic data.\nBuilding on these insights, we propose NOMAD (No Masking Data Synthesizer), a novel approach that specifically addresses these challenges. In particular, when only small size train samples are available, synthetic data generated by NOMAD outperforms baselines (i.e., using train set only) by 1.5% on average, with >4% gains in TriviaQA and >2% in GSM8K. With larger size train samples, such advantages persist since this is the only one that can outperform the baseline even the synthesis data is only 5% of original train data.\nMoreover, to give a deeper interpretation behind these two factors, we propose to evaluate the synthetic data quality through the dual lenses of \"relevance\" and \"novelty,\" providing insights into optimization."}, {"title": "Problem Statement", "content": "Given a pretrained student model $M_s$, a pretrained teacher model $M_t$, and an existing high-quality instruction dataset $X_{train}$, our goal is to generate additional synthetic data $X_{synthesis}$, comprising new prompts and responses, from a data generation model training perspective. Specifically, in this paper, we aim to propose novel methods to train $M_t$ using $X_{train}$ to generate supplementary $X_{synthesis}$.\nTo measure the effectiveness of our proposed methods, we train $M_s$ on a mixture of the original $X_{train}$ and the newly generated $X_{synthesis}$, and compare its performance with an $M_s$ trained solely on the original $X_{train}$.\nNote that previous works have primarily focused on designing various prompting methods to query an already instruction-fine-tuned teacher model. Those approaches implicitly leverage the external data used to train such a teacher model. In contrast, our work assumes access only to the pretrained version of the teacher model, ensuring rigorous control over the instruction data used."}, {"title": "Our strategy", "content": "Our main strategy is shown in Fig. 1, which can be divided into $M_{synthesis}$ training, $X_{synthesis}$ generation and filtering stages, as detailed below.\n$M_{synthesis} Training$ we've identified two critical factors that significantly differentiate this process from standard language model training\n\u2022 No-Prompt-Masked Training: Traditional instruction fine-tuning focuses on improving response quality by computing loss only on the response part. However, with the advent of powerful language models, generating high-quality responses has become relatively straightforward. The real challenge lies in creating diverse and helpful prompts. Our no-prompt-masked training addresses this by exposing the model to complete instruction-response pairs. This approach offers several advantages: This enables the model to learn the characteristics of high-quality prompts and ensures that generated prompts align with the $X_{train}$ domain and style, avoiding the pitfall of mixing disparate $X_{train}$ and $X_{synthesis}$ in final model training. Therefore, to improve the \"relevance\" as defined later in Section 4.3. As a side product, it also allows for simultaneous generation of both prompts and responses, eliminating the need for separate generation steps as seen in previous works like Xu et al. (2024).\n\u2022 Proper (Usually Smaller) Training Set Size: While we aim to avoid mixing significantly different datasets, which can challenge the model's capacity, we also want to prevent the synthetic data from being too similar to the original, as this would limit its supplementary value. To strike a balance between relevance and novelty as discussed detailedly Section 4.3, we discover that selecting a subset of a large available dataset often yields superior supplementary synthetic data. This finding challenges the conventional wisdom of using as much data as possible.\n$X_{synthesis} Generation$ To isolate the effects of data generation from prompt engineering, we adopt the prompting strategy proposed in Xu et al. (2024). Specifically, we input only \"User: \", which is the standard beginning of all our instruction data, allowing the model to generate both the prompt and response autonomously. Then we post-process the data by retaining only the first-round conversation and discard any data that fails to generate a complete conversion. It's important to note that our method is potentially compatible with existing prompt-engineering based approaches, offering opportunities for future integration and enhancement.\nSimple Filters To address two common issues in synthetic data generation: content quality decay with increasing sentence length and poor performance in generating coding-type data. To tackle these, we implement a repeated words removal filter using pattern matching and a coding filter using keyword searches. Importantly, these filtering processes are computationally inexpensive, requiring negligible time while significantly improving performance. We postpone the details of filters to Appendix A.5."}, {"title": "Experiment", "content": "4.1 Setup\nModels We choose Llama3-8B (Dubey et al., 2024) as the backbone of the teacher model $M_{synthesis}$ and Phi-mini-v3.1 (Abdin et al., 2024) as the backbone of the student model $M_s$.\nTraining Data As discussed in Section 3, existing training data or its subset can be used in both"}, {"title": "Main Result", "content": "Results with Small $X_{train}$ In Table 1, by using just 15K samples for both the $M_{synthesis}$ and the student model $M_s$, our NOMASKEDFILTERED method outperforms the baseline average by approximately 1.5% when supplementing the original training data $X_{train}$. Notable improvements include > 4% gain in TriviaQA and> 2% in GSM8K. In contrast, $X_{synthesis}$ from prompt-masked training, regardless of filtering, degrades performance when combined with the original dataset, highlighting the critical importance of no-prompt-masked training for $M_{synthesis}$\nResults with Large $X_{train}$ Previous result, however, assumes the available train data size is already small and therefore it's hard to distinguish whether the small size requirement is necessary during the $M_{synthesis}$ training or the $M_s$. To further illustrate this, we consider a much larger 300K $X_{train}$ but may not use the whole set when training $M_{synthesis}$. Under this setting, we surprisingly show in Table 2 that, using all 300k data to train $M_{synthesis}$ actually downgrades the performance of baseline no matter what training method we use. On the other hand, data generated from 15K no-prompt-masked trained $M_{synthesis}$ is the only one that outperforms baseline."}, {"title": "Property of the synthetic data", "content": "Definition of dataset similarity To understand the relationship between $X_{synthesis}$ and the original 300K TULU dataset $X_{TULU}$, we introduce a similarity score called NormSim, initially proposed by (Wang et al., 2024). For each generated synthetic data point x, we define:\n$NormSim(x) = max_{z \\in X_{TULU}} (f(z) \\cdot f(x))$\nwhere f is the all-mpnet-base-v2 (Henderson et al., 2019) used to extract embeddings. Instead of checking whether the generated data has the same coverage as TULU (demonstrated in App. B), our measurement considers x to have high similarity if it is similar to any target sample.\nRelevance v.s. Novelty Intuitively, similarity close to 1 suggests repetition of existing TULU data, while one close to 0 indicates a potential poisoning to the current distribution. Ideally, we want more data to be concentrated around themedian similarity, balancing novelty and relevance. This intuition aligns with our observation in Fig. 2 and Table. 2 where $X_{synthesis}$ with more median similarity yeid best performance. Prompt-masked training can lead to low relevance due to lack of exposure to prompts (see App.B.1 for details), while large $X_{syn}$ can result in low novelty due to overfitting to $X_{train}$.\nFinally, both relevance and novelty require using $X_{train}$ as a reference, but is this necessary? We provide an affirmative answer by demonstrating that the performance resulting from training on $X_{synthesis}$ alone does not correlate with training on a mixture of $X_{synthesis} + X_{train}$ (see App.B.2)."}, {"title": "Detailed Experiment Setting", "content": "A.1 Model training\nFor all model training, we choose learning rate = 2e-5 and batch size = 128.\nA.2 Data generation\nWe use the prompt strategy as explained in Section 3 with generation temperate=1 and choose top_p = 0.9 when $X_{syn}$ is 15K since smaller top_p can generate low quality data. When $X_{syn}$ is 300K, we tried both top_p=0.9 and 0.7, as shown in appendix C.1, while different hyperparameters lead to slightly different performance, they does not contradict the main conclusion of this paper.\nA.3 Details on evaluation metrics\nA.3.1 Generation-free evaluation metrics\nTriviaQA TriviaQA is a reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. This metric can be used to test the model's retrieval ability when a retrieval module is added. When being used alone here, this exam the models knowledge capacity.\nTruthfulQA_gen QA dataset where the model generates a 1-2 sentence answer for each question. This answer is evaluated against a true and false reference answer. The final metric is the [similarity to true reference answer] - [similarity to false reference answer] with RougeL. This dataset test the truthfulness metric, which is close to the knowledge metric, but allows the model to response with absence.\nBBH A suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH) to test models reasoning ability. These are the task for which prior language model evaluations did not outperform the average human-rater. Here we use both the chain-of-though and non-chain-of-thought version with 3 shot examples.\nGSM8k (Cobbe et al., 2021) : A benchmark of grade school math problems aiming for evaluating multi-step (2-8 steps) mathematical reasoning capabilities. These problems are illustrated by natural language and require using four basic arithmetic operations to reach the final answer.\nIFEval One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of \"verifiable instructions\" such as \"write in more than 400 words\" and \"mention the keyword of AI at least 3 times\". Here report the prompt-level loose accuracy.\nA.4 Problem of IFEval\nWhen we choose Xtrain as 300K TULU, we find out the baseline (i.e. instruction finetuning on whole 300K TULU) give 34.38 accuracy, which is even smaller than the baseline with Xtrain =15K TULU. So we think maybe the original data itself is less effective on such instruction following, and therefore can confuse our methodology study.\nA.5 Filters\nAs we mentioned in Section 3, our rule-based filters contains two parts: code removing and repeated words removing, here are some details.\nA.5.1 Coding Samples\nDespite the effectiveness of our data synthesis methods on general tasks, we find it struggles on generating a high-quality coding samples. Specifically, coding samples frequently suffer from:\n\u2022 Lack necessary context to complete problem\n\u2022 Incorrect outputs due to problem difficulty\nThe sample generated prompt below is one such example where there is no context given for the problem."}, {"title": "More interpretations", "content": "B.1 OOD in prompt-masked training\nData generated from prompt-masked training can have very different distributions than original data, in the following we list two typical prompt-response phenomenon that only occurs in prompt-masked training with 15K TULU.\nRole switch between user and assistant Data generated from prompt-masked training has its user behave like an assistant, and the assistant may try to continue the conversation or give comments on the \"response\" from the user as shown in the following examples."}, {"title": "Quality of   alone is not an effective metric", "content": "Intuitively, it is easy to regard such OOD data as low-quality. However, in Table 3, we show that such a dataset alone can still be helpful and even"}, {"title": "More results on multi-choice metrics", "content": "In Section 4.2, we have shown the advantage of our methods on free-generation metrics. Nevertheless, we find that the proposed synthetic data generation methodology is less effective in multi-choice metrics.\nC.1 Details on evaluation metrics\nIn multi-choice metrics, the learner are given a fixed set of candidates (e.g. A,B,C,D) and choose the result with maximum digits among those candidates. Here we consider the following metrics:\nMMLU (Henderson et al., 2019; Hendrycks et al., 2021) (Knowledge) It evaluates models across 57 diverse subjects, ranging from STEM fields to humanities and social sciences. This comprehensive test requires broad knowledge spanning elementary to professional-level expertise. Each task consists of multiple-choice questions, making it a robust measure of a model's acquired knowledge..\nARC Challenge (Clark et al., 2018) (Knowledge+reasoning) It specifically focuses on grade-school science questions. The Challenge Set contains questions that cannot be answered by simple retrieval or word association methods, requiring both scientific knowledge and complex reasoning abilities. Questions often involve multi-step logical inference, causal reasoning, and the application of scientific principles to novel scenarios.\nhellaswag (Zellers et al., 2019) (Knowledge+reasoning) It is a challenging commonsense reasoning benchmark that consists of multiple-choice questions where systems must complete a sentence or short paragraph with the most contextually appropriate ending from four options.\nWinogrande (Sakaguchi et al., 2019) (Knowledge+reasoning) Winogrande is an evolved version of the Winograd Schema Challenge, designed to test common sense reasoning through pronoun resolution tasks. The dataset consists of sentences with ambiguous pronouns that can only be correctly resolved through understanding of context and real-world knowledge. What sets Winogrande apart is its carefully curated adversarial examples that minimize dataset artifacts, making it a more robust test of genuine reasoning capabilities. The questions require both implicit knowledge about how the world works and the ability to apply this knowledge in context-dependent ways.\nTruthfulQA_mc2 (Lin et al., 2022) (Truthfulness) It is a specialized benchmark designed to evaluate a model's tendency to generate truthful versus false or misleading information. We have used its free-generation version in our main result. Here we instead use the multiple-choice version (mc2).\nAGIEval (Zhong et al., 2023) (Instruct-follow) AGIEval is a comprehensive benchmark designed to assess instruction-following capabilities and general intelligence in language models. It incorporates a diverse set of tasks that mirror real-world cognitive challenges, including professional certification questions, academic tests, and complex problem-solving scenarios. The benchmark is structured to evaluate not just the model's ability to understand instructions but also its capacity to apply knowledge in context-appropriate ways.\nC.2 Results\nAs shown in Table 3, in contrast to the significant improvements observed in free-generation metrics under 15K TULU, neither synthetic method demonstrates notable performance gains over the baseline."}, {"title": "More results on 300K parameters", "content": "We present the comprehensive results in Table 4 using  =300K TULU, including experiments with generation parameter top_p = 0.7. Note that we excluded the top_p = 0.7 configuration under the  =15K TULU setting due to its inability to generate coherent sentences. The results demonstrate that all synthetic data generated using  =300K TULU underperforms compared to the Baseline, with no significant variations across different top_p values. This observation reinforces our hypothesis that utilizing the full 300K dataset for  generation yields outputs that closely mirror the original TULU distribution, regardless of other parameter choices."}]}