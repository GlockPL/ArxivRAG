{"title": "EgoSim:\nAn Egocentric Multi-view Simulator and Real Dataset\nfor Body-worn Cameras during Motion and Activity", "authors": ["Dominik Hollidt", "Paul Streli", "Jiaxi Jiang", "Yasaman Haghighi", "Changlin Qian", "Xintong Liu", "Christian Holz"], "abstract": "Research on egocentric tasks in computer vision has mostly focused on head-\nmounted cameras, such as fisheye cameras or embedded cameras inside immersive\nheadsets. We argue that the increasing miniaturization of optical sensors will lead\nto the prolific integration of cameras into many more body-worn devices at various\nlocations. This will bring fresh perspectives to established tasks in computer vision\nand benefit key areas such as human motion tracking, body pose estimation, or\naction recognition-particularly for the lower body, which is typically occluded.\nIn this paper, we introduce EgoSim, a novel simulator of body-worn cameras\nthat generates realistic egocentric renderings from multiple perspectives across\na wearer's body. A key feature of EgoSim is its use of real motion capture data\nto render motion artifacts, which are especially noticeable with arm- or leg-worn\ncameras. In addition, we introduce MultiEgoView, a dataset of egocentric footage\nfrom six body-worn cameras and ground-truth full-body 3D poses during several\nactivities: 119 hours of data are derived from AMASS motion sequences in four\nhigh-fidelity virtual environments, which we augment with 5 hours of real-world\nmotion data from 13 participants using six GoPro cameras and 3D body pose\nreferences from an Xsens motion capture suit.\nWe demonstrate EgoSim's effectiveness by training an end-to-end video-only 3D\npose estimation network. Analyzing its domain gap, we show that our dataset and\nsimulator substantially aid training for inference on real-world data.", "sections": [{"title": "1 Introduction", "content": "The newest generation of AI-based personal devices evidently requires an understanding of the world\nfrom a user's perspective to provide meaningful context. For example, Meta's Ray-Ban glasses [1],\nHu.ma.ne AI pin [2], or the glasses demoed at Google I/O 2024 all share the wearer's perspective to\nanalyze their surroundings. Such emerging devices in addition to existing immersive Mixed Reality\nplatforms have further spurred research efforts on egocentric perception tasks [3,4].\nWhile head-worn cameras have primarily been used for localization [5, 6], they are ideally positioned\nto simultaneously capture the wearer's arm motions, for example, to estimate upper body poses [7-9]\nor detect user input from hand poses and actions [10, 11]. For egocentric pose estimation, previous\nwork has commonly used head-mounted fisheye cameras pointing down [12-14], which can capture\nmuch of the upper body. This promise has spurred interest in egocentric pose estimation, for which"}, {"title": "2 Related Work", "content": "Synthetic datasets and simulators. The advancement of deep learning in recent years has necessi-\ntated larger and more varied datasets that can be acquired using simulated data. Visual synthetic data\nproved its benefits in many fields such as human mesh recovery [26], visual-inertial odometry [57],\nvisual SLAM [58, 59], and human pose estimation [26, 60]. Microsoft AirSim [61] stands out as\none of the most effective simulators. It has facilitated the creation of photo-realistic datasets such as\nTartanAir [20], optimized for Visual SLAM tasks, and Mid-air [62], designed for low-altitude drone\nflights. So far, AirSim [61] and other simulators [63] fall short in tasks centered on human dynamics,\nsuch as 3D human pose estimation or multi-actor interactions. Only recently, the Habitat 3 [64] simu-\nlator targets human-robot interaction tasks and progresses in this area but offers limited configuration\nfor sensor placement and environmental diversity. EgoGen as a novel human-centered simulator\ndemonstrates promise by focusing on human motion synthesis [19]. Traditionally, datasets simulate\ncameras either statically or with smooth movements. Such datasets fail to generalize to egocentric\nscenarios where the camera's position dynamically changes in relation to the wearer's movements.\nEgoSim advances this field by being specifically designed for human-centric research with wearable\ncameras that follow the natural non-smooth movements within the human body. It uniquely supports\ncomplex multi-character interactions in varied environments, both indoor and outdoor, enabling more\ncomprehensive and diverse studies in this field.\nHuman motion datasets. In controlled settings, multiple third-person view cameras and motion\ncapture equipment offer accurate ground-truth data [47,65-69]. Fitting 3D body models [70\u201372] to\npoint cloud marker sets [47] or using RGBD camera data can provide ground-truth poses. However,\nthe complexity of these setups mostly limits their scalability to indoor environments [3, 73, 74].\nPseudo-ground truth pose annotations can overcome these limitations for outdoor environments.\nSeveral methods use 2D keypoints [75\u201377], which are easy to label at a large scale, but provide 2D\nconstraints only on the human pose. Alternatively, fitting 3D body models such as SMPL [70] to\nimages provides pseudo-ground truth parameters [78\u201380]. You2Me [81] and EgoBody [3] capture\nhuman pose data for interacting individuals using head-mounted cameras in indoor settings. Recently,\nEgohumans [4] has expanded the scope to include up to four interacting individuals in both indoor\nand outdoor settings. Meanwhile, larger datasets like Ego4D [17, 18] offer extensive data from head-\nmounted cameras for tasks such as social interaction and hand-object interaction, but they lack data\nfrom additional body-worn cameras. The recently published Nymeria dataset [82] addresses this gap\npartially and includes real-world videos from wrist-mounted cameras. Our real-world MultiEgoView\ndataset further extends to a setup with six body-worn cameras with additional sensors at the knees and\npelvis. To overcome limitations in real-world datasets, realistic synthetic datasets offer an alternative\nthat offers diversity and quality ground truth annotations [15,26,27]. Our work expands on this\napproach by introducing a configurable simulator tailored to body-worn sensors, with adjustable\nparameters for lighting, scene, and camera placement. EgoSim complements real-world datasets like\nNymeria by enabling the rendering of synthetic images from adjustable body-worn cameras based on\ntheir captured motion sequences.\nEgocentric perception. Wearable cameras serve as the primary input for research on egocentric\nperception tasks. Currently, real and synthetic egocentric datasets mainly feature head-mounted\nsensors. Some systems [13, 14, 83] use a single head-mounted, body-facing, fisheye camera to\nestimate 3D ego-body pose, while others rely on a stereo configuration [15,73,74]. Head-mounted,\nbody-facing cameras benefit from capturing visible joints in image space to aid ego-body pose\nestimation. Other methods recover the 3D pose from non-body-facing cameras. HPS [84] integrates\nmultiple body-worn IMUs with camera-based localization using structure from motion. Kinpoly [85]\nrecovers the whole body pose from a front-facing camera using physics simulation with reinforcement\nlearning, while EgoEgo [5] combines SLAM with a diffusion model to recover the ego-body pose.\nAvatarPoser [33] and its subsequent work [8,34] predict full-body poses based on head and hand\nposes tracked by commercial mixed reality devices. HOOV [86] extends hand tracking beyond the\nfield of view of head-mounted cameras using inertial signals captured at the wrist.\nSo far, egocentric datasets have mainly focused on head-mounted cameras that either point down\ntoward the body [12, 14, 15] or forward [5, 16,87], often designed for specific devices [3,4, 18, 88].\nOur work extends egocentric datasets to multiple body-worn cameras by providing an adaptable\nsimulation platform and a real-world dataset of six body-worn cameras."}, {"title": "3 EgoSim Simulation Platform", "content": "EgoSim is designed for body-worn camera simulation. We extend Microsoft's AirSim simulator [61]\nintegrated within the Unreal Engine [89] to leverage its flexibility and realistic output renders\n(e.g., [26, 27]). Specifically, we augment the platform with the capability of simulating body-worn\ncameras during realistic human motion, generating dynamic changes in camera motion that correspond\nto a person's movements, including potentially irregular, rough, and non-smooth moments.\nSimulating images. EgoSim renders footage through Unreal Engine's cinematic camera [90]\nfor realistic images. The camera model and noise parameters are adjustable. EgoSim supports\nsimultaneously rendering multiple modalities (Figure 2), including RGB, depth, normal maps, and\nsemantic segmentation masks. These modalities are complementary and can serve as input to various\ncomputer vision tasks in the future.\nSimulating physical attachment and motion artifacts. A key feature of EgoSim is the consid-\neration of camera attachment to account for motion artifacts during simulation. Since body-worn\ncameras are non-rigidly mounted, often coupled to clothing or strapped to the limbs like a smartwatch,\nthe loose attachments can lead to slip and drag in the camera's position and orientation. EgoSim\nsimulates these using spring arm mounts that connect the avatar's body and the virtual cameras. We\ndemonstrate that spring-damper systems as a camera mounting model help to realistically capture the\neffects of loose camera attachment as found in the real world (Section 6.3).\nSimulating diverse environments. EgoSim benefits from the vast selection of indoor and outdoor\nenvironments available in Unreal and previous work, e.g., [26]. As shown in Figure 3, it can render\nboth, large, realistic hand-modeled scenes and scanned scenes that closely resemble their real-world\ncounterparts. The used scenes are in wide open spaces where motion capture is traditionally hard to\nperform. Additional details about EgoSim's features are provided in the appendix Table 3.\nSynchronizing multi-sensor and multi-person recordings. Synchronizing multiple cameras poses\nchallenges in real-world recordings, yet it is straightforward to generate synchronized multi-modal\ndata in EgoSim while obtaining ground-truth characteristics of the environment or avatars. In addition\nEgoSim is capable of simulating and rendering data from across multiple avatars and to obtain\ncorresponding ground-truth poses and camera positions. EgoSim supports a flexible number of\nsensors, sensor characteristics, and attachment locations\u2014independently for each avatar."}, {"title": "4 MultiEgo View Dataset", "content": "MultiEgoView contributes a sizeable and synchronized dataset of RGB data from six body-worn\ncameras, along with ground-truth body poses and activity annotations. Our dataset includes real\nand synthetic data, providing a challenging and interesting testbed for training and benchmarking\nbody-pose estimation, activity classification, dynamic camera localization, and mapping algorithms.\nSynthetic data generation. Using EgoSim, we rendered a dataset of 77.4 M RGB images corre-\nsponding to 119.4 hours captured by six virtual cameras on a virtual avatar. Images were rendered\nwith a 118\u00b0 field of view (FOV) at a resolution of 640 \u00d7 360 and a framerate of 30 fps. Cameras\nwere attached to the head, pelvis, wrists, and knees, facing outwards to capture both the environment\nand parts of the wearer's body. This considerably extends the focus of prior work on head-mounted\ncameras [12, 13, 15, 73] and better resembles emerging wearable platforms devices [3, 17,74]. \u03a4\u03bf\nensure realistic motions, we animated avatars using motion capture sequences from AMASS [47],\nconverted to FBX format for EgoSim support [71]. We randomly varied avatar appearances in terms\nof skin color and clothing texture using BEDLAM's assets [26]. Our dataset features 24 locations\nacross 4 scenes: (1) a hand-built virtual outdoor environment of a city, (2) the front courtyard of a\nuniversity building that we scanned using Polycam, with an accurate public point cloud scan and\nstructure-from-motion model available [91], (3) Downtown city with skyscrapers and (4) a park with\nsport courts, lawn, vegetation and water. Each scene includes up to four simultaneously animated\navatars to increase diversity and support multi-view multi-human pose estimation [4,92]. In addi-\ntion to the RGB data, we provide ground-truth camera and 3D avatar poses, as well as simulated\naccelerometer and gyroscope readings from all six cameras.\nReal-world data collection. We captured a dataset of ~5 hours in the real world using six GoPro\ncameras (5\u00d7HERO 10, 1\u00d7HERO 9) [93], worn at the same body locations as in our simulation. We\nrecruited 13 participants from places around our institution for this collection, who consented to\nparticipation and data recording. The study considers ETH ethics guidelines and Participants received\na small gratuity for their time. Data was recorded in the same university front courtyard that was\nscanned for the synthetic environment (2), using GoPros set to a resolution of 1080p at 30 fps and\na horizontal FOV of 118\u00b0. The 13 participants (4 female, 9 male, ages 21-30, mean = 26.4) were\nrecruited from our institution, with heights ranging from 160-190 cm (mean = 176.1, SD = 9.5) and"}, {"title": "5 Baseline Method: Wearable Multi-Camera Body Pose Estimation", "content": "To demonstrate the benefits of MultiEgoView, we trained a neural network to estimate 3D ego body\nposes using multiple body-worn cameras. The input to the network consists of the aligned video\nsequences $X \\in [0, 1]^{C\\times F\\times 3\\times H\\times W}$, with F frames from C body-attached cameras. Based on these\ninputs, the network predicts a pose pi for each input frame i.\n5.1 Network architecture\nOur network is a Vision Transformer Model based on Sparse Video Tube ViTs [95]. We extract\nfeature vectors from each input video using a sparse view tokenizer SVT with a shared interpolated\nkernel. The extracted feature vectors from the sparse tube tokenizers are then added to their fixed\nspatio-temporal position encoding kp and their learnable view encoding $\\kappa_{v,c}$, per camera c.\n$$V_c = SVT(X, W) + K_p + K_{v,c},$$\nwhere W are the shared weights of the kernel.\nThe resulting feature vectors for the different cameras v\u00ba are concatenated with the pose token\n$\\Phi_j = \\tau(j) + \\psi, j \\in [0, F - 1]$, where $\\psi$ is a trainable pose token and \u03c4 is a sinusoidal positional\nencoding. The resulting token sequence is then processed using a Vision Transformer Encoder.\n$${z_0,..., z_{F-1}} = ViT(concat(\\Phi_0, ..., \\Phi_{F-1}, V_0, ..., V_{c-1}) )$$\nBased on each embedded pose token z, we obtain the 6D representation [96] of the SMPL pose\nparameters 0, the 6D relative rotation $R_r$, and 3D relative translation of the root $t_r$ with respect the\nprevious frame.\n$$0 = W_\\theta z, R_r = W_{R_r} z, t_r = W_t z$$\nTo improve generalization, the network is trained to predict the pose difference, i.e., the relative root\npose with respect to the previous pose, instead of directly predicting global root poses.\nUsing Forward Kinematics, we obtain the global body pose p with respect to the starting pose.\n$${p_0,..., p_{F-1}} = FK_0(\\theta, R_g, t_g, \\beta), where R_g, t_g = FK_g(R_r, t_r)$$\nWhere \u03b2 are the shape parameters of the SMPL-X model [71] for a given person.\nWe use 4 tubes with the following configurations: 16 \u00d7 16 \u00d7 16 with stride (12, 48, 48) and offset\n(0, 0, 0), 24 \u00d7 6 \u00d7 6 with stride (12, 32, 32) and offset (8, 12, 12), 12 \u00d7 24 \u00d7 24 with stride (24, 48, 48)\nand offset (0, 28, 28), and 1\u00d732\u00d732 with stride (12, 64, 64) and offset (0, 0, 0). The pose embedding\nparameter is initialized using the Kaiming uniform distribution [97], and the pose token is initialized\nusing the Normal distribution.\n5.2 Loss function\nWe supervise the network with the following loss function:\n$$L = \\lambda_\\theta L_\\theta + \\lambda_p L_p + \\lambda_v L_v + \\lambda_{t_r} L_{t_r} + \\lambda_{R_r} L_{R_r} + \\lambda_{t_g} L_{t_g} + \\lambda_{R_g} L_{R_g} + \\lambda_z L_z$$\nThe angle loss $L_\\theta$ encourages the model to learn the SMPL angles 0, while the joint position loss Lp\nforces the predicted joint positions through forward kinematics to be close to the ground-truth joint\npositions. This way, both the local and the accumulated errors are considered.\n$$L_\\theta = ||\\theta_{6D} - \\theta_{6D}||_1 and L_p = ||p - p||_1,$$"}, {"title": "6 Experiments", "content": "We empirically study the effectiveness of MultiEgoView for egocentric body pose estimation. Follow-\ning the BABEL-60 split [49] (60%/20%/20%), sequences of synthetic data are divided into segments\nof up to 5 seconds. The baseline model directly takes inputs from all six cameras, normalizes the\nimages to the ImageNet mean, and downsamples them to 224 \u00d7 224 pixels at 10 fps. We accelerate\nthe training process with a pre-trained sparse tube tokenizer on UCF101 [98,99]. The model is trained\nusing the Adam optimizer with a learning rate of 1 \u00d7 10-4 on an Nvidia GeForce RTX 4090 with a\nbatch size of 12 for 135k steps, taking around 3 days.\nFor real data, we use a random 80%/20% split with the same 5-second chunking and training\nparameters. We also conduct a cross-participant evaluation, using 10 participants for training and 3\nfor testing, to demonstrate the model's generalization ability.\n6.1 Quantitative metrics\nWe evaluate our model on a series of metrics using the body joints of the SMPLX model as follows:\nGlobal MPJPE (m) Evaluates the mean l2-norm between predicted and ground truth joint positions,\npunishing both pose and global position errors.\nPA-MPJPE (m) Assesses pose estimation accuracy after aligning joint positions up to a similar-\nity transform, isolating pure pose errors.\nMTE (m) Mean Translation Error measures the mean l2-norm of global root translation\nerrors, indicating global translation accuracy.\nMRE Mean Rotation Error reports global orientation error using $||RR^{-1} - I||$.\nMJAE (\u00b0) Mean Joint Angle Error compares predicted joint angle errors in degrees\nwithout considering forward kinematic chain errors.\nJerk (m/s\u00b3) Measures the smoothness of the predicted movement, indicating temporal\ncontinuity and naturalness of motion.\n6.2 Evaluation results\nTable 2 shows the results of our multi-view pose transformer when trained on MultiEgoView. Training\non synthetic data shows a low PA-MPJPE, implying a very good pose estimation. The slightly higher\nglobal MPJPE error arises due to a worse estimation of the root translation and rotation. The\ncombination of synthetic and real data in MultiEgoView is crucial, as direct sim-2-real and training\nsolely on real data fails to achieve accurate pose estimation. Pretraining on synthetic data followed by\nfine-tuning on real data improves the global MPJPE by 3.1-4 times and also lowers the PA-MPJPE\nby at least 2.7 cm, indicating a knowledge transfer of pose understanding from the large synthetic\ndataset to the real-world data. Even with a reduced fine-tuning train split of 20%, the network predicts\naccurate poses, though with a 8.8% increase in translation error. This showcases the benefit of\nsynthetic data in improving pose estimation on scarce real training data. The results of the cross-\nparticipant evaluation lag behind the others. Indicating that more diversity could be required to obtain\nstable cross-participant results."}, {"title": "6.3 Spring Damper", "content": "Body-worn cameras experience motion artifacts, especially when mounted on limbs that move\nquickly, due to non-rigid mounting points. EgoSim models these motion artifacts via a Spring Arm.\nWe demonstrate that a spring-damper system approximates real camera motion better than a rigid\nmount. For that, we used an OptiTrack motion capture system to track both the attached body and the\nGoPro that we loosely attached to the body. Using an OptiTrack motion capture system, we tracked\nboth the body and a loosely attached GoPro. Results show the spring-damper model yields a lower\nmean position error (1.98 cm) compared to the rigid model (2.35 cm), highlighting its effectiveness."}, {"title": "7 Discussion", "content": "While egocentric pose estimation has been well explored, in prior implementations head-mounted\ncameras faces challenges such as self-occlusion, reduced resolution for lower body reconstruction,"}, {"title": "8 Conclusion", "content": "We have proposed EgoSim, an egocentric multi-view simulator for body-worn cameras that generates\nmultiple data modalities to support emerging wearable motion capture and method development.\nUsing EgoSim, we partially generated MultiEgoView, the first dataset that complements existing head-\nfocused egocentric datasets with synchronized footage from six cameras worn at other locations on the\nbody, simulated from accurate and real human motion and artifacts. We complement MultiEgoView's\n119 hours of synthetic data with 5 hours of actual recordings from 6 body-worn GoPro cameras and\n13 participants during a wide range of motions and activities in the wild with annotated 3D body\nposes and classification labels to bridge the gap between simulation and real-world data.\nIn the wake of the emerging area of vision-based method development from one or more body-worn\nsensors, we believe that our release of EgoSim and MultiEgoView will be a useful resource for future\nwork to increase our understanding of human activities and interactions in the real world."}, {"title": "A Data Access", "content": "The MultiEgoView dataset, its structural description, and usage information can be found here:\nhttps://siplab.org/projects/EgoSim. We will release EgoSim's code to facilitate future\nresearch and data generation. An overview of EgoSim's rich customization options can be found in\nTable 3. An overview of the diversity of our scenes is shown in Figure 5."}, {"title": "B Model Complexity and Ablation Studies", "content": "Our multiview transformer is ViT-based and has 114M trainable parameters and requires roughly\n1.7GB VRAM for inference. With an input/output window of up to 5 seconds, the inference time is\n17.6ms on an RTX 4090 with a batch size of 1, making the system real time capable. Training with 6\ncameras and a batch size of 12 increases VRAM demands to 20GB.\nB.1 Analysis of Cameras\nPrevious work utilized varying numbers of body-worn cameras [5,6,35,85]. In our ablation study\n(Table 4), we demonstrate the benefits of using more cameras. In this ablation study, we use scenes\n(1) and (2) of our dataset. Our multiview transformer achieves the lowest global-MPJPE with six\ncameras. Even with fast-moving cameras (see Section C) attached to knees and writs, our method\naccurately recovers body pose, though global translation error increases. Using only head and wrist\ncameras results in higher pose errors, particularly in leg and foot movements (0.068/0.106/0.145m\nroot-aligned foot position error for the three configurations respectively). The use of additional\ncameras also leads to more pronounced and active motions. The model tends to average poses over\nsequences rather than capturing rapid movements. This is evidenced by a decreased jerk with fewer\ncameras, a finding further supported by qualitative analysis.\nThis highlights the advantage of additional cameras, especially for accurately estimating limb poses,\neven when attached to fast-moving mounting points. Thus, showing that we do not require cameras\non stable positions, e.g. head or pelvis."}, {"title": "B.2 Analysis of Scenes", "content": "Within the main paper, we investigated the sim-to-real transfer of our model. Here, we investigate the\nmodel's ability to transfer its knowledge between scenes. Table 5 shows that there is a significant\nrise in pose prediction error when transferring scenes. Interestingly the model is generally able to\npredict the root pose (low MTE) and lower body pose of the avatar while the arms are badly predicted,\nas confirmed by a qualitative inspection. The trend is similar when training on just 1 scene and\nevaluating on the scene (2) and when training on (1) and (2) and evaluating on the very diverse scenes\n(3) and (4).\nThis indicates the opportunity for future scene generation to improve the dataset's generalizability. As\nwe will publish EgoSim upon acceptance, future research can tailor the synthetic scenes to achieve\nmaximal performance in the target domain."}, {"title": "C Analysis of Camera Positions", "content": "Limb-based cameras, such as those mounted on wrists or knees, experience higher velocities, accel-\nerations, and jerks, making them harder to track and localize. As shown in Table 6, the head and\npelvis are the most stable mounting points, with the least movement. In contrast, wrists have the\nhighest average acceleration due to rapid arm movements during activities like walking. Knees follow\nslightly behind as they are mostly steady for all standing motions. Overall, MultiEgoView offers\nmany body camera positions with varying stability."}, {"title": "D Data Recording Procedure", "content": "Participation in the data recording was entirely voluntary. Participants were required to sign a consent\nform for both the data recording and the subsequent publication of the data. They retained the right to\nwithdraw their consent for recording and publication at any time before or during the data collection\nprocess. The names and identities of the participants will remain confidential and undisclosed. As a\ntoken of appreciation, participants received a small gift for their involvement in the study.\nUpon obtaining informed consent, participants were given a brief overview of the recording procedure\nand the specific movements required for the study. The recording session commenced from a\nstandardized starting position, followed by a brief calibration process. Cameras were then activated\nand synchronized by a clap. Participants performed the prescribed movements within a predefined\narea, executing them in a sequential order. To introduce variability in both position and camera\nperspectives, participants were instructed to take one to five steps between each movement repetition.\nThe recording session concluded with participants returning to the initial starting position. A recording\nsession took on average 10:28 minutes with a standard deviation of 1:50 minutes, up to 3 sessions\nwere recorded per participant."}, {"title": "E Data Annotation", "content": "To gain insights into the semantics of human movement, we manually annotated the real-world\nrecordings following the categories from BABEL [49]. BABEL densely annotated the majority\nof the AMASS dataset with action labels. Annotators identified segments and assigned labels to\nthese segments. The raw, language-based annotations were then categorized into action categories.\nBuilding on the BABEL framework, we included commonly found movements from BABEL in our\nrecordings, see Table 7.\nOur annotations cover the entire sequence from the starting position to the return to the starting\nposition. During the recording, participants performed different movements sequentially, often\nwalking a few steps between motions. These intermediary steps were not annotated as separate\nsegments unless they exceeded a few steps.\nMost action classes are featured for around 6 minutes in the dataset. Walking is the most prominent\nas participants often walk between different movements. MultiEgoView features a wide coverage of\ndifferent movements from leg and arm motions to sports activities, making it an ideal resource for\nevaluating BABEL-based systems on real-life data."}, {"title": "F Ethical Considerations", "content": "EgoSim's high-fidelity simulation of camera footage addresses several ethical implications associated\nwith motion capture, particularly in real-world settings. Motion capture is afflicted by privacy\nconcerns for recorded individuals, especially given the need for larger-scale capture of representative\nhuman data with diverse participants. Our simulator mitigates this by synthesizing data from realistic\navatars whose appearances can be flexibly adjusted while expressing behavior based on actual human\nmotion. This not only preserves individual privacy but also allows the creation of diverse datasets\nthat include a wide range of ethnic backgrounds, which is crucial for the effective generalization of\nlearned algorithms. Consequently, our simulator provides a valuable tool for advancing real-world\nperception inference while respecting ethical considerations.\nBody-worn cameras capture extensive environmental details, offering the potential for simultaneous\nego-body and environment understanding. Capturing data with more cameras always opens up more\nopportunities for surveillance. In our case, the amount of cameras results in the unintended exposure\nof individuals in the proximity of the participant to data recording. MultiEgoView's focus is on the\nego-body, therefore we minimize the exposure of other people in the dataset by selecting a recording\narea with a limited number of passersby. Additionally, to protect the privacy of bystanders, we\nautomatically detected and blurred all faces using deface [105]. Examples of blurred images are\nshown in Figure 6.\nIn conclusion, we have addressed data-related concerns by compensating participants, obtaining\nsigned consent for data recording, preserving privacy through face blurring, and ensuring no personal\ninformation is disclosed. We believe our work will not result in any harmful consequences or negative\nsocietal impact."}, {"title": "G License, Data Accessibility and Maintenance", "content": "The data including its documentation will be released under the CC BY-NC-SA license and is\navailable at https://siplab.org/projects/EgoSim. The dataset is composed of PNG images\nand CSV files, which are in open and widely used formats, ensuring ease of access and usability.\nGround truth joint poses of the synthetic data in the SMPL-X format can be obtained via the AMASS\nwebsite. Detailed explanations on how to read and utilize the dataset are provided on the hosted\nwebsite. Upon acceptance, the code for EgoSim and our method will be released on GitHub under\nthe GPL-3.0 license. The dataset and code will be hosted on ETH servers, ensuring long-term"}]}