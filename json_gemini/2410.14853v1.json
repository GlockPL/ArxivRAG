{"title": "DFLOW: Diverse Dialogue Flow Simulation with Large Language Models", "authors": ["Wanyu Du", "Song Feng", "James Gung", "Lijia Sun", "Yi Zhang", "Saab Mansour", "Yanjun Qi"], "abstract": "Developing language model-based dialogue agents requires effective data to train models that can follow specific task logic. However, most existing data augmentation methods focus on increasing diversity in language, topics, or dialogue acts at the utterance level, largely neglecting a critical aspect of task logic diversity at the dialogue level. This paper proposes a novel data augmentation method designed to enhance the diversity of synthetic dialogues by focusing on task execution logic. Our method uses LLMs to generate decision tree-structured task plans, which enables the derivation of diverse dialogue trajectories for a given task. Each trajectory, referred to as a \"dialog flow\", guides the generation of a multi-turn dialogue that follows a unique trajectory. We apply this method to generate a task-oriented dialogue dataset comprising 3,886 dialogue flows across 15 different domains. We validate the effectiveness of this dataset using the next action prediction task, where models fine-tuned on our dataset outperform strong baselines, including GPT-4. Upon acceptance of this paper, we plan to release the code and data publicly.", "sections": [{"title": "Introduction", "content": "A task-oriented dialogue system typically needs to manage user requests according to a designated \"plan guide\" which includes predefined task logics and policy constraints. This guideline could indicate different task trajectories or flows that lead to task completions. For example, when exploring a car rental service, the system will first ask if the user has a preferred rental company, if yes, then the system will further let the user to specify the preferred company from a constrained list. Previous research (Mosig et al., 2020; Chen et al., 2021) has introduced dialogue flows to enable dialogue systems to follow task logic and constraints when addressing user requests. However, manually curating such dialogue flows is challenging due to the intricate task logic and alignment with policy constraints across various domains. Consequently, existing task-oriented dialogue datasets often suffer from sparse flow annotations, and lack efficient and generalizable mechanisms for automated dialogue flow generation (Budzianowski et al., 2018; Byrne et al., 2019; Rastogi et al., 2020; Shalyminov et al., 2020).\nIn this work, we aim at designing an automatic data augmentation method to generate task-oriented dialogues with structured dialogue flows. Previous studies have utilized LLMs to augment task-oriented dialogues, focusing primarily on enhancing linguistic diversity (Li et al., 2022), exploring varied topics (Xu et al., 2023; Ding et al., 2023), or proposing different combinations of dialogue acts at the utterance level (Chen et al., 2023). However, these efforts neglect the crucial need for maintaining task logic consistency at the dialogue level. Our work proposes to use LLMs to augment diverse dialogues that consistently follow the task logic and constraints defined by dialogue flows.\nTo achieve this goal, we leverage the planning capabilities of LLMs (Yao et al., 2023; Wang et al., 2023a) to automatically construct diverse dialogue flows. Starting with a task instruction, our framework employs an LLM planner to generate a decision tree-structured task plan that outlines diverse solution paths. Then depth-first-search is applied to parse all valid paths within this plan. Each path represents a dialogue flow, ensuring a coherent task logic to effectively solve the given task. Subsequently, our framework uses the simulated dialogue flows to control an LLM synthesizer to generate multi-turn dialogues that follow task logics at the dialogue level. As shown in Figure 1, the LLM planner can simulate diverse dialogue flows, and the LLM synthesizer can generate coherent dialogues based on different dialogue flows."}, {"title": "Related Work", "content": "Dialogue Simulation with LLMs. Many prior works leverage LLMs to simulate dialogue datasets. Li et al. (2022) propose to prompt GPT-3 with retrieved dialogue examples from existing datasets to generate new dialogues in a controllable way. However, the diversity of the generated dialogues is constrained by the retrieved dialogue examples. Chen et al. (2023) design a soft-prompt tuning method to create a diverse set of prompts to synthesize diverse utterances using LLMs. But they only promote diversity at the utterance-level, ignoring the task logic at the dialogue-level. Other works (Wang et al., 2023b; Ding et al., 2023; Chan et al., 2024) propose to use the LLM-generated knowledge texts to synthesize diverse dialogues. However, the knowledge text does not decompose the complex task into a step-by-step plan, making the dialogue generation process less controllable.\nTask Planning with LLMs. Recent works enhance the planning ability of LLMs to solve complex tasks. Yao et al. (2023) proposes a tree-of-thought prompt to frame the problem solving as a search over a tree, and design search algorithms to obtain the best solution path to the problem. Wang et al. (2023a) designs a plan-and-solve prompting method, which generates a plan to divide the en-"}, {"title": "DFLOW Simulation Framework", "content": "Figure 1 provides an overview of our framework that includes three steps: (1) generating task plan based on task instruction, (2) sampling dialogue flows from task plan, and (3) using flow to control and generate dialogue.\nStep 1: Task Plan Generation. Given a task instruction and one in-context example, the LLM planner is prompted to generate a decision tree-structured task plan that outlines multiple solution paths, as shown in Figure 5. Our task plan adopts the decision tree-structure to cover diverse paths, because we aim at prompting the diversity of task logic at the dialogue-level. Each step in the plan has two components: (1) a system action which collects user information in order to fulfill user requests, (2) a set of values which guides or constrains the system when performing certain action. In this work, we categorize system actions into 4 types: yes/no questions, multiple choice questions, user information requests, and recommendations. Note that we categorize system actions by viewing them as different types of nodes in the decision tree, in order to reflect different structures of generated task plans, more details in Appendix A.\nStep 2: Dialogue Flow Sampling. For each task plan, the depth-first-search is applied to extract all valid paths, where each path represents a dialogue flow. At each step, we sample one value under the current action, and proceed to the next step based on the selected value. The search continues until reaching the end of the task plan.\nFurthermore, to enhance dialogue system to address abnormal user requests, such as inquiries for products or services not offered by the system, or early ending conversations before task completion, we introduce two additional types of error-handling flows to manage these scenarios:\n\u2022 Out-of-scope Request Flow: This flow is designed to handle requests that violate system constraints. To implement it, we prompt the LLM synthesizer to simulate user providing inputs that violate the system constraints, and asking the system to throw out error message and guide the user to enter valid inputs as shown in Figure 7.\n\u2022 Early-stop Conversation Flow: This flow addresses scenarios where the user decides to end the conversation before the task is completed. To implement it, we prompt the LLM synthesizer to simulate user rejecting system recommendations and ending the conversation before task completion, as demonstrated in Figure 8.\nStep 3: Dialogue Generation. Given a dialogue flow, the LLM synthesizer generates a multi-turn dialogue, where each turn is associated with each step in the dialogue flow, as demonstrated in Figure 6. An automatic filter is then applied to filter out low-quality dialogues with repetitive utterances or utterances that are not associated with any step in the dialogue flow."}, {"title": "Experiments", "content": "4.1 Dataset Generation\nSeed Task Instruction Construction. To cover a wide range of real-world tasks, we first build a seed task pool by selecting popular domains, and constructing task instructions under each domain with GPT-3.5-turbo (OpenAI, 2023). The construction details are as follows. First, we selecting 15 domains based on Zhang et al. (2024), including bank, insurance, travel, car rental, restaurant, shopping, doctor, event, apartment, meeting, ride sharing, payment, weather, calendar, navigate. Next, we prompt GPT-3.5-turbo to generate task instructions under each domain. Concretely, we break the task instruction generation into two steps: (1) service name generation, where we prompt GPT-3.5-turbo with \u201cPlease generate 20 common services in {domain name} domain for task-oriented dialogue systems\u201d; (2) intent description generation, where we prompt GPT-3.5-turbo with \u201cPlease convert the above services into user intents with intent descriptions\". Finally, we manually select 130 task instructions across the above 15 domains.\nDialogue Simulation. We initiate the simulation by inputting task instructions into our proposed framework. During this process, the same LLM initializes both the planner and synthesizer com-"}, {"title": "Intrinsic Evaluation", "content": "After dataset construction, we evaluate both the diversity and quality of the DFLOW dataset to ensure that our simulation framework is capable of generating dialogues of high diversity and high quality.\nData Diversity. We assess the diversity of DFLOW from two perspectives: plan diversity and flow diversity. For plan diversity, we apply multiple LLMs to generate various task plans across 15 task domains, as shown in Figure 2 (a), each reflecting different task logics. Additionally, the generated task plans cover different types of system actions, as shown in Figure 2 (a) and Table 3, each fulfilling different user requests. For flow diversity, we not only sample diverse dialogue flows from the generated task plan as shown in Figure 2 (c), but also introduce two error-handling flows to address abnormal user requests as shown in Figure 3. This variety in dialogue flows also enriches the diversity"}, {"title": "Extrinsic Evaluation", "content": "Previous studies (Zhou et al., 2023; Mekala et al., 2024) found that a few thousand high quality training data can significantly improve the instruction-tuned language models. Therefore, we evaluate the effectiveness of the DFLOW dataset by fine-tuning instruction-tuned models with 7B parameters in next action prediction tasks.\nData Format Setup. We convert original datasets to the instruction-tuning data format following Hattami et al. (2023). For the next action prediction task, the model's input is a dialogue context $C_{t-1} = {u_1,..., u_{t-1}}$ and a dialogue flow $F = {a_1[v_1],\u2026, a_\\tau[v_m] ]}$, where $u_i$ is the i-th utterance, $a_i$ is the action for the i-th utterance, and $[v_1], . . ., v_m]$ is the value set for action $a_i$. The output of the model is the next system action $a_t[v_1],\u2026, v_m]$ at the t-th utterance.\nEvaluation Datasets. Since there is lack of fine-grained annotations of dialogue flows in existing dialogue datasets, we choose our DFLOW and ABCD as the benchmark test sets to evaluate the dialogue understanding ability of LLMs.\nFor DFLOW, we obtain 8906 instruction-tuning training data and 468 instruction-tuning test data. For ABCD, we sample 290 instruction-tuning training data and 500 instruction-tuning test data to"}, {"title": "Ethical Considerations", "content": "We honor the ethical code in the ACL Code of Ethics. Our simulation datasets respect the copyrights of original LLM authors. During the data evaluation process, the privacy of all human annotators is respected. The dataset collection process and conditions are detailed in the paper, and the characteristics of the dataset are described in the paper as well. Our work has no potential harm to marginalized or vulnerable populations. Our datasets do not contain any identity characteristics (e.g., gender, race, ethnicity), and will not have ethical implications of categorizing people."}, {"title": "Limitations", "content": "While our framework successfully generates diverse task-oriented dialogues across 15 domains, the chosen domains may not comprehensively represent the broader range of possible scenarios encountered in real-world applications. The diversity and accuracy of the generated dialogues heavily rely on the underlying planning capabilities of the LLMs employed. Although we utilize state-of-the-art models like Mistral-8x7B-instruct and GPT-3.5-turbo, these models are still susceptible to biases present in their training data or inherent limitations in understanding nuanced user intents. The inference of some LLMs requires large GPU resources, and future research on memory-efficient inference may enable 100B+ LLMs. In addition, the current method only generates text-based dialogues, future research may further explore different data sources for task plan generation and dialogue simulation, such as images, graphs, and tabular data."}, {"title": "System Actions Details", "content": "We categorize our system actions by viewing them as different types of nodes in the decision tree, and different categories of system actions are used to reflect different structures of the generated task plans. Here are the details of each system action:\n\u2022 Yes/No Questions: This system action collects user information or feedback by asking binary choice questions, which leads to a switch to different branches for task completion.\n\u2022 Multiple Choice Questions: This system action collects user information or feedback by asking multiple choice questions, which leads to a continuation to the next valid action.\n\u2022 User Information Requests: This system action collects user information or feedback by asking user entering texts, which also leads to a continuation to the next valid action.\n\u2022 Recommendations: This system action provides final system recommendation to fulfill the user request, which marks the end of the flow.\nNote that we don't categorize the system action based on their functionality in the task, because it will be domain- and task-specific, and requires further validation from external knowledge, which is out of the scope of this work."}]}