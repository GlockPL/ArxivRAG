{"title": "A Comprehensive Benchmark of Machine and Deep Learning Across Diverse Tabular Datasets", "authors": ["Assaf Shmuel", "Oren Glickman", "Teddy Lazebnik"], "abstract": "The analysis of tabular datasets is highly prevalent both in scientific research and real-world applications of Machine Learning (ML). Unlike many other ML tasks, Deep Learning (DL) models often do not outperform traditional methods in this area. Previous comparative benchmarks have shown that DL performance is frequently equivalent or even inferior to models such as Gradient Boosting Machines (GBMs). In this study, we introduce a comprehensive benchmark aimed at better characterizing the types of datasets where DL models excel. Although several important benchmarks for tabular datasets already exist, our contribution lies in the variety and depth of our comparison: we evaluate 111 datasets with 20 different models, including both regression and classification tasks. These datasets vary in scale and include both those with and without categorical variables. Importantly, our benchmark contains a sufficient number of datasets where DL models perform best, allowing for a thorough analysis of the conditions under which DL models excel. Building on the results of this benchmark, we train a model that predicts scenarios where DL models outperform alternative methods with 86.1% accuracy (AUC 0.78). We present insights derived from this characterization and compare these findings to previous benchmarks.", "sections": [{"title": "1 Introduction", "content": "Machine learning (ML) has long been considered superior to deep learning (DL) when it comes to handling tabular data [1, 2, 3], a common data format in many real-world applications and fields like medicine [4, 5, 6], economy [7, 8], and operations [9, 10], to name a few. Nonetheless, this generalization does not hold universally [11]. While traditional ML algorithms, such as Random Forest [12] and XGboost [13], often excel in this domain, there are scenarios where DL models outperform ML methods, challenging the prevailing notion [11]. Understanding the conditions under which DL models can surpass ML methods on tabular data is crucial for practitioners seeking to leverage the full potential of these advanced techniques.\nSeveral benchmarking studies have attempted to compare the performance of ML and DL models across various types of data, in general [14, 15, 16], and for tabular data, in particular [17]. For instance, [11] proposed the TabNet model, a DL model specifically designed to handle tabular data, and showed competitive performance against traditional ML approaches. [1] used 45 tabular datasets from various domains (mainly from OpenML [18]) with heterogeneous columns, below 500 columns but over 3000 rows with at least ten times more rows than columns, no time-series, and without deterministic target column (like poker games' data). The authors removed rows with missing data, used one hot encoding [19] for categorical columns, and for regression cases used a log transformation to the target variable. Based on these settings, the authors compared MLP [20], ResNet [21], FT transformer [22], and SAINT [23] for the DL models and the RF [12], XGboost [13], histogram-gradient boosting tree [24], and gradient boosting tree [25] for the ML models. The authors show that XGboost outperformed all DL models for both classification and regression tasks while showing that tuning hyperparameters does not make DL models outperform the ML models. They also suggest that DL models are challenged by uninformative features."}, {"title": null, "content": "Similarly, [2] used 11 tabular datasets with both classification and regression problems with 10 to 2000 columns and between 7000 and a million rows. The authors performed standardization (aka z-score normalization) of each feature in each dataset to have a mean of zero and a standard deviation of one. The authors take into consideration the XGboost model as a representer of the ML models, four DL models including TabNet, and three ensemble models of the DL model (with and without XGboost). The authors concluded that the DL models underperform compared to the ML models while an ensemble combining both model types produces better results, on average. [26] used 176 classification datasets from OpenML and CC-18 and 19 algorithms including 11 DL models and 8 ML models. The authors found that, on average, CatBoost [27] and XGboost outperformed the other models while also being an order of magnitude more computationally effective. The authors used the PyMFE [28] to compute a feature vector for each dataset and used it to analyze the properties in which DL models outperform ML models, on average, finding that irregular, with a large number of rows, and a high ratio of size to number of features actually decrease the DL models performance. [29] compared 300 datasets including both classification and regression tasks from multiple domains further supporting the conclusion that, on average, tree-based ML models outperform DL models.\nDespite these efforts, a comprehensive understanding of the nuanced conditions under which DL models excel over ML models, particularly on tabular data, remains underexplored. In particular, measurable (statistical) features of the dataset for the binary prediction of either DL or ML model will provide a better performance, are still unknown.\nIn this paper, we aim to fill this gap by conducting an extensive benchmark study involving 111 datasets encompassing both regression and classification tasks. We evaluate 20 different model configurations, including 7 DL-based models, 7 Tree-based Ensemble models (TE), and 6 classical ML-based models, to ascertain their performance on tabular data. Based on these results, we adopted a meta-learning approach, profiling the properties of datasets where DL outperforms ML models. Our results reveal complex patterns while generic behavior like a small number of rows and a large number of columns as well as large kurtosis results in DL models outperforming ML models. We also find that the gap between the two groups is smaller for classification tasks compared to regression tasks. Our key contributions are:"}, {"title": null, "content": "\u2022 Conducted an extensive and diverse benchmark involving 111 datasets encompassing both regression and classification tasks.\n\u2022 Evaluated the performance of 20 different model configurations, including 7 DL-based models, 7 Tree-based Ensemble models (TE), and 6 classical ML-based models.\n\u2022 Identified dataset characteristics, such as small number of rows and high kurtosis, that favor DL models over other ML models.\n\u2022 Trained a meta-learning model to predict whether DL models will outperform ML models, achieving 86.1% accuracy (AUC 0.78).\n\u2022 Presented explainable models, namely logistic regression and symbolic regression, which predict where DL models may perform better than alternative models.\n\u2022 Provided detailed insights into the comparative performance of DL and ML models on a diverse set of tabular datasets.\nThe remainder of this paper is structured as follows: Section 2 describes the datasets and methodologies used in our benchmarking experiments as well as the evaluation strategy and profiling method. In Section 3, we present our experimental results. Finally, Section 4 discusses the applicative outcomes of these findings and suggests promising future research directions."}, {"title": "2 Experimental setup", "content": "In this section, we formally outline the experimental setup used to explore when DL models outperform ML models for tabular data. Initially, we present the datasets included in the analysis. Afterwards, the ML and DL"}, {"title": "2.1 Datasets", "content": "In order to create a diverse and comprehensive benchmark for tabular datasets, aiming to provide insights into the scenarios where DL models outperform ML models, we incorporated a wide array of datasets exhibiting considerable variability and diversity of real-world tasks and characteristics. The benchmark includes datasets for both regression and classification tasks sourced from various domains such as economics and medicine to ensure relevance across different application areas. Additionally, we selected datasets of varying sizes in terms of both the number of rows (43-245,057) and columns (4-267), which are known to be crucial to the performance of ML and DL models, from previous studies.\nIn our selection process, we also prioritized datasets containing categorical features. Categorical features are prevalent in real-world datasets and pose unique challenges, often necessitating specific preprocessing and modeling approaches. Ensuring their inclusion allows our benchmark to accurately reflect real-world complexities and facilitates an assessment of how well different models handle such features. Moreover, we ensured that our dataset selection varies in terms of difficulty, with some datasets presenting straightforward predictive tasks while others pose more complex challenges with respect to model prediction accuracy. This varying degree of difficulty ensures that our benchmark can comprehensively evaluate and differentiate model performance across simpler as well as more challenging predictive tasks. None of the datasets in this benchmark allowed a perfect prediction with all models. In Table 1 we compare the characteristics of our datasets with previous works."}, {"title": "2.2 Machine learning and deep learning models", "content": "We use 20 ML and DL models to capture a representative set of algorithms from both groups for our analysis. These models are mainly adopted from previous benchmarking studies and due to their overall popularity [26, 1]. Formally, for tree-based ensemble models, we use XGBoost [13], Random Forest [12], AdaBoost [31], LightGBM [32], CatBoost [27], and the H2O-GBM AutoML library [33]. For DL models we use two AutoDL libraries (AutoGluon [34] and H2O [33], both restricted to DL models), a ResNet-like model [21], MLP [20]. Additional models include AutoML libraries (TPOT [35], AutoGluon without the DL restriction [34]), and also SVM [36], KNN [37], Decision Tree [38], a symbolic regression model (GPLearn) [39], and Linear Regression or Logistic"}, {"title": "2.3 Evaluation strategy", "content": "Inspired by [26], we present the mean results of a 10-fold cross-validation evaluation. For the regression tasks we calcualte root mean squared error (RMSE), mean absolute error (MAE), and coefficient of determination (R2). For the classification tasks, we present accuracy, Area Under the receiver operating characteristic (ROC) Curve (AUC), and F\u2081 score. For each dataset, we evaluate the performance of each one of the models. We then present the performance summaries and ranking of each model. We also calculate the ranking by model groups (GBM/ML, DL, and others)."}, {"title": "2.4 Meta-analysis profiling", "content": "In order to analyze for which datasets DL models outperform ML models, we adopted a meta-learning approach, following the same analysis proposed by [41]. Namely, we aim to find a meta-learning ML algorithm (A*) that receives as input a set of datasets (D), a set of ML models (ML), and a set of DL models (DL). It outputs a model (e.g., function) (M) such that given a new dataset and the same sets of ML and DL models, the model (M) returns whether ML or DL is best performing on the given dataset, according to some loss function (L). Formally, the algorithm A* satisfies:\n$A^* := \\min_{A \\in \\mathbb{A}} \\sum_{d \\in \\mathbb{D}} L(A(d, ML, DL)),$ (1)\nwhere A is the set of all possible meta-learning models and A \u2208 A is a meta-learning model. We solve this optimization problem using a meta-learning approach. First, we construct a meta-dataset which operates as the data for the learning model. Second, we find a learning model that optimizes Eq. (1) using a search algorithm.\nIn order to obtain A*, we propose a meta-learning approach that requires a dataset to learn from. Thus, we constructed a meta-dataset as follows. First, each dataset is converted into a meta-feature vector with 20 parameters (full description provided in Table 8), marked as X. This feature space is constructed from a simple feature [42] such as the number of records and features, statistical properties of the dataset itself [43] such as the fourth standardized moment, and statistical features measuring the connections between the independent features and the target feature [44] such as the average Pearson correlation between the independent features and the target feature. These features have been used to obtain good results in previous meta-learning tasks [42, 43, 44]. In addition, we compute the performance of each of the 20 algorithms (see Section 2.2) on the dataset (RMSE for regression, and AUC for classification), taking the model with the best performance. If this model belongs to the ML algorithms group, the target column (Y) of the meta-learning is set to 1 and 0 otherwise. Based on these two sets (X, Y), we define a meta-dataset such that (X) are the source features and (\u1ef8) is the target feature of the dataset.\nClassifying whether ML or DL will best perform for a given dataset is a binary classification problem. We formalize this task as a search problem in which one needs to find the optimal configuration, as defined in the"}, {"title": null, "content": "ML pipeline in Eq. (1). One way to solve this classification problem is by using machine learning models. To this end, we used both a symbolic regression model and a ML model and to obtain both explainability and investigate the best possible theoretical prediction, respectively. We used aggressive grid search optimization for the hyperparameters of both models, as well as the 10-fold cross-validation to ensure robustness. Specifically, due to the characteristics of this meta-dataset, we chose H2O-DL as the main model for this task."}, {"title": "3 Results", "content": "In this section, we present the results of the benchmarking analysis. We begin by outlining the performance of each of the models on the datasets, summarizing 4,000 computation hours. Afterward, we explore the influence of several central properties of datasets and their influence on ML and DL performance. Finally, we provide a measurable profiling of when DL outperforms ML models on tabular tasks."}, {"title": "3.1 Model Ranking", "content": "Table 3 outlines the performance of the Tree-based Ensemble models (TE) and DL models on the 111 datasets. The models are ordered from top to bottom according to the number of datasets where they outperformed the other datasets. One can notice that ML models occupy the first four lines, led by CatBoost with 19/111 (17.1%). The first DL model appears on the fifth row with 11/111 (9.9%) datasets where it is best-performing. This ranking is preserved by other metrics such as average ranking and median ranking."}, {"title": "3.2 Meta-Analysis Profiling", "content": "We now present the results of the prediction of whether ML or DL will perform better in each dataset. Table 6 presents the coefficients of a logistic regression for this classification task. The model reveals several important findings. First, it demonstrates with statistical significance that the relative performance of DL models (compared to TE models) is better in classification tasks than in regression tasks. Second, we find that the Kurtosis variable is statistically significant. Finally, we find that the PCA components are also positive and almost statistically significant. We discuss these findings in Section 4.\nNext, we repeat this analysis after limiting the datasets to cases where the performance of ML/DL models was significantly different (with p < 0.05). This restriction leaves 36 datasets, 11 of which demonstrate higher performance for DL and 25 for TE. As we show later on, and as previously noted by [26], DL models have an advantage in small datasets; we therefore use the H2O-DL model to predict whether TE or DL performs better based on the properties of each dataset. Remarkably, H2O-DL provides a high performance in this classification task, with an AUC of 0.78, accuracy of 86.1%, and F1 score of 0.61. As a baseline, we also train a logistic regression, an explainable model, which obtains lower but still impressive performance (AUC of 0.68, accuracy of 80.6%, and F1 score of 0.44).\nBased on the predictions of the logistic regression model, we provide further insights into the most influential factors for TE versus DL performance. Fig. 3 presents heatmaps of four dataset's configurations and their influence on the probability that a DL model would outperform the ML model for a given dataset, including (a) the impact of the number of columns and rows; (b) the influence of numerical and categorical feature counts; (c) the effect of X-kurtosis and row count; and (d) the role of PCA components necessary to maintain 99% of the variance. As one can see from sub-figure (a), for a small number of rows, increasing the number of columns results in a higher probability that the DL model would outperform an ML model. However, this effect decreases relatively quickly as the number of rows increases. Notably, for all the explored configurations, the probability does not increase over 0.5 which indicates no configuration found where DL models would outperform ML models, on average. For sub-figure (b), a more clear gradient is revealed where a smaller number of rows and a higher number of columns increase the probability that DL models outperform ML models. Interestingly, this heatmap reveals configurations where the probability is higher than 0.5. For sub-figure (c), one can notice that the number of rows does have"}, {"title": null, "content": "much influence on the probability while a large X-kurtosis signals that DL models are probably preferred over ML models. Finally, sub-figure (d), shows a similar trend to the previous sub-figures where a small number of rows and columns, especially if these are more \u201ccomputationally-attractive\" like PCA-based features, results in a higher probability for DL models to outperform ML models.\nFinally, we trained a symbolic regression (SR) model to search for more complex equations of this prediction task. The performance of the SR model was relatively close to that of the logistic regression. The formula output, which we present in Eq. 2, also predicts higher relative DL success rate in small datasets and with high Kurtosis values.\nlogreg (0.005 \u00b7 Xkurtosis 4.3 \u00d7 10-5 Xrow_count 0.053 Xstd_coefficient_of_anomaly 23.0\u00b7 Xstd_linearly_to_target +0.89) (2)\nwhere,\nlogreg(z) = 1 / 1+ e-z\nIkurtosis is the kurtosis\nXrow_count is the number of rows\nXstd_coefficient_of_anomaly is the standard deviation of the coefficient of anomaly\nXstd_linearly_to_target is the standard deviation linearly to the target\nNext, to explore the effect of data size, we repeated the training of 10 large datasets after sampling them to 1000 training samples. The original and revised rankings for these 10 datasets are summarized in Table 7. While the ranking of some DL models (AutoGluon-DL and ResNet) improves in the smaller datasets, TE models still dominate this set. Although this is a relatively small sample of 10 datasets, it provides further evidence that sample size is an important factor but does not determine which model will have the best performance by itself."}, {"title": "4 Discussion", "content": "In this study, we benchmark the performance of 20 data-driven models, divided between ML and DL models, for the tasks of regression and classification of tabular data from 111 datasets. While DL models are currently state-of-the-art in multiple computational tasks such as computer vision, natural language processing, and signal processing, to name a few, they are outperformed by ML models for tabular data. Following several recent bench-mark studies, we computed one of the most comprehensive benchmarking of ML and DL models' performance on tabular data and their profiling. In this study, we focused on providing measurable (statistical) features of datasets, available before running any model, which can indicate when DL models would outperform ML models in both regression and classification tasks. The trained model obtained a remarkable accuracy of 86.1% and AUC of 0.76. We also presented explainable models, logistic regression and symbolic regression, with slightly lower performance.\nSimilar to previous benchmarking studies [45, 2], our analysis shows that ML models, on average, outperform DL models on tabular data. Specifically, Tree-based Ensemble models consistently exhibit the highest performance in this field. This behavior is consistent for the best-performing model, as well as the mean rank, and 3-top model, indicating that on a random tabular dataset, ML models would be the safe \"bate\", as indicated by Table 4. In"}, {"title": null, "content": "particular, we found that AutoGluon, an automatic ML model [46] that uses ensembles of both ML and DL models, outperforms the other models by a large margin. This outcome aligns with the findings presented by [26]. In addition, we found that TabNet actually performs worse than DL models that are not specifically designed for tabular data such as MLP and H2O, which agrees with previous attempts of using TabNet as part of benchmarking attempts [47].\nMoreover, previous studies do not show clear agreement on the influence of the number of rows and cols on the probability that DL models would outperform ML models [1, 2, 26], we tackle this challenge as shown in Fig. 3. Our results clearly indicate a somewhat linear trend where a smaller number of rows and a larger number of columns, on average, results in a higher probability that DL models would outperform ML models. However, the analysis in Table 7 which tries to isolate the size factor does not show clear results; our conclusion is that while DL models may outperform other ML models in small datasets in many cases, this is just one of many other factors which affect the relative performance of the two model groups.\nRegarding the profiling of the dataset characteristics to predict whether either DL model or ML model would provide the best-performing results, a logistic regression analysis (see Table 6) reveals only two statistically signif-icant dataset's features - is the task a regression or classification and the kurtosis metric. In particular, the relative performance of DL models compared to other ML models is better in classification tasks compared to regression"}, {"title": null, "content": "tasks. This is also highlighted by the separate regression and classification ranking in the appendix (Tables 14, 20). A possible explanation for this phenomenon is that in classification tasks, all errors contribute equally to the performance metric while in regression tasks, large errors have more weight (RMSE is unbounded). Due to the large parameter space of DL models, it is possible that they could sometimes exhibit large errors which are heavily penalized by metrics such as RMSE. Indeed, when ranking the models by MAE for regression tasks, the relative ranking of DL models improves significantly with AutoGluon-DL positioned second after CatBoost (Table 16). Finally, focusing on the kurtosis metric, a large value indicates a long \u201ctail\" distribution where DL is known to excel compared to ML models [48].\nLimitations and future work. While this study presented an exhaustive evaluation of the different models over 111 datasets, it still has several limitations. First, the choice to include diverse datasets, in contrast to several previous benchmarks, has many advantages but also some disadvantages. For example, including small datasets could introduce more noise into the results. In addition, including many types of datasets inevitably means each type will have fewer instances. We tackled this problem by including a large number of datasets, but including one type of homogeneous datasets would obviously result in more instances for this type. Second, an analysis of feature selection or feature engineering, which is known to have a significant impact on the down-the-line model's performance, has not been included in this work. Finally, while this study included diverse regression and classification datasets, there are additional tasks that have not been included, such as time-series or multilabel classifications. These extensions can reveal additional insights regarding the performance of ML and DL on tabular data and are promising research future venues."}, {"title": "5 Supplementary Material", "content": "5.1 Description of models\nTPOT is an open-source library that automates the process of designing and optimizing ML pipelines. It uses genetic programming to explore a wide range of models and preprocessing steps, aiming to find the best pipeline for a given dataset. TPOT also performs hyperparameter optimization. We ran the AutoML library with mostly default settings, limiting each model's run to one hour.\nH2O is an open-source software for data analysis that facilitates the development and deployment of machine learning models. It provides a scalable and fast platform for building models, with support for a variety of al-gorithms, including generalized linear models, gradient boosting, and deep learning. H2O's automated machine learning functionality assists in discovering the best models by automatically training and tuning multiple models within a user-specified time frame. We ran two H2O models, both the H2O Gradient Boosting Machine (labeled H2O-GBM) and the H2O Deep Learning (labeled H2O-DL). Both were limited to one hour for each model run.\nXGBoost is an open-source ML algorithm developed for supervised learning tasks. It is an implementation of gradient boosted decision trees. Technically, XGBoost enhances performance by optimizing for speed and scalability, using techniques like parallel processing, tree pruning, and regularization to prevent overfitting. It also supports missing value handling and a range of objective functions. Widely recognized for its superior predictive power and fast execution, XGBoost has been a top choice in ML tabular tasks on a wide range of domains. We performed hyperparameter optimization using TPOT, with a one hour time limit.\nRandom Forest extends the bagging method by incorporating both bagging and feature randomness to gen-erate an uncorrelated ensemble of decision tree models. Feature randomness creates a random subset of features, ensuring low correlation among the decision trees which improves the generalization of Random Forest compared to other bagging decision tree models. We performed hyperparameter optimization using TPOT, with a one hour time limit.\nAdaBoost classifier is a meta-estimator that starts by fitting a classifier, usually a decision tree, to the original dataset. It then fits additional copies of the classifier to the same dataset, adjusting the weights of incorrectly classified instances so that subsequent classifiers focus more on the challenging cases a method usually called boosting. We performed hyperparameter optimization using TPOT, with a one hour time limit.\nCatBoost short for Categorical Boosting, is an open-source boosting library. It is tailored for regression and classification problems with a large number of independent features. Unlike traditional gradient boosting methods, CatBoost can directly handle both categorical and numerical features without needing feature encoding techniques (like One-Hot Encoding or Label Encoding). Moreover, it employs an algorithm called symmetric weighted quantile sketch to automatically handle missing values, thereby reducing overfitting and enhancing the overall performance of the model. We performed hyperparameter optimization using TPOT, with a one hour time limit.\nDecision Tree is a family of models that defines the connections between features and their possible conse-quences as a tree-like structure of nodes, where each internal node represents a feature (or attribute) test, each branch represents the outcome of the test, and each leaf node represents a class label (for classification) or a con-tinuous value (for regression). The algorithm splits the dataset into subsets based on the feature that results in the highest information gain or the lowest impurity, according to the popular Scikit-learn library, but not limited to these options. This process occurs recursively until no further division is possible. We performed hyperparameter optimization using TPOT, with a one hour time limit.\nLinear Regression and Logistic Regression. model the relationship between a set of features by fitting a linear equation to the observed data. Linear regression aims to minimize the sum of squared differences between"}, {"title": null, "content": "the observed and predicted values. It is widely used due to its simplicity, interpretability, and efficiency, making it suitable for a variety of applications such as trend analysis, forecasting, and inferential statistics.\nGPLearn, or Genetic Programming for scikit-learn, is an open-source Python library that applies genetic programming to ML tasks. It allows for the automatic generation of analytical (formula-based) models by evolving programs to fit data, using principles inspired by biological evolution such as selection, mutation, and crossover (also known as symbolic regression). GPlearn can be used for regression and classification problems, where it evolves mathematical expressions to optimize predictive performance. We configured the genetic programming algorithm with 50 generations and a parsimony coefficient of 0.001 to control model complexity.\nSVM is a supervised ML algorithm used for classification and regression tasks. SVM works by finding the optimal hyperplane that best separates the data points of different classes in a high-dimensional space. This hyperplane maximizes the margin, which is the distance between the closest data points (support vectors) of each class. SVM can handle both linear and non-linear classification by using kernel functions to transform the input data into a higher-dimensional space where a linear separator can be found. Popular kernels include linear, polynomial, and radial basis functions. We performed hyperparameter optimization using TPOT, with a one hour time limit.\nKNN is a simple yet powerful supervised ML algorithm used for classification and regression tasks. It operates on the principle of similarity, where new instances are classified based on the majority class or average of the k nearest neighbors in the feature space. The algorithm does not involve explicit training; instead, it stores the entire training dataset and performs computations at prediction time. We performed hyperparameter optimization using TPOT, with a one hour time limit.\nFT-Transformer The FT-Transformer (FTT) model is a novel deep learning architecture specifically designed to handle tabular data effectively. It leverages the principles of the Transformer model, which is widely used in natural language processing, to capture the intricate relationships within tabular datasets. The model employs feature tokenization to handle numerical and categorical features and uses self-attention mechanisms to learn complex interactions between features. In our implementation, we used Optuna to optimize the hyperparameters of the FT-Transformer. As in [22], we set the number of heads to 8. We varied the number of Transformer blocks (n_blocks) between 1 and 6. The dimension of the token embeddings (d_token) was set to be a multiple of 8, ranging from 8 to 32 times 8. The dropout rate for the attention mechanism (attention_dropout) and the feed-forward network (ffn_dropout) was varied between 0.1 and 0.5. The hidden dimension in the feed-forward network (ffn_d_hidden) was set between 64 and 256, and the residual dropout (residual_dropout) was also varied between 0.1 and 0.5. The learning rate (learning_rate) was set between 10-4 and 10-2, and the batch size (batch_size) was chosen as one of 32, 64, or 128.\nResNet is a DL architecture designed to address the vanishing gradient problem in very deep neural networks. It introduces skip connections, also known as residual connections, that allow gradients to flow more effectively during training. ResNet architectures typically stack multiple residual blocks to form a deep network. The skip connections in each block enable the gradient to propagate more efficiently through the network, alleviating the vanishing gradient problem and enabling the training of very deep models. we utilized Optuna to optimize the hyperparameters by suggesting a range of values for each parameter. Specifically, we set the number of blocks (n_blocks) between 1 and 5. For the main dimension of each block (d_main), the value was set between 16 and 64, and for the hidden dimension (d_hidden) within each block, the value was set between 16 and 64. The dropout rates for the first dropout layer (dropout_first) and the second dropout layer (dropout_second) were varied between 0.1 and 0.5. The learning rate (learning_rate) was set between 10-4 and 10-2, and the batch size (batch_size) was chosen as one of 32, 64, or 128.\nMLP, a Multilayer Perceptron is a type of artificial neural network that consists of multiple layers of intercon-nected nodes (neurons). MLPs are widely used for supervised learning tasks such as classification and regression. The network typically consists of an input layer, one or more hidden layers, and an output layer. Each neuron in an MLP performs a weighted sum of its inputs, followed by the application of an activation function to produce an output. The weighted sum is calculated by multiplying each input by a corresponding weight and summing them up, usually with the addition of a bias term. The activation function introduces non-linearity into the network, enabling it to learn complex relationships in the data. To train the MLP model in this code, we implemented a MLP model using PyTorch, and optimized its hyperparameters with Optuna to minimize the mean squared error on the validation set. We set the number of layers (n_layers) between 1 and 5. For each layer, the number of units"}, {"title": null, "content": "(n_units_li) was set between 4 and 128, and dropout rates (dropout_li) for each layer were set between 0.1 and 0.5. The learning rate (learning_rate) was varied between 10-4 and 10-2, and the batch size (batch_size) was chosen as one of 32, 64, or 128.\nAutoGluon is an open-source library for automated machine learning. AutoGluon supports a wide range of ML tasks, including classification, regression, and even object detection for computer vision applications. It is built on top of the deep learning framework Apache MXNet, providing scalability and efficiency for training models on large datasets. It automatically handles tasks such as feature selection, algorithm selection, hyperparameter tuning, and model ensembling. We ran either the \"full\" library (labeled as the AutoGluon model) or by restricting it to DL architectures (labeled as AutoGluon-DL) with a limit of 200 epochs."}, {"title": "5.2 Meta-learning features", "content": "Table 8 mostly adopted from [41] and contains 20 features computationally useful for meta-learning tasks."}, {"title": "5.3 Additional results", "content": "In this section, we provide several additional results to further support the claims presented in this study. In particular, we provide the model's performance in several contexts such as larger datasets, few dimensions, and"}, {"title": "5.4 Computer resources", "content": "We ran the experiments with 15 Google Colab sessions, using the high-RAM (51 GB) configuration with CPU. The total computing time including initial attempts and robustness tests is estimated at 4,000 hours."}]}