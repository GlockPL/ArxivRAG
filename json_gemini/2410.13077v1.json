{"title": "TUNING LANGUAGE MODELS BY MIXTURE-OF-DEPTHS ENSEMBLE", "authors": ["Haoyan Luo", "Lucia Specia"], "abstract": "Transformer-based Large Language Models (LLMs) traditionally rely on final-layer loss for training and final-layer representations for predictions, potentially overlooking the predictive power embedded in intermediate layers. Surprisingly, we find that focusing training efforts on these intermediate layers can yield training losses comparable to those of final layers, with complementary test-time performance. We introduce a novel tuning framework, Mixture-of-Depths (MoD), which trains late layers as ensembles contributing to the final logits through learned routing weights. With the auxiliary distillation loss and additional normalization modules, we ensure that the outputs of the late layers adapt to language modeling. Our MoD framework, which can be integrated with any existing tuning method, shows consistent improvement on various language modelling tasks. Furthermore, by replacing traditional trainable modules with MoD, our approach achieves similar performance with significantly fewer trainable parameters, demonstrating the potential of leveraging predictive power from intermediate representations during training.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) are predominantly Transformer-based, processing sequences of input tokens by representing them as vectors and transforming them through multiple layers of transformers (Vaswani et al., 2017). Prior research has demonstrated the intermediate hidden states can carry meaningful information (Li et al., 2024), and leveraging these hidden states during decoding can improve trustworthiness (Chuang et al., 2024) and reasoning capabilities (O'Brien & Lewis, 2023). However, how to effectively utilize these intermediate layers during training remains unexplored. While each layer transformation creates new token representations added to the residual stream, only the final layer representations are used to obtain training loss. Consequently, loss minimization directly optimizes these final representations, leaving hidden representations optimized only implicitly, thereby obscuring their potential predictive power.\nIn this work, we investigate the predictive power of the late layers,\u00b9 which have proven to be task-aware in early exiting language models (Schuster et al., 2022; Din et al., 2023). We begin by training models on late layers by applying the pretrained language model heads to each layer's output to calculate the loss. Our initial observations indicate that the training loss curves for the later layers started at higher values but eventually converged to similar levels, aided by simple distillation losses with respect to the output of the last layer, even without incorporating the weights of the subsequent layers (Figure 1). Figure 2 demonstrates that the trained \"models\" at these layers can even provide complementary evaluation results. These findings"}, {"title": "MIXTURE-OF-DEPTHS", "content": "Recent language models consist of an embedding layer, n stacked transformer layers L, and an affine layer $\\phi(\\cdot)$ for predicting the next-word distribution, often referred to as the language model head (Geva et al., 2022; Luo & Specia, 2024). We aim to identify a layer range k, where the last k layers carry higher-level task-aware information and can map hidden states to meaningful predictive logits (Belrose et al., 2023). For an LLM with n layers, we define the set of the last k layers as K = {Ln-k+1,Ln-k+2,..., Ln}. As"}, {"title": "EARLY-EXIT FOR LATE LAYERS", "content": "The idea of applying language heads directly to the hidden states of the middle layers, known as early exit (Teerapittayanon et al., 2016; Elbayad et al., 2020; Schuster et al., 2022), has proven effective even without a special training process (Kao et al., 2020). The residual connections (He et al., 2016) in transformer layers allow hidden representations to evolve gradually, enabling the formation of task-aware representations without abrupt changes.\nGiven a sequence of tokens {x1, x2,..., Xt\u22121}, the embedding layer first converts the tokens into a sequence of vectors Ho = {h_1^{(0)},..., h_t^{(0)}\\}, where h_t^{(0)} \u2208 Rd and d is the hidden state dimension. This sequence Ho is then processed successively by each transformer layer, with the output of the j-th layer denoted as Hj. The vocabulary head $\\phi(\\cdot)$ then outputs the logits lt of the next token xt over the vocabulary set V:\nl(x_t | X<t) = \\phi(N_p(h^{(N)}_t)),  x_t \\in V.\nHere, Np is the pre-trained normalization module before the vocabulary head. This method is often considered a form of logit lens (Nostalgebraist, 2020), which uses the vocabulary head to probe into inner representations. However, the trainable predictive power of these representations remains unexplored. In \u00a72.2, we show how to combine the train-time predictive power of late layers with final layer logits."}, {"title": "MoD ROUTING NETWORK", "content": "Instead of applying $\\phi(\\cdot)$ only on the final layer, we incorporate the predictive power of late layers into the final prediction. We want to route the most informative representation for training to the final logit"}, {"title": "LATE LAYERS ADAPTATION BY NORMALIZATION AND DISTILLATION", "content": "Directly combining the logits of late layers using the LM head can result in worse training loss at the start of tuning (Figure 1). Previous works (Belrose et al., 2023) have attempted to learn an affine matrix Ae to map hidden states of layer l to the input space of the LM head. We aim to investigate more efficient adaptation methods while minimizing interference with model predictions and avoiding excessive additional trainable parameters.\nInspired by normalization studies in neural networks and the effectiveness of tuning the normalization module for domain adaptation (Zhao et al., 2023), we propose tuning an additional normalization module for each late layer as a simple yet powerful adaptation method. We set the additional normalization module Nk to match the architecture of the pretrained Np. For instance, in the LLaMA2 model (Touvron et al., 2023b), we follow the LayerNorm setting (Ba et al., 2016). The learnable parameters in the normalization, Yk and Bk, are trained individually for each k-th late layer to ensure specific adaptation for each layer.\nFollowing our assumption in \u00a72.2, we treat each of the k - 1 late layers (excluding the final layer) as smaller models, with the final layer as the larger model with the most predictive power. We use the final layer as the teacher model to supervise the output of earlier layers for adaptation. We define a teacher-enforced distillation loss that measures the difference between the predictions of the intermediate models and the final layer's predictions. The distillation loss is computed as the sum of the KL divergence between each intermediate layer's output distribution P\u2081 and the final layer's output distribution Pn:\nL_{distill} = \\sum_{i=0}^{k-2} KL(P_i || P_n),\nwhere Pi is the output distribution of layer i, and Pn is the output distribution of the final layer. The final loss is then the sum of the task loss and the distillation loss:\nL_{MOD} = L_{task} + \\lambda L_{distill},\nwhere $\\lambda$ is a hyperparameter that controls the weight of the distillation loss. By tuning with the normalization modules and distillation loss, we adapt the k - 1 layer representations to be more suitable for the language modeling task, ensuring their contributions are aligned with the original task loss."}, {"title": "EXPERIMENTS", "content": "We evaluate the MoD framework on two types of language modeling tasks: arithmetic reasoning and commonsense reasoning. The MoD framework minimally increases trainable parameters and can be integrated with any existing training method, as the hidden state dimensions remain consistent during training. We use LORA (Hu et al., 2022) as our base tuning method, which has been shown to reduce the number of tunable parameters while maintaining performance comparable to full finetuning. We define a single LoRA layer as LLORA. We use two baselines:\n1. The model tuned with LoRA excluding the last k layers, denoted as LoRA\u00acK.\n2. The model tuned with LoRA on all layers, denoted as LoRAall.\nThe notation LoRAall represents the model tuned with LoRA applied to all layers, including the last k layers which is identical to LoRA\u00acK + LLORA \u00d7 |K| specified in the tables.\nAs shown in Table 1, MoD consistently improves performance when applied on top of LoRAall with minimally added parameters. Though MoD is not designed as an additional training architecture, experiments also demonstrate that it can replace the LoRA module while retaining similar or even better performance"}, {"title": "ARITHMETIC REASONING", "content": "Arithmetic reasoning includes seven datasets for math word problems: AddSub (Hosseini et al., 2014), AQUA (Ling et al., 2017), GSM8K (Cobbe et al., 2021a), MAWPS (Koncel-Kedziorski et al., 2016), SingleEq (Koncel-Kedziorski et al., 2015), and SVAMP (Patel et al., 2021). Models need to generate chain-of-thought (Wei et al., 2022) reasoning steps before the final answer. We replicate the experimental setup from Hu et al. (2023) on a combined dataset of these seven arithmetic reasoning tasks with LM-generated chain-of-thought steps (MATH7K) and report scores on all test sets. We only evaluate the correctness of the final numeric or multiple-choice answer. Details of the dataset are provided in Appendix A.1. For MATH7K, we set k to 3 for both LLaMA-1 and LLaMA-2 models across all datasets. Note that different models and datasets might benefit from a different value of k, or we could dynamically select k during training, which we leave for future research.\nThe results in Table 1 show that the MoD framework consistently improves performance on arithmetic reasoning tasks when applied on top of LoRA\u00acK. Furthermore, MoD alone, even with only 0.19% added parameters, provides competitive performance with LoRAall. These results validate our approach of utilizing late layer during training to enhance model performance in complex reasoning tasks."}, {"title": "\u0421\u043e\u043cMONSENSE REASONING AND GENERAL LANGUAGE MODELLING", "content": "Commonsense reasoning is evaluated using four datasets: the Challenge Set and Easy Set of ARC (Clark et al., 2018), BoolQ (Clark et al., 2019), and OBQA (Mihaylov et al., 2018a). These tasks are formulated"}, {"title": "INSTRUCTION FOLLOWING", "content": "We evaluate the effectiveness of MoD across LLaMA-7B and LLaMA2-7B for instruction tuning using a 10K subset of the cleaned Alpaca dataset (Taori et al., 2023). The fine-tuned models are then assessed on the MT-Bench benchmark (Zheng et al., 2023) by generating responses to a predefined set of 80 multi-turn questions. These responses are subsequently evaluated by GPT-4 (OpenAI, 2023), which reviews each answer and assigns a numerical score out of 10.\nOur findings indicate that the performance of MoD is comparable to the LoRA baseline, though no significant performance gains were observed. We hypothesize that this could be due to the nature of instruction-following tasks, which may require more processing in the later layers to appropriately format instructed responses. MoD, by contrast, bypasses these processes while maintaining similar performance. Future work may explore how the MoD framework can be adapted to enhance instruction-following capabilities in language modeling by learning more robust instruction-tuning mechanisms."}, {"title": "LEARNED ROUTING PATTERN ACROSS TOKENS", "content": "In this section, we analyze the routing patterns learned with MoD for the K ensemble layers during training. With a Gaussian-initialized routing network, we measure the sparsity of the weights across the training tokens, i.e., how many weights are close to zero. We calculate the proportion of weights below a threshold, $\\epsilon$, which we set to 1 \u00d7 10-5. A lower level of sparsity often implies that the model is selectively using current routes while ignoring others, leading to the discussion in \u00a74.2. We also record the mean and variance to measure the tendency and dispersion for each k route, as detailed in Appendix C.1. We evaluate MoD trained on top of LoRA and MoD trained without k LoRA layers using the LLaMA 7B model on the ARC easy subset. According to Figure 4, we notice an interesting learned pattern discrepancy between MoD trained with or without LoRA layers. When trained without k LoRA layers, the sparsity score for the last layer remains low, while the sparsity level of layer 30 is high initially and then decreases, and the sparsity level of layer 29 increases through training. This suggests that the model generally learns to rely more on the last two layers' outputs, especially the last layer for the ensemble. However, when trained with k LORA layers in the ensemble, the sparsity level of the last layer is much higher, while the levels for the other two layers remain low. This indicates that the additional trainable modules inside MoD help the late layers contribute more to the ensembles and become more task-informative, aligning with our assumption in \u00a72.3. Notably, both methods yield better performance than the baseline according to Table 3.2, suggesting that there is still significant predictive potential through different weight combinations for the ensembles."}, {"title": "MOD SPARSE ROUTING", "content": "As shown in \u00a74.1, the sparsity level of the MoD routing output can be high, suggesting the potential for sparse routing vectors during inference. In this section, we investigate whether we can train the MoD with the GTopk variant introduced in \u00a72.2. Ideally, if the routing can be sparse without compromising the ensemble effectiveness, we can improve inference efficiency by enabling early"}, {"title": "ABLATION STUDY ON ADAPTATION MODULES", "content": "Figure 1 shows the loss curve of late layers when optimizing the loss based on the last layer output when late layers are optimized implicitly and the loss curves when optimizing the loss on each late layer output with the distillation loss Ldistill w.r.t. the last layer. We find that the introduction of Ldistill makes bringing the activations to the final logits prediction at the very start of the training more stable within the first 200 steps even model's pretraining paradigm is not doing so. We also conduct an ablation study to analyze the impact of Ldistill with the adaptation module Nk introduced in \u00a72.3. We name different ablations of MoD as follows: 1) MoD w.o. Nk: Instead of using a trained normalization for each ensemble layer, we use the pre-trained normalization before the LM head for all k ensemble layers."}, {"title": "RELATED WORK", "content": "Early Exit in Transformer Layers Early exit strategies in language models are often explored to improve efficiency. Several works focus on enhancing inference efficiency by terminating computation at dynamically-decided earlier layer outputs (Xin et al., 2020; Schuster et al., 2022). A common approach for adapting intermediate layer output to language modeling involves training an affine transformation (Belrose et al., 2023; Din et al., 2023). Early exit strategies have also been explored for interpretability, analyzing the linearity properties of transformer components (Geva et al., 2023; Hernandez et al., 2023). However, the utilization of intermediate layer output during training remains largely unexplored. A recent work (Elhoushi et al., 2024) applies layer dropout and an early exit loss to increase the accuracy of early exits, but its primary focus is still on inference efficiency. To the best of our knowledge, our work is the first to utilize early exit logits together with the final layer logits to incorporate task-aware representations from intermediate layers into the loss calculation.\nLogit-Level Arithmetic Operations at the logit level have proven effective in steering the output of LLMs (Luo & Specia, 2024). From a multi-model perspective, there has been a growing body of work focusing on \"mixturing\" the abilities of different trained models in line with the Mixture-of-Experts framework (Shazeer et al., 2017; Jiang et al., 2024). Liu et al. (2021); Gera et al. (2023) have also shown the effectiveness of ensembling logits from multiple LMs. From a single model perspective, contrasting logits from different layers of a model (Chuang et al., 2024; Gera et al., 2023) has shown promising performance improvements in the trustworthiness of generation and addressing the resource-intensive issues of larger models (Liu et al., 2024). Our work builds upon logit-level arithmetic and follows the line of ensembling logits, focusing not on a multi-model perspective but rather on utilizing the late layers' outputs within a single model for tuning. This approach has been considered only during inference in previous work."}, {"title": "CONCLUSION", "content": "In this paper, we explored the predictive power of late layers in LLMs and introduced the Mixture-of-Depths (MoD) tuning framework. By tuning LLMs using ensembled logits from MoD routing and adaptation components, we demonstrated consistent improvements in reasoning tasks with minimal additional parameters."}, {"title": "LIMITATIONS", "content": "Future work could explore dynamic layer selection methods and refine the layer range of the MoD framework to maximize its potential, rather than relying on empirical selection. Additionally, more effective tuning of other hyperparameters, such as \u5165, the weight of the distillation loss, should be investigated. Improving the effectiveness of MoD on a broader range of tasks, such as instruction following, remains an open question, as discussed in \u00a73.3. Extending MoD to evaluate its performance on bidirectional LLMs, such as RoBERTa (Liu et al., 2019), would help determine if it generalizes well across different transformer-based language models. Due to hardware limitations, our experiments were restricted to LLMs at the 7B scale. Exploring the impact of MoD on larger models is an important direction for future research."}, {"title": "ARITHMETIC REASONING", "content": "We conduct extensive empirical studies on fourteen benchmark datasets, focusing on two categories of reasoning problems: Arithmetic Reasoning: 1. GSM8K (Cobbe et al., 2021b): A dataset comprising high-quality, linguistically diverse grade school math word problems created by human problem writers. 2. SVAMP (Patel et al., 2021): A benchmark of one-unknown arithmetic word problems designed for up-to-4th grade students, created by making simple modifications to problems from an existing dataset. 3. MultiArith (Roy & Roth, 2016): A dataset featuring math word problems that require multiple reasoning steps and operations. 4. AddSub (Hosseini et al., 2014): A collection of arithmetic word problems focused on addition and subtraction. 5. AQUA (Ling et al., 2017): A dataset of algebraic word problems accompanied by natural language rationales. 6. SingleEq (Koncel-Kedziorski et al., 2015): A set of grade-school algebra word problems that map to single equations of varying lengths."}, {"title": "\u0421\u043e\u043cMONSENSE REASONING", "content": "We trained our method on four commonsense reasoning dataset separately. They are: 1. BoolQ (Clark et al., 2019): A question-answering dataset containing 15,942 naturally occurring yes/no questions generated in"}, {"title": "EXPERIMENT SETTINGS", "content": "We mainly follow the experimental settings of Hu et al. (2023). We maintain a batch size of 16 and set the learning rate for all methods to 3e-4. Each method is fine-tuned for two epochs on each dataset."}, {"title": "ANALYSIS", "content": "In this section, we analyze the routing patterns learned with MoD for the K ensemble layers during training. We measure the mean and variance of the weights across the training tokens. A higher mean suggests that the model consistently chooses this route, while a higher variance indicates variability in the routes learned for different tokens. We evaluate MoD trained on top of LoRA and MoD trained without k LoRA layers using the LLaMA 7B model on the ARC easy subset."}, {"title": "MEAN AND VARIANCE FOR ROUTING PATTERN ACROSS TOKENS", "content": "According to Figure 7, for the mean metric, we observe a reverse trend with respect to the sparsity score in Table 4. This aligns with our intuition that when the sparsity score of the current route is low, the routing value will be relatively larger than other routes. For the variance, we notice that when MoD is trained without k LoRA layers, it maintains a high variance throughout tuning. This suggests that many tokens are trained to select this route, but they are dynamically changing. When MoD is trained with LoRA, both the variance and mean levels stay low, indicating that the other two layers primarily contribute to the final ensemble logits. This suggests that the additional k trainable module within the MoD framework provides more predictive power to the ensemble layers, aligning with our analysis in \u00a74.1."}, {"title": "MOD SPARSE ROUTING WITH DIFFERENT TOP-K VALUES", "content": "We also select a larger ensemble layer range to increase opportunities for early exit. We use k = 4 for this section, with results for BoolQ, OBQA, and MAWPS presented in Figure 8"}]}