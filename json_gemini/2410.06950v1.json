{"title": "Faithful Interpretation for Graph Neural Networks", "authors": ["Lijie Hu", "Tianhao Huang", "Lu Yu", "Wanyu Lin", "Tianhang Zheng", "Di Wang"], "abstract": "Currently, attention mechanisms have garnered increasing attention in Graph Neural Networks (GNNs), such as Graph Attention Networks (GATs) and Graph Transformers (GTs). It is not only due to the commendable boost in performance they offer but also its capacity to provide a more lucid rationale for model behaviors, which are often viewed as inscrutable. However, Attention-based GNNs have demonstrated instability in interpretability when subjected to various sources of perturbations during both training and testing phases, including factors like additional edges or nodes. In this paper, we propose a solution to this problem by introducing a novel notion called Faithful Graph Attention-based Interpretation (FGAI). In particular, FGAI has four crucial properties regarding stability and sensitivity to interpretation and final output distribution. Built upon this notion, we propose an efficient methodology for obtaining FGAI, which can be viewed as an ad hoc modification to the canonical Attention-based GNNs. To validate our proposed solution, we introduce two novel metrics tailored for graph interpretation assessment. Experimental results demonstrate that FGAI exhibits superior stability and preserves the interpretability of attention under various forms of perturbations and randomness, which makes FGAI a more faithful and reliable explanation tool.", "sections": [{"title": "Introduction", "content": "Graph Neural Networks (GNNs) have experienced a rapid proliferation, finding versatile applications across a spectrum of domains such as communication networks (Jiang 2022), medical diagnosis (Ahmedt-Aristizabal et al. 2021), and bioinformatics (Yi et al. 2022). As the adoption of deep neural networks continues to expand within these diverse fields, the significance of interpreting deep models to improve decision-making and establish trust in their results has grown commensurately. In this context, Attention-based GNNs, such as Graph Attention Network (GAT) (Veli\u010dkovi\u0107 et al. 2018) and Graph Transformer (GT) (Dwivedi and Bresson 2021), have arisen as a prominently utilized approach for model interpretation, which offers the promise of deciphering complex relationships within graph-structured data using attention vectors, providing valuable insights into the decision-making processes of deep models.\nHowever, the maturation of GNNs has revealed a critical concern that the increasing prevalence of adversarial attacks targeting graph-structured data (Dai et al. 2018; Z\u00fcgner, Akbarnejad, and G\u00fcnnemann 2018; Z\u00fcgner et al. 2020) has also exposed potential vulnerabilities in Attention-based GNNs. This necessitates a meticulous examination of Attention-based GNNs' robustness and reliability of interpretation in real-world applications. As illustrated in Figure 1, our investigations have uncovered that even slight perturbations, such as the addition of edges to a node and the introduction of a new node, can significantly change the predictions and interpretations of GAT. Notably, these perturbations lead to shifts in top indices of the attention vector, resulting in significant alterations to the prediction for node 7. This inherent instability fundamentally undermines Attention-based GNNs' capacity to serve as a truly faithful tool for model explanation.\nInstability has been a recognized problem in interpretation methods within deep learning. An unstable interpretation is susceptible to noise in data, hindering users from comprehending the underlying logic of model predictions. Furthermore, instability reduces the reliability of interpretation as a diagnostic tool, where even slight input perturbations can drastically alter interpretation outcomes (Ghorbani, Abid,"}, {"title": "Related Work", "content": "The relevant prior work encompasses two key domains: robustness in graph neural networks and faithfulness in explainable methods. See Appendix for more related work.\nRobustness in Graph Neural Networks. There has been some research investigating the robustness of graph-based deep learning models. Verma and Zhang (2019) first derived generalization bounds for GNNs and explored stability bounds on semi-supervised graph learning. Z\u00fcgner and G\u00fcnnemann (2019) and Bojchevski and G\u00fcnnemann (2019) independently developed effective methods for certifying the robustness of GNNs, and they also separately introduced novel robust training algorithms for GNNs. However, to the best of our knowledge, there has been limited research conducted on the robustness of GAT. Brody, Alon, and Yahav (2021) indeed pointed out that their proposed GATv2 has an advantage over the vanilla GAT in terms of robustness. Nevertheless, their approach sacrifices the interpretability that is present in the original attention.\nFaithfulness in Explainable Methods. Faithfulness is an essential property that explanation models should satisfy, which refers to the explanation should accurately reflect the true reasoning process of the model (Wiegreffe and Pinter 2019; Herman 2017; Jacovi and Goldberg 2020; Lyu, Apidianaki, and Callison-Burch 2022; Hu et al. 2024a,b,c). Faithfulness is also related to other principles like sensitivity, implementation invariance, input invariance, and completeness (Yeh et al. 2019). The completeness means that an explanation should comprehensively cover all relevant factors to prediction (Sundararajan, Taly, and Yan 2017). The other three terms are all related to the stability of different kinds of perturbations. The explanation should change if heavily perturbing the important features that influence the prediction (Adebayo et al. 2018), but stable to small perturbations (Hu et al. 2023b,a; Gou et al. 2023; Lai et al. 2023). Thus, stability is an important factor in explanation faithfulness. Some preliminary work has been proposed to obtain stable interpretations. For example, Yeh et al. (2019) theoretically analyzed the stability of post-hoc interpretation and proposed to use smoothing to improve interpretation stability. Yin et al. (2022) designed an iterative gradient descent algorithm to get counterfactual interpretation, which shows desirable stability. While some prior work has proposed techniques to attain stable interpretations, these approaches have primarily focused on post-hoc interpretation and text data, making them less directly applicable to the complexities of graph data. In this context, our work bridges these research areas by addressing the stability problem in graph interpretation, providing a rigorous definition of Faithful Graph Attention-based Interpretation (FGAI), proposing efficient methods aligned with this definition, and introducing novel evaluation metrics tailored to the unique demands of graph interpretation, thereby advancing the faithfulness and robustness of graph-based deep learning models."}, {"title": "Toward Faithful Graph Attention-based Interpretation", "content": "Graph Attention Networks\nBefore clarifying the definition of the model with stable interpretability, we first introduce the attention layer in Attention-based GNNs. Note that our approach can be directly applied to any graph attention-based network. Here, we follow the notations in (Veli\u010dkovi\u0107 et al. 2018) for typical GAT. We have input nodes with features h"}, {"title": "Faithful Graph Interpretation", "content": "Motivation: As we discussed in the introduction section, for Attention-based GNNs, both the interpretability of the attention vector and the performance of the classifier are unstable to slight perturbations on the graph structure, invading it as a faithful explanation tool. Thus, we aim to find a variant of the attention vector that is stable while preserving the key characteristics of intermediate representation explanation and prediction. We call such a variant \"stable attention\". Before diving into our rigorous definition of \"stable attention\", we first need to intuitively think about what properties it should have. The first one is keeping the similar interpretability as the vanilla attention. In the vanilla attention vector for a node, we can easily see that the rank of its entries can reflect the importance of its neighboring nodes. Thus, \u201cstable attention\u201d should also have almost the same order for each entry as in the vanilla one. However, keeping the rank for all entries is too stringent, motivated by the fact that the interpretability and the prediction always rely on the most important entries. Here, we can relax the requirement to keep the top-k indices almost unchanged.\nIn fact, such a property is not enough as it could also be unstable. Modeling such instability is challenging as the perturbation could be caused by multiple resources, which is significantly different from adversarial robustness. Our key observation is that wherever a perturbation comes from, it will subsequently change the attention vector. Thus, if the interpretability of \u201cstable attention\u201d, i.e., the top-k indices, is resilient to noise, we can naturally think it is robust to those different perturbations.\nHowever, it is still insufficient. The main reason is that keeping interpretability does not indicate keeping the prediction performance. This is because keeping interpretability can only guarantee the rank of indices unchanged, but cannot ensure the magnitude of these entries, which determine the prediction, unchanged. For example, suppose the vanilla attention vector is (0.5, 0.3, 0.2), then the above \u201cstable attention\u201d might be (0.9, 0.051, 0.049), which is significantly different from the original one. Based on these, we should also enforce the prediction performance, i.e., the output distribution, to be almost the same as the vanilla one. Moreover, its output distribution should also be robust to perturbations shown in Figure 4 in Appendix.\nBased on our above discussion, our takeaway is that \u201cstable attention\u201d should make its top-k indices and output distribution almost the same as vanilla attention while also being robust to perturbations. In the following, we will translate the previous intuitions into rigorous mathematical definitions. Specifically, we call the above \u201cstable attention\u201d as Faithful Graph Attention-based Interpretation (FGAI). We first give the definition of top-k overlaps to measure the interpretability stability."}, {"title": "Toward Faithful Graph Attention-based Interpretation", "content": "Definition 0.1 (Top-k overlaps). For vector x \u2208 Rd, we define the set of top-k component Tk (\u00b7) as follow,\nTk (x) = {i : i \u2208 [d] and {|{xj \u2265 xi : j\u2208 [d]}| \u2264 k}}.\nAnd for two vectors x, x', the top-k overlap function Vk (x,x') is defined by the overlapping ratio between the top-k components of two vectors,\u00b9 i.e., Vk (x,x') =\n|Tk (x) \u2229 Tk (x')|.\nMoreover, from (1) and (2), we can see the vanilla attention vector wi for node i depends on input hi, thus we can think of it as a function of hi denoted as w(hi). Similarly, the output distribution yi is a function of an attention vector w and the input node hi, denoted as y(hi, w). Based on the above notation, we can formally define a (node-level) FGAI as follows.\nDefinition 0.2 ((Node-level) FGAI). We call a map w is a (D, R, \u03b1, \u03b2, k1,k2)-Faithful Graph Attention-based Interpretation mechanism for the vanilla attention mechanism w if it satisfies for any node i with feature hi,\n\u2022 (Similarity of Interpretability) Vk\u2081 (w(hi), w(hi)) \u2265 \u03b2\u2081 for some 1 \u2265 \u03b2\u2081 \u2265 0.\n\u2022 (Stability of Interpretability) Vk\u2082 (w(hi), w(hi) + p) \u2265 B2 for some 1 \u2265 2 \u2265 0 and all ||p|| \u2264 R1, where || \u00b7 || is a norm and R1 \u2265 0.\n\u2022 (Closeness of Prediction) D(y(hi, w), y(hi, w)) \u2264 \u03b11 for some a1 \u2265 0, where D is some loss or divergence, y(hi, w) = \u03c3(\u03a3j\u2208N; (W(hi))jWhj).\n\u2022 (Stability of Prediction) D(y(hi, w), y(hi, w + \u03b4)) \u2264 02 for all ||8|| < R2, where D is some loss or divergence, and y(h, w+\u03b4) = \u03c3(\u2211j\u2208N;(W(hi)+d);Whj), R2 \u2265 0, where a = min{1,2}, \u03b2 = max{\u03b21, \u03b22}, and R = min{R1, R2}.\nIt is notable that the dimensions of x, x' in the above definition could be different."}, {"title": "Faithful Graph Interpretation", "content": "Moreover, for any input node with feature h, we call w(h) as a (D, R, \u03b1, \u03b2, k1, k2)-Faithful Graph Attention-based Interpretation for this node.\nNote that in the previous definition, there are several parameters. There are two properties - similarity and stability for prediction and interpretability, respectively.\nThe first two conditions are the similarity and stability of the interpretability. We ensure \u1ff6 has similar interpretability with the vanilla attention mechanism. There are two parameters, k\u2081 and \u1e9e1. k\u2081 could be considered prior knowledge, i.e., we believe the top-k\u2081 indices of attention will play the most important role in making the prediction, or their corresponding k\u2081 features can almost determine its prediction. B\u2081 measures how much interpretability does \u1f66 inherit from vanilla attention. When B\u2081 = 1, then this means the top-k\u2081 order of the entries in w is the same as it is in vanilla attention. Thus, B\u2081 should close to 1. The term stability involves two parameters, R\u2081 and B2, which correspond to the robust region and the level of stability, respectively. Ideally, if w satisfies this condition with R\u2081 = \u221e and B2 = 1, then \u1f66 will be extremely stable w.r.t any randomness or perturbations. Thus, in practice, we wish R\u2081 to be as large as possible and \u1e9e2 to be close enough to 1.\nThe last two conditions are the similarity and stability of prediction based on attention. In the third condition, 01 measures the closeness between the prediction distribution based on w and the prediction distribution based on vanilla attention. When &\u2081 = 0, then w = w. Therefore, we hope a1 to be as small as possible. It is also notable that D is the loss to measure the closeness of two distributions, which could also be some divergence. Similarly, the term stability involves two parameters, R2 and 02, which correspond to the robust region and the level of stability, respectively. Ideally, if w satisfies this condition with R2 = \u221e and a2 = 0, then w will be extremely stable w.r.t any randomness or perturbations. Thus, in practice, we wish R\u2082 to be as large as possible and 02 to be sufficiently small.\nThus, based on these discussions, we can see Definition 0.2 is consistent with our above intuition on graph faithful attention, and it is reasonable and well-defined.\nMore discussions on top-k conditions. In fact, the inclusion of a top-k condition within the graph interpretation framework serves as a dual-purpose mechanism that significantly motivates our research efforts. Firstly, this condition plays a pivotal role in retaining the most salient characteristics of node information. By focusing on the top-k elements, we ensure that the most critical aspects of the graph structure and relevance are preserved, allowing for a more focused and interpretable process. This selective retention of key features is particularly valuable in complex and large-scale graph data, where identifying the most influential nodes can lead to more meaningful insights and informed decision-making.\nSecondly, the top-k condition acts as a potent sparsity accelerator for graph computation. By narrowing down the attention scope to the most relevant nodes, we effectively reduce the computational burden associated with graph processing. In our extensive experiments, we empirically demonstrate that the computational cost incurred"}, {"title": "Faithful Graph Interpretation", "content": "by our approach is well-contained, requiring no more than 150% of the GPU memory utilized by the vanilla GAT and GT. This efficiency gain not only ensures the scalability of our approach but also underscores its practical viability for real-world applications. In essence, our research is driven by the dual aspiration of enhancing the interpretability and computational efficiency of graph-based deep learning models. By integrating the top-k condition, we strike a balance between retaining essential information and optimizing computational resources, thereby empowering graph interpretation with greater fidelity and scalability.\nDifferences with adversarial robustness. While both FGAI and adversarial robustness consider perturbations or noises on graph data. There are many critical differences: (1) In adversarial robustness, the goal is only to make the prediction unchanged under perturbation on the input (the property of stability of prediction), while in FGAI we should additionally keep the prediction close to the vanilla one. (2) Not only the prediction, we should also make the interpretability stable and close to the vanilla attention. Due to these additional conditions, our method for achieving FGAI is totally different from the methods in adversarial robustness, such as certified robustness or adversarial training. See Section for details. (3) The way of modeling robustness in FGAI is also totally different from adversarial robustness. In adversarial robustness, it usually models the robustness to perturbation on input data. However, in FGAI, due to the requirement on interpretability, i.e., the top-k indices of the vector, we cannot adopt the same idea. Firstly, directly requiring the top-k indices robust to perturbation on the input will make the optimization procedure challenging (which is a minimax optimization problem) as the top-k indices function is non-differentiable, and calculating the gradient of the attention is costly. Secondly, rather than perturbation on input data, as we mentioned, the perturbation could come from multi-resources, such as a combined perturbation of edges and additional nodes. Thus, from this perspective, our stability of interpretability is more suitable for \u201cstable attention\u201d."}, {"title": "Finding FGAI", "content": "In the last section, we presented a rigorous definition of faithful node-level graph interpretation. To find such an FGAI, we propose to formulate a minimax optimization problem that involves the four losses associated with the four conditions in Definition 0.2.\nBased on Definition 0.2, we can see that a natural way to find a w is to minimize the prediction closeness with the vanilla attention while also satisfying other three conditions:\nmin En [D(y(h, w), y(h,w))]\ns.t. En [max Vk\u2082 (w(h), w(h) + \u03c1)] \u2265 \u03b22,\nP||<R\nEh [Vk\u2081 (w(h), w(h))] \u2265 \u03b21,\nEn[max D(y(h, \u0169), y(h, \u0169 + \u03b4))] \u2264 \u03b12.\n||P||<R\nHowever, the main challenge is that the top-k overlap function is non-differential, which makes the optimization problem hard to train. Thus, we seek to design a surrogate loss"}, {"title": "Finding FGAI", "content": "Lk() for -Vk(), which can be used in training. The details are in the Appendix .\nFinal objective function and algorithm. By transforming each constraint to a regularization, we can finally get the following objective function. Details are in the Appendix .\nmin En [D(y(h, w), y(h, w)) + 11Lk\u2081 (w, w)\n+ 12 max D(y(h, \u0169), y(h, w + \u03b4))\n||8||\u2264R\n+ 13 max Lk2 (\u1ff6, \u1f66 + \u03c1)],\n||p||\u2264R\nwhere Lk (\u00b7) is defined in (10). The second term top-k is substituted by a surrogate loss, which is differentiable and practical to compute via backpropagation. This term guarantees the explainable information of the attention. The third term D is a min-max optimization controlled by hyperparameter A2 in order to find the maximum tolerant perturbation to the attention layer, which affects the final prediction. The final term Lk2 is also a min-max optimization to find the maximum tolerant perturbation to the intrinsic explanation of the attention layer. In other words, we derive a robust region using this min-max strategy.\nTo solve the above minimax optimization problem, we propose Algorithm 1. See Appendix for more details."}, {"title": "Experiments", "content": "Experimantal Setup\nDatasets. Here, we employ five node classification datasets covering small to large-scale graphs to compare our approach with baseline methods. Cora, Citeseer and Pubmed (Sen et al. 2008), Amazon CS and Amazon Photo (McAuley et al. 2015), Coauthor CS and Coauthor Physics (Shchur et al. 2018), ogbn-arXiv (Wang et al. 2020; Mikolov et al. 2013). We present the statistics of the selected datasets in Table 3 and Table 4.\nBaselines. We employ three attention-based models, namely GAT (Veli\u010dkovi\u0107 et al. 2018), GATv2 (Brody, Alon, and Yahav 2021), and GT (Dwivedi and Bresson 2021), as base models for the downstream tasks, namely node classification and link prediction. Due to space constraints, the results of the link prediction task are presented in Appendix. Following the work of (Zheng et al. 2021), we compare our method with vanilla methods and two general defense techniques: layer normalization (LN) and adversarial training (AT). We refer readers to Appendix for implementation details.\nPost-hoc vs Self-explained. Here, we need to clarify the distinction between our approach and post-hoc learning frameworks such as PGExplainer (Luo et al. 2020), SubgraphX (Yuan et al. 2021), RC-Explainer (Wang et al. 2022), etc. As shown in the research by Kosan et al. (2023)., these explainers often exhibit significant instability when facing perturbations. Our method, on the contrary, has a distinct advantage in this regard. In other words, our approach is specifically designed to address this issue."}, {"title": "Experiments", "content": "Evaluation Metrics. For assessing the model's performance, we utilize the F1-score as a primary metric. To comprehensively assess the stability of the model when encountering various forms of perturbations and randomness, as well as its ability to maintain the interpretability of attention, we present graph-based Jensen-Shannon Divergence (g-JSD) and Total Variation Distance (g-TVD) as metrics for measuring model stability. Furthermore, we design two additional novel metrics, namely, F+ slope and Fslope, to fully evaluate the interpretability of the graph attention-based neural networks.\ni) Graph-based Total Variation Distance. g-TVD is a metric employed to quantify the dissimilarity between two probability distributions. It is defined mathematically as\ng-TVD(y, \u1ef9) = 1/2\u039d \u03a3|Yi-Yi|,\nwhere y and \u1ef9 represent the outputs of the model before and after perturbation of the graph, respectively. Note that compared to the original TVD, here we rescale it by dividing |N|, where |N| is the number of nodes in the graph.\nii) Graph-based Jensen-Shannon Divergence. g-JSD is a metric used to quantify the similarity or dissimilarity between two probability distributions. It is defined as follows,\ng-JSD(w, \u1ff6) = -(KL(w||W) + KL(\u0169||\u016b)),\nwhere w and w represent the attention vectors of the model before and after perturbation of the graph, w = (w + w), and KL is Kullback-Leibler Divergence which can be defined as KL(w||W) = \u2211i=1wi log(). Note that compared to the original JSD, here we rescale it by dividing |E|, where |E| is the number of edges in the graph.\niii) F-slope. Building upon the foundation laid by (Yuan et al. 2022), we propose F+ slope and F slope to better evaluate the interpretability of different Attention-based GNNs. In detail, let T represent the set of nodes correctly classified by the model. Then, we rank the importance of edges based on the attention values assigned to each edge by the trained model. A higher attention value indicates greater importance, while a lower value suggests lesser importance for the respective edge on the graph. Next, we utilize M as a mask for important edges and M as a mask for unimportant edges, where r represents the proportion of deleted edges in the graph. In this way, Face (r) for positive perturbation and Face(r) for negative perturbation can be computed as\nFatec(r)= \u03a4\u03a3\u0399\u039c\nFace(r)= \u03a4\u03a3\u0399\u039c\nFrom the above definitions we can see Face and Face are functions of r, our metrics Fstone and Flo is the slop"}, {"title": "Results", "content": "Stability Evaluation\nWe first conduct a comprehensive assessment of the stability against perturbations in interpretability and the output performance of FGAI in comparison to other baseline methods. In this setting, we primarily utilize g-JSD to measure attention vector stability (stability of interpretability) and g-TVD to evaluate the stability of output distributions (stability of prediction). To assess the stability of node interpretation, we initiate the testing process by randomly selecting a small number of specific nodes within the graph and introducing perturbations by adding edges and neighboring nodes to these selected nodes. We then calculate the g-JSD of the attention vectors of these two cases. Similarly, we can also evaluate the g-TVD of the two output distributions.\nThe results are presented in Table 1 and Table 2. We can observe that FGAI achieves the best F1 score on almost all base models and datasets, both before and after perturbations. This indicates the stability of its performance. Moreover, FGAI exhibits smaller g-JSD values compared to the vanilla model, signifying enhanced attention stability, while a similar trend is observed in the g-TVD metrics, demonstrating the stability of predictions. These findings collectively emphasize the effectiveness of FGAI in bolstering both attention and prediction stability, thereby positioning it as a reliable and robust approach for graph interpretation. However, while LN and AT exhibit lower g-JSD and g-TVD values than FGAI on some datasets, their F1 scores are very low. This suggests that they do not guarantee the stability of the performance of the base model.\nInterpretability Evaluation\nWe calculate the proportion (dependent variable, i.e., Face and Face in the figure) of nodes that the model originally predicts correctly and still predicts correctly after removing edges based on attention weights according to the specified proportion (independent variable, i.e., r in the figure). Figure 2 presents our experimental results regarding positive and negative perturbations. Due to space limitations, more results can be found in Appendix (Figure 5, 6, 7, and Table"}, {"title": "Results", "content": "6). On the clean graph, all methods across all models exhibit interpretability: the proportion of nodes predicted correctly in the face of negative perturbation decreases much less than when facing positive perturbation. However, on the graph after an slight injection attack, we can observe that vanilla GAT and GATv2 show a positive slope when facing negative perturbation, indicating a complete loss of interpretability in the presence of perturbations. The same situation occurs with GT+LN and GT+AT, and since LN and AT already exhibit a significant decrease in prediction accuracy, they are not faithful interpretations.\nOnly FGAI, across all base models, demonstrates interpretability on both clean and attacked graphs: its performance decreases slowly when facing negative perturbation and rapidly when facing positive perturbation. This indicates that even on perturbed graphs, FGAI can ensure stable attention distributions from the base model, providing a faithful explanation. Due to space constraints, additional analyses on the experimental results can be found in Appendix .\nVisualization Results\nOur visualization results come in two forms: (1) We showcase the attention values of a selected subset of nodes and edges from the graph data before and after perturbation, as depicted in Figure 3. (2) We highlight the top-k most important neighboring nodes and edges connected to a specific node before and after perturbation, as presented in Figure 3. Due to space limitations, some of the visualization results are presented in Appendix (Figure 8 and 9). In Figure 3, we observe that the attention values of GAT significantly decrease after perturbation, especially in the Amazon-photo and Amazon-cs datasets, compromising the topological importance structure of the graph and thus losing their interpretability for the graph. In contrast, FGAI maintains relatively consistent attention values before and after perturbation, demonstrating its superior performance in resisting perturbations while preserving interpretable stability. Likewise, Figure 3 reinforces our model's stability in retaining the most crucial characteristics of neighboring node information, reinforcing the faithfulness of our approach.\nAblation Study and Computation Cost\nWe also conducted experiments on the ablation study in Table 7 as well as time and storage complexities comparison in Table 8, see Appendix and for details. These findings reinforce the central roles played by the top-k loss and TVD"}, {"title": "Results", "content": "loss in improving the faithfulness of the model. For computational cost, we observe that, compared to these methods, FGAI is a more efficient and cost-effective approach. Our method incurs relatively minimal additional overhead compared to the vanilla model, offering an efficient and cost-saving solution for the graph defense community."}, {"title": "Conclusions", "content": "In this study, we investigated the faithfulness issues in Attention-based GNNs and proposed a rigorous definition for FGAI. FGAI is characterized by four key properties emphasizing stability and sensitivity, making it a more reliable tool for graph interpretation. To assess our approach rigorously, we introduced two novel evaluation metrics for graph interpretations. Results show that FGAI excels in preserving interpretability while enhancing stability, outperforming other methods under perturbations and adaptive attacks."}, {"title": "Optimization", "content": "In the definition section, we presented a rigorous definition of faithful node-level graph interpretation. To find such an FGAI, we propose to formulate a min-max optimization problem that involves the four conditions in Definition 0.2. Specifically, the formulated optimization problem takes the third condition (closeness of prediction) as the objective and subjects it to the other three conditions. Thus, we can get a rough optimization problem according to the definition. Specifically, we first have\nmin En D(y(h, w),y(h, w)).\nEquation (5) is the basic optimization goal. That is, we want to get a vector that has a similar output prediction with vanilla GAT for all input nodes h. If there is no further constraint, then we can see the minimizer of (5) is just the vanilla GAT w. We then consider constraints for this objective function:\nWh s.t. max D(y(h, \u0169), y(h, \u0169 + \u03b4)) \u2264 \u03b1,\n||8||\u2264R\nVk\u2081 (w(h), w(h)) \u2265 \u03b2,\nmax Vk\u2082 (w(h), w(h) + \u03c1) \u2265 \u03b2.\n||p||\u2264R\nEquation (6) is the constraint of stability, Equation (7) corresponds to the condition of similarity of explanation, and Equation (8) links to the stability of explanation. Combining equations (5)-(8) and using regularization to deal with constraints, we can"}, {"title": "Optimization", "content": "get the following objective function.\nmin En [D(y(h, w), y(h, w)) + \u03bb\u2081(\u03b2 \u2013 Vk\u2081 (w(h), w(h)))\n+ 2(max D(y(h, \u0169), y(h, \u0169 + \u03b4)) \u2013 \u03b1) + 13 max (\u03b2 \u2013 Vk\u2082 (\u0169(h), \u0169(h) + \u03c1))],||8||<R\nwhere \u03bb\u2081 > 0, X2 > 0 and 13 > 0 are hyperparameters.\nFrom now on, we convert the problem of finding a vector that satisfies the four conditions in Definition 0.2 to a min-max stochastic optimization problem, where the overall objective is based on the closeness of prediction condition with constraints on stability and top-k overlap.\nNext, we consider how to solve the above min-max optimization problem. In general, we can use the stochastic gradient descent-based methods to get the solution of outer minimization and use PSGD (Projected Stochastic Gradient Descent) to solve the inner maximization. However, the main difficulty is that the top-k overlap function Vk\u2081 (w(h), w(h)) and Vk\u2082 (\u0169(h), \u0169(h) + p) is non-differentiable, which impede us from using gradient descent. Thus, we need to consider a surrogate loss of -Vk(\u00b7). Below, we provide details.\nProjected gradient descent to find the perturbation \u03b4. Motivated by (Madry et al. 2018), we can interpret the perturbation as the attack to \u1f66 via maximizing \u03b4. Then, \u03b4 can be updated by the following procedure in the p-th iteration.\n1\n\u03b4\u03c1 = \u03b4-1+pBD(y(h, w),y(h, w + 8\u22121));\nB\n8 = arg min ||8 \u2013 \u0431\u0440||,\np\nwhere Yp is a parameter of step size for PGD, Bp is a batch and |Bp| is the batch size. Using this method, we can derive the optimal 8* in the t-th iteration of outer minimization for the inner optimization. Specifically, we find a d as the maximum tolerant of perturbation w.r.t w in the t-th iteration of outer SGD.\nTop-k overlap surrogate loss. Now, we seek to design a surrogate loss Lk(\u00b7) for \u2013Vk(\u00b7) which can be used in training. We takes the Lk (w) for \u2013Vk(\u1ff6, w) as an example. To achieve this goal, one possible naive surrogate objective might be some distance (such as l\u2081-norm) between \u1ff6 and w, e.g., L(\u1ff6) = ||\u1ff6 \u2013 w||1. Such a surrogate objective seems like it could ensure the top-k overlap when we obtain the optimal or near-optimal solution (i.e., w arg min L(w) and w \u2208 arg min \u2013Vk\u2081 (\u1ff6, w))."}, {"title": "Optimization", "content": "However, it lacks consideration of the top-k information, which makes it a loose surrogate loss. Since we only need to ensure high top-k indices overlaps between \u1ff6 and w, one improved method is minimizing the distance between w and w constrained on the top-k entries only instead of the whole vectors, i.e., ||wsk \u2013 ws ||1, where wsk, W\u03c2 \u2208 Rk is the vector w and \u1ff6 constrained on the indices set She respectively and She is the top-k indices set of w. Since there are two top-k indices sets, one is for w, and the other one is for w, we need to use both of them to involve the top-k indices formation for both vectors. Thus, based on our above idea, our surrogate can be written as follows,\nLk (w, w) = (||ws - ws || + ||\u016bs - ws||1).\nNote that besides the l\u2081-norm, we can use other norms. However, in practice, we find l\u2081-norm achieves the best performance. Thus, throughout the paper, we only use l\u2081-norm.\nProjected gradient descent to find the perturbation p. Similarly, we can use the PGD and the surrogate loss of Lk (.) to get the optimal p* in the t-th iteration of outer SGD.\n1\nPq = Pq-1+Tq\n\u0422\u0430 \u0412\u03bf \u03a3\u00a3k\u2082 (W, W + Pq\u22121);\nheBq\np = arg min ||p \u2013 Pq||,\nwhere Tq is a parameter of step size for PGD, Bq is a batch and |Bq| is the batch size.\nFinal objective function and algorithm. Based on the above discussion, we can derive the following overall objective function\nmin Ex [D(y(h, w), y(h, w)) + 11Lk\u2081 (w, \u1ff6)\n+ 12 max D(y(h, \u0169), y(h, \u0169 + \u03b4)) + 13 max Lk\u2082 (\u1ff6, \u1f66 + \u03c1)"}]}