{"title": "What Do Machine Learning Researchers Mean by \u201cReproducible\u201d?", "authors": ["Edward Raff", "Michel Benaroch", "Sagar Samtani", "Andrew L. Farris"], "abstract": "The concern that Artificial Intelligence (AI) and Machine\nLearning (ML) are entering a \"reproducibility crisis\" has\nspurred significant research in the past few years. Yet with\neach paper, it is often unclear what someone means by \"repro-\nducibility\". Our work attempts to clarify the scope of \"repro-\nducibility\" as displayed by the community at large. In doing\nso, we propose to refine the research to eight general topic\nareas. In this light, we see that each of these areas contains\nmany works that do not advertise themselves as being about\n\"reproducibility\", in part because they go back decades be-\nfore the matter came to broader attention.", "sections": [{"title": "Introduction", "content": "The Artificial Intelligence (AI) and Machine Learning (ML)\ncommunities are increasingly concerned with the \"repro-\nducibility\" of their fields. This has come on the heels of a\nreproducibility crisis noted in many others. We will refer to\nthis overarching concern, that the science of research is be-\ning done with some error rate, as a generic scientific rigor\nconcern. This concern is justified, and it is increasingly chal-\nlenging to evaluate the state of research around scientific\nrigor due to confused and incompatible usage of the same\nfew terms like \"reproducibility\" (Plesser 2018).\nDue to confusing and often inconsistently used terminol-\nogy in the literature, it is challenging to understand precisely\nwhat issues of scientific rigor the community is tackling. In\nlight of these issues, we propose a new formulation of cur-\nrent scientific rigor research by surveying the current arti-\ncles by the topics they cover. In doing so, we observe that\nmany historical works tackled these very issues \u2013 with dif-\nferent motivations and no particular thematic name like \u201cre-\nproducibility\u201d as it was not an urgent concern at the time.\nIn this article, we will expand the ACM's proposed termi-\nnology of Repeatability, Reproducibility, and Replicability\nwhich we find useful, although still insufficient to capture\nthe breadth of work done to date. Our contribution classi-\nfied current AI / ML research in scientific rigor into eight\naspects we label as repeatability, reproducibility, replicabil-\nity, adaptability, model selection, label/data quality, meta &\nincentive, and maintainability. These eight aspects are de-"}, {"title": "The Current Scope of Work", "content": "Our literature survey identifies at least eight primary aspects\nof scientific rigor studied in the AI/ML literature. Each ma-\njor sub-section will repeat one of the eight rigors defined\nin Table 1, and include further delineation for nuanced sub-\ncategories that are present or noteworthy in the literature.\nA key criterion for being included in Table 1 is that the pa-\nper must self-identify itself as being about \"repeatability, re-\nproducibility, or replicability\u201d since those are the three pre-\nexisting terminologies used (interchangeably) in the prior\nliterature. These delimitations reflect the current scope of\nwhat researchers actively consider \u201creproducibility\" con-\nsider worthy of study and effort. As our bibliography will\nshow though, many more papers exist in these topical areas\nthat were published before 2017, and thus before the AI/ML\ncommunities started to put renewed effort into the issue of\nscientific rigor. We include such articles in the discussion of\neach section to establish the full scope of available work and\nto connect the current reproducibility-themed motivation to\nits historical precedents, as the historical literature is often\nunknown to existing researchers on this topic.\nBefore we detail these aspects, it is worth noting\nthat many existing articles are best summarized as opin-\nion pieces with varying degrees of formalization of\ntheir arguments. Most of these articles propose strate-\ngies or arguments on how to obtain \"reproducibility\",\nwithout evidence of effect (Gundersen and Kjensmo 2018;\nMatsui and Goya 2022; Publio, Esteves, and Zafar 2018;\nTatman, Vanderplas, and Dane 2018; Sculley et al. 2018;\nVollmer et al. 2020; Drummond 2009, 2018; Raff and Farris\n2022; Lin 2022). The contents of our article are focused on"}, {"title": "Repeatability", "content": "Repeatability concerns the authors who obtain the same re-\nsults using the original source code and data. Interesting\nquestions in repeatability include how to develop code and\nsystems that make it easy for the developer to keep track\nof how they came to their experimental results from an ex-\nperimental design perspective (Gardner, Brooks, and Baker\n2018; Paganini and Forde 2020). In Human-Computer In-\nteraction (HCI) research, there has been significant study\non the iterative development nature of computational note-\nbooks (e.g., Jupyter) that are widely used in AI/ML de-\nvelopment processes. These notebooks can be prone to"}, {"title": "Reproducibility", "content": "Reproducibility alters repeatability by requiring that a differ-\nent individual/team be able to produce the same results us-\ning the original source code and data. This is a high focus of\nthe AI/ML community and incentivization of Open Source\nSoftware (OSS) by major conferences and paper submission\nquestionnaires/guidelines. Current work can be divided into\nthose that explore surface-level issues such as unquantified\nproposals or exact procedure reproductions, vs those that at-"}, {"title": "Replicability", "content": "Replicability concerns the ability of a different person/team\nto produce qualitatively similar results from the original arti-\ncle by writing their own code and potentially different data.\nThe aspect of replicability is highly understudied, likely due\nto the challenges this aspect presents. Replicability can be\nsubdivided into empirical replicability and theoretical repli-\ncability.\nEmpirical Replicability Empirical Replicability requires\nre-implementing a target method's code from scratch, which\nis a labor-intensive process. Notable work in this direc-\ntion was done by (Raff 2019), who attempted to reimple-\nment 255 papers, and computed features to quantify what\nproperties correlated with a replicable paper. Smaller scale\nreplications have also been performed (Belz et al. 2022), in-\ncluding a volunteer effort by ReproducedPapers.org collect-\ning some (most are reproduction attempts) Replicability at-\ntempts (Yildiz et al. 2021) based on which a thorough study\nin IR has been performed (Wang et al. 2022). (Chen et al.\n2022b; Ganesan et al. 2021) identified issues with a specific\ncommon baseline method XML-CNN in multi-label learn-\ning. Famously, (Henderson et al. 2018) replicated recent re-\ninforcement learning results and discovered various aspects,\nsuch as the seed and scale of rewards, that significantly al-\ntered the perception of improvement.\n(Johnson, Pollard, and Mark 2017) Replicate studies in\nmortality prediction in a healthcare context, highlighting\nthe difficulty of producing comparable results when repli-\ncation also requires collecting new data of the same in-\ntrinsic nature (that is, patient data in this context). Textual\ndescriptions presented in the original studies were found\nto be insufficient for collecting new data that would repli-\ncate. (Hegselmann et al. 2018) extended this observation by\nshowing how to produce replicable data collection schemes\nfor survival analysis against medical repositories such as\nSEER.\nWe are not aware of other work within AI/ML on em-\npirical replicability. This state of affairs is common to\nother (relatively) code-free disciplines such as medicine\n(Ioannidis 2005) economics (Camerer et al. 2016) social sci-\nences (Camerer et al. 2018). In these disciplines, replication\nstudies are necessary and representative because they are the\nleast costly way to evaluate a result. Other aspects of scien-\ntific rigor have a significantly lower barrier to entry, largely\nbecause AI/ML has a large open-source culture.\nTheoretical Replicability More recent work has ad-\nvanced a theoretical definition of replicability in terms of\nconstraints on the output distribution as a function of the\ninput distribution. (Impagliazzo et al. 2022; Kalavasis et al.\n2023; Bun et al. 2023) have developed much of this founda-\ntion by showing various desirable statistical properties such\nas that Total Variation (TV) between outputs drawn from the\nsame distribution using the same algorithm (i.e., a congruent\ndefinition of replicability) is equivalent to results in approx-\nimate differential privacy and robust statistics. This idea has\nsince been expanded to bandits (Esfandiari et al. 2023b), op-\ntimization (Zhang et al. 2023), clustering (Esfandiari et al."}, {"title": "Model Selection", "content": "Model selection deals with the common task of AI/ML pa-\npers: given two competing methods (one of which may\nbe the paper's own proposal), how do we conclude which\nmethod is better? As the AI/ML literature has advanced sig-\nnificantly through the presentation of empirically \"better\"\nalgorithms, it is not surprising that most historical and cur-\nrent work has focused on the question of model selection.\nThis includes how to pick and evaluate criteria to decide\n\"better\", how to build benchmarks for a problem, and the\nprocess to determine \u201cbetter\u201d given criteria in a statistically\nsound way. There are also multiple resurgences of this issue\nas ML is incorporated into other fields, and comparisons that\nmay be invalid in a new field occur as both communities be-\ngin to merge and discover what is/is not acceptable (Hoefler\n2022).\nEvaluation Criteria & Methodology Before selecting\nthe \"better\" of one or more methods, it is necessary first to\ndetermine how the quality of a method is determined. The\nscope of evaluation metrics and scores is larger than that\nof scientific rigor, and this article is concerned with cases\nwhere an invalid or errant procedure was identified and\nremediated. The literature in this direction is old, starting\nin the late 1990s on the various pros and cons of metrics\nlike Area Under the Curve (AUC) for evaluation (Bradley\n1997; Hand 2009; Lobo, Jim\u00e9nez-Valverde, and Real\n2008). Likewise, work has addressed issues in scoring\nfrom leaderboards (Blum and Hardt 2015), and subtle\nissues in using cross-validation to produce test scores\n(Varma and Simon 2006; Bergmeir, Hyndman, and Koo\n2018; Varoquaux 2018; Bates, Hastie, and Tibshirani 2021;\nMathieu and Preux 2024). Niche examples of the evaluation\nconcern also exist. For example, three decades of malware\ndetection performed subtle train/test leakage by adjusting\nfor a target false-positive rate incorrectly (Nguyen et al.\n2021) and time series anomaly detection scores being overly\ngenerous to \"near hits\u201d (Kim et al. 2022).\nBuilding Problem-Specific Benchmark Suites It is be-\ncoming increasingly popular to build benchmarks of mul-\ntiple datasets, pre-prepared evaluation code, and method-\nology for specific problem domains (Blalock et al. 2020;\nEggensperger et al. 2021; Sun et al. 2020; Saul et al. 2024;\nLiu et al. 2024; Ordun et al. 2021; Kebe et al. 2021). Such\na benchmark construction is popular, although it has yet\nto evolve into a science of how to build benchmarks, with\nlimited study at a macro level (Koch et al. 2021). Some\ndomains may require additional thought to how methods"}, {"title": "Adaptability and its Second-Class Status", "content": "Adaptability is the study of a different person/team, using\nthe original code but applying it to their own and different\ndata. Very little work on scientific rigor in AI/ML focuses\non Adaptability. To be clear, many prior works have studied\nthe question of generalization in machine learning, of which\nthere is recent evolution due to the advance of deep learning\n(Zhang et al. 2017). However, generalization assumes some\nform of intrinsic relationship (usually I.I.D.) between the\ntraining and testing distribution. Under Adaptability, there\nis no direct train/test split to compare. Instead, it is a ques-\ntion of the methodology's effectiveness on an entirely differ-\nent statistical distribution at training and test time. Thus, our\nconcern is more focused on the practical, real-world issues\nthat enable or inhibit a method to generalize. Our contention\nis the lack of study on adaptability is one of the most glar-\ning shortfalls in the current scientific rigor literature, with"}, {"title": "Label & Data Quality", "content": "Label & Data Quality is focused on the reliability of data and\nlabel acquisition, error rates, and working to understand how\nthey occur, detect them, or work around them. The distinc-\ntion we make from research in inferring a single label from\nlabelers is that scientific rigor is concerned with the pro-\ncess of how labels are collected, defined, and have impacted\nresearch conclusions (e.g., inferring a 99% accurate model\nwhen labels have a 5% noise level would imply a failure in\nprocess). Many works today identify these issues long after\ndataset construction, in part due to the high accuracies now\nbeing achieved, making the errors more pronounced. For ex-"}, {"title": "Meta and Incentives", "content": "Very few papers have studied incentives for scientific rigor.\nThe study of the scientific process itself is often termed\nmetascience and, when applied to AI/ML research, would\nfall into this category. Such research could include basic\nstudies of incentives, drivers of scientific rigor, and surveys\nacross various AI/ML research domains. Sample studies fo-\ncused on drivers such as the rate of data and code sharing in\ncomputational linguistics (Wieling, Rawee, and van Noord\n2018) and the use of statistical testing (Dror et al. 2018).\nRelated work has found that code sharing and replica re-\nsearch are correlated with higher citations (Raff 2022;\nObadage, Rajtmajer, and Wu 2024), although most meta-\nstudies have looked at the rate of code sharing in their sub-\ndisciplines (McDermott et al. 2021; Olszewski et al. 2023;\nArvan, Pina, and Parde 2022a; Arvan, Do\u011fru\u00f6z, and Parde\n2023; Cavenaghi et al. 2023). A unique aspect of code avail-\nability is studied by (Storks et al. 2023), who perform a\nuser study with students on the time and difficulty fac-\ntors for students to reproduce the results of three NLP pa-\npers. Another study focuses on how evaluation and com-\nparison practices evolve throughout the Machine Transla-\ntion community (Marie, Fujita, and Rubino 2021). The last\nwork we are aware of challenged the treatment of repli-\ncability as a binary \"yes/no\" question and instead sug-\ngested a survival model, where replicability is a function\nof time/effort (Raff 2021) and quantifying a reproducibility\nscore (Belz, Popovic, and Mille 2022)."}, {"title": "Maintainability", "content": "Maintainability is similar to Repeatability, in that we are\nconcerned with producing the same results with the orig-\ninal authors (though new users could also occur) us-\ning the original code and data. The key difference that\ndistinguishes maintainability is that time is a factor, as\nthe ability to repeat results degrades over time as nu-\nances of labels (Inel, Draws, and Aroyo 2023) or depen-"}, {"title": "Connections between Rigor Types", "content": "Having defined a set of eight rigor types that are being\nworked on, we further elaborate on our perception of con-\nnections between these rigors. In particular, there are direct\nand indirect relationships, which are summarized in Figure 1\nwith solid and dashed lines, respectively."}, {"title": "Direct Relationships", "content": "The most obvious, and intuitive connections are from re-\npeatability to reproducibility to repeatability, as each re-\nquires a progressive step of difficulty from the prior. If a sin-\ngle person/team cannot repeat their own experiments, there\nis no reason to believe that a different person with the same\ncode would be able to reproduce those results. Extended fur-\nther, if they cannot reproduce the results with the original\ncode, there is no special reason to believe that by writing\ntheir own code or using different data, they would be able to\nreplicate the results.\nLess obvious are the interactions between maintainabil-\nity, repeatability, and replicability. The first is the two-way\nrelationship between repeatability and maintainability. If an\nAI/ML system is not repeatable, it cannot be maintainable,"}, {"title": "Indirect Relationships", "content": "Beyond the general influence of meta- and incentives-based\nrigor having a relationship to all parts of scientific rigor,\nwe can further draw other connections that are of particu-\nlar note. The most straightforward of these is that of model\nselection on repeatability, reproducibility, and replicability,\neach of which will often incorporate the model selection\ntask as part of the motivation for why the proposed work\nshould be used (i.e., it was demonstrated to \"be better\" than\nsomething prior). Thus, by its nature, different approaches\nto model selection will influence each. For example, the\nuse of random search as a hyperparameter tuning method\n(Bergstra and Bengio 2012) is potentially a hindrance to\nreplicability due to higher variance, even if it is easily re-\npeatable and reproducible given the original code with initial\nseed values for the pseudorandom number generator.\nUpstream from this concern is then label and data qual-\nity, which will influence what features are selected. This\nis particularly notable as many datasets reach high accura-\ncies where \"errors\" in the model's predictions are discov-"}, {"title": "Conclusions", "content": "We have synthesized eight current directions in the literature\nof scientific rigor for machine learning, disentangling them\nfrom the commonly repeated moniker of \u201creproducibility\"\nand thus quantified the proportion of each type as studied\ntoday. These rigor types have been further characterized by\ntheir interactions/dependencies with each other."}, {"title": "Papers Used", "content": "In the following table, we cite all the papers (101) we categorized as being significantly related to AI/ML and self-identify\nas being about \u201creproducibility\u201d in some sense to build Table 1. Articles are listed in no particular order. Articles that did not\nself-identify as being about \u201creproducibility\" are excluded as the purpose was to determine what researchers currently identify,\nthough we found many more articles that discuss the same themes/issues in both historical and current literature (as the long\nbibliography demonstrates). The category assigned is our subjective call as to the most important/prominent theme of the paper,\nthough many papers discussed more than one issue."}]}