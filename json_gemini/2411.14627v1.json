{"title": "Generative AI for Music and Audio", "authors": ["Hao-Wen Dong"], "abstract": "Generative Al has been transforming the way we interact with technology and consume content. In the next decade, AI technology will reshape how we create audio content in various media, including music, theater, films, games, podcasts, and short videos. In this dissertation, I introduce the three main directions of my research centered around generative AI for music and audio: 1) multitrack music generation, 2) assistive music creation tools, and 3) multimodal learning for audio and music. Through my research, I aim to answer the following two fundamental questions: 1) How can AI help professionals or amateurs create music and audio content? 2) Can AI learn to create music in a way similar to how humans learn music? My long-term goal is to lower the barrier of entry for music composition and democratize audio content creation.", "sections": [{"title": "Introduction", "content": "AI is the study of how to make computers do things at which, at the moment, people are better. Generative Al has been transforming the way we interact with technology and consume content. The recent success of large language model-based chatbots (e.g., OpenAI's ChatGPT\u00b9 and Google's Gemini\u00b2), AI assistants (e.g., GitHub and Microsoft Copilot) and text-to-image generation systems (e.g., Adobe Firefly,\u00b3 Midjourney and Stable Diffusion) showcases how AI-powered technology can be integrated into professional workflows and boost human productivity. In the next decade, generative Al technology will also reshape how we create audio content in the $2.3 trillion global entertainment industry, including the music, film, TV, podcast and gaming sectors. Take AI-powered music creation for example: On one hand, we have witnessed major progress in automatic music composition (Briot et al., 2017; Huang et al., 2020), which has long been considered as a grand challenge of AI. On the other hand, our expectations of AI Music today has expanded to cover the whole music creation process-from composition, arrangement, sound production, recording to mixing (De Man et al., 2019). With a growing momentum in both academia and industry, AI-powered audio creation has been gaining attention in the broader AI community.\nMy research springs from two fundamental questions: 1) How can AI help professionals or amateurs create music and audio content? 2) Can AI learn to create music in a way similar to how humans learn music? \nhumans learn music? From a musical perspective, technology has always been a driving factor of music evolution. For example, the study of acoustics and musical instrument making fostered the development of classical music; the invention of synthesizers and drum machines helped popularize electronic music. I am thus interested in exploring how the latest AI technology can empower artists to create novel contents. From a technical perspective, music possesses a unique complexity in that music follows rules and patterns while being creative and expressive at the same time. I am thus fascinated about the idea of building intelligent systems that can learn, create and play music like humans do. I envision the future development of AI Music to be a two-way process\u2014new technology creates new music; new music inspires new technology.\nMotivated by this belief, I study a wide range of topics centered around Generative AI for Music and Audio, including multitrack music generation (Dong et al., 2018a; Dong et al., 2017; Dong and Yang, 2018; Dong et al., 2023a; Xu et al., 2023; Liu et al., 2018a), automatic instrumentation (Dong et al., 2021), automatic arrangement (Dong et al., 2018a; Liu et al., 2018a), automatic harmonization (Yeh et al., 2021), music performance synthesis (Dong et al., 2022), text-queried sound separation (Dong et al., 2023d), text-to-audio synthesis (Dong et al., 2023c; Dong et al., 2023b) and symbolic music processing software (Dong et al., 2018b; Dong et al., 2020).\nMy research can be categorized into three main directions: 1) multitrack"}, {"title": "Multitrack Music Generation", "content": "Researchers have been working on automatic music composition for decades, and it has long been viewed as a grand challenge of AI. I started this thread of research on multitrack music generation in 2017. Back then, prior work on deep learning-based music generation had focused on generating melodies, lead sheets (i.e., melodies and chords) or four-part chorales Briot et al., 2017. However, modern pop music often consists of multiple instruments or tracks. To make deep learning technology applicable in modern music production workflow, it is important to modernize deep learning models for multitrack music generation.\nWitnessing the lack of infrastructure for symbolic music generation when conducting these research projects, I developed libraries for processing symbolic music to consolidate the infrastructure of music generation research (Dong et al., 2018b; Dong et al., 2020). The Python toolkits I developed to process symbolic music for machine learning applications have been widely used in the field. With these libraries, researchers can easily download commonly used datasets programmatically and be liberated from reimplementing tedious data processing routines. The toolkits also allowed me to conduct the first large-scale experiment that measures the cross-dataset generalizability of deep neural networks for music (Dong et al., 2020). This work will be presented in Chapter 2.\nMoreover, I study efficient music generation model with an eye to enabling real time improvi-sation or near real time creative applications. In Dong et al., 2023a, I proposed the Multitrack Music"}, {"title": "Assistive Music Creation Tools", "content": "Music creation today is still largely limited to professional musicians for it requires a certain level of knowledge in music theory, music notation and music production tools. Apart from generating new music content from scratch, another line of my research focuses on developing AI-augmented tools to assist amateurs to create and perform music. My long-term goal along this research direction is to lower the entry barrier of music composition and make music creation accessible to everyone.\nFor example, in (Dong et al., 2021), I developed the first deep learning model for automatic instrumentation. Instrumentation refers to the process where a musician arranges a solo piece for a certain ensemble such as a string quartet or a rock band. This can be challenging for amateur composers as it requires domain knowledge of each target instrument. In this work, I proposed a new machine learning model that can produce convincing instrumentation for a solo piece by framing this problem as a sequential multi-class classification problem. Such an automatic instrumentation system can suggest potential instrumentation for amateur composers, especially useful when arranging for an unfamiliar ensemble. Further, the proposed model can empower a musician to play multiple instruments on a single keyboard at the same time. This work will be presented in Chapter 4.\nAnother example is my work on music performance synthesis (Dong et al., 2022). While synthe-sizers play a critical role and are intensively used in modern music production, existing synthesizers either requires an input with expressive timing or allows only monophonic inputs. In light of the similarities between text-to-speech (TTS) and score-to-audio synthesis, I showed in this work that we can adapt a state-of-the-art TTS model for music performance synthesis. Moreover, I proposed a novel mechanism to enable polyphonic music synthesis. This work represents the first deep learning based"}, {"title": "Multimodal Learning for Audio and Music", "content": "The third line of my research focuses on multimodal learning for audio and music. Sound is an integral part of movies, dramas, documentaries, podcasts, games, short videos and audiobooks. In these media, audio and music production tools need to interact with inputs from other modalities such as text and images, and thus multimodal models are critical in enabling controllable creation tools for music and audio in these applications.\nAlong this direction, I have worked on text-queried sound separation (Dong et al., 2023d) and text-to-audio synthesis (Dong et al., 2023c; Dong et al., 2023b). Unlike existing work that relies on a large amount of paired audio-text data, I explore a new direction of approaching bimodal learning for text and audio through leveraging the visual modality as a bridge. The key idea behind my study is to combine the naturally-occurring audio-visual correspondence in videos and the multimodal repre-sentation learned by contrastive language-vision pretraining (CLIP). Based on this idea, I developed the first text-queried sound separation model that can be trained without any text-audio pairs (Dong et al., 2023d). Text-queried sound separation aims to separate a specific sound out from a mixture of sounds given a text query, which has many downstream applications in audio post-production such as editing and remixing. I showed that the proposed model can successfully learn text-queried sound separation using only noisy unlabeled videos, and it even achieves competitive performance against a supervised model in some settings. Moreover, I built the first text-to-audio synthesis model that requires no text-audio pairs during training (Dong et al., 2023c; Dong et al., 2023b). The proposed model learns to synthesize audio given text queries, which can find applications in video and audio editing software. One of the key benefits of the approach studied in my work lies in its scalability to large video datasets in the wild as we only need unlabeled videos for training. This work will be presented in Chapters 6 and 7."}, {"title": "Dissertation Organization", "content": "The rest of this dissertation is organized as follows: Chapters 2 to 7 are reprints of six conference papers published during my PhD. Specifically, Chapter 2 is a reprint of \"MusPy: A Toolkit for Symbolic"}, {"title": "MusPy: A Toolkit for Symbolic Music Generation", "content": "In this paper, we present MusPy, an open source Python library for symbolic music generation. MusPy provides easy-to-use tools for essential components in a music generation system, including dataset management, data I/O, data preprocess-ing and model evaluation. In order to showcase its potential, we present statistical analysis of the eleven datasets currently supported by MusPy. Moreover, we conduct a cross-dataset generalizability experiment by training an autoregressive model on each dataset and measuring held-out likelihood on the others-a process which is made easier by MusPy's dataset management system. The results provide a map of domain overlap between various commonly used datasets and show that some datasets contain more representative cross-genre samples than others. Along with the dataset analysis, these results might serve as a guide for choosing datasets in future research. Source code and documentation are available at https://github.com/salu133445/muspy.\nRecent years have seen progress on music generation, thanks largely to advances in machine learning (Briot et al., 2017). A music generation pipeline usually consists of several steps-data collection, data preprocessing, model creation, model training and model evaluation, as illustrated in Figure 2.1. While some components need to be customized for each model, others can be shared across systems. For symbolic music generation in particular, a number of datasets, representations and metrics have been proposed in the literature (Briot et al., 2017). As a result, an easy-to-use toolkit that implements standard versions of such routines could save a great deal of time and effort and might lead to increased reproducibility. However, such tools are challenging to develop for a variety of reasons.\nFirst, though there are a number of publicly-available symbolic music datasets, the diverse organization of these collections and the various formats used to store them presents a challenge. These formats are usually designed for different purposes. Some focus on playback capability (e.g., MIDI), some are developed for music notation softwares (e.g., MusicXML (Good, 2001) and LilyPond (LilyPond n.d.)), some are designed for organizing musical documents (e.g., Music Encoding Initiative (MEI)(Hankinson et al., 2011)), and others are research-oriented formats that aim for simplicity and read-ability (e.g., MuseData (Hewlett, 1997) and Humdrum (Huron, 1997). Oftentimes researchers have to implement their own preprocessing code for each different format. Moreover, while researchers can implement their own procedures to access and process the data, issues of reproducibility due to the inconsistency of source data have been raised in (Bittner et al., 2019) for audio datasets.\nSecond, music has hierarchy and structure, and thus different levels of abstraction can lead to different representations (Dannenberg, 1993). Moreover, a number of music representations designed specially for generative modeling of music have also been proposed in prior art, for example, as a sequence of pitches (Mozer, 1994; Eck and Schmidhuber, 2002; Boulanger-Lewandowski et al., 2012;Roberts et al., 2018), events (Oore et al., 2020; Huang et al., 2019; Donahue et al., 2019; Huang and Yang, 2020), notes (Mogren, 2016) or a time-pitch matrix (i.e., a piano roll) (Yang et al., 2017; Dong et al.,2018a).\nFinally, efforts have been made toward more robust objective evaluation metrics for music generation systems (Yang and Lerch, 2018) as these metrics provide not only an objective way for"}, {"title": "Related Work", "content": "Few attempts, to the best of our knowledge, have been made to develop a dedicated library for music generation. The Magenta project (Magenta n.d.) represents the most notable example. While MusPy aims to provide fundamental routines in data collection, preprocessing and analysis, Magenta comes with a number of model instances, but is tightly bound with TensorFlow (Abadi et al., 2016). In MusPy, we leave the model creation and training to dedicated machine learning libraries, and design MusPy to be flexible in working with different machine learning frameworks.\nThere are several libraries for working with symbolic music. music21 (Cuthbert and Ariza,2010) is one of the most representative toolkits and targets studies in computational musicology.While music21 comes with its own corpus, MusPy does not host any dataset. Instead, MusPy provides functions to download datasets from the web, along with tools for managing different collections, which makes it easy to extend support for new datasets in the future. jSymbolic (Mckay and Fujinaga,2006) focuses on extracting statistical information from symbolic music data. While jSymbolic can serve as a powerful feature extractor for training supervised classification models, MusPy focuses on generative modeling of music and supports different commonly used representations in music"}, {"title": "MusPy", "content": "MusPy is an open source Python library dedicated for symbolic music generation. Figure 2.2 presents the system diagram of MusPy. It provides a core class, MusPy Music class, as a universal container for symbolic music. Dataset management system, I/O interfaces and model evaluation tools are then built upon this core container. We provide in Figure 2.3 examples of data preparation and result writing pipelines using MusPy."}, {"title": "MusPy Music class and I/O interfaces", "content": "We aim at finding a middle ground among existing formats for symbolic music and design a unified format dedicated for music generation. MIDI, as a communication protocol between musical devices, uses velocities to indicate dynamics, beats per minute (bpm) for tempo markings, and control messages for articulation, but it lacks the concepts of notes, measures and symbolic musical markings. In contrast, MusicXML, as a sheet music exchanging format, has the concepts of notes, measures and symbolic musical markings and contains visual layout information, but it falls short on playback-"}, {"title": "Dataset management", "content": "MusPy provides an easy-to-use dataset management system similar to torchvision datasets(Marcel and Rodriguez, 2010) and TensorFlow Dataset (TensorFlow Datasets n.d.). Table 2.2 presentsthe list of datasets currently supported by MusPy and their comparisons. Each supported dataset comes with a class inherited from the base MusPy Dataset class. The modularized and flexible design of the dataset management system makes it easy to handle local data collections or extend support for new datasets in the future. Figure 2.4 illustrates the two internal processing modes when iterating over a MusPy Dataset object. In addition, MusPy provides interfaces to PyTorch (Paszke et al., 2019)and TensorFlow (Abadi et al., 2016) for creating input pipelines for machine learning (see Figure 2.3(a)for an example)."}, {"title": "Representations", "content": "Music has multiple levels of abstraction, and thus can be expressed in various representations. For music generation in particular, several representations designed for generative modeling ofsymbolic music have been proposed and used in the literature (Briot et al., 2017). These representations can be broadly categorized into four types\u2014the pitch-based (Mozer, 1994; Eck and Schmidhuber, 2002; Boulanger-Lewandowski et al., 2012; Roberts et al., 2018), the event-based (Oore et al., 2020; Huang et al., 2019; Donahue et al., 2019; Huang and Yang, 2020), the note-based (Mogren, 2016) and thepiano-roll (Yang et al., 2017; Dong et al., 2018a) representations. Table 2.3 presents a comparison of them. We provide in MusPy implementations of these representations and integration to the dataset management system. Figure 2.3(a) provides an example of preparing training data in the piano-roll representation from the NES Music Database using MusPy."}, {"title": "Model evaluation tools", "content": "Model evaluation is another critical component in developing music generation systems. Hence, we also integrate into MusPy tools for audio rendering as well as score and piano-roll visualizations. These tools could also be useful for monitoring the training progress or demonstrating the final results.Moreover, MusPy provides implementations of several objective metrics proposed in the literature (Mogren, 2016; Dong et al., 2018a; Wu and Yang, 2020). These objective metrics, as listed below, could be used to evaluate a music generation system by comparing the statistical difference between the training data and the generated samples, as discussed in (Yang and Lerch, 2018).\n\u2022 Pitch-related metrics\u2014polyphony, polyphony rate, pitch-in-scale rate, scale consistency, pitch entropy and pitch class entropy.\n\u2022 Rhythm-related metrics\u2014empty-beat rate, drum-in-pattern rate, drum pattern consistency and groove consistency."}, {"title": "Summary", "content": "To summarize, MusPy features the following:\n\u2022 Dataset management system for commonly used datasets with interfaces to PyTorch and Tensor-Flow.\n\u2022 Data I/O for common symbolic music formats (e.g., MIDI, MusicXML and ABC) and interfaces to other symbolic music libraries (e.g., music21, mido, pretty_midi and Pypianoroll).\n\u2022 Implementations of common music representations for music generation, including the pitch-based, the event-based, the piano-roll and the note-based representations.\n\u2022 Model evaluation tools for music generation systems, including audio rendering, score and piano-roll visualizations and objective metrics.\nAll source code and documentation can be found at https://github.com/salu133445/muspy."}, {"title": "Dataset Analysis", "content": "Analyzing datasets is critical in developing music generation systems. With MusPy's dataset management system, we can easily work with different music datasets. Below we compute the statistics"}, {"title": "Experiments and Results", "content": "In this section, we conduct three experiments to analyze the relative complexities and the cross-dataset generalizabilities of the eleven datasets currently supported by MusPy. We implement four autoregressive models\u2014a recurrent neural network (RNN), a long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997), a gated recurrent unit (GRU) network (Cho et al., 2014) and a Transformer network (Vaswani et al., 2017)."}, {"title": "Experiment settings", "content": "For the data, we use the event representation as specified in Table 2.3 and discard velocity events as some datasets have no velocity information (e.g., datasets using ABC format). Moreover, we also include an end-of-sequence event, leading to in total 357 possible events. For simplicity, we downsample each song into four time steps per quarter note and fix the sequence length to 64, which is equivalent to four measures in 4/4 time. In addition, we discard repeat information in MusicXMLdata and use only melodies in Wikifonia dataset. We split each dataset into train-test-validation sets with a ratio of 8 : 1 : 1. For the training, the models are trained to predict the next event given the previous events. We use the cross entropy loss and the Adam optimizer (Kingma and Ba, 2015). For evaluation, we randomly sample 1000 sequences of length 64 from the test split, and compute the perplexity of these sequences. We implement the models in Python using PyTorch. For reproducibility, source code and hyperparmeters are available at https://github.com/salu133445/muspy-exp."}, {"title": "Autoregressive models on different datasets", "content": "In this experiment, we train the model on some dataset D and test it on the same dataset D. We present in Figure 2.8 the perplexities for different models on different datasets. We can see that all models have similar tendencies. In general, they achieve smaller perplexities for smaller, homogeneous datasets, but result in larger perplexities for larger, more diverse datasets. That is, the test perplexity could serve as an indicator for the diversity of a dataset. Moreover, Figure 2.9 shows perplexities versus dataset sizes (in hours). By categorizing datasets into multi-pitch (i.e., accepting any number of concurrent notes) and monophonic datasets, we can see that the perplexity is positively correlated to the dataset size within each group."}, {"title": "Cross-dataset generalizability", "content": "In this experiment, we train a model on some dataset D, while in addition to testing it on the same dataset D, we also test it on each other dataset D'. We present in Figure 2.10 the perplexities for each train-test dataset pair. Here are some observations:\n\u2022 Cross dataset generalizability is not symmetric in general. For example, a model trained on LMD generalizes well to all other datasets, while not all models trained on other datasets generalize to LMD, which is possibly due to the fact that LMD is a large, cross-genre dataset.\n\u2022 Models trained on multi-pitch datasets generalize well to monophonic datasets, while models trained on monophonic datasets do not generalize to multi-pitch datasets (see the red block in Figure 2.10).\n\u2022 The model trained on JSBach Chorale Dataset does not generalize to any of the other datasets (see the orange block in Figure 2.10). This is possibly because its samples are downsampled to a resolution of quarter note, which leads to a distinct note duration distribution.\n\u2022 Most datasets generalize worse to NES Music Database compared to other datasets (see the green block in Figure 2.10). This is possibly due to the fact that NES Music Database contains only game soundtracks."}, {"title": "Effects of combining heterogeneous datasets", "content": "From Figure 2.10 we can see that LMD has the best generalizability, possibly because it is large, diverse and cross-genre. However, a model trained on LMD does not generalize well to NES Music Database (see the brown block in the close-up of Figure 2.10). We are thus interested in whether combing multiple heterogeneous datasets could help improve generalizability.\nWe combine all eleven datasets listed in Table 2.2 into one large unified dataset. Since these datasets differ greatly in their sizes, simply concatenating the datasets might lead to severe imbalance problem and bias toward the largest dataset. Hence, we also consider a version that adopts stratified sampling during training. Specifically, to acquire a data sample in the stratified dataset, we uniformly choose one dataset out of the eleven datasets, and then randomly pick one sample from that dataset.Note that stratified sampling is disabled at test time."}, {"title": "Conclusion", "content": "We have presented MusPy, a new toolkit that provides essential tools for developing music generation systems. We discussed the designs and features of the library, along with data pipeline examples. With MusPy's dataset management system, we conducted a statistical analysis and experi-ments on the eleven currently supported datasets to analyze their relative diversities and cross-dataset generalizabilities. These results could help researchers choose appropriate datasets in future research. Finally, we showed that combining heterogeneous datasets could help improve generalizability of a machine learning model."}, {"title": "Multitrack Music Transformer", "content": "Existing approaches for generating multitrack music with transformer models have been limited in terms of the number of instruments, the length of the music segments and slow inference. This is partly due to the memory requirements of the lengthy input sequences necessitated by existing representations. In this work, we propose a new multitrack music representation that allows a diverse set of instruments while keeping a short sequence length. Our proposed Multitrack Music Transformer (MMT) achieves comparable performance with state-of-the-art systems, landing in between two recently proposed models in a subjective listening test, while achieving substantial speedups and memory reductions over both, making the method attractive for real time improvisation or near real time creative applications. Further, we propose a new measure for analyzing musical self-attention and show that the trained model attends more to notes that form a consonant interval with the current note and to notes that are 4N beats away from the current step."}, {"title": "Introduction", "content": "Prior work has investigated various approaches for symbolic music generation (Briot et al., 2017; Ji et al., 2020), among which, the transformer model (Vaswani et al., 2017) has become popular given its recent successes in piano music generation (Huang et al., 2019; Huang and Yang, 2020; Hsiao et al., 2021; Muhamed et al., 2021). At the core of a transformer model is the self-attention mechanism that allows the model to dynamically attend to different parts of the input sequence and"}, {"title": "Related Work", "content": "Multitrack music generation. Prior work has explored various approaches for symbolic music generation (Briot et al., 2017; Ji et al., 2020), among which generating multitrack music is considered more challenging for its complex interdependency between voices and instruments. In (Dong et al., 2018a; Dong and Yang, 2018), the authors used a convolutional generative adversarial network to generate short, five-track pop music segments. In (Simon et al., 2018), the authors used a variational autoencoder with recurrent neural networks to learn a latent space for multitrack measures. In (Donahue et al., 2019; Payne, 2019), the authors used decoder-only transformer models to generate four-track game music and multi-instrument classical music, respectively. In (R\u00fctte et al., 2022), the authors used a transformer model to generate multitrack music given a fine-grained description of the characteristics of the desired music. Unlike these systems, our proposed model is built upon a more compact representation that allows it to accommodate longer sequences under the same GPU memory constraint.\nTransformers for symbolic music. Another relevant line of research is on modeling sym-bolic music with transformer models (Vaswani et al., 2017). Some prior work focused on unconditioned"}, {"title": "Proposed Method", "content": "We represent a music piece as a sequence of events x = (X\u2081, ..., Xn), where each event x\u012f isencoded as a tuple of six variables: (x\u1d57\u02b8\u1d56\u1d49, x\u1d47\u1d49\u1d43\u1d57, x\u1d56\u1d52\u02e2\u2071\u1d57\u2071\u1d52\u207f, x\u1d56\u2071\u1d57\u1d9c\u02b0, x\u1d48\u1d58\u02b3\u1d43\u1d57\u2071\u1d52\u207f, x\u2071\u207f\u02e2\u1d57\u02b3\u1d58\u1d50\u1d49\u207f\u1d57).\nThe first variable x\u1d57\u02b8\u1d56\u1d49 determines the type of the event, among the following five event types:\n\u2022 Start-of-song: Indicates the beginning of the song.\n\u2022 Instrument: Specifies an instrument used in the song.\n\u2022 Start-of-notes: Indicates the end of the instrument list and the beginning of the note list. (This event splits the sequence into two parts: a list of instrument events followed by a list of note events, making a trained autoregressive model readily applicable to instrument-informed generation task; see Section 3.3.2.)\n\u2022 Note: Specifies a note, whose onset, pitch, duration and instrument are defined by the other five variables: x\u1d47\u1d49\u1d43\u1d57, x\u1d56\u1d52\u02e2\u2071\u1d57\u2071\u1d52\u207f, x\u1d56\u2071\u1d57\u1d9c\u02b0, x\u1d48\u1d58\u02b3\u1d43\u1d57\u2071\u1d52\u207f and x\u2071\u207f\u02e2\u1d57\u02b3\u1d58\u1d50\u1d49\u207f\u1d57.\n\u2022 End-of-song: Indicates the end of the song.\nFor any non-note-type event, the variables x\u1d47\u1d49\u1d43\u1d57, x\u1d56\u1d52\u02e2\u2071\u1d57\u2071\u1d52\u207f, x\u1d56\u2071\u1d57\u1d9c\u02b0, x\u1d48\u1d58\u02b3\u1d43\u1d57\u2071\u1d52\u207f, x\u2071\u207f\u02e2\u1d57\u02b3\u1d58\u1d50\u1d49\u207f\u1d57 are set to zero, which is reserved for undefined values. Figure 3.1 shows an example of our proposed representation.\nFollowing (Huang and Yang, 2020), we decompose the note onset information into beat and position information, where x\u1d47\u1d49\u1d43\u1d57 denotes the index of the beat that the note lies in, and x\u1d56\u1d52\u02e2\u2071\u1d57\u2071\u1d52\u207f the"}, {"title": "Data Representation", "content": "We represent a music piece as a sequence of events x = (X\u2081, ..., Xn), where each event xi is encoded as a tuple of six variables:\n(x\u1d57\u02b8\u1d56\u1d49, x\u1d47\u1d49\u1d43\u1d57, x\u1d56\u1d52\u02e2\u2071\u1d57\u2071\u1d52\u207f, x\u1d56\u2071\u1d57\u1d9c\u02b0, x\u1d48\u1d58\u02b3\u1d43\u1d57\u2071\u1d52\u207f, x\u2071\u207f\u02e2\u1d57\u02b3\u1d58\u1d50\u1d49\u207f\u1d57)\nThe first variable x\u1d57\u02b8\u1d56\u1d49 determines the type of the event, among the following five event types:\n\u2022 Start-of-song: Indicates the beginning of the song.\n\u2022 Instrument: Specifies an instrument used in the song.\n\u2022 Start-of-notes: Indicates the end of the instrument list and the beginning of the note list. (This event splits the sequence into two parts: a list of instrument events followed by a list of note events, making a trained autoregressive model readily applicable to instrument-informed generation task; see Section 3.3.2.)\n\u2022 Note: Specifies a note, whose onset, pitch, duration and instrument are defined by the other five variables: x\u1d47\u1d49\u1d43\u1d57, x\u1d56\u1d52\u02e2\u2071\u1d57\u2071\u1d52\u207f, x\u1d56\u2071\u1d57\u1d9c\u02b0, x\u1d48\u1d58\u02b3\u1d43\u1d57\u2071\u1d52\u207f and x\u2071\u207f\u02e2\u1d57\u02b3\u1d58\u1d50\u1d49\u207f\u1d57.\n\u2022 End-of-song: Indicates the end of the song.\nFor any non-note-type event, the variables x\u1d47\u1d49\u1d43\u1d57, x\u1d56\u1d52\u02e2\u2071\u1d57\u2071\u1d52\u207f, x\u1d56\u2071\u1d57\u1d9c\u02b0, x\u1d48\u1d58\u02b3\u1d43\u1d57\u2071\u1d52\u207f, x\u2071\u207f\u02e2\u1d57\u02b3\u1d58\u1d50\u1d49\u207f\u1d57 are set to zero, which is reserved for undefined values. Figure 3.1 shows an example of our proposed representation.\nFollowing (Huang and Yang, 2020), we decompose the note onset information into beat and position information, where x\u1d47\u1d49\u1d43\u1d57 denotes the index of the beat that the note lies in, and x\u1d56\u1d52\u02e2\u2071\u1d57\u2071\u1d52\u207f the"}, {"title": "Model", "content": "We present the Multitrack Music Transformer (MMT) for generating multitrack music using the representation proposed in Section 3.3.1. We base the proposed model on a decoder-only transformer model (Liu et al., 2018b; Brown et al., 2020). Unlike a standard transformer model, whose inputs and outputs are one-dimensional, the proposed model has multi-dimensional input and output spaces similar to (Hsiao et al., 2021), as illustrated in Figure 3.2. The model is trained to minimize the sum of the cross entropy losses of different fields under an autoregressive setting. We adopt a learnable absolute positional embedding (Vaswani et al., 2017). Once the training is done, the trained transformer model can be used in three different modes, depending on the inputs given to the model to start the generation:\n\u2022 Unconditioned generation: Only a \u2018start-of-song' event is provided to the model. The model generates the instrument list and subsequently the note sequence.\n\u2022 Instrument-informed generation: The model is given a \u2018start-of-song' event followed by a sequence of instrument codes and a 'start-of-notes' event to start with. The model then generates the note sequence. Note that we need the \u2018start-of-notes' event as it marks the end of the instrument list, otherwise the model may continue to generate instrument events.\n\u2022 N-beat continuation: All instrument and note events in the first N beats are provided to the model.The model then generates subsequent note events that continue the input music.\nDuring inference, the sampling process is stopped when an \u2018end-of-song' event is generated or the maximum sequence length is reached. We adopt the top-k sampling strategy on each field and set k to 10% of the number of possible outcomes per field. Moreover, since the type and beat fields in our representation are always sorted, we further enforce a monotonic constraint during decoding. For example, when sampling for x\u1d57\u02b8\u1d56\u1d49, we set the probability of getting a value smaller than x\u1d57\u02b8\u1d56\u1d49 to zero. This prohibits the model from generating events in certain invalid order, e.g., an 'note' event before an 'instrument' event.\nFinally, while existing multitrack music generation systems (Ens and Pasquier, 2020; R\u00fctte et al., 2022) need to combine several generated tokens to form a note, the proposed MMT model generates a note at each inference step, i.e., a line in Figure 3.1(b) and (c). This offers MMT a significantly faster"}, {"title": "Results", "content": "In this work, we consider the Symbolic Orchestral Database (SOD) (Crestel et al., 2017). We set the temporal resolution to 12 time steps per quarter note. We discard tempo and velocity information as not all data contains such information. Further, we discard all drum tracks. We end up with 5,743 songs (357 hours). We reserve 10% of the data for validation and 10% for testing. We use MusPy (Dong et al., 2020) to process the data. For the proposed MMT model, we use 6 transformer decoder blocks, with a model dimension of 512 and 8 self-attention heads. All input embeddings have 512 dimensions.We trim the code sequences to a maximum length of 1,024 and a maximum beat of 256. During training, we augment the data by randomly shifting all the pitches by s ~ U(-5, 6) (s \u2208 Z) semitones"}, {"title": "Experiment Setup", "content": "In this work, we"}]}