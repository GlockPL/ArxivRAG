{"title": "Affinity-Graph-Guided Contractive Learning for Pretext-Free Medical Image Segmentation with Minimal Annotation", "authors": ["Zehua Cheng", "Di Yuan", "Thomas Lukasiewicz"], "abstract": "The combination of semi-supervised learning (SemiSL) and contrastive learning (CL) has been successful in medical image segmentation with limited annotations. However, these works often rely on pretext tasks that lack the specificity required for pixel-level segmentation, and still face overfitting issues due to insufficient supervision signals resulting from too few annotations. Therefore, this paper proposes an affinity-graph-guided semi-supervised contrastive learning framework (Semi-AGCL) by establishing additional affinity-graph-based supervision signals between the student and teacher network, to achieve medical image segmentation with minimal annotations without pretext. The framework first designs an average-patch-entropy-driven inter-patch sampling method, which can provide a robust initial feature space without relying on pretext tasks. Furthermore, the framework designs an affinity-graph-guided loss function, which can improve the quality of the learned representation and the model's generalization ability by exploiting the inherent structure of the data, thus mitigating overfitting. Our experiments indicate that with merely 10% of the complete annotation set, our model approaches the accuracy of the fully annotated baseline, manifesting a marginal deviation of only 2.52%. Under the stringent conditions where only 5% of the annotations are employed, our model exhibits a significant enhancement in performance-surpassing the second-best baseline by 23.09% on the dice metric and achieving an improvement of 26.57% on the notably arduous CRAG and ACDC datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "The precise delineation of medical imagery furnishes pivotal and discerning data for medical practitioners for suitable diagnostic evaluations, monitoring disease evolution, and formulating effective treatment strategies. Supervised methods based on deep learning have achieved a remarkable performance in medical image segmentation [1]. However, these methods largely benefit from extensive annotation datasets [2], and acquiring pixel-level annotations on a broad scale frequently demands a significant time investment and specialized knowledge, and entails substantial expenses. To alleviate the dependence on a large amount of annotated data, semi-supervised learning (SemiSL) and contrastive learning (CL) complement each other and are widely used in medical image segmentation. In detail, the pseudo-labels generated by SemiSL enhance the discriminative ability of CL by providing supplementary guidance for the metric learning method [3], while the crucial class discriminative feature learning of CL enhances the multi-class segmentation efficacy of SemiSL, allowing SemiSL to produce more ideal pseudo-labels [4], [5].\nHowever, these methods have two obvious shortcomings: (i) Relying on pretext tasks leads to a poor generalization ability. First, these methods suffer from sampling biases and exacerbated class collision [6] that undermine the model's performance. Then, these methods do not account for substantial domain differences, resulting in a poor performance across datasets for models trained well in pretext tasks [7], which is particularly prevalent in medical images. (ii) The lack of supervision signal leads to an overfitting problem. Most of these methods use regression, pixel-wise cross entropy or mean square error loss terms, and their variants to evaluate and generate ideal pseudo labels to assist the model in generating relatively accurate segmentation results. However, the loss functions have notable limitations, i.e., they cannot enforce intra-class compactness and inter-class separability [8], [9], thus limiting their full learning potential. Besides, there is a domain shift problem, i.e., these methods employ self-integration strategies and are designed for a singular dataset [10], which brings challenges to generalization across different domains.\nThe essence of solving the above problems lies in further utilizing the feature consistency in the manifold space with respect to the regional feature interconnections. Essentially, the effectiveness of SemiSL is based on the manifold assumption and the cluster assumption [11], which conceptualizes data points as components of low-dimensional manifolds embedded within a larger, high-dimensional space. Data points situated in the same feature space possess identical labels. However, when dealing with a limited amount of labeled data, the demarcation of cluster boundaries becomes ambiguous, which impedes the accurate delineation of the manifold's shape, leading to difficulties in correctly assigning labels to unlabeled data and thus hurting the quality of the learned feature representation. To mitigate these issues, the construction of a semantic graph presents a viable solution. By representing data points as nodes and their interconnections as edges based on feature similari-"}, {"title": "II. RELATED WORKS", "content": "Semi-supervised learning (SemiSL) harnesses valuable representations from a vast array of unlabeled samples, concurrently with supervised learning on a handful of labeled samples, which encompasses pseudo labeling [12], consistency regularization [13], [14], and entropy minimization [15], [16]. The most common one is the pseudo-labeling method based on the Mean Teacher framework. UA-MT [17] exploits the uncertainty information of the teacher model to guide the student model to learn from meaningful and reliable targets. Double-UA [18] uses a double-uncertainty weighted method to make the teaching-learning process accurate and reliable. SASSNet [19] incorporates a flexible geometric representation to enforce a global shape constraint and handle objects with varying poses or shapes. DTC [20] jointly predicts a pixel-wise segmentation map and a geometry-aware level set representation of the target. URPC [21] designs an uncertainty rectifying module to enable the framework to learn from meaningful and reliable consensual regions at different scales of pyramid predictions. MC-Net [22] designs a cycled pseudo label scheme between the prediction discrepancies of two decoders to encourage mutual consistency. SS-Net [8] explores pixel-level smoothness and inter-class separation at the same time.\nContrastive learning (CL) can increase the mutual information of similar samples by maximizing the similarity of positive samples and minimizing the similarity of negative samples, so it has been widely used in computer vision [23], [24]. However, it also faces some challenges when applied to medical image segmentation. First, sampling bias and exacerbated class collision will be led due to the uninformed and unguided selection of negative samples during contrastive learning [6], which degrades the discriminative ability of the learned representations, thus hurting the segmentation performance [25]. Then, a common practice in CL is to transfer well-trained models in the pretext task of large-scale natural image datasets to downstream tasks in the medical image field [26]. However, significant domain shifts across such heterogeneous datasets often have a negative impact on the final task performance [27]. Lastly, designing an appropriate pretext task itself can be an arduous exercise, and the choice of the pretext task may not generalize well across datasets [28].\nTherefore, how to overcome the limitations of CL in semi-supervised learning with limited labeled data and effectively utilize its representation capabilities for medical image segmentation remains an open challenge. Our work is the first attempt to alleviate this gap by proposing an affinity-graph-guided semi-supervised patch-based CL framework that synergistically combines CL and SemiSL through joint optimization. Unlike existing techniques, our framework does not require any additional pre-training and can be trained end-to-end for medical image segmentation."}, {"title": "III. METHODOLOGY", "content": "Given a labeled image set alongside its respective label set $D_L$ and an unlabeled image set $D_U$, comprising $N_L$ and $N_U$ images (where $N_L << N_U$), respectively, we propose a patch-wise contrastive learning strategy with a teacher-student model to target the assimilate information from both $D_L$ and $D_U$ directed by pseudo-labels. In our framework, we first delineate the patch generation process, steered by the"}, {"title": "A. Patch-wise Class-centric Sampling", "content": "Let $X_i \\in \\mathbb{R}^M$ denote the ith image in a mini-batch, containing $M$ pixels. The value of the $m$th pixel in image $X_i$ is denoted by $X_i(m)$. The key idea behind our patch-wise class-centric sampling is to select positive and negative patches for contrastive learning in an informed manner using the pseudo-labels. This prevents forceful contrasting of semantically similar patches, i.e., class collision. To achieve this, we first generate a class-specific confidence map $C_k$ for each class $k \\in \\{1,2,..., K\\}$, where $K(\\geq 1)$ indicates the total number of classes. This confidence map reflects the likelihood of each pixel belonging to class k. By performing an element-wise product between $C_k$ and the image $X_i$, which accentuates the regions relevant to class k, i.e., $X_i^k=X_iC_k$. Although the confidence map $C_i^k$ is much more informative comparing to the segmentation mask. $X_i^k$ retains the image intensity/texture information along with masking information from the groundtruth and provides a richer representation for entropy calculation.\nTo sample informative patches, we compute an average patch entropy $Ent_{i,j}^k$ for each patch $P_k^{i,j}$ based on the pixel intensity values in the attended image $X_i^k$. This entropy reflects three key types of information: confidence of belonging to class k, uncertainty regarding other classes, and intensity appearance from the original image $X_i$. A high entropy value indicates the patch likely contains the class k object but also has some confusion with other classes. The entropy thereby provides a richer metric for sampling, instead of simple random selection. This guided sampling focuses the learning on informative patches. Therefore, the average patch entropy allows robust, semantically meaningful sampling of positives and negatives to serve as a highly informative supervision signal to the self-supervised learning model. $Ent_{i,j}^k$ is formulated as follows:\n$Ent_{i,j}^k = -\\frac{1}{mP_k^{i,j}} \\sum_{m \\in P_k^{i,j}} X_i^k(m) log(X_i^k(m)) + (1 - X_i^k(m)) log(1 - X_i^k(m)),$ (1)"}, {"title": "B. Affinity-Graph-Guided Contrastive Loss between Pseudo Labels", "content": "Unlike traditional semi-supervised learning frameworks, a patch-wise approach necessitates the incorporation of regional information to maximize the utility of the data. Consequently, it is our contention that not all patches should be regarded as equally significant. Therefore, we introduce an affinity graph to regularize patch importance by constructing fine-grained alignment in the outputs of student network ($\\hat{Y}_s$) and teacher network ($\\hat{Y}_T$). By directly encoding prediction vector similarities as edge weights between graph nodes, the discrete topology inherently captures the continuous semantic affinities that we intend to align. Concurrently, the graph Laplacian regularization enforces smoothness priors, forefending collapse into trivial solutions. Maximizing the resultant diagonal trace impels convergence of the patch-wise pseudo-labels.\nSpecifically, we construct a patch-wise affinity graph $A \\in \\mathbb{R}^{N\\times N}$ between the pseudo-labels from the teacher network $\\hat{y}_t$ and student network $\\hat{y}_s$, where N is the number of patches. The edge weight $A_{ij}$ is defined using a Gaussian kernel based on the L2 distance between pseudo-label vectors $\\hat{y}_i^s$ and $\\hat{y}_j^t$:\n$A_{ij} = exp(-\\frac{\\|y_i^s - y_j^t\\|^2_2}{2 \\sigma^2}),$ (2)\n$\\sigma$ is an adaptive bandwidth parameter. We choose the Gaussian kernel, because it has strictly localized support, smooth variation, and efficient computability, which are theoretically well-founded for representing granular semantic relationships among samples. First, locality is imparted through the exponential term that precipitously decays affinity weight as distance in the embedding hyperspace grows. This realizes the expectation that closer samples exhibit greater semantic similarity and relatedness. It also guarantees gradual weight transitions with distance alterations, regulated by $\\sigma$, thereby preventing abrupt changes. The modulating impact of $\\sigma$ further provides control over the rate of falloff and spatial scope of similar neighborhoods. Additionally, the Gaussian kernel satisfies constraints of radial symmetry and positive semi-definiteness suitable for modeling sample-wise relationships rather than discrete differences. Efficient computability facilitates constructing weighted graphs over large corpora encompassing tens of thousands of nodes.\nThe affinity graph construct provides a prudent approach here, as directly encoding prediction vector similarities as edge weights inherently captures the desired semantic affinities to align. Concurrently, the discrete topology is regulated through smoothness priors. In other words, we harness the diagonal entries $A_{ii}$, measuring self-similarities between teacher and student pseudo-labels on patch i. By maximizing the trace $\\mathbb{E}[tr(A)]$, we promote convergence of the patch-wise student and teacher predictions. Simultaneously, we minimize the nuclear norm $\\|A\\|_*$ via convex relaxation.\nThe nuclear norm serves as a convex lower bound on the intractable matrix rank function. We first perform this convex relaxation through singular value thresholding [31]. This decomposes A into $A = U\\sum V^T$ via singular value decomposition, where $\\sum$ contains the singular values $\\sigma_i$. We then soft-threshold these singular values by an amount proportional to the subgradient of the nuclear norm, iteratively zeroing out unimportant dimensions. Thereby, minimizing $\\|A\\|_*$ serves as an efficient, tractable proxy for minimizing rank.\nIn summary, by excavating this salient low-rank alignment pattern between student and teacher outputs amidst noisy inconsequential variations, our approach safeguards the model from overfitting during contrastive learning. In effect, the convex relaxation extracts the most essential signals while filtering out extraneous dimensions. Explicitly, we get the affinity graph guided contrastive loss $L_{AGG}^{PL}$ between the pseudo labels of $Y_s$ and $Y_T$ with its affinity graphs A as:\n$L_{AGG}^{PL} = \\sum_{i=1}^{N} exp(-\\frac{\\|y_i^s - y_i^t\\|^2_2}{2 \\sigma^2}) - \\gamma ||A||_*,$ (3)\nwhere $\\gamma$ balances the relative importance. By directly encoding alignment similarities and extracting low-rank structure, this loss function applied alongside standard supervised objectives aligns student and teacher representations robustly even with few labels. The affinity graph topology provides an interpretable, flexible mechanism for semi-supervised contrastive learning. We note that the quantities needed for the affinity graph in Equation 2 implementation are easily computed in parallel across examples within a batch using deep learning."}, {"title": "C. Affinity-Graph-Guided Hard-Negative Reweighting", "content": "[32], [33] argue that the performance of contrastive learning could be improved by the incorporation of hard negative samples (i.e., samples $\\hat{y}$ that are difficult to distinguish from an anchor $x_i$). In this context, rather than considering two arbitrary data points as negative pairs, these methods construct a negative pair from two random data points that are not too far from each other. The affinity graph constructed by Equation 2 also can meassure the hard negative samples under L2 distance between $\\hat{y}$ and $\\hat{y}_s$. Therefore, utilizing the hard negative samples selected from Equation 2 could further improve the performance of the current framework.\nIn contrastive learning, we get the query q and the corresponding key k embeddings from the positive pair. We construct the query-key paris (q, k) with the encoder-projection routine for the student networks, where we get $q = H(E_s(P))$, $E_s$ is the encoder of student networks, and H is the projection head. The key vector set K is formulated by amalgamating both positive and negative keys, represented as $K = K^+ \\cup K^-$, $K^+$ consisting of positive keys $k^+$ with"}, {"title": "IV. EXPERIMENTS", "content": "To assess the performance of our proposed method, we conducted experiments using three different datasets, each representing a distinct input modality. We processed 3D data"}, {"title": "B. Implementation Details", "content": "We conduct all experiments on a DGX A100 server with fixed random seeds. The model's convergence is achieved through the utilization of an ADAM optimizer, with the specifications of a batch size set at 16 and a learning rate designated at le-4. The parameters $eta$ and $lambda$ in Equation 5 are assigned values of 0.2 and 4, respectively, as guided by the precedent set in [38]. In Equation 3, we set the gamma as -1 in all experiments. Within the scope of Section III-A, which discusses n-nearest entropy-based sampling, the parameter n is determined through validation to hold the values of 0.999, 0.25, 0.2, and 20, respectively. For all the baselines, we follow the hyperparameters defined in the original paper, and use Optuna [39] to tune the learning rate."}, {"title": "C. Main Results", "content": "The proposed framework is compared with the state-of-the-art CL- and SemiSL-based segmentation methods at different markers (i.e., 5% and 10%), that is, UA-MT [17], Double-UA [18], SASSNet [19], DTC [20], URPC [21], MC-Net [22], and SS-Net [8]. We also present results with distillation-based semi-supervised learning methods with ACTION [40] and ARCO [41]. To note that both ACTION and ARCO are built pretrained model and fine-tuning strategies to built the model which involved extensive computational budget. We mannully choose the lowest loss of the pretrained model and then fine-tuning the model with fixed hyperparameters in the orignal papers.\nResults from other competitors are reported in the identical experimental setting in SS-Net [8] for fair comparisons."}, {"title": "D. Ablation Studies", "content": "Effectiveness of each module. Under different labeling ratios, we compare different loss functions between pseudo-labels and loss functions based on different positive and negative sample selection methods, proving the effectiveness and optimality of the proposed loss functions (i.e., L*AGG and LCRWAGG). First, for L*AGG, we find that the experimental results of GK + L* used to calculate $A_{ij}$ are not much different, but adding $lambda ||A||_*$ used to calculate $A_{ii}$, the segmentation results have improved (the random of CRW"}, {"title": "V. CONCLUSION", "content": "To alleviate the problem that methods combining semi-supervised learning and contrastive learning rely on pretext tasks and insufficient supervision signals, we propose an affinity-graph-guided semi-supervised contrastive learning"}]}