{"title": "Controlling Grokking with Nonlinearity and Data Symmetry", "authors": ["Ahmed Salah", "David Yevick"], "abstract": "This paper demonstrates that grokking behavior in modular arithmetic with a modulus P in a neural network can be controlled by modifying the profile of the activation function as well as the depth and width of the model. Plotting the even PCA projections of the weights of the last NN layer against their odd projections further yields patterns which become significantly more uniform when the nonlinearity is increased by incrementing the number of layers. These patterns can be employed to factor P when P is nonprime. Finally, a metric for the generalization ability of the network is inferred from the entropy of the layer weights while the degree of nonlinearity is related to correlations between the local entropy of the weights of the neurons in the final layer.", "sections": [{"title": "1. Introduction", "content": "The phenomenon of \"grokking\" during neural network training was first observed and empirically described in the context of Transformers by Power et al. (2022) [1]. Before the onset of grokking, the model exhibits a high level of training accuracy but a low test accuracy. After further training, however, the test performance improves markedly. Similar behavior is observed in double descent which is also associated with the difference in the time required to learn dissimilar patterns in the training test sets. Davies et al. (2023) [2].\nIn the context of simple physical or algorithmic models, grokking provides a controlled environment in which to examine the conditions required to learn underlying relations in the dataset. These include, but are not limited to, the similarity in the table of the dataset, the data set size, training fraction and the influence of nonlinearity. As well, the physical meaning of collective properties such as the PCA projections of neural network (NN) weights can be investigated as well as the correlations between the weights of the neurons.\nGrokking has recently been studied extensively Davies et al.[2], 2023, Nanda et al., 2023[3], Thilak et al.[4], 2022, Varma et al.[5], 2023, Liu et al., 2022a[6],b[7], Gromov, 2023[8], Schaeffer et al., 2023[9], Michaud et al. 2023[10]) and several semi-analytical frameworks have been advanced. While some of these require weight decay [Liu et al., 2022[6], Varma et al., 2023[5]], a two-layer MLP without weight decay also exhibits grokking Kumar et al. [11]. In [4], grokking without regularization was accompanied by a \u201cslingshot\u201d event a sudden spike in training loss followed by improved generalization. The analysis of Varma et al. [5] and Liu et al. [7] do not incorporate slingshot or oscillation behavior but instead is based on the relative strength of memorization and generalization. Liu et al. [7] attribute generalization to learning a representation of the input embeddings which, in the case of algorithmic datasets, is generally accompanied by the emergence of structure in the embeddings. This manifests itself as a circular pattern when consecutive PCA components of the inputs are plotted against each other. In this paper, however, a similar generalization behavior is achieved in a standard MLP in the absence of embeddings and transformers while it is further observed that grokking can occur without inducing an observable structure in PCA space.\nThe principal conclusions of this paper are the following:\n\u2022Grokking occurs even if the neurons properties are aperiodic.\n\u2022The interaction among the neurons within a layer is quantified by a correlation function that changes its properties.\n\u2022The nonlinearity of the NN, which depends on the form of the activation function and the width and depth of the network, can be employed to control the grokking behavior.\n\u2022The NN nonlinearity further impacts the symmetries inherent in the PCA eigenvectors of the NN weights.\n\u2022These symmetries, especially for small datasets, can yield a factorization of the modulus.\n\u2022Grokking also occurs when the PCA projections are not manifestly symmetric.\n\u2022Grokking and the effect of nonlinearity can be partially characterized by the entropy of the NN weights as well as their mutual correlations."}, {"title": "2. Preliminaries and Setup", "content": "Grokking modular arithmetic. The calculations of this paper apply a two-layer MLP without embedding to data generated through modular addition, $Y = (i + j)\\%P$, for $i, j = 0,1, ... . P \u2212 1$ where $i$ and $j$ are separately one-hot encoded. These vectors are then concatenated to generate a single tensor $X$ of length 2P. The dimension of the output vector is $P$, while the size of the dataset is $P\u00b2$. Since this corresponds to a classification problems with the label $Y$, cross entropy loss is utilized together with an Adam optimizer (Loshchilov & Hutter, 2019) [12] with weight decay, as weight decay has been proven to significantly impact grokking (Power et al., 2022[1]; Nanda et al., 2023[3]; Varma et al., 2023[5]).\nFor a quadratic activation function the network mapping becomes (Gromov, 2023) [8],\n$f(x) = \\frac{1}{DN} W^{(2)}(W^{(1)}x)^2$ (1)\nwhere $D$ is the input dimension, $N$ is the hidden layer width, and $W^{(i)}$ are the ith layer weights matrices. The model size is varied by adjusting the hidden size dimension, HIDDEN_DIM, while the fraction of the data employed for training data is TRAIN_FRAC. The MLP utilized here consists of two layers; the input layer of size (HIDDEN_DIM) and the output one of size (P). The effect of inserting an intermediate layer of size (HIDDEN_DIM) will also be considered.\nThe weights $W^{(1)}_{kn}$ and $W^{(2)}_{qk}$ that are consistent with modular addition problem are given by\n$W^{(1)}_{kn} = \\begin{pmatrix} cos (\\frac{k}{p}2\\pi n_1 + \\varphi_k^{(1)} )\\\\ cos (\\frac{k}{p}2\\pi n_2 + \\varphi_k^{(2)})\\end{pmatrix}^T , n = (n_1, n_2)$ (2)\n$W^{(2)}_{qk} = cos(-\\frac{k}{p}2\\pi q - \\varphi_k^{(3)})$ (3)\nwith $n\u2081, n\u2082 = 0,1, ... . p \u2212 1$. For inputs (n, m), the unnormalized outputs of the first layer are\n$h^{(1)}_k(n, m) = cos (\\frac{k}{p}2\\pi n + \\varphi_k^{(1)}) + cos (\\frac{k}{p}2\\pi m + \\varphi_k^{(2)})$ (4)\nwhere for modular addition"}, {"title": "3. Experiments and Findings", "content": "This ensures that the terms $\\varphi_k^{(3)} = \\varphi_k^{(1)} + \\varphi_k^{(2)}$ (5)\nof $h^{(2)}_q(n, m)$, interfere constructively and are therefore described by\n$h^{(2)}_q(n,m) = \\frac{1}{N}\\sum_{k=1}^N cos (\\frac{2\\pi k}{p}(n+m-q))$ (6)\nAdditionally, degree 2 polynomial activation functions of the form \u00d8(x) = bx + ax\u00b2, which have been shown to provide additional accuracy when suitably optimized (Yevick, 2024) [13] will be considered.\nBarak et al. (2022) [14] introduced and theoretically analyzed metrics that are associated with emergent behavior. Here a metric is introduced to quantify the extent of grokking of a network based on of the weights of the last layer. This differs from the progress measures suggested by Nanda [3] that are based on mechanistic interpretability but is consistent with results for the entropy associated with the explained variance ratio derived from the PCA of the embeddings obtained by Liu et al. (2022) [7]."}, {"title": "3.1 Weights for Different Connections of the NN and Correlation of Neurons", "content": "The harmonic behavior of the weights described in the previous section was observed in many of our numerical calculations. In particular, the weights of the connections from each neuron in the last layer to a single neuron in the first layer as illustrated in Fig. 1 is quasi-sinusoidal for some of the first layer neurons while the corresponding Fourier spectrum in Fig. 2 is sharply peaked at the positive and negative frequencies present in the cosine function. However, the weights for the connections to other first layer neurons can be nonperiodic without eliminating the grokking behavior as seen for instance with P=27 in Fig.2, in apparent contradiction to (Gromov, 2023) [8]. If weight decay is employed, a certain fraction of the last layer connection weights decreases below 10-41 many epochs after grokking as shown in Fig.3, without affecting the test accuracy. Evidently, the surviving connections are those that convey the largest fraction of the information present in the data.\nThe degree of periodicity in the model weights can be quantified by evaluating correlation function of the weights given by\n$Corr(k) = \\sum_{l=0}^{N-1}\\ x(l)x(l-k)$ (7)\nwhere $x(l)$ is the value of the input sequence, weights of the connections of the last layer nodes, at index $l$, while $x(l \u2013 k)$ is the value of the sequence shifted by $k$ in the presence of periodic boundary conditions. The length of the sequence is $N$ and the correlation at each point of overlap between the sequences is calculated and the values are then normalized. The resulting correlation of the weights of the neurons in the output layer that are associated with the connections to a single neuron in the first layer is described for certain output notes by the damped oscillating function of Fig. 4. However, other sets of connections to the first layer do not display a harmonic behavior. Since a positive correlation indicates heuristically that the network operates in a similar fashion when applied to certain input features, the four oscillations in Fig. 4 are plausibly related to the number of rotations in the PCA diagrams. The rate of decay of the correlation could then be associated with the deviation of the rotational patterns from perfectly repeating structures. The groups of connections with oscillating correlations would then be associated with the nodes that have learned the periodicity in the data resulting from the modular addition."}, {"title": "3.2 Activation Function", "content": "Changing the form of the activation function from $x\u00b2$ to the non-even functions $x + ax\u00b2$ illustrated in Fig. 5 significantly affects the grokking behavior. Thus in Fig. 6, for P=27 and TRAIN_FRAC = 0.8, the number of epochs before grokking occurs decreases as the value of $a$ increases. Evidently, the odd, $x$, term delays the rise of the test accuracy and hence enhances the grokking behavior. Grokking also does not occur for odd activation functions $x\u00b2sign(x)$ and $x\u00b3$. In the latter case the test accuracy rises to a maximum value before decreasing, as evident in Fig. 7. In contrast, for a |x3| activation function, the test accuracy rises and then remains at a maximum value. Taken together, these results establish that a rapid onset of grokking requires that the activation function be even and that the onset of grokking can be delayed an arbitrary number of epochs by incorporating an odd, non-grokking component into the activation function."}, {"title": "3.3 PCA Projections", "content": "If an additional dense layer with neurons is added to the model and an activation function of the form $x + ax\u00b2$, is employed for the first two layers, plotting the odd order PCA projections of the third layer weights against the even order projections as demonstrated by Power et al. (2022) [1], and Liu et al. (2022)[7] yields a circular pattern for some pairs of lower-order principal components. This structure is not present in a two-layer system, presumably because of insufficient nonlinearity. While grokking is observed for all $a$ values, a circular structure in PCA space only results when $a < 0.5$, as illustrated in Fig. 9, for $a = 0.25$, with P=53, TRAIN_FRAC = 0.5 and trained for 13k epochs in both cases. The corresponding structures for the higher-order PCA components are localized near the origin, as evident from Fig. 9."}, {"title": "3.4 PCA Factoring", "content": "Decreasing P to 20 reduces the size of the dataset and hence the effect of grokking. If the ratio of the number of samples in the training set to the total number of samples, TRAIN_FRAC = 0.7 the test accuracy decreases after reaching 1 so that TRAIN_FRAC = 0.8 is instead employed. While 64 neurons are then sufficient to produce grokking, successive groups of PCA projections of weights then do not display a circular structure. With 256 neurons in the two hidden layers, plotting the projections of the output patterns onto the zeroth and first and second and third lowest PCA components in Fig. 10 yields a circular pattern containing 5 clusters of 4 points while the pattern generated by the 4th and 5th lowest order PCA components contains 4 clusters with 5 points. Accordingly, the procedure has led to a factorization of 20 as 5 \u00d7 4.\nThe activation function $x + 0.1x\u00b2$ in place of $x + 0.25x\u00b2$ yields a different circular PCA space pattern generated by the projections on the 4th and 5th lowest order PCA components. Here the clusters consist of pairs of numbers that differ by 10 as illustrated in Fig. 11 yielding the factorization 20 = 2 \u00d7 10. Similarly, for P=21 the PCA space yields the 3 and 7 point clusters. In each of these cases, the patterns only appear after the program has executed a certain number of epochs beyond the occurrence of grokking."}, {"title": "3.5 Data Dependence", "content": "Grokking in the modular addition problem could be explained by symmetries in the addition table or by the concatenated hot encoding of the two input values. One symmetry in the dataset is associated with the commutation relation $i + j = j + i$, and can be removed by considering only the values of $i + j$ with $j \u2265 i$, which, however, reduces the size of the dataset. In this case, for P = 53 and two network layers the maximum test accuracy is 0.85 after grokking as shown in Fig. 12 (b) for an $x\u00b2$ activation function. Similar results are obtained for a $x + 2x\u00b2$activation function or for a 3 layer model.\nIf the data set is expanded to the same number of values that would have been generated if both $i + j$ and $j + i$ had been employed for P=53 by increasing P to 75, then for an $x\u00b2$ activation function the test accuracy is substantially increased but does not reach unity as evident in Fig. 12(e). If the training set fraction is instead increased to 0.8 instead of 0.5, generalization occurs for P=53. The symmetry of the data set can be further reduced by, e.g. excluding the pairs with $j \u2212 i = 1,2,5$ and 8, which is however found not to affect the grokking unless the additional restriction that $i > j$ is also imposed. In the latter case, grokking is absent unless the training set fraction is set to 0.8, which then yields a maximum test accuracy of 0.984."}, {"title": "3.6 Entropy Analysis", "content": "The entropy of the weights of a NN layer, defined as $S = \u2212 \u2211_i w_i\\log w_i$, where $w_i$ is the weight of each neuron in the layer, provides a metric for the extent of learning during training as the entropy quantifies the level of randomness in the weight distribution. Evenly distributed weights result in large entropy values indicating that the network is still exploring different configurations or is becoming unnecessarily complex leading to overfitting. For the simplest case of a $x\u00b2$ activation function with P=53 and TRAIN_FRAC =0.5, Fig. 13 demonstrates that the entropy initially decreases while the training accuracy increases as the network learns relevant features. However, the entropy then increases further as the test accuracy starts to grok and only decreases again once the test accuracy reaches a maximum value. Hence the entropy is predictive of the additional learning that is required before the network can correctly distinguish the unseen test patterns. As further illustrated in Fig. 14, for an activation function of the form $x + x\u00b2$, with P=53 and TRAIN_FRAC =0.8, after the initial rise in training accuracy, the decrease in entropy is less pronounced until it falls rapidly after the test accuracy reaches a maximum. If a third layer is employed, the entropy decreases to lower values than in the two layer network, presumably as a result of the increased nonlinearity. The initial fluctuations in the entropy during training are also dependent on the network nonlinearity, in agreement with Notsawo et al. (2023)[15], who predicted the number of epochs required to observe grokking from the loss landscape during early training stages."}, {"title": "4. Conclusion", "content": "This paper has examined the relationship between the properties of a simple neural network and its nonlinear structure in the context of modular addition. The nonlinearity was controlled through the form of the activation function, the number of layers and the number of neurons per layer. A procedure was identified based on these studies that enabled the number of epochs before the onset of grokking to be adjusted by incorporating a linear component into the activation function. In a similar manner, increasing the total nonlinearity by increasing the width or depth of the network was demonstrated to increase the delay between the rises in training and test accuracy. A mechanism for factoring non-prime numbers for networks with a sufficient number of layers and hence degree of nonlinearity was then proposed based on the PCA projections of the weights of the last layer. These results were then shown to apply even when the symmetries inherent in the dataset were partially eliminated if a larger fraction of the data is employed as training samples. Finally, metrics were advanced based on both the correlation between the weights of the connections to a neuron in the last layer and on the entropy of the neurons in each layer. These can be employed to quantify the learning process of the NN and its grokking behavior.\nFurther studies of these topics could further elucidate the relationship between the overall nonlinearity and the number of epochs required to achieve grokking as well as the predictive ability of the correlation and entropy metrics. A more systematic examination of the relationship between the correlation function of the neural network weights and the structures that arise in PCA space could potentially also be of practical significance. Finally, the degree to which the features of the highly simplified model examined in this paper are present in large scale networks that exhibit grokking and the associated similarities and differences between the two models should be examined in detail."}]}