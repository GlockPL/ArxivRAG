{"title": "HybridFC: A Hybrid Fact-Checking Approach for Knowledge Graphs", "authors": ["Umair Qudus", "Michael R\u00f6der", "Muhammad Saleem", "Axel-Cyrille Ngonga Ngomo"], "abstract": "We consider fact-checking approaches that aim to predict the veracity\nof assertions in knowledge graphs. Five main categories of fact-checking ap-\nproaches for knowledge graphs have been proposed in the recent literature, of\nwhich each is subject to partially overlapping limitations. In particular, current\ntext-based approaches are limited by manual feature engineering. Path-based and\nrule-based approaches are limited by their exclusive use of knowledge graphs as\nbackground knowledge, and embedding-based approaches suffer from low accu-\nracy scores on current fact-checking tasks. We propose a hybrid approach-dubbed\nHybridFC-that exploits the diversity of existing categories of fact-checking ap-\nproaches within an ensemble learning setting to achieve a significantly better\nprediction performance. In particular, our approach outperforms the state of the\nart by 0.14 to 0.27 in terms of Area Under the Receiver Operating Characteristic\ncurve on the FactBench dataset. Our code is open-source and can be found at\nhttps://github.com/dice-group/HybridFC.\n\nKeywords: fact checking ensemble learning knowledge graph veracity.", "sections": [{"title": "1 Introduction", "content": "Knowledge graphs (KGs) are an integral part of the Web. A recent crawl of 3.2 billion\nHTML pages found over 82 billion RDF statements distributed over roughly half of\nthe Web pages that were crawled.\u00b9 The increasing adoption of RDF at Web scale is\nfurther corroborated by the Linked Open Data cloud, which now contains over 10,000\nKGs with more than 150 billion assertions and 3 billion entities.\u00b2 Large-scale KGs like\nWikiData [30], DBpedia [2], Knowledge Vault [13], and YAGO [44] contain billions\nof assertions, and describe millions of entities. They are being used as background\nknowledge in a growing number of applications, including healthcare [26], autonomous\nchatbots [1], and in-flight entertainment [31]. However, it is well established that current\nKGs are partially incorrect. For example, roughly 20% of DBpedia's assertions are\nassumed to be false in the literature [20,39]. Fostering the further uptake of KGs at Web"}, {"title": "2 Preliminaries", "content": "In this section, we define the terminology and notation used throughout this paper. We\nbuild upon the definition of fact checking for KGs suggested in [47]:\n\nDefinition 1 (Fact Checking). Given an assertion, a reference KG G, and/or a reference\ncorpus, fact checking is the task of computing the likelihood that the given assertion is\ntrue or false [47].\n\nThroughout this work, we rely on RDF KGs:\n\nDefinition 2 (RDF Knowledge Graph). An RDF KG G is a set of RDF triples G \u2286\n(EUB) XPX (EUBUL), where each triple (s, p, o) \u2208 G comprises a subject s, a\npredicate p, and an object o. E is the set of all RDF resource IRIs, B the set of all blank\nnodes, PCE the set of all RDF predicates, and L the set of all literals [48].\n\nIn our approach, we use multiple representations of RDF KGs. In addition to their\nrepresentation as sets of assertions, we also exploit representations in continuous vector\nspaces, called embeddings [51,10].\n\nDefinition 3 (KG Embeddings). A KG embedding function 4 maps a KG G to a\ncontinuous vector space. Given an assertion (s, p, o), \u03c6(s), \u03c6(p), and (o) stand for the\nembedding of the subject, predicate, and object, respectively. Some embedding models\nmap the predicate embedding into a vector space that differs from the space wherein (s)\nand (0) are mapped. For those models, we use 4*(p) to denote predicate embeddings.\n\nDifferent embedding-based approaches use different scoring functions to compute\nembeddings [51]. The approaches considered in this paper are shown in Table 1."}, {"title": "3 Related Work", "content": "We divide the existing fact-checking approaches into 5 categories: text-based [47,20],\npath-based [48,41], rule-based [17,16,27], KG-embedding-based [24,7,29], and hybrid\napproaches [28,15,14]. In the following, we give a brief overview of state-of-the-art\napproaches in each category along with their limitations."}, {"title": "3.1 Text-based Approaches", "content": "Approaches in this category validate a given assertion by searching for evidence in a\nreference text corpus. FactCheck [47] and DeFacto [20] are two instantiations of this\ncategory. Both approaches search for pieces of text that can be used as evidence to\nsupport the given assertion by relying on RDF verbalisation techniques. TISCO [39]\nrelies on a temporal extension of DeFacto. All three approaches rely on a set of manually\nengineered features to compute a vectorial representation of the texts they retrieved\nas evidence. This manual feature engineering often leads to a suboptimal vectorial\nrepresentation of textual evidence [5]. In contrast, we propose the use of embeddings\nto represent pieces of evidence gathered from text as vectors. First, this ensures that\nour approach is aware of the complete piece of textual evidence instead of the fragment\nextracted by previous approaches. Second, it removes the need to engineer features\nmanually and hence reduces the risk of representing text with a possibly suboptimal set\nof manually engineered features."}, {"title": "3.2 Path-based Approaches", "content": "Path-based approaches generally aim to validate the input assertion by first computing\nshort paths from the assertion's subject to its object within the input KG. These paths are\nthen used to score the input assertion. Most of the state-of-the-art path-based approaches,\nsuch as COPAAL [48], Knowledge stream [41], PRA [19], SFE [18], and KG-Miner [40]\nrely on RDF semantics (e.g., class subsumption hierarchy, domain and range information)\nto filter useful paths. However, the T-Box of a large number of KGs provides a limited\nnumber of RDFS statements. Furthermore, it may also be the case that no short paths\ncan be found within the reference KG, although the assertion is correct [48]. In these\nscenarios, path-based approaches fail to predict the veracity of the given assertion\ncorrectly."}, {"title": "3.3 Rule-based Approaches", "content": "State-of-the-art rule-based models such as KV-Rule [25], AMIE [17,16,27], OP [8], and\nRuDiK [34] extract association rules to perform fact checking or fact prediction on KGs.\nTo this end, they often rely on reasoning [27,45]. These approaches are limited by the\nknowledge contained within the KG, and mining rules from large-scale KGs can be a\nvery slow process in terms of runtime (e.g., OP takes \u2265 45 hours on DBpedia [27])."}, {"title": "3.4 Embedding-based Approaches", "content": "Embedding-based approaches use a mapping function to represent the input KG in\na continuous low-dimensional vector space [24,7,29,12,50,21,42]. For example, Es-\nther [42] uses compositional embeddings to compute likely paths between resources.\nTKGC [21] checks the veracity of assertions extracted from the Web before adding them\nto a given KG. The veracity of assertions is calculated by creating a KG embedding\nmodel and learning a scoring function to compute the veracity of these assertions. In\ngeneral, embedding-based approaches are mainly limited by the knowledge contained\nwithin the continuous representation of the KG. Therefore, these approaches encounter\nlimitations with respect to their accuracy in fact-checking scenarios [22] as well as their\nscalability when applied to large-scale KGs [51]."}, {"title": "3.5 Hybrid Approaches", "content": "While the aforementioned categories have their limitations, they also come with their\nown strengths. Consider the assertion in Listing 1.1. The text-based approach FactCheck\ncannot find evidence for the assertion. A possible reason might be that West Hollywood\nis not mentioned on the Wikipedia page of Johnny Carson. However, COPAAL finds\nevidence in the form of corroborative paths that connect the subject and the object\nin DBpedia. For example, the first corroborative path in this particular example from\nFactBench [20] encodes that if two individuals share a death place, then they often\nshare several death places. While this seems counter-intuitive, one can indeed have\nseveral death places by virtue of the part-of relation between geo-spatial entities, e.g.,\none's death places can be both the Sierra Towers and West Hollywood. In our second\nexample shown in Listing 1.2, COPAAL is not able to find any relevant paths between\nthe subject and the object. This shows one of the weaknesses of COPAAL which does\nnot perform well for rare events, e.g., when faced with the : award property [48]. In\ncontrast, TransE [7] is able to classify the assertion as correct. These examples support\nour hypothesis that there is a need for a hybrid solution in which the limitations of one\napproach can be compensated by the other approaches."}, {"title": "4 Methodology", "content": "The main idea behind our approach, HybridFC, is to combine fact-checking approaches\nfrom different categories. To this end, we created components for a text-based, a path-\nbased and a KG embedding-based fact-checking algorithm. Figure 1 depicts a high-level\narchitecture of our approach. We fuse the results from the three components and feed\nthem into a neural network component, which computes a final veracity score. In the\nfollowing, we first describe the three individual components of our approach in detail.\nThereafter, we describe the neural network component that merges their results."}, {"title": "4.1 Text-based Component", "content": "Text-based approaches typically provide a list of scored text snippets that provide\nevidence for the given assertion, together with a link to the source of these snippets and\na trustworthiness score [20,47]. The next step is to use machine learning on these textual\nevidence snippets to evaluate a given assertion. In HybridFC, we refrain from using the\nmachine learning module of text-based approaches. Instead, we compute an ordering for\nthe list of text snippets returned by text-based approaches. To this end, we first determine\nthe PageRank scores for all articles in the reference corpus [35] and select evidence\nsentences. Our evidence sentence selection module is based on the following hypothesis:\n\"Documents (websites) with higher PageRank score provide better evidence sentences\".\nErgo, once provided with scored text snippets by a text-based approach, we select the\ntop-k evidence sentences coming from documents with top-k PageRank scores. To each\ntext snippet, we assign the PageRank score of its source article. Then, we sort the list of\ntext snippets and use the k snippets with the highest PageRank score.\nWe convert each of the selected snippets t; into a continuous vector representation\nusing a sentence embedding model. We concatenate these sentence embeddings along\nwith the trustworthiness scores [32] of their respective sources to create a single vector\n\u05d0. In short:\n\\( \\aleph = \\bigoplus_{i=1}^{k} (b(t_i) \\oplus T_i),\\)\nwhere stands for the concatenation of vectors, b(t\u2081) is the sentence embeddings of t; and\nT\u2081 is the trustworthiness score of t\u2081. Our approach can make use of any text-based fact-\nchecking approach that provides text snippets and a trustworthiness score, and allows us\nto compute PageRank score. Moreover, we can use any sentence embedding model. For\nour experiments, we adapt the state-of-the-art text-based approach FactCheck [47] as\na text-based fact checking approach, and make use of a pre-trained SBert Transformer\nmodel for sentence embeddings [37]."}, {"title": "4.2 Path-based Component", "content": "Path-based approaches determine the veracity of a given assertion by finding evidence\npaths in a reference KG. Our path-based component can make use of any existing path-\nbased approach that takes the given assertion as input together with the reference KG"}, {"title": "4.3 KG Embedding-based Component", "content": "KG embedding-based approaches generate a continuous representation of a KG using\na mapping function. Based on a given KG embedding model, we create an embedding\nvector for a given assertion (s, p, o) by concatenating the embedding of its elements and\ndefine the embedding mapping function for assertions ((s, p, o)) as follows:\n\\( \\varphi((s, p, 0)) = \\varphi(s) \\oplus \\varphi(p) \\oplus \\varphi(o). \\)\nIn our approach, we can make use of any KG embedding approach that returns both\nentities and relations embeddings. However, only a few approaches provide pre-trained\nembeddings for large-scale KGs (e.g., DBpedia). We use all approaches that provide\npre-trained embeddings for DBpedia entities and relations in our experiments."}, {"title": "4.4 Neural Network Component", "content": "The output of the three components above is the input to our neural network component.\nAs depicted in Figure 2, the neural network component consists of three multi-layer\nperceptron modules that we name 9;.5 Each of these modules consists of a Linear layer,\na Batch Normalization layer, a ReLU layer, a Dropout layer and a final Linear\nlayer. The output of the text-based component o\u05d0 is fed as input to the first module. The\noutput of the KG embedding-based component ((s, p, o)) is fed to the second module.\nThe output of the 2 modules and the veracity score of the path-based component are"}, {"title": "5 Experimental Setup", "content": "We evaluate our approach by comparing it with seven state-of-the-art fact-checking\napproaches. In the following, we first describe the datasets we rely upon. Then, we\ndescribe our experimental setting."}, {"title": "5.1 Datasets", "content": "Fact-checking Datasets. In our experiments, we use two recent fact-checking datasets\nthat are often used in the literature [47,20,48]: FactBench and BirthPlace/DeathPlace\n(BD). We use these datasets because they comprise entities of DBpedia, which is (i) large,\nand (ii) for which multiple pre-trained embedding models are available.\nWe only use a subset of the original FactBench dataset because it was created in\n2014, and is based on DBpedia version 3.9 [20]. Ergo, some of the facts it contains are\noutdated. For example, (: B. Obama, presidentof, : USA) was a correct assertion when\nthe benchmark was created but is currently incorrect (without the date information). We\nperformed the following list of changes to obtain the benchmark used herein:"}, {"title": "5.2 Evaluation Metric", "content": "As suggested in the literature, we use the area under the receiver operator characteristic\ncurve (AUROC) to compare the fact-checking results [25,48,47]. We compute this score\nusing the knowledge-base curation branch of the GERBIL framework [36,33]."}, {"title": "5.3 Setup Details and Reproducibility", "content": "Within the sentence embedding module, we use a pre-trained SBert model.7 Furthermore,\nwe set k = 3 in the sentence selection module. The size of the sentence embedding\nvectors generated by SBert is 768, and the trustworthiness score values against each\nsentence vector, which leads to |6x| = (3 \u00d7 768) + 3 = 2307.\nWe use embeddings from 5 KG embedding models, where pre-trained DBpedia\nembeddings are available 8. These models include: TransE [7], ConEx [12], QMult [11],\nComplEx [50], and RDF2Vec [38]. For the FactBench dataset, we do not include experi-\nments using RDF2Vec embeddings, because these embeddings were generated using\na different version of DBpedia (i.e., 2015-10) and missing embeddings of multiple\nentities (i.e., 40/1800). However, we included RDF2Vec embedding in the BD dataset\ncomparison. Different KG embedding models provide embedding vectors with different\nlengths. For example, the TransE model used within our experiment maps each entity\nand each relation to a vector with 100 dimensions. This leads to a total size for 4(s,p,o) of\n300.\nWe use the Binary Cross Entropy (BCE) as loss function for training our neural\nnetwork component. We set the maximum number of epochs to 1000 with a batch size\nof 1/3 of the training data size. The training may have to be stopped earlier in case the\nneural network component starts to overfit. To this end, we calculate the validation loss\nevery 10th epoch and if this loss does not decrease for 50 epochs, the training is stopped."}, {"title": "5.4 Competing Approaches", "content": "We compare HybridFC in different configurations to FactCheck [47], COPAAL [48], and\nKV-Rule [25], which are the state-of-the-art approaches of the text-, path- and rule-based\ncategories, respectively. We also compare our results to those four KG embedding-based\napproaches for which pre-trained DBpedia embedding models are available. We employ\nthese models for fact checking by training the neural network module 82 of our approach\nbased only on the output of the KG-based component. The output of this neural network\nmodule is then directly used as input for the final sigmoid function. We do not compare\nour results with results of the hybrid approaches mentioned in Section 3 because ExFaKT\nand Tracy mainly focus on generating human-comprehensible explanations and do not\nproduce the veracity score, and FACTY focuses on calculating the veracity of assertions\ncontaining long-tail vertices (i.e., entities from less popular domains, for example, cheese\nvarieties)."}, {"title": "6 Results and Discussion", "content": "Tables 4 and 5 show the AUROC scores for the different hybrid and competing ap-\nproaches on the FactBench train and test datasets, respectively. We can see that HybridFC\nperforms best when it uses the TransE embedding model. This is not unexpected as\nTransE is one of the simplest embedding models that supports property composition:\nGiven two properties p\u2081 and p2, TransE entails that (p1 p2) \u2248 \u03c6(p1) + (p2). With\nTransE as its embeddings model, HybridFC significantly outperforms all competing\napproaches on the test data.10\nNote that FactCheck does not achieve the performance reported in [47] within our\nevaluation. This is due to (i) the use of a different English Wikipedia as reference corpus-\nSyed et al. showed that they achieve better results with the larger ClueWeb corpus-and\n(ii) the fact that we had to remove triples from the FactBench dataset.\nThe overall performance of COPAAL is better than the performance of FactCheck,\nConEx, QMult and KV-Rule on the test set. However, we observe large performance\ndifferences with respect to the different properties. While COPAAL achieves the second\nbest AUROC scores after HybridFC for 6 out of the 8 properties it struggles to achieve\ngood results for: award and author. These experimental results suggest that our\napproach makes good use of the diversity of the performance of the approaches it\nincludes. In particular, it seems to rely on COPAAL's good performance on most of the\nproperties while being able to complement COPAAL's predictions with that of other\nalgorithms for properties on which COPAAL does not perform well.\nOn the BD dataset, KV-rule outperforms all other approaches on the test split. CO-\nPAAL achieves the second best score, closely followed by the TransE-based HybridFC"}, {"title": "7 Ablation Study", "content": "Our previous experiments suggest that HybridFC performs best in combination with\nTransE. Hence, we use it as default setting throughout the rest of the paper and overload\nHybridFC to mean HybridFC with TransE embeddings. To evaluate the contribution of\nthe different components of HybridFC to its performance, we rerun our evaluation for\neach component (i.e., text-based (TC), path-based (PC), and embedding-based (EC))\nindividually and as pairwise combination of different components (TC+PC, TC+EC,\nPC+EC). The results for the FactBench test and the BD datasets are shown in Tables 7a\nand 7b.11 The results suggest that the individual path-based and embedding-based\ncomponents achieve results similar to those of COPAAL and TransE, respectively. Our"}, {"title": "8 Conclusion", "content": "In this paper, we propose HybridFC\u2013a hybrid fact-checking approach for KGs. HybridFC\naims to alleviate the problem of manual feature engineering in text-based approaches,\ncases in which paths between subjects and objects are unavailable to path-based ap-\nproaches, and the poor performance of pure KG-embedding-based approaches by com-\nbining these three categories of approaches. We compare HybridFC to the state of the\nart in fact checking for KGs. Our experiments show that our hybrid approach is able\nto outperform competing approaches in the majority of cases. As future work, we will\nexploit the modularity of HybridFC by integrating rule-based approaches. We also plan\nto explore other possibilities to select the best evidence sentences."}]}