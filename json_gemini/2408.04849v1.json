{"title": "Ensemble BERT: A student social network text sentiment\nclassification model based on ensemble learning and BERT\narchitecture.", "authors": ["Kai Jiang", "Honghao Yang", "Yuexian Wang", "Qianru Chen", "Yiming Luo"], "abstract": "The mental health assessment of middle school students has always been one of the\nfocuses in the field of education. This paper introduces a new ensemble learning network based\non BERT, employing the concept of enhancing model performance by integrating multiple\nclassifiers. We trained a range of BERT-based learners, which combined using the majority\nvoting method. We collect social network text data of middle school students through China's\nWeibo and apply the method to the task of classifying emotional tendencies in middle school\nstudents' social network texts. Experimental results suggest that the ensemble learning network\nhas a better performance than the base model and the performance of the ensemble learning\nmodel, consisting of three single-layer BERT models, is barely the same as a three-layer BERT\nmodel but requires 11.58% more training time. Therefore, in terms of balancing prediction\neffect and efficiency, the deeper BERT network should be preferred for training. However, for\ninterpretability, network ensembles can provide acceptable solutions.", "sections": [{"title": "1. Introduction", "content": "Research based on neural network technology has gradually become more and more widely used in\nmany fields [1]. With the rapid growth of Deep Learning, especially in the field of Natural Language\nProcessing (NLP), a growing number of researchers are exploring the implementation of deep\nnetworks for NLP tasks, especially text classification [2]. Deep networks, such as the transformational\nBERT, have achieved unprecedented performance in several NLP tasks. However, significantly\nincreasing the depth of networks often results in a substantial rise in computational costs, which is\nparticularly problematic in resource-limited scenarios. Additionally, deep networks face certain\nbarriers in terms of interpretability [3]. Therefore, this study introduces a novel ensemble learning\nframework that combines the powerful semantic capturing ability of transformers with the efficiency\nof ensemble learning. Ensemble learning can increase robustness and reduce bias, while shallow\nnetworks can facilitate interpretive analysis.\nThis paper aims to investigates the following questions, and the N is the number of single-layer\nBERT base model:\nQuestion 1: Can an ensemble of N single-layer base models achieve good results in student\nsentiment classification?\nQuestion 2: Does the predictive ability of an ensemble model with N single-layer base models\nsurpass that of training a single N-layered BERT model?\nQuestion 3: Is the training time for an ensemble of N single-layer base models shorter than training\na single N-layered BERT model when there is equal predictive accuracy?"}, {"title": "2. Dataset", "content": "We demonstrate the effectiveness of our approach by applying it to sentiment trend analysis tasks on\nsocial network text. The single-layer decoder architecture of the BERT model as the base model of the\nensemble is used, and the classification results are output using an average voting method. To simplify\nthe experiment, we select N(number) as 3. The data is social network data from students in three\nmiddle schools in Xiangtan City, Hunan Province, China. Weibo is the largest social network platform\nin China. After obtaining students' consent, students uploaded their Weibo accounts anonymously. We\ncollected the account data of 324 students and captured 100 Weibo text contents of each student in the\nlast 3 years. If there are not 100 pieces of content, all tweet content will be used. Finally, a total of\n30012 pieces of data were included. The students and their parents approved and consented to this data,\nand the students and psychological education experts double-checked the data annotation. Next, we\nwill present related work, followed by an explanation of the model architecture. Then, we will present\nand analyze the experimental results."}, {"title": "3. Related work", "content": "Traditional text vectorization methods such as one-hot encoding, bag-of-words, and TF-IDF\nmodels have inherent limitations [4]. They often lead to sparse text representations, resulting in\nsignificant computational overhead and insufficient understanding of contextual semantic relationships.\nThese limitations prevent them from solving the challenges posed by polysemy [5]. However,\nextracting text features through neural network models can effectively resolve these semantic issues in\ntext features, effectively addressing the challenges of polysemy and language Ambiguity in text\nclassification [6]. Particularly, the use of pre-trained models enhances the acquisition of text semantic\nrepresentations. Widely used neural network-based text representation models are modelled based on\nthe relationship between context and target words. The commonly used models include the\npre-training model based on the static word embedding method like Word2Vec and the BERT model,\nwhich based on the dynamic word embedding method.Word2Vec is a neural network that converts\neach word segment (token) in text data into a vector in k-dimensional space by training the network on\na given corpus[7].Training models based on Word2Vec include the Skip-gram model, which can\npredict the context of a known target word or token[6].BERT is a pre-training model based on deep\nlearning for language representation proposed by Google in 2018[8]. It is also one of the most\nimportant pre-training models in NLP now. It uses a bidirectional encoder structure, so it can well\ncapture the differences between text contexts. semantic relationship, and our base model comes from\nthis. The structure of BERT is shown in Figure 1. The first layer is the word embedding layer. After\nword embedding, it enters 12 transformer layers and obtains the final feature representation."}, {"title": "4. Model architecture and experimental process", "content": "We use a single-layer BERT network as the base model, integrate three base models, and finally\noutput the final classification based on the majority voting principle. The structure of the model is\nshown in figure 3.\nThe specific steps of the experiment are as follows: Python is used for the experimental processing,\nand pandas are used to read the data, which contains two columns: one for the review texts and the\nother for the sentiment labels. The review texts are then tokenized using the Chinese BERT tokenizer\n(Bert-base-Chinese). The tokenized inputs and sentiment labels are then converted into tensors and a\ntensor dataset is created. This dataset is then divided into training and validation sets. Three basic\nBERT models, each with a single hidden layer, are looped and trained. Each model is independently\ntrained using the train dataset and evaluated using the validate dataset. The weights of the models are\ninitialized with the built-in initial weights of the BERT model, and the order of the data batches is\nrandomized in each epoch during training. The predictions of all models are then combined using the\nmajority voting method to determine the final prediction labels. Metrics such as the accuracy,\nprecision, recall, F1 score, and confusion matrix of the ensemble prediction are calculated. For\ncomparative research, a three-layer BERT network and a full 12-layer BERT are also trained for text\nclassification, using an MLP for the output layer. The training times for these models are recorded\nusing the tqdm [10] toolkit. The training is conducted on a CPU, specifically a Ryzen 5800H, and each\nmodel is fixed to train for three epochs."}, {"title": "5. Model results", "content": "To answer the question 1, we compare the prediction effect of the base model and the prediction\neffect of ensemble learning."}, {"title": "6. Conclusion", "content": "This paper presents a novel ensemble learning network based on BERT. We trained a set of\nBERT-based learners and combined them using the majority voting method. We proposed three\nquestions and applied this method to classify emotional tendencies in the social network texts of\nmiddle school students. Experimental results indicate that ensemble learning shows some\nimprovement over the base model, but the performance of the ensemble learning model composed of\nthree single-layer BERT models is roughly equivalent to that of a three-layer BERT model, although it\nrequires additional training time. Therefore, in terms of predictive effectiveness, deeper networks\nshould be considered the preferred choice. However, in terms of interpretability, an ensemble of\nshallow networks may provide an acceptable solution. In addition, a limitation of this model is that it\nonly discusses the case where N equal to 3. Future research could explore the optimal balance between\ntime and efficiency for different numbers of layers and discuss the impact of different base models on\nsentiment classification of student texts."}]}