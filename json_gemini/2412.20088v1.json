{"title": "An archaeological Catalog Collection Method Based on Large Vision-Language Models", "authors": ["Honglin Pang", "Yi Chang", "Tianjing Duan", "Xi Yang"], "abstract": "Archaeological catalogs, containing key elements such as artifact images, morphological descriptions, and excavation information, are essential for studying artifact evolution and cultural inheritance. These data are widely scattered across publications, requiring automated collection methods. However, existing Large Vision-Language Models (VLMs) and their derivative data collection methods face challenges in accurate image detection and modal matching when processing archaeological catalogs, making automated collection difficult. To address these issues, we propose a novel archaeological catalog collection method based on Large Vision-Language Models that follows an approach comprising three modules: document localization, block comprehension and block matching. Through practical data collection from the Dabagou and Miaozigou pottery catalogs and comparison experiments, we demonstrate the effectiveness of our approach, providing a reliable solution for automated archaeological catalogs.", "sections": [{"title": "1 Introduction", "content": "Archaeological catalogs, containing crucial information such as artifact images, morphological descriptions, and excavation details, serve as fundamental resources for understanding artifact evolution patterns and cultural inheritance [9, 16]. These archaeological datasets can be leveraged for various computational tasks, including classification tasks for artifact categorization, generative tasks for artifacts reconstruction, and visual question answering tasks, as shown in Figure 1. However, these valuable data are widely dispersed across numerous archaeological publications, and Traditional manual collection methods are labor-intensive and error-prone, making it challenging to meet the growing demands of modern archaeological research.\nRecent advances in Large Vision-Language Models (VLMs), have shown promising potential in automated data collection and processing. These models have demonstrated remarkable capabilities in understanding and analyzing both visual and textual information in various fields. However, when applied to archaeological catalogs, existing VLMs[1, 2, 7, 8, 11, 15] and their derivative data collection methods [3, 10, 12-14] encounter technical challenges in accurate image detection and modal matching, leading to collection failures.\nTo address these challenges, we propose a novel archaeological catalog collection method based on large vision-language models that follows an approach comprising three modules: document localization, block comprehension, and block matching. First, we employ open-set object detection models to localize and segment document blocks. Then, we process these document blocks for comprehension and describe them in terms of attributes. Finally, we implement matching rules based on foreign key linkages and bipartite graph matching to complete modal matching.\nTo validate the effectiveness of our proposed method, we conducted practical data collection experiments and comparison experiments using the Dabagou and Miaozigou pottery catalogs. Through these experiments, we demonstrated that our approach significantly improves the accuracy of automated archaeological catalog collection."}, {"title": "2 Related Work", "content": "Recent advances in VLMs have enabled significant progress in data collection, particularly in two key aspects: key information extraction and relationship extraction.\nKey Information Extraction Based on VLM. Existing work can extract key information from documents [3, 10, 13, 14] and reorganize it into related claims [4]. This key information can be extracted from both text and images. Sometimes, the key information is in the form of images, and current methods [6, 7, 11, 15] can directly ground the information in images. However, for specialized data, direct grounding remains challenging. Particularly when images contain complex specialized information, existing grounding methods may not accurately identify and extract this information.\nRelationship Extraction Based on VLM. Recent methods have demonstrated the ability to leverage Vision-Language Models (VLMs) to extract relationships between entities and construct knowledge graphs [5]. LMDX [10] utilizes these relationships for binding, but it is limited to a single modality. On the other hand, PDFChatAnnotator [12] connects knowledge across different modalities within documents. However, the rules for modality binding in this method remain relatively simple, leading to potential issues in complex scenarios."}, {"title": "3 Method", "content": "To address the challenges in archaeological catalog collection, we propose a pipeline consisting of three main modules: Document Localization Module, Block Comprehension Module, and Block Matching Module (as illustrated in Figure 2). Document Localization Module first segments the input catalogs into distinct regions, separating images from their corresponding annotations. Subsequently, Block Comprehension Module processes both visual and textual information, converting them into structured textual attributes for storage. Finally, Block Matching Module employs matching algorithms to align information from different modalities, thereby completing the data collection process."}, {"title": "3.2 Document Localization Module", "content": "To accurately locate images and annotations in archaeological catalogs, we leverage an open-set object detection model with specifically designed prompts. Given an input catalog page I, we formulate the detection task as:\n$B = f(I, P_{img}, P_{text})$\nwhere B represents the detected boxes, f is the detection model, and $P_{img}$, $P_{text}$ denote the prompts designed for detecting image blocks and text blocks respectively. The model outputs two sets of bounding boxes B = {$B_{img}, B_{text}$ }, where $B_{img}$ = {$b_1^i, b_2^i, ..., b_n^i$ }represents image blocks and $B_{text}$ = {$b_1, b_2, ..., b_m$} represents text blocks. Each bounding box $b_k$ = ($x_c$, $y_c$, w, h) specifies the location and size of the detected region."}, {"title": "3.3 Block Comprehension Module", "content": "For each detected region, we employ a VLM to generate structured attributes. Given the set of bounding boxes B from the detection module, the comprehension process can be formalized as:\n$C = g(B, P_c)$\nwhere C = {$C_{img}, C_{text}$ } represents the comprehension results, consisting of image region comprehension results $C_{img}$ = {$c_1^i, c_2^i, ..., c_h^i$} and text region comprehension results $C_{text}$ = {$c_1, c_2, ..., c_m$}. g is the VLM, and $P_c$ is the prompt template guiding the description format. For each region $b_q \\in B$, its comprehension result $c_q$ is a dictionary containing key attributes:\n$c_q$ = {($k_1$ : $v_1$), ($k_2$ : $v_2$), ..., ($k_p$ : $v_p$)}"}, {"title": "3.4 Block Matching Module", "content": "To establish correspondences between image blocks and text blocks, we propose a two-stage matching strategy.\nForeign Key Matching. For each element $c_q$ in image region comprehension results $C_{img}$ and text region comprehension results $C_{text}$, we define its foreign key set $FK_q$ as:\n$FK_q$ = {$v_j|k_j \\in K_{foreign}$}\nwhere $K_{foreign}$ is the predefined set of foreign key attributes. The matching degree between two blocks $c_a$ and $c_b$ can be measured by the overlap of their foreign key sets:\n$\u041c(c_a, c_b) = \\frac{|FK_a \\cap FK_b|}{|FK_a \\cup FK_b|}$\nWhen $\u041c(c_a, c_b)$ = 1, indicating complete foreign key matching between two blocks, we consider these blocks to be corresponding. This can be represented as a set of matching pairs:\n$Matches_{FK}$ = {(ca, cb)|$c_a \\in C_{img}$, $c_b \\in C_{text}$, M(ca, cb) = 1}\nDistance Bipartite Matching. Multiple image regions may share identical foreign keys, especially when they belong to the same logical group or represent related information. For example, several product images might reference the same product ID. Therefore, for many-to-many matching cases in MatchesFK, we further refine the correspondences using bipartite matching based on spatial distance. Given a group of matched blocks G = {($c_{img}, c_{text}$)} $\\subset G_k$ where $G_k$ represents one many-to-many matching group, we construct a bipartite graph where nodes represent image blocks and text blocks respectively.\nThe cost matrix D for bipartite matching is defined using the Euclidean distance between block centers. We then solve the minimum-weight bipartite matching problem. The final block correspondences are obtained by combining both matching stages. The process can be expressed using the following formula:\n$D_{ij} = ||center(b_{img}) \u2013 center(b_{text})||^2$\n$M^* = arg \\min \\sum_{(i,j) \\in G_k} D_{ij}$\n$Matches_{final}$ = ($Matches_{FK}$ $\\cup G_k$) $\\cup M^*$\nwhere center() returns the center coordinates of a block's bounding box, $M^*$ is the optimal bipartite matching for group $G_k$. This process preserves one-to-one matches from the foreign key stage while resolving many-to-many cases through distance-based bipartite matching."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Data Collection and Statistics.", "content": "The pottery data collection from Miaozigou and Dabagou sites is crucial for archaeological research, as these sites represent the significant Miaozigou Culture. This data provides essential evidence for understanding the unique developmental trajectory of Neolithic cultures in central and southern Inner Mongolia.\nUsing the proposed method, we collected 302 pottery catalog PDF pages from Dabagou and Miaozigou sites. With minimal manual correction, we obtained 2,301 data pairs, demonstrating the effectiveness of our approach.\nAnnotations. We compiled the collected attributes, location information and file information into annotations. For each sample, we recorded the following information: original catalog figure name, pottery image, Excavation unit, morphological class and bounding box coordinates.\nStatistics. We conducted statistical analysis on the dataset as shown in Figure 4. The dataset contains 44 classes. As can be observed, the distribution across shape classes exhibits a long-tail pattern, with significant class imbalance. Additionally, we analyzed the number of artifacts per excavation unit, These artifacts were excavated from 310 different units, with most units containing fewer than 10 artifacts."}, {"title": "4.2 Comparison Experiments", "content": "We use the cleaned dataset as ground truth to calculate metrics for results obtained from each method, in order to evaluate their effectiveness.\nMetrics. We evaluate our method using Average Precision (AP) with an IoU threshold of 90%. The high IoU threshold ensures strict evaluation of localization accuracy.\nComparison between different VLMs. To demonstrate the generalizability of our pipeline across different vision-language models, we conduct experiments with GPT-40[8], Claude 3.5[1] Sonnet, and Qwen-VL[2].\nAs shown in Table 1, Claude 3.5 Sonnet achieves the best performance with an AP of 35.4%, outperforming other VLMs. This demonstrates that while our pipeline is model-agnostic, the choice of VLM can impact the overall performance.\nComparison with baseline. We compare our method with two baseline methods: (1) using GPT 40, Claude 3.5 Sonnet and Qwen-VL alone for detection and comprehension (2) combining Claude 3.5 Sonnet with open-set object detection. In both (1) and (2), we first use a VLM to analyze catalog numbers, excavation units, and other information. In (1), we then use the same VLM model to ground the artifact images, while in (2), we apply Grounding-DINO model for grounding the images. The results are presented in Table 1.\nOur experimental results validate the effectiveness of the proposed pipeline. While GPT-40 and Claude 3.5 Sonnet lack object detection capabilities and thus could not complete the catalog collection task, our method achieved a 33.8% improvement in AP compared to using Qwen-VL alone. Although large language models can leverage existing object detection models, these detectors'"}, {"title": "5 Conclusion", "content": "In this paper, we presented a novel approach for archaeological catalog collection utilizing Large Vision-Language Models. Our method addresses the challenges in automated archaeological data processing through localization, comprehension and modal matching.\nExperimental validation using the Dabagou and Miaozigou pottery catalogs demonstrated the effectiveness and reliability of our approach. The results show significant improvements in automated archaeological catalog collection accuracy compared to existing VLM-based methods. Our proposed framework provides a robust solution for processing archaeological documentation.\nFuture work could focus on extending this approach to diverse archaeological artifacts and exploring additional matching algorithms to further enhance the system's performance."}]}