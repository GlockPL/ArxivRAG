{"title": "Cycle Pixel Difference Network for Crisp Edge Detection", "authors": ["Changsong Liu", "Wei Zhang", "Yanyan Liu", "Mingyang Li", "Wenlin Li", "Yimeng Fan", "Xiangnan Bai", "Liang Zhang"], "abstract": "Edge detection, as a fundamental task in computer vision, has garnered increasing attention. The advent of deep learning has significantly advanced this field. However, recent deep learning-based methods which rely on large-scale pre-trained weights cannot be trained from scratch, with very limited research addressing this issue. This paper proposes a novel cycle pixel difference convolution (CPDC), which effectively integrates image gradient information with modern convolution operations. Based on the CPDC, we develop a U-shape encoder-decoder model named CPD-Net, which is a purely end-to-end network. Additionally, to address the issue of edge thickness produced by most existing methods, we construct a multi-scale information enhancement module (MSEM) to enhance the discriminative ability of the model, thereby generating crisp and clean contour maps. Comprehensive experiments conducted on three standard benchmarks demonstrate that our method achieves competitive performance on the BSDS500 dataset (ODS=0.813), NYUD-V2 (ODS=0.760), and BIPED dataset (ODS=0.898). Our approach provides a novel perspective for addressing these challenges in edge detection.", "sections": [{"title": "1. Introduction", "content": "Edge detection is a fundamental task in digital image processing and computer vision that aims to identify points in a digital image where the intensity changes sharply or has discontinuities. These points, commonly organized into a set of curved line segments termed edges, are crucial in understanding image features and content. The significance of edge detection lies in its ability to reduce the amount of data to be processed by filtering out less relevant information while preserving the important structural properties of an image. This process is essential in various applications, including object detection [10, 24], image segmentation [9, 12, 32], and salient object detection [11, 21, 33].\nThe advent of deep learning has significantly propelled advancements in edge detection. Neural networks, particularly convolutional neural networks (CNNs), have demonstrated remarkable capabilities in learning complex features and patterns, leading to substantial improvements in edge detection performance and robustness. However, two major issues still need to be addressed currently: (1) most CNN-based methods rely on large-scale pre-trained models, resulting in low feature utilization efficiency and expensive computational cost; (2) most CNN-based methods produce thick and noisy edge maps, which is detrimental to their application in high-level vision tasks.\nIn this work, we attempt to address these two issues simultaneously. Our goal is to implement training of the network from scratch and guide it to generate crisp and clean edge maps. In response to issue (1), we leverage the idea from traditional gradient-based operators, incorporating such a concept into the modern CNN architecture, developing a cycle pixel difference convolution (CPDC) for targeted and efficient encoding of image edge features. Utilizing the proposed CPDC, we construct fundamental building blocks, upon which we further develop a four-stage backbone network. This backbone network generates feature maps rich in edge information, thereby enhancing feature utilization efficiency without necessitating pre-training on large-scale datasets. As for issue (2), the problem of edge thickness reflects the model with insufficient discriminative capability for edge pixels. Therefore, we design a multi-scale information enhancement module (MSEM) that can capture multi-scale contextual information to improve the discriminative ability of the model. Additionally, we introduce the Squeeze-and-Excitation [15] channel attention mechanism into MSEM, enabling the model to focus more effectively on edge information.\nIn addition, we build a dual residual connection-based (DRC) decoder to increase the robustness of the model by mitigating the reliance on a single propagation path. We also add a lateral connection between the original input image and the last stage DRC decoder output. The lateral connection includes a ConvNext-V2 [36] module to capture the long-range information for precise object boundary delineation. In the end, we propose a U-shape encoder-decoder network named CPD-Net which consists of these three parts. We perform a series of experiments to demonstrate the effectiveness of CPD-Net, and the main contributions of our work can be summarized as follows:\n1. We integrate the traditional gradient concept with modern convolution operation, developing a novel cycle pixel difference convolution (CPDC), which enables effective encoding of image edge information, thereby mitigating the reliance on large-scale pre-trained models.\n2. We propose a U-shape encoder-decoder network named CPD-Net for edge detection. Our CPD-Net mainly consists of three parts: a four-stage backbone based on the CPDC, a multi-scale information enhancement module (MSEM), and a dual residual connection-based (DRC) decoder.\n3. We perform extensive experiments to demonstrate the advantages of CPD-Net and the results show that our method can achieve a comparable performance on three benchmark datasets without any large-scale pre-trained weights.\nThe structure of this paper is as follows: Section 2 reviews relevant literature in the field. Section 3 provides a detailed description of our method. Section 4 presents a comprehensive evaluation of our method, including the implementation details, the evaluation protocol, an ablation study of each component, and a comparison with current SOTA edge detection methods. Finally, Section 5 concludes the paper with a discussion of our work and potential avenues for future research."}, {"title": "2. Related Work", "content": "Edge detection, a fundamental problem in computer vision, has been the subject of intensive research for over four decades, resulting in a vast body of literature. This section provides an overview of representative works in this field, categorized into two main groups: traditional approaches and deep learning-based methods.\nTraditional approaches: Early edge detection approaches primarily relied on image derivative calculations. Roberts [25] introduces a simple first-order derivative operator using diagonal pixel differences, while Sobel [29] employs 3\u00d73 kernels for horizontal and vertical gradient computation. Canny [3] enhances robustness through a multi-stage process involving noise reduction, gradient calculation, non-maximum suppression, and hysteresis thresholding. The Laplacian detector [17] utilizes second-order derivatives to identify rapid intensity changes. Subsequent research integrates additional low-level features such as image texture, color, and gradient information. Methods like Pb [22], gPb [1], and SE [7] employ a classifier to generate object-level boundaries. While these approaches demonstrate improved performance over purely derivative-based methods, their reliance on hand-crafted features and lack of semantic information ultimately constrains their potential for further advancement.\nDeep learning-based methods: In recent years, state-of-the-art (SOTA) edge detection methods mainly adopt convolutional neural networks (CNNs). These methods leverage the strong feature extraction ability of CNNs, achieving an impressive performance with higher F-scores, and some of them even surpass humans on several benchmarks. HED [37] employs a fully convolutional neural network with multi-scale side outputs to perform end-to-end edge detection. They also propose a weighted cross-entropy loss function for solving the issue of imbalanced pixel distribution. RCF [20] further utilizes features from all convolutional layers in a fully convolutional network to capture both fine details and high-level semantics, resulting in accurate edge maps. BDCN [13] employs a bi-directional cascade structure to train each network layer with layer-specific supervision, allowing it to focus on edges at different scales. However, these methods generally suffer from the problem of producing excessively thick boundaries.\nConsequently, researchers have proposed some solutions to address this issue. CED [34] introduces a novel refinement architecture for edge detection, incorporating a top-down backward refinement pathway and sub-pixel convolution to improve the location accuracy of edge pixels. LPCB [6] employs an encoder-decoder network with a bottom-up/top-down architecture to leverage multi-scale features, enabling the generation of crisp and accurate edge maps without post-processing. DRC [4] proposes a novel deep refinement network for crisp boundary detection, which utilizes multiple refinement modules and an adaptive weighting loss function to achieve more accurate edge feature representation. DSCD [5] introduces a novel loss function based on SSIM [35] that penalizes the structural difference between prediction and groundtruth, as well as a hyper convolutional module on top of an encoder-decoder network to enhance semantic feature extraction, achieving SOTA performance. CATS [16] proposes a tracing loss that performs feature unmixing by tracing boundaries, and a context-aware fusion block that tackles side mixing by aggregating complementary merits of side edges, resulting in more accurately localized edge predictions without relying on post-processing. EDTER [23] utilizes a global transformer encoder to capture long-range context information in Stage I and a local transformer encoder to extract fine-grained cues in Stage II, effectively combining global and local information for crisp and accurate edge detection.\nAlthough these methods have achieved significant performance, they all rely on large-scale pre-trained weights, leading to cumbersome training processes and excessive parameter counts. Therefore, the method proposed in this paper successfully achieves training the network from scratch while simultaneously enhancing the ability of the model to locate edge pixels, thereby generating crisp edge maps."}, {"title": "3. Methodology", "content": "In this section, we describe our CPD-Net in detail. The whole network adopts an asymmetric U-shape architecture, which mainly consists of three parts as shown in Fig. 1: encoder, skip-connection, and decoder. Specifically, the encoder part consists of the cycle pixel difference convolution block (CPDC block), the multi-scale information enhancement module (MSEM) serves as the skip-connection, and the decoder component is the dual residual connection-based (DRC) decoder.\nAdditionally, we add a lateral connection in the whole network and introduce the ConvNext-V2 module into this structure. The ConvNext V2 module employs 7 \u00d7 7 convolution, which increases the receptive field, enabling the network to capture a wider range of features. It also utilizes the GELU activation function, effectively mitigating the issue of gradient explosion and enhancing the nonlinear expressive capability of the model. These advantages significantly contribute to improving the completeness of contour extraction. The final prediction of the network is obtained by channel-wise concatenation of the outputs from the lateral connection and the DRC decoder."}, {"title": "3.1. Cycle pixel difference convolution", "content": "In current SOTA edge detection methods, most of them employ classification networks pre-trained on ImageNet as a backbone, such as VGG [28] and ResNet [14], and are then fine-tuned on edge detection datasets through transfer learning to output the final edge maps. Although such a strategy enhances the performance of edge detection algorithms, it also results in low feature utilization. Inspired by Sobel operator and Roberts operator, we explicitly encode edge features and embed them into standard convolution, developing four directions of cycle pixel difference convolution (CPDC) operators: CPDC_v (vertical), CPDC_h (horizontal), CPDC_c (cross), and CPDC_d (diagonal). The proposed CPDC effectively encodes image complete edge features from four specific directions, thereby enabling training the network from scratch and reducing the number of parameters."}, {"title": "3.2. Multi-scale information enhancement module", "content": "Contemporary CNN-based edge detection methods tend to produce edge maps that are excessively thick and blurry. This quality of edge maps typically reflects the insufficient discriminative capability of the model. Therefore, it is crucial to enhance the discriminative power of the model to locate edge pixels more accurately.\nBased on the above analysis, we propose a multi-scale information enhancement module (MSEM) to improve the discriminative ability. MSEM leverages dilated convolutions with different dilation rates to capture multi-scale information. Smaller dilation rates capture local details, facilitating precise localization of fine edges, while larger dilation rates expand the receptive field, aiding in the capture of long-range information.\nAs illustrated in Fig. 5, MSEM consists of four parallel branches. Each branch initially compresses the feature map channel into 1/4 through 1\u00d71 convolution, followed by 3\u00d73 convolutions with four distinct dilation rates r \u2208 {1,2,3,4} to capture multi-scale contextual information. The resulting four scales of feature maps are then concatenated along the channel dimension to restore the number of channels. A 1 \u00d7 1 convolution is subsequently applied to the concatenated feature maps for information fusion. Finally, the fused feature maps are passed through the Squeeze-and-Excitation (SE) module.\nThe SE attention mechanism enhances channels relevant to edge pixels while suppressing irrelevant ones, implementing an adaptive feature selection mechanism. This strategy increases the signal-to-noise ratio of edge features, thereby enhancing the discriminative capability of the model. Furthermore, we introduce a residual connection between the input and the output, which not only accelerates the network convergence but also promotes the capacity of complex feature representation. The MSEM is formulated as follows:\n$MSEM = I + SE[C_{r=1}^4 \\sigma(Conv_{1x1}^r(\\sigma(Conv_{3x3}^r(I))))]$"}, {"content": "Contemporary CNN-based edge detection methods tend to produce edge maps that are excessively thick and blurry. This quality of edge maps typically reflects the insufficient discriminative capability of the model. Therefore, it is crucial to enhance the discriminative power of the model to locate edge pixels more accurately.\nBased on the above analysis, we propose a multi-scale information enhancement module (MSEM) to improve the discriminative ability. MSEM leverages dilated convolutions with different dilation rates to capture multi-scale information. Smaller dilation rates capture local details, facilitating precise localization of fine edges, while larger dilation rates expand the receptive field, aiding in the capture of long-range information.\nAs illustrated in Fig. 5, MSEM consists of four parallel branches. Each branch initially compresses the feature map channel into 1/4 through 1\u00d71 convolution, followed by 3\u00d73 convolutions with four distinct dilation rates r \u2208 {1,2,3,4} to capture multi-scale contextual information. The resulting four scales of feature maps are then concatenated along the channel dimension to restore the number of channels. A 1 \u00d7 1 convolution is subsequently applied to the concatenated feature maps for information fusion. Finally, the fused feature maps are passed through the Squeeze-and-Excitation (SE) module.\nThe SE attention mechanism enhances channels relevant to edge pixels while suppressing irrelevant ones, implementing an adaptive feature selection mechanism. This strategy increases the signal-to-noise ratio of edge features, thereby enhancing the discriminative capability of the model. Furthermore, we introduce a residual connection between the input and the output, which not only accelerates the network convergence but also promotes the capacity of complex feature representation. The MSEM is formulated as follows:\n$MSEM = I + SE[C_{r=1}^4 \\sigma(Conv_{1x1}^r(\\sigma(Conv_{3x3}^r(I))))]$"}, {"title": "3.3. Dual residual connection-based decoder", "content": "The proposed dual residual connection-based (DRC) decoder represents an innovative architectural paradigm designed to decode and refine edge feature maps. This sophisticated structure leverages dual residual connections, strategically positioned convolutional layers, and non-linear activations to enhance edge localization and definition. The primary residual connection spans the entire module, while a secondary residual path links the initial and terminal 1\u00d71 convolutions, establishing a multi-path information flow. This design effectively mitigates the gradient vanishing problem and preserves edge information as completely as possible, which is crucial for accurate edge detection. The whole architecture of the DRC decoder is shown in Fig. 6.\nThe alternating deployment of 3\u00d73 and 1\u00d71 convolutions facilitates both local spatial feature extraction and cross-channel information integration, thereby enabling the effective fusion of multi-path features. The 1 \u00d7 1 convolutions serve as feature purification mechanisms, accentuating edge-related channels while suppressing noise. Notably, the strategic placement of ReLU activations at the end of element-wise addition, rather than directly at the beginning, allows for the propagation of negative values, preserving fine edge details that might otherwise be lost.\nThis intricate design theoretically enhances the discriminative power of the model, significantly improving its ability to distinguish between edge and non-edge features. It directly addresses the common issues of edge thickening and blurring often encountered in traditional CNN-based edge detection methods. In essence, our DRC decoder presents a novel approach to effectively decode deep features into high-quality edge representations. Its unique architecture promises to advance the state of the art in edge detection, offering improved accuracy and robustness in various challenging scenarios."}, {"title": "3.4. Hybrid focal loss function", "content": "We adopt our pioneering work [19] proposed hybrid focal loss function to address the imbalanced pixel distribution issue in edge detection. The hybrid focal loss function can be decomposed into two constituent parts: the focal Tversky loss and the focal loss. The focal Tversky loss can be written as follows:\n$L_{FT} = \\frac{\\sum_{i=1}^N p_i g_i + C}{\\sum_{i=1}^N p_i g_i + \\alpha \\sum_{i=1}^N (p_i (1-g_i))^2 + \\beta \\sum_{i=1}^N ((1-p_i)g_i)^2 + C}$ \nwhere p\u1d62 and g\u1d62 represent the value of i-th pixel on an output edge map and its corresponding label image, respectively. p\u1d62g\u1d62, p\u1d62(1 - g\u1d62), and (1 \u2212 p\u1d62)g\u1d62 represent true edge pixels (TPs), false edge pixels (FPs), and false non-edge pixels (FNs) in an image, respectively. y = 0.75 represents the focusing parameter and C is a constant number whose value is 1 x 10-7 to prevent the numerator/denominator from being 0. We set \u03b1 = 0.3 and \u03b2 = 0.7 to balance the weights between the FPs and FNs. The focal Tversky loss guides the model training process from image-level information.\nThe focal loss can be defined as $L_{FL} = -\\omega \\sum_{i=1}^N [(1-p_i)^\\delta g_i log p_i + p_i (1-g_i) log (1 - p_i)]$ where N represents the total number of pixels in an image. (1 \u2212 p\u1d62)\u1d79 is a modulating factor and \u03c9 is a balance factor for positive and negative pixels. Here, we set \u03c9 = 0.25 and \u03b4 = 2. The focal loss constraints the model training process from pixel-level information.\nIn the end, the whole hybrid focal loss is a weighted fusion between the focal Tversky loss and the focal loss, which can be defined as follows:\n$L_H = L_{FT} + \\lambda L_{FL}$ \nwhere the \u03bb = 0.001 to balance the weight between these two parts. The hybrid focal loss function can effectively address the issue of imbalanced class distribution from both image-level and pixel-level information, consequently guiding the network to generate high-quality edge maps."}, {"title": "4. Experiments", "content": "In this section, we thoroughly describe the implementation details, including the training hyper-parameters, datasets description, and augmentation strategy. Next, we present the evaluation method we utilize in this study, followed by a series of ablation experiments on our approach. Finally, we compare the CPD-Net with several state-of-the-art (SOTA) edge detection methods, showcasing the effectiveness of our method."}, {"title": "4.1. Implementation details", "content": null}, {"title": "4.1.1. Training hyper-parameters", "content": "We use the PyTorch deep learning toolchain to build our network. The training hyper-parameters are set as follows: the mini-batch is 8, the initial learning rate is 1 \u00d7 10\u20134, and the number of training epochs is 25. The learning rate is reduced by a factor of 10 every 5 epochs, and the Adam [18] optimizer is employed to optimize the parameters of our network where the weight decay is set to 5 \u00d7 10\u20134. All the experiments are conducted on a single Tesla A40 GPU."}, {"title": "4.1.2. Datasets description", "content": "BSDS500: The Berkeley Segmentation Dataset and Benchmark (BSDS500) is a widely recognized dataset in the field of computer vision, specifically designed for evaluating edge detection algorithms. This dataset comprises 500 natural images, accompanied by multiple human-annotated groundtruth boundaries. The BSDS500 is structured with 200 training images, 100 validation images, and 200 test images, all standardized to a 481 \u00d7 321 pixel resolution. Its diverse collection of images, spanning various natural scenes, coupled with multiple expert annotations per image, provides a robust framework for assessing the performance of edge detection algorithms.\nNYUD-V2: The NYU Depth Dataset V2 (NYUD-V2) is a significant benchmark dataset in computer vision, particularly valuable for edge detection research. It consists of 1449 RGB-D image pairs captured from 464 diverse indoor scenes using Microsoft Kinect sensors. The dataset provides rich information for edge detection tasks, including RGB images, corresponding HHA maps, and human-annotated edge maps. The importance of NYUD-V2 in edge detection stems from its multimodal nature, combining color and depth information, which enables researchers to develop and evaluate RGB-D edge detection methods. The dataset with diverse indoor scenes and high-quality annotations make it an invaluable resource for advancing edge detection techniques capable of handling complex, real-world environments.\nBIPED: The Barcelona Images for Perceptual Edge Detection (BIPED) dataset is a significant contribution to edge detection research, specifically designed to address limitations in existing datasets. Comprising 250 high-resolution (1280 x 720 pixels) outdoor scenes captured using multiple cameras, BIPED offers diverse environments and lighting conditions. Its key features include pixel-accurate groundtruth edge maps manually annotated by experts, focusing on perceptually relevant edges including both object boundaries and salient texture edges. BIPED is particularly valuable for developing and evaluating robust edge detection algorithms capable of handling the complexities of outdoor environments."}, {"title": "4.1.3. Augmentation strategy", "content": "To enhance the generalization capability of our model, we implement a comprehensive data augmentation strategy. Specifically, our protocol involved a series of geometric transformations applied to the image-label pairs. First, we systematically rotate each pair at 15\u00b0 intervals, encompassing a full 360\u00b0 range. Subsequently, we apply random cropping and flipping operations to these rotated images, ensuring that the original resolution is maintained throughout the process. It is noteworthy that this augmentation methodology is consistently applied across all three datasets utilized in our study."}, {"title": "4.2. Evaluation protocol", "content": "In this work, we report three standard metrics for evaluating the quality of generated edge maps: ODS F-score, OIS F-score, and AP. All of them are based on the Precision $PR = \\frac{TP}{TP+FP}$ and Recall $RE = \\frac{TP}{TP+FN}$, where TP, FP, and FN represent the number of correctly classified edge pixels, the number of incorrectly classified edge pixels, and the number of missed edge pixels, respectively. The AP is derived by calculating the area under the Precision-Recall curve. As for F-score $F = \\frac{2*PR*RE}{PR+RE}$ is calculated in two ways: a) aggregating F-scores across all images, where an optimal fixed threshold has been determined for the whole dataset yields the optimal dataset scale (ODS) F-score; b) aggregating the optimal F-score for each image, which is extracted from all possible confidence thresholds yields the optimal image scale (OIS) F-score. The equations for ODS and OIS are depicted as follows:\n$ODS = max_{\\tau} {\\frac{1}{N_{img}} \\sum_{i} {F_i^\\tau} | \\forall \\tau \\in [0.01, ..., 0.99]}}$\n$OIS = \\frac{1}{N_{img}} \\sum_{i} {max_{\\tau} F_i^\\tau | \\forall \\tau \\in [0.01, ..., 0.99]}$ \nwhere Nimg indicates the total number of images in the dataset, i represents the index of images, t means the confident threshold, and F refers to the F-score."}, {"title": "4.3. Ablation study", "content": "This subsection presents a series of ablation experiments for evaluating key components and settings of our CPD-Net. All these experiments are conducted on the BSDS500 benchmark.\nWe first explore the impact of the base channel count in the backbone network on overall performance. The comparative results are presented in Table 1. As evidenced by the results, there is a clear positive correlation between an increase in channel count and enhanced network performance. It is noteworthy that even our most computationally expensive network configuration maintains a relatively modest parameter size of 9.75M.\nWe also demonstrate the effectiveness of each component and the results are shown in Table 2. We adopt the CPD-Net with a base channel count of 16 for this ablation study. It can be observed that a substantial decline in performance when the cycle pixel difference convolution is replaced with standard convolution. This significant performance gap provides compelling evidence for the effectiveness of our novel convolution operator. In addition, the removal of the MSEM and the DRC decoder results in varying degrees of performance degradation, which underscores the crucial roles that both the MSEM and DRC decoder play in enhancing the overall capability of the network, thereby validating the judicious design choices incorporated in the proposed method."}, {"title": "4.4. Comparison to SOTA methods", "content": "In this subsection, we compare our CPD-Net with some other SOTA methods, conducting comparative experiments using the three datasets mentioned earlier: BSDS500 [1], NYUD-V2 [27], and BIPED [30].\nBSDS500: Firstly, we evaluate the proposed method against several high-performing edge detection algorithms using the BSDS500 dataset. The comparison includes a selection of recent SOTA edge detectors, which can be categorized into two distinct groups: the first is traditional methods including Canny, gPb-UCM, and SE; the second is deep learning-based methods including DeepContour [26], DeepEdge [2], HED [37], RCF [20], BDCN [13], CED [34], LPCB [6], DRC [4], PiDiNet [31], EDTER [23] and CHRNet [8]. Table 3 presents the quantitative comparison results and Fig. 7 shows some examples from different methods. The Precision-Recall curves are drawn in Fig. 8 (a).\nThe comparative results presented in Fig. 7 demonstrate the efficacy of our method across diverse image types, including portraits, natural scenes, and wildlife. Our approach exhibits remarkable performance in capturing salient edges while preserving fine details, as evidenced by the clear delineation of facial features, clothing textures, and avian plumage. In comparison to established algorithms such as HED, RCF, and CED, our method achieves a superior balance between edge crispness and noise suppression. Notably, it excels in complex textural regions, such as the intricate patterns of foliage and feathers, where other methods over-segment or lose vital details. The results suggest that our method maintains high fidelity to the label image while surpassing existing techniques in terms of edge coherence and detail retention. This comprehensive evaluation across varied visual contexts underscores the robustness and versatility of our proposed edge detection methodology, indicating its potential to advance the SOTA in computer vision applications.\nThe quantitative results shown in Table 3 and Fig. 8 (a) offer a more direct comparison of various edge detection methods. Remarkably, our method demonstrates highly competitive performance against recent SOTA methods, despite being trained from scratch without any pre-trained weights. This is a significant achievement, as most of the other deep learning-based methods in the comparison utilize pre-trained models. Our best performance achieves impressive scores of 0.813, 0.835, and 0.840 for ODS, OIS, and AP respectively, positioning it among the top-performing methods. EDTER exhibits the highest overall performance across the three metrics. However, the marginal difference between EDTER and our method (0.011 in ODS, 0.006 in OIS, and 0.040 in AP) underscores the efficacy of our approach, especially considering the independence of our method from pre-training. Furthermore, our method demonstrates substantially superior performance compared to PiDiNet, which is also trained from scratch. Specifically, our model outperforms PiDiNet by a margin of 3.04% in ODS and 3.99% in OIS. These comparison results fully demonstrate the effectiveness of our method.\nNYUD-V2: We perform another comparison experiment on the NYUD-V2 dataset. As shown in Fig. 9, despite NYUD-V2 predominantly featuring indoor scenes, our method exhibits consistent performance with that observed on the BSDS500 dataset. The contour maps predicted by the CPD-Net are clean and crisp, demonstrating robust generalization capabilities and superior pixel-level discriminative power. Compared with DRC, whose purpose is also addressing the thickness problem of boundaries, our approach consistently produces more refined and continuous edges that closely align with the groundtruth, suggesting enhanced fidelity to the actual structure scene.\nTable 4 and Fig. 8 (b) present the quantitative results of various edge detection methodologies on the NYUD-V2 dataset across RGB images, HHA images, and averaged input modalities. Our method demonstrates robust performance, achieving competitive results across all input types. Specifically, our RGB-HHA version (ODS=0.760, OIS=0.775, AP=0.770) performs on par with SOTA methods such as BDCN-RGB-HHA (ODS=0.765, OIS=0.781, AP=0.813) and DRC-RGB-HHA (ODS=0.769, OIS=0.782, AP=0.771), but we do not utilize any pre-trained weights. The consistent performance of our method across different datasets demonstrates its adaptability and stability. These comparison results validate the efficacy of our approach.\nBIPED: To further demonstrate the efficacy of our proposed method, we conduct additional comparison experiments utilizing the BIPED dataset. It is noteworthy that, due to the recent proposal of the BIPED dataset, a limited number of methods have been evaluated on this benchmark. Consequently, we selected four deep learning-based SOTA approaches for comparative analysis: RCF [20], BDCN [13], CATS [16], and DexiNed [30]. The quantitative results of this comparative study are presented in Table 5. Our method achieves the top performance among several SOTA edge detection algorithms using transfer learning. Specifically, we can observe an enhancement across all three metrics compared to DexiNed. By achieving SOTA performance without relying on pre-trained weights, our method challenges the prevailing paradigm in the field and opens new avenues for developing highly effective, yet computationally efficient, edge detection methods."}, {"title": "5. Conclusion", "content": "In this work, we proposed a novel cycle pixel difference network (CPD-Net) for crisp edge detection. The key innovations of our approach include: 1) a new backbone based on cycle pixel difference convolution (CPDC) that effectively encodes edge features from four directions, enabling training from scratch without relying on pre-trained weights; 2) a multi-scale information enhancement module (MSEM) that improves the discriminative ability of the model for edge pixels; 3) a dual residual connection-based (DRC) decoder that enhances edge localization. Extensive experiments on three benchmark datasets demonstrate that our method achieves competitive performance compared to state-of-the-art approaches, even without using pre-trained weights. Notably, our approach generates cleaner and crisper edge maps while maintaining accuracy. This work provides a new perspective on edge detection by showing it is possible to achieve high performance without relying on large-scale pre-trained models, potentially opening new avenues for efficient edge detection in resource-constrained environments. Future work could explore the application of our method to other vision tasks."}]}