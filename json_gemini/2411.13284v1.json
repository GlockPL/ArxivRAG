{"title": "DATTA: Domain-Adversarial Test-Time Adaptation\nfor Cross-Domain WiFi-Based Human Activity Recognition", "authors": ["Julian Strohmayer", "Rafael Sterzinger", "Matthias W\u00f6dlinger", "Martin Kampel"], "abstract": "Cross-domain generalization is an open problem in WiFi-\nbased sensing due to variations in environments, devices,\nand subjects, causing domain shifts in channel state infor-\nmation. To address this, we propose Domain-Adversarial\nTest-Time Adaptation (DATTA), a novel framework combin-\ning domain-adversarial training (DAT), test-time adapta-\ntion (TTA), and weight resetting to facilitate adaptation to\nunseen target domains and to prevent catastrophic forget-\nting. DATTA is integrated into a lightweight, flexible ar-\nchitecture optimized for speed. We conduct a comprehen-\nsive evaluation of DATTA, including an ablation study on\nall key components using publicly available data, and ver-\nify its suitability for real-time applications such as human\nactivity recognition. When combining a SotA video-based\nvariant of TTA with WiFi-based DAT and comparing it to\nDATTA, our method achieves an 8.1% higher F1-Score. The\nPyTorch implementation of DATTA is publicly available at:\nhttps://github.com/StrohmayerJ/DATTA.", "sections": [{"title": "1. Introduction", "content": "WiFi has emerged as a promising modality in person-centric\nsensing due to its advantages over optical approaches such\nas cost-effectiveness, unobtrusiveness, visual privacy pro-\ntection, and the ability to perform long-range sensing\nthrough walls [3, 17]. Jointly, these qualities enable ef-\nficient, contactless monitoring of human activities in con-\nfined indoor environments without per-room sensor deploy-\nment, providing a significant economic advantage [21].\nChannel State Information (CSI) serves as the founda-\ntion for modern WiFi-based person-centric sensing. CSI\nis a metric obtained in the Orthogonal Frequency-Division\nMultiplexing (OFDM) scheme, which subdivides a WiFi\nchannel into multiple sub-channels with different carrier\nfrequencies (subcarriers) [9]. This subdivision allows for\nfast, parallel transmission of data, while CSI provides de-\ntailed information about how each subcarrier is affected by\na given domain, enabling the correction of domain-specific\nnoise at the receiver on a per-subcarrier basis. By corre-\nlating the distinctive patterns of amplitude attenuation and\nphase shifts in CSI caused by specific human activities,\ntasks such as Human Activity Recognition (HAR) can be\nperformed [13].\nA significant challenge in WiFi-based HAR that limits\npractical applications is the poor cross-domain generaliza-\ntion of trained models [1]. In this context, the term domain\nencompasses various factors such as the physical environ-\nment, the morphological variability among monitored indi-\nviduals, the sensing hardware used, and the electromagnetic\nnoise from surrounding WiFi devices. Altogether, these fac-\ntors constitute the characteristics of a domain. This gener-\nalization problem stems fundamentally from the nature of\nCSI itself: designed to capture amplitude and phase pertur-\nbations in a transmitted signal induced by a given domain,\nmodels trained on such data are inherently domain-specific\nand do not generalize well.\nWhile various approaches such as domain-invariant fea-\nture extraction and virtual sample generation have been\nexplored to improve cross-domain generalization in WiFi-\nbased HAR (cf. [1] and references therein), they often\nhave significant limitations: domain-invariant feature ex-\ntraction typically requires complex and impractical hard-\nware setups, alongside computationally expensive feature\npre-processing [29]. On the other hand, virtual sample\ngeneration (data augmentation) depends on prior knowl-\nedge of the target domain and struggles to adapt to dy-\nnamic signal variations over time [20]. Given the substan-\ntial differences between WiFi domains, models trained ex-\nclusively on source domain data often overfit, learning fea-\ntures that are ineffective in new domains. Moreover, WiFi\ndomains can shift unpredictably due to minor environmen-\ntal changes, such as moving a chair or adding a new WiFi\ndevice, which significantly alters signal characteristics.\nFor these reasons, an approach is required that not only\nlearns domain-invariant features during training but also"}, {"title": "2. Related Work", "content": "Domain-Adversarial Training In their seminal work on\ndomain-adversarial training, Ganin and Lempitsky [6] in-\ntroduce the Domain-Adversarial Neural Network (DANN).\nThis architecture utilizes a Gradient Reversal Layer (GRL)\nto align feature representations between source and target\ndomains by minimizing domain discrepancy in an adver-\nsarial manner while preserving discriminative task-related\nfeatures. An extended version of this work [7] provided fur-\nther theoretical insights and empirical validation, establish-\ning DANN as a cornerstone in domain adaptation research.\nBuilding on DANN, Tzeng et al. [26] improved\ncross-domain generalization through domain confusion and\ntarget-specific classifiers, allowing for more nuanced adap-\ntation to target-specific features. Long et al. [15] further\nadvanced DAT with Conditional Domain-Adversarial Net-\nworks (CDAN), which integrate conditional task labels to\nrefine alignment between domains and capture complex de-\npendencies. Peng et al. [19] recently extended DAT with\nMoment Matching for Multi-Source Domain Adaptation\n(M\u00b3SDA), which adapts knowledge from multiple labeled\nsource domains to an unlabeled target domain by dynam-\nically aligning higher-order moments of feature distribu-\ntions. M\u00b3SDA's use of multiple classifiers enables domain-\nspecific adaptation but results in higher inference costs,\nmaking it less suitable for real-time applications.\nIn contrast, Jiang et al. [11] presented one of the first\nsuccessful applications of DAT in WiFi-based HAR. Their\napproach, combining a single CNN-based feature extractor\nwith an MLP-based domain discriminator and activity rec-\nognizer, maintains lower inference times by using a single\nclassifier. This architecture is thus better suited for real-time\nWiFi-based HAR, which is why we combine it with TTA, as\nexplored in this work, to enable effective adaptation to un-\nseen domains while maintaining computational efficiency.\nTest-Time Training Methods on handling distribution\nshifts at test time can be grouped into two categories [12]:\nthe first involves modifying the training process, often with\na self-supervised auxiliary task; the second approach, more\naligned with our method, adapts pre-trained models directly\nat test time without modifying training.\nTTT methods belong to the first group, leveraging aux-\niliary tasks during training to facilitate adaptation when en-\ncountering shifts at test time. Sun et al. [24] propose a TTT\napproach that updates model weights with each test sample,\nusing an architecture with two separate heads, one for self-\nsupervision and another for the main task, enabling real-\ntime adaptation. Building on this, Gandelsman et al. [5]\nemploy masked autoencoders and a reconstruction loss to\nenhance the learning signal. For video data, Wang et al. [28]\napply TTT with a sliding window of recent frames, en-\nabling continuous updates in response to changing con-\ntexts. Further advancing TTT, Sun et al. [25] propose an\napproach where a hidden state gets introduced to the model\nthat serves as an adaptive representation, updating with each\nself-supervised step without preset priors.\nTest-Time Adaptation In contrast, TTA methods,\naligned with the second approach, focus on adapting a pre-\ntrained model during inference without altering its training.\nLin et al. [12] introduce a TTA method for RGB video that\nadjusts test-time statistics to match those from training,\npromoting consistency across temporally augmented views\nto enhance coherence in model predictions. Wang et\nal. [27] address challenges in continual adaptation with\nposthoc regularization techniques, averaging predictions\nacross model weights and augmentations to reduce error\naccumulation and restoring model weights to their original\nstate intermittently to prevent catastrophic forgetting. Liu\net al. [14] further enhance adaptability by using a self-\nsupervised contrastive learning objective to align source\nand target features, improving robustness without temporal\ndependencies. Extending these TTA approaches, Ma et\nal. [16] propose a so-called improved self-training method\nthat refines pseudo-labels using graph-based correction and\nstabilizes adaptation with a parameter moving average."}, {"title": "3. Domain-Adversarial Test-Time Adaptation", "content": "Our proposed framework, DATTA, combines DAT with\nTTA to enable robust cross-domain generalization in WiFi-\nbased HAR. Through DAT, the model learns domain-\ninvariant features by leveraging data from diverse domains,\nachieving offline adaptation to varied data characteristics.\nOur DAT architecture builds on the adversarial structure for\ndomain-invariant feature learning outlined in [11], with a\nWiFlexFormer-based central feature extractor [23] to ensure\nefficient, real-time HAR. To further enhance cross-domain\ngeneralization, we incorporate a specialized augmentation\nmodule tailored to the unique properties of WiFi CSI. De-\nspite DAT's effectiveness, residual domain shifts can still\noccur at test time due to environmental changes that lead\nto distribution shifts in data. To account for these shifts as\nwell, we apply the TTA framework from [12] to align the\nfeature distributions of target and source domains, enabling\nreal-time adaptation to previously unseen data distributions\nduring test time. Additionally, we leverage random weight\nresetting [2] to prevent catastrophic forgetting of the learned\ndomain invariance, ensuring sustained model stability."}, {"title": "3.1. Domain-Adversarial Training", "content": "Figure 1 illustrates the DAT architecture used in DATTA,\nconsisting of the Feature Extractor, Activity Recognizer,\nand Domain Discriminator. The feature extractor processes\nCSI amplitude data to learn domain-invariant features by\ngenerating representations that are informative for activ-\nity recognition while disregarding domain-specific aspects.\nDuring training, the extracted features are fed to the activity\nrecognizer and domain discriminator. The domain discrimi-\nnator enforces domain invariance by applying an adversarial\nloss, pushing the feature extractor to produce features that\nare indistinguishable across domains. After training, the do-\nmain discriminator is discarded, leaving a model optimized\nfor cross-domain generalization.\nModel Input/Output Our DAT architecture takes CSI\namplitude spectrograms $s \u2208 S$ as input, each associated\nwith an activity label $a \u2208 A$ and a domain label $d \u2208 D$,\nwhere $A$ and $D$ represent the sets of activities and do-\nmains, respectively. The output is the predicted activity la-\nbel $\\hat{a} \u2208 A$."}, {"title": "3.2. Test-Time Adaptation", "content": "While DAT is effective in learning domain-invariant fea-\ntures, large domain shifts still lead to a drop in performance\nduring test time. In order to reduce the impact of such\ndomain shifts, we employ TTA, allowing off-the-shelf pre-\ntrained models to adapt online to new target domains with-\nout requiring additional labeled data.\nBuilding upon the framework proposed by Lin et al. [12],\nwe adapt TTA from RGB video to CSI amplitude spectro-\ngrams to further enhance the generalization of DAT dur-\ning test time by performing feature distribution alignment,\ni.e., aligning source statistics of the model with online esti-\nmates of the target statistics. Additionally, to prevent over-\nfitting to the target distribution during prolonged adapta-\ntion, i.e. catastrophic forgetting [2], we implement random\nweight resetting, following the approach proposed by Wang\net al. [27]. Specifically, a subset of model parameters is re-\nverted to their source model values to keep them closer to\nthe domain-invariant feature space.\nFeature Map Alignment To address the distribution\nshift, we align the statistics of feature maps, i.e., matching\nthe means and variances, computed for both the training and\ntest spectrograms. Let $\\phi_l(s; \\theta)$ represent the feature map of\nthe $l$-th layer of network $\\theta$, computed for a spectrogram $s$\nwith parameters $\\theta$. Each feature map is a matrix of dimen-\nsions $(t_l, f_l)$, where $t_l$ and $f_l$ correspond to the time steps\nand frequency channels (subcarrier), respectively.\nComputing the mean of the $l$-th layer features for a\ndataset $S$ across the time dimension results in a mean vector\nof size $f_l$, which can be expressed as:\n$\\mu_l(S; \\theta) = \\mathbb{E}_{s \\in S} \\mathbb{E}_{t \\in [1, t_l]} [\\phi_l(s; \\theta)[t]],$ (8)\nand the variance of the $l$-th layer features is given by:\n$\\sigma_l^2(S; \\theta) = \\mathbb{E}_{s \\in S} \\mathbb{E}_{t \\in [1, t_l]} [(\\phi_l(s; \\theta)[t] - \\mu_l(S; \\theta))^2].$ (9)\nFor the remainder of this work, we denote the mean and\nvariance computed on the training set with $\\bar{\\mu}_l$ and $\\bar{\\sigma}_l^2$. When\ntraining data is unavailable, these statistics can be estimated\nfrom batch norm layers as well, though with a small de-\ncrease in performance [12].\nAt test time, updates are performed iteratively, adjusting\nthe discrepancy between the test statistics of a batch $B$ of\nselected layers $L$ with those computed during training:\n$L_{TTA} = \\sum_{l \\in L} ||\\mu_l (B; \\theta) - \\bar{\\mu}_l||^2 + ||\\sigma_l^2 (B; \\theta) - \\bar{\\sigma}_l^2||^2.$ (10)\nIn our experiments, we observe that optimal results are\nobtained by selecting $L$ to contain only the first out of"}]}