{"title": "Improving ensemble extreme precipitation forecasts using generative artificial intelligence", "authors": ["Yingkai Sha", "Ryan A. Sobash", "David John Gagne II"], "abstract": "An ensemble post-processing method is developed to improve the probabilistic forecasts of extreme precipitation events across the conterminous United States (CONUS). The method combines a 3-D Vision Transformer (ViT) for bias correction with a Latent Diffusion Model (LDM), a generative Artificial Intelligence (AI) method, to post-process 6-hourly precipitation ensemble forecasts and produce an enlarged generative ensemble that contains spatiotemporally consistent precipitation trajectories. These trajectories are expected to improve the characterization of extreme precipitation events and offer skillful multi-day accumulated and 6-hourly precipitation guidance. The method is tested using the Global Ensemble Forecast System (GEFS) precipitation forecasts out to day 6 and is verified against the Climate-Calibrated Precipitation Analysis (CCPA) data. Verification results indicate that the method generated skillful ensemble members with improved Continuous Ranked Probabilistic Skill Scores (CRPSSs) and Brier Skill Scores (BSSs) over the raw operational GEFS and a multivariate statistical post-processing baseline. It showed skillful and reliable probabilities for events at extreme precipitation thresholds. Explainability studies were further conducted, which revealed the decision-making process of the method and confirmed its effectiveness on ensemble member generation. This work introduces a novel, generative-AI-based approach to address the limitation of small numerical ensembles and the need for larger ensembles to identify extreme precipitation events.", "sections": [{"title": "Introduction", "content": "The accurate prediction of extreme precipitation events is crucial for saving lives and property but continues to challenge our best prediction systems (e.g. Sukovich et al. 2014; Herman and Schumacher 2016). State-of-the-art global numerical weather prediction (NWP) models typically have 10-50 km horizontal grid spacing (e.g. Molteni et al. 1996; Buizza et al. 2018; Zhou et al. 2017, 2022). These horizontal resolutions require parameterization schemes to approximate small-scale processes that contribute to the generation of rainfall, such as convection and the microphysical interactions of cloud and precipitation water particles (Stensrud 2009). Despite the large improvements made over recent decades (Bauer et al. 2015), systematic error remains in these parameterization schemes, particularly for the modeling of extreme precipitation events (Wilcox and Donner 2007; Wehner et al. 2010; Sun and Liang 2020).\nPost-processing methods have been proposed for the bias correction and calibration of precipitation forecasts. These methods include both parametric methods, which assume a known predictive distribution, and non-parametric methods, which derive a predictive distribution from the training data with no prior distribution assumptions. One common parametric method is nonhomogeneous regression (Scheuerer and Hamill 2015; Baran and Nemoda 2016), which is an approach where a regression model predicts the parameters of a parametric distribution, such as a censored, shifted Gamma distribution for precipitation. Bayesian model averaging (Sloughter et al. 2007) estimates a multi-modal predictive distribution from an ensemble of deterministic predictions by learning the predicted variance and weight of each ensemble member. Nonparametric methods include the Analog Ensemble (Hamill and Whitaker 2006; Monache et al. 2013), which identifies the most similar prior NWP predictions to a new prediction and creates a distribution of observed values mapped to those NWP analogs, and quantile regression (Bremnes 2004), which optimizes regression models to predict conditional predictive quantiles rather than the most likely values.\nWhile these methods have achieved great success, their primary focus was the overall post- processing quality, which was dominated by the calibration performance of mild-to-moderate precipitation events. Machine-learning-based methods have been introduced to specifically improve extreme precipitation forecasts [e.g. random forest (Herman and Schumacher 2018), feed-forward neural network (Bodri and \u010cerm\u00e1k 2000), deep neural network (Li et al. 2022)]. However, a key challenge of these methods is generating skillful forecast trajectories that represent the evolution of extreme precipitation events. Many machine learning models were trained to predict probabilistic values on locations and forecast lead times independently, whereas end-users may look for multivariate forecast trajectories for flood risk assessments (e.g. Lai et al. 2020; Huang et al. 2021) and water resource management (e.g. Strauch et al. 2012).\nProducing trajectories that can represent extreme precipitation events properly requires a large ensemble set. Currently, the ensemble size of operational global NWP models is typically limited to 30-50 (e.g. Leutbecher 2019; Zhou et al. 2022), which is insufficient to capture extreme events on the very tail side of the precipitation intensity spectra (Bevacqua et al. 2023). Most statistical and machine-learning-based post-processing methods cannot solve this problem because they are designed to utilize available ensemble members; they can hardly create new forecast scenarios from an existing ensemble set to improve the estimation of extreme events. One possible solution is producing hundreds of numerical ensemble members from a regional numerical model configuration (e.g. Ghazvinian et al. 2024), although such efforts are computationally costly.\nRecent advances in generative Artificial Intelligence (AI) have brought new insights into extreme weather prediction problems. State-of-the-art generative AI can learn distribution properties from training data and produce conditional samplings from the target distribution (Creswell et al. 2018; Bond-Taylor et al. 2021; Yang et al. 2023). Compared to physics-based NWP ensembles, generative AI can expand ensemble sizes by producing more forecast members at minimal computational cost. This enlarged generative ensemble set would contain possible evolutions of the state of the atmosphere, thus providing better support for the estimation of high-impact extreme weather.\nSeveral studies have leveraged generative AI in either NWP or AI-based weather prediction. On regional scales, Sha et al. (2024) found that generated ensemble members from deterministic convection-allowing model forecasts improved probabilistic estimations of tornadoes, hail, and wind gusts. On global scales, Li et al. (2024) applied generative AI to post-process and emulate ensemble forecasts, resulting in improved forecast skill and more accurate predictions of extreme weather. Zhong et al. (2023) integrated generative AI with an AI weather prediction model to produce multi-day forecasts with finer-scale spatial details. The generated forecasts outperformed the original AI weather forecasts on various extreme-weather-based metrics.\nMotivated by the challenge of multivariate extreme precipitation post-processing and the application of generative AI in extreme weather prediction, this research proposes a post-processing framework that incorporates generative AI to improve the estimation of extreme precipitation events. Specifically, we aimed to produce a skillful generative ensemble of 6-hourly precipitation forecasts out to 6 days. This generative ensemble is expected to provide precipitation forecast trajectories that characterize extreme precipitation events properly and can be summarized with probabilistic guidance.\nThe methodology of this research was developed over the conterminous United States (CONUS) using the Global Ensemble Forecast System (GEFS) as inputs and the Climate-Calibrated Precipitation Analysis (CCPA) as targets. The following research questions are addressed: (1) How can generative AI methods be incorporated into precipitation forecast post-processing, and how well do they verify at producing reliable and discriminative probabilistic forecasts at extreme precipitation thresholds? (2) Can we explain the performance of AI-based precipitation post-processing methods, and what insights can we gain from such explainability analysis? By answering these, the authors examine the effectiveness of generative AI in extreme precipitation forecasts and explore its decision-making mechanisms for post-processing. Broadly, the authors also wish to introduce generative AI to severe-weather-related studies and inspire future creative works."}, {"title": "Research domain and data", "content": "a. Region of interest and the definition of extreme precipitation events\nThis research focuses on precipitation events within the CONUS (Fig. Sla). Following the definition of the Intergovernmental Panel on Climate Change (IPCC), Sixth Assessment Report (AR6), gridpoint-wise 99th percentile values were used as thresholds to identify extreme precipitation events (P\u00f6rtner et al. 2022). These values were estimated separately for different time periods of the day (00\u201306, 06\u201312, 12\u201318, and 18\u201300 UTC) to capture diurnal variations. The 00\u201306 and 12\u201318 UTC values were provided as examples in Fig. S1b and c, respectively.\nThe percentile-based peak-over-threshold approach can be inconsistent due to large regional differences (Wang and Tang 2020). Thus, a fixed precipitation rate threshold of 40 mm per 6 hours was used to identify extreme precipitation events. Fig. Sle-f provide the corresponding percentiles of the 40 mm per 6 hours threshold for 00\u201306 and 12\u201318 UTC. Overall, this fixed threshold is more extreme than the 99th percentile definition; it largely ignores dry areas on the west side of the Rocky Mountains and emphasizes precipitation in the Great Plains and the Southeastern US (cf. Fig. Sld and e-f). The 00\u201306 UTC (i.e., evening-to-night local time) patterns in Fig. Sle are primarily explained by the nocturnal convection over the Great Plains (e.g. Jiang et al. 2006; Johnson and Wang 2017; Blake et al. 2017), whereas the 12\u201318 UTC (i.e., morning-to-afternoon local time) patterns in Fig. S1f are related to summertime deep convection (e.g. Tian et al. 2005), small-scale convection introduced by the sea-breeze circulation (e.g. Hill et al. 2010), and tropical cyclones originating from the Gulf of Mexico (e.g. Shepherd et al. 2007). These types of extreme precipitation events are difficult to predict but also have a high impact by causing flash floods (e.g. Nair et al. 1997; Caracena et al. 1979; Smith et al. 2001). In addition, other fixed thresholds, ranging from 1 mm to 35 mm per 6 hours, were examined to provide comprehensive views of the performance of post-processing methods.\nb. Forecast data\nThis research aimed to improve the extreme precipitation forecasts from GEFS version 12 (GEFSv12; Zhou et al. 2022). The GEFSv12 is a state-of-the-art real-time ensemble forecast system operated by NOAA since September 2020. GEFSv12 implements the Global Fluid Dynamics Laboratory (GFDL) Finite-Volume Cubed-Sphere (FV3) dynamical core, ensemble Kalman filter- based data assimilation to generate initial condition uncertainty, and the GFDL cloud microphysics. Its quantitative precipitation forecasts over CONUS were largely improved from its previous ver- sions (Zhou et al. 2022). GEFSv12 has 0.25\u00b0 output horizontal grid spacing and 64 vertical hybrid levels. It produces 31-member ensemble forecasts 4 times per day, with 3-hourly output avail- able within the first 10 forecast days. In this research, the 00 UTC GEFSv12 initializations and 6 hourly total precipitation forecast (APCP) were selected as the main variable, whereas total-column precipitable water was used for one of the baseline methods.\nThe operational GEFSv12 comes with a 30-year reforecast archive to support post-processing studies (Guan et al. 2022). This reforecast dataset was produced from the same dynamical core, ensemble generation, and model physics as the operational GEFSv12, but with 5 members and 0000 UTC initializations only. The phase-two, 2000\u20132019 reforecasts, initialized from the GEFSv12 reanalysis (Hamill et al. 2022), were used by this research as training data.\nc. Analysis data\nCCPA (Hou et al. 2014) was used in this research to represent the analyzed state of precipitation. CCPA is a precipitation dataset that covers the entire CONUS. It statistically adjusts and combines the National Centers for Environmental Prediction (NCEP), Climate Prediction Center (CPC) unified global daily gauge analysis, and the NCEP Stage IV multi-sensor quantitative precipitation estimation (Hou et al. 2014). CCPA was used as the verification target of the operational GEFS system (Zhou et al. 2017, 2022) and has been applied as training and verification targets in various GEFS-based post-processing studies (e.g. Scheuerer and Hamill 2015; Hamill and Scheuerer 2018; Stovern et al. 2023; Hamill et al. 2023). The CCPA was also used as a climatology reference, including the estimation of gridpoint-wise precipitation Cumulative Distribution Functions (CDFs), which were used to define percentile-based extreme precipitation events (see Section 2.a) and compute skill scores."}, {"title": "Methods", "content": "Two neural-network-based post-processing steps were combined as the main methodology of this research (Fig. S2). First, a 3-D Vision Transformer (ViT) was proposed to reduce the forecast bias of each GEFSv12 member. Second, each bias-corrected member was used as the conditional input of a diffusion model, which generates post-processed members as outputs. The two steps above were conducted within the latent space created by a Vector-Quantized Variational Autoencoder (VQ-VAE), with the VQ-VAE encoder projecting GEFS members into the latent space and the VQ-VAE decoder projecting the latent space outputs back to the real space. The term \u201cLatent Diffusion Model (LDM)\" was used to highlight the application of VQ-VAE, and hereafter, the method is named \u201cViT-LDM\".\nThe ViT-LDM post-processing was trained from 2002 to 2019 using the 6-hourly GEFS refore- casts out to 6 days (i.e., 06-144 hours lead times) and the CCPA data. The validation set was a 10% random sampling from the training set and fixed for all training steps. When applied to the operational GEFSv12 in 2021, ViT-LDM generates 2 post-processed members from each GEFSv12 member, thus producing 62 generative members from all 31 operational members. The generated 62 members were verified against the CCPA data from 1 January to 31 December 2021 with a focus on extreme precipitation events. The basics and applications of VQ-VAE, ViT, and LDM are introduced in this section. Their hyperparameter optimization and other related information are summarized in the Appendix.\na. Latent space projection using VQ-VAE\nA VQ-VAE (Van Den Oord et al. 2017) was employed to convert gridded precipitation fields, either from the GEFS APCP forecasts or the CCPA data, into a compressed and regularized latent space, enabling effective bias correction and ensemble generation. The VQ-VAE of this research was designed based on 2-D convolutional layers. Its encoder contains two 4-by-4 downsampling layers with 16 times compression on latitude and longitude dimensions. The VQ-VAE decoder has two 4-by-4 upsampling layers; it takes the encoded latent space features as input and projects them to the original size with minimum information loss. The output section of the VQ-VAE decoder has an extended sub-structure that accepts elevation and CCPA climatology as additional inputs to improve the decoding quality. The technical highlight of VQ-VAE is its Vector-Quantization (VQ) layer. The VQ layer converts continuous encoded information into discrete values by selecting the closest-distanced vector from a discrete and learnable \u201ccodebook\u201d. VQ-VAE can be viewed as a VAE that produces discrete latent space embeddings.\nThe use of VQ-VAE in this research brought two major benefits: (1) The VQ-VAE latent space projection reduces data size, so the ViT and LDM can be designed and trained more effectively. (2) A regularized VAE latent space disentangles the input data. This means each VAE latent variable would represent its own factors of variation, and small perturbations within the VAE latent space would not lead to dramatically different outputs. The disentanglement property benefits the stability of ensemble member generation and model interpretation. Compared to direct diffusion, a known disadvantage of VAE-based latent diffusion is the over-smoothness of its outputs (e.g. Yang and Mandt 2024). Two steps were applied to solve this problem: (1) The embedded GEFS APCP forecasts were linearly combined with the generated latent information to guide the VQ-VAE decoder in producing more physically realistic outputs in inference (Fig. S2); (2) The decoder sub- structure that incorporates elevation and climatology information, as mentioned in the previous paragraph, was also aimed to mitigate the over-smoothness issue (Fig. S2, Fig. S3).\nVQ-VAE features self-supervised training. Its optimization objective contains three components (Van Den Oord et al. 2017):\n$\\mathcal{L}(x) = ||x \u2212 z_d [z_e (x)] ||^2_2 + ||sg [z_e (x)] - e||^2_2 +\\beta ||z_e (x) - sg (e) ||^2_2$ (1)\nWhere x is a training batch, $||...||^2_2$ is the mean squared error computation, $z_e$ and $z_d$ are the VQ-VAE encoder and decoder, respectively, e is the codebook, and \u201csg\u201d is the stop-gradient operator, which fixes the target from being updated by the current gradient descent step. The first term of equation (1) is the reconstruction loss; it minimizes the difference between the input and the reconstructed input. The second term is the codebook loss; it updates the discrete codebook values to keep them close to the continuous encoded information. The last term of the equation (1) is the commitment loss; it regularizes the encoder to prevent its encoded continuous values from diverging from the current codebook. \u03b2is a constant hyperparameter that defines the relative importance of commitment loss.\nThe VQ-VAE described above was trained on the 1/8\u00b0 CCPA data with (224, 464) input sizes; its decoder produces [(14, 29, 4) sized latent variables as outputs (the last dimension represents hidden-layer channels). The 0.25\u00b0 GEFS APCP forecasts were linearly interpolated to 1/8\u00b0 before encoding. The same interpolation and VQ-VAE weights were applied to all forecast lead times and both the GEFS reforecasts and operational forecasts. For data pre-processing, 6 hourly precipitation and the CCPA-based climatology were normalized by using a re-scaled logarithm transformation: y = log (0.1x + 1); the elevation input was normalized to [-1,1] by a linear scaler. Value truncation was applied to the decoder outputs. Precipitation rates lower than 0.1 mm per 6 hours will be replaced by zero.\nb. ViT-based forecasts bias correction\nA 3-D ViT (Dosovitskiy et al. 2020; Vaswani et al. 2021; Arnab et al. 2021) was applied within the VQ-VAE latent space for the bias correction of GEFS APCP forecasts. Its architecture consists of three components: (1) an input section that converts 3-D tensors into embedded patches, (2) stacked ViT blocks that perform attention-based learning, and (3) an output section that converts embedded patches to the original tensor size. The input section conducts patch partition using 3-D convolution kernels, and the positional indices are embedded by a dense layer. This design is similar to many AI-based weather forecast models (e.g. Chen et al. 2023). The ViT block follows the conventional design of Arnab et al. (2021); it features multi-head self-attention to learn the cross-relationships among embedded patches. More advanced ViT designs, such as Shift-window- based Transformers (SwinTs; Liu et al. 2021), were examined during the hyperparameter search, but they did not bring better performance.\nThe 3-D ViT operates (1, 1, 1) sized patch partitions with 128 embedded dimensions. Its ViT block has 8 stacks with 4 attention heads. This configuration was trained using the encoded GEFS reforecast ensemble mean as inputs and encoded CCPA as targets. It processes 8 temporal dimensions at once and was trained using the 06-54 hour reforecasts only. This training strategy will be discussed further within the context of AI explainability studies. For inference, the same 3-D ViT was applied to the operational GEFS members on 06\u201354, 54\u2013102, and 102\u2013144-hour forecasts to generate bias-corrected ensemble trajectories within the VQ-VAE latent space.\nc. Ensemble generation using LDM\nAn LDM was implemented to produce generative ensembles conditioned on the bias-corrected GEFS members. The archetype of LDM is the Denoising Diffusion Probabilistic Model (DDPM) proposed by Ho et al. (2020), and it was extended to a 3-D configuration that supports the generation of the entire forecast trajectory.\nDDPM contains forward and reverse diffusion processes. For a given sample of the target distribution $X_0 \\sim q (X_0)$, the forward diffusion process adds Gaussian noise into the sample iteratively by following a variance schedule:\n$q (X_t|X_{t-1}) = \\mathcal{N} (X_t; \\sqrt{1 \u2013 b_t} X_{t-1}, b_t I)$ (2)\nWhere t = {0,1,\u2026,T} are the diffusion timesteps, $B = {b_0, b_1,\u2026\u2026,b_T}$ is the diffusion schedule. The reverse diffusion process is achieved by a neural network \u03b8 that approximates $q_e (X_{t\u22121}|X_t)$ and recovers the noised sample to its original state $X_0$ iteratively. The optimization objective of DDPM is summarized as follows (Ho et al. 2020; Dhariwal and Nichol 2021):\n$\\mathcal{L} (t) = ||\\epsilon_t - \\theta (X_t, b_t, V) ||^2_2$ (3)\nWhere $e_t$ is the mean squared error of the predicted accumulated effect of forward diffusion, V is an optional, conditional input that can be incorporated during the reverse diffusion processes to influence the estimation of $q_\\theta (X_{t\u22121}|X_t)$. For the sample generation of DDPM, $X_T$ is a random draw from $N (0, I)$, and reverse diffused to $X_0$, which results in a generated sample.\nThe LDM of this research was designed based on the DDPM above and applied within the VQ-VAE latent space. Its architecture is similar to a 3-D Unet (e.g. Ronneberger et al. 2015; \u00c7i\u00e7ek et al. 2016) but without down- and upsampling levels. The LDM was configured with a 100-step linear schedule; it takes three inputs (Fig. S5a): (1) the output of the previous reverse diffusion step, (2) a ViT bias-corrected ensemble member as conditional input, and (3) the diffusion schedule of the current step; it produces the reverse diffusion output on the current step. The LDM was trained using the ViT-corrected reforecasts members as inputs and CCPA as targets. During the sample generation process, the weights of LDM were modified using the exponential moving average.\nd. Baseline methods\nThe combination of Analog Ensemble (AnEn; Hamill and Whitaker 2006) and Ensemble Copula Coupling (ECC; Schefzik et al. 2013) was considered as the baseline of this research (hereafter \u201cAnEn-ECC\u201d). AnEn is a regression-based method that performs univariate bias correction and ensemble calibration. For each forecast lead time and location, AnEn identifies similar historical dates/times within its reforecast training set and forms an ensemble composed of the CCPA training target at the identified date/times. As a nonparametric method, AnEn leverages a large reforecast archive without requiring an a priori distribution assumption; it is easy to implement and can produce realizations with flexible ensemble sizes. These strengths make AnEn a good option for precipitation forecast post-processing. The AnEn baseline here follows its improved version as introduced by Hamill et al. (2015) but without supplemental locations. It was trained using the 2002\u20132019 GEFS reforecasts and the CCPA target.\nECC is a multivariate, nonparametric method that recovers spatiotemporal consistencies from univariate post-processing outputs. Given calibrated AnEn members, ECC applies 31 operational GEFS members as \u201cdependence templates\" and re-indexes 31 AnEn members based on the rank structure of the selected templates.\nMore advanced precipitation post-processing methods were considered for use as a baseline, such as Scheuerer and Hamill (2015) and Stovern et al. (2023), but these methods typically produce probabilistic values directly rather than forecasted trajectories with physics-based units. We prefer AnEn-ECC because, similar to ViT-LDM, AnEn-ECC can post-process GEFS precipitation ensembles into forecast trajectories, which allows the flexibility of extreme precipitation verification with different definitions and thresholds (see Section 2.a).\nThe original 31 operational GEFS members (hereafter \u201cGEFS-Raw\u201d) were also used as a baseline. The two baselines, AnEn-ECC and GEFS-Raw, will be contrasted with ViT-LDM in extreme precipitation verification. Note that each of the two baselines contains 31 members, whereas the ViT-LDM generates 62 members. Although the total number of ensemble members is unequal, we think such a comparison is still fair because generating more ensemble members is part of the methodology and purpose of ViT-LDM.\ne. Verification methods\nViT-LDM and the two baselines were verified from 1 January to 31 December 2021. The general post-processing performance of all methods was examined using the Continuous Ranked Probability Scores (CRPS) and CRP Skill Scores (CRPSS), whereas the performance of extreme precipitation forecasts was verified using the Brier Score (BS) and Brier Skill Score (BSS; Murphy 1973). The climatology reference of CRPSS and BSS was derived from the 2002\u20132019 \u0421\u0421\u0420\u0410 data (Section 2.b).\nThe computation of spatiotemporally aggregated BSSs follows Hamill and Juras (2006), with the BS on individual gridpoint and forecast lead times being computed and aggregated first and then converted to BSS by applying the climatology reference. The three-component decomposition of BS and reliability diagrams were also computed to attribute the BSS difference; their computation follows Murphy (1973) and Hsu and Murphy (1986). Bootstrapping was applied to estimate the confidence intervals of skill scores. It was conducted separately on positive (i.e., extreme precipitation cases) and negative samples to preserve their relative ratios. Two-sided Wilcoxon signed-rank tests were applied to the CRPSS comparisons to determine if skill scores were statistically significantly different."}, {"title": "Results", "content": "a. Case-based assessments\nA case-based assessment is presented to demonstrate the generative ensemble produced by ViT- LDM. In Fig. S6", "evident": "n1. Precipitation patterns generated by ViT-LDM shared roughly the same locations as the C\u0421\u0420\u0410 verification target. The LDM-based conditional sampling from the bias-corrected GEFS ensemble members has the ability to preserve the broad-scale structure of the forecast event. This ensures that the generative ensemble would not exhibit large spatial discrepancies and place negative impacts on the prediction of extreme precipitation events.\n2. Differences in terms of the shape and intensity of the generated precipitation patterns can be found. For example", "no-skill\\\" reference line. The GEFS-Raw has improved resolution compared to AnEn-ECC because it over-predicted extreme precipitation events among all its members, which resulted in the probabilistic forecasts being distinguishable from the climatological mean. However, the GEFS-Raw forecasts were found to have poor reliability as their forecasted probabilities often did not co-occur with observed extreme precipitation events.\nThe ViT-LDM forecasts showed the best performance in this verification. Its 06-54-hour cal- ibration performance was impressive, with the number of high-probability forecasts comparable to that of the GEFS-Raw, and a reliability curve followed the perfectly reliable line. For longer forecast lead times, particularly 102-144 hours, the resolution of ViT-LDM decreased, mainly due to the reduced predictability of these extreme events. Nonetheless, ViT-LDM still outperformed the two baselines.\nOverall, for extreme precipitation events closely related to deep and intense convection in the Great Plains and the Southeastern US, ViT-LDM exhibited excellent calibration performance for short forecast lead times and clearly outperformed the two baselines for all verified forecast lead times. This result is also aligned with Sha et al. (2024), which revealed the good performance of generative AI in predicting severe weather events in this area.\nFig. $10 examines forecast reliability for extreme precipitation events identified based on the gridpoint-wise 99th percentile thresholds. Similar to the 40-mm-based verification in Fig. S9, the resolution of AnEn-ECC was too low, which reduced its calibration performance. Since the actual precipitation rate of 99th percentile thresholds were typically lower than 40 mm per 6 hours, AnEn-ECC generated more large probabilities (Fig. S9). The GEFS-Raw was capable of issuing higher probabilities for extreme precipitation events as well, however, probabilities were often overforecast and its reliability curves stayed around the \u201cno-skill\\\" reference line.\nFig. $11. As in Fig. S9, but for extreme events of 6-day accumulated precipitation amount > gridpoint-wise 99th percentile thresholds.\nAmong the three methods, the ViT-LDM had the best calibration. Its reliability was comparable to the AnEn-ECC baseline but preserved the resolution of the GEFS-Raw forecasts. The latter can be further confirmed by the frequency of occurrence plots, where the number of high-probability extreme precipitation forecasts issued by the ViT-LDM was comparable to that of the GEFS-Raw. Meanwhile, the AnEn-ECC rarely produced extreme precipitation probabilities > 0.5. Overall, for 6 hourly extreme precipitation events defined based on 99th percentile thresholds, the post- processed ensemble trajectories produced by ViT-LDM were verified to be skillful compared to the two baseline forecasts.\nFinally, we examine the reliability of 6-day accumulated precipitation greater than the gridpoint- wise 99th percentile (Fig. S11). Temporally```json\n{\n    {\n      \"title\"": "Discussion"}, {"content": "This research utilized generative AI for the ensemble post-processing of extreme precipitation events. Two research questions were examined. The first was related to the feasibility of generative AI and its performance in forecasting extreme precipitation. The implementation of generative AI in this study was successful, and its technical approach was similar to that of Li et al. (2024) and Zhong et al. (2023), which applied diffusion models using forecasted fields as conditional inputs. An important technical choice of ViT-LDM that differs from other research is the use of latent space projection, which reduced the overall data sizes and simplified the training of 3-D ViT and LDM. In the early stage of this research, the authors experimented with 3-D ViTs configured with larger patch sizes and without using a VQ-VAE. Strong checkerboard artifacts were identified with this choice. The VQ-VAE-based latent space projection and patch size 1 were then proposed, leading to the success of this generative AI application. Elevation and climatology inputs were incorporated into the ViT-LDM pipeline as background information. This option benefits the generation of precipitation fields with better quality, and its effectiveness has been discussed in other downscaling (e.g. Sha et al. 2020a,b; Wang et al. 2021) and bias-correction(e.g. Sha et al. 2022) studies. The performance of ViT-LDM in predicting extreme precipitation events were verified to be better than the AnEn-ECC baseline, a set of widely used post-processing techniques, as well as the original GEFS ensemble. The good performance of ViT-LDM is especially evident from the verification of 40 mm per 6-hour events, which many existing post-processing methods cannot calibrate properly. We think generative AI has the potential to be applied to the ensemble generation of regional precipitation forecasts and improve the prediction of extreme precipitation events.\nThe second research question focused on the explainability of ViT-LDM. Here, the example- based explainability studies were conducted. An important finding of this study is that the VQ-VAE latent space is capable of separating GEFS forecasts and the CCPA targets, which further explains the effectiveness of 3-D ViT in relocating the forecast-oriented representations. The role of LDM was identified as expanding the spatial area of latent space representations, which improved both the general post-processing performance and the calibration of extreme precipitation events. The explainability of AI-based forecast post-processing methods are generally lacking. For many neural-network-based post-processing methods, it is unclear how they have improved the quality of numerical forecasts. This research provided an example of exploring the decision-making mechanism of these neural networks. In addition, explorations of performance with varying ensemble sizes in Section 4.d have also brought evidence and new insights on the potential benefits of implementing generative AI in extreme weather problems.\nThe ViT-LDM was found to be suboptimal for calibrating mild and moderate precipitation events, defined by 1 mm and 5 mm per 6-hour thresholds. Future work could be conducted to tackle this challenge. Based on the explainability results, we think the order of the two post-processing steps can be changed. That is, generative AI can be applied to the raw forecast directly, and the resulting generative ensemble can be re-calibrated by another post-processing method. This choice avoids the use of a deterministic post-processing neural network and enables the flexibility of implementing ensemble-based calibration methods."}, {"title": "Conclusions", "content": "A novel post-processing method, ViT-LDM, was proposed by incorporating a Vector Quantised-Variational AutoEncoder (VQ-VAE), a 3-D Vision Transformer (ViT), and a Latent Diffusion Model (LDM). The method takes 6-hourly precipitation forecasts from numerical ensembles as inputs and generates post-processed trajectories that are skillful for the probabilistic estimation of extreme precipitation events. The 3-D ViT aims to reduce conditional bias from the original numerical ensemble, while the LDM produces an expanded generative ensemble that better characterizes extreme events.\nThe method was trained using the Global Ensemble Forecast System version 12 (GEFSv12) re- forecasts as inputs and the Climate-Calibrated Precipitation Analysis (CCPA) as targets from 2002 to 2019 and tested with the operational GEFSv12 over the Conterminous United States (CONUS) from 1 January 2021 to 31 December 2021. Verification results showed that the method gener- ated skillful precipitation forecast trajectories, as indicated by Continuous Ranked Probabilistic Skill Scores (CRPSSs) and Brier Skill Scores (BSSs). Its calibration performance for extreme precipitation events was superior to that of the operational GEFS and the combination of Analog Ensemble (AnEn) and Ensemble Copula Coupling (ECC). Reliability diagrams demonstrated that the probabilistic extreme precipitation forecasts of ViT-LDM were as reliable as that of the AnEn- based calibrations but with better resolution scores. For the verification of 6-day accumulated extreme precipitation events, ViT-LDM maintained the same good reliability and resolution seen across the individual 6-hourly forecast lead times, indicating that its generated trajectories were spatiotemporally consistent and could be aggregated to provide multi-day forecast guidance.\nExplainability studies were conducted to examine the decision-making process of ViT-LDM and provided evidence on the potential benefits of implementing generative methods in severe weather problems. These studies revealed that the VQ-VAE latent space provided good separations between the GEFS forecasts and the CCPA analysis, while the 3-D ViT was capable of relocating the latent space representations of the raw GEFS members to achieve bias correction. It was also confirmed that the latent space representations of the LDM-generated members were clustered around the location of the CCPA verification target with an enlarged spatial coverage. This enlarged cluster with expanded ensemble size improved the characterization of extreme precipitation events. A potential weakness of ViT-LDM was its suboptimal calibration performance for mild and moderate precipitation events, attributed to the smoothness effect of the VQ-VAE decoder and the determin- istic nature of the 3-D ViT bias-correction network. Possible solutions were discussed as future research directions.\nIn summary, ViT-LDM leverages a generative Artificial Intelligence (AI) approach for extreme precipitation forecasts. It produces skillful and spatiotemporally consistent precipitation forecast trajectories, bridging the gap between limited numerical ensemble sizes and the need for large ensemble sets to assess extreme precipitation events. More broadly, it provides a framework for implementing generative AI methods to address weather forecasting challenges."}, {"title": "APPENDIX", "content": "Improving Ensemble Extreme Precipitation Forecasts using Generative Artificial Intelligence: supplemental material\nThe main article introduced ViT-LDM, a post-processing framework that incorporates a Vector- Quantized Variational Autoencoder (VQ-VAE), a 3-D Vision Transformer (ViT), and a Latent Diffusion Model (LDM) for the ensemble generation of the Global Ensemble Forecast System (GEFS) total precipitation forecasts (APCP). This supplemental document includes the hyper- parameter choices and training procedures of the above neural networks. Descriptions of the additional information here are paired to Section 3.a-c of the main article.\na. VQ-VAE\nThe VQ-VAE contains two parts: an encoder-decoder as its main structure and an output section that takes elevation and precipitation climatology as inputs to produce refined precipitation fields. The two parts were trained separately. Table A1 summarizes the hyperparameters that were considered for the main structure. These hyperparameters were examined during the VQ-VAE training stage and selected based on the validation set performance after a fixed number of training epochs; similar validation-set-based hyperparameter selections were applied to other models in this research. VQ-VAE may exhibit convergence problems. A few solutions were proposed during its training stage: (1) pre-train a vanilla autoencoder to initialize the VQ-VAE weights. (2) Freeze the VQ-VAE decoder and fine-tune the encoder and VQ-layer. (3) Cosine annealing with warm re-start (Loshchilov and Hutter 2016).\nThe fraction of the commitment loss was set as \u1e9e = 0.25. The separated output section of VQ-VAE was trained with the frozen main structure. Its hyperparameters were considered based on Table S1 directly. Adam optimizers (Kingma and Ba 2014) were used throughout. After training, the VQ-VAE may occasionally produce negative precipitation values. As introduced in Section 3.a, a 0.1 mm truncation was applied to solve the problem.\nb. 3-D ViT\nThe 3-D ViT was trained using GEFS reforecast APCP ensemble mean as inputs and the CCPA data as targets. It was validated using 5 GEFS reforecast members on randomly selected initializa- tions and Continuous Ranked Probability Score (CRPS) as the metric. The training process relies on the pre-trained VQ-VAE. Tabel A2 summarizes the hyperparameters of 3-D ViT.\nFor patch sizes, lat=1 and lon=1 are highly preferred because this avoids the checkerboard artifacts. We have experimented with larger spatial patch sizes followed by additional convolutional layers, and the results were suboptimal. We did not experiment with more attention head options because this feature is expensive to compute. Shift-window-based Transformers (SwinTs; Liu et al. 2021) were tested during the hyperparameter search, but it did not improve the performance. A possible reason is that the embedded patches have both spatial and forecast lead time dimensions; con- ventional ViT with global-scale cross-attentions may work better on capturing the spatiotemporal relationships.\nc. 3-D LDM\nThe LDM of this research requires the CCPA data as its training target and the GEFS reforecasts as conditional inputs. The training of LDM relies on both the pre-trained VQ-VAE and the 3-D ViT. The LDM was initially validated by computing the loss of reconstructed noise. For fine-tuning, it was later validated by generating forecasts on randomly selected initializations and CRPS as the metric. The hyperparameters of the LDM are summarized in Tabel A3.\nLinear diffusion schedules were applied in this research. Under this meta, more diffusion steps are generally helpful but also require more computation for sample generations. We are aware that more advanced diffusion model designs and diffusion scheduling have been proposed, and we hope that future studies can introduce them to the field of weather forecasting."}]}