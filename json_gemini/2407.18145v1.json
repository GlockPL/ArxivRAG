{"title": "Taxonomy-Aware Continual Semantic Segmentation in Hyperbolic Spaces for Open-World Perception", "authors": ["Julia Hindel", "Daniele Cattaneo", "Abhinav Valada"], "abstract": "Semantic segmentation models are typically trained on a fixed set of classes, limiting their applicability in open-world scenarios. Class-incremental semantic segmentation aims to update models with emerging new classes while preventing catastrophic forgetting of previously learned ones. However, existing methods impose strict rigidity on old classes, reducing their effectiveness in learning new incremental classes. In this work, we propose Taxonomy-Oriented Poincar\u00e9-regularized Incremental-Class Segmentation (TOPICS) that learns feature embeddings in hyperbolic space following explicit taxonomy-tree structures. This supervision provides plasticity for old classes, updating ancestors based on new classes while integrating new classes at fitting positions. Additionally, we maintain implicit class relational constraints on the geometric basis of the Poincar\u00e9 ball. This ensures that the latent space can continuously adapt to new constraints while maintaining a robust structure to combat catastrophic forgetting. We also establish eight realistic incremental learning protocols for autonomous driving scenarios, where novel classes can originate from known classes or the background. Extensive evaluations of TOPICS on the Cityscapes and Mapillary Vistas 2.0 benchmarks demonstrate that it achieves state-of-the-art performance. We make the code and trained models publicly available at http://topics.cs.uni-freiburg.de.", "sections": [{"title": "I. INTRODUCTION", "content": "Automated vehicles rely on scene semantics predicted from online sensor data [1] as well as HD maps [2] for safe navigation. The dominant paradigm for scene understanding exploits semantic [3] or panoptic segmentation models [4] trained on a dataset with a fixed number of predetermined semantic categories. However, such vehicles operate in an open-world scenario where training data with new object classes appear over time. While one line of research focuses on detecting unknown objects [5], Class-Incremental Learning (CIL) aims to update the model with new classes at periodic timesteps [6]. On one hand, training a new model from scratch every time new classes appear is not only computationally inefficient but also requires past and present data to be available. On the other hand, simply updating a trained model with new data will result in catastrophic forgetting of old knowledge as the model will be biased towards new classes [7]. Consequently, CIL methods aim to balance observing characteristics of new classes while preserving patterns of formerly learned classes as the model is evaluated on all seen classes [6].\nClass-Incremental Semantic Segmentation (CISS) incorporates the background shift as an additional challenge. This phenomenon occurs as pixels that belong to old classes are labeled"}, {"title": "II. RELATED WORK", "content": "In this section, we summarize existing works in class-incremental semantic segmentation, hyperbolic neural networks, and hierarchical learning.\nClass-Incremental Semantic Segmentation: CISS methods rely on data replay, expansion, or distillation to avoid catastrophic forgetting. Prior data is recreated with GANs [14] or a small subset of prior data is stored in memory-buffers [11]. To further reduce memory constraints, prior work stores selected feature representations instead of raw data [15]. Expansion-based methods dedicate separate network components for particular semantic knowledge. For example, one branch of parallel convolutions adapts to the new data and is merged into the frozen branch after every incremental step [16]. Follow-up work extends this idea by fusing only endpoints of a trainable and frozen model in combination with distillation [17]. Distillation approaches maintain prior model weights to restrain the current model for equivalent responses to the input data [6]. The pioneering approach MiB [8] relates prior background logits to the combination of novel-class and background logits in the new model. This method is enhanced with gradient-based attribution weight initialization which identifies relevant classifier weights for novel classes from prior weights of the background class [18]. On the other hand, PLOP [9] labels the background with prior model predictions and distills pooled intermediate feature representations. Subsequent work focuses on learning an enhanced weighting term for distillation [19] or adapting this principle to transformer architectures [7], [20]. The method SATS [20] also highlights the benefit of relation distillation between self-attention vectors in a SegFormer model. This weaker constraint allows the model to avoid forgetting while not constraining its plasticity. Prior work also trains segmentation models with sigmoid activation and binary cross-entropy loss as the instability of softmax activations hinders incremental learning [11]. DKD [21] further combines this approach with decoupled knowledge distillation while other approaches completely freeze the feature extractor and segment unknown background classes with saliency detectors [11] or pre-trained models [10].\nHowever, all observed methods focus on novel classes from the background which significantly hinders their applicability in real-world scenarios where incremental learning could also entail a refinement of known classes. In this paper, we propose to simultaneously benchmark CISS methods for incremental learning from known classes and the background in the context of autonomous driving. Further, CISS methods do not utilize semantic relationships between classes to balance plasticity and rigidity. We hypothesize that a hierarchical mapping of class features facilitates learning new classes while it constrains forgetting old classes.\nHyperbolic Neural Networks: Hyperbolic neural networks have been first proposed to capture tree-like structures in text and graphs. For CNNs, the hyperbolic classification is modeled as a prototype-based approach [22], [23] or multinomial logistic regression as proposed by [24]. For the first, a cone entailment loss enforces all descendant prototypes to lie in the same geometric cone [25]. In multinomial logistic regression, semantic classes are geometrically interpreted as hyperplanes, and hierarchies are explicitly modeled with a hierarchical softmax [26] or cosine margins [27]. While hyperbolic neural networks have been extensively explored in image classification [27], [28] and metric learning [25], few prior works focus on semantic segmentation. Atigh et al. [26] first shows the potential of applying hyperbolic multinomial regression for image segmentation. Follow-up work highlights the dense calibration capabilities of this network [29] and leverages it for active learning [30]. One pioneering work also explores hyperbolic spaces for class-incremental image classification and highlights the positive impact of geometric-induced clear decision boundaries [23]. Motivated by them, our work is the first to explore hyperbolic spaces for CISS.\nHierarchical Learning: Hierarchical segmentation methods outperform flat classifiers on leaf categories by modeling the semantic hierarchy of classes in features [31] or combining logits with those of ancestor classes [26], [32]. While hierarchical semantic segmentation primarily addresses closed class settings, prior work in image classification concentrates on taxonomic class-incremental learning with network expansion [31] and replay-buffers [33]. Lin et al. first focused on taxonomic incremental semantic segmentation [33]. However, they require all ancestor classes to be present in the base training and allow the usage of all history data which contradicts CISS principles. We introduce a more realistic form of taxonomic class-incremental semantic segmentation which allows increments from both background and known classes and complies with other CISS task definitions."}, {"title": "III. TECHNICAL APPROACH", "content": "In this section, we first introduce taxonomic CISS. We then present our TOPICS approach which is tailored to incrementally learn new classes from either background or prior known classes as shown in Fig. 2. We leverage the class taxonomy and implicit relations between prior classes to avoid catastrophic forgetting in incremental learning steps. We first train the model on the base dataset. The class hierarchy is explicitly enforced in the final network layer which is mapped in hyperbolic space. This geometric space ensures that classes are equidistant to each other irrespective of their hierarchy level which facilitates learning tree-like class hierarchy structure. During the incremental steps, we leverage the old model's weights to create pseudo-labels for the background and employ scarcity and relation regularization losses to maintain important relations of old classes while learning the novel classes in a supervised manner."}, {"title": "A. Class-Incremental Semantic Segmentation", "content": "CISS aims at training a model f\u03b8 over t = 1,..., T incremental tasks. The first task (t = 1) is denoted as base training while all subsequent tasks t = 2, ..., T are referred to as incremental steps. Every task is defined by its own disjoint label space Ct and training dataset (xt, yt) \u2208 Dt. xt refers to task-specific input images and yt \u2208 Yt their pixelwise label according to yt = bt \u222a Ct. The background class bt includes all pixels whose true semantic class (y) is not included in Ct. We consider the more realistic overlapped setting of CISS where training images (xt) may include pixels whose dataset ground truth labels are old, current, or future classes. Their corresponding training label (yt) is re-defined according to Yt. After every task t, the network is challenged to make predictions on C1:t whereas only true background pixels should not be associated with a semantic class.\nIn comparison to [8]\u2013[11], [21], we do not constrain future classes to originate from the background in taxonomic CISS. We regard incremental scenarios where future classes are refinements of known classes or the background as shown in Fig. 3. We define C1:T according to a class taxonomy tree H which has l hierarchy levels and N\u2081 leaf nodes. Unlike taxonomic CIL as defined in [31], we allow unbalanced trees and ancestor nodes to be introduced when its first leaf node NL, is observed as illustrated in Fig. 3a. Consequently, we generate more realistic incremental learning scenarios which are not restricted to breadth-first increments. When classes are refined from known classes (Fig. 3b), we define disjoint subsets Dt according to a fixed ratio, i.e. Yt of every xi is constant and the same image cannot be observed with different labeling taxonomies at different time steps."}, {"title": "B. Semantic Segmentation with the Poincar\u00e9 Model", "content": "We model the class hierarchy in hyperbolic space due to its favorable property of equidistant node connections on all hierarchy levels. Consequently, distances are inversely proportional to the semantic similarity of classes. The hyperbolic space follows the geometry of constant negative curvature which is defined in the variable c. The Poincar\u00e9 model is a stereographic projection of the upper sheet of a two-sheeted hyperboloid and is, therefore, represented by a unit ball as shown in Fig. 4. This hyperbolic model is formally defined by the manifold $D_r = \\{x \\in \\mathbb{R}^n : c||x|| < 1\\}$ and Riemannian metric\n$g^\\mathbb{D}^n_x = \\left( \\frac{2}{1 - c ||x||^2} \\right)^2 g^\\mathbb{E}$,\nwhere $g^\\mathbb{E}$ is the euclidean tensor and $\\lambda^2_x$ the conformal factor. When c = 0, the Euclidean geometry is recovered.\nFor our TOPICS model, we only map the last neural network layer to hyperbolic space. Therefore, we first project the Euclidean features $e_i$ on the Poincar\u00e9 ball at its origin which is defined as\n$h_i = \\exp_0(e_i) = \\tanh(\\sqrt{c}||e_i||) (e_i/(\\sqrt{c}||e_i||)).$\nThe geometric interpretation of multinomial regression in hyperbolic space suggests that every class y is represented as a hyperplane in the Poincar\u00e9 ball with offset $o_y \\in \\mathbb{D}^N$ and orientation $r_y \\in T_{o_y}\\mathbb{D}^N$ (Fig. 4)\n$H_y = \\{h_i \\in \\mathbb{D}: < -o_y \\oplus_c h_i, r_y > = 0 \\}.$\nConsequently, the likelihood of a class is defined as\n$p(y = y | h_i) \\propto \\exp(\\mathcal{l}_y(h_i)),$\nwhere $\\mathcal{S}_y$ is the signed distance of the feature hi to the hyperplane of y which is approximated according to\n$\\mathcal{S}_y (h_i) = \\frac{\\mathcal{X}_o ||r_y||}{\\sqrt{c}} \\sinh^{-1} \\left( \\frac{2\\sqrt{c}<-o_y \\oplus_c h_i, r_y>}{(1-c||-o_y \\oplus_c h_i||^2)||r_y||} \\right),$\nwith being the M\u00f6bius addition\n$v \\oplus_c w = \\frac{(1 + 2c<v, w> + c||w||^2) v + (1 - c||v||^2) w}{1 + 2c<v, w> + c^2 ||v||^2||w||^2}.$"}, {"title": "C. Hierarchical Segmentation", "content": "We model the hierarchy of semantic classes in the last layer of the network. We opt for a binary cross-entropy loss to ensure magnitudes of old and new class predictions do not correlate. Specifically, we extend the state-of-the-art hierarchical segmentation loss [32] to multi-hierarchy levels and show its beneficial impact for incremental learning in hyperbolic space. Therefore, we model leaf nodes and all their ancestors as separate output classes V and use a combination of ancestor A and descendant D logits (s) in the loss function. We follow the tree-min loss [32] defined as\n$L_{TM} = \\sum_{\\nu \\in V} - \\mathbb{1}_{y_\\nu} \\log \\min_{\\mu \\in A_\\nu} (s_\\mu) - (1 - \\mathbb{1}_{y_\\nu}) \\log \\left(1 - \\max_{\\nu \\in D_{\\mu}} (s_{\\mu}) \\right),$ which penalizes hierarchical-inconsistent predictions for every class. For a correct prediction 1, this loss penalizes the smallest logit of its ancestors $A_{\\nu}$. On the other hand, for wrong predictions $1 - \\mathbb{1}_{y_\\nu}$, $L_{TM}$ punishes the maximum logits of its descendant $D_{\\mu}$ to reduce the score of complete root-to-leaf branches.\nFurther, we separately employ a categorical cross-entropy ($L_{CE}$) on every hierarchy level. Therefore, we first retrieve $\\hat{s} = \\max_{\\mu \\in D_{\\nu}} (s_{\\mu})$ and convert the labels $y_t$ into unique binary labels for every hierarchy level l. This loss penalizes high prediction scores of sibling class descendants. The complete hierarchical loss is defined as\n$L_{hier} = \\alpha L_{TM} + \\beta L_{CE}.$"}, {"title": "D. Hierarchical Relation Distillation", "content": "We reason that apart from explicit taxonomy relations between classes, an image model also captures implicit relations between classes in the form of relative similarity in feature space. Therefore, we aim to constrain these implicit relations to maintain relevant old class knowledge. In line with [20], [34], we argue that distilling relations is preferable to direct feature distillation as the latter restricts the model from re-distributing the feature space according to new classes. In comparison to the named prior work which distills relations based on feature maps, we propose to regularize the last layer's weights with this constraint. Consequently, we employ an InfoNCE loss on the hyperbolic class hyperplanes $H_t$ to maintain closely grouped classes of the prior model in a similar constellation in the updated model. We define a distance between two classes $y^1$ and $y^2$ as the distance between one class offset $o_{y^1}$ and the hyperplane of the other class $H_{y^2}$.\nAs outlined in Sec. III-B, an offset $o_{y^1}$ is a vector in hyperbolic space which defines the hyperplane of a class $y^1$ with orientation $r_{y^1}$. We retrieve absolute distances $d^{y^1}_{Ay^2}$ from signed distances which are computed using $\\mathcal{S}_{y^2}(o_{y^1})$ as defined in Eq. (5). Before beginning the training procedure, we utilize the old model's weights to compute the top k most similar hyperplanes $H_y$ for every offset $o_y$ in $C_{1:t\u22121}$. We neglect the background class in computing distances as we do not want to make constraints based on this variable class. Further, we denote all positive anchors of a class, $k^+$, $k_{y^1}$, as the top k smallest absolute distances to $O_{y^1}$ and enforce these relations to be maintained during the incremental training. We apply an InfoNCE-inspired loss:\n$L_{rel} = - \\log \\frac{\\exp (1-\\tau \\cdot d_{k^+}/d_{max})}{\\sum_i \\exp(1 - \\tau \\cdot d_i/d_{max})}.$\nwith $\\tau$ being the temperature hyper-parameter. With this constraint, we ensure relative implicit relations between old classes are maintained which creates an additional supervision for prior classes $C_{1:t-1}$."}, {"title": "E. Hyperbolic Distance Correlation", "content": "Prior research highlights the correlation of the hyperbolic radius to the scarcity of observed features [30] or the uncertainty of predictions in low dimensional space [26]. As incremental data is unbalanced with new classes appearing more frequently, we aim to constrain the radii of features to be unchanged between the old and new models. Therefore, we enforce features of the new and old model to be equidistant from the center of the Poincar\u00e9 ball. We hypothesize that this constraint results in the hyperplanes of old classes rotating around the center and prevents a shift of the complete taxonomic tree in favor of the new classes. New space is allocated for new classes while the respective scarcity of old classes is not affected."}, {"title": "IV. EXPERIMENTAL EVALUATION", "content": "In this section, we present quantitative and qualitative results of TOPICS on nine CISS settings in addition to a comprehensive ablation study to underline the importance of our contributions. Further, we detail the applied CISS settings and the training protocol that we employ."}, {"title": "A. Datasets", "content": "We evaluate TOPICS on the Cityscapes [12] and Mapillary Vistas 2.0 [13] datasets. For both datasets, we define CISS protocols where incremental classes either primarily originate from the background or known classes. We only consider the more realistic case of overlapped CISS where image pixels can belong to old, current, or future classes at any timestep. The Cityscapes dataset consists of 19 semantic classes in addition to a void class. For CISS from the background, we adapt the 14-1 (6 tasks) and 10-1 (10 tasks) setting as proposed in [18]. The first 10 or 14 classes are learned during base training while one class is added per incremental step. The task count includes base training as the first task. For CISS from known classes, we learn 7 base classes that correspond to the official sub-categories defined for Cityscapes and increment the model in a 7-4 (4 tasks) or 7-18 (2 tasks) manner.\nFor Mapillary Vistas 2.0, we leverage 111 valid semantic classes and collapse all void pixels into one background class. Consequently, we define the settings of 51-30 (3 tasks), and 71-10 (5 tasks) for CISS from the background. On the other hand, we evaluate taxonomic incremental capabilities with 39-84 (2 tasks) and 39-21 (5 tasks) on this dataset. For both datasets, we use the official validation split for testing and split the training data into training vs. validation with an 80:20 ratio. We note that the validation and test data remain constant for all incremental steps. For CISS from known classes, we divide the dataset into base and incremental dataset splits according to the number of learned classes within each step."}, {"title": "B. Experimental Setup", "content": "In line with prior work [8]\u2013[10], we use the DeepLabV3 model with the ResNet-101 backbone which is pre-trained on ImageNet for all the experiments. We employ the Geoopt library [35] to project the Euclidean features to a Poincar\u00e9 ball with c = 2.0 (different curvatures are explored in Sec. IV-D.5). Further, we follow the M\u00f6bius approximation defined in [26] for more efficient computations. We train TOPICS for 60 epochs per task with batch size 24 for Cityscapes and 16 for Mapillary Vistas 2.0 using the Riemannian SGD optimizer with momentum of 0.9 and weight decay of 0.0001. We use a poly learning rate scheduler with initial learning rates of 0.05 for base training and 0.01 in all incremental steps. We additionally ablate lower learning rates in Sec. IV-D.2. For the hierarchical loss function, we set the hyper-parameters to \u03b1 = 5 and \u03b2 = 1 and ablate different hierarchical functions in Sec. IV-D.4. For Mapillary Vistas 2.0, we rescale the longest size to 2177 pixels before taking a non-empty crop of (1024,1024) and horizontal flipping. On the other hand, we train on random non-empty crops of (512,1024) with horizontal flipping for Cityscapes. Non-empty cropping biases image crops to include labeled masks (i.e. new classes) which could be neglected when applying random cropping."}, {"title": "C. Quantitative Results", "content": "We compare TOPICS with five state-of-the-art CISS methods: PLOP [9], MiB [8], MiB+AWT [18], DKD [21] and MicroSeg [10]. For each method, we use the respective author's published code and use the same augmentations outlined in Sec. IV-B. For Cityscapes, we train the method PLOP on 512 \u00d7 512 crops as the method is restricted to squared input images. We evaluate the models using the mean intersection-over-union (mIoU) metric. Specifically, we evaluate the mIoU over all the base classes (C1) and novel classes (C2:T) separately as an indication of rigidity and plasticity. We present the results on Cityscapes in Tab. I. On this dataset, TOPICS outperforms all baselines by at least 9.88pp on the CISS from the background. While the difference in base IoU measures 4.2pp, our method significantly exceeds the benchmarks by at least 16.9pp in terms of novel IoU. This finding emphasizes that a balance between plasticity and rigidity is crucial to achieving superior results for class incremental learning. Further, we note the largest performance difference on incremental scenarios from known classes where TOPICS exceeds the best baseline by 29.94pp in mIoU. Consequently, we highlight the versatility of our method to balance plasticity and rigidity in all tested CISS settings. DKD [21] achieves the highest benchmark on CISS from the background in our setting but does not generalize on the bifurcation of previously observed classes which we reason with frozen layers in incremental steps. The MicroSeg approach also does not result in a favorable performance on Cityscapes and Mapillary Vistas 2.0 which is caused by low-quality proposals retrieved from the COCO dataset in addition to fully freezing the backbone. Subsequently, we present the mIoU over different increments for the 10-1(10 tasks) scenario on Cityscapes in Fig. 5. We note that for most benchmarks the mIoU increasingly deteriorates after incremental step 3. On the other hand, TOPICS achieves a constant performance from step 1 to step 6 after which the performance again stabilizes. Therefore, our approach supremely maintains base as well as incremental class knowledge while not restricting the adaptability to new classes."}, {"title": "V. CONCLUSION", "content": "We present TOPICS, a novel CISS approach that models features conforming to taxonomy-tree structures on the Poincar\u00e9 ball to balance rigidity and plasticity in incremental learning. TOPICS further maintains implicit class relations between old class hyperplanes and constraints features to have equidistant radii. We presented extensive experimental evaluations of eight incremental settings on Cityscapes and Mapillary Vistas 2.0 that demonstrated that TOPICS achieves state-of-the-art performance. Our method is one of the early works that uniformly addresses the bifurcation of previously observed classes and incremental classes from the background. Further, we emphasize the benefit of hierarchical modeling in hyperbolic space and motivate future work to explore its potential for various open-world challenges."}, {"title": "A. Class-Incremental Semantic Segmentation", "content": "CISS aims at training a model f\u03b8 over t = 1,..., T incremental tasks. The first task (t = 1) is denoted as base training while all subsequent tasks t = 2, ..., T are referred to as incremental steps. Every task is defined by its own disjoint label space Ct and training dataset (xt, yt) \u2208 Dt. xt refers to task-specific input images and yt \u2208 Yt their pixelwise label according to yt = bt \u222a Ct. The background class bt includes all pixels whose true semantic class (y) is not included in Ct. We consider the more realistic overlapped setting of CISS where training images (xt) may include pixels whose dataset ground truth labels are old, current, or future classes. Their corresponding training label (yt) is re-defined according to Yt. After every task t, the network is challenged to make predictions on C1:t whereas only true background pixels should not be associated with a semantic class."}, {"title": "B. Semantic Segmentation with the Poincar\u00e9 Model", "content": "We model the class hierarchy in hyperbolic space due to its favorable property of equidistant node connections on all hierarchy levels. Consequently, distances are inversely proportional to the semantic similarity of classes. The hyperbolic space follows the geometry of constant negative curvature which is defined in the variable c. The Poincar\u00e9 model is a stereographic projection of the upper sheet of a two-sheeted hyperboloid and is, therefore, represented by a unit ball as shown in Fig. 4. This hyperbolic model is formally defined by the manifold Dr = {x \u2208 Rn : c||x|| <1} and Riemannian metric\ng = (X)2gE: = (1 2 )2 gE,\nc||x||2\nwhere gE is the euclidean tensor and X the conformal factor. When c = 0, the Euclidean geometry is recovered.\nFor our TOPICS model, we only map the last neural network layer to hyperbolic space. Therefore, we first project the Euclidean features ei on the Poincar\u00e9 ball at its origin which is defined as\nhi = expo(ei) = tanh(\u221ac||ei||)(ei/(\u221ac||ei||)).\nThe geometric interpretation of multinomial regression in hyperbolic space suggests that every class y is represented as a hyperplane in the Poincar\u00e9 ball with offset oy \u2208 DN and orientation ry \u2208 TDN (Fig. 4)\nH = {hi \u2208 D, (-oy \u2295chi, ry) = 0} .\nConsequently, the likelihood of a class is defined as\np (y = y | hi) x exp (ly (hi)),\nwhere Sy is the signed distance of the feature hi to the hyperplane of y which is approximated according to\nSy (hi) = Xo ||ry|| sinh-1(\n2\u221ac(-oychi, ry)\n\u221ac\n(1-c||-oy @chi||2)||ry||\nwith being the M\u00f6bius addition\nvc w =\n(1 + 2c(v, w) + c||w||2) v + (1 \u2212 c||v||2) w.\n1 + 2c(v, w) + c2 ||v||2||w||2"}, {"title": "C. Hierarchical Segmentation", "content": "We model the hierarchy of semantic classes in the last layer of the network. We opt for a binary cross-entropy loss to ensure magnitudes of old and new class predictions do not correlate. Specifically, we extend the state-of-the-art hierarchical segmentation loss [32] to multi-hierarchy levels and show its beneficial impact for incremental learning in hyperbolic space. Therefore, we model leaf nodes and all their ancestors as separate output classes V and use a combination of ancestor A and descendant D logits (s) in the loss function. We follow the tree-min loss [32] defined as\nLTM = \u2211-1log min (Su)\n\u03bd\u2208V\nUEAv\n-(1-1) log (1 (1-max (su))\n,\nVED\nwhich penalizes hierarchical-inconsistent predictions for every class. For a correct prediction 1, this loss penalizes the smallest logit of its ancestors Av. On the other hand, for wrong predictions 1 \u2013 1, CTM punishes the maximum logits of its descendant D to reduce the score of complete root-to-leaf branches.\nFurther, we separately employ a categorical cross-entropy (LCE) on every hierarchy level. Therefore, we first retrieve s = maxu\u2208D, (su) and convert the labels yt into unique binary labels for every hierarchy level l. This loss penalizes"}]}