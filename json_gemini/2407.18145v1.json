{"title": "Taxonomy-Aware Continual Semantic Segmentation in Hyperbolic Spaces for Open-World Perception", "authors": ["Julia Hindel", "Daniele Cattaneo", "Abhinav Valada"], "abstract": "Semantic segmentation models are typically trained on a fixed set of classes, limiting their applicability in open-world scenarios. Class-incremental semantic segmentation aims to update models with emerging new classes while preventing catastrophic forgetting of previously learned ones. However, existing methods impose strict rigidity on old classes, reducing their effectiveness in learning new incremental classes. In this work, we propose Taxonomy-Oriented Poincar\u00e9-regularized Incremental-Class Segmentation (TOPICS) that learns feature embeddings in hyperbolic space following explicit taxonomy-tree structures. This supervision provides plasticity for old classes, updating ancestors based on new classes while integrating new classes at fitting positions. Additionally, we maintain implicit class relational constraints on the geometric basis of the Poincar\u00e9 ball. This ensures that the latent space can continuously adapt to new constraints while maintaining a robust structure to combat catastrophic forgetting. We also establish eight realistic incremental learning protocols for autonomous driving scenarios, where novel classes can originate from known classes or the background. Extensive evaluations of TOPICS on the Cityscapes and Mapillary Vistas 2.0 benchmarks demonstrate that it achieves state-of-the-art performance. We make the code and trained models publicly available at http://topics.cs.uni-freiburg.de.", "sections": [{"title": "I. INTRODUCTION", "content": "Automated vehicles rely on scene semantics predicted from online sensor data [1] as well as HD maps [2] for safe navigation. The dominant paradigm for scene understanding exploits semantic [3] or panoptic segmentation models [4] trained on a dataset with a fixed number of predetermined semantic categories. However, such vehicles operate in an open-world scenario where training data with new object classes appear over time. While one line of research focuses on detecting unknown objects [5], Class-Incremental Learning (CIL) aims to update the model with new classes at periodic timesteps [6]. On one hand, training a new model from scratch every time new classes appear is not only computationally inefficient but also requires past and present data to be available. On the other hand, simply updating a trained model with new data will result in catastrophic forgetting of old knowledge as the model will be biased towards new classes [7]. Consequently, CIL methods aim to balance observing characteristics of new classes while preserving patterns of formerly learned classes as the model is evaluated on all seen classes [6].\nClass-Incremental Semantic Segmentation (CISS) incorporates the background shift as an additional challenge. This phenomenon occurs as pixels that belong to old classes are labeled"}, {"title": "II. RELATED WORK", "content": "In this section, we summarize existing works in class-incremental semantic segmentation, hyperbolic neural networks, and hierarchical learning.\nClass-Incremental Semantic Segmentation: CISS methods rely on data replay, expansion, or distillation to avoid catastrophic forgetting. Prior data is recreated with GANs [14] or a small subset of prior data is stored in memory-buffers [11]. To further reduce memory constraints, prior work stores selected feature representations instead of raw data [15]. Expansion-based methods dedicate separate network components for particular semantic knowledge. For example, one branch of parallel convolutions adapts to the new data and is merged into the frozen branch after every incremental step [16]. Follow-up work extends this idea by fusing only endpoints of a trainable and frozen model in combination with distillation [17]. Distillation approaches maintain prior model weights to restrain the current model for equivalent responses to the input data [6]. The pioneering approach MiB [8] relates prior background logits to the combination of novel-class and background logits in the new model. This method is enhanced with gradient-based attribution weight initialization which identifies relevant classifier weights for novel classes from prior weights of the background class [18]. On the other hand, PLOP [9] labels the background with prior model predictions and distills pooled intermediate feature representations. Subsequent work focuses on learning an enhanced weighting term for distillation [19] or adapting this principle to transformer architectures [7], [20]. The method SATS [20] also highlights the benefit of relation distillation between self-attention vectors in a SegFormer model. This weaker constraint allows the model to avoid forgetting while not constraining its plasticity. Prior work also trains segmentation models with sigmoid activation and binary cross-entropy loss as the instability of softmax activations hinders incremental learning [11]. DKD [21] further combines this approach with decoupled knowledge distillation while other approaches completely freeze the feature extractor and segment unknown background classes with saliency detectors [11] or pre-trained models [10].\nHowever, all observed methods focus on novel classes from the background which significantly hinders their applicability in real-world scenarios where incremental learning could also entail a refinement of known classes. In this paper, we propose to simultaneously benchmark CISS methods for incremental learning from known classes and the background in the context of autonomous driving. Further, CISS methods do not utilize semantic relationships between classes to balance plasticity and rigidity. We hypothesize that a hierarchical mapping of class features facilitates learning new classes while it constrains forgetting old classes.\nHyperbolic Neural Networks: Hyperbolic neural networks have been first proposed to capture tree-like structures in text and graphs. For CNNs, the hyperbolic classification is modeled as a prototype-based approach [22], [23] or multinomial logistic regression as proposed by [24]. For the first, a cone entailment loss enforces all descendant prototypes to lie in the same geometric cone [25]. In multinomial logistic regression, semantic classes are geometrically interpreted as hyperplanes, and hierarchies are explicitly modeled with a hierarchical softmax [26] or cosine margins [27]. While hyperbolic neural networks have been extensively explored in image classification [27], [28] and metric learning [25], few prior works focus on semantic segmentation. Atigh et al. [26] first shows the potential of applying hyperbolic multinomial regression for image segmentation. Follow-up work highlights the dense calibration capabilities of this network [29] and leverages it for active learning [30]. One pioneering work also explores hyperbolic spaces for class-incremental image classification and highlights the positive impact of geometric-induced clear decision boundaries [23]. Motivated by them, our work is the first to explore hyperbolic spaces for CISS.\nHierarchical Learning: Hierarchical segmentation methods outperform flat classifiers on leaf categories by modeling the semantic hierarchy of classes in features [31] or combining logits with those of ancestor classes [26], [32]. While hierarchical semantic segmentation primarily addresses closed class settings, prior work in image classification concentrates on taxonomic class-incremental learning with network expansion [31] and replay-buffers [33]. Lin et al. first focused on taxonomic incremental semantic segmentation [33]. However, they require all ancestor classes to be present in the base training and allow the usage of all history data which contradicts CISS principles. We introduce a more realistic form of taxonomic class-incremental semantic segmentation which allows increments from both background and known classes and complies with other CISS task definitions."}, {"title": "III. TECHNICAL APPROACH", "content": "In this section, we first introduce taxonomic CISS. We then present our TOPICS approach which is tailored to incrementally learn new classes from either background or prior known classes as shown in Fig. 2. We leverage the class taxonomy and implicit relations between prior classes to avoid catastrophic forgetting in incremental learning steps. We first train the model on the base dataset. The class hierarchy is explicitly enforced in the final network layer which is mapped in hyperbolic space. This geometric space ensures that classes are equidistant to each other irrespective of their hierarchy level which facilitates learning tree-like class hierarchy structure. During the incremental steps, we leverage the old model's weights to create pseudo-labels for the background and employ scarcity and relation regularization losses to maintain important relations of old classes while learning the novel classes in a supervised manner.\nA. Class-Incremental Semantic Segmentation\nCISS aims at training a model \\(f_{\\theta}\\) over \\(t = 1,..., T\\) incremental tasks. The first task (\\(t = 1\\)) is denoted as base training while all subsequent tasks \\(t = 2, ..., T\\) are referred to as incremental steps. Every task is defined by its own disjoint label space \\(C_t\\) and training dataset \\((x_t, y_t) \\in \\mathcal{D}_t\\). \\(x_t\\) refers to task-specific input images and \\(y_t \\in \\mathcal{Y}_t\\) their pixelwise label according to \\(y_t = b_t \\cup C_t\\). The background class \\(b_t\\) includes all pixels whose true semantic class (\\(y\\)) is not included in \\(C_t\\). We consider the more realistic overlapped setting of CISS where training images \\((x_t)\\) may include pixels whose dataset ground truth labels are old, current, or future classes. Their corresponding training label \\((y_t)\\) is re-defined according to \\(\\mathcal{Y}_t\\). After every task \\(t\\), the network is challenged to make predictions on \\(C_{1:t}\\) whereas only true background pixels should not be associated with a semantic class.\nIn comparison to [8]\u2013[11], [21], we do not constrain future classes to originate from the background in taxonomic CISS. We regard incremental scenarios where future classes are refinements of known classes or the background as shown in Fig. 3. We define \\(C_{1:T}\\) according to a class taxonomy tree \\(\\mathcal{H}\\) which has \\(l\\) hierarchy levels and \\(N_l\\) leaf nodes. Unlike taxonomic CIL as defined in [31], we allow unbalanced trees and ancestor nodes to be introduced when its first leaf node \\(N_L\\), is observed as illustrated in Fig. 3a. Consequently, we generate more realistic incremental learning scenarios which are not restricted to breadth-first increments. When classes are refined from known classes (Fig. 3b), we define disjoint subsets \\(\\mathcal{D}_t\\) according to a fixed ratio, i.e. \\(Y_t\\) of every \\(x_i\\) is constant and the same image cannot be observed with different labeling taxonomies at different time steps.\nB. Semantic Segmentation with the Poincar\u00e9 Model\nWe model the class hierarchy in hyperbolic space due to its favorable property of equidistant node connections on all hierarchy levels. Consequently, distances are inversely proportional to the semantic similarity of classes. The hyperbolic space follows the geometry of constant negative curvature which is defined in the variable \\(c\\). The Poincar\u00e9 model is a stereographic projection of the upper sheet of a two-sheeted hyperboloid and is, therefore, represented by a unit ball as shown in Fig. 4. This hyperbolic model is formally defined by the manifold \\(\\mathbb{D}^n = \\{x \\in \\mathbb{R}^n : c||x|| < 1\\}\\) and Riemannian metric\n\\[g^\\mathbb{D}^n(x) = \\lambda^2(x) g^E = \\left(\\frac{2}{1 - c||x||^2}\\right)^2 g^E,\\]\nwhere \\(g^E\\) is the euclidean tensor and \\(\\lambda_x^\\mathbb{D}^n\\) the conformal factor. When \\(c = 0\\), the Euclidean geometry is recovered.\nFor our TOPICS model, we only map the last neural network layer to hyperbolic space. Therefore, we first project the Euclidean features \\(e_i\\) on the Poincar\u00e9 ball at its origin which is defined as\n\\[h_i = \\exp_0(e_i) = \\tanh(\\sqrt{c}||e_i||)(\\frac{e_i}{\\sqrt{c}||e_i||}).\\]\nThe geometric interpretation of multinomial regression in hyperbolic space suggests that every class \\(y\\) is represented as a hyperplane in the Poincar\u00e9 ball with offset \\(o_y \\in \\mathbb{D}^n\\) and orientation \\(r_y \\in T_{o_y}\\mathbb{D}^n\\) (Fig. 4)\n\\[H_y = \\{h_i \\in \\mathbb{D}^n : \\langle -o_y \\oplus_c h_i, r_y\\rangle = 0\\}.\\]\nConsequently, the likelihood of a class is defined as\n\\[p(y=y | h_i) \\propto \\exp(\\zeta_y(h_i)),\\]\nwhere \\(\\zeta_y\\) is the signed distance of the feature \\(h_i\\) to the hyperplane of \\(y\\) which is approximated according to\n\\[\\zeta_y(h_i) = \\frac{\\lambda_{o_y}||r_y||}{\\sqrt{c}} \\sinh^{-1}\\left(\\frac{2\\sqrt{c}\\langle -o_y \\oplus_c h_i, r_y\\rangle}{(1-c||-o_y \\oplus_c h_i||^2)||r_y||}\\right),\\]\nwith \\(\\oplus_c\\) being the M\u00f6bius addition\n\\[v \\oplus_c w = \\frac{(1 + 2c\\langle v, w\\rangle + c||w||^2) v + (1 - c||v||^2) w}{1 + 2c\\langle v, w\\rangle + c^2 ||v||^2||w||^2}.\\]\nC. Hierarchical Segmentation\nWe model the hierarchy of semantic classes in the last layer of the network. We opt for a binary cross-entropy loss to ensure magnitudes of old and new class predictions do not correlate. Specifically, we extend the state-of-the-art hierarchical segmentation loss [32] to multi-hierarchy levels and show its beneficial impact for incremental learning in hyperbolic space. Therefore, we model leaf nodes and all their ancestors as separate output classes \\(V\\) and use a combination of ancestor \\(A\\) and descendant \\(D\\) logits \\((s)\\) in the loss function. We follow the tree-min loss [32] defined as\n\\[\\mathcal{L}_{TM} = \\sum_{\\nu \\in V} -\\log(\\min_{u \\in A_\\nu} (s_u)) - (1 - l_{\\nu}) \\log\\left(1 - \\max_{u \\in D_\\nu} (s_u)\\right)\\]\nwhich penalizes hierarchical-inconsistent predictions for every class. For a correct prediction \\(l\\), this loss penalizes the smallest logit of its ancestors \\(A_\\nu\\). On the other hand, for wrong predictions \\(1 - l\\), \\(\\mathcal{L}_{TM}\\) punishes the maximum logits of its descendant \\(D_\\nu\\) to reduce the score of complete root-to-leaf branches.\nFurther, we separately employ a categorical cross-entropy (\\(\\mathcal{L}_{CE}\\)) on every hierarchy level. Therefore, we first retrieve \\(s^\\ast = \\max_{u \\in D_\\nu} (s_u)\\) and convert the labels \\(y_t\\) into unique binary labels for every hierarchy level \\(l\\). This loss penalizes high prediction scores of sibling class descendants. The complete hierarchical loss is defined as\n\\[\\mathcal{L}_{hier} = \\alpha \\mathcal{L}_{TM} + \\beta \\mathcal{L}_{CE}.\\]\nDuring inference, we multiply the logits of every leaf node \\(u_L\\) with the logits of all its ancestors \\((A_u)\\) and remove non-leaf classes from the evaluation.\nD. Hierarchical Relation Distillation\nWe reason that apart from explicit taxonomy relations between classes, an image model also captures implicit relations between classes in the form of relative similarity in feature space. Therefore, we aim to constrain these implicit relations to maintain relevant old class knowledge. In line with [20], [34], we argue that distilling relations is preferable to direct feature distillation as the latter restricts the model from re-distributing the feature space according to new classes. In comparison to the named prior work which distills relations based on feature maps, we propose to regularize the last layer's weights with this constraint. Consequently, we employ an InfoNCE loss on the hyperbolic class hyperplanes \\(H_t\\) to maintain closely grouped classes of the prior model in a similar constellation in the updated model. We define a distance between two classes \\(y^1\\) and \\(y^2\\) as the distance between one class offset \\(o_{y1}\\) and the hyperplane of the other class \\(H_{y2}\\). As outlined in Sec. III-B, an offset \\(o_{y1}\\) is a vector in hyperbolic space which defines the hyperplane of a class \\(y^1\\) with orientation \\(r_{y1}\\). We retrieve absolute distances \\(d^\\ast_{y1}\\) from signed distances which are computed using \\(\\zeta_{y^2}(o_{y1})\\) as defined in Eq. (5). Before beginning the training procedure, we utilize the old model's weights to compute the top \\(k\\) most similar hyperplanes \\(H_y\\) for every offset \\(o_y\\) in \\(C_{1:t-1}\\). We neglect the background class in computing distances as we do not want to make constraints based on this variable class. Further, we denote all positive anchors of a class, \\(k_+, k_y\\), as the top \\(k\\) smallest absolute distances to \\(O_{y_1}\\) and enforce these relations to be maintained during the incremental training. We apply an InfoNCE-inspired loss:\n\\[\\mathcal{L}_{rel} = - \\log \\frac{\\exp(1 - \\tau \\cdot d_{k^+}/d_{max})}{\\sum_i \\exp(1 - \\tau \\cdot d_i/d_{max})}.\\]\nwith \\(\\tau\\) being the temperature hyper-parameter. With this constraint, we ensure relative implicit relations between old classes are maintained which creates an additional supervision for prior classes \\(C_{1:t-1}\\).\nE. Hyperbolic Distance Correlation\nPrior research highlights the correlation of the hyperbolic radius to the scarcity of observed features [30] or the uncertainty of predictions in low dimensional space [26]. As incremental data is unbalanced with new classes appearing more frequently, we aim to constrain the radii of features to be unchanged between the old and new models. Therefore, we enforce features of the new and old model to be equidistant from the center of the Poincar\u00e9 ball. We hypothesize that this constraint results in the hyperplanes of old classes rotating around the center and prevents a shift of the complete"}, {"title": "V. CONCLUSION", "content": "We present TOPICS, a novel CISS approach that models features conforming to taxonomy-tree structures on the Poincar\u00e9 ball to balance rigidity and plasticity in incremental learning. TOPICS further maintains implicit class relations between old class hyperplanes and constraints features to have equidistant radii. We presented extensive experimental evaluations of eight incremental settings on Cityscapes and Mapillary Vistas 2.0 that demonstrated that TOPICS achieves state-of-the-art performance. Our method is one of the early works that uniformly addresses the bifurcation of previously observed classes and incremental classes from the background. Further, we emphasize the benefit of hierarchical modeling in hyperbolic space and motivate future work to explore its potential for various open-world challenges."}]}