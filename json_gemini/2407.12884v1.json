{"title": "SurroFlow: A Flow-Based Surrogate Model for Parameter Space Exploration and Uncertainty Quantification", "authors": ["Jingyi Shen", "Yuhan Duan", "Han-Wei Shen"], "abstract": "Existing deep learning-based surrogate models facilitate efficient data generation, but fall short in uncertainty quantification, efficient parameter space exploration, and reverse prediction. In our work, we introduce SurroFlow, a novel normalizing flow-based surrogate model, to learn the invertible transformation between simulation parameters and simulation outputs. The model not only allows accurate predictions of simulation outcomes for a given simulation parameter but also supports uncertainty quantification in the data generation process. Additionally, it enables efficient simulation parameter recommendation and exploration. We integrate SurroFlow and a genetic algorithm as the backend of a visual interface to support effective user-guided ensemble simulation exploration and visualization. Our framework significantly reduces the computational costs while enhancing the reliability and exploration capabilities of scientific surrogate models.", "sections": [{"title": "INTRODUCTION", "content": "In fields like fluid dynamics, climate, and weather research, ensemble simulations have become critical for estimating uncertainty, improving decision-making, and enhancing the scientific understanding of complex systems. To fully study scientific phenomena and analyze the sensitivity of simulation parameters, scientists often need to conduct a sequence of simulations and compare the results generated from different configurations. However, scientific simulations can be computationally expensive and time-consuming. To speed up the simulation, analysis, and knowledge discovery cycle, a major research effort has been put into developing simulation and visualization surrogates to approximate the results of complex and computationally expensive simulations with much reduced costs.\nCurrently, many deep learning-based surrogate models have been proposed in scientific visualization fields to assist parameter space exploration for ensemble simulations [9, 11,27]. Although they have begun to gain traction, there still exist several limitations. The first limitation is the lack of uncertainty quantification for surrogate models. Surrogate models are approximations of the actual simulations and inherently contain uncertainties, it is important to convey uncertainties associated with the surrogate model's outputs so that scientists know how much they can trust the model's predictions. Uncertainty quantification also provides insights into how sensitive the surrogate models' predictions are with respect to the changes in the simulation parameters. This sensitivity information is crucial for scientists as it guides the exploration direction about where to focus more, for example, regions with high data variance, by running additional simulations. The second challenge is the lack of efficient exploration across the parameter space. Current works only support limited functionality such as predicting the data for a given simulation parameter set, but do not assist in discovering potential optimal simulation configurations across the large parameter space. Parameter space exploration based on heuristic or brute-force approaches can be computationally intensive and inefficient. A systematic and efficient way for parameter space exploration that considers scientists' interests is needed. The third challenge is reverse prediction, particularly the ability to predict the simulation parameters that can produce a specific simulation result. This ability is crucial when scientists explore the optimal outcomes by directly manipulating the simulation outputs but lack the knowledge of the underlying simulation parameters that could generate those manipulated data. Thus, there is a pressing need for effective approaches that can infer simulation parameters from observed outcomes.\nIn this work, we propose SurroFlow, a surrogate model based on the normalizing flow [7,15,24]. Normalizing flow is a type of generative model for modeling complex probability distributions. By learning the invertible transformation between a simple base distribution (e.g., Gaussian distribution) and a complex target distribution, we address the above challenges as follows. First, SurroFlow is a conditional normalizing flow model combined with an autoencoder, trained on pairs of simulation parameters and corresponding simulation outcomes. As it models the distribution of simulation data conditioned on the simulation parameters, SurroFlow can quantify the uncertainty in the surrogate modeling process. SurroFlow takes simulation parameters as the conditional input, by sampling from the simple base distribution multiple times and reconstructing them in the complex conditional distribution, the variations among the reconstructed outputs can reflect the uncertainties in the surrogate prediction. Second, the proposed uncertainty-aware surrogate model SurroFlow can assist interactive exploration of simulation parameters guided by scientists' interests and goals. This is done by integrating the trained SurroFlow with a genetic algorithm, empowered by an interactive visual interface for user-guided automatic parameter space exploration. Scientists can specify their preferences and optimization objectives via the visual interface. The genetic algorithm then iteratively explores generations of simulation parameters based on scientists' inputs, while SurroFlow operates in the backend to efficiently predict data and uncertainties for the given simulation parameters. Once scientists set up the objectives and trade-offs among data similarity, diversity, and uncertainty interactively on the visual interface, simulation parameter space exploration can proceed automatically. Third, another essential benefit of flow-based models is that they can perform prediction in both forward and reverse directions. In the forward direction, SurroFlow can predict data conditioned on the input simulation parameters, allowing for the efficient generation of high-quality synthetic data. Conversely, in the backward direction, SurroFlow can predict the underlying simulation parameters that produce the given simulation data. This bidirectional prediction allows an effective framework for both surrogate modeling and reverse prediction, enhancing the flexibility of parameter space exploration and post-hoc analysis.\nIn contrast to other surrogate models in the visualization field [11, 27, 28], SurroFlow has made significant advancements by combining the conditional normalizing flows with autoencoders for efficient latent space modeling, allowing uncertainty quantification of the surrogate model's outputs. SurroFlow also features an improved architecture for bi-directional predictions. Additionally, it integrates a genetic algorithm and an interactive visual interface for efficient simulation parameter space exploration. Through qualitative and quantitative evaluations, we demonstrate the effectiveness of the proposed approach for automatic user-guided simulation parameter recommendation and exploration. In summary, the contributions of our work are:\n\u2022 First, we propose an invertible flow-based uncertainty-aware surrogate model.\n\u2022 Second, we utilize a genetic algorithm that considers the similarity, diversity, and uncertainty of data during the automatic parameter space exploration process.\n\u2022 Third, we employ an interactive visual interface for uncertainty analysis and provide scientists with effective control over simulation parameter space exploration."}, {"title": "RELATED WORKS", "content": "We employ an invertible normalizing flow for surrogate modeling with uncertainty estimation. In this section, we provide an overview of related research on visualization surrogate models for scientific data, global sensitivity analysis techniques, and uncertainty estimation for deep neural networks.\nSurrogate Models for Parameter Space Exploration. In the fields of oceanography and climate science, scientists frequently run computational simulations [14, 19] to explore parameter spaces. Replacing these costly and time-consuming simulation processes, numerous surrogate models [1,4, 23] have been proposed. Among these, Gramacy et al. [9] construct non-stationary Gaussian process trees that adaptively sample within the input space, enabling the selection of more efficient designs. He et al. [11] introduce InSituNet, an image-based surrogate model designed for real-time parameter space exploration. However, its reliance on regular grid mappings limits its application to unstructured data. To overcome this limitation, Shi et al. [28] propose GNN-Surrogate, specifically designed for navigating simulation outputs on irregular grids. Shi et al. [27] also introduce the VDL-Surrogate model based on view-dependent neural-network latent representations. It employs ray casting from varied viewpoints to aggregate samples into compact latent representations, thereby optimizing computational resource use and enhancing high-resolution explorations. Similarly, Danhaive et al. [6] propose a surrogate model for performance-driven design exploration within parametric spaces using conditional variational autoencoders. They employ a sampling algorithm to distill a dataset that captures valuable design insights and employ the variational autoencoders as surrogates for the simulation process. However, existing learning-based works do not account for uncertainties within the surrogate model. To address this deficiency, we propose a flow-based surrogate to enable uncertainty quantification of the surrogate model's prediction process.\nGlobal Sensitivity Analysis. Global sensitivity analysis methods include variance-based, differential-based, and regression-based approaches. They are crucial for understanding how parameters influence model outcomes. Variance-based methods, also known as ANalysis Of VAriance (ANOVA), decompose the output's variance into a sum of contributions from inputs and their interactions. Based on the application, Sobol indices [29] or Design of Experiment [18] are used to evaluate inputs' effects on the output. Differential-based methods, like the Morris [20], calculate input effects by varying each factor individually while keeping others fixed. This method is computationally efficient and easy to implement. Regression-based methods utilize linear approximations of the function to compute sensitivity based on metrics like Correlation Coefficients and Standardized Regression coefficients [13]. There are traditional surrogate models with global sensitivity analysis, such as Ballester et al. [3]'s Sobol tensor train decomposition. While efficient, these surrogate methods are outperformed by deep neural networks.\nUncertainty in Deep Neural Networks. Deep neural networks are often viewed as black boxes due to unclear decision-making rules. For scientific visualization and analysis, uncertainty quantification helps measure the reliability of the model, offering scientists a level of confidence about the model's outputs. A large number of uncertainty quantification methods have been proposed, including probabilistic models (e.g., Bayesian neural networks [21], Deep Gaussian process [5]), ensemble methods (e.g., DeepEnsemble [17]), and deep generative model-based methods [8, 12, 16]. In our work, we employ a deep generative model called normalizing flow [15, 22, 24, 26] as the surrogate model to quantify uncertainties."}, {"title": "BACKGROUND: NORMALIZING FLOW", "content": "SurroFlow is based on the normalizing flow for the conditional generation of simulation data conditioned on the simulation parameters. Normalizing flow models are a type of generative models that aim to learn a mapping from a simple probability distribution to a more complex one, allowing for the generation of high-quality data samples in the target distribution.\nThe key idea behind normalizing flow is to learn a series of invertible transformation functions that can gradually map samples from a simple distribution into a complex target distribution of interest, as shown in Fig. 1. Since the mapping is invertible, one can perform the transformation in the opposite direction, i.e., encode samples in the complex distribution into latent vectors with a simple and tractable distribution. Formally, denote the observed variables as x and latent variables as z, the mapping in the normalizing flow model, given by \\(f_e: \\mathbb{R}^n \rightarrow \\mathbb{R}^n\\), is invertible such that \\(x = f_e(z)\\) and \\(z = f_e^{-1}(x)\\). The invertible mapping function \\(f_e\\) consists of a sequence of bijective functions \\(f_e = f_k \\circ f_{k-1} \\circ \\dots \\circ f_1\\), so that\n\\[x = z_K = f_k \\circ f_{k-1} \\circ \\dots \\circ f_1(z_0),\\]\nwhere each \\(f_i\\) (for \\(i = 1, \\dots, K\\)) is learnable and invertible. With the stacked K invertible functions, a normalizing flow can transform a latent variable \\(z_0\\), which follows the simple distribution (e.g., Gaussian distribution), into the target variable \\(z_k\\) with a more complex target distribution \\(p(x) = p_K(z_K)\\). Using the change of variables, the log-likelihood of \\(z_k\\) can be computed as\n\\[log \\; p_K(z_K) = log \\; p_0(z_0) - \\sum_{i=1}^K log \\; det \\frac{\\partial f_i(z_{i-1})}{\\partial z_{i-1}},\\]\nGiven the density of \\(z_0\\) and the learnable transformation functions \\(f_e\\), one can compute the log-likelihood of any x through Eq. (2). The optimization goal for normalizing flows is to find the \\(\\theta\\) that maximizes the log-likelihood of training samples, computed by Eq. (2). The invertibility of normalizing flows is theoretically constrained by design. Various learnable and invertible functions for \\(f_i (i = 1, ..., K)\\) have been proposed, but the most efficient and simplest approach is affine coupling layers [7]. The input variable z of this coupling layer is split into two parts \\(z_{1:j}\\) and \\(z_{j+1:d}\\) at index j, where the first part is unchanged and is used to parameterize the affine transformation for the second part. The output variable z' can be formulated as\n\\[\\begin{aligned} z_{1:j}' &= z_{1:j}, \\\\ z_{j+1:d}' &= T(z_{1:j}) + S(z_{1:j}) \\odot z_{j+1:d} \\end{aligned}\\]\nThe function S(\u00b7) and T(\u00b7) are neural networks and are not invertible. The sum and multiplication are element-wise operations. The complexity of S(\u00b7) and T(\u00b7) is essential for the modeling capability of coupling layers. Inverting the coupling layer can be performed by element-wise subtraction and division:\n\\[\\begin{aligned} z_{1:j} &= z_{1:j}', \\\\ z_{j+1:d} &= (z_{j+1:d}' - T(z_{1:j}))/S(z_{1:j}). \\end{aligned}\\]\nBy using multiple invertible masking [7,15], normalization [15], and coupling layers, we can ensure the input variables are fully processed and the model has enough capacity to learn the invertible transformations, enabling the model to transform between complex and simple distributions.\nNormalizing flow models inherently provide a measure of uncertainty, as they can capture the complex posterior distribution of data. In our work, we employ normalizing flow models to capture the conditional distribution of simulation data conditioned on the simulation parameters, facilitating uncertainty quantification and efficient parameter space exploration for ensemble data."}, {"title": "METHOD", "content": "Existing surrogate models lack uncertainty estimation in the prediction process and they only support predicting simulation results based on the simulation parameters specified by scientists. However, the critical issue often faced by scientists is the lack of knowledge about which simulation parameters are of interest. The exploration process can be computationally intensive and inefficient. This underscores the need for efficient user-guided exploration of the simulation parameter space. To address this gap, we propose an uncertainty-aware surrogate model SurroFlow and integrate it with a genetic algorithm to support efficient user-driven exploration of the simulation parameter space."}, {"title": "Overview", "content": ""}, {"title": "Uncertainty-Aware Surrogate Model", "content": "In this section, we discuss details about the proposed surrogate model SurroFlow, including (1) simulation data representation extraction, (2) conditional data generation and uncertainty quantification of the surrogate model, and (3) reverse prediction of simulation parameters for given simulation data."}, {"title": "Data Representation Extraction", "content": "SurroFlow contains two parts that contribute to efficient data generation. The first part is an autoencoder model for 3D data reduction. Flow-based models require constant data dimensionality to maintain invertibility, which ensures that there exists a one-to-one correspondence between the input and output of each transformation. Despite normalizing flows' success in density modeling, training and testing these models on high-dimensional data is computationally expensive. To reduce the computational costs, we pre-train a 3D autoencoder to project the original high-dimensional data onto a lower-dimensional latent space. Given a simulation data \\(x \\in \\mathbb{R}^{D \\times H \\times W}\\), with dimensions D, H, and W representing depth, height, and width, respectively, we train an autoencoder consisting of an encoder \\(g_e\\) and a decoder \\(g_d\\), both designed based on convolutional neural networks with residual connections [10], to extract a compact latent representation z. This encoding ensures the latent representation z captures the essential characteristics of the data x with reduced dimensionality, allowing the decoder to accurately reconstruct x' from latent representation z with low loss compared to the original data x. This process can be formulated as\n\\[z = g_e(x), \\quad \\text{and} \\quad x' = g_d(z).\\]\nIn the later stage, the normalizing flow model will be trained on the latent representations of data instead of their original high-dimensional counterparts. During inference, the flow-generated results will be further processed by the decoder of the autoencoder so that scientists can get the reconstructed data in the original data space."}, {"title": "Conditional Modeling for Uncertainty Quantification", "content": "In our work, we model the relationship between simulation parameters and simulation data as probabilistic distributions instead of deterministic mappings. This decision is motivated by two main factors. First, the neural network parameterized mapping between simulation parameters and data is an approximation, which inherently contains uncertainties. These uncertainties come from the limitations of the neural network in capturing the true complexity of the simulation relationships. Second, a probabilistic model is preferred since it is in general more robust. In most cases, since we do not have a large enough training dataset to approximate the complex but unknown mapping function, probabilistic modeling can better account for the range of possible outcomes.\nAs shown in Fig. 4, SurroFlow is a surrogate model based on a conditional normalizing flow to approximate the complex and time-consuming simulation process. SurroFlow learns a probabilistic mapping function of the simulation data conditioned on the simulation parameter. The learned probabilistic mapping will facilitate uncertainty quantification of the surrogate model. To reduce the computational cost, we train the normalizing flow in the latent space. This means the simulation data x used for training are first encoded into latent representations z by the trained encoder \\(g_e\\), as described in Eq. (5).\nGiven pairs of simulation parameters and the latent representations of the corresponding simulation output, our goal is to model the conditional distribution \\(p(z | c)\\), where \\(c \\in \\mathbb{R}^n\\) denotes the condition (i.e., an n-dimensional vector representing a set of simulation parameters) and z denotes the latent representation of the corresponding simulation result. However, this conditional distribution \\(p(z | c)\\) is complex and intractable. To model this distribution, we introduce a latent variable \\(z_0\\) with a well-defined Gaussian distribution \\(p_0(z_0 | c)\\), and model the unknown conditional distribution \\(p(z | c)\\) by learning a transformation function f. This function f transforms a sample \\(z_0\\) from the simple, known distribution \\(p_0(z_0 | c)\\) to a sample z within the complex, target distribution \\(p(z | c)\\), formulated as\n\\[z = z_K = f(z_0, c),\\]"}, {"title": "Reverse Prediction", "content": "To enable the reverse prediction of simulation parameters from simulation data, we introduce a constraint in the latent space of the flow, ensuring it can accurately predict the conditional information. Specifically, we impose a loss during training to ensure a sub-vector \\(z_c\\) in m matches the simulation parameters c, where \\(m = \\langle ..., z_c \\rangle\\) is the output of the conditional normalizing flow, as illustrated in Eq. (9). Once the model is well-trained, \\(z_c\\) will accurately approximate c.\nDuring inference, given a simulation outcome x, scientists first utilize encoder \\(g_e\\) to obtain the latent representation \\(z_K = g_e(x)\\). Then, as illustrated by the black solid arrows in Fig. 4, the inverse of the unconditional normalizing flow \\(f_u^{-1}\\) can predict the simulation parameters c associated with encoded data as follows:\n\\[\\langle ..., z_c \\rangle = f_u^{-1}(g(x)), \\quad \\\\ c = z_c.\\]\nIn summary, SurroFlow enables an uncertainty-aware bidirectional mapping between simulation parameters and simulation data. Given simulation parameters, SurroFlow can produce the corresponding simulation data and quantify the uncertainties in the data generation process. In the reverse direction, SurroFlow can predict the simulation parameters from the given data. This bidirectional prediction ability is highly beneficial, especially when scientists prefer a robust model for both simulation data generation and simulation parameter prediction, a challenging task for other generative models such as Generative Adversarial Networks (GANs) or Variational Autoencoders (VAEs)."}, {"title": "Loss Functions", "content": "We first train the autoencoder model as discussed in Sec. 4.2.1, and then train SurroFlow in Sec. 4.2.2 and Sec. 4.2.3. Our training data are pairs of simulation parameters c and corresponding simulation data x. The autoencoder is trained on the simulation data with a Mean Squared Error (MSE) loss between the original data x and the reconstructed data x':\n\\[\\mathcal{L}_{ae} = \\mathbb{E}_x [||x - x'||^2].\\]\nTraining data for SurroFlow are pairs of simulation parameters and latent representations of the corresponding simulation output. Given data x, we first extract its latent representation z via the trained encoder \\(g_e\\) and utilize z for SurroFlow training. SurroFlow is trained based on a combination of two losses:\n\\[\\mathcal{L}_{flow} = \\mathcal{L}_f + \\alpha \\cdot \\mathcal{L}_c.\\]\nThe hyperparameter \\(\\alpha\\) balances these two losses. First, we aim to find model parameters that accurately describe the conditional distribution \\(p(z | c)\\), as detailed in Eq. (8). The first loss \\(\\mathcal{L}_f\\) is the standard log-likelihood loss for normalizing flow training:\n\\[\\mathcal{L}_f = \\mathbb{E}_{z,c}[-\\text{log } p(z | c)].\\]"}, {"title": "Genetic Algorithm for Parameter Candidate Generation", "content": "SurroFlow offers accurate data prediction for a given parameter configuration and also the ability to quantify the uncertainties in the data generation process. In this section, we discuss how we leverage the strengths of SurroFlow for efficient simulation parameter space exploration for ensemble data. Specifically, we introduce a Genetic Algorithm (GA) for simulation parameter candidate generation, in which SurroFlow helps guide the evolutionary algorithm to identify the simulation parameters that align with scientists' interests."}, {"title": "Fitness Function Design", "content": "Genetic algorithms are widely used to solve complex optimization problems by mimicking the process of natural selection and genetics. The key idea behind the genetic algorithm is to operate on a set of candidates in the current generation and improve the population gradually for an optimization goal. This approach is effective for new high-quality candidate discovery. In this optimization process, a fitness function is crucial in evaluating the \"fitness\" of candidates in each generation. It directly relates to the objective functions of the problem and influences the direction of the evolutionary process.\nInitially, scientists provide several simulation parameters that they might be interested in. The genetic algorithm then identifies new simulation parameters that meet scientists' interests based on quantitative measurements such as data similarity and diversity. In this case, the simulation outcome for each candidate parameter configuration is required. However, running the complex simulation for all candidates can be prohibitively slow. To speed up the optimization process, we utilize SurroFlow to quickly produce the simulation outputs for candidate parameters. We also incorporate the uncertainty information produced by the surrogate model SurroFlow into the genetic algorithm. Incorporating uncertainties can increase the robustness and reliability of the parameter recommendation and exploration process. For example, candidates with high uncertainty will have low fitness scores, despite their data being similar to scientists' preferences.\nOur fitness function considers the similarity, diversity, and uncertainty of candidate solutions (i.e., parameter sets). For a candidate simulation parameter configuration c, we utilize SurroFlow to predict data x and associated uncertainty \\(x_{var}\\), as described in Algorithm 1. Similarly, for a collection of simulation parameters \\(C_{usr}\\) selected by scientists, SurroFlow predicts the data list \\(X_{usr}\\). For the set of selected parameters \\(C_{usr}\\), scientists will assign preference scores \\(S_{usr} \\in [-1,1]\\) to each of them. A higher value indicates scientists are more interested in this parameter configuration. Our fitness function is defined as\n\\[\\begin{aligned} F(c, C_{usr}, S_{usr}) = &w_1 \\cdot \\text{sim}(x, X_{usr}, S_{usr}) \\\\ &+ w_2 \\cdot \\text{div}(c, C_{usr}) + w_3 \\cdot \\text{unc}(x_{var}), \\end{aligned}\\]\nwhere \\(w_1\\), \\(w_2\\), and \\(w_3\\) are weights in range [-1,1] to balance these metrics. By adjusting these weights appropriately, the fitness function in Eq. (15) can favor solutions with different objectives. For example, find simulation parameters that are diverse but have simulation outcomes similar to the user's preferences, while also encouraging low uncertainty in predicted data. This function can also be customized to identify simulation parameters with high uncertainty, helping training data augmentation for SurroFlow. The three components of the fitness function are:\n1. Similarity Score measures how closely the predicted data x of a candidate parameter c, matches the user preferred outcomes \\(X_{usr}\\):\n\\[\\text{sim}(x, X_{usr}, S_{usr}) = \\sum_{x_{usr} \\in X_{usr}} \\frac{1}{\\sum_{x_{ust} \\in X_{ust}} S_{usr}} \\cdot S_{usr} \\cdot \\frac{1}{\\text{distance}(x, x_{usr})}.\\]\nSimilar to content-based filtering widely used in modern recommendation systems, this metric recommends simulation parameters that have similar outputs to the user's preferences. The inverse distance is used to measure similarity. Candidates whose data are similar to the user's selection with high user-specified preference scores in \\(S_{usr}\\) will have high similarity scores.\n2. Diversity Score evaluates how distinct c is compared to the user preferred set of parameters \\(C_{usr}\\):\n\\[\\text{div}(c, C_{usr}) = \\sum_{c_{usr} \\in \\mathcal{N}_k(c)} \\text{distance}(c, c_{usr}).\\]\nThis metric encourages the variety among the recommended simulation parameters. Formally, the diversity is measured as the total distance to k nearest neighbors of c in the user's selection set \\(C_{usr}\\), represented as \\(\\mathcal{N}_k(c)\\). We set k = 5.\n3. Uncertainty Score quantifies the variance in the SurroFlow's output x:\n\\[\\text{unc}(x_{var}) = - \\frac{1}{n} \\sum_i (x_{var})_i.\\]\nIt is measured as the mean of the estimated uncertainty field, where \\((x_{var})_i\\) is the uncertainty value at the i-th index of \\(x_{var}\\) and n is the dimensionality of \\(x_{var}\\). High variance indicates low confidence in the model's output.\nTo speed up the similarity and uncertainty computation during optimization, the calculation is based on the latent representations of data generated by SurroFlow, which can be done much more efficiently than using the reconstructed data generated by the decoder. This avoids the costly and unnecessary full reconstruction of data for similarity comparison. Hence, \\(\\text{distance}(x, x_{usr})\\) in Eq. (16) is evaluated using the inverse of cosine similarity between latent representations, while \\(\\text{distance}(c, c_{usr})\\) in Eq. (17) is based on L1 differences between candidate and user-selected parameters. The associated uncertainty is assessed through the variance of these latent representations."}, {"title": "Candidates Optimization Process", "content": "As outlined in Algorithm 2, the genetic algorithm begins with a set of randomly initialized candidates for the first generation. Subsequent generations proceed as follows: all current candidates are evaluated using the fitness function in Eq. (15), where higher scores indicate better solutions. Then, selection is performed to choose parent candidates for reproduction. Samples with higher fitness scores will have a high probability of being chosen. After selection, crossover is applied, i.e., candidate parameter vectors are divided into chunks and recombined to create new offspring. This crossover ensures that the offspring inherits characteristics from both parents. To ensure diversity and prevent local optima, the mutation is then performed at a rate of \\(r_m\\) by adding Gaussian noise to the candidate parameter. The Gaussian noise has a mean of zero and a standard deviation of \\(r_o\\). We set \\(r_m = 0.2\\) and \\(r_o = 0.1\\). This mutation introduces controlled variability which is essential for effective exploration of the simulation parameter space. These new candidates form the subsequent generation. The optimization process terminates when it reaches the maximum number of generations. After generations of optimization, we can identify simulation parameters that align with scientists' preferences."}, {"title": "Visual Interface for Parameter Space Exploration", "content": "We integrate the trained SurroFlow with the genetic algorithm as the backend and develop a visual interface to assist exploration of the vast parameter space. This combination improves the efficiency of the exploration process by leveraging scientists' knowledge to identify simulation parameters of interest.\nThe visual interface facilitates the parameter exploration process in three ways. First, it displays volume rendering images of the SurroFlow's prediction results and the quantified uncertainties for user-selected simulation parameters, enabling informed decision-making. Second, the visual system enables scientists to assign preference scores to candidates after carefully inspecting the visualization results. Then, the genetic algorithm in the backend computes fitness scores for candidate simulation parameters according to scientists' preferences, guiding the optimization toward scientists' preferred directions. Third, the interface provides an overview of the genetic algorithm's optimization process across generations, offering insights into its progression. An overview of the visual exploration and optimization process is shown in Fig. 5. It contains three major steps.\nStep 1: Specify the scientist's preference. In the simulation parameter selection view of the visual interface, scientists choose multiple simulation parameters to define their interests. The trained surrogate model SurroFlow in the backend can efficiently generate data for the selected simulation parameters. The reconstruction and uncertainty quantification results of the selections are visualized in Fig. 6 (2). After carefully analyzing the visualization results, scientists can assign preference scores to these selected parameters. A higher score indicates a higher interest. The user-selected parameters and corresponding preference scores are recorded in the table in Fig. 6 (1).\nStep 2: Optimize through the genetic algorithm. The second step is to optimize the randomly initialized candidate parameters through the genetic algorithm running in the backend of the visual system. The optimization process is controlled and visualized in Fig. 6 (3). In this view, scientists can interactively assign different weights for similarity, diversity, and uncertainty for different optimization objectives, as discussed in Sec. 4.3. The initialized simulation parameters will then evolve through selection, crossover, and mutation based on the calculated fitness scores in each generation. The average fitness scores over generations are displayed in the line graph, where the x-axis represents the generation index and the y-axis represents the fitness score value. By brushing on the line graph, the Sankey diagram will be updated. Nodes in the Sankey represent candidate parameters and links depict the parent-child relationship. Node color is set via a drop-down menu, allowing color by similarity, diversity, or uncertainty. Each step in the Sankey diagram represents one generation. Hovering over a link highlights its ancestors and descendants from the first to the last generation within the brushed area. The iterative optimization ultimately presents scientists with simulation parameters that closely match their interests. Moreover, if scientists identify highly interesting new parameters during optimization, they can make further selections by clicking on nodes in the Sankey diagram to refine their selected parameter set for future optimization rounds.\nStep 3: Cluster and reverse prediction of representative simulation parameters. Given the potentially large set of recommended simulation parameters, running all recommendations through SurroFlow and visually inspecting all rendering results is impractical. To address this, we extract representative simulation parameters as our final recommendation. For all candidate simulation parameters in the last generation of the genetic algorithm, we first retrieve latent representations for data samples corresponding to these parameters. K-means clustering is then applied to take cluster centers as representative latent representations. We utilize the reverse prediction ability of SurroFlow to determine the simulation parameters for these cluster centers. We also project all latent representations onto a 2D plane using t-SNE (t-distributed Stochastic Neighbor Embedding) and visualize the clustering and reverse prediction results in Fig. 6 (4)."}, {"title": "RESULTS", "content": "We evaluate SurroFlow's ability as a surrogate model for data generation, uncertainty quantification, and reverse prediction. We also conduct a case study for simulation parameter recommendation and exploration with our visual interface."}, {"title": "Dataset and Implementation", "content": "The proposed model SurroFlow is evaluated using two scientific ensemble simulation datasets in Tab. 1, which also includes the simulation parameter ranges for both datasets.\nMPAS-Ocean [25] dataset contains the simulation results from the MPAS-Ocean model for ocean system simulation, developed by the Los Alamos National Laboratory. According to domain scientists' interests, we study four input parameters: Bulk wind stress Amplification (BwsA), Gent-McWilliams Mesoscale eddy transport parameterization (GM), Critical Bulk Richardson Number (CbrN), and Horizontal Viscosity (HV). During training, 128 parameter settings and corresponding data samples with a resolution of 192 \u00d7 96 \u00d7 12 are used. For testing, 20 parameter settings are utilized.\nNyx [2] is a cosmological simulation dataset from compressible cosmological hydrodynamics simulations by Lawrence Berkeley National Laboratory. Based on domain scientists' suggestions, we study three input parameters: the Omega Matter parameter (OmM) representing the total matter density, the Omega Baryon parameter (OmB) representing the density of baryonic matter, and the Hubble parameter (h) quantifying expansion rate of the universe. We utilize 70 parameter configurations for training and 20 for testing. The log density field of high-resolution 128 \u00d7 128 \u00d7 128 is used for experiments.\nSurroFlow is developed based on PyTorch\u00b9 and is trained on a single NVIDIA A100 GPU with a learning rate of \\(10^{-4}\\). The visual system is implemented with Vue.js\u00b2 for the frontend and Flask\u00b3 for the backend"}, {"title": "Quantitative Evaluation", "content": "In this section, we quantitatively evaluate SurroFlow's performance in terms of surrogate modeling and reverse prediction."}, {"title": "Surrogate Prediction", "content": "We use two metrics to assess the data generation quality of SurroFlow for a given simulation parameter input. For data-level evaluation, we employ peak signal-to-noise ratio (PSNR) to quantify the voxel-level differences between surrogate-generated data and actual simulation results. For image-level evaluation, we utilize the structural similarity index measure (SSIM) to evaluate the quality of volume rendering images from generated data against the simulation outcomes. Higher values of PSNR and SSIM indicate better quality.\nAs discussed in 4.2, SurroFlow contains two components: an autoencoder (AE) for data reduction and a flow-based model for distribution modeling. The flow-based model is trained on the AE's latent space. The quality of the AE's reconstructions is critical as it directly impacts the effectiveness of the flow-based modeling. Therefore, we first evaluate AE's reconstruction quality using the above metrics. As shown in Tab. 3, the AE model achieves high PSNR and SSIM scores for test data of all datasets, indicating minimal information loss in latent representations. This enables the SurroFlow to train on latent representations, reducing computational costs compared to using raw data.\nThen we compare SurroFlow's prediction ability with one state-of-the-art baseline, i.e., VDL-Surrogate [27] (shorted for VDL), a view-dependent surrogate model that utilizes latent representations from selected viewpoints for fast training and interpolation during inference. We use the original implementation of VDL. The model sizes and training time for VDL and SurroFlow are detailed in Tab. 2. The quantitative results in Tab. 3 show that SurroFlow can achieve comparable performance with the baseline. SurroFlow performs slightly better on the MPAS-Ocean dataset. For the Nyx dataset, since VDL utilizes an importance-driven loss to ensure high-density values are preserved, it performs better than SurroFlow. However, SurroFlow offers additional benefits including quantifying uncertainties in the surrogate model and predicting simulation parameters for given simulation outcomes."}, {"title": "Reverse Prediction", "content": "Different from other surrogate models, SurroFlow has the bidirectional prediction ability. In this section, we measure the ability of SurroFlow to predict simulation parameters for a given simulation outcome.\nTo assess the accuracy of predicted simulation parameters against the ground truth, we employ mean absolute error (MAE) and cosine similarity as our evaluation metrics. MAE is lower the better and cosine similarity is higher the better. Given that the parameters span diverse ranges, before we evaluate them with the metrics, we apply min-max normalization to every dimension of the predicted parameter vector based on the range detailed in Tab. 1. This evaluation for simulation parameter prediction is conducted on 2```json\n0 randomly selected simulation data for each dataset.\nTable 4 displays the average quantitative evaluation results on all test data of each dataset. Given the low MAE and high similarity of our reverse prediction results to the ground truth simulation parameters, it's evident that our model achieves significant accuracy in reconstructing the original simulation conditions. In Tab. 5, we also represent some examples of the parameter prediction results. Comparing the predicted parameters with the ground truth, we observe a close alignment, demonstrating the effectiveness of our model in simulation parameter prediction through the reverse direction of the surrogate model."}, {"title": "Qualitative Evaluation", "content": "In this section, we qualitatively compare the SurroFlow's results with the baseline model, VDL-Surrogate, via the volume rendering images. The volume rendering settings are kept the same for the same dataset. Figure 7 compares the volume rendering images of the ground truth data with the predicted ones on the MPAS-Ocean dataset. There are three neural network models in comparison, including an autoencoder (AE) for data reduction and two surrogate models, i.e., the baseline VDL and the proposed model SurroFlow. The two columns show results from two different simulation parameter configurations. In each result, we provide a zoom-in view at the bottom of volume rendering images for better comparison across models. The zoom-in region is near the region of interest for domain scientists called the Pacific Cold Tongue where the temperatures drop to about 24\u00b0C in this area. On the left side of the zoom-in figures, we also show the corresponding squared error map where we set the white color to low error regions. By comparing the rendering results of all models with the ground truth, we found that overall the AE used for latent representation extraction achieves the best results, with low errors and a clearer boundary of the feature in the zoom-in region. Comparing SurroFlow with the baseline model VDL, we found SurroFlow on average has a low error, and VDL can have higher errors in some feature boundaries.\nFigure 8 shows volume rendering images of Nyx data, comparing the ground truth with images generated by the baseline model VDL, the autoencoder (AE), and SurroFlow. Each row corresponds to the results from one simulation parameter setting. The AE model exhibits superior reconstruction quality due to its relatively straightforward task. Although both the baseline model VDL and SurroFlow successfully capture crucial features, VDL shows more clear high-density details than SurroFlow. However, VDL tends to exaggerate high-density outputs, as shown in the zoomed-in views for both rows. Regions expected to have low-density values now have relatively high values. The high-density feature preservation ability of VDL comes from their adoption of importance-driven loss, which may amplify the density values into a higher range. The proposed SurroFlow does not incorporate such important information, so the results are less biased, and with uncertainty quantification ability, SurroFlow is more reliable.\nIn summary, SurroFlow offers performance on par with the baseline model VDL when serving as a surrogate model. However, SurroFlow provides features like uncertainty quantification in the surrogate modeling process and the capability for reverse prediction of simulation parameters for the given simulation outcomes, functions that are not available for the baseline."}, {"title": "Uncertainty Quantification", "content": "Due to the explicit distribution modeling, SurroFlow captures the variations of predicted simulation outputs for the conditional input simulation parameters. In this section, we assess the uncertainties in the surrogate model SurroFlow using the method outlined in Algorithm 1. The number of samples for uncertainty quantification N is set to 20.\nIn Fig. 9, we show the volume rendering images of the estimated uncertainty field for the MPAS-Ocean dataset. These images are generated from three different simulation parameter configurations. We set parameters GM, CbrN, and HV to 900, 0.625, and 200, and explore the uncertainty introduced by the surrogate model SurroFlow when we change the parameter BwsA. The BwsA values are set to 0.5, 2.5, and 4.5 from left to right, respectively. As the value of BwsA increases, the uncertainty values in the eastern Pacific region also rise, as detailed in the zoom-in images in Fig. 9. Other regions maintain relatively low uncertainty levels. The uncertainty comes from the model's sensitivity to BwsA values, highlighting areas where SurroFlow can be less reliable. Despite this, the overall unreliability range remains at a lower level, underscoring the model's overall robustness."}, {"title": "Case study: Simulation Parameter Space Exploration", "content": "This section demonstrates the effectiveness of our visual system for efficient simulation parameter recommendation and exploration.\nThe case study is to explore the simulation parameter space for the MPAS-Ocean dataset, which includes four simulation parameters detailed in Tab. 1. The case study involves a scientist with over 15 years of experience in ocean science simulation analysis and model development. The scientist aims to analyze the cold tongue in the eastern equatorial Pacific Ocean. The cold tongue phenomenon is caused by the upwelling of deep, cold waters to the sea surface, forming a strip of cold water stretching along the equator. In the exploration process, with the interactive visual system, the scientist starts by selecting several simulation parameters randomly within their valid ranges. This provides a starting point for exploration, after which domain knowledge is applied to refine and guide the parameter exploration based on the scientist's expertise and the visualization results. After selection, SurroFlow running in the backend will predict the corresponding simulation outputs for the input simulation parameters. Based on the volume-rendered images of SurroFlow's outputs (as shown in Fig. 10), the scientist interactively assigns high preference scores to those with a relatively lower temperature in the east Pacific region, a feature he is interested in. The visual system uses a \"Cool to Warm\" colormap for the MPAS-Ocean dataset, allowing for intuitive temperature visualization. Parameters predicting higher temperatures in the target area are assigned with low preference scores to refine the focus on relevant simulations.\nThe weights in the genetic algorithm's fitness function were determined iteratively, guided by the scientist's domain knowledge and practical evaluation. Starting with initial guesses based on the optimization goal, the scientist gets the desired weights for similarity, diversity, and uncertainty (0.8, 0.6, and -0.8, respectively) through repeated testing. This configuration aims to discover new parameters that closely match the preferred simulation outcomes while ensuring diversity and reliability. The negative weight for uncertainty is strategically chosen to penalize parameters with high uncertainty. The quantified uncertainty stems from surrogate model approximation errors. Focusing on low-uncertainty parameters ensures reliable predictions for fitness function computation and exploration. However, high-uncertainty regions can also offer valuable insights, highlighting areas that need improvement and guiding future simulations and data augmentation to enhance model quality and reliability.\nWith the genetic algorithm optimized for 5 generations, the fitness scores plateau. As illustrated in the line chart in Fig. 11, there is a clear increasing trend in the fitness score over generations. By brushing from generation 1 to 5 on the line chart, the Sankey diagram will be updated, illustrating the evolution of candidates across these generations. Nodes in the Sankey diagram are color-coded based on the fitness scores, with darker colors indicating higher fitness scores. The overall fitness scores increase from left to right across generations. Nodes with higher fitness scores are more likely to be chosen as parents for the next generation. Nodes with larger lengths are those with a higher number of offspring. By hovering over a link in the Sankey diagram, the connected paths are highlighted in purple, tracing to the node's ancestors and descendants. Scientists can interact with the Sankey diagram by clicking on nodes to include the associated parameter configurations in the preferred selections and assign preference scores.\nBy altering the colors of the nodes in the Sankey diagram, scientists can gain insights into the simulation parameter optimization process. As depicted in Fig. 12, for the brushed region from generation 3 to 5, nodes in the Sankey diagram are color-coded according to each candidate's fitness score, similarity, diversity, and uncertainty, respectively. It is clear that, as the evolutionary algorithm proceeds, the similarity scores as well as the diversity scores for candidates are increased across generations. Furthermore, given the algorithm's design to minimize uncertainty (indicated by a negative weight on uncertainty), there is a clear trend toward reduced uncertainty. The Sankey results demonstrate the parameters optimization process aligns with the predefined scientists' objectives.\nGiven that each generation typically contains a lot of candidates, visualizing and analyzing results for all candidates may have duplicates and be inefficient. To address this, our visual system employs the K-means clustering algorithm to group the optimized candidates into K clusters based on their latent representations, as shown in Fig. 13. Then, the system leverages SurroFlow's reverse prediction ability to derive K representative parameter configurations from the K cluster centers. We set K = 3 in the experiment. As shown in Fig. 13, the recommended parameters settings are (1) BwsA = 3.914, GM = 993.87, CbrN = 0.678, HV = 235.086, (2) BwsA = 4.059, GM = 1030.204, CbrN = 0.688, HV = 239.752, and (3) BwsA = 4.084, GM = 1031.02, CbrN = 0.707, HV = 243.876. Through simulation runs, the scientist verifies that these identified configurations produce simulation outcomes that match the expected behavior of the cold tongue, a phenomenon he is interested in within the MPAS-Ocean dataset. Analyzing the cold tongue phenomenon is crucial because it is closely linked to significant climate patterns like El Ni\u00f1o and La Ni\u00f1a, which greatly impact global weather and climate variability. Accurate modeling of the cold tongue is essential for climate simulations. Our system's recommended parameters, such as higher BwsA values enhancing the cold tongue effect, provide a deep understanding of the contributing factors. Additionally, the system can identify less obvious configurations that influence the cold tongue, potentially guiding future research. This case study shows the SurroFlow and genetic algorithm-assisted visual system helps efficient simulation parameter exploration and recommendation."}, {"title": "DISCUSSION AND FUTURE WORK", "content": "Compared to existing surrogate models, the proposed SurroFlow has extra benefits such as uncertainty quantification and bidirectional prediction. Using SurroFlow and the genetic algorithm as the backend, we have demonstrated the effectiveness of the visual system for parameter space exploration driven by scientists' preferences. However, there are still several future directions we can explore.\nFirst, although the normalizing flow excels at learning complex distributions and has shown outstanding performance in SurroFlow, its learning ability is constrained by the invertible requirement. Other more powerful generative models, such as denoising diffusion models [12], can serve as potential alternatives for our task. However, they lack the bidirectional prediction ability. Second, in our case study, scientists prioritize data samples with low uncertainty to ensure reliability. However, samples with high uncertainty are also valuable. They reflect the areas where the network does not model well. By incorporating those highly uncertain data samples into the training dataset, we can enhance the robustness of the surrogate model. Third, applications for the scientist-guided simulation parameter recommendation can be broader. The proposed simulation parameter exploration system can be useful for a variety of real-world problems. In the future, we would like to explore the possibility of applying SurroFlow across different domains for various tasks such as optimizing engineering design and scientific discovery."}, {"title": "CONCLUSION", "content": "In this paper, we introduce SurroFlow, a novel surrogate model for efficient simulation parameter recommendation and exploration. SurroFlow effectively captures the conditional distribution of simulation outcomes conditioned on the simulation parameters, facilitating efficient data generation for different simulation parameters. The complex conditional distribution is modeled by applying a series of invertible and learnable transformations on a base Gaussian distribution. The variance in the Gaussian distribution reflects the uncertainty in the surrogate modeling process. Remarkably, these transformations also enable reverse computation, allowing for the prediction of simulation parameters from simulation data. We integrate SurroFlow with a genetic algorithm and utilize them as the backend for a visual interface, where scientists can interactively specify their optimization objectives through the visual system. After that, the optimization process starts automatically for scientists' preference-guided parameter exploration. Our quantitative and qualitative results demonstrate SurroFlow's ability to predict high-fidelity simulation outputs, navigate simulation parameter spaces efficiently, and quantify uncertainties."}, {"title": "ACKNOWLEDGMENTS", "content": "This work is supported in part by the US Department of Energy SciDAC program DE-SC0021360 and DE-SC0023193, National Science Foun-dation Division of Information and Intelligent Systems IIS-1955764, and Los Alamos National Laboratory Contract C3435."}]}