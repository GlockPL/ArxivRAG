{"title": "SurroFlow: A Flow-Based Surrogate Model for Parameter Space Exploration and Uncertainty Quantification", "authors": ["Jingyi Shen", "Yuhan Duan", "Han-Wei Shen"], "abstract": "Existing deep learning-based surrogate models facilitate efficient data generation, but fall short in uncertainty quantification, efficient parameter space exploration, and reverse prediction. In our work, we introduce SurroFlow, a novel normalizing flow-based surrogate model, to learn the invertible transformation between simulation parameters and simulation outputs. The model not only allows accurate predictions of simulation outcomes for a given simulation parameter but also supports uncertainty quantification in the data generation process. Additionally, it enables efficient simulation parameter recommendation and exploration. We integrate SurroFlow and a genetic algorithm as the backend of a visual interface to support effective user-guided ensemble simulation exploration and visualization. Our framework significantly reduces the computational costs while enhancing the reliability and exploration capabilities of scientific surrogate models.", "sections": [{"title": "1 INTRODUCTION", "content": "In fields like fluid dynamics, climate, and weather research, ensemble simulations have become critical for estimating uncertainty, improving decision-making, and enhancing the scientific understanding of complex systems. To fully study scientific phenomena and analyze the sensitivity of simulation parameters, scientists often need to conduct a sequence of simulations and compare the results generated from different configurations. However, scientific simulations can be computationally expensive and time-consuming. To speed up the simulation, analysis, and knowledge discovery cycle, a major research effort has been put into developing simulation and visualization surrogates to approximate the results of complex and computationally expensive simulations with much reduced costs.\nCurrently, many deep learning-based surrogate models have been proposed in scientific visualization fields to assist parameter space exploration for ensemble simulations [9, 11,27]. Although they have begun to gain traction, there still exist several limitations. The first limitation is the lack of uncertainty quantification for surrogate models. Surrogate models are approximations of the actual simulations and inherently contain uncertainties, it is important to convey uncertainties associated with the surrogate model's outputs so that scientists know how much they can trust the model's predictions. Uncertainty quantification also provides insights into how sensitive the surrogate models' predictions are with respect to the changes in the simulation parameters. This sensitivity information is crucial for scientists as it guides the exploration direction about where to focus more, for example, regions with high data variance, by running additional simulations. The second challenge is the lack of efficient exploration across the parameter space. Current works only support limited functionality such as predicting the data for a given simulation parameter set, but do not assist in discovering potential optimal simulation configurations across the large parameter space. Parameter space exploration based on heuristic or brute-force approaches can be computationally intensive and inefficient. A systematic and efficient way for parameter space exploration that considers scientists' interests is needed. The third challenge is reverse prediction, particularly the ability to predict the simulation parameters that can produce a specific simulation result. This ability is crucial when scientists explore the optimal outcomes by directly manipulating the simulation outputs but lack the knowledge of the underlying simulation parameters that could generate those manipulated data. Thus, there is a pressing need for effective approaches that can infer simulation parameters from observed outcomes.\nIn this work, we propose SurroFlow, a surrogate model based on the normalizing flow [7,15,24]. Normalizing flow is a type of generative model for modeling complex probability distributions. By learning the invertible transformation between a simple base distribution (e.g., Gaussian distribution) and a complex target distribution, we address the above challenges as follows. First, SurroFlow is a conditional normalizing flow model combined with an autoencoder, trained on pairs of simulation parameters and corresponding simulation outcomes. As it models the distribution of simulation data conditioned on the simulation parameters, SurroFlow can quantify the uncertainty in the surrogate modeling process. SurroFlow takes simulation parameters as the conditional input, by sampling from the simple base distribution multiple times and reconstructing them in the complex conditional distribution, the variations among the reconstructed outputs can reflect the uncertainties in the surrogate prediction. Second, the proposed uncertainty-aware surrogate model SurroFlow can assist interactive exploration of simulation parameters guided by scientists' interests and goals. This is done by integrating the trained SurroFlow with a genetic algorithm, empowered by an interactive visual interface for user-guided automatic parameter space exploration. Scientists can specify their preferences and optimization objectives via the visual interface. The genetic algorithm then iteratively explores generations of simulation parameters based on scientists' inputs, while SurroFlow operates in the backend to efficiently predict data and uncertainties for the given simulation parameters. Once scientists set up the objectives and trade-offs among data similarity, diversity, and uncertainty interactively on the visual interface, simulation parameter space exploration can proceed automatically. Third, another essential benefit of flow-based models is that they can perform prediction in both forward and reverse directions. In the forward direction, SurroFlow can predict data conditioned on the input simulation parameters, allowing for the efficient generation of high-quality synthetic data. Conversely, in the backward direction, SurroFlow can predict the underlying simulation parameters that produce the given simulation data. This bidirectional prediction allows an effective framework for both surrogate modeling and reverse prediction, enhancing the flexibility of parameter space exploration and post-hoc analysis.\nIn contrast to other surrogate models in the visualization field [11, 27, 28], SurroFlow has made significant advancements by combining the conditional normalizing flows with autoencoders for efficient latent space modeling, allowing uncertainty quantification of the surrogate model's outputs. SurroFlow also features an improved architecture for bi-directional predictions. Additionally, it integrates a genetic algorithm and an interactive visual interface for efficient simulation parameter space exploration. Through qualitative and quantitative evaluations, we"}, {"title": "2 RELATED WORKS", "content": "We employ an invertible normalizing flow for surrogate modeling with uncertainty estimation. In this section, we provide an overview of related research on visualization surrogate models for scientific data, global sensitivity analysis techniques, and uncertainty estimation for deep neural networks.\nSurrogate Models for Parameter Space Exploration. In the fields of oceanography and climate science, scientists frequently run computational simulations [14, 19] to explore parameter spaces. Replacing these costly and time-consuming simulation processes, numerous surrogate models [1,4, 23] have been proposed. Among these, Gramacy et al. [9] construct non-stationary Gaussian process trees that adaptively sample within the input space, enabling the selection of more efficient designs. He et al. [11] introduce InSituNet, an image-based surrogate model designed for real-time parameter space exploration. However, its reliance on regular grid mappings limits its application to unstructured data. To overcome this limitation, Shi et al. [28] propose GNN-Surrogate, specifically designed for navigating simulation outputs on irregular grids. Shi et al. [27] also introduce the VDL-Surrogate model based on view-dependent neural-network latent representations. It employs ray casting from varied viewpoints to aggregate samples into compact latent representations, thereby optimizing computational resource use and enhancing high-resolution explorations. Similarly, Danhaive et al. [6] propose a surrogate model for performance-driven design exploration within parametric spaces using conditional variational autoencoders. They employ a sampling algorithm to distill a dataset that captures valuable design insights and employ the variational autoencoders as surrogates for the simulation process. However, existing learning-based works do not account for uncertainties within the surrogate model. To address this deficiency, we propose a flow-based surrogate to enable uncertainty quantification of the surrogate model's prediction process.\nGlobal Sensitivity Analysis. Global sensitivity analysis methods include variance-based, differential-based, and regression-based approaches. They are crucial for understanding how parameters influence model outcomes. Variance-based methods, also known as ANalysis Of VAriance (ANOVA), decompose the output's variance into a sum of contributions from inputs and their interactions. Based on the application, Sobol indices [29] or Design of Experiment [18] are used to evaluate inputs' effects on the output. Differential-based methods, like the Morris [20], calculate input effects by varying each factor individually while keeping others fixed. This method is computationally efficient and easy to implement. Regression-based methods utilize linear approximations of the function to compute sensitivity based on metrics like Correlation Coefficients and Standardized Regression coefficients [13]. There are traditional surrogate models with global sensitivity analysis, such as Ballester et al. [3]'s Sobol tensor train decomposition. While efficient, these surrogate methods are outperformed by deep neural networks.\nUncertainty in Deep Neural Networks. Deep neural networks are often viewed as black boxes due to unclear decision-making rules. For scientific visualization and analysis, uncertainty quantification helps measure the reliability of the model, offering scientists a level of confidence about the model's outputs. A large number of uncertainty quantification methods have been proposed, including probabilistic models (e.g., Bayesian neural networks [21], Deep Gaussian process [5]), ensemble methods (e.g., DeepEnsemble [17]), and deep generative model-based methods [8, 12, 16]. In our work, we employ a deep generative"}, {"title": "3 BACKGROUND: NORMALIZING FLOW", "content": "SurroFlow is based on the normalizing flow for the conditional generation of simulation data conditioned on the simulation parameters. Normalizing flow models are a type of generative models that aim to learn a mapping from a simple probability distribution to a more complex one, allowing for the generation of high-quality data samples in the target distribution.\nThe key idea behind normalizing flow is to learn a series of invertible transformation functions that can gradually map samples from a simple distribution into a complex target distribution of interest, as shown in Fig. 1. Since the mapping is invertible, one can perform the transformation in the opposite direction, i.e., encode samples in the complex distribution into latent vectors with a simple and tractable distribution. Formally, denote the observed variables as x and latent variables as z, the mapping in the normalizing flow model, given by \\(f_e : \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\), is invertible such that \\(x = f_e (z)\\) and \\(z = f_e^{-1} (x)\\). The invertible mapping function \\(f_e\\) consists of a sequence of bijective functions \\(f_e = f_k \\circ f_{k-1} \\circ \\dots \\circ f_1\\), so that\n\\[x = z_K = f_k \\circ f_{k-1} \\circ \\dots \\circ f_1 (z_0),\\]\nwhere each \\(f_i\\) (for \\(i = 1,..., K\\)) is learnable and invertible. With the stacked K invertible functions, a normalizing flow can transform a latent variable \\(z_0\\), which follows the simple distribution (e.g., Gaussian distribution), into the target variable \\(z_K\\) with a more complex target distribution \\(p(x) = p_K(z_K)\\). Using the change of variables, the log likelihood of \\(z_K\\) can be computed as\n\\[log p_K(z_K) = log p_0 (z_0) - \\sum_{i=1}^{K}log det \\frac{\\partial f_i(z_{i-1})}{\\partial z_{i-1}}.\\]\nGiven the density of \\(z_0\\) and the learnable transformation functions \\(f_e\\), one can compute the log-likelihood of any x through Eq. (2). The optimization goal for normalizing flows is to find the \\(\\theta\\) that maximizes the log-likelihood of training samples, computed by Eq. (2). The invertibility of normalizing flows is theoretically constrained by design. Various learnable and invertible functions for \\(f_i (i = 1, ..., K)\\) have been proposed, but the most efficient and simplest approach is affine coupling layers [7]. The input variable z of this coupling layer is split into two parts \\(z_{1:j}\\) and \\(z_{j+1:d}\\) at index j, where the first part is unchanged and is used to parameterize the affine transformation for the second part. The output variable z' can be formulated as\n\\[z_{1:j} = z_{1:j},\\]\\[z_{j+1:d} = T (z_{1:j}) + S(z_{1:j}) \\odot z_{j+1:d}\\]\nThe function \\(S(\\cdot)\\) and \\(T(\\cdot)\\) are neural networks and are not invertible. The sum and multiplication are element-wise operations. The complexity of \\(S(\\cdot)\\) and \\(T(\\cdot)\\) is essential for the modeling capability of coupling layers. Inverting the coupling layer can be performed by element-wise subtraction and division:\n\\[z_{1:j} = z'_{1:j}\\]\\[z_{j+1:d} = (z'_{j+1:d} - T(z'_{1:j}))/S(z'_{1:j}).\\]\nBy using multiple invertible masking [7,15], normalization [15], and coupling layers, we can ensure the input variables are fully processed"}, {"title": "4 METHOD", "content": "Existing surrogate models lack uncertainty estimation in the prediction process and they only support predicting simulation results based on the simulation parameters specified by scientists. However, the critical issue often faced by scientists is the lack of knowledge about which simulation parameters are of interest. The exploration process can be computationally intensive and inefficient. This underscores the need for efficient user-guided exploration of the simulation parameter space. To address this gap, we propose an uncertainty-aware surrogate model SurroFlow and integrate it with a genetic algorithm to support efficient user-driven exploration of the simulation parameter space."}, {"title": "4.2 Uncertainty-Aware Surrogate Model", "content": "In this section, we discuss details about the proposed surrogate model SurroFlow, including (1) simulation data representation extraction, (2) conditional data generation and uncertainty quantification of the surrogate model, and (3) reverse prediction of simulation parameters for given simulation data."}, {"title": "4.2.1 Data Representation Extraction", "content": "SurroFlow contains two parts that contribute to efficient data generation. The first part is an autoencoder model for 3D data reduction. Flow-based models require constant data dimensionality to maintain invertibility, which ensures that there exists a one-to-one correspondence between the input and output of each transformation. Despite normalizing flows' success in density modeling, training and testing these models on high-dimensional data is computationally expensive. To reduce the computational costs, we pre-train a 3D autoencoder to project the original high-dimensional data onto a lower-dimensional latent space. Given a simulation data \\(x \\in \\mathbb{R}^{D\\times H\\times W}\\), with dimensions D, H, and W representing depth, height, and width, respectively, we train an autoencoder consisting of an encoder \\(g_e\\) and a decoder \\(g_d\\), both designed based on convolutional neural networks with residual connections [10], to extract a compact latent representation z. This encoding ensures the latent representation z captures the essential characteristics of the data x with reduced dimensionality, allowing the decoder to accurately reconstruct x' from latent representation z with low loss compared to the original data x. This process can be formulated as\n\\[z = g_e(x), \\text{ and } x' = g_d(z).\\]\nIn the later stage, the normalizing flow model will be trained on the latent representations of data instead of their original high-dimensional counterparts. During inference, the flow-generated results will be further processed by the decoder of the autoencoder so that scientists can get the reconstructed data in the original data space."}, {"title": "4.2.2 Conditional Modeling for Uncertainty Quantification", "content": "In our work, we model the relationship between simulation parameters and simulation data as probabilistic distributions instead of deterministic mappings. This decision is motivated by two main factors. First, the neural network parameterized mapping between simulation parameters and data is an approximation, which inherently contains uncertainties. These uncertainties come from the limitations of the neural network in capturing the true complexity of the simulation relationships. Second, a probabilistic model is preferred since it is in general more robust. In most cases, since we do not have a large enough training dataset to approximate the complex but unknown mapping function, probabilistic modeling can better account for the range of possible outcomes.\nAs shown in Fig. 4, SurroFlow is a surrogate model based on a conditional normalizing flow to approximate the complex and time-consuming simulation process. SurroFlow learns a probabilistic mapping function of the simulation data conditioned on the simulation parameter. The learned probabilistic mapping will facilitate uncertainty quantification of the surrogate model. To reduce the computational cost, we train the normalizing flow in the latent space. This means the simulation data x used for training are first encoded into latent representations z by the trained encoder \\(g_e\\), as described in Eq. (5).\nGiven pairs of simulation parameters and the latent representations of the corresponding simulation output, our goal is to model the conditional distribution \\(p(z | c)\\), where \\(c \\in \\mathbb{R}^n\\) denotes the condition (i.e., an n-dimensional vector representing a set of simulation parameters) and z denotes the latent representation of the corresponding simulation result. However, this conditional distribution \\(p(z | c)\\) is complex and intractable. To model this distribution, we introduce a latent variable \\(z_0\\) with a well-defined Gaussian distribution \\(p_0 (z_0 | c)\\), and model the unknown conditional distribution \\(p(z | c)\\) by learning a transformation function f. This function f transforms a sample \\(z_0\\) from the simple, known distribution \\(p_0(z_0 | c)\\) to a sample z within the complex, target distribution \\(p(z | c)\\), formulated as\n\\[z = z_K = f(z_0, c),\\]"}, {"title": "4.2.3 Reverse Prediction", "content": "To enable the reverse prediction of simulation parameters from simulation data, we introduce a constraint in the latent space of the flow, ensuring it can accurately predict the conditional information. Specifically, we impose a loss during training to ensure a sub-vector \\(z_c\\) in m matches the simulation parameters c, where \\(m = < ...,z_c >\\) is the output of the conditional normalizing flow, as illustrated in Eq. (9). Once the model is well-trained, \\(z_c\\) will accurately approximate c.\nDuring inference, given a simulation outcome x, scientists first utilize encoder \\(g_e\\) to obtain the latent representation \\(z_K = g_e(x)\\). Then, as illustrated by the black solid arrows in Fig. 4, the inverse of the unconditional normalizing flow \\(f_u\\) can predict the simulation parameters c associated with encoded data as follows:\n\\[<..., z_c>=f_u^{-1}(g_e(x)),\\]\\[c = z_c.\\]\nIn summary, SurroFlow enables an uncertainty-aware bidirectional mapping between simulation parameters and simulation data. Given simulation parameters, SurroFlow can produce the corresponding simulation data and quantify the uncertainties in the data generation process. In the reverse direction, SurroFlow can predict the simulation parameters from the given data. This bidirectional prediction ability is highly beneficial, especially when scientists prefer a robust model for both simulation data generation and simulation parameter prediction, a challenging task for other generative models such as Generative Adversarial Networks (GANs) or Variational Autoencoders (VAEs)."}, {"title": "4.2.4 Loss Functions", "content": "We first train the autoencoder model as discussed in Sec. 4.2.1, and then train SurroFlow in Sec. 4.2.2 and Sec. 4.2.3. Our training data are pairs of simulation parameters c and corresponding simulation data x. The autoencoder is trained on the simulation data with a Mean Squared Error (MSE) loss between the original data x and the reconstructed data x':\n\\[L_{ae} = E_x [||x-x'||^2].\\]\nTraining data for SurroFlow are pairs of simulation parameters and latent representations of the corresponding simulation output. Given data x, we first extract its latent representation z via the trained encoder \\(g_e\\) and utilize z for SurroFlow training. SurroFlow is trained based on a combination of two losses:\n\\[L_{flow}=L_f+\\alpha L_c.\\]\nThe hyperparameter \\(\\alpha\\) balances these two losses. First, we aim to find model parameters that accurately describe the conditional distribution \\(p(z | c)\\), as detailed in Eq. (8). The first loss \\(L_f\\) is the standard log-likelihood loss for normalizing flow training:\n\\[L_f = E_{z,c}[-log p(z | c)].\\]"}, {"title": "4.3 Genetic Algorithm for Parameter Candidate Generation", "content": "SurroFlow offers accurate data prediction for a given parameter configuration and also the ability to quantify the uncertainties in the data generation process. In this section, we discuss how we leverage the strengths of SurroFlow for efficient simulation parameter space exploration for ensemble data. Specifically, we introduce a Genetic Algorithm (GA) for simulation parameter candidate generation, in which SurroFlow helps guide the evolutionary algorithm to identify the simulation parameters that align with scientists' interests."}, {"title": "4.3.1 Fitness Function Design", "content": "Genetic algorithms are widely used to solve complex optimization problems by mimicking the process of natural selection and genetics. The key idea behind the genetic algorithm is to operate on a set of candidates in the current generation and improve the population gradually for an optimization goal. This approach is effective for new high-quality candidate discovery. In this optimization process, a fitness function is crucial in evaluating the \"fitness\" of candidates in each generation. It directly relates to the objective functions of the problem and influences the direction of the evolutionary process.\nInitially, scientists provide several simulation parameters that they might be interested in. The genetic algorithm then identifies new simulation parameters that meet scientists' interests based on quantitative measurements such as data similarity and diversity. In this case, the simulation outcome for each candidate parameter configuration is required. However, running the complex simulation for all candidates can be prohibitively slow. To speed up the optimization process, we utilize SurroFlow to quickly produce the simulation outputs for candidate parameters. We also incorporate the uncertainty information produced by the surrogate model SurroFlow into the genetic algorithm. Incorporating uncertainties can increase the robustness and reliability of the parameter recommendation and exploration process. For example, candidates with high uncertainty will have low fitness scores, despite their data being similar to scientists' preferences.\nOur fitness function considers the similarity, diversity, and uncertainty of candidate solutions (i.e., parameter sets). For a candidate simulation parameter configuration c, we utilize SurroFlow to predict data x and associated uncertainty \\(x_{var}\\), as described in Algorithm 1. Similarly, for a collection of simulation parameters \\(C_{usr}\\) selected by scientists, SurroFlow predicts the data list \\(X_{usr}\\). For the set of selected parameters \\(C_{usr}\\), scientists will assign preference scores \\(S_{usr} \\in [-1,1]\\) to each of them. A higher value indicates scientists are more interested in this parameter configuration. Our fitness function is defined as\n\\[F(c, C_{usr}, S_{usr}) = w_1 \\cdot sim(x, X_{usr}, S_{usr}) + w_2 \\cdot div(c, C_{usr}) + w_3 \\cdot unc(x_{var}),\\]\nwhere \\(w_1\\), \\(w_2\\), and \\(w_3\\) are weights in range [-1,1] to balance these metrics. By adjusting these weights appropriately, the fitness function in Eq. (15) can favor solutions with different objectives. For example, find simulation parameters that are diverse but have simulation outcomes similar to the user's preferences, while also encouraging low uncertainty in predicted data. This function can also be customized to identify simulation parameters with high uncertainty, helping training data augmentation for SurroFlow. The three components of the fitness function are:\n1.  Similarity Score measures how closely the predicted data x of a candidate parameter c, matches the user preferred outcomes \\(X_{usr}\\):\n\\[sim(x, X_{usr}, S_{usr}) = \\frac{\\sum_{x_{usr} \\in X_{usr}} S_{usr} \\cdot \\frac{1}{distance(x,x_{usr})}}{\\sum_{x_{ust} \\in X_{ust}} S_{usr}}.\\]\nSimilar to content-based filtering widely used in modern recommendation systems, this metric recommends simulation parameters that have similar outputs to the user's preferences. The inverse distance is used to measure similarity. Candidates whose data are similar to the user's selection with high user-specified preference scores in \\(S_{usr}\\) will have high similarity scores.\n2.  Diversity Score evaluates how distinct c is compared to the user preferred set of parameters \\(C_{usr}\\):\n\\[div(c, C_{usr}) = \\sum_{c_{usr}\\in N_k(c)} distance(c, c_{usr}).\\]\nThis metric encourages the variety among the recommended simulation parameters. Formally, the diversity is measured as the total distance to k nearest neighbors of c in the user's selection set \\(C_{usr}\\), represented as \\(N_k(c)\\). We set k = 5.\n3.  Uncertainty Score quantifies the variance in the SurroFlow's output x:\n\\[unc(x_{var}) = \\frac{1}{n} \\sum_{i} (x_{var})_i.\\]\nIt is measured as the mean of the estimated uncertainty field, where \\((x_{var})_i\\) is the uncertainty value at the i-th index of \\(x_{var}\\) and n is the dimensionality of \\(x_{var}\\). High variance indicates low confidence in the model's output.\nTo speed up the similarity and uncertainty computation during optimization, the calculation is based on the latent representations of data generated by SurroFlow, which can be done much more efficiently than using the reconstructed data generated by the decoder. This avoids the costly and unnecessary full reconstruction of data for similarity comparison. Hence, distance(x,xusr) in Eq. (16) is evaluated using the inverse of cosine similarity between latent representations, while distance(c, cusr) in Eq. (17) is based on L1 differences between candidate and user-selected parameters. The associated uncertainty is assessed through the variance of these latent representations."}, {"title": "4.3.2 Candidates Optimization Process", "content": "As outlined in Algorithm 2, the genetic algorithm begins with a set of randomly initialized candidates for the first generation. Subsequent generations proceed as follows: all current candidates are evaluated using the fitness function in Eq. (15), where higher scores indicate better solutions. Then, selection is performed to choose parent candidates for reproduction. Samples with higher fitness scores will have a high probability of being chosen. After selection, crossover is applied, i.e., candidate parameter vectors are divided into chunks and recombined to create new offspring. This crossover ensures that the offspring inherits characteristics from both parents. To ensure diversity and prevent local optima, the mutation is then performed at a rate of rm by adding Gaussian noise to the candidate parameter. The Gaussian"}, {"title": "4.4 Visual Interface for Parameter Space Exploration", "content": "We integrate the trained SurroFlow with the genetic algorithm as the backend and develop a visual interface to assist exploration of the vast parameter space. This combination improves the efficiency of the exploration process by leveraging scientists' knowledge to identify simulation parameters of interest.\nThe visual interface facilitates the parameter exploration process in three ways. First, it displays volume rendering images of the SurroFlow's prediction results and the quantified uncertainties for user-selected simulation parameters, enabling informed decision-making. Second, the visual system enables scientists to assign preference scores to candidates after carefully inspecting the visualization results. Then, the genetic algorithm in the backend computes fitness scores for candidate simulation parameters according to scientists' preferences, guiding the optimization toward scientists' preferred directions. Third, the interface provides an overview of the genetic algorithm's optimization process across generations, offering insights into its progression. An overview of the visual exploration and optimization process is shown in Fig. 5. It contains three major steps.\nStep 1: Specify the scientist's preference. In the simulation parameter selection view of the visual interface (Fig. 6 (1)), scientists choose multiple simulation parameters to define their interests. The trained surrogate model SurroFlow in the backend can efficiently generate data for the selected simulation parameters. The reconstruction and uncertainty quantification results of the selections are visualized in Fig. 6 (2). After carefully analyzing the visualization results, scientists can assign preference scores to these selected parameters. A higher score indicates a higher interest. The user-selected parameters and corresponding preference scores are recorded in the table in Fig. 6 (1).\nStep 2: Optimize through the genetic algorithm. The second step is to optimize the randomly initialized candidate parameters through the genetic algorithm running in the backend of the visual system. The optimization process is controlled and visualized in Fig. 6 (3). In this view, scientists can interactively assign different weights for similarity, diversity, and uncertainty for different optimization objectives, as discussed in Sec. 4.3. The initialized simulation parameters will then evolve through selection, crossover, and mutation based on the calculated fitness scores in each generation. The average fitness scores over generations are displayed in the line graph, where the x-axis represents the generation index and the y-axis represents the fitness score value. By brushing on the line graph, the Sankey diagram will be updated. Nodes in the Sankey represent candidate parameters and links depict the parent-child relationship. Node color is set via a drop-down menu, allowing color by similarity, diversity, or uncertainty. Each step in the Sankey diagram represents one generation. Hovering over a link highlights its ancestors and descendants from the first to the last generation within the brushed area. The iterative optimization ultimately presents scientists with simulation parameters that closely match their interests. Moreover, if scientists identify highly interesting new parameters during optimization, they can make further selections by clicking on nodes in the Sankey diagram to refine their selected parameter set for future optimization rounds.\nStep 3: Cluster and reverse prediction of representative simulation parameters. Given the potentially large set of recommended simulation parameters, running all recommendations through SurroFlow and visually inspecting all rendering results is impractical. To address this, we extract representative simulation parameters as our final recommendation. For all candidate simulation parameters in the last generation of the genetic algorithm, we first retrieve latent representations for data samples corresponding to these parameters. K-means clustering is then applied to take cluster centers as representative latent representations. We utilize the reverse prediction ability of SurroFlow to determine the simulation parameters for these cluster centers. We also project all latent representations onto a 2D plane using t-SNE (t-distributed Stochastic Neighbor Embedding) and visualize the clustering and reverse prediction results in Fig. 6 (4)."}, {"title": "5 RESULTS", "content": "We evaluate SurroFlow's ability as a surrogate model for data generation, uncertainty quantification, and reverse prediction. We also conduct a case study for simulation parameter recommendation and exploration with our visual interface."}, {"title": "5.1 Dataset and Implementation", "content": "The proposed model SurroFlow is evaluated using two scientific ensemble simulation datasets in Tab. 1, which also includes the simulation parameter ranges for both datasets.\nMPAS-Ocean [25] dataset contains the simulation results from the MPAS-Ocean model for ocean system simulation, developed by the Los Alamos National Laboratory. According to domain scientists' interests, we study four input parameters: Bulk wind stress Amplification (BwsA), Gent-McWilliams Mesoscale eddy transport parameterization (GM), Critical Bulk Richardson Number (CbrN), and Horizontal Viscosity (HV). During training, 128 parameter settings and corresponding data samples with a resolution of 192 \u00d7 96 \u00d7 12 are used. For testing, 20 parameter settings are utilized.\nNyx [2] is a cosmological simulation dataset from compressible cosmological hydrodynamics simulations by Lawrence Berkeley National Laboratory. Based on domain scientists' suggestions, we study three input parameters: the Omega Matter parameter (OmM) representing the total matter density, the Omega Baryon parameter (OmB) representing the density of baryonic matter, and the Hubble parameter (h) quantifying expansion rate of the universe. We utilize 70 parameter configurations for training and 20 for testing. The log density field of high-resolution 128 \u00d7 128 \u00d7 128 is used for experiments.\nSurroFlow is developed based on PyTorch\u00b9 and is trained on a single NVIDIA A100 GPU with a learning rate of 10-4. The visual system is implemented with Vue.js\u00b2 for the frontend and Flask\u00b3 for the backend"}, {"title": "5.2 Quantitative Evaluation", "content": "In this section, we quantitatively evaluate SurroFlow's performance in terms of surrogate modeling and reverse prediction."}, {"title": "5.2.1 Surrogate Prediction", "content": "We use two metrics to assess the data generation quality of SurroFlow for a given simulation parameter input. For data-level evaluation, we employ peak signal-to-noise ratio (PSNR) to quantify the voxel-level differences between surrogate-generated data and actual simulation results. For image-level evaluation, we utilize the structural similarity index measure (SSIM) to evaluate the quality of volume rendering images from generated data against the simulation outcomes. Higher values of PSNR and SSIM indicate better quality.\nAs discussed in 4.2, SurroFlow contains two components: an autoencoder (AE) for data reduction and a flow-based model for distribution modeling. The flow-based model is trained on the AE's latent space. The quality of the AE's reconstructions is critical as it directly impacts the effectiveness of the flow-based modeling. Therefore, we first evaluate AE's reconstruction quality using the above metrics. As shown in Tab. 3, the AE model achieves high PSNR and SSIM scores for test data of all datasets, indicating minimal information loss in latent representations. This enables the SurroFlow to train on latent representations, reducing computational costs compared to using raw data.\nThen we compare SurroFlow's prediction ability with one state-of-the-art baseline, i.e., VDL-Surrogate [27] (shorted for VDL), a view-dependent surrogate model that utilizes latent representations from selected viewpoints for fast training and interpolation during inference. We use the original implementation of VDL. The model sizes and training time for VDL and SurroFlow are detailed in Tab. 2. The quantitative results in Tab. 3 show that SurroFlow can achieve comparable performance with the baseline. SurroFlow performs slightly better on the MPAS-Ocean dataset. For the Nyx dataset, since VDL utilizes an importance-driven loss to ensure high-density values are preserved, it performs better than SurroFlow. However, SurroFlow offers additional benefits including quantifying uncertainties in the surrogate model and predicting simulation parameters for given simulation outcomes."}, {"title": "5.2.2 Reverse Prediction", "content": "Different from other surrogate models, SurroFlow has the bidirectional prediction ability. In this section, we measure the ability of SurroFlow to predict simulation parameters for a given simulation outcome.\nTo assess the accuracy of predicted simulation parameters against the ground truth, we employ mean absolute error (MAE) and cosine similarity as our evaluation metrics. MAE is lower the better and cosine"}, {"title": "5.3 Qualitative Evaluation", "content": "In this section, we qualitatively compare the SurroFlow's results with the baseline model, VDL-Surrogate, via the volume rendering images. The volume rendering settings are kept the same for the same dataset. Figure 7 compares the volume rendering images of the ground truth data with the predicted ones on the MPAS-Ocean dataset. There are"}, {"title": "5.4 Uncertainty Quantification", "content": "Due to the explicit distribution modeling, SurroFlow captures the variations of predicted simulation outputs for the conditional input simulation parameters. In this section, we assess the uncertainties in the surrogate model SurroFlow using the method outlined in Algorithm 1. The number of samples for uncertainty quantification N is set to 20.\nIn Fig. 9, we show the volume rendering images of the estimated uncertainty field for the MPAS-Ocean dataset. These images are generated from three different simulation parameter configurations. We set parameters GM, CbrN, and HV to 900, 0.625, and 200, and explore the uncertainty introduced by the surrogate model SurroFlow when we"}, {"title": "5.5 Case study: Simulation Parameter Space Exploration", "content": "This section demonstrates the effectiveness of our visual system for efficient simulation parameter recommendation and exploration.\nThe case study is to explore the simulation parameter space for the MPAS-Ocean dataset, which includes four simulation parameters detailed in Tab. 1. The case study involves a scientist with over 15 years of experience in ocean science simulation analysis and model development. The scientist aims to analyze the cold tongue in the eastern equatorial Pacific Ocean. The cold tongue phenomenon is caused by the upwelling of deep, cold waters to the sea surface, forming a strip of cold water stretching along the equator. In the exploration process, with the interactive visual system, the"}]}