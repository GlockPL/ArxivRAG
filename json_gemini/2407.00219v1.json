{"title": "Evaluating Human Alignment and Model Faithfulness of LLM Rationale", "authors": ["Mohsen Fayyaz", "Fan Yin", "Jiao Sun", "Nanyun Peng"], "abstract": "We study how well large language models (LLMs) explain their generations with rationales \u2013 a set of tokens extracted from the input texts that reflect the decision process of LLMs. We examine LLM rationales extracted with two methods: 1) attribution-based methods that use attention or gradients to locate important tokens, and 2) prompting-based methods that guide LLMs to extract rationales using prompts. Through extensive experiments, we show that prompting-based rationales align better with human-annotated rationales than attribution-based rationales, and demonstrate reasonable alignment with humans even when model performance is poor. We additionally find that the faithfulness limitations of prompting-based methods, which are identified in previous work, may be linked to their collapsed predictions. By fine-tuning these models on the corresponding datasets, both prompting and attribution methods demonstrate improved faithfulness. Our study sheds light on more rigorous and fair evaluations of LLM rationales, especially for prompting-based ones.", "sections": [{"title": "Introduction", "content": "The rise of large language models (LLMs) has significantly transformed the field of natural language processing (NLP) (Touvron et al., 2023; Team et al., 2023; OpenAI et al., 2024), enabling a wide range of applications from web question answering to complex reasoning tasks. However, they are not always reliable and usually cannot clearly explain their outputs (Ji et al., 2023), which limits the deployment of these models in high-stakes scenarios.\nRationales, i.e., tokens of the input text that are most influential to the models' predictions, are widely studied in the NLP community prior to the era of LLMs to interpret model predictions (Lei et al., 2016; DeYoung et al., 2019; Wiegreffe and Pinter, 2019; Jacovi and Goldberg, 2020). For smaller and open-source models like BERT (Devlin et al., 2019), rationales are extracted with attribution-based methods like attention weights (Wiegreffe and Pinter, 2019) or gradients (Li et al., 2016). For decoder-only LLMs, besides attribution-based methods, rationales can also be extracted by leveraging the instruction-following ability of LLMs and guiding them with explicit prompts to explain their predictions (Figure 1). We call these prompting-based rationales.\nTo evaluate different rationales, previous works on model interpretation establish two properties of rationales that are critical for successful interpretability: human alignment (DeYoung et al., 2019; Hase and Bansal, 2022) and faithfulness (Jacovi and Goldberg, 2020). Human alignment refers to the degree to which the rationales match or align with human-annotated rationales, while faithfulness assesses whether the rationales truly reflect the model's internal process. A longstanding debate exists regarding the relationship between these two aspects (Agarwal et al., 2024). However, studies on LLM rationales either focus on the faithfulness of off-the-shelf LLMs (Huang et al., 2023a; Madsen et al., 2024), or their human alignment (Chen et al., 2023), but lack a comprehensive exploration of the two properties together. Specifically, recent works (Huang et al., 2023a; Madsen et al., 2024) study prompting-based methods and show that they might not be faithful to the reasoning process of LLMs. Moreover, they only consider LLMs as out-of-box models, without fine-tuning on specific tasks. How fine-tuning of LLMs on downstream tasks influences the human alignment and faithfulness of LLM rationales is under-explored.\nIn this paper, we conduct extensive experiments to evaluate LLM rationales more comprehensively and bridge the gap in existing research. We consider five state-of-the-art LLMs, encompassing both open-source models (Llama2 (Touvron et al., 2023), Llama3, Mistral (Jiang et al., 2023)) and proprietary models (GPT-3.5-Turbo, GPT-4-Turbo (OpenAI et al., 2024)). Our study leverages two annotated natural language classification datasets, e-SNLI (Camburu et al., 2018a) and MedicalBios (Eberle et al., 2023), to evaluate and compare rationale extraction methods based on prompting strategies and feature attribution-based techniques such as Input\u00d7Gradient (Li et al., 2016).\nThrough our experiments, we find that while prompting-based rationales are generally less faithful than attribution-based methods, they tend to align better with human-annotated rationales, both before and after fine-tuning. Surprisingly, even when prompting-based rationales exhibit poor performance, they can still produce explanations that reasonably align with human reasoning.\nWe also observe that low classification performance and collapsing predictions might be related to the faithfulness limitation of LLM rationales. Fine-tuning LLMs on specific datasets improves the quality of rationales for both prompting and, particularly, attribution techniques in terms of faithfulness and human alignment. This finding complements the observations in Madsen et al. (2024), where faithful evaluation was conducted using only out-of-the-box LLMs.\nIn summary, our work contributes to the ongoing efforts to enhance the interpretability and trustworthiness of LLMs by providing empirical evidence and practical recommendations for extracting and evaluating rationales from these models."}, {"title": "Related Work", "content": "Interpretability Recent literature in natural language processing (NLP) has seen a surge in interpretability methods aimed at making models more transparent and understandable. The traditional interpretability methods include 1) attribution-based methods, which leverage the attention weights in models like transformers to identify which parts of the input the model focuses on when making a decision (Vaswani et al., 2023; Clark et al., 2019; Abnar and Zuidema, 2020), 2) Gradient-based methods, which provide explanations by identifying which input tokens most influence the model's output, often using techniques like gradient-based saliency maps (Simonyan et al., 2014a), or its extension by incorporating the input vector norms or integration (Sundararajan et al., 2017). 3) Vector-based methods that propagate the decomposed representations throughout the model achieving the best faithfulness results on encoder-based models (Kobayashi et al., 2020, 2021; Ferrando et al., 2022; Modarressi et al., 2022, 2023). More recently, researchers have been using rationales, written in natural language, to serve interpretability where they can reveal the \"reasoning\" behind model decisions.\nRationales Rationales can be categorized as free-form or extractive. Free-form rationales use natural language to explain the model's reasoning, filling in commonsense knowledge gaps. They can improve model performance (Sun et al., 2022a) and user interpretability (Sun et al., 2022b). Extractive rationales highlight specific parts of the input text that provide sufficient evidence for a prediction, independent of the rest of the input (Lei et al., 2016; DeYoung et al., 2020). They can also enhance model performance (Huang et al., 2021; Carton et al., 2021) and improve human interpretability (Strout et al., 2019). Our work focuses on extractive rationales for interpretability evaluation. In this research area, Huang et al. (2023b) studied faithfulness in ChatGPT, comparing prompting and Lime (Ribeiro et al., 2016). Madsen et al. (2024) investigated LLM faithfulness on models like Llama2 (Touvron et al., 2023), Falcon (Penedo et al., 2023), and Mistral (Jiang et al., 2023), noting its dependence on both model and dataset. Despite this, there is still a scarcity of analyses comparing LLM-generated rationales to interpretability methods. To the best of our knowledge, no studies seem to have assessed human alignment and model faithfulness jointly or used fine-tuning to overcome faithfulness evaluation limitations and explore its effects."}, {"title": "Experimental Setup", "content": "We utilize two natural language classification datasets that have been annotated with human rationales indicating which input words were pivotal for the ground truth label.\ne-SNLI This dataset (Camburu et al., 2018b) is a natural language inference task with 3 classes including Entailment, Contradiction, and Neutral, showing the relation between the premise and hypothesis sentences. This dataset is annotated for rationales supporting the classification label by DeYoung et al. (2019). We utilize 5,000 examples from the training set and 300 examples from the test set.\nMedicalBios MedicalBios (Eberle et al., 2023) consists of human rationale annotations for a subset of 100 samples (five medical classes) from the BIOS dataset (De-Arteaga et al., 2019) for the occupation classification task."}, {"title": "Models", "content": "We employ five of the latest large language models, encompassing both open-source and proprietary ones. From the open-source models, we utilize Llama2 (Touvron et al., 2023), LLama3, and Mistral (Jiang et al., 2023). For proprietary models, we include GPT3.5-Turbo and GPT4-Turbo (OpenAI et al., 2024). All models are prompted without sampling during generation, leading to deterministic outputs."}, {"title": "Methods", "content": "We employ various prompting strategies to explore the effects of prompt wording and model alignment in generating text of similar length to human-annotated rationale. The following prompts are used to evaluate these aspects.\nWe test two versions of prompts to examine how the clarity and length of the prompt influence the model's output:\nNormal Prompt This version provides a detailed explanation, including all the points the model should consider. It is longer and aims to ensure the model fully understands the task.\nShort Prompt Given that long prompts may confuse LLMs, we also use a shorter version that conveys the necessary information in a few sentences.\nWe also experiment with three versions of the introduced prompts to manage the number of words the model generates.\nUnbound Prompt In this method, the model is not restricted in the number of words it can generate. It needs to establish the appropriate length autonomously.\nTop-Var Prompt This prompt requires the model to generate exactly the same number of words as in the human rationale annotations for each sentence. This method controls for word count in our experiments, enabling us to assess model alignment independently of its word importance threshold.\nTop-Ratio Prompt In this approach, the model is guided to identify the top k most important words within a given sentence. The value of k is derived from a predetermined percentage of the total word count in the input sentence, a ratio established based on the training set. For example, this ratio is set at 20% for sentences in the e-SNLI dataset and 13% for those in the MedicalBios dataset."}, {"title": "Attribution-Based Methods", "content": "We employ the Inseq library (Sarti et al., 2023) to implement attribution-based methods for LLMs. Specifically, we select three available options: (i) Attention Weight Attribution, which utilizes the model's internal attention weights (Wiegreffe and Pinter, 2019); (ii) Simple Gradients (Saliency), which is based on the gradients of the output with respect to the inputs (Simonyan et al., 2014b); and (iii) Input Gradient, which factors in both the input vector size and the gradient in its calculations (Li et al., 2016). We choose these methods because of their demonstrated faithfulness in previous work on NLP models (Atanasova et al., 2020; Modarressi et al., 2022, 2023), and their potential for efficient execution on large language models with limited computational resources."}, {"title": "Results", "content": "In this section, we delve into utilizing both prompting-based and attribution-based approaches to extract rationale from the model, focusing on two aspects: human alignment and model faithfulness. Furthermore, we conduct fine-tuning experiments on open LLMs to examine how this process influences alignment and faithfulness."}, {"title": "Human Alignment", "content": "The annotated rationales provide explanations for the ground truth label. Therefore, for the evaluation of human alignment, we first request the model to provide a rationale for the provided label. With this, we create an array of binary values where for each word in the input sentence, we indicate a 1 if it is present among the generated words (0 otherwise). By comparing these vectors with the equivalent binary representations of human annotations, we calculate the F1 score. This F1 score for human alignment is reported for the e-SNLI and MedicalBios datasets.\nFirstly, prompting-based methods outperform attribution-based methods on human alignment. Short or normal prompting demonstrates superior performance compared to attribution-based methods in nearly all datasets and models except for Llama-3-8B on e-SNLI, which is also comparable. This gap can be attributed to the reliance of attribution-based methods on the classification capability of LLMs (which might be subpar).\nSecondly, we note that providing additional information about the number of words selected by humans in Top-Var settings enhances alignment, indicating disparities between model thresholds for word importance in Unbound prompting compared to human annotators. Furthermore, the random baseline, which involves selecting Top-Var random words in each sentence, yields F1 scores of 0.27\u00b10.04 and 0.22\u00b10.01 for e-SNLI and MedicalBios respectively across 100 seeds. Contrasted with the Top-Var rows in Table 2, this indicates that both attribution-based and prompting-based approaches exhibit superior alignment compared to a random baseline.\nThirdly, across models, the performance comparison between normal and short prompts is varying and inconclusive. For example, short prompts perform better for Mistral, LLama-3, and GPT-4-Turbo, while they perform worse for LLama-2 and GPT-3.5 models.\nFinally, comparing the evaluated models reveals GPT-4-Turbo to be the most aligned with humans. However, other models demonstrate task-dependent alignment, with some excelling in e-SNLI and others in MedicalBios."}, {"title": "Effect of Fine-tuning on Alignment", "content": "As described by Wang et al. (2024) and Zhong et al. (2023), zero-shot LLMs may underperform small fine-tuned models such as BERT. And smaller LLMs like LLaMA-2-7B might even collapse entirely. We observe similar failure patterns, where models generate a single label from the possible options regardless of the input sentence. This phenomenon is illustrated in Figure 2, where the pre-trained (PT) open LLMs achieve near-random accuracy (33%) on the e-SNLI dataset. This issue raises the question of whether fine-tuning these models to improve their classification performance also aids in aligning their explanations more closely with human expectations.\nTo address this issue, we fine-tune the LLMs using LORA (Hu et al., 2022), a parameter-efficient fine-tuning technique. The hyperparameters for fine-tuning are provided in Table 7. Figure 2 demonstrates that the classification performance of LLaMA-2, LLaMA-3, and Mistral improves significantly after fine-tuning on the e-SNLI dataset and shows slight improvements on the MedicalBios dataset, outperforming GPT-4-Turbo in both cases.\nWe rerun the human alignment experiments for \"Short Prompt Top-Var\u201d and \u201cInput\u00d7Gradient Top-Var\" across all epochs. The results suggest a general trend of improved alignment of both methods with increasing epochs.\nTo analyze the generalization of this trend more comprehensively, we conduct alignment experiments on all prompting and attribution methods in Table 3 on the final epoch (5th epoch) of fine-tuning. The difference in human alignment between the fine-tuned and pre-trained models (Table 2) is reported in parentheses.\nOverall, the results show more positive change than negative in alignment. Among the prompting-based methods, \u201cShort Prompt Top-Var\u201d demonstrates the highest gains from fine-tuning. Attribution-based methods, particularly in LLaMA models, exhibit significant alignment improvements after fine-tuning. Moreover, fine-tuning can guide the model's attention to the correct words, especially in datasets like e-SNLI where pre-trained classification accuracy was low. We have demonstrated qualitative examples of such cases in Figure 5. As a result, these gradient-based methods can identify more human-aligned rationales by tracing back the attributions from the output label to the input sentence in fine-tuned models."}, {"title": "Faithfulness to the Model", "content": "While human alignment provides a useful measure of the plausibility of LLM rationales, it is also important to consider the faithfulness of these rationales to the model's actual decision-making process. A word may be crucial for the model's decision even if it does not align with human rationale and vice versa. Therefore, we must ask: Are the self-explanations genuinely influential in the model's decision-making process?\nTo evaluate faithfulness, we employ a perturbation-based experiment similar to previous work (Madsen et al., 2024; Modarressi et al., 2023). In this experiment, we mask the important words identified by the prompting and attribution methods and measure the flip rate of the predicted label during classification. A higher flip rate indicates that the masked words are indeed important to the model, leading it to change its previous decision, and this suggests that the explanation is more faithful to the model's decision-making process."}, {"title": "Limitations of Faithfulness Evaluation before Fine-Tuning", "content": "Table 4 presents the faithfulness flip rate of the pre-trained LLMs. A noteworthy finding is that in the e-SNLI dataset, where classification accuracy was notably low (Figure 2), both attribution-based and prompting-based methods resulted in a very small flip rate. Even more concerning, masking all the words in the input sentence led to less than a 3% flip rate for the Mistral and LLaMA-2 models (Mask EVERYTHING).\nIn further exploration, Figure 4 illustrates the Input Gradient attributions of the predicted label, shown in green, to all instruction and input words, shown in shades of red. We notice that in the pre-trained Llama-2 model, the prediction for the label \u201centailment\" is incorrect, with the model placing excessive emphasis on the word \u201centailment\" in the instruction while largely ignoring the input sentence including the premise and hypothesis sentences. However, after fine-tuning, the attribution distribution becomes less skewed, leading to a correct prediction by the model.\nTherefore, we hypothesize that the pre-trained model focuses more on the instruction rather than the input sentence. Moreover, we investigate two masking scenarios. The first scenario, denoted as INPUT, involves masking only the words from the input sentence (e.g., the premise and hypothesis in e-SNLI) while leaving the instruction intact, similar to Table 4. The second scenario, denoted as INPUT&INSTRUCTION, extends masking to the entire instruction and input, constituting the entire prompt. When we extend the masking to include the instruction, the flip rate can increase up to 100%. This indicates that the model relies heavily on the instruction for its decisions, regardless of the input sentence. This phenomenon aligns with findings by Yin et al. (2023) and Kung and Peng (2023), who both found that, among all segments of a prompt, label information or output space is essential for the model's performance. This raises concerns about the reliability of this experiment for measuring model faithfulness in LLMs.\nConsequently, we argue that to conduct a more robust faithfulness experiment on LLMs, it is not advisable to solely rely on pre-trained models, as their classification accuracy can vary depending on the model and dataset (Madsen et al., 2024). Instead, we suggest aligning the experiment more closely with the scenario of fine-tuned encoder-based models (Ferrando et al., 2022; Modarressi et al., 2023) by training the LLMs and assessing faithfulness on the fine-tuned model."}, {"title": "Faithfulness after Fine-Tuning", "content": "Table 5 displays the faithfulness flip rate of the fine-tuned open models on e-SNLI and MedicalBios. The number of masked words can directly influence the prediction flip rate in this experiment. To ensure a fair comparison, we limit the number of words considered in the Top-Ratio and Top-Var selections to the top k words for each sentence across different methods. This approach is particularly important for prompting-based methods, as LLMs struggle to follow instructions involving fine-grained hard constraints (Sun et al., 2023).\nFirst, we see that fine-tuning has effectively addressed the near-zero flip rate (Table 4) in e-SNLI, indicating that the model is no longer completely disregarding the input sentence.\nSecond, a comparison of results in each selection group of \"Top-Ratio\" and \"Top-Var\" (with a similar number of masked words) reveals that attribution-based methods generally outperform prompting. Consistent results can be seen in our top-k experiments in the appendix A.1 and Table 8. This difference can be attributed to the fact that attribution methods base their explanations on the model's internal processes, whereas prompting may provide plausible answers without direct access to this information, potentially diverging from the truth of the model's inner workings. Additionally, prompting is affected by the model's ability to follow instructions, which may result in the generation of an inaccurate number of words or the inclusion of words not present in the input sentence, leading to less faithful results.\nThird, we also present the flip rate after masking human rationales in Table 5. These rates are comparable to the \"Top-Var\" selection of other methods, as they involve the same number of masked words. Despite expectations that the model would better recognize the importance of words for its own decisions, prompting methods consistently underperform human rationales, and attribution methods did so in half of the cases. This result emphasizes that while the current methods demonstrate a degree of faithfulness, there remains room for further refinement and enhancement."}, {"title": "Conclusions", "content": "In this study, we investigated the extraction of rationales in Large Language Models (LLMs) with a focus on human alignment and model faithfulness. Our experiments encompassed both prompting-based and attribution-based methods across various LLM architectures and datasets. Before fine-tuning, we observed that prompting generally yielded better human alignment, even when classification performance was poor. However, the reliability of faithfulness evaluations was compromised by low classification performance and collapsing predictions in pre-trained models highlighting the need for refining faithfulness evaluation setup.\nTo address this, we fine-tuned the models to enhance their accuracy on classification tasks, which led to improvements in aligning their explanations more closely with human expectations. In this scenario, although prompting showed superior alignment before, its faithfulness in reflecting model decision-making was not as strong as that of attribution-based methods.\nDespite these improvements, a gap remained between the models' rationales and human rationales in both alignment and faithfulness. This highlights the need for the development of more advanced explanation methods to bridge this gap."}, {"title": "Limitations", "content": "LLM instruction-following abilities. In our implementation of prompting strategies, we heavily rely on the LLM's capability to follow instructions accurately. For example, when requesting the top-k words separated by a specific delimiter character, we expect the model to output a list of words in our desired format and quantity with no extra explanations. However, LLMs are still not fully adept at adhering to prompts precisely (Sun et al., 2023), which can lead to outputs in various formats different from our expectations. Since our primary focus in this paper is not to evaluate the format-following ability of LLMs, we have taken measures to address discrepancies in the outputs as much as possible.\nTo mitigate these discrepancies, we adopt tailored parsing approaches to handle unexpected output formats. For instance, if a model separates words in the output with a \u201c,\u201d character instead of the instructed character \u201cI\u201d, we adjust our parsing method accordingly. Fortunately, each model tends to adhere to a relatively consistent output format across the dataset, which enables us to adapt our parsing approach accordingly. Nonetheless, it's worth noting that an LLM with enhanced instruction-following abilities could potentially yield even better parsing results and consequently achieve higher performance levels.\nAttribution-based methods In selecting the explanation methods based on the inner workings of the models we opted for the ones that were already implemented for LLMs and were relatively efficient to execute given the large size of the models. Nonetheless, we acknowledge that recent vector-based methods have shown promising faithfulness results by decomposing the representations (Kobayashi et al., 2020, 2021; Modarressi et al., 2022; Ferrando et al., 2022; Modarressi et al., 2023) on smaller models such as BERT (Devlin et al., 2019) compared with the gradient-based methods. Our study highlights the gap that could be filled by implementing these methods for LLMs.\nPrompt Engineering Although we reported various versions of prompts for extracting rationales in this paper and conducted preliminary prompt engineering, we acknowledge that better prompts could potentially achieve higher performance. However, this approach diverges from realistic use cases where users may ask questions in various wordings. This limitation is inherent to prompting methods, whereas attribution-based methods are not susceptible to this issue. Therefore, addressing this limitation calls for continued exploration and refinement of both prompting and attribution-based methods in rationale extraction.\nLarger Models In our experiments, we evaluated open models with less than 8B parameters due to resource limitations. However, we acknowledge that larger models could potentially perform better in following instructions, leading to improved human alignment and model faithfulness in their self-explanations.\nPerturbation-based faithfulness evaluation In this paper, we conduct faithfulness evaluation of LLM rationales using perturbation-based metrics. Those metrics assume that removing critical features based on rationales would largely affect model performance. However, Whether perturbation-based metrics truly reflect rationale faithfulness is a widely discussed but unsolved question, as they would produce out-of-distribution counterfactuals. For example, Yin et al. (2022) show that with different kinds of perturbations such as removal or noise in hidden representations, the faithful sets vary significantly. For consistency, we follow previous work (DeYoung et al., 2019; Huang et al., 2023a). We leave deeper study into faithfulness measurements of LLM rationales to future work."}]}