{"title": "Evaluating Human Alignment and Model Faithfulness of LLM Rationale", "authors": ["Mohsen Fayyaz", "Fan Yin", "Jiao Sun", "Nanyun Peng"], "abstract": "We study how well large language models (LLMs) explain their generations with rationales \u2013 a set of tokens extracted from the input texts that reflect the decision process of LLMs. We examine LLM rationales extracted with two methods: 1) attribution-based methods that use attention or gradients to locate important tokens, and 2) prompting-based methods that guide LLMs to extract rationales using prompts. Through extensive experiments, we show that prompting-based rationales align better with human-annotated rationales than attribution-based rationales, and demonstrate reasonable alignment with humans even when model performance is poor. We additionally find that the faithfulness limitations of prompting-based methods, which are identified in previous work, may be linked to their collapsed predictions. By fine-tuning these models on the corresponding datasets, both prompting and attribution methods demonstrate improved faithfulness. Our study sheds light on more rigorous and fair evaluations of LLM rationales, especially for prompting-based ones.", "sections": [{"title": "1 Introduction", "content": "The rise of large language models (LLMs) has significantly transformed the field of natural language processing (NLP) (Touvron et al., 2023; Team et al., 2023; OpenAI et al., 2024), enabling a wide range of applications from web question answering to complex reasoning tasks. However, they are not always reliable and usually cannot clearly explain their outputs (Ji et al., 2023), which limits the deployment of these models in high-stakes scenarios.\nRationales, i.e., tokens of the input text that are most influential to the models' predictions, are widely studied in the NLP community prior\nto the era of LLMs to interpret model predictions (Lei et al., 2016; DeYoung et al., 2019; Wiegreffe and Pinter, 2019; Jacovi and Goldberg, 2020). For smaller and open-source models like BERT (Devlin et al., 2019), rationales are extracted with attribution-based methods like attention weights (Wiegreffe and Pinter, 2019) or gradients (Li et al., 2016). For decoder-only LLMs, besides attribution-based methods, rationales can also be extracted by leveraging the instruction-following ability of LLMs and guiding them with explicit prompts to explain their predictions (Figure 1). We call these prompting-based rationales.\nTo evaluate different rationales, previous works\non model interpretation establish two properties of rationales that are critical for successful interpretability: human alignment (DeYoung et al., 2019; Hase and Bansal, 2022) and faithfulness (Jacovi and Goldberg, 2020). Human alignment refers to the degree to which the rationales match or align with human-annotated rationales, while faithfulness assesses whether the rationales truly reflect the model's internal process. A longstanding debate exists regarding the relationship between these two aspects (Agarwal et al., 2024). However, studies on LLM rationales either focus on the faithfulness of off-the-shelf LLMs (Huang et al., 2023a; Madsen et al., 2024), or their human alignment (Chen et al., 2023), but lack a comprehensive exploration of the two properties together. Specifically, recent works (Huang et al., 2023a; Madsen et al., 2024) study prompting-based methods and show that they might not be faithful to the reasoning process of LLMs. Moreover, they only consider LLMs as out-of-box models, without fine-tuning on specific tasks. How fine-tuning of LLMs on downstream tasks influences the human alignment and faithfulness of LLM rationales is under-explored.\nIn this paper, we conduct extensive experiments to evaluate LLM rationales more comprehensively and bridge the gap in existing research. We consider five state-of-the-art LLMs, encompassing both open-source models (Llama2 (Touvron et al., 2023), Llama3, Mistral (Jiang et al., 2023)) and proprietary models (GPT-3.5-Turbo, GPT-4-Turbo (OpenAI et al., 2024)). Our study leverages two annotated natural language classification datasets, e-SNLI (Camburu et al., 2018a) and MedicalBios (Eberle et al., 2023), to evaluate and compare rationale extraction methods based on prompting strategies and feature attribution-based techniques such as Input\u00d7Gradient (Li et al., 2016).\nThrough our experiments, we find that while prompting-based rationales are generally less faithful than attribution-based methods, they tend to align better with human-annotated rationales, both before and after fine-tuning. Surprisingly, even when prompting-based rationales exhibit poor performance, they can still produce explanations that reasonably align with human reasoning.\nWe also observe that low classification performance and collapsing predictions might be related to the faithfulness limitation of LLM rationales. Fine-tuning LLMs on specific datasets improves the quality of rationales for both prompting and, particularly, attribution techniques in terms of faithfulness and human alignment. This finding complements the observations in Madsen et al. (2024), where faithful evaluation was conducted using only out-of-the-box LLMs.\nIn summary, our work contributes to the ongoing efforts to enhance the interpretability and trustworthiness of LLMs by providing empirical evidence and practical recommendations for extracting and evaluating rationales from these models."}, {"title": "2 Related Work", "content": "Interpretability Recent literature in natural language processing (NLP) has seen a surge in interpretability methods aimed at making models more transparent and understandable. The traditional interpretability methods include 1) attribution-based methods, which leverage the attention weights in models like transformers to identify which parts of the input the model focuses on when making a decision (Vaswani et al., 2023; Clark et al., 2019; Abnar and Zuidema, 2020), 2) Gradient-based methods, which provide explanations by identifying which input tokens most influence the model's output, often using techniques like gradient-based saliency maps (Simonyan et al., 2014a), or its extension by incorporating the input vector norms or integration (Sundararajan et al., 2017). 3) Vector-based methods that propagate the decomposed representations throughout the model achieving the best faithfulness results on encoder-based models (Kobayashi et al., 2020, 2021; Ferrando et al., 2022; Modarressi et al., 2022, 2023). More recently, researchers have been using rationales, written in natural language, to serve interpretability where they can reveal the \"reasoning\" behind model decisions.\nRationales Rationales can be categorized as free-form or extractive. Free-form rationales use natural language to explain the model's reasoning, filling in commonsense knowledge gaps. They can improve model performance (Sun et al., 2022a) and user interpretability (Sun et al., 2022b). Extractive rationales highlight specific parts of the input text that provide sufficient evidence for a prediction, independent of the rest of the input (Lei et al., 2016; DeYoung et al., 2020). They can also enhance model performance (Huang et al., 2021; Carton et al., 2021) and improve human interpretability (Strout et al., 2019). Our work focuses on extractive rationales for interpretability evaluation. In this research area, Huang et al. (2023b) studied faithfulness in ChatGPT, comparing prompting and Lime (Ribeiro et al., 2016). Madsen et al. (2024) investigated LLM faithfulness on models like Llama2 (Touvron et al., 2023), Falcon (Penedo et al., 2023), and Mistral (Jiang et al., 2023), noting its dependence on both model and dataset. Despite this, there is still a scarcity of analyses comparing LLM-generated rationales to interpretability methods. To the best of our knowledge, no studies seem to have assessed human alignment and model faithfulness jointly or used fine-tuning to overcome faithfulness evaluation limitations and explore its effects."}, {"title": "3 Experimental Setup", "content": "We utilize two natural language classification\ndatasets that have been annotated with human ra-\ntionales indicating which input words were pivotal\nfor the ground truth label.\ne-SNLI This dataset (Camburu et al., 2018b) is\na natural language inference task with 3 classes\nincluding Entailment, Contradiction, and Neutral,\nshowing the relation between the premise and hy-\npothesis sentences.\nMedicalBios MedicalBios (Eberle et al., 2023) con-\nsists of human rationale annotations for a subset of\nWe employ five of the latest large language models,\nencompassing both open-source and proprietary\nones. From the open-source models, we utilize\nLlama2 (Touvron et al., 2023), Llama3, and Mis-\ntral (Jiang et al., 2023). For proprietary models, we\ninclude GPT3.5-Turbo and GPT4-Turbo (OpenAI\net al., 2024). All models are prompted without sam-\npling during generation, leading to deterministic\noutputs.\nWe employ various prompting strategies to explore\nthe effects of prompt wording and model align-\nment in generating text of similar length to human-\nannotated rationale.\nWe test two versions of prompts to examine how\nthe clarity and length of the prompt influence the\nmodel's output:\nNormal Prompt This version provides a detailed\nexplanation, including all the points the model\nshould consider. It is longer and aims to ensure\nthe model fully understands the task.\nShort Prompt Given that long prompts may con-\nfuse LLMs, we also use a shorter version that con-\nveys the necessary information in a few sentences.\nWe also experiment with three versions of the\nintroduced prompts to manage the number of words\nthe model generates.\nUnbound Prompt In this method, the model is\nnot restricted in the number of words it can gen-\nerate. It needs to establish the appropriate length\nautonomously.\nTop-Var Prompt This prompt requires the model\nto generate exactly the same number of words as\nin the human rationale annotations for each sen-\ntence. This method controls for word count in our\nexperiments, enabling us to assess model alignment\nindependently of its word importance threshold.\nTop-Ratio Prompt In this approach, the model is\nguided to identify the top k most important words\nwithin a given sentence. The value of k is de-\nrived from a predetermined percentage of the total\nword count in the input sentence, a ratio established\nbased on the training set."}, {"title": "4 Results", "content": "In this section, we delve into utilizing both\nprompting-based and attribution-based approaches\nto extract rationale from the model, focusing on\ntwo aspects: human alignment and model faithful-\nness. Furthermore, we conduct fine-tuning experi-\nments on open LLMs to examine how this process\ninfluences alignment and faithfulness.\nThe annotated rationales provide explanations for\nthe ground truth label. Therefore, for the evaluation\nof human alignment, we first request the model to\nprovide a rationale for the provided label. With this,\nwe create an array of binary values where for each\nword in the input sentence, we indicate a 1 if it is\npresent among the generated words (0 otherwise).\nBy comparing these vectors with the equivalent\nbinary representations of human annotations, we\nFirstly, prompting-based methods outperform\nattribution-based methods on human alignment.\nShort or normal prompting demonstrates superior\nperformance compared to attribution-based meth-\nods in nearly all datasets and models except for\nLlama-3-8B on e-SNLI, which is also compara-\nble. This gap can be attributed to the reliance of\nattribution-based methods on the classification ca-\npability of LLMs (which might be subpar).\nSecondly, we note that providing additional in-\nformation about the number of words selected by\nhumans in Top-Var settings enhances alignment,\nindicating disparities between model thresholds\nfor word importance in Unbound prompting com-\npared to human annotators. Furthermore, the ran-\ndom baseline, which involves selecting Top-Var\nrandom words in each sentence, yields F1 scores\nof 0.27\u00b10.04 and 0.22\u00b10.01 for e-SNLI and Medi-\ncalBios respectively across 100 seeds. Contrasted\nwith the Top-Var rows in Table 2, this indicates\nthat both attribution-based and prompting-based\napproaches exhibit superior alignment compared\nto a random baseline.\nThirdly, across models, the performance com-\nparison between normal and short prompts is vary-\ning and inconclusive. For example, short prompts\nAs described by Wang et al. (2024) and Zhong et al.\n(2023), zero-shot LLMs may underperform small\nfine-tuned models such as BERT. And smaller\nLLMs like LLaMA-2-7B might even collapse en-\ntirely. We observe similar failure patterns, where\nmodels generate a single label from the possible\noptions regardless of the input sentence.\nTo address this issue, we fine-tune the LLMs\nusing LORA (Hu et al., 2022), a parameter-efficient\nfine-tuning technique."}, {"title": "4.3 Faithfulness to the Model", "content": "While human alignment provides a useful measure of the plausibility of LLM rationales, it is also important to consider the faithfulness of these rationales to the model's actual decision-making process. A word may be crucial for the model's decision even if it does not align with human rationale and vice versa. Therefore, we must ask: Are the self-explanations genuinely influential in the model's decision-making process?\nTo evaluate faithfulness, we employ a perturbation-based experiment similar to previous work (Madsen et al., 2024; Modarressi et al., 2023). In this experiment, we mask the important words identified by the prompting and attribution methods and measure the flip rate of the predicted label during classification. A higher flip rate indicates that the masked words are indeed important to the model, leading it to change its previous decision, and this suggests that the explanation is more faithful to the model's decision-making process.\nIn further exploration, Figure 4 illustrates the Input Gradient attributions of the predicted label, shown in green, to all instruction and input words, shown in shades of red. We notice that in the pretrained Llama-2 model, the prediction for the label \u201centailment\" is incorrect, with the model placing excessive emphasis on the word \u201centailment\" in the instruction while largely ignoring the input sentence including the premise and hypothesis sentences. However, after fine-tuning, the attribution distribution becomes less skewed, leading to a correct prediction by the model.\nTherefore, we hypothesize that the pre-trained model focuses more on the instruction rather than the input sentence. Moreover, in Table 10, we investigate two masking scenarios. The first scenario, denoted as INPUT, involves masking only the words from the input sentence (e.g., the premise and hypothesis in e-SNLI) while leaving the instruction intact. The second scenario, denoted as INPUT&INSTRUCTION, extends masking to the entire instruction and input, constituting the entire prompt. When we extend the masking to include the instruction, the flip rate can increase up to 100%. This indicates that the model relies heavily on the instruction for its decisions, regardless of the input sentence. This phenomenon aligns with findings by Yin et al. (2023) and Kung and Peng (2023), who both found that, among all segments of a prompt, label information or output space is\nconsequently, we argue that to conduct a more robust faithfulness experiment on LLMs, it is not advisable to solely rely on pre-trained models, as their classification accuracy can vary depending on the model and dataset (Madsen et al., 2024). Instead, we suggest aligning the experiment more closely with the scenario of fine-tuned encoder-based models (Ferrando et al., 2022; Modarressi et al., 2023) by training the LLMs and assessing faithfulness on the fine-tuned model.\nFirst, we see that fine-tuning has effectively addressed the near-zero flip rate (Table 4) in e-SNLI, indicating that the model is no longer completely disregarding the input sentence.\nSecond, a comparison of results in each selection group of \"Top-Ratio\" and \"Top-Var\" (with a similar number of masked words) reveals that attribution-"}, {"title": "5 Conclusions", "content": "In this study, we investigated the extraction of rationales in Large Language Models (LLMs) with a focus on human alignment and model faithfulness. Our experiments encompassed both prompting-based and attribution-based methods across various LLM architectures and datasets. Before fine-tuning, we observed that prompting generally yielded better human alignment, even when classification performance was poor. However, the reliability of faithfulness evaluations was compromised by low classification performance and collapsing predictions in pre-trained models highlighting the need for refining faithfulness evaluation setup.\nTo address this, we fine-tuned the models to enhance their accuracy on classification tasks, which led to improvements in aligning their explanations more closely with human expectations. In this scenario, although prompting showed superior alignment before, its faithfulness in reflecting model decision-making was not as strong as that of attribution-based methods.\nDespite these improvements, a gap remained between the models' rationales and human rationales in both alignment and faithfulness. This highlights the need for the development of more advanced explanation methods to bridge this gap."}, {"title": "Limitations", "content": "LLM instruction-following abilities. In our implementation of prompting strategies, we heavily rely on the LLM's capability to follow instructions accurately. For example, when requesting the top-k words separated by a specific delimiter character, we expect the model to output a list of words in our desired format and quantity with no extra explanations. However, LLMs are still not fully adept at adhering to prompts precisely (Sun et al., 2023), which can lead to outputs in various formats different from our expectations. Since our primary focus in this paper is not to evaluate the format-following ability of LLMs, we have taken measures to address discrepancies in the outputs as much as possible.\nTo mitigate these discrepancies, we adopt tailored parsing approaches to handle unexpected output formats. For instance, if a model separates words in the output with a \u201c,\u201d character instead of the instructed character \u201cI\u201d, we adjust our parsing method accordingly. Fortunately, each model tends to adhere to a relatively consistent output format across the dataset, which enables us to adapt our parsing approach accordingly. Nonetheless, it's worth noting that an LLM with enhanced instruction-following abilities could potentially yield even better parsing results and consequently achieve higher performance levels.\nAttribution-based methods In selecting the explanation methods based on the inner workings of the models we opted for the ones that were already implemented for LLMs and were relatively efficient to execute given the large size of the models. Nonetheless, we acknowledge that recent vector-based methods have shown promising faithfulness results by decomposing the representations (Kobayashi et al., 2020, 2021; Modarressi et al., 2022; Ferrando et al., 2022; Modarressi et al., 2023) on smaller models such as BERT (Devlin et al., 2019) compared with the gradient-based methods. Our study highlights the gap that could be filled by implementing these methods for LLMs.\nPrompt Engineering Although we reported various versions of prompts for extracting rationales in this paper and conducted preliminary prompt engineering, we acknowledge that better prompts could potentially achieve higher performance. However, this approach diverges from realistic use cases where users may ask questions in various wordings. This limitation is inherent to prompting methods, whereas attribution-based methods are not susceptible to this issue. Therefore, addressing this limitation calls for continued exploration and refinement of both prompting and attribution-based methods in rationale extraction.\nLarger Models In our experiments, we evaluated open models with less than 8B parameters due to resource limitations. However, we acknowledge that larger models could potentially perform better in following instructions, leading to improved human alignment and model faithfulness in their self-explanations.\nPerturbation-based faithfulness evaluation In this paper, we conduct faithfulness evaluation of LLM rationales using perturbation-based metrics. Those metrics assume that removing critical features based on rationales would largely affect model performance. However, Whether perturbation-based metrics truly reflect rationale faithfulness is a widely discussed but unsolved question, as they would produce out-of-distribution counterfactuals. For example, Yin et al. (2022) show that with different kinds of perturbations such as removal or noise in hidden representations, the faithful sets vary significantly. For consistency, we follow previous work (DeYoung et al., 2019; Huang et al., 2023a). We leave deeper study into faithfulness measurements of LLM rationales to future work."}, {"title": "A Appendix", "content": "As previously noted, the number of masked words\nsignificantly impacts the flip rate. To explore this\nfurther, we conducted a Top-k experiment, mask-\ning k = 1, 2, 3, 4, 5, 10 words and calculating the\nflip rate (see Table 8). The results consistent with\nTable 5 indicate that on average, attribution-based\nmethods surpass prompting in terms of faithfulness.\nTotal Batch Size\nLearning Rate E-SNLI\nLearning Rate MedicalBios\nNum Epochs\nLearning Rate Scheduler Warmup Steps\nTraining Dataset Size\nLORA r\nLORA alpha\nLORA drop out\n64\n1e-05\n5e-06\n5\n10\n5000\n32\n16\n0.05"}]}