{"title": "Relation Learning and Aggregate-attention for Multi-person Motion Prediction", "authors": ["Kehua Qu", "Rui Ding", "Jin Tang"], "abstract": "Multi-person motion prediction is an emerging and intricate task with broad real-world applications. Unlike single person motion prediction, it considers not just the skeleton structures or human trajectories but also the interactions between others. Previous methods use various networks to achieve impressive predictions but often overlook that the joints relations within an individual (intra-relation) and interactions among groups (inter-relation) are distinct types of representations. These methods often lack explicit representation of inter&intra-relations, and inevitably introduce undesired dependencies. To address this issue, we introduce a new collaborative framework for multi-person motion prediction that explicitly modeling these relations: a GCN-based network for intra-relations and a novel reasoning network for inter-relations. Moreover, we propose a novel plug-and-play aggregation module called the Interaction Aggregation Module (IAM), which employs an aggregate-attention mechanism to seamlessly integrate these relations. Experiments indicate that the module can also be applied to other dual-path models. Extensive experiments on the 3DPW, 3DPW-RC, CMU-Mocap, MuPoTS-3D, as well as synthesized datasets Mix1 & Mix2 (9~15 persons), demonstrate that our method achieves state-of-the-art performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Human motion prediction (HMP) aims to predict future human motion sequences based on the observed sequences. The HMP plays a major role in many real-world applications, such as autonomous driving [61], [36], robotics [63], [48], surveillance systems [64]. Due to the remarkable development of deep learning, the HMP has made unprecedented progress in recent years [39], [29], [62], [6]. However, single-person motion prediction methods focus solely on intra-relations, which contain the relative positions and movement patterns among body joints (e.g., hip, ankle, wrist) of the same individual. Humans are intuitively social agents, and they continuously interact with other people and their motion may also be influenced by the motions of others. Therefore, multi-person motion prediction task carries more practical significance compared to single-person motion prediction. This task is more challenging because of the sophisticated interactions across different individuals.\n The previous works on multi-person motion prediction generally tend to exploring interactive behaviors by global relation modeling in which all skeleton joints are treated as a whole to establish the relations between them via self-attention [43], [42], [45], [55]. As shown in Fig. 1(a), these methods ignore the distinct representations of inter-relation and intra-relation. Noted that we define inter-relation as the relation of across different individuals' joints, as the blue dash line shown. Corresponding, we define intra-relation as the relations of an individual's joints, as the red dash line shown. By inputting all skeleton joints as a whole into the network, these methods may inevitably introduce undesired relations which weakens the learning effects of interaction information and simplifies the constraints of joints.\n To reveal the advantages of explicit representation relations, we conducted some Transformer-based architecture experiments on the CMU-Mocap dataset: (i) The explicit relation modeling adopts cross-attention to learn inter-relations between different individuals' joints, and self-attention to learn intra-relation of each individual's joints. (ii) The global modeling utilizes self-attention to learn all relations of all inputting skeleton joints. Both baselines are based on Transformer architecture. Inspired by methods [3], [4], we employ the Pearson correlation coefficient (PCC) to model the correlation between different people in the observed scene. The calculation can be"}, {"title": "II. RELATED WORK", "content": "Early research in single-person motion prediction focused on time series models [32], [1], [38]. Wang et al. [38] propose a novel Position-Velocity Recurrent Encoder-Decoder that effectively utilizes pose velocities and temporal positional information. Despite their success, it seems inappropriate to consider human motion purely as a time series. Human motion involves multiple joint points, each with inherent spatial rela- tionships. For example, the motion relationship between the knee and the ankle differs from that between the elbow and the shoulder. These spatial relationships cannot be captured separately in a time series. To address this issue, GCN-based models are now widely used [39], [40], [33], [6]. Sofianos et al. [40] propose a classic model based on GCN that integrates joint spatio-temporal features. This approach enhances the interaction between body joint connections and their temporal motion patterns, facilitating more effective modeling of human dynamics. Recently, Transformer-based methods have gained widespread attention in the field of motion prediction [31], [30], [29], [62], [2]. Specifically, Aksan et al. [30] employ a dual-stream architecture based on the Transformer to address both spatial and temporal modeling. However, these methods primarily address single-person scenarios and are unsatisfac- tory for multi-person tasks."}, {"title": "B. Human-Human/Object Interaction.", "content": "Humans are intuitively social agents, they continuously interact with other people and objects. Modeling such in- teractions has been proven to be effective in various tasks such as 3D human pose estimation [22], [13], [8], human action recognition [18], [25], [49], and human/object trajectory prediction [15], [50], [17].\n Cao et al. [50] incorporate scene context (human-object) into motion prediction but overlook human-human interac- tions, which are more complex due to the dynamic nature of humans compared to static objects. Corona et al. [13] expand the utilization of interactive information in motion prediction, but only limited human-human or human-object correlations are modeled due to the traditional time-series networks. Adeli et al. [49] develop a social context-aware motion prediction framework to capture the interaction between human and human, but the social pooling of method eliminates the char- acterization of different people's actions. Nevertheless, while these models can capture human-human/object interactions, they cannot be directly applied to HMP due to objectives."}, {"title": "C. Multi-Person Motion Prediction.", "content": "Obviously, it does not make sense to divide a multi-person task into multiple single-person tasks [60], [9]. Adeli [42] is the first to consider capturing spatial relations of all persons. Subsequently, more methods [43], [44], [45], [55] devote to considering the human interaction. Parsaeifard et al. [60] use a traditional LSTM [26] encoder-decoder to predict human trajectories, but this approach does not account for crucial interactions between human actions. Adeli et al. [42] adopt GAT [27] to model both people and objects as graph nodes to capture their interactions. Since the attention mechanism of GAT only acts on direct neighbor nodes, the model can capture only local features at each aggregation, making it difficult to directly access global features. This limitation can result in unsatisfactory predictions when global context is crucial. Nowadays, more methods [43], [44], [45], [55], [19] demonstrate the effectiveness of Transformer in this task, as each node can directly focus on all other nodes, not just its neighbors. This capability allows the model to capture global features. Despite these advancements, there is potential for further improvement. Wang et al. [43] employ a local- range Transformer to encode the motion of an individual in the sequence and a global-range Transformer to encode the motion of multiple individuals. Both encoded motions are then sent to a Transformer-based decoder to predict future motion. Guo et al. [19] proposes a novel cross interaction attention mechanism that exploits historical information of both persons, and learns to predict cross dependencies between the two pose sequences. This method primarily focuses on capturing human-to-human interactions and ignore the temporal features modeling, which are equally crucial for accurate prediction."}, {"title": "III. METHODOLOGY", "content": "We design a novel and explicit framework for multi-person motion prediction, including encoder-decoder, inter/intra- relation learning and relation aggregation, as shown in Fig. 4."}, {"title": "A. Overview", "content": "Given a scene with N persons, each has J skeleton joints, we define the observed sequence of the n-th person as Xn = {x1, x2,...x7}, where n \u2208 {1, 2, ..., N}, N denotes the number of observed people, T represents the observed motion sequence length, and each x RJX3 (It,1,I,2,..., I,J) \u2208 denotes the joints' 3-D coordinates of the n-th person at the t-th motion sequence. Our objective is to predict the future motion sequence of the n-th person, denoted as \u0176n = {YT+1, YT+2,..., YT+P}, using the observed sequence Xn, where P denotes the predicted motion sequence length. The ground-truth of the n-th person can be defined as Yn = {YT+1, YT+2,\u2026, YT+P}."}, {"title": "B. Encoder", "content": "Instead of using joint position coordinates as input, we utilize the position differences between consecutive time steps to represent individual velocities, thereby augmenting the joint information. Each velocity vrt is thus expressed as follows:\n\\begin{equation}vt^{n} = \\begin{cases}\n0 & (t = 1)\\\\frac{x_{t}^{n} - x_{t-1}^{n}}{T} & (2 \\leq t \\leq T)\n\\end{cases}\n\\end{equation}\nFollowing this approach, we obtain a new sequence Vn = {1,2,...7} as the input of the encoder. The output Fem of encoder is as follows:\n\n\nFen = MLP(Vn)\n\n\n\n\nThe MLP block comprises three linear layers, an activation function, and a dropout layer, designed to expand the feature dimensions."}, {"title": "C. Inter/Intra-Relation Learning", "content": "1) Inter-Relation Learning: To explicitly learn the inter- relation Fimter among multiple individuals, we introduce a novel module that utilizes cross-attention mechanism. The overall interaction score for each individual is obtained by weighted summing of the attention scores between that indi- vidual and the others, as shown in Fig. 5. The inter-relation feature for the n-th person is calculated as follows:\nFninter =\\sum_{m=1}^{N} \\alpha(n, m) \\times S_{(n, m)} \\quad (n\\neq m)\n\nThe a(n,m) is the weight coefficient in the Eq. 4, and deter- mined by the relative distance between the two individuals. It is calculated as follows:\n\\alpha(n, m) =\\frac{1}{\\sigma(\\lambda) \\times \\mu \\left[\\sum_{t=1}^{T} \\left||x_{t}^{n} - x_{t}^{m}\\right|\\right]+1}\n\n\nWhere X is the learnable decay factor, and \u03c3 (\u00b7) is the Sigmoid function, which constrains A within the range of 0 1. \u03bc denotes the average operation. The weight coefficient decreases as the distance between the n-th person and the m-th person increases.\n The S denotes the score of cross-attention mechanism. We use l to indicate the number of cross-attention blocks, where l\u2208 {1, 2, ..., L\u2081}. The calculation can be expressed as:\nS_{(n, m)} =\\begin{cases}\nATT(v^{n}, v^{m}) & (l = 1)\\\\ATT(S_{l-1}^{(n, m)}, v^{m}) & (l<l < L_{1})\n\\end{cases}\n\n\n\nATT (\u00b7) is the cross-attention mechanism.\n\\text {ATT}(n, m) = \\text {MH}\\left(\\text {softmax}\\left(\\frac{Q^{n}(K^{m})^{T}}{\\sqrt{d_{k m}}}\\right) V^{m}\\right)\n\nMH() denotes the computing of multi-head attention. Qn denotes the query of the n-th person, Vm and Km denote the value and key of the m-th person. \u221adkm is the feature size of the key Km.\n2) Intra-Relation Learning: In a mean time, we utilize GCN to exploit individual's intra-relations, as shown in Fig. 6. We feed Fem into the GC-blocks to get the intra-relation information\nH^{n, l} =\\begin{cases}\n\\text {Drop}\\left(\\sigma\\left(\\mathcal{A}^{l}\\left(\\text {BN}\\left(F^{e n}\\right)\\right) W^{l}\\right)\\right) & (l=1)\\\\\n\\text {Drop}\\left(\\sigma\\left(\\mathcal{A}^{l}\\left(\\text {BN}\\left(H^{n, l-1}\\right)\\right) W^{l}\\right)\\right) & (1<l<L_{2})\n\\end{cases}\n\n\n\n\n\nWhere I represents the 1-th GC-block (in our method 1 < l < L2), and Hn,l represents the output of the l-th GC-block of the n-th person. Drop(\u00b7) is a regularization, \u03c3(\u00b7) is an activation function, and the A\u00b9 represents the adjacent matrix of the l-th GC-block. The matrix A\u00b9 \u2208 RC\u00d7C is specifically designed to describe the relationships between nodes in GCN, where C = J \u00d7 3 denotes the 3-dimension of J input body joints. BN(\u00b7) represents Batch Normalization and W\u00b9 is the trainable parameter matrix. Ultimately, we derive the intra- relation features Futra of the n-th person."}, {"title": "D. Relation Aggregation", "content": "In this section, we introduce a novel feature aggregation module named IAM. It adopts a variant attention mechanism to aggregate intra&inter-relation features, as illustrated in Fig. 7.\nTo enhance the integration of the two types of relations, we project the intra-relation features into a new dimension, which can be formulated as follows:\nF_{intra }^n=PROJ \\left(F_{intra}^n\\right)=\\sigma \\left(F_{intra}^n W_{1}\\right)+\\sigma\\left(W_{2} \\left(F_{intra}^n W_{1}\\right) \\star W_{3} \\left(F_{intra}^n W_{1}\\right)\\right)\n\n\n\nWhere W1, W2 and W3 are learnable parameters, represents Hadamard product.\nWe then introduce a novel Interaction Aggregation Module (IAM) to enable our predictions to simultaneously utilize both inter&intra-relation features, as shown in Fig. 7:\nG_{i n}^{n, l}=\\begin{cases}\\text { PE }\\left(F_{i n t r a}^n\\right)+F_{i n t e r}^n & (l=1)\\\\\n\\mathcal{L}_{i n}^{n, l-1} \\odot \\mathcal{L}_{o u t}^{l-1} & (1<l<L_{3})\\end{cases}\n\n\n\n\n\\mathcal{G}_{i n}^{n, l}=\\begin{cases}\\text { PE }\\left(F_{i n t e r}^n\\right)+F_{i n t r a}^n & (l=1)\\\\\n\\mathcal{G}_{o u t}^{n, l-1} & (1<l<L_{3})\\end{cases}\n\n\n\nWhere \u201cPE(\u00b7)\u201d denotes the operation of position encoding. L and God denotes the input of l-th aggregation layer"}, {"title": "E. Decoder", "content": "After obtaining the refined features Grits and Lit, we concatenate these and feed them into the decoder:\n\\widehat{Y}^{n}=\\text {FC}\\left(\\operatorname{Concat}\\left(\\mathcal{G}_{out}^{n, L_{3}}, \\mathcal{L}_{o u t}^{n, L_{3}}\\right)\\right)\n\n\nFollowing this step, we can finally get the prediction Yn of the n-th person."}, {"title": "F. Loss functions", "content": "We define the final loss function Las the sum of Lp and Lo to train the model jointly. The final loss function can be expressed as follows:\n\\mathcal{L} = \\mathcal{L}_{p} + \\mathcal{L}_{v} \n= [lz (\\widehat{Y}^{n}, Y^{n})]^2+ [lz (\\widehat{Y}_{v}^{n}, Y_{v}^{n})]^2\n\nLp aims to minimize the l2 -norm between the predicted motion Yn and ground-truth Yn. L aims to minimize the l2 -norm between the predicted motion Yn and ground-truth of velocity Yn calculated by position difference."}, {"title": "IV. EXPERIMENT AND DISCUSSIONS", "content": "We select a range of base models to verify the effective- ness of our method, including classical approaches such as LTD [59], TRIPOD [42], DVITA [60], and MRT [43], the single-person motion prediction method TCD [20], as well as state-of-the-art models JRT [45] and TBIFormer [5] as our baselines. LTD [59] is a foundational model in the field of human motion prediction that encodes temporal information by operating in trajectory space. TRIPOD [42] first models interactions in human-to-human and human-to-object. DVITA [60] divides multi-person prediction tasks into several single- person prediction tasks. MRT [43] emphasizes the importance of interaction in multi-person prediction tasks by adopting a multi-range Transformer. JRT [45] explores the physical relations of the human body in conjunction with interactions to yield noteworthy outcomes. TBIFormer [5] addresses the human-to-human interactions by focusing on skeletal body parts. It converts all pose sequences into Multi-Person Body- Part sequences, utilizing these transformed sequences to learn the dynamics of body parts for both inter- and intra-individual interactions."}, {"title": "B. Datasets", "content": "We adopt four multi-person motion prediction benchmarks in the experiments: 3DPW [46], 3DPW-RC [45], CMU-Mocap [51], MuPoTS-3D [52] and synthesized datasets Mix1&Mix2 [43]. Details of the four datasets are presented below.\n3DPW [46] 3D Poses in the Wild Dataset (3DPW) is a large-scale 3D motion dataset collected by moving mobile phone cameras. It contains 60 videos and about 68,000 frames covering multiple scenarios and actions. In this paper, we follow the setting of [28], [42], [55], [45]. Each scene con- tains two persons, and our goal is to predict future 900ms (14 frames) motion using the historical 1030ms (16 frames) motion.\n3DPW-RC [45] According to the findings from the Joint- Relation Transformer [45], camera movement in the 3DPW dataset induces a significant unnatural drift, negatively im- pacting the modeling of multi-person interactions. The 3DPW- RC dataset subtracts the estimated camera velocity for better evaluation.\nCMU-Mocap [51] The Carnegie Mellon University Motion Capture Database (CMU-Mocap) consists of data from 112 subjects. Most scenes capture the movements of one person, and only 9 scenes include the movements and interactions of two persons. The study in [43] combines samples from both one-person and two-person scenes to create sequences featuring three individuals. We use the training and test sets provided by Wang et al. [43] to train and evaluate our model. We aim to predict future 3000ms (45 frames) motion using the historical 1000ms (15 frames) motion.\nMuPoTS-3D [52] Multiperson Pose Test Set in 3D (MuPoTS-3D) consists of over 8000 frames collected from 20 sequences with 8 subjects. Following previous works [43], [45], we evaluate our model's performance with the same segment length as CMU-Mocap on the test set.\nMix1&Mix2 [43] In order to evaluate the performance of our proposed model in scenarios involving a larger number of individuals, we adopt the methodology presented in the MRT [43] paper. We sample data from the CMU-Mocap and Panoptic [21] datasets to generate the Mix1 traning set. This training set contains approximately 3,000 samples, each featuring 9 to 15 people in the scene. Next we combine CMU-"}, {"title": "C. Implementation Details", "content": "In practice, for the 3DPW and 3DPW-RC datasets, we set the input length T = 16, the output length P = 14, and the number of person N = 2. For the CMU-Mocap and MuPoTS- 3D datasets, we set T = 15, P = 45 and N = 3.\nOur model consists of L\u2081 = 4 cross-attention blocks with H = 8 attention heads for inter-relation learning, L2 = 13 GC-blocks for intra-relation learning, and L3 = 4 aggregation layers in the IAM. We pre-train the model on the AMASS [58] dataset following previous works [45], [44], which pro- vides massive motion sequences. We utilize the PyTorch deep learning framework to develop our models and optimize the training with AdamW [53] optimizer. The learning rate is set to 1 \u00d7 10-5 for both pre-train and finetune and decay by 0.8 every 10 epochs. The batch size is set to 256 for pre-train, 128 for finetune. The training is performed on an NVIDIA 3080Ti GPU for 100 epochs."}, {"title": "D. Metrics", "content": "VIM [42] We adopt the Visibility-Ignored Metric (VIM) to measure the displacement in the joint vector, which has a dimension of J \u00d7 3, following previous works. This metric is used on the 3DPW and 3DPW-RC datasets.\nVIM@t =\\frac{1}{N} \\sum_{n=1}^{N} \\frac{1}{J} \\sum_{j=1}^{J} \\gamma_{n j}^{t} \\left|\\left|\\widehat{v}_{n j}^{t}-v_{n j}^{t}\\right|\\right|_{2}\n\nMPJPE [41] Mean Per Joint Position Error (MPJPE) is a commonly used metric in human motion prediction, which calculates the average Euclidean distance between the predic- tion and the ground truth of all joints. We use this metric on CMU-Mocap and MuPoTS-3D.\nMPJPE =\\frac{1}{P} \\frac{1}{N} \\frac{1}{J} \\sum_{p=1}^{P} \\sum_{n=1}^{N} \\sum_{j=1}^{J} \\gamma_{n j}^{p}\\left|\\left|\\widehat{v}_{n j}^{p}-v_{n j}^{p}\\right|\\right|_{2}"}, {"title": "E. Quantitative Results", "content": "1) Results on 3DPW and 3DPW-RC: For a fair comparison, we follow the same VIM criterion as established in previous works [60], [45], [47], [43], [42]. The experimental results in VIM on the 3DPW and 3DPW-RC datasets are shown in Table.I, where our method achieves the best performance on each dataset. Compared to the current state-of-the-art method Joint-Relation Transformer [45], our method reduces the VIM on AVG from 47.2 to 46.5 on 3DPW dataset and from 39.5 to 39.0 on 3DPW-RC dataset. These improve- ments demonstrate the effectiveness of our method. Traditional single-person motion prediction methods DViTA [60] and LTD [59] focuses on trajectory and ignores human interaction in multi-person motion prediction, so their experimental results are uncompetitive compared to subsequent methods. TRIPOD [42] employs GAT [27] to capture the connections between different targets. However, GAT is limited to capturing local information, as it performs attention operations only on adja- cent nodes, thus failing to capture global information. MRT [43] utilizes a multi-range Transformer to model interactions"}, {"title": "3) Effectiveness of IAM:", "content": "In order to verify the effectiveness of IAM, we designed an study on the 3DPW test set using DVITA [42], MRT [43], and JRT[45] as the base models. The experimental results are shown in Table. 8 and Fig. 9. In these presentations, \u201cTRIPOD+IAM\u201d and \u201cMRT+IAM\" refer to the DVITA [42], MRT [43] enhanced with our IAM module. In addition, we replace the fusion module of JRT[45] with our IAM, denotes as \"JRTIAM\". The results indicate that our IAM provides improvements to the base models, reducing the average error from 84.2 to 77.9 for TRIPOD, from 59.2 to 54.3 for MRT, and 47.2 to 46.8 for JRT. The results confirm that IAM effectively aggregates features from two different branches. These outcomes verify the strong ability of plug- and-play. Additionally, an ablation study was conducted to assess the impact of removing IAM from our method and the results demonstrate the importance of IAM."}, {"title": "F. Qualitative Results", "content": "1) Visualization of prediction result: We provide a quali- tative comparison on 3DPW-RC test set between our method and other recent methods, including MRT [43], FutureMotion [47] and JRT [45], as shown in Fig. 10. Compared with the prediction of FutureMotion [47], MRT [43], and JRT [45], our results are more natural and closer to the ground truth, particularly in the movement of the lower limbs. We also provide the visualization results on CMU-Mocap dataset to verify our method's effectiveness on scenes of 3 persons as shown in Fig. 11. We can notice that both LTD [59] and MRT [43] generate unnatural arm distortions that do not appear in our approach, as shown in the red circles. The visualization results on different datasets demonstrate the generalization and accuracy of our method.\n2) Visualization of attention scores: We visualize the learned attention matrices in the first layer to demonstrate the effectiveness of the intra-relation learning and inter-interaction learning, as shown in Fig. 12. We observe that in Fig. 12(a) the values between\" left foot and right foot\" for person 1 and \"right foot and knee\" for person 2 are significantly"}, {"title": "G. Ablation Study", "content": "In this section, we present the model's complexity and the number of parameters. We also conduct the ablation study on the framework without velocity augment, without intra-relation learning, and without inter-relation learning. The results are shown in Table. IV, Table. V, and Table. VI.\nEffectiveness of velocity augment We remove the input of velocity augment and utilize the original position coordinates as input to verify its validity. The experimental results in Table. V demonstrates that the introduction of velocity information has advantages in modeling the joint-level motion dynamics.\nEffectiveness of intra/inter-relation learning We compare our model with other three settings to demonstrate the effec- tiveness of intra-relation learning and inter-relation learning: i) We only perform inter-relation learning between different individuals (\u201cw/o intra-relation\") and remove intra-relation learning; ii) We only capture the individual's intra-relation (\u201cw/o inter-relation\") and remove inter-relation learning; iii) We remove the inter/intra-relation learning and adopt a 4- layer Transformer decoder (\u201cw/o intra&inter-relations\u201d). It is obvious that the inter-relation learning and the intra-relation learning are both contribute to the improvement of motion prediction performance.\nEffectiveness of loss function. In this section, we perform extensive ablation studies on the 3DPW-RC datasets to inves- tigate the contribution of the different loss functions; see Table"}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a novel framework for multi- person motion prediction. It aims to avoid undesired and unnatural dependencies between persons by explicitly mod- eling intra-relations and inter-relations. In addition, we pro- pose a novel plug-and-play aggregation module called the Interaction Aggregation Module (IAM). This module employs an aggregate-attention mechanism to seamlessly inte- grate intra&inter-relations. Our experiments demonstrate the strong plug-and-play capability of the IAM, and our framework's ability to accurately and naturally predict multi-person 3D motion across the 3DPW, 3DPW-RC, CMU-Mocap, MuPoTS- 3D, and synthesized datasets Mix1&Mix2 (9~15 persons).\nNevertheless, our method still have some limitations. Our method mainly focuses on human-to-human interactions in the scene and lacks consideration of the impact of the en- vironments and objects, which is inconsistent with the factors that influence a person's actions in real situations. In future work, we plan to consider the environment and objects to better comply with real-world."}]}