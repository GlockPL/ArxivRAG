{"title": "A Novel Adaptive Fine-Tuning Algorithm for Multimodal Models: Self-Optimizing Classification and Selection of High-Quality Datasets in Remote Sensing", "authors": ["Yi Ren", "Tianyi Zhang", "Zhixiong Han", "Weibin Li", "Zhiyang Wang", "Wenbo Ji", "Chenhao Qin", "Chenbin Liang", "Licheng Jiao"], "abstract": "General multimodal large models pre-trained with large-scale data can effectively perform domain-specific downstream tasks in the computer vision field, but they cannot be seamlessly scalable to remote sensing due to the modal disparity. However, multimodal large models tailored to remote sensing are still under-explored, and the main challenge is how to obtain optimal performance without substantially increasing computational overheads. Therefore, we propose an adaptive fine-tuning algorithm for multimodal large models. The core steps of this algorithm involve two stages of truncation. First, the vast amount of data is projected into a semantic vector space, and the MiniBatchKMeans algorithm is used for automated clustering. This classification ensures that the data within each cluster exhibit high semantic similarity. Next, we process the data in each cluster, calculating the translational difference between the original and perturbed data in the multimodal large model's vector space. This difference serves as a generalization metric for the data. Based on this metric, we select the data with high generalization potential for training. We applied this algorithm to train the InternLM-XComposer2-VL-7B model on two 3090 GPUs using one-third of the GeoChat multimodal remote sensing dataset. The results demonstrate that our algorithm outperforms the state-of-the-art baselines. various baselines. The model trained on our optimally chosen one-third dataset, based on experimental validation, exhibited only 1% reduction in performance across various remote sensing metrics compared to the model trained on the full dataset. This approach significantly preserved general-purpose capabilities while reducing training time by 68.2%. Furthermore, the model achieved scores of 89.86 and 77.19 on the UCMerced and AID evaluation datasets, respectively, surpassing the GeoChat dataset by 5.43 and 5.16 points. It only showed a 0.91-point average decrease on the LRBEN evaluation dataset. Our code is open-sourced and available at (https://github.com/renllll/).", "sections": [{"title": "I. INTRODUCTION", "content": "The emergence of large language models (LLMs) has brought significant advancements to the field of artificial intelligence, demonstrating remarkable capabilities across various natural language processing tasks. For instance, models like ChatGPT[1] and GPT-4[2] exhibit strong zero-shot and few-shot[3] learning abilities, which allow them to generalize well across many domains. However, when applied to specialized fields such as healthcare, law, and hydrology, these general-purpose models often experience performance degradation, since their insufficient training in domain-specific knowledge results in a lack of understanding of tasks within these specialized areas..\nTo address this issue, researchers have begun exploring specialized training and fine-tuning of LLMs for specific domains, and notable achievements have been made. For example, in the medical field[4-s], Google and DeepMind introduced Med-PaLM[5], a model designed for medical dialogue, which excels in tasks such as medical question answering, diagnostic advice, and patient education. Han et al. proposed MedAlpaca[6], a model fine-tuned on a large corpus of medical data based on Stanford Alpaca[7], aimed at serving medical question answering and consultation scenarios. Wang et al. developed BenTsao[8], which was fine-tuned using Chinese synthetic data generated from medical knowledge graphs and literature, providing accurate Chinese medical consultation services. In the legal field, Zhou et al. introduced LaWGPT[9], which was developed through secondary pre-training and instruction fine-tuning on large-scale Chinese legal corpora, enabling robust legal question answering capabilities. In the field of hydrology, Ren et al. proposed WaterGPT[10], a model based on Qwen-7B-Chat[11] and Qwen2-7B-Chat[12], which successfully achieved knowledge-based question answering and intelligent tool invocation within the hydrology domain through extensive secondary pre-training and instruction fine-tuning on domain-specific data.\nWith the success of LLMs in various fields, researchers have gradually started to explore the development of domain-specific multimodal models. For instance, in the medical field, Wang et al. introduced XrayGLM[13] to address challenges in interpreting various medical images. Li et al. proposed LLaVA-Med[14], aiming to build a large language and vision"}, {"title": "II. DATASET CREATION", "content": "A. Training Data\nThe RS multimodal instruction following dataset is a multimodal instruction-following dataset designed for remote sensing image understanding. It integrates various tasks such as image description, visual question answering, and visual dialogue, aiming to enhance the model's ability to handle complex reasoning, object attribute understanding, and spatial relationships. The dataset contains a total of 318,000 instruction pairs.\nB. Evaluation Datasets\nOur evaluation datasets include two parts: the remote sensing evaluation dataset and the general multimodal evaluation dataset.\n(1) Remote Sensing Evaluation Datasets:\nLRBEN (Land Use and Land Cover Remote Sensing Benchmark Dataset): This dataset is designed for land use and land cover classification tasks in remote sensing. It includes high-resolution images annotated for various types of land cover, such as urban areas, forests, water bodies, and agricultural fields. LRBEN is used to benchmark models' performance in visual question answering, scene classification, and other tasks in remote sensing.\nUC Merced Land Use Dataset: This dataset contains aerial imagery of various land use classes, such as agricultural, residential, and commercial areas. The images are high-resolution and cover 21 different classes, each with 100 images, making it suitable for scene classification tasks. It is widely used for evaluating remote sensing models' ability to classify and understand different land use types.\nAID (Aerial Image Dataset): AID is a large-scale dataset for aerial scene classification. It contains images from various scenes, such as industrial areas, residential areas, and transportation hubs. The dataset is designed to help in developing and benchmarking algorithms for scene classification, image retrieval, and other remote sensing tasks. AID includes a significant number of images for each category, providing a comprehensive benchmark for evaluating model performance.C. General Multimodal Evaluation Datasets:\nMMBench_DEV_EN: MMBench is a benchmark suite for evaluating the multimodal understanding capabilities of large vision-language models (LVLMs). It contains approximately 2974 multiple-choice questions covering 20 capability dimensions. Each question is single-choice, ensuring the reliability and reproducibility of the evaluation results. MMBench uses a strategy called cyclic evaluation to more reliably test the performance of vision-language models.\nMME (Multi-Modal Evaluation): MME is a comprehensive evaluation benchmark for large multimodal language models, aiming to systematically develop a holistic evaluation process. The MME dataset includes up to 30 of the latest multimodal large language models and consists of 14 sub-tasks to test the models' perceptual and cognitive abilities. The MME data annotations are all manually designed to avoid potential data leakage issues that might arise from using public datasets.\nSEEDBench IMG: SEEDBench is an image dataset specifically designed for training and evaluating multimodal models. It contains high-quality image data with detailed annotations, suitable for various multimodal tasks such as image classification, object detection, and scene understanding. The SEEDBench dataset aims to assist researchers in developing and optimizing multimodal models by providing a comprehensive benchmark."}, {"title": "III. METHODS", "content": "A. Adaptive Self-Tuning for Multimodal Models\nB. Selection of Generalizable Tasks\nThe autonomous selection of task instruction datasets with greater generalization has been a research hotspot. For instance, Sid-dhant and Lipton's work on uncertainty-based active learning [33] provides significant insights.\nInspired by these studies, we propose a new generalization measure: vector space translation difference. Since large models predict the next word based on context, changes in the context vector affect subsequent content generation. We evaluate the uncertainty of instructions by randomly deleting words from the instruction context as perturbation information and observing the degree of change in the model's vector space. Generally, entries with stronger uncertainty yield better generalization effects after training. Specifically, the vector space translation difference measures the translation difference in the vector space of the model's projection vectors when given complete and perturbed task instructions, assessing the generalization of the instruction. This quantifies the model's responsiveness to uncertain instructions, enabling better evaluation of the model's generalization performance.\nThe detailed flowchart is shown in Figure 4, and the specific steps are as follows:\n1. For the massive data pool X, we use the bge-large-en-v1.5[34] model to project each data entry into ector space, and then perform automated clustering using the MiniBatchKMeans algorithm. Specifically, we perform clustering calculations for different numbers of clusters using the MiniBatchKMeans algorithm, record the SSE (Sum of Squared Errors) and silhouette coefficient for each cluster number, and select the optimal number of clusters based on the highest silhouette coefficient. The data is eventually divided into p clusters.The specific steps are as follows:\n(1) Data projection onto vector space:\n$V_i = BGE(X_i)$\nHere, $X_i$ represents the ith data item in the data pool, and $V_i$ represents the vector representation projected through the bge-large-en-v1.5 model.\n(2) Calculation of the Sum of Squared Errors (SSE):\n$SSE = \\sum_{j=1}^{k} \\sum_{V_i \\in C_j} ||V_i - \\mu_j||^2$\nHere, k represents the number of clusters, Cj denotes the jth cluster, and uj is the centroid of the jth cluster. Vi represents the vector belonging to the jth cluster. The SSE measures the sum of the distances between data points and their respective cluster centroids, serving as one of the indicators to evaluate clustering performance. A smaller SSE indicates that the points within a cluster are more tightly grouped. By plotting the SSE values for different numbers of clusters p, one can preliminarily assess the reasonable range for the number of clusters.\n(3) Calculation of the Silhouette Coefficient:"}, {"title": "V. CONCLUSION", "content": "This study addresses the issue of data selection for multimodal large models in various domain tasks by proposing an adaptive fine-tuning algorithm. Most current research directly trains on large-scale multimodal data, which not only requires substantial computational resources but also results in significant performance degradation when randomly selecting a small subset of data. To resolve this, we first project the large-scale data into vector space and use the MiniBatchKMeans algorithm for automated clustering. Then, we measure the generalizability of the data by calculating the translation difference in the multimodal large model's vector space between the original and perturbed data, and autonomously select data with high generalizability for training.\nOur experiments, based on the InternLM-XComposer2-VL-7B model, were conducted on the remote sensing multimodal dataset proposed by GeoChat. The results show that using the adaptive fine-tuning algorithm, our method outperforms the random sampling and KCenterGreedy clustering algorithms in training with a 5,000-entry dataset, achieving the best domain and general performance with a 10,000-entry dataset. Ultimately, using only 105,000 data entries-one-third of the GeoChat dataset and training on a single 3090 GPU, our model achieved performances of 89.86 on the UC Merced dataset and 77.19 on the AID dataset, which are 5.43 and 5.16 points higher than GeoChat, respectively. On the LRBEN evaluation dataset, our model was only 0.91 points lower on average. Furthermore, comparing the performance of models trained on the full dataset versus our one-third dataset, we found that our approach reduced training time by more than 68.2% while maintaining general-domain capabilities with only a 1% average decrease in remote sensing accuracy.\nIn summary, our adaptive fine-tuning algorithm effectively selects high-quality data, enhancing model performance in specific domains while maintaining general performance under limited computational resources. This algorithm has significant practical value for training multimodal large models, especially in scenarios with constrained computational resources."}]}