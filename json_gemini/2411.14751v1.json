{"title": "TOPOSD: TOPOLOGY-ENHANCED LANE SEGMENT PERCEPTION WITH SDMAP PRIOR", "authors": ["Sen Yang", "Minyue Jiang", "Ziwei Fan", "Xiaolu Xie", "Xiao Tan", "Yingying Li", "Errui Ding", "Liang Wang", "Jingdong Wang"], "abstract": "Recent advances in autonomous driving systems have shifted towards reducing reliance on high-definition maps (HDMaps) due to the huge costs of annotation and maintenance. Instead, researchers are focusing on online vectorized HDMap construction using on-board sensors. However, sensor-only approaches still face challenges in long-range perception due to the restricted views imposed by the mounting angles of onboard cameras, just as human drivers also rely on bird's-eye-view navigation maps for a comprehensive understanding of road structures. To address these issues, we propose to train the perception model to \"see\" standard definition maps (SDMaps). We encode SDMap elements into neural spatial map representations and instance tokens, and then incorporate such complementary features as prior information to improve the bird's eye view (BEV) feature for lane geometry and topology decoding. Based on the lane segment representation framework, the model simultaneously predicts lanes, centrelines and their topology. To further enhance the ability of geometry prediction and topology reasoning, we also use a topology-guided decoder to refine the predictions by exploiting the mutual relationships between topological and geometric features. We perform extensive experiments on OpenLane-V2 datasets to validate the proposed method. The results show that our model outperforms state-of-the-art methods by a large margin, with gains of +6.7 and +9.1 on the mAP and topology metrics. Our analysis also reveals that models trained with SDMap noise augmentation exhibit enhanced robustness.", "sections": [{"title": "1 Introduction", "content": "Autonomous driving has witnessed remarkable advancements in recent years, becoming increasingly integral to the future of transportation. As a crucial component, perceiving the complex road scenarios to estimate the lane geometry and road topological connections is critical not only to the downstream planning, but also to ensuring the reliability and explainability of the overall system. As the foundational infrastructure for autonomous driving, high-definition maps (HDMaps) can provide a detailed and accurate source of road structures and geometries. Nevertheless, the annotation and maintenance costs of HDMaps are substantial, which poses limitations on their scalability across widespread areas. To alleviate these issues, recent researches such as (Li et al., 2022a; Liu et al., 2023; Liao et al., 2022, 2023b; Ding et al., 2023) are exploring how to construct online HD maps using onboard sensor input powered by deep learning models. However, relying solely on onboard sensors to accurately recognize lane-level geometry and topology remains challenging in real-world environments. They may produce low-quality lane lines or erroneous topology connections due to constrained camera views and limited visual ranges, and the situations are particularly exacerbated during severe weather conditions or occlusion."}, {"title": "2 Related work", "content": "Lane detection & Online HD Map construction Lane detection is the common task of detecting lane elements in the road scenes. Many of them focus on single-view lane recognition and they can be classified into segmentation-based (Pan et al., 2018; Abualsaud et al., 2021), anchor-based (Tabelini et al., 2021; Qin et al., 2020; Xiao et al., 2023) and keypoint-based (Ko et al., 2021; Wang et al., 2022) methods. Recently, great progress has been made in the field of online HD map construction. BEV-LaneDet (Wang et al., 2023c) and HDMapNet (Li et al., 2022a) adopt a typical rasterized map representations, which outputs segmentation results and embeddings for clustering. However, such methods need extra post-processing to generate maps for downstream planning modules. In contrast, vectorized map representation induces an end-to-end learning paradigm, with various methods (Liu et al., 2023; Liao et al., 2022, 2023b; Ding et al., 2023; Zhang et al., 2024; Qiao et al., 2023a,b) having been proposed. MapTR (Liao et al., 2022) and MapTRv2 (Liao et al., 2023b) employ point-level queries for each lane instance and an end-to-end learning paradigm, effectively enhancing the perceptual accuracy of vectorized maps. PivotNet (Ding et al., 2023) proposes a compact pivot-based map representation and attempts to model the topology in dynamic point sequences by introducing the concept of sequence matching. GeMap (Zhang et al., 2023) proposes to learn Euclidean shapes and relations of map instances beyond basic perception. MapTracker (Chen et al., 2024) formulates the map construction as a tracking task, uses the memory buffer to ensure consistent reconstructions over time and augments the mAP metrics with consistency checks.\nLane Topology Reasoning. Lane topology reasoning is directly related to the detection of centerlines and their connectivity. STSU (Can et al., 2021) introduces a DETR-like network to detect centerlines, and uses a MLP to infer their connectivity to form a directed graph. CenterLineDet (Xu et al., 2023) regards centerlines as vertices in a graph and employs a model trained through imitation learning to update the topology. TopoMLP (Wu et al., 2023) uses two high-performance detectors and two MLP networks for lane detection and topology reasoning. LaneGAP (Liao et al., 2023a) uses a path-wise approach to translate the lane graph into continuous and complete paths and a heuristic-based algorithm to recover the lane graph. TopoNet (Li et al., 2023) explicitly models the connectivity of centerlines and integrates traffic elements to learn a comprehensive understanding of the driving scene. LaneSegNet (Li et al., 2024) introduces a new representation of lane segments. It leverages both geometric and topological modeling, further enhancing the prediction ability of road structure. In this paper, we use the same representation of lane segment but introduce the SDMap information as prior and design a topology-guided decoder to further improve the accuracy of geometry and topology predictions.\nMap Fusion. Recent approaches make attempts to leverage some prior map for online HD mapping. Neural Map Prior (Xiong et al., 2023) builds a neural representation of global maps as a strong prior map, which are fused and updated when conducting local map inference. (Gao et al., 2023) proposes using satellite maps to complement onboard sensors to improve HD map construction. The satellite image features are fused into the BEV feature using a hierarchical fusion module. StreamMapNet (Yuan et al., 2024) fuses the temporal information from the memory feature updated by history frames to improve performance. MapEX (Sun et al., 2023) proposes to improve online HD construction using existing maps. It encodes the elements of HDMap into the map queries and leverage the decoder to utilize the existing map. There are some concurrent works incorperate SDMap as extra inforamtion to improve the onlien HD mapping. P-MapNet (Jiang et al., 2024) incorporates both SDMap and HDMap as prior to improve the model performance. It uses attention-based architecture to fuse the relevant SDMap skeletons for map construction and pre-trains a HDMap prior module to refine the map segmentation results. SMERF (Luo et al., 2023) integrates SD maps into online map construction. It encodes the class and coordinates of SDMap polylines into vectors using a Transformer encoder, and the map features are fused into the BEV feature using cross-attentions. The proposed map tokenization in this paper builds upon this method to encode a larger range of SDMap. Additionally, we propose a spatial representation encoding to enhance the geometric and topological attributes of SDMap."}, {"title": "3 Method", "content": "We aim to tackle the task of driving scene structure perception and reasoning, particularly focusing on the lane detection and the topology prediction. Built upon the lane segment based representation (Wang et al., 2023b; Li et al., 2024), we exploit standard-definition maps (SDMaps) as prior to enrich the perception information in BEV, as SDMaps can offer rough road geometry and topology information to generate map structure. To exploit the mutual relationships between the topological and geometrical feature, we employ a"}, {"title": "3.1 Lane segment perception task", "content": "In this task, a lane segment is a minimum unit to predict which contains a centerline, a left-boundary, and a right-boundary of a lane instance in form of polylines, denoted as $V = \\{v_c, v_l, v_r\\}$ respectively. For the left or the right boundary, the line type of them $\\{a_l, a_r\\}$ are defined within: non-visible, solid, and dashed. Besides, following LaneSegNet (Li et al., 2024), we convert the pedestrian crossing into the format of lane segment and exclude the prediction of road boundary.\nThe task of lane segment perception is not only to accurately detect the geometries of lane segment but is to generate the topological relationships between detected lane segments, i.e., the lane graph. This lane graph is represented as a directed graph $G = (V, E)$. Each lane segment V is denoted as a node in the set V, and the edges in set E represent connections between lane segments. Each edge signifies a directed connection between two lane segments that have preceding and succeeding relationships."}, {"title": "3.2 SDMap Encoding and Fusion", "content": "We use the SDMaps as the extra input, which conveys information about the road type, the road shape and topological connection. To make full use of them, we encode the map entities in the map into a representation that the neural network can learn from by using two distinct encoding methodologies as follows.\n1) spatial map encoding . This encoding method is to draw SDMap elements into 2D canvas maps. Assuming a 2D canvas is drawn according to the geometry and types of roads in the SDMap, the pixels in this canvas convey the SDMap information locally and the road structure in the bird's eye view can be expressed in the 2D maps. In light of this, the SDMap polyline elements are first encoded into different canvas maps. These maps are drawn with thick lines to describe the geometry, connections, shape, and types of the roads. And we employ cosines and sines of the inclination angle of the road line segments to express the curvature of the roads. And then these maps are processed by a CNN to achieve the SD feature $F_s \\in R^{d\\times h \\times w}$. Refer to Appendix A for more details on encoding.\n2) map tokenization. As the SD features only encode the SDMap information locally, we use another approach - map tokenization to encode the class and coordinate information in a global scope. Inspired by the polyline sequence representation in SMERF (Luo et al., 2023), we encode S polyline instances in SDMap as S token vectors, each of which is combined by a one-hot category vector representation with K dimensions and N point coordinate embeddings with c dimension. In other words, the dimension of each SD token vector"}, {"title": "SDMap Pre-fusion.", "content": "Given this new input modal, how and where to incorporate such SDMap information is critical to the model performance. Considering that SDMap only contains coarse road structure information, we advocate for introducing the SDMap to the model at an earlier stage of processing, rather than integrating it during the final lane prediction phase when the local information is much more crucial.\nTo this end, we propose to pre-fuse SDMap in the stage of constructing BEV feature and expect to reduce the possible negative interference when the inconsistency between sensor data and map occurs. We adopt a BEVFormer-based (Li et al., 2022b) encoder to generate the BEV feature. We add SD features $F_s$, to initial BEV queries $B_q \\in R^{D\\times h\\times w}$ and the output of the BEVformer Encoder. In the stage of BEV feature learning, the BEV queries can further aggregate image features from surrounding perspective-view images via a cross-attention mechanism. After this cross-attention, another cross-attention layer is appended to query the BEV feature with SD tokens $T_s$. The purpose of this design is to enable bev queries to select the most relevant tokens to fuse. Finally, the obtained SD-enhanced BEV feature $F_B \\in R^{D&h\\times w}$ are sent to the decoder for further processing. In experiments, we find all these designs are indispensable for achieving good performance."}, {"title": "3.3 Topology-Guided Decoder", "content": "Although this task of lane segment perception is to uniformly learn the geometry and topology of the road structure, the mutual influence of topology and geometry has not been fully explored in current approaches. In LaneSegNet (Li et al., 2024), the topology information is inferred using the final queries after the geometrical locations of centerlines have been predicted. However, this approach ignores the fact that topology information may affect the geometric position of the centerline. Intuitively, for two lanes with topological connections, their geometric endpoints are also connected with each other. If carefully designed, an approach should benefit from the relationship between lane topology and geometric layout. Therefore, we insert a topology-guided self-attention mechanism in each decoder layer, which allows the predicted topology information to influence the prediction of geometric information layer by layer, thus promoting mutual interaction between topology information and geometric positions.\nWe employ a deformable DETR (Zhu et al., 2020) style decoder to map the SDMap-enhanced BEV feature to final outputs through multiple heads. The learnable instance queries $Q \\in R^{N\\times D}$ represent lane segments. For the interactions with the BEV feature, we still keep the Lane Attention mechanism proposed in LaneSegNet to cross-attention with BEV feature $F_B$, obtaining its outputted instance queries.\nTopology-guided Self Attenion Mechanism. After the Lane Attention, we insert Topology-guided Self-Attenion. In Topology-guided Self-Attenion, a topology head is used to predict the topology adjacency matrix $M_{topo} \\in R^{N\\times N}$. Then we use this predicted topology matrix to fuse the geometrical information of the predecessor and the successor. More specifically, we leverage the adjacency matrix $M_{topo}$ to represent the topological connectivity. Each element in the matrix has a value between 0 and 1, a higher score representing a higher connectivity possibility. An element in the matrix $M_{topo}$ indexed with (i, j) represents the possibility of the endpoint of i-th lane segment connected with the start point of j-th lane segment. Assuming the feature outputted by the self-attention is $F$. By left-multiplying F with $M_{topo}$, we obtain the successor connection enhanced feature: $F_{succ} = M_{topo}F \\in R^{N\\times D}$. Similarly, left-multiplying the transpose of $M_{topo}$ with F yields the predecessor connection enhanced feature $F_{prede} = M_{topo}^TF$. We carry out these two operations right after the self-attention layer in the decoder. These three features $F$, $F_{succ}$, and $F_{prede}$ are concatenated with MLPs to form the final topology-enhanced feature:\n$F = MLP(Concatenate(F, MLP(F_{succ}), MLP(F_{prede})) \\in R^{N\\times D}.$   (1)\nAs a result, these enhanced features $F$ have incorporated the original self-attention information with successor and predecessor connection features, providing a more comprehensive representation of the interactions between different instances. We embed this topology-guided attention operation in each decoder layer. Through multiple decoder layers, the geometric information of lanes can be optimized by the topology matrix and the topology adjacent matrix in each decoder layer is predicted by the updated lane segment queries, thereby mutually enhancing the accuracy of both topology and geometric predictions.\nHeads. Like LaneSegNet, we adopt multiple MLP heads to decode the class, line types, centerline coordinates, and offsets from each instance query. The left and right boundary lines can be obtained by subtracting and adding the predicted offset to the predicted centerline, respectively: $\\hat{v}_l = \\hat{v}_c - \\hat{o}$, $\\hat{v}_r = \\hat{v}_c + \\hat{o}$. And the final output instance queries are sent to the topology head to predict the adjacency matrix. Due to the fact that the"}, {"title": "4 Experiments", "content": "Dataset. We conducted experimental validation on the subset A set of OpenLaneV2 Dataset (Wang et al., 2023a). OpenLaneV2 is a large-scale 3D lane dataset and comprises 1000 segments of various scenarios, including daytime, nighttime, sunny, rainy, urban, rural, and more. Each scenario lasts approximately 15 seconds, effectively providing feedback on the algorithm's efficacy. The annotations of lane segments and the perception range are within \u00b150m along the x-axis and \u00b125m along the y-axis. For the used SDMaps, we pre-process the original SDMap polylines to a large range within \u00b1100m along the x-axis and \u00b150m along the y-axis, the center of which is still aligned with the center of the perception range.\nMetric. As we mainly focus on the lane segmentation perception task, we report the results on the specifically designed metrics based on the lane segment distance $Dis$, following (Li et al., 2024). It induces the average precision APIs and APped to evaluate the accuracy of lane segments, and pedestrian crossings and the mean AP is computed as the average of APIs and APped. We use $TOP_{lsls}$ to evaluate the accuracy of topological connections between centerlines. See more information about the implementation details and metrics in Appendix C."}, {"title": "4.1 Comparison with state-of-the-art", "content": "Due to that the LaneSegNet is the first method performing on the lane segment perception benchmark, we mainly compare our models with it on the overall metrics and report the results of other HDMap constructing methods. As shown in Table 1, ours-1 model with SDMap pre-fusion substantially outperforms the LaneSegNet with +6.4 on the mAP and +6.6 on the $TOP_{lsls}$ metric. Such results demonstrate the SDMap can provide a strong prior to help generate the maps and improve the predictions on the geometry and topology of lane segments. Further enhanced by the Topology-Guided Decoder (TGD), our model achieves a new set of state-of-the-art performance with 40.2% on mAP and 34.5% on $TOP_{lsls}$, gaining obvious improvements with +6.7 on mAP and +9.1 on $TOP_{lsls}$ compared with LaneSegNet. To ensure a fair comparison with contemporary works, SMERF (Luo et al., 2023) and P-MapNet (Jiang et al., 2024), we integrated them with LaneSegNet. For the LaneSegNet model incorporating P-MapNet, we utilized our spatial encoded maps as SDMap inputs. The comparative results presented in Table 1 demonstrate that our proposed models (such as 'our-1') exhibit superior performance across multiple metrics.\nTo show the overall performances on the complex road scene perception and understanding, we train our model the map bucket with multiple tasks on OpenLaneV2 based on the lane segment representation. The pedestrian and road boundary are detected by an additional MapTR head (Liao et al., 2022). The traffic elements are detected by a Deformable DETR head (Zhu et al., 2020). The hyper-parameters are roughly set. As shown in Table 2, our model still surpasses the LaneSegNet model on all metrics."}, {"title": "4.2 Ablation study", "content": "In this section, we conduct ablations to validate the proposed SDMap encoding and fusion methods, as well as the Topology-guided decoder.\nAblations on SD encoding. Since we propose two types of SDMap encodings, we validate the effectiveness of two encoding methods respectively, as well as the effect of combining both SD encoding methods. As shown in Table 3, our findings indicate that both encoding methods independently bring significant gains, and their combination results in even higher gains. This means that two types of SD encoding methods can play different roles without conflicts at different levels, particularly for the map tokenization method that encodes a larger range of SDMap road polylines than the spatial map encoding.\nAblations on the fusion method. For the encoding of SD map tokenization, we use cross-attention layers in the BEV Encoder to fuse SD tokens with the BEV feature by default. But especially for the utilization of SD spatial map encodings, there are still multiple choices to fuse the SD feature.\nWe observe that fusing the SD feature into the BEV query (Exp-5) results in greater improvements in mAP (+4.8) and $TOP_{lsls}$ (+6.4) compared to fusing the SD feature (Exp-4) into the BEV feature, which showed improvements in mAP (+3.6) and $TOP_{lsls}$ (+5.1). Such results imply that incorporating 2D spatial SD structure information in the BEV query may provide a stronger prior and give more room for the BEV query to aggregate online visual information from cameras. And we find that adding the SD feature to both the BEV query and BEV feature still gains further improvements (Exp-6 in Table 3).\nAblation on Topology-guided decoder. Based on the SD fusion model, we validate the effectiveness of the topology-guided decoder. The results in Table 3 show that the topology-guided decoder can gain improvements of 0.8 and 2.5 on the APIs and $TOP_{lsls}$ metrics, which means that the geometry and topology lane segments are specifically optimized thanks to the topology enhanced decoder."}, {"title": "4.3 Study on the error problems of SDMap", "content": "In practical applications, SDMap errors are a crucial consideration, particularly concerning system localization and map annotation. These errors can arise from factors such as imprecise GPS signals and ambiguous road centerline positions in the forward direction. To simulate these errors in real-world scenarios, we conducted experiments involving the addition of random shifting and rotational noise during training and testing.\nAssuming the baseline SD model is trained using the original SDMap annotations, we train the same model with different SDMap noise injection by adding a random shifting sampled from a Gaussian distribution and a random rotation sampled from a uniform distribution. We set three variables: the standard deviation (std, with meter as its unit) of the Gaussian distribution for shifting noise and the maximum rotation angle (rot) for the random rotation, and the probability (prob) of whether to add random noise. We control these variables to combine several configurations such as rot5_std2_prob0.5."}, {"title": "4.4 Computational complexity and efficiency analysis", "content": "In Table 5, we report the inference speeds and model parameters. Our model utilizes a lightweight ResNet-18 (13M parameters) to extract SD features for the map spatial encoding component and directly add the SD feature to the BEV feature. The increased latency is primarily attributed to the CNN-based SDMap encoder. P-MapNet uses cross-attention to fuse the 2D-grid based SDMap feature with BEV queries, the complexity of which is proportional to $O(h_{bev} * W_{bev} * h_{SD} * W_{SD})$. If their resolutions are large, such as 200 \u00d7 100 in"}, {"title": "4.5 Qualitative Results", "content": "In Figure 4, we present a comparison between the predicted lane segment results of our proposed model and LaneSegNet. Overall, our model demonstrates superior accuracy in predicting lane geometry and topology. The predicted lane directions align closely with the SDMap road lines. However, LaneSegNet faces challenges in detecting key intersections and long-distance lane lines due to less prominent visual features. In contrast,"}, {"title": "5 Discussion", "content": "In this work we propose to incorporate SDMap information as prior to enhance the predictions of geometry and topology in the lane segment perception. We conduct two complementary methods to encode the geometry and topology information in SDMap and pre-fuse the SD feature and tokens into the BEV feature. To further explore the mutual relationships between the geometrical and topological features, we design a topology-guided decoder to iteratively optimize both geometry and topology. The experiments validate the effectiveness of two combined encoding methods and the proposed topology-guided decoder. We also study the effect of SDMap noise on the performance considering real-world practical applications. Our model achieves state-of-the-art performance on the OpenLaneV2 dataset.\nLimitation. While SDMaps offer valuable information regarding the geometry and topology of road structures, the information is currently restricted to the road level, lacking lane-level attributes. In addition, the discrepancy between SDMaps and the actual visual environment pose challenges for the perception model in practical applications. Moreover, SDMap may contain errors of several meters due to the positioning shifting and their inherent ambiguity. Future works should focus more on improving the quality of SDMaps and increasing the robustness of the model when the maps are inconsistent with real environments."}, {"title": "C Metrics", "content": "Following LaneSegNet Li et al. (2024), we use the defined lane segment distance to measure the average precision of the detected lane segments. The lane segment distance is defined as a weighted sum of distances between left/right lane boundaries and centerlines and their direction:\n$Dis (v, \\hat{v}) = 0.5 \\cdot [Chamfer ([\\hat{v}_l, \\hat{v}_r], [v_l, v_r]) + Frechet (\\hat{v}_c, v_c)].$  (3)\nBased on this distance metric, the average precision, $AP_{ls}$, is computed over three matching thresholds: 1.0m, 2.0m, 3.0m. The APped is based on the Chamfer distance to evaluate the non-directional pedestrian crossing, with thresholds of 0.5m, 1.0m, and 1.5m for evaluation.\nSimilar to $TOP_U$, $TOP_{lsls}$ represents the similarity between the predicted lane graph among lane segments and the ground truth. It is defined as the averaged vertice mAP between the ground truth $G = (V, E)$ and the predicted graph $(\\hat{V}, \\hat{E})$:\n$TOP = \\frac{1}{|V|} \\sum_{v \\in V} \\frac{1}{|N(v)|} \\sum_{\\hat{v}\\_i \\in \\hat{N}(v)} 1_{condition} (\\hat{n}' \\in N (v))$     (4)\nwhere N(v) denotes the ordered list of neighbors of vertex v in the ground truth ranked by confidence and $P(\\hat{v}_i)$ is the precision of the i-th vertex v in the predicted ordered list. The $TOP_{lsls}$ is for topology among lane segments on the graph $(V_{ls}, E_{lsls})$, while the $TOP_{lste}$ is for topology between lane segments and traffic elements on the graph $(V_{ls} \\cup V_{te}, E_{lste})$.\nBesides, we also report the results on the performances on the multiple tasks of OpenLaneV2 map element bucket in Table 2, with extra metrics of $DET_{t}$, and $TOP_{lste}$. The $DET_{t}$ is to evaluate the task of traffic element detection, which is based on IoU distance between the detected traffic element boxes and the ground truth boxes and is averaged over different traffic element attributes. The $TOP_{lste}$ is to evalaute the task of topology prediction between lane segments and traffic elements."}, {"title": "D More ablation results", "content": "Ablation on the topology head. We present the results of the ablations on the design of the topology head. LaneSegNet Li et al. (2024) firstly uses two MLPs to project the instance queries $Q \\in R^{N\\times D}$ to two embeddings $E_1 \\in R^{NxD_e}$ and $E_2 \\in R^{NX D_e}$, and then broadcast both embeddings to new shapes of $IR^{NXNxc}$. Finally, two embeddings with shape of $R^{N\\times N\\times D_e}$ are concatenated at the feature dimension to form a shape of $R^{N\\times N \\times 2*D_e}$ and sent to an association MLP to predict the adjacent matrix with shape of $R^{N\\times N \\times 1}$. In the implementation of the proposed connect head, we also use two MLPs to project the queries to two embeddings $E_s \\in R^{NX D_e}$ and $E_e \\in R^{NX D_e}$, but we simply compute their inner-products as the adjacent matrix among different lane segment instances. As shown in Table 6, with fewer parameters, the topology head via inner-product computing has achieved similar result w.r.t the mAP metric and better result w.r.t the topology metrics in comparison to the association MLP.\nAblation on the fusion position for SDMap. As SDMaps provide road-level rather than lane-level geometry and topology, there inevitably existing meter-level errors or inconsistent road description. Thus it is critical to"}, {"title": "E More visualization results", "content": "The Figure 7 show more visualization examples and the comparisons with LaneSegNet. See more examples in the supplementary materials. All the visualization results of LaneSegNet is based on the official release weight"}]}