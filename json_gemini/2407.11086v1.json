{"title": "Pre-training with Fractional Denoising to Enhance Molecular Property Prediction", "authors": ["Yuyan Ni", "Shikun Feng", "Xin Hong", "Yuancheng Sun", "Wei-Ying Ma", "Zhi-Ming Ma", "Qiwei Ye", "Yanyan Lan"], "abstract": "Deep learning methods have been considered promising for accelerating molecular screening in drug discovery and material design. Due to the limited availability of labelled data, various self-supervised molecular pre-training methods have been presented. While many existing methods utilize common pre-training tasks in computer vision (CV) and natural language processing (NLP), they often overlook the fundamental physical principles governing molecules. In contrast, applying denoising in pre-training can be interpreted as an equivalent force learning, but the limited noise distribution introduces bias into the molecular distribution. To address this issue, we introduce a molecular pre-training framework called fractional denoising (Frad), which decouples noise design from the constraints imposed by force learning equivalence. In this way, the noise becomes customizable, allowing for incorporating chemical priors to significantly improve molecular distribution modeling. Experiments demonstrate that our framework consistently outperforms existing methods, establishing state-of-the-art results across force prediction, quantum chemical properties, and binding affinity tasks. The refined noise design enhances force accuracy and sampling coverage, which contribute to the creation of physically consistent molecular representations, ultimately leading to superior predictive performance.", "sections": [{"title": "1 Introduction", "content": "Molecular Property Prediction (MPP) is a critical task for various domains such as drug discovery and material design [1-5]. Traditional approaches, including first-principle calculations and wet-lab experiments, require a huge cost [6, 7], thus prohibiting high-throughput screening of the molecules with desirable properties. Therefore, deep learning methods have been considered a promising way to reduce the cost and substantially accelerate the screening process [8, 9].\nThe main difficulty faced by deep learning MPP methods is the scarcity of labeled molecular data. To alleviate the difficulty, various self-supervised molecular pre-training methods have been proposed to exploit intrinsic information in unlabeled molecular data. Existing pre-training strategies are largely inspired by computer vision (CV) [10, 11] and natural language processing (NLP) [12, 13] techniques, such as contrastive learning and masking. Contrastive methods aim to maximize the representation agreement of augmentations derived from the same molecule and minimizing the representation agree- ment of different molecules [14-20], while masking, on the other hand, leverages the model to recover the complete molecule from a masked molecular string [21, 22], graph [23, 24] or structure [25].\nUnfortunately, directly borrowing the prevalent pre-training tasks in CV and NLP can be unsuit- able for molecules, as they neglect the underlying chemical characteristics and physical principles of molecules. For instance, graph-level augmentations for contrastive learning, such as edge perturbation, and subgraph extraction, can significantly alter molecular properties, leading to ineffective represen- tations for property prediction. As for masking, recovering masked atom type can be trivial when 3D coordinates are known [26]. Therefore, it is crucial to incorporate chemical priors and the laws of physics in AI for scientific discovery [27], to design suitable pre-training methods for molecules and strengthen generalization and robustness of the molecular representations.\nRecent advances of denoising methods [28-32] have introduced a physically interpretable pre-training task, which is equivalent to learning approximate atomic forces of sampled noisy conformations. In these methods, equilibrium conformations are initially perturbed by noise, and neural networks are then trained to predict the noise based on the noisy conformation. The noise type in the previous denoising framework was restricted to set as coordinate Gaussian noise (CGN) with isotropic noise variance, to maintain the force learning interpretation. However, the use of isotropic CGN noise leads to a biased molecular distribution, focusing on isotropic vibrations around equilibrium positions, since molecules exhibit not only small-scale vibrations but also rotation along rotatable single bonds on a relatively large scale.\nThus, the subsequent challenge lies in effectively modeling the comprehensive molecular distribu- tion, while simultaneously preserving the essential physical interpretation of force learning. Given the difficulty in modeling the true molecular distribution, we choose to characterize the distribution more comprehensively by introducing chemical priors about molecular distribution into noise design, which is prohibited in previous methods due to the restricted noise distribution. Therefore, we propose a novel molecular pre-training framework called fractional denoising (Frad), which is proven to hold the force learning interpretation. Specifically, given an equilibrium molecular conformation, a hybrid noise of chemical-aware noise (CAN) and CGN is added and a noisy conformation is obtained, the model is trained to predict CGN from the noisy conformation. The term \"fractional\" refers to recovering a fraction of the entire noise introduced.\nTo test how well Frad tackles the challenge, we carry out extensive experiments, validating superior performance over existing denoising methods in the following aspects. Firstly, Frad allows for a more comprehensive exploration of the energy surface. Whereas Frad enables a broader exploration of the energy surface by a higher noise level on torsion angles of rotatable bonds. Empirical results show that Frad outperforms coordinate denoising (Coord) even with a perturbation scale 20 times larger. Secondly, Frad learns more accurate atomic forces. Finally, the generality of the Frad framework permits introducing various chemical priors by alternating CAN, thereby accommodating a wide range of molecular systems and benefiting different downstream tasks.\nTherefore, Frad is able to achieve 18 new state-of-the-art on a total of 21 quantum chemical property and binding affinity prediction tasks from MD17, MD22, ISO17, QM9, and Atom3D datasets."}, {"title": "2 Results", "content": ""}, {"title": "2.1 Frad framework", "content": "To achieve physical-consistent self-supervised pre-training, we propose a novel fractional denoising framework that is equivalent to learning approximate atomic forces in molecules. The paradigm is depicted in Figure 1.\nGiven an equilibrium molecular conformation $x_{eq}$, a hybrid of chemical-aware noise (CAN) and coordinate Gaussian noise (CGN) are added, where the equilibrium conformation refers to the structure at local minima of the potential energy surface of the molecule. Then the model is trained to predict CGN from the noisy conformation, namely fractional denoising, as it recovers a portion of the introduced noise. Concretely, an equivariant graph neural network (GNN) is utilized to extract features from the noisy conformation, then a noise prediction head predicts the CGN from the features. We employ TorchMD-NET [34] as the backbone model to obtain equivariant features from 3D molecular inputs.\nDuring fine-tuning, we initialize the equivariant GNN from the pre-training weights and subsequently employ a distinct property prediction head tailored for each individual downstream task. The pre- trained GNN weights along with the parameters in the prediction head continue to be updated under the supervision of downstream labels. More details about Frad are shown in \"Methods\". The entire pipeline is illustrated in Figure 1."}, {"title": "2.1.1 Atomic forces learning interpretation", "content": "We present a theorem establishing the equivalence between fractional denoising and the learning of atomic forces in molecules, thereby enhancing the interpretability of the denoising task. Unlike previous works, we seek to obtain the minimal conditions for this equivalence, affording greater flexibility in handling the noise distribution. Prior to presenting the theorem, we first introduce relevant notations and assumptions that help to theoretically formulate the denoising task.\nAssumption I (Boltzmann Distribution [35]). The probability that a conformation occurs relates to its energy in the following way:\n$p(x) \\propto exp(-\\frac{E(x)}{kT}),$\nwhere $x$ represents any conformation of the molecule, $E(x)$ denotes the potential energy function, $T$ signifies the temperature, and $k$ stands for the Boltzmann constant.\nWe consider conformations in a fixed temperature, i.e. $kT$ is a constant. As an immediate conse- quence, a corollary arises: the score function of the conformation distribution equals the molecular forces up to a constant factor, i.e. $\\nabla log p(x) = -\\nabla_{x}E(x)$, where the score function is the gradient of the log of the probability density function of a probability distribution, and the gradient of the potential energy with respect to atomic coordinates is referred to as atomic forces or force field in our context.\nThe pre-training dataset comprises equilibrium conformations of a large amount of molecules, where each conformation is represented by its atomic coordinates. For each molecule, we denote the equilibrium conformation as $x_{eq}$, the intermediate conformation after the introduction of CAN as $X_{med}$, and the final noisy conformation after adding the entire hybrid noise as $x_{fin}$. These variables describe the Cartesian coordinates of the conformations and are all real-valued vectors in $R^{3N}$, where $N$ is the number of atoms constituting the molecule.\nAssumption II (Mixture Model of Molecules). The molecular distribution is approximated by a mixture model:\n$P(x_{fin}) = \\sum_{i=1}^{n} P(x_{fin}|x_{eq,i})P(x_{eq,i}).$\nThe component distribution $p(x_{fin}|x_{eq,i})$ is the distribution of hybrid noise, while the pre-determined mixing probability $p(x_{eq,i})$ characterizes the distribution of the equilibrium conformation and can be approximated by the sample distribution of the pre-training dataset. $n$ denotes the number of equilibrium conformations.\nTherefore, we can sample a molecule by adding hybrid noise to the equilibrium conformations from the dataset. Also, in order to precisely model the molecular distribution, we should design the hybrid"}, {"title": "2.1.2 Chemical-aware noise design", "content": "To faithfully model the true molecular distribution in equation (2), ensuring realistic conformation sampling and precise force targets in equation (7b), we should meticulously design the hybrid noise to capture the true distribution landscape surrounding the equilibriums. Fortunately, theorem I imposes constraints solely on the coordinate Gaussian noise (CGN) to establish the equivalence, thereby affording flexibility to Chemical-Aware Noise (CAN) in describing the molecular distribution.\nOne basic requirement for the hybrid noise is that $x_{eq}$ is the local maximum value of $p(x_{fin})$, so that the equilibrium conformations are local minima of the potential energy surface. A simple approach to satisfy this requirement is to employ CGN only, with no CAN introduced: $p(x_{med}|X_{eq}) = I_{xmed=xeq}$, $I$ is an indicator function, and $p(x_{fin}|x_{med}) \\sim N(x_{med},\\tau^{2}I_{3N})$. By choosing a relatively small value for $\\tau$, the requirement can be satisfied. This is exactly the prevalent coordinate denoising method [28, 29, 32]. Nevertheless, coordinate Gaussian noise is suboptimal for approximating the molecular distribution, as it can solely capture small-scale vibrations and does not account for rotations, which is significant for sampling broader low-energy conformations in the actual molecular distributions.\nTo address this limitation, we incorporate CAN to capture the intricate characteristics of molecular distributions. Initially, we introduce rotation noise (RN) that perturbs torsion angles of rotatable bonds by Gaussian noise, with the probability distribution given by $p(\\psi_{med}|\\psi_{eq}) \\sim N(\\psi_{eq}, \\sigma^{2}I_{m})$, where $\\psi_{eq}, \\psi_{med} \\in [0, 2\\pi)^{m}$ are the torsion angles of rotatable bonds of $X_{eq}, X_{med}$ respectively. $m$ is the number of rotatable bonds in the molecule. The torsion angle refers to the angle formed by the intersection of two half-planes through two sets of three atoms, where two of the atoms are shared between the two sets. This model is termed Frad(RN).\nTo provide a more comprehensive description of anisotropic vibrations, we model the conformation changes by vibration on bond lengths, bond angles, and torsion angles. As a result, we propose vibration and rotation noise (VRN) that perturbs bond lengths, bond angles, torsion angles, including that of rotatable bonds by independent Gaussian noise. The respective probability distributions are specified as follows: $p(r_{med}|r_{eq}) \\sim N(r_{eq},\\sigma_{r}^{2}I_{m1}),p(\\theta_{med}|\\theta_{eq}) \\sim N(\\theta_{eq},\\sigma_{\\theta}^{2}I_{m2}),p(\\phi_{med}|\\phi_{eq}) \\sim N(\\Phi_{eq},\\sigma_{\\phi}^{2}I_{m3}),p(\\psi_{med}|\\Psi_{eq}) \\sim N(\\psi_{eq},\\sigma_{\\psi}^{2}I_{m})$, where $r, \\theta, \\phi, \\psi$ denote bond lengths, angles, torsion angles of non- rotatable bonds and rotatable bonds respectively, $m1, m2, m3, m$ are their numbers in the molecule."}, {"title": "2.2 Frad boosts the performances of property prediction", "content": "To assess the efficacy of Frad in predicting molecular properties, we conducted a series of challenging downstream tasks. These tasks encompass atom-level force prediction, molecule-level quantum chemical properties prediction, and protein-ligand complex-level binding affinity prediction. Our model is system- atically compared against established baselines, including pre-training approaches, as well as property prediction models without pre-training. In experimental results, we use the abbreviation \u2018Coord' to refer to the coordinate denoising pre-training method in Zaidi et al. [28], which shares the same backbone as our model. The data splitting methods adhere to standard practices in the literature, where MD17, MD22, and QM9 adopt uniformly random splittings while ISO17 and LBA utilize out-of-distribution splitting settings. More information regarding datasets and baseline models is provided in \"Method\"."}, {"title": "2.3 Frad is robust to inaccurate conformations", "content": "A large pre-training dataset with equilibrium conformations constitutes a fundamental prerequisite for effective 3D molecular pre-training. However, constructing such a dataset can be costly, as it typically requires the use of density functional theory (DFT) to calculate the equilibrium conformations. As a result, we turn to explore the sensitivity of the model to the accuracy of the pre-train data, evaluating whether Frad maintains efficacy when conformations are computed using fast yet less accurate methods.\nWe employ RDKit [45] for regenerating 3D conformers on the original PCQM4Mv2 pre-training dataset, which is less accurate but much faster than DFT. Subsequently, we perform pre-training on Frad($\\tau$ = 0.04, $\\sigma$ = 2) and Coord ($\\tau$ = 0.04) using this inaccurate dataset. The results show that pre-training on inaccurate conformations leads to larger mean absolute errors. However, denoising pre-training methods remain effective and outperform the model trained from scratch. In particular, Frad consistently outperforms Coord. Intriguingly, Frad trained with inaccurate conformations even surpasses Coord trained with accurate conformations. These findings verify that Frad is a highly effective pre-training model, even when using inaccurate conformations, making it possible to pre-train on a larger scale while using less accurate pre-training datasets."}, {"title": "2.4 Comparisons to coordinate denoising methods", "content": "As discussed in \"Introduction\", coordinate denoising methods face the challenge of biased molecular distribution modeling, leading to restricted sampling coverage and inaccurate force targets. In this section, we quantitatively validate that Frad can augment the sampling coverage as well as enhance force accuracy, thereby yielding superior downstream performance."}, {"title": "2.4.1 Frad achieves higher force accuracy", "content": "To evaluate the accuracy of the force targets in denoising pre-training, we quantify the precision by using the Pearson correlation coefficient $\\rho$ between the estimated forces and ground truth. The force target estimation method is elucidated in detail in Supplementary Information D. Cerror denotes the estimation error. The ground truth forces are established by leveraging a supervised force learning method SGDML [46]. For a fair comparison, we decouple the sampling and force calculation. The samples are drawn by perturbing the equilibrium conformation of molecule aspirin with noise setting ($\\tau$ = 0.04), ($\\sigma$ = 1, $\\tau$ = 0.04), ($\\sigma$ = 20, $\\tau$ = 0.04), representing samples from near to far from the equilibrium.\nPrimarily, across all sampling settings, a hybrid of rotation noise and coordinate noise consistently outperforms the exclusive use of coordinate noise in terms of force accuracy. Specifically, the config- uration of $\\sigma$ = 20,$\\tau$ = 0.04 demonstrates the optimal alignment with the ground truth force field. Considering $C_{error}$ tends to increase as $\\sigma$ becomes larger, we choose $\\sigma$ = 2, $\\tau$ = 0.04 as the noise scale of Frad(RN). Furthermore, the accuracy gap between the hybrid noise and the coordinate noise is par- ticularly evident when incorporating samples located farther from the equilibrium. As a consequence, the introduction of chemical-aware noise emerges as a pivotal strategy to enhance the accuracy of force learning targets."}, {"title": "2.4.2 Frad can sample farther from the equilibriums", "content": "To compare the sampling coverage of different noise types, we measure it by perturbation scale, defined as the mean absolute coordinate changes of all atoms after applying noise. The perturbation scales, along with corresponding downstream performances of Coord and Frad measured by MAE, are depicted in Extended Data Fig.3.\nThe findings are threefold. Firstly, the challenge of low sampling coverage is evident in Coord. Specifically, the downstream performance is sensitive to the variance of CGN, where $\\tau$ = 0.04 behaves best and both larger and smaller noise scales degenerate the performance significantly. This phenomenon can be attributed to larger noise scales yielding more irrational noisy samples, while smaller scales result in trivial denoising tasks. Such behavior aligns with the findings of Zaidi et al. [28], who identified $\\tau$ = 0.04 as the optimal hyperparameter. Secondly, rotation noise(RN) alleviates the low sampling coverage problem. Notably, the perturbation scale of RN can be increased significantly without losing competence. Even with $\\sigma$ = 20, a setting with a perturbation scale 20 times larger than that of the most effective Coord configuration ($\\tau$ = 0.04), Frad(RN) still outperforms Coord in all setups. Thirdly, the"}, {"title": "3 Conclusion", "content": "In this work, we present a novel molecular pre-training framework, namely Frad, to learn effective molecular representations. Specifically, we propose a fractional denoising method coupled with a hybrid noise strategy to guarantee a force learning interpretation and meanwhile enable flexible noise design. We incorporate chemical priors to design chemical-aware noise and achieve a more refined molecular distribution modeling. Thus Frad can sample farther low-energy conformations from the equilibrium and learn more accurate forces of the sampled structures. As a result, Frad outperforms pre-training and non-pre-training baselines on force prediction, quantum chemical property prediction, and bind- ing affinity prediction tasks. Besides, we showcase the robustness of Frad to inaccurate 3D data. We also validate that Frad surpasses coordinate denoising through improved force accuracy and enlarged sampling coverage.\nOur work offers several promising avenues for future exploration. Firstly, augmenting the volume of pre-training data has the potential to significantly enhance overall performance. The currently employed pre-training dataset is still much smaller than 2D and 1D molecular datasets due to the high cost of obtaining accurate molecular conformations. We anticipate more 3D molecular data to be available. Secondly, our current focus lies in property prediction using 3D inputs. By integrating with other pre- training methods, a model that can handle molecular tasks across data modalities may be produced. Ultimately, how to design chemical-aware noise for typical categories of molecules is worth investigation, such as nucleic acids, proteins and materials, so that Frad can be efficiently applied to a broader range of fields and expedite drug and material discovery. These advancements hold the potential to establish Frad as a strong molecular foundation model applicable to diverse molecular tasks. Such progress could catalyze breakthroughs in critical areas like drug discovery and material science."}, {"title": "4 Methods", "content": ""}, {"title": "4.1 Model details", "content": "In Frad pre-training, the equilibrium molecules extracted from the pre-training dataset are pre-processed by adding random hybrid noise. Then the noisy 3D molecules are encoded by a graph-based equivariant Transformer denoted as Encoder. A denoising head NoiseHead made up of MLPs is adopted to predict the coordinate Gaussian noise (CGN) noise from the encoded features. The pre-training objective is given by\n$L_{Frad} = E_{xeq}E_{p(x_{fin}|x_{med})p(x_{med}|xeq)p(xeq)}||NoiseHead(Encoder(x_{fin})) - (X_{fin} - X_{med})||^{2},$\nwhere $x_{eq}, X_{med}, X_{fin}$ are equilibrium conformations, intermediate and final noisy conformations respec- tively. During fine-tuning, the model is initialized from pre-training and property prediction heads PropHead specified for each downstream tasks are further optimized. The fine-tuning objective is given by\n$L_{FT} = E_{x}||PropHead(Encoder(x)) \u2013 Label(x)||^{2}.$\nAdditionally, during fine-tuning, we also utilize Noisy Nodes techniques to further improve the per- formance. We proposed \"Frad Noisy Nodes\" for tasks that are sensitive to input conformations such as MD17. Detailed algorithms and results of an ablation study are provided in Supplementary Information D.1 and C.4.\nFor the equivariant Transformer, it primarily follows the structure of TorchMD-NET [34]. A model illustration is exhibited in . We make some minor modifications to TorchMD-NET marked in dotted orange boxes in the figure: Firstly, to stabilize training, we add an additional normalization module in the residue updating, which is effective for both the QM9 and LBA tasks. Secondly, for the LBA task, due to the complexity of the input protein-ligand complex, we enhance the model's expressivity by incorporating angular information into the molecular geometry embedding used in the attention module.\nThe model contains an embedding layer and multiple update layers. In the embedding layer, atom types are encoded by an atom-intrinsic embedding and a neighborhood embedding. These embeddings"}, {"title": "4.2 Dataset", "content": "All datasets used in pre-training and fine-tuning are listed in Table 4.\nWe utilize PCQM4Mv2 [47] as the pre-training dataset. It contains 3.4 million organic molecules, with one equilibrium conformation and one label calculated by density functional theory (DFT). We do not use the label since our method is self-supervised.\nWe adopt three different kinds of downstream tasks. The splitting methods are different for different tasks to maintain chemical diversity or to test out of-distribution generalization. The splittings all adhere to standard practices in the literature to ensure fair comparisons.\nFor atom-wise force prediction, we utilized molecular dynamics trajectories dataset MD17 [36], MD22 [38] and ISO17 [37].\nMD17 is a dataset of molecular dynamics trajectories, containing 8 small organic molecules with conformations, total energy, and force labels computed by the electronic structure method. For each molecule, 150k to nearly 1M conformations are provided. For MD17 data splitting, the existing approaches all perform uniformly random splitting, but diverge on taking large (9500) or small (950 or 1000) size of training data. As the size of the training dataset affects the force prediction significantly, we perform Frad with both splitting for fair comparisons.\nCompared to MD17, MD22 presents new challenges with respect to system size, flexibility, and degree of nonlocality, containing molecular dynamics trajectories of proteins, carbohydrates, nucleic acids, and supramolecules. ISO17 consists of molecular dynamics trajectories of 129 isomers of"}, {"title": "A Supplementary theoretical analysis", "content": ""}, {"title": "A.1 Challenges of coordinate denoising", "content": "In this section, we first provide the proof of physical interpretation for coordinate denoising, and then discuss its challenge of sampling and force accuracy. To simplify notations, we substitute xf for $X_{fin}$ to denote the final noisy conformation.\nTheorem II (Learning Forces via Coordinate Denoising [28]). If the distribution of coordinate noise is isotropic Gaussian $p(x_{f}|x_{eq}) \\sim N(x_{eq}, \\tau^{2}I_{3N})$, then coordinate denoising is equivalent to learning the atomic forces that correspond to the approximate molecular distribution by Boltzmann Distribution.\n$E_{p(xf|xeq)p(xeq)}||GNN_{\\theta}(x_{f}) \u2013 (x_{f} - X_{eq})||^{2}$\n$\\simeq E_{p(xf)}||GNN_{\\theta}(x_{f}) - \\nabla_{xf}E(x_{f})||^{2},$\n(4a)\n(4b)\n$\\simeq$ denotes the equivalence as optimization objectives. (4a) is the optimization objective of coordinate denoising [28, 29, 32]. (4b) is the regression task of fitting an approximate force field.\nWe make some adjustments to the form of the proof provided by Zaidi et al. [28].\nProof. Define $GNN_{\\theta}(x_{f})$ as $\\frac{1}{\\tau^{2}} \\cdot GNN_{\\theta}(x_{f})$ and $GNN_{\\theta}(x_{f})$ as $kT \\cdot GNN_{\\theta}(x_{f})$.\n$LHS =E_{p(xf,xeq)}T \\frac{1}{\\tau^{4}}||GNN_{\\theta}(x_{f})-\\frac{1}{\\tau^{2}}(x_{f} -xeq))||^{2}$\n= $E_{p(xf,xeq)}T \\frac{1}{\\tau^{4}}||GNN_{\\theta}(x_{f}) \u2013 \\nabla_{xf} log p(x_{f}|x_{eq})||^{2}$\n$\\simeq E_{p(xf)}T \\frac{1}{\\tau^{4}}||GNN_{\\theta}(x_{f}) \u2013 \\nabla_{xf} logp(x_{f})||^{2} + T_{5}$\n= $E_{p(x_{f})} \\frac{T4}{(kT)^{2}}||GNN_{\\theta}(x_{f}) - (-\\frac{\\nabla_{xf}E(x)}{kT})||^{2}+T_{5}$\n= $E_{p(xf)} (\\frac{kT}{T})^{2}||GNN_{\\theta}(x_{f}) - (-\\nabla_{xf}E(x))||^{2} +T_{5} \\simeq RHS$\n(5a)\n(5b)\n(5c)\n(5d)\n(5e)\nThe equality between (5a) and (5b) uses the Gaussian distribution and the mixture model, i.e. $p(x_{f}, x_{eq}) = p(x_{f}|x_{eq})p(x_{eq})$. The next equality between (5b) and (5c) uses the result in [61], which is also provided in Proposition I. $T_{5}$ represents terms do not contain $\\theta$. The next equality between (5c) and (5d) applies the Boltzmann Distribution in Assumption I. The equivalence in (5e) holds because the constant factors $T_{5}$ and $\\frac{T4}{(kT)^{2}}$ do not affect the optimization for $\\theta$. Besides, $GNN_{\\theta_{1}}$ and $GNN_{\\theta_{2}}$ are up to a constant factor, which is viewed as equivalence in [28]. Another understanding is that the two GNNs learn the same force field label but up to unit conversion.\nFrom the proof, we obtain an equivalent learning target for coordinate denoising as follows.\n$L_{Denoising} \\simeq E_{p(xf)}||GNN_{\\theta}(x_{f}) - (-\\nabla_{xf} log p(x_{f}))||^{2},$\n(6)\nwhere $(-\\nabla_{xf}logp(x_{f})) = \\nabla_{xf}E(x_{f})$ indicate the force learning targets. In (6), the learning target is affected by the modeled molecular distribution $p(x_{f})$ in two aspects, i.e. sampling and force target. As a result, the molecular distribution described by isotropic Gaussian causes two challenges to coordinate denoising:\n*   Low Sampling Coverage. Existing coordinate denoising approaches often set a very small noise level to prevent the generation of distorted substructures such as distorted aromatic rings. Experi- mental observations, as reported by Zaidi et al. [28], indicate a significant drop in performance when the noise level is increased. A similar phenomenon is discussed in section 2.4.2. While the small noise level strategy effectively avoids rare and undesired noisy structures, it struggles to encompass preva- lent structures with low energy, leading to low efficiency of force learning. Consequently, current coordinate denoising methods are constrained in learning forces for common low-energy structures far from the provided equilibriums.\n*   Inaccurate Forces. Existing coordinate denoising methods assume noise with an isotropic covari- ance, implying a uniform slope of the energy function in all directions around a local minimum. However, the intrinsic nature of a molecule's energy function is anisotropic. As depicted in Figure la, in low energy conformations, some substructures are rigid while some single bonds can rotate in a large scale. Consequently, current methods deviate from the true energy landscape, resulting in inaccurate force learning targets."}, {"title": "A.2 Non-fractional denoising fails to hold the force learning interpretation", "content": "Frad is a distinctive denoising framework that solely denoises a part of the noise introduced, presenting a departure from conventional methods which seek to predict the entire noise introduced and recover the original conformations. While denoising the complete hybrid noise seems like a feasible strategy, our analysis in this section reveals that it does not preserve the force learning interpretation. This limitation highlights the necessity and significance of the Frad approach.\nIn establishing the equivalence in theorem I, a crucial step is that the conditional score function is proportional to the coordinate changes. It is a property naturally satisfied by CGN. To investigate whether denoising CAN or the whole hybrid noise is meaningful, we consider CAN in the asymptotic case, i.e. when the noise scale approaches zero, so that the conditional score function is approximately linearly related to the coordinate changes.\nSupplementary Table 5: The conformation distribution and con- ditional score function corresponding to various noise types. The covariances $\\Gamma_{1}(x_{eq}) = C(x_{eq})E C(x_{eq})^{T}$ and $\\Gamma_{2}(x_{eq}) = \\tau^{2}I_{3N} + C(x_{eq})EC(x_{eq})^{T}$ are dependent on the input structure $X_{eq}$, indicating denoising the complete hybrid noise or chemical-aware noise fails to hold the force learning interpreta- tion and highlights the necessity of Frad. Detailed forms of C and E are provided in Supplementary Information B.2.\nNoise Type  Noise Distribution  Conditional Score Function CGN  $p(x_{eq})$ XCGN $\u223c N (x_{eq}, \\tau^{2}I_{3N})$ $\\nabla log p(x_{eq})$ CAN  Xmed $\u223c N (x_{eq}, \\Gamma_{1}(x_{eq}))$ -$\\tau^{-2}(x_{CGN}-X_{eq})$ -$\\Gamma_{1}(x_{eq})^{-1}(x_{med} - X_{eq})$ Hybrid(CAN+CGN) Xfin $\u223c N (x_{eq}, \\Gamma_{2}(x_{eq}))$ -$\\Gamma_{2}(x_{eq})^{-1} (x_{fin} - X_{eq})$\nThe molecular conformation distributions and conditional score functions corresponding to different noise types are presented in Supplementary Table 5. In the asymptotic scenario, the conditional score function is linear to the coordinate changes. We find only in the case of CGN, the linear coefficient is a constant. This implies that denoising is equivalent to fitting the forces because a constant can be viewed as a unit convert and maintain the force learning interpretation [28]. In contrast, in the case of other noise types, the linear coefficient is dependent on the input equilibrium structure xeq. Consequently, denoising these types of noise is not equivalent to fitting the atomic forces. The proof for the results presented in the table is provided in Supplementary Information B.2."}, {"title": "B Theoretical proofs", "content": "To simplify notations, we substitute xf and xm for $X_{fin}$ and $X_{med}$, denoting the final noisy conformation and the intermediate conformation respectively in this section."}, {"title": "B.1 Proofs for Theorem I", "content": "Theorem I (Learning Forces via Fractional Denoising). If the distribution of hybrid noise satisfies $p(X_{fin}|x_{med}) \\sim N(x_{med},\\tau^{2}I_{3N})$ is a coordinate Gaussian noise (CGN), then fractional denoising is equivalent to learning the atomic forces that correspond to the approximate molecular distribution by Boltzmann Distribution.\n$E_{p(xf|xm)p(xm|xeq)p(xeq)}||GNN_{\\theta}(x_{f}) \u2013 (x_{f} - X_{m}) ||^{2}$  $\\simeq E_{p(xf)}||GNN_{\\theta}(x_{f}) \u2013 \\nabla_{xf}E(x_{f})||^{2},$\n(7a)\n(7b)\n$\\simeq$ denotes the equivalence as optimization objectives. (7a) is the optimization objective of Frad. (7b) is the regression task of fitting an approximate force field.\nProof. Define $GNN_{\\theta}(x_{f})$ as $\\frac{1}{\\tau^{2}} \\cdot GNN_{\\theta}(x_{f})$ and $GNN_{\\theta}(x_{f})$ as $kT \\cdot GNN_{\\theta}(x_{f})$.\n$LHS =E_{p(xf|xm)p(xm|xeq)p(xeq)}T \\frac{1}{\\tau^{4}}||GNN_{\\theta}(x_{f}) - \\frac{1}{\\tau^{2}}(x_{f} - x_{m})||^{2}$\n(8a)"}, {"title": "C Supplementary experiments", "content": "In this section, we augment our experiments to comprehensively validate our motivations, theories, method designs, and model robustness. To verify our motivation, we confirm in Section C.1 that learning the force field serves as an effective molecular pre-training target. Certain downstream results suggest that Frad has reached the ceiling of the force learning pre-training approach. For the validation of our theories and method designs, we confirm the theoretical analysis in Section A.2 within Section C.2.\nSubsequently, in Section C.3, we verify the flexibility of CAN design and propose that our CAN design may have already achieved optimal performance. Regarding the design of the downstream fine-tuning method, we validate the effectiveness of the Frad noisy nodes design in Section C.4. Lastly, in terms of model robustness, we demonstrate in Section C.5 that Frad exhibits effectiveness across different model architectures."}, {"title": "C.1 Learning force field help the downstream tasks", "content": "In this section, we conduct the force pre-training experiment to assess the potential enhancement of downstream performance through force learning. As obtaining the true force labels can be time- consuming, we randomly select 10,000 molecules with fewer than 30 atoms from the PCQM4Mv2 dataset and calculate their precise force labels using DFT. We then pre-train the model by predicting these force labels, followed by fine-tuning the model on three sub-tasks from the QM9 and MD17 datasets."}, {"title": "C.2 Comparing Frad and CAN denoising", "content": "Our theoretical analysis in A.2 and Supplementary Table 5 shows that denoising just Chemical Aware Noise (CAN) does not preserve the force learning interpretation, which may compromise the perfor- mance of themodel. To validate this conjecture, we conduct experiments to evaluate the performance of denoising just CAN. Specifically, we test two types of CAN: RN and VRN. The results are shown in Supplementary Table 7. The results indicate that pre-training with only VRN or only RN has no sig- nificant effect. The downstream results are similar to those obtained from training from scratch, with some cases even showing negative transfer. This suggests that direct denoising of CAN alone does not capture useful molecular knowledge and fractional denoising is necessary."}, {"title": "C.3 Replacing CAN with Conformation Generation", "content": "In our theory, CAN can be flexibly constructed, and its design goal is to make $p(x_{fin})$ more consistent with the molecular conformation distribution. In fact, a more accurate method is to use computational chemistry tools to sample molecular conformations. Theoretically, employing computational chemistry tools instead of CAN should yield a more accurate force field, potentially enhancing the effect of pre-training. This comparison can also help judge how much room for improvement in CAN designing under the Frad framework.\nTherefore, we conduct a comparative experiment between denoising using CAN and conformation generation. To keep data generation time manageable, we select a random subset of the pre-training data comprising 100,000 molecules and use the RDKit toolkit to generate conformations via the Distance Geometry method, followed by optimization with the Merck molecular force field (MMFF). Since Frad is pre-trained for 8 epochs, with Chemical Aware Noise (CAN) added to each molecule once per epoch, for fairness, we generate 8 conformations for each molecule using RDKit. During training, one conformation is used per epoch, with a total of 8 epochs trained.\nThe model that uses generated low-energy conformations and performs coordinate denoising is referred to as Frad(RDKit+Coord). For comparison, we also pre-trained Frad on the same subset of data, denoted as Frad(CAN+Coord). Their performance comparison is shown in Table Supplementary Table 8."}, {"title": "C.4 Effectiveness of Frad noisy nodes", "content": "Noisy Nodes [62] is a method for improving downstream task performance by denoising during the fine- tuning phase. It involves corrupting the input structure with coordinate noise and training the model to predict the downstream properties and the noise from the same noisy structure. We have included the pseudocode for Noisy Nodes in Algorithm 2. However, we found that it cannot converge on force prediction tasks in the MD17 dataset with Noisy Nodes.\nTo address this issue, we proposed a series of modifications to help us understand why the traditional Noisy Nodes failed and improve it. We use the same model pre-trained by Frad (RN, $\\sigma$ = 2, $t$ = 0.04) and fine-tune it on the Aspirin task in MD17 with distinct Noisy Nodes settings. The results are presented in Supplementary Table 10.\nFirstly, in the case of traditional Noisy Nodes (Setting 2), it fails to converge. Next, in Setting 3, we change the noise type into rotation noise and succeed in converging. We conjecture that this is because the task in MD17 is sensitive to the input conformation, whereas Noisy Nodes corrupt the input conformation by Coordinate noise leading to an erroneous mapping between inputs and property labels.\nThis is validated by Setting 3, where the rotation noise has less perturbation on the forces, allowing the convergence. Additionally, decoupling the input of denoising and downstream tasks (Setting 4) ensures an unperturbed input for downstream tasks, and fundamentally corrects the mapping, making it work effectively. Finally, further substitute the denoising task to be Frad (Setting 5) can further promotes the performance, indicating that learning a more accurate force field contributes to downstream tasks."}, {"title": "C.5 Frad with other Network Architecture", "content": "Many equivariant molecular networks can be used in Frad. To evaluate the robustness of our results to the architecture selection, we try our method on other commonly used equivariant networks, specif- ically EGNN and PaiNN. The results are shown in Supplementary Table 11 and Supplementary Table 12. The results show that Frad remains effective across different model structures, consistently and"}, {"title": "D Experimental settings", "content": ""}, {"title": "D.1 Pre-training and fine-tuning algorithms", "content": "We propose a physics-informed pre-training framework that encompasses a fractional denoising (Frad) pre-training method as well as fine-tuning methods that are compatible with denoising pre-training.\nIn Frad pre-training, it involves adding a customizable chemical-aware noise(CAN), a coordinate Gaussian noise (CGN), and denoising the CGN noise. Frad pre-training is showcased by pseudocode Algorithm 1.\nIn fine-tuning, we utilize Noisy Nodes techniques to further improve the performance. We apply traditional Noisy Nodes [62] for tasks that is not very sensitive to conformations such as QM9. The traditional Noisy Nodes incorporates an auxiliary loss for coordinate denoising in addition to the original property prediction objective, as demonstrated in Algorithm 2.\nAs for tasks that are very sensitive to conformations such as force prediction tasks, traditional Noisy Nodes fail to converge. This is because traditional Noisy Nodes have to corrupt the input conformation leading to an erroneous mapping between inputs and property labels. Therefore, we utilize our proposed \"Frad Noisy Nodes\" on the force prediction tasks such as MD17. Specifically, it firstly decouples the denoising task and the downstream task, and secondly substitutes Frad for the coordinate denoising. The algorithm is shown in Algorithm 3 with distinctions from traditional Noisy Nodes highlighted in the color blue. The ablation study in section C.4 verifies the effectiveness of Frad Noisy Nodes.\nThe noise application including searching all the rotatable single bonds, perturbing the torsion angles, bond angles, bond lengths, and atomic coordinates in the molecule can be efficiently com- pleted using RDKit, which is a fast cheminformatics tool [45, 63]. When searching rotatable bonds, the"}, {"title": "D.2 Hyperparameter settings", "content": "Hyperparameters for pre-training are listed in Supplementary Table 13. Details about Learning rate decay policy can be referred in https://hasty.ai/docs/mp-wiki/scheduler/reducelronplateau#strong- reducelronplateau-explained-strong. Hyperparameters for fine-tuning on MD17 are listed in Supple- mentary Table 14. We test our model in two ways of data splitting. Correspondingly, there are two batch sizes proportional to the training data size.\nHyperparameters for fine-tuning on MD22 and ISO17 are listed in Supplementary Table 15 and Supplementary Table 16, respectively. For MD22 and ISO17, we apply a customized learning rate scheduler with a linear warmup and patience steps in the first training epoch. We also provide different batch sizes and learning rates for the 7 tasks in MD22.\nHyperparameters for fine-tuning on QM9 are listed in Supplementary Table 17. The cosine cycle length is set to be 500000 for a, ZPVE, U\u2080, U, H, G and 300000 for other tasks for fully converge. Notice that because the performance of QM9 and MD17 is quite stable for random seed, we will not run cross-validation. This also follows the main literature [30, 48, 52, 53].\nHyperparameters for fine-tuning on LBA are listed in Supplementary Table 18."}, {"title": "E More related work", "content": ""}, {"title": "E.1 Molecular pre-training methods", "content": "Molecular pre-training has become a prevalent method for obtaining molecular representations due to the lack of downstream data. In general, molecular pre-training can be categorized by input data modality. Traditionally, emphasis has been placed on two primary modalities of input molecular data: 1D SMILES strings [21, 67-71] and 2D molecule graphs [14, 23, 24, 72-76]. Most of the methods draw"}, {"title": "E.2 Denoising and score matching", "content": "Using noise to improve the generalization ability of neural networks has a long history [81, 82]. Denoising autoencoders, as proposed by Vincent et al. [83, 84], introduce a denoising strategy to acquire robust and effective representations. In this context, denoising is interpreted as a method for learning the data manifold. In the domain of Graph Neural Networks (GNNs), researchers have demonstrated the performance improvement achieved through training with noisy inputs [62, 85-87]. Specifically, Noisy Nodes [62] incorporates denoising as an auxiliary loss to alleviate over-smoothing and enhance molecular property prediction.\nScore matching is an energy-based generative model to maximize the likelihood for unnormalized probability density models whose partition function is intractable. The connection between denoising and score matching is established when the noise is standard gaussian [61]. This is successfully applied in generative modeling [85, 88-90] and in energy-based modeling of molecules for pre-training [28]. While both generative models and forces learning interpretation for denoising draw upon the findings of [61], they differ in their assumptions and aims in practice."}]}