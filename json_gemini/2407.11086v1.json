{"title": "Pre-training with Fractional Denoising to Enhance Molecular Property Prediction", "authors": ["Yuyan Ni", "Shikun Feng", "Xin Hong", "Yuancheng Sun", "Wei-Ying Ma", "Zhi-Ming Ma", "Qiwei Ye", "Yanyan Lan"], "abstract": "Deep learning methods have been considered promising for accelerating molecular screening in drug discovery and material design. Due to the limited availability of labelled data, various self-supervised molecular pre-training methods have been presented. While many existing methods utilize common pre-training tasks in computer vision (CV) and natural language processing (NLP), they often overlook the fundamental physical principles governing molecules. In contrast, applying denoising in pre-training can be interpreted as an equivalent force learning, but the limited noise distribution introduces bias into the molecular distribution. To address this issue, we introduce a molecular pre-training framework called fractional denoising (Frad), which decouples noise design from the constraints imposed by force learning equivalence. In this way, the noise becomes customizable, allowing for incorporating chemical priors to significantly improve molecular distribution modeling. Experiments demonstrate that our framework consistently outperforms existing methods, establishing state-of-the-art results across force prediction, quantum chemical properties, and binding affinity tasks. The refined noise design enhances force accuracy and sampling coverage, which contribute to the creation of physically consistent molecular representations, ultimately leading to superior predictive performance.", "sections": [{"title": "1 Introduction", "content": "Molecular Property Prediction (MPP) is a critical task for various domains such as drug discovery and material design [1-5]. Traditional approaches, including first-principle calculations and wet-lab experiments, require a huge cost [6, 7], thus prohibiting high-throughput screening of the molecules with desirable properties. Therefore, deep learning methods have been considered a promising way to reduce the cost and substantially accelerate the screening process [8, 9].\nThe main difficulty faced by deep learning MPP methods is the scarcity of labeled molecular data. To alleviate the difficulty, various self-supervised molecular pre-training methods have been proposed to exploit intrinsic information in unlabeled molecular data. Existing pre-training strategies are largely inspired by computer vision (CV) [10, 11] and natural language processing (NLP) [12, 13] techniques, such as contrastive learning and masking. Contrastive methods aim to maximize the representation agreement of augmentations derived from the same molecule and minimizing the representation agree- ment of different molecules [14-20], while masking, on the other hand, leverages the model to recover the complete molecule from a masked molecular string [21, 22], graph [23, 24] or structure [25].\nUnfortunately, directly borrowing the prevalent pre-training tasks in CV and NLP can be unsuit- able for molecules, as they neglect the underlying chemical characteristics and physical principles of molecules. For instance, graph-level augmentations for contrastive learning, such as edge perturbation, and subgraph extraction, can significantly alter molecular properties, leading to ineffective represen- tations for property prediction. As for masking, recovering masked atom type can be trivial when 3D coordinates are known [26]. Therefore, it is crucial to incorporate chemical priors and the laws of physics in AI for scientific discovery [27], to design suitable pre-training methods for molecules and strengthen generalization and robustness of the molecular representations.\nRecent advances of denoising methods [28-32] have introduced a physically interpretable pre-training task, which is equivalent to learning approximate atomic forces of sampled noisy conformations. In these methods, equilibrium conformations are initially perturbed by noise, and neural networks are then trained to predict the noise based on the noisy conformation. The noise type in the previous denoising framework was restricted to set as coordinate Gaussian noise (CGN) with isotropic noise variance, to maintain the force learning interpretation. However, the use of isotropic CGN noise leads to a biased molecular distribution, focusing on isotropic vibrations around equilibrium positions, since molecules exhibit not only small-scale vibrations but also rotation along rotatable single bonds on a relatively large scale, as illustrated in Figure la. Modeling this biased molecular distribution leads to inaccuracies in force targets and constraining the sampling range around equilibriums, as indicated by our theoretical analysis in Supplementary Information A.1, and ultimately hinders the model's performance on downstream tasks.\nThus, the subsequent challenge lies in effectively modeling the comprehensive molecular distribu- tion, while simultaneously preserving the essential physical interpretation of force learning. Given the difficulty in modeling the true molecular distribution, we choose to characterize the distribution more comprehensively by introducing chemical priors about molecular distribution into noise design, which is prohibited in previous methods due to the restricted noise distribution. Therefore, we propose a novel molecular pre-training framework called fractional denoising (Frad), which is proven to hold the force learning interpretation. Specifically, given an equilibrium molecular conformation, a hybrid noise of chemical-aware noise (CAN) and CGN is added and a noisy conformation is obtained, the model is trained to predict CGN from the noisy conformation. The term \"fractional\" refers to recovering a fraction of the entire noise introduced, with the necessity of the design discussed in Supplemen- tary Information A.2. Notably, CAN is customizable enabling Frad to incorporate chemical priors to optimize molecular distribution modeling. Inspired by the chemical priors that describe molecular con- formational changes, we present two versions of CAN. Specifically, rotation noise (RN) is advocated to capture rotations of single bonds, while vibration and rotation noise (VRN) is put forward to reflect anisotropic vibrations.\nTo test how well Frad tackles the challenge, we carry out extensive experiments, validating superior performance over existing denoising methods in the following aspects. Firstly, Frad allows for a more comprehensive exploration of the energy surface. Previous methods are vulnerable to generating irra- tional substructures like distorted aromatic rings and thus need to set the noise level quite low. Whereas Frad enables a broader exploration of the energy surface by a higher noise level on torsion angles of rotatable bonds. Empirical results show that Frad outperforms coordinate denoising (Coord) even with a perturbation scale 20 times larger. Secondly, Frad learns more accurate atomic forces. As is proved in \"Result\", the equivalent force learning target is derived from the modeled molecular distribution by Boltzman distribution. Thus by improving molecular distribution modeling, the force targets align bet- ter with the true atomic forces than previous methods, which is also validated by experiments. Finally, the generality of the Frad framework permits introducing various chemical priors by alternating CAN, thereby accommodating a wide range of molecular systems and benefiting different downstream tasks. Therefore, Frad is able to achieve 18 new state-of-the-art on a total of 21 quantum chemical property and binding affinity prediction tasks from MD17, MD22, ISO17, QM9, and Atom3D datasets."}, {"title": "2 Results", "content": ""}, {"title": "2.1 Frad framework", "content": "To achieve physical-consistent self-supervised pre-training, we propose a novel fractional denoising framework that is equivalent to learning approximate atomic forces in molecules. The paradigm is depicted in Figure 1.\nGiven an equilibrium molecular conformation xeq, a hybrid of chemical-aware noise (CAN) and coordinate Gaussian noise (CGN) are added, where the equilibrium conformation refers to the structure at local minima of the potential energy surface of the molecule. Then the model is trained to predict CGN from the noisy conformation, namely fractional denoising, as it recovers a portion of the intro- duced noise. Concretely, an equivariant graph neural network (GNN) is utilized to extract features from the noisy conformation, then a noise prediction head predicts the CGN from the features. We employ TorchMD-NET [34] as the backbone model to obtain equivariant features from 3D molecular inputs, as shown in Figure 1. Notably, our theoretical analysis reveals that the task, irrespective of the distribu- tion of CAN, possesses a force learning interpretation, whereas the CAN distribution affects the force targets and sampling distribution. Therefore, we meticulously design CAN to align with true molecu- lar conformation distribution, resulting in more precise force targets and a wider sampling distribution when compared to existing denoising methods.\nDuring fine-tuning, we initialize the equivariant GNN from the pre-training weights and subsequently employ a distinct property prediction head tailored for each individual downstream task. The pre- trained GNN weights along with the parameters in the prediction head continue to be updated under the supervision of downstream labels. More details about Frad are shown in \"Methods\". The entire pipeline is illustrated in Figure 1."}, {"title": "2.1.1 Atomic forces learning interpretation", "content": "We present a theorem establishing the equivalence between fractional denoising and the learning of atomic forces in molecules, thereby enhancing the interpretability of the denoising task. Unlike previous works, we seek to obtain the minimal conditions for this equivalence, affording greater flexibility in handling the noise distribution. Prior to presenting the theorem, we first introduce relevant notations and assumptions that help to theoretically formulate the denoising task.\nAssumption I (Boltzmann Distribution [35]). The probability that a conformation occurs relates to its energy in the following way:\n$p(x) \\propto exp(-\\frac{E(x)}{kT}),$ (1)\nwhere x represents any conformation of the molecule, E(x) denotes the potential energy function, T signifies the temperature, and k stands for the Boltzmann constant.\nWe consider conformations in a fixed temperature, i.e. kT is a constant. As an immediate conse- quence, a corollary arises: the score function of the conformation distribution equals the molecular forces up to a constant factor, i.e. \u2207 log p(x) = \u2212\u2207xE(x), where the score function is the gradient of the log of the probability density function of a probability distribution, and the gradient of the potential energy with respect to atomic coordinates is referred to as atomic forces or force field in our context.\nThe pre-training dataset comprises equilibrium conformations of a large amount of molecules, where each conformation is represented by its atomic coordinates. For each molecule, we denote the equilibrium conformation as xeq, the intermediate conformation after the introduction of CAN as Xmed, and the final noisy conformation after adding the entire hybrid noise as xfin. These variables describe the Cartesian coordinates of the conformations and are all real-valued vectors in R3N, where N is the number of atoms constituting the molecule.\nAssumption II (Mixture Model of Molecules). The molecular distribution is approximated by a mixture model:\n$P(x_{fin}) = \\sum_{i=1}^{n} P(x_{fin}|x_{eq,i})P(x_{eq,i}).$ (2)\nThe component distribution p(xfin|xeq,i) is the distribution of hybrid noise, while the pre-determined mixing probability p(xeq,i) characterizes the distribution of the equilibrium conformation and can be approximated by the sample distribution of the pre-training dataset. n denotes the number of equilibrium conformations.\nTherefore, we can sample a molecule by adding hybrid noise to the equilibrium conformations from the dataset. Also, in order to precisely model the molecular distribution, we should design the hybrid"}, {"title": "2.1.2 Chemical-aware noise design", "content": "To faithfully model the true molecular distribution in equation (2), ensuring realistic conformation sampling and precise force targets in equation (7b), we should meticulously design the hybrid noise to capture the true distribution landscape surrounding the equilibriums. Fortunately, theorem I imposes constraints solely on the coordinate Gaussian noise (CGN) to establish the equivalence, thereby affording flexibility to Chemical-Aware Noise (CAN) in describing the molecular distribution.\nOne basic requirement for the hybrid noise is that xeq is the local maximum value of p(x fin), so that the equilibrium conformations are local minima of the potential energy surface. A simple approach to satisfy this requirement is to employ CGN only, with no CAN introduced: p(xmed|Xeq) = Ixmed=xeq, I is an indicator function, and p(xfin|xmed) \u223c N(xmed,T2I3N). By choosing a relatively small value for \u03c4, the requirement can be satisfied. This is exactly the prevalent coordinate denoising method [28, 29, 32]. Nevertheless, coordinate Gaussian noise is suboptimal for approximating the molecular distribution, as it can solely capture small-scale vibrations and does not account for rotations, which is significant for sampling broader low-energy conformations in the actual molecular distributions.\nTo address this limitation, we incorporate CAN to capture the intricate characteristics of molecular distributions. Initially, we introduce rotation noise (RN) that perturbs torsion angles of rotatable bonds by Gaussian noise, with the probability distribution given by p(\u03c8med|\u03c8eq) \u223c N(\u03c8eq, \u03c3\u00b2Im), where \u03c8eq, \u03c8med \u2208 [0, 2\u03c0)m are the torsion angles of rotatable bonds of xeq, xmed respectively. m is the number of rotatable bonds in the molecule. The torsion angle refers to the angle formed by the intersection of two half-planes through two sets of three atoms, where two of the atoms are shared between the two sets. This model is termed Frad(RN).\nTo provide a more comprehensive description of anisotropic vibrations, we model the conformation changes by vibration on bond lengths, bond angles, and torsion angles. As a result, we propose vibration and rotation noise (VRN) that perturbs bond lengths, bond angles, torsion angles, including that of rotatable bonds by independent Gaussian noise. The respective probability distributions are specified as follows: p(rmed|req) \u223c N(req, or\u00b2Im1), p(\u03b8med|\u03b8eq) \u223c N(\u03b8eq, \u03c3\u00b2Im2), p(\u03c6med|\u03c6eq) \u223c N(\u03c6eq, \u03c3\u00b2Im3), p(\u03c8med|\u03c8eq) \u223c N(\u03c8eq, \u03c3\u00b2Im), where r, \u03b8, \u03c6, \u03c8 denote bond lengths, angles, torsion angles of non-rotatable bonds and rotatable bonds respectively, m1, m2, m3, m are their numbers in the molecule."}, {"title": "2.2 Frad boosts the performances of property prediction", "content": "To assess the efficacy of Frad in predicting molecular properties, we conducted a series of challenging downstream tasks. These tasks encompass atom-level force prediction, molecule-level quantum chemical properties prediction, and protein-ligand complex-level binding affinity prediction. Our model is system- atically compared against established baselines, including pre-training approaches, as well as property prediction models without pre-training. In experimental results, we use the abbreviation \u2018Coord' to refer to the coordinate denoising pre-training method in Zaidi et al. [28], which shares the same backbone as our model. The data splitting methods adhere to standard practices in the literature, where MD17, MD22, and QM9 adopt uniformly random splittings while ISO17 and LBA utilize out-of-distribution splitting settings. More information regarding datasets and baseline models is provided in \"Method\"."}, {"title": "2.3 Frad is robust to inaccurate conformations", "content": "A large pre-training dataset with equilibrium conformations constitutes a fundamental prerequisite for effective 3D molecular pre-training. However, constructing such a dataset can be costly, as it typically requires the use of density functional theory (DFT) to calculate the equilibrium conformations. As a result, we turn to explore the sensitivity of the model to the accuracy of the pre-train data, evaluating whether Frad maintains efficacy when conformations are computed using fast yet less accurate methods.\nWe employ RDKit [45] for regenerating 3D conformers on the original PCQM4Mv2 pre-training dataset, which is less accurate but much faster than DFT. Subsequently, we perform pre-training on Frad(\u03c4 =\n0.04, \u03c3 = 2) and Coord (\u03c4 = 0.04) using this inaccurate dataset. Shown in Extended Data Fig.1 are their downstream performance in HOMO, LUMO, Gap prediction tasks on the QM9 dataset compared to the models trained on the original pre-train dataset.\nThe results show that pre-training on inaccurate conformations leads to larger mean absolute errors. However, denoising pre-training methods remain effective and outperform the model trained from scratch. In particular, Frad consistently outperforms Coord. Intriguingly, Frad trained with inaccurate conformations even surpasses Coord trained with accurate conformations. These findings verify that Frad is a highly effective pre-training model, even when using inaccurate conformations, making it possible to pre-train on a larger scale while using less accurate pre-training datasets."}, {"title": "2.4 Comparisons to coordinate denoising methods", "content": "As discussed in \"Introduction\", coordinate denoising methods face the challenge of biased molecular distribution modeling, leading to restricted sampling coverage and inaccurate force targets. In this section, we quantitatively validate that Frad can augment the sampling coverage as well as enhance force accuracy, thereby yielding superior downstream performance."}, {"title": "2.4.1 Frad achieves higher force accuracy", "content": "To evaluate the accuracy of the force targets in denoising pre-training, we quantify the precision by using the Pearson correlation coefficient \u03c1 between the estimated forces and ground truth. The force target estimation method is elucidated in detail in Supplementary Information D. Cerror denotes the estimation error. The ground truth forces are established by leveraging a supervised force learning method SGDML [46]. For a fair comparison, we decouple the sampling and force calculation. The samples are drawn by perturbing the equilibrium conformation of molecule aspirin with noise setting (\u03c4 = 0.04), (\u03c3 = 1, \u03c4 = 0.04), (\u03c3 = 20, \u03c4 = 0.04), representing samples from near to far from the equilibrium. The results are shown in Extended Data Fig.2.\nPrimarily, across all sampling settings, a hybrid of rotation noise and coordinate noise consistently outperforms the exclusive use of coordinate noise in terms of force accuracy. Specifically, the config- uration of \u03c3 = 20,\u03c4 = 0.04 demonstrates the optimal alignment with the ground truth force field. Considering Cerror tends to increase as o becomes larger, we choose \u03c3 = 2, \u03c4 = 0.04 as the noise scale of Frad(RN). Furthermore, the accuracy gap between the hybrid noise and the coordinate noise is par- ticularly evident when incorporating samples located farther from the equilibrium. As a consequence, the introduction of chemical-aware noise emerges as a pivotal strategy to enhance the accuracy of force learning targets."}, {"title": "2.4.2 Frad can sample farther from the equilibriums", "content": "To compare the sampling coverage of different noise types, we measure it by perturbation scale, defined as the mean absolute coordinate changes of all atoms after applying noise. The perturbation scales, along with corresponding downstream performances of Coord and Frad measured by MAE, are depicted in Extended Data Fig.3.\nThe findings are threefold. Firstly, the challenge of low sampling coverage is evident in Coord. Specifically, the downstream performance is sensitive to the variance of CGN, where \u0442 = 0.04 behaves best and both larger and smaller noise scales degenerate the performance significantly. This phenomenon can be attributed to larger noise scales yielding more irrational noisy samples, while smaller scales result in trivial denoising tasks. Such behavior aligns with the findings of Zaidi et al. [28], who identified T = 0.04 as the optimal hyperparameter. Secondly, rotation noise(RN) alleviates the low sampling coverage problem. Notably, the perturbation scale of RN can be increased significantly without losing competence. Even with \u03c3 = 20, a setting with a perturbation scale 20 times larger than that of the most effective Coord configuration (r = 0.04), Frad(RN) still outperforms Coord in all setups. Thirdly, the"}, {"title": "3 Conclusion", "content": "In this work, we present a novel molecular pre-training framework, namely Frad, to learn effective molecular representations. Specifically, we propose a fractional denoising method coupled with a hybrid noise strategy to guarantee a force learning interpretation and meanwhile enable flexible noise design. We incorporate chemical priors to design chemical-aware noise and achieve a more refined molecular distribution modeling. Thus Frad can sample farther low-energy conformations from the equilibrium and learn more accurate forces of the sampled structures. As a result, Frad outperforms pre-training and non-pre-training baselines on force prediction, quantum chemical property prediction, and bind- ing affinity prediction tasks. Besides, we showcase the robustness of Frad to inaccurate 3D data. We also validate that Frad surpasses coordinate denoising through improved force accuracy and enlarged sampling coverage.\nOur work offers several promising avenues for future exploration. Firstly, augmenting the volume of pre-training data has the potential to significantly enhance overall performance. The currently employed pre-training dataset is still much smaller than 2D and 1D molecular datasets due to the high cost of obtaining accurate molecular conformations. We anticipate more 3D molecular data to be available. Secondly, our current focus lies in property prediction using 3D inputs. By integrating with other pre- training methods, a model that can handle molecular tasks across data modalities may be produced. Ultimately, how to design chemical-aware noise for typical categories of molecules is worth investigation, such as nucleic acids, proteins and materials, so that Frad can be efficiently applied to a broader range of fields and expedite drug and material discovery. These advancements hold the potential to establish Frad as a strong molecular foundation model applicable to diverse molecular tasks. Such progress could catalyze breakthroughs in critical areas like drug discovery and material science."}, {"title": "4 Methods", "content": ""}, {"title": "4.1 Model details", "content": "In Frad pre-training, the equilibrium molecules extracted from the pre-training dataset are pre-processed by adding random hybrid noise. Then the noisy 3D molecules are encoded by a graph-based equivariant Transformer denoted as Encoder. A denoising head NoiseHead made up of MLPs is adopted to predict the coordinate Gaussian noise (CGN) noise from the encoded features. The pre-training objective is given by\n$L_{Frad} = E_{x_{eq}}E_{p(x_{fin}|x_{med})p(x_{med}|x_{eq})p(x_{eq})}||NoiseHead(Encoder(x_{fin})) - (x_{fin} - x_{med})||^{2},$ \nwhere xeq, xmed, x fin are equilibrium conformations, intermediate and final noisy conformations respec- tively. During fine-tuning, the model is initialized from pre-training and property prediction heads PropHead specified for each downstream tasks are further optimized. The fine-tuning objective is given by\n$L_{FT} = E_{x}||PropHead(Encoder(x)) \u2013 Label(x)||^{2}.$\nAdditionally, during fine-tuning, we also utilize Noisy Nodes techniques to further improve the per- formance. We proposed \"Frad Noisy Nodes\" for tasks that are sensitive to input conformations such as MD17. Detailed algorithms and results of an ablation study are provided in Supplementary Information D.1 and C.4.\nFor the equivariant Transformer, it primarily follows the structure of TorchMD-NET [34]. A model illustration is exhibited in ??. We make some minor modifications to TorchMD-NET marked in dotted orange boxes in the figure: Firstly, to stabilize training, we add an additional normalization module in the residue updating, which is effective for both the QM9 and LBA tasks. Secondly, for the LBA task, due to the complexity of the input protein-ligand complex, we enhance the model's expressivity by incorporating angular information into the molecular geometry embedding used in the attention module.\nThe model contains an embedding layer and multiple update layers. In the embedding layer, atom types are encoded by an atom-intrinsic embedding and a neighborhood embedding. These embeddings"}]}