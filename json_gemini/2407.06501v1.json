{"title": "STORYSUMM: Evaluating Faithfulness in Story Summarization", "authors": ["Melanie Subbiah", "Faisal Ladhak", "Griffin Adams", "Akankshya Mishra", "Lydia B. Chilton", "Kathleen McKeown"], "abstract": "Human evaluation has been the gold standard for checking faithfulness in abstractive summarization. However, with a challenging source domain like narrative, multiple annotators can agree a summary is faithful, while missing details that are obvious errors only once pointed out. We therefore introduce a new dataset, STORYSUMM, comprising LLM summaries of short stories with localized faithfulness labels and error explanations. This benchmark is for evaluation methods, testing whether a given method can detect challenging inconsistencies. Using this dataset, we first show that any one human annotation protocol is likely to miss inconsistencies, and we advocate for pursuing a range of methods when establishing ground truth for a summarization dataset. We finally test recent automatic metrics and find that none of them achieve more than 70% balanced accuracy on this task, demonstrating that it is a challenging benchmark for future work in faithfulness evaluation.", "sections": [{"title": "Introduction", "content": "As Large Language Models (LLMs) are able to perform more open generation tasks, challenges in evaluation have arisen (Gabriel et al., 2020). Summarization is one such task. Some aspects of summary quality like readability or coherence (Goyal et al., 2022; Chang et al., 2023) can be judged by looking at the summary alone. However, judging faithfulness (whether all details in the summary are faithful to the source) requires carefully checking a multi-sentence summary against a multi-paragraph source document (Krishna et al., 2023). Summaries that misrepresent source documents can easily spread disinformation, so it is critical we evaluate summary faithfulness, despite how labor-intensive it is.\nMethods for detecting inconsistencies have generally used one of two tools: 1) trained models, or 2) human crowdworkers. Model-based approaches typically build on QA or entailment strategies. QA strategies generate questions about the summary and compare answers retrieved from the summary vs. the source document (Durmus et al., 2020; Fabbri et al., 2021b). Entailment-based approaches align facts in the summary with evidence from the source and determine for each pair if the evidence entails the fact (Utama et al., 2022; Laban et al., 2022; Maynez et al., 2020). More recent work explores prompting strategies for LLMs to identify faithfulness errors (Min et al., 2023; Kim et al., 2024a; Si et al., 2023; Luo et al., 2023; Manakul et al., 2023).\nWith human annotators, prior work has shown that human judgments have increased variability when evaluating long summaries (Krishna et al., 2023). Reducing the problem to evaluating individual sentences or claims helps to produce more reliable results (Krishna et al., 2023; Ye et al., 2023; Min et al., 2023). However, these works have focused on factuality in news summaries or real-world articles where ground truth is based in reality and facts are stated explicitly.\nAs LLMs continue to grow in capabilities, there is a pressing need for evaluation of their accuracy to grow with them. We therefore produce a new benchmark, STORYSUMM, which can be used to improve evaluation methods for faithfulness. STORYSUMM consists of 96 short stories and LLM-generated summaries with localized faithfulness errors and explanations. Each unfaithful summary is labeled as easy or hard to detect.\nLLM summaries often contain subtle errors, particularly for narrative text which requires nuanced interpretation. This benchmark therefore introduces new challenges when compared to fact-checking or summarization datasets in the news domain.  The example in Figure 1 demonstrates that assessing the summary requires correct interpretation of sentences like, Her heart was mine., which have multiple meanings and are misleading without carefully reading the entire story. By focusing on faithfulness in narrative summarization and using real-world data from LLMs and Reddit, STORYSUMM poses a realistic but hard benchmark to push our methods forward.\nWe first explore how to establish ground-truth on this dataset by comparing different human annotation protocols and manually inspecting the results. We find that different protocols catch unique but legitimate inconsistencies and have only fair agreement with each other. We therefore manually review and merge label sets across three annotation protocols.\nWe analyze the errors found by each protocol, and formulate a set of recommendations for human evaluation of faithfulness in narrative summarization. Most importantly, we show that it is important to use a variety of annotators and protocols when establishing ground truth for faithfulness. We then explore how well recent automatic metrics perform on this dataset. We find that no metric achieves more than 70% balanced accuracy on this task and even the best metric misses almost 50% of the hard inconsistencies."}, {"title": "STORYSUMM Dataset", "content": "We design our benchmark with a focus on three principles which distinguish it from existing datasets. First, the stories need to be short enough that humans can easily read them, so that we can affordably test human protocols. Second, the stories should not be so famous that LLMs have likely trained on summaries of them, potentially biasing LLM summary or evaluation quality. Third, the summaries should be representative of powerful LLMs so that we can assess how difficult it is to find errors in fluent and convincing summaries.\nMotivated by these principles, we opt for short narratives from Reddit and use GPT-series and Claude-series models to generate summaries. We do not include any human-written summaries as the purpose of this dataset is to improve detection of errors in LLM-generated summaries. We show summary statistics for the dataset in Table 1 and full examples of stories/summaries in Appendix A."}, {"title": "Stories", "content": "We collect a dataset of 32 short stories from two popular subreddits where users can submit their original short stories for others to enjoy and comment on. We filter out posts that are marked NSFW (Not Safe For Work, meaning inappropriate content) and also posts that have fewer than three up-votes. The stories are typically less than one page long. We note that users do not write summaries for their stories, and since these stories are not popular, they're unlikely to be summarized elsewhere; therefore, there is little concern about data contamination for LLMs. Additionally, as LLMs are now being used to summarize lots of different data online, it is important to evaluate them on more colloquial narrative like this rather than just benchmarks of published/popular stories."}, {"title": "Summaries", "content": "For each story, we generate 3 different summaries using 3 different models, resulting in 96 story-summary pairs (see Appendix B for prompting details). Each summary is about a paragraph long. To simulate real evaluation conditions, we split the dataset into a validation split of 33 summaries which are generated by an older set of models (Davinci-3, ChatGPT, and Claude-2) and a test set of 66 summaries from newer models (GPT-3.5, GPT-4, Claude-3). This allows us to assess whether automatic metrics that require threshold tuning for classification can be tuned on a validation set of labeled summaries from older models and still work well as newer models are coming out. We use disjoint sets of 11 and 22 stories to generate the summaries for the validation and test sets respectively."}, {"title": "Labels", "content": "The question we ask annotators is: Is the information in the summary consistent with the story? We define a consistent summary as: The events and details described in the summary should not misrepresent details from the story or include details that are unsupported by the story. We ask you to ignore commentary in evaluating consistency. Commentary means sentences like, The story reflects the enduring bonds of friendship and the role of companionship during times of hardship., which interpret the story to find themes rather than just detail the plot.\nFor annotator recruitment, we first compare Amazon Mechanical Turk and Upwork, asking four annotators from each platform to assign a binary faithful/unfaithful label to each summary. We mark a summary as faithful if three or more annotators in a group label it as such. We find that MTurk workers label 97% of summaries faithful whereas Upwork workers label 64% as faithful. When the authors perform the same task, we find 45% of summaries faithful, so we conclude that Upwork workers are more astute at catching errors and we use them for the remainder of our experiments. We caution future work to avoid using MTurk for faithfulness evaluation as it will dramatically inflate performance. Marshall et al. (2023) also showed Mturk response quality has dramatically declined in the last decade and is now mostly unusable.\nTo establish gold labels, we build on Krishna et al. (2023), which shows that fine-grained evaluation encourages inter-annotator agreement. We recruit three annotators from Upwork who are fluent in English and successfully complete a pilot exercise shown in Appendix C. We then ask them to assign a binary faithfulness label to each sentence in a summary. When they mark a sentence as unfaithful, they also provide a brief written justification. Since annotators can see all of the sentences they are labeling in context, we do not take the additional step of generating atomic claims as proposed by Min et al. (2023). The full interface for experiments is shown in Appendix D. We pay each annotator $100 for annotation of all 96 summaries.\nIf two or more annotators mark the same sentence as unfaithful, we mark the whole summary as unfaithful. If all three annotators mark the same sentence as unfaithful, we label that unfaithful summary as easy to detect, whereas it is hard to detect if one annotator labels the sentence as faithful. These difficulty distinctions allow for more meaningful error analysis of different evaluation methods."}, {"title": "All that glitters is not gold...", "content": "Typically, annotator labels with almost perfect inter-annotator agreements like ours are just assumed to be ground truth. However, we hypothesize that errors in narrative summaries may be difficult to catch and the annotators likely missed some. Therefore, we compare our gold labels against other human evaluation protocols to gain a better sense of their quality. In addition to our gold labels, we compare the following two methods:\nExpert. Three of the authors review each summary and label it as faithful or unfaithful. We consider the authors \"expert\" annotators as they have experience in faithfulness research and are motivated to produce thoughtful labels (modeled after Kry\u015bci\u0144ski et al. (2019b) who also use the authors as expert annotators for factual consistency). The three experts adjudicate their labels by discussing any disagreements until all three agree on the label. This process is completed before the authors view any other labels for the dataset so they can remain unbiased. All three experts initially agreed on only 46% of the labels, demonstrating that even experts struggle to catch every error.\nHybrid. We have GPT-4 generate multiple possible inconsistencies between the summary and story (see example inconsistencies in Figure 2 and prompt in Appendix B). Three workers from Upwork read these inconsistencies before labeling the summary overall, and write a short response justifying why they agree or disagree with each inconsistency. We hypothesize that identifying specific inconsistencies workers miss is useful support an LLM can provide. Presenting multiple possible options from the LLM raises the chances of one of them being accurate."}, {"title": "Label Comparison", "content": "In Table 5, we show the agreement and accuracy of the expert and hybrid protocols relative to the gold labels. We can see that both have lower inter-annotator agreement (Fleiss-kappa 0.2-0.4), likely because annotations are done at the summary rather than sentence level (Krishna et al., 2023). Both methods detect 93% of the easy inconsistent summaries but a lower percentage of the hard summaries (52% for the experts and 76% for the hybrid method). The experts have higher balanced accuracy despite detecting a lower percentage of the hard inconsistencies because the hybrid method detects many inconsistencies and is less precise.\nBoth methods have only fair agreement with the gold labels (Cohen's kappa 0.2-0.4). We show the breakdown of label overlap in Figure 3. The counts where the gold labels say faithful and an alternate method says unfaithful suggest that the gold labels miss real inconsistencies (19 new unfaithful summaries detected by the experts and 36 by the hybrid method). Since the expert labels are adjudicated between the three authors, we are sure the 19 expert inconsistencies that the gold labels miss are correct. We can also see that even the experts miss inconsistencies as they miss 11 that are detected by the gold labels. Before accepting the hybrid inconsistencies, we need to check their quality in the next section since the hybrid method is a novel annotation protocol."}, {"title": "Expanded Labels", "content": "Since each human annotation method clearly detects different inconsistencies, we want to merge their labels to get better coverage of the errors. For the gold and expert labels, we take the union of their detected errors since these are established and trusted protocols. Therefore, a summary is labeled unfaithful if either the gold or expert labels find it to be unfaithful. We manually merge their written error explanations.\nFor the hybrid labels, we manually review and filter out illegitimate errors.  For example, Table 2 shows a case that annotators incorrectly label as an error. GPT-4's generated inconsistency #1 convinces annotators Hope is not \"a victim of\" the curse because technically Hope's father is the target of the curse (\"She cursed my father\"). However, Hope also suffers under the curse and says, \"[Margaret] cursed me to this life\", so she is also a victim of the curse and this is not a real inconsistency.\nIn this process, we create a new set of labels for the dataset that are an amalgamation of the gold, expert, and hybrid labels from the three human annotation methods, and we also provide a written description of the inconsistencies detected in each summary. We call these labels the 'expanded' set and update the easy/hard breakdown for these labels to be based on whether all three methods detect an unfaithful summary.\nIn Figure 4, we show the confusion matrices of each method with the expanded set of labels, demonstrating that each additional human annotation protocol adds new inconsistencies. Table 6 shows that 2 unfaithful summaries are detected only by the gold labels, 4 only by the expert labels, and 6 only by the hybrid method. Table 4 shows examples for these cases and we see that these are real errors even though they are easy to miss. For example, one of the errors detected only by the hybrid method is that the summary says the protagonist is rejected and insulted by the same man, but in the story one man rejects her and his husband insults her. The easy and hard examples shown exhibit a general pattern that easy errors tend to be about core story events, whereas hard errors are often about smaller details or subtle twists of meaning that are easy to mentally skip over (e.g., an incorrect pronoun)."}, {"title": "Recommendations", "content": "Through this error analysis, we form several recommendations for faithfulness human evaluation:\n1.) Use multiple protocols and sets of annotators for good coverage of errors; otherwise performance is most likely inflated. Using just the fine-grained annotation protocol with Upwork workers, we find only 2/3 of the errors in the expanded set. Protocols that localize and explain errors make it easier to check and merge error sets.\n2.) The quality of the annotator pool affects how many errors are found. In our case, MTurk workers find almost no errors, Upwork workers find more, and experts (who also have to discuss the labels with each other) find the most.\n3.) When precision matters, use a fine-grained annotation approach (by sentence or claim). Krishna et al. (2023) originally recommended this approach and our work supports it. We see almost perfect inter-annotator agreement for the line-by-line approach and the errors detected are legitimate.\n4.) When coverage matters, include a high-coverage protocol such as our hybrid method. The hybrid method finds the most errors, but some of these are not real errors as annotators are highly influenced by the model suggestions. Using a high-coverage method requires an additional filtering step for legitimate errors but finds errors not found by other protocols.\nPrior work (Falke et al., 2019; Gillick and Liu, 2010) has also advocated for expert involvement by showing typical annotation settings do not match expert labels. Our work additionally shows that expert labels may be missing inconsistencies as well and uses expert review to merge annotation sets. We recommend future work use the expanded labels, but include the gold labels as well to study how using labels from standard annotation protocols affects calibration of metrics, which we show in the next section."}, {"title": "Benchmarking Automatic Metrics", "content": "Having established a source of ground truth, we benchmark recent automatic methods against our gold and expanded labels. We try the following metrics:\nBinary. We prompt GPT-4 (Achiam et al., 2023), Claude-3, and Mixtral-8x7B to assign a binary faithfulness label to each summary using the same definition of faithfulness as used for the human annotators.\nCoT. We prompt GPT-4, Claude-3, and Mixtral-8x7B to assign a binary faithfulness label to each summary, but to first provide some reasoning in a chain-of-thought style (Wei et al., 2022; Kojima et al., 2023). Models are prompted to: Consider whether there are any details in the summary that are inconsistent with the story and provide a couple sentences of reasoning for why the summary is or is not consistent with the story.\nFABLES. We use the approach from FABLES (Kim et al., 2024b) of asking ChatGPT to convert each summary to a list of claims and then asking GPT-4 to assign a binary faithfulness label to each claim. We then label the summary as faithful if all the claims are faithful.\nMiniCheck. We use the approach from MiniCheck (Tang et al., 2024a) of using a Flan-T5-Large model (Chung et al., 2024) finetuned on their synthetically generated dataset to check summary claims against passages from the story.\nUniEval. We use the approach from UniEval (Zhong et al., 2022) which uses multi-task learning across a unified framework of tasks to develop evaluation models. We use their Consistency variant.\nAlignScore. We use the approach from Align-Score (Zha et al., 2023) which uses multi-task training across a unified framework of tasks to determine if one piece of text is consistent with another."}, {"title": "Results", "content": "We first show the results of the different methods against the gold labels in Table 7 to see how automatic metrics vs. other human protocols compare against the gold labels. For UniEval and Align-Score, we tune their classification thresholds on the validation set and then use this threshold for the test set. For the remaining methods, we show results on the full dataset We see that the purely prompting-based LLM approaches predict most of the summaries as faithful and therefore have relatively low balanced accuracy scores. MiniCheck is best for detection of hard errors as it predicts lots of unfaithful summaries.\nThe best overall method is the FABLES approach with GPT-4 as a base, which achieves 67% balanced accuracy and is the most precise when it predicts a summary is faithful. FABLES detects more of the hard errors whereas the humans detect more of the easy errors. Both human approaches detect 93% of the easy errors, suggesting that these errors are generally obvious to humans regardless of protocol but not necessarily to models (FABLES finds 72% of easy errors). Interestingly, the expert human balanced accuracy is only 2% higher than for FABLES. This is important to note as without the expanded set of labels, someone might conclude that FABLES is performing as well as expert human annotators.\nNext we show the results against the expanded labels in Table 8, and we see that FABLES is still the best automatic method but its balanced accuracy remains similar (65%) and there is a drop of 14% in the number of hard errors it catches. This shows us that its balanced accuracy score against the gold labels was not artificially lowered because it was catching hard errors that the gold labels missed, but actually because it was labeling summaries as faithful that should have been unfaithful. We can also see this in the drop from 0.8 precision at detecting faithful summaries to 0.5 precision. Lastly, the only methods that significantly improve against the expanded labels are UniEval and AlignScore which jump 5-10% in balanced accuracy, but are still 10% worse than FABLES. All of these changes between the results against the gold and expanded labels indicate that model performance may be inflated or appear similar to humans when judged against flawed human annotations.\nOverall these results show that automatic methods have a lot of room for improvement on this dataset. We can also observe the range in percent of faithful summaries as labeled by different metrics from 18% using MiniCheck to 97% using Mixtral. We need to be careful what evaluation method we use so as not to mistake an unfaithful summarizer for a faithful one."}, {"title": "Related Work", "content": "Datasets. There are many datasets for fact-checking or inconsistency detection in news (Tang et al., 2022; Laban et al., 2022; Maynez et al., 2020; Huang et al., 2020; Pagnoni et al., 2021; Kry\u015bci\u0144ski et al., 2019b; Falke et al., 2019) and dialogue (Tang et al., 2024b) summarization. However, the summarization datasets specifically for narrative either use books and stories that most LLMs have trained on (Kryscinski et al., 2022; Wang et al., 2022) or use books that have to be purchased (Kim et al., 2024b).\nAutomatic Metrics. Many inconsistency detec-tioin methods have been developed on the above datasets, which we cite in Section 1 and Section 4. We test the current best metrics on our benchmark.\nCalibration against Humans. Krishna et al. (2023), Min et al. (2023) also propose recom-mendations for human evaluation of faithfulness. Other works (Fabbri et al., 2021a; Kry\u015bci\u0144ski et al., 2019a; Gabriel et al., 2020) have demonstrated that standard evaluation metrics are not well correlated with human judgments. Subbiah et al. (2024), Kim et al. (2024b), and Wang et al. (2022) find new ways to use human evaluation for narrative summa-rization specifically, focusing on the challenges of very long source stories."}, {"title": "Conclusion", "content": "We introduce a new benchmark for testing methods for faithfulness evaluation. In producing the benchmark, we demonstrate that faithfulness in narrative summarization is still a significant concern for LLMs, and we formulate recommendations for better evaluation of faithfulness in summaries. Finally, we demonstrate that recent automatic evaluation metrics have room for improvement on this task. In the future, we hope to use this dataset to improve methods for reliable evaluation of narrative summarization. In particular, we would like to develop automatic methods to merge error sets across evaluation protocols and check for correctness in error reason, not just localization."}, {"title": "Limitations", "content": "One limitation of this work is that we use a relatively small dataset. This size enables affordable experimentation with different human annotation protocols, and allows us to read and review all of the annotations, stories, and summaries to arrive at the conclusions presented in this paper. Additionally, since annotations are done on a sentence-level, the set of annotations and explanations is much bigger and quite rich. We do not have space to analyze results in this paper for detecting inconsistencies at the sentence level, but we hope to explore this in future work.\nAnother limitation is that the stories we use are amateur-written. Some of the stories can have confusing elements or unintentional ambiguities given that they were originally written for a casual Reddit community. However, we removed any stories that were too ambiguous for us to agree on. Finally, using more casually written stories allows us to challenge current annotation and model frameworks to see how well they perform with data that requires more interpretation.\nA final limitation is that the labels discussed in this paper depend on a small pool of annotators and experts. It would be interesting to see if the results are consistent across different sets of annotators and experts but each human annotation experiment is quite expensive to run."}, {"title": "Ethics Statement", "content": "There are not significant ethical concerns with this work as it is generally positive to have better evaluation of faithfulness in model summaries. We strictly collect publicly available stories that are written and shared by Reddit users who have full rights to their own work. These stories should not be re-shared under another name. Finally, we release the dataset without user-identifying information to protect user privacy. One of the authors, Melanie Subbiah, has an equity interest in OpenAI."}]}