{"title": "Multi-View Spectral Clustering for Graphs with Multiple View Structures", "authors": ["Yorgos Tsitsikas", "Evangelos E. Papalexakis"], "abstract": "Despite the fundamental importance of clustering, to this day, much of the relevant research is still based on ambiguous foundations, leading to an unclear understanding of whether or how the various clustering methods are connected with each other. In this work, we provide an additional stepping stone towards resolving such ambiguities by presenting a general clustering framework that subsumes a series of seemingly disparate clustering methods, including various methods belonging to the wildly popular spectral clustering framework. In fact, the generality of the proposed framework is additionally capable of shedding light to the largely unexplored area of multi-view graphs whose each view may have differently clustered nodes. In turn, we propose Gen-Clus: a method that is simultaneously an instance of this framework and a generalization of spectral clustering, while also being closely related to k-means as well. This results in a principled alternative to the few existing methods studying this special type of multi-view graphs. Then, we conduct in-depth experiments, which demonstrate that GenClus is more computationally efficient than existing methods, while also attaining similar or better clustering performance. Lastly, a qualitative real-world case-study further demonstrates the ability of GenClus to produce meaningful clusterings.", "sections": [{"title": "1 Introduction", "content": "Theoretical and computational developments in linear algebra and graph theory have set the foundations for the so-called spectral clustering family of methods [19], [11],[22],[6], which has proven to be one of the most successful clustering paradigms. However, although researchers have been defining novel and intricate graph types that aim to capture more complex information compared to traditional graphs [10], extending spectral clustering to such graphs is still not very well understood. Multi-view graphs are a popular instance of such graphs, which consist of sets of simple graphs called views. Each view consists of the same set of nodes, but a different set of edges. Prominent examples are time-evolving graphs [20], where different views emerge as edges appear or disappear at different points in time, and multi-relational knowledge graphs [13], which describe the relationships between entities, and where each relation type gives birth to a different view. The main rationale for developing models for multi-view graphs is that they are often able to obtain embeddings that better capture the intricasies of the graph structure as compared to when considering only individual views [15].\nNote that, existing research on multi-view graph has primarily focused on graphs whose views are all part of the same underlying node clustering structure [21],[9]. However, recently there have also been attempts to extend such methods to multi-view graphs with multiple view structures [12],[4], [2]. This category of multi-view graphs may contain multiple groups of views, each of which corresponds to a unique clustering of nodes as shown in Figure 1. Also, although there have been attempts to adapt spectral clustering to multi-view graphs [21],[9], to the best of our knowledge no such extension exists for multi-view graphs with multiple view structures. Additionally, since many of such methods have been developed independently of each other, it is not known whether or how existing methods of this type are related to each other in some way. To this end, in this work we make the following contributions:\n\u2022 Unifying clustering framework: We present a"}, {"title": "2 Proposed Unifying Framework", "content": "Consider K adjacency matrices, {X^{(k)}}_{k=1}^K, each of which is of size I \u00d7 I, and corresponds to a different view of a multi-view graph. Additionally, assume that the views can be clustered into M groups, with each group corresponding to a different clustering of nodes. For convenience, we will be referring to such a node clustering structure as view structure. In this section, we present an overview of existing methods that attempt to answer the question of how such a structure can be modelled in an unsupervised fashion, and a more detailed discussion on these methods can be found in subsection A.1. Then, we show how they can all be expressed as special cases of a common framework as shown in Figure 2. Specifically, we show that they can all be expressed as constrained versions of [U, V, AB] which is just a PARAFAC [1], [5] with its third factor matrix constrained to be the product of two matrices.\nPlease note that the goal of this common framework is not necessarily to precisely encapsulate the exact optimization problems of these methods. Rather, its goal is to abstract their essence. For example, we will assume that a method is conceptually aiming to calculate the same model independent of whether it algorithmically imposes a constraint as a hard constraint or as a soft constraint. Also, we will assume that it conceptually remains the same independent of whether it imposes constraints like non-negativity or sparsity. The reason is that such constraints are usually incorporated as a means to improve the quality of the solutions rather than to fundamentally alter the essence of the model."}, {"title": "2.1 ComClus", "content": "ComClus [12] can be expressed as\n\ninf_{U,W,A,B\u22650} \\sum_{k=1}^{K}||x^{(k)} \u2013 O^{(k)}UD(W) (O^{(k)}U)^T||^2 +\nr(U, W, A, B)\n\nwhere r(U, W,A,B) := \u03b2||W \u2013 AB||^2 + p(||U||_1 + ||A||_1 + ||B||_1), each O^{(k)} is a user-defined indicator matrix, and \u03b2 and p are user-defined penalty parameters. By making the closed-world assumption [13], the first term becomes equal to\n\n\\sum_{k=1}^{K}||X^{(k)} UDUT||= ||X- [U, U, W]||\n\nwhere X_k := X^{(k)}. Therefore, ComClus can be interpreted as aiming to approximate X by [U, U, W] such that W\u2248 AB, where U, A, B are sparse and non-negative, and W is non-negative."}, {"title": "2.2 Richcom", "content": "Richcom [4] can be expressed as"}, {"title": "2.3 Centroid-based Multilayer Network Clustering (CMNC)", "content": "CMNC [2] is very similar to Rich-com with their only differences being that for CMNC U^{(m)} = V^{(m)} for all m, a_i^{(m)} = 1 for all i, no sparsity is imposed and each view of the data tensor is preprocessed as D^{(k)}=X^{(k)}D^{(k)} where D^{(k)} := diag(\\sum_{i=1}^M X_{ik}). Therefore, similar arguments apply."}, {"title": "2.4 Spectral Clustering", "content": "In spectral clustering [19],[11],[22],[6] an important quantity is the Laplacian of X defined as L := D \u2013 X, where D := diag(\\sum_i X_{:i}). In turn the normalized Laplacian is defined as L_{sym} := D^{-\u00bd}LD^{-\u00bd}. Note that the eigenspace of L_{sym} corresponding to its lowest eigenvalue"}, {"title": "2.4.1 Single-View Spectral Clustering", "content": "Single-view spectral clustering can be formulated as inf_{U^TU=I} ||-L_{sym} \u2013 UU^T ||, and notice that UU^T = [U, U, 1^T] = [U, U, AB], where A := 1 and B := 1^T."}, {"title": "2.4.2 Multi-View Spectral Clustering", "content": "In [9] the authors proposed the MC-TR-I-EVDIT method which models multi-view spectral clustering as\n\nsup_{U^TU=I} \\sum_{k=1}^K Tr (U^TS^{(k)}U).\na\u22650,||a||=1\n\nThey also proposed the MC-TR-I method which can be seen as a variant of MC-TR-I-EVDIT where all the elements of a are the same.\nThen, in [8] the authors propose two forms of co-regularized spectral clustering with one utilizing a centroid-based co-regularization as\n\nsup_{U^{(k) T}U^{(k)}=I\\forall k} \\sum_{k=1}^K Tr (U^{(k)T}S^{(k)}U^{(k)}) +\nU^*TU^*=I\\\\\n\u03bb \\sum_{i\u2260j} Tr \\Big(U^{(i)T}U^{(j) }U^{(j)T}U^{(i)} \\Big)\n\nand the other utilizing the pairwise co-regularization\n\n\u03bb \\sum_k Tr \\Big(U^{(k)}U^{(k) T}U^*U^{*T} \\Big)\n\ninstead. Note that both of these formulations can be seen as relaxed versions of MC-TR-I where instead the embeddings from different views are forced to only be similar to each other instead of exactly equal. Specifically, the regularization terms force the columnspaces of all U^{(k)} to become more similar with each other as A and A_k"}, {"title": "3 Proposed Method", "content": "In this section, we develop GenClus, a principled graph clustering method which can be viewed as an instance of this framework. GenClus also generalizes spectral clustering [19] to multi-structure multi-view graphs, and, as we will show in subsection 3.3 it is closely connected to k-means as well.\nBased on our discussion in subsection 2.4 we will consider the modified data tensor y such that Y_k := D^{(k)}XD^{(k)} where D^{(k)} := diag (1/\\sqrt{\\sum_{j=1}^{I} X_{ij}}).\nWe can now see that in the special case of a single-view graph with K = 1, M = 1 and R equal to the true number of communities, we can use Theorem A.1 along with the fact that we expect y to have R eigenvalues close to 1 to show that\n\narginf_{U^TU=I} inf_{a\u22650,b\u22650} Tr(U(y - aU diag (b) U^T))= \\\\\n\narginf_{U^TU=I} || y \u2013 UU^T ||\n\nNote that allowing a to be negative may make (3.6) to not hold for any arbitrary y since the left hand side problem will select eigenvectors corresponding to its largest negative eigenvalues if they are of larger magnitude than its positive eigenvalues. This is in contrast to the right hand side problem which always retrieves eigenvectors corresponding to the maximum eigenvalues. Similarly, if we allow b to have negative elements, then the left hand side problem is a direct application of the Eckart-Young theorem [3] and will, therefore, select eigenvectors corresponding to the maximum magnitude eigenvalues instead."}, {"title": "3.1 Optimization Steps", "content": "We propose solving (3.7) in a block coordinate descent fashion by alternatingly updating A, and then U and B simultaneously. Note that, although in this work we do not provide arguments regarding convergence, our optimization scheme is guar-anteed to monotonically improve the objective function after each update of A, B and U."}, {"title": "3.1.1 Steps for A", "content": "By observing that ||Y \u2013 [U, U, AB]|| = ||Y^{(3)} - AB(UU^T)||, we"}, {"title": "3.1.2 Steps for U and B", "content": "Here, first notice that (3.7) can be reexpressed as\n\narginf_{U^{(m) T}U^{(m)}=I\\forall m} \\sum_{k=1}^K  \\sum_{n=1}^{M}||Y_{::k} - A_{km}U^{(m)} D^{(m)} U^{(m)} ^T||^2\nBT \u2208 I\n\nwhere D_{B}^{(m)} is defined to be a diagonal matrix containing only the non-zero elements of B_{m:}. We also define \\digg S_m as the set of indices of the views assigned to cluster m. Therefore, for a fixed A, we again distinguish three types of constraints for the non-zero elements of B:\nAll-ones B\nIn this case, we can show that for a fixed A (3.8) can be reexpressed as\n\narg sup_{U^{(m) T}U^{(m)}=I\\forall m}  Tr \\Big(U^{(m) T}Z^{(m)} U^{(m)} \\Big)\nBT \u2208 I\n\nwhere Z^{(m)} :=  A_{km}Y_{::k} - ||A_{:m}||^2 I  (see subsection A.5 for detailed derivation). Therefore, we can see that for a fixed B and for all m the optimal U^{(m)} has columns the eigenvectors of Z^{(m)} corresponding to its largest eigenvalues, and the corresponding summand in the objective function of (3.9) will be the sum of these eigenvalues. Thus, if we define \\diggE_m to be a set containing the eigenvalues of Z^{(m)}, and \\diggE_{max} to be a set containing the largest R elements of \\bigcup_{M} \\diggE_m, then we can see that the optimal value of the objective function in (3.9) cannot be greater than the sum of the elements of \\diggE_{max}. In fact, we can achieve this value by setting U^{(m)} to have columns the eigenvectors of Z^{(m)} corresponding to the eigenvalues in \\diggE_m \\bigcap \\diggE_{max}. Then, the optimal pair (U, B) can be given by setting U = [U^{(1)}, U^{(2)}, ..., U^{(M)}] and then deriving the optimal B by noticing that B_{mr} is non-zero if, and only if, any of the columns of U^{(m)} was placed as the r-th column of U.\nUnconstrained B\nIn this case, for a fixed A (3.8) can be reexpressed as\n\narginf  \\sum_{m=1}^{M}  \\Big||Z^{(m)} - ||A_{:m}||^2 U^{(m)} D_B^{(m)} U^{(m)} ^T ||^2 \\Big|\nU^{(m) T}U^{(m)}=I BT \u2208 I\n\nwhere Z^{(m)} :=  A_{km}Y_{::k}/||A_{:m}||  (see subsection A.6 for detailed derivation). Similarly, if we leave the non-zero elements of B unconstrained, then, for all m, U^{(m)} will have a fixed number of columns which will be optimal when they consist of the eigenvectors of  Z^{(m)} corresponding to the eigenvalues of largest magnitude. In turn, the non-zero elements of the optimal B_{m:} are the same eigenvalues divided by ||A_{:m}||. Also, notice that the corresponding summand in the objective function of (3.10) will be the sum of squares of all the remaining eigenvalues. Therefore, if we define \\diggE_m to be a set containing the eigenvalues of Z^{(m)}, and \\diggE_{max} to be a set containing the R elements of largest magnitude of  \\bigcup_{M} \\diggE_m, then we can see that the optimal value of the objective function in (3.10) cannot be lower than the sum of the squared elements of  \\bigcup_{M} \\diggE_m \\backslash \\diggE_{max}. In fact, we can achieve this value by setting the columns of U^{(m)} to be the eigenvectors of Z^{(m)} corresponding to the eigenvalues in \\diggE_m \\bigcap \\diggE_{max}. Then, the optimal pair (U, B) can be given by setting U = [U^{(1)}, U^{(2)}, ..., U^{(M)}] and by assigning the elements of \\diggE_m \\bigcap \\diggE_{max} divided by ||A_{:m}|| as the non-zero elements of B_{m:}.\nNon-negative B\nHere first notice that each of the M summands in (3.10) corresponds to calcu-lating the best positive semi-definite approximation of  A_{km}Y_{::k}/||A_{:m}||. In other words, we can see that the same arguments as in the unconstrained case apply with the difference that  \\diggE_m is instead defined to contain the largest eigenvalues of  A_{km}Y_{::k}/||A_{:m}|| after its negative eigenvalues are set equal to zero."}, {"title": "3.2 Space & Time complexity", "content": "We can show that the total space complexity of our method is O (K + MI^2 + IR), and for t iterations its total time complexity is O (tMI^3 + t(K + R + M)I^2). For more details, please refer to subsection A.7."}, {"title": "3.3 Model Interpretation", "content": "By looking at the updates for U, A and B we can see that there is a very natural way of interpreting GenClus. First, from (3.9) and (3.10) we can see that for a fixed A GenClus computes the nodes clustering for each of the M views clusters. Specifically, the m-th nodes clustering is calculated by essentially performing spectral clustering based on Z^{(m)} which corresponds to the weighted summation of the Laplacians of all views that belong to the m-th views cluster. On the other hand, for fixed U and B, and by noticing that [U, U, AB] = [U,U,B] \u00d7_3 A, we can see that each row of A is calculated in exactly the same fashion as the cluster assignment step of k-means or a k-lines method, depending on the type of constraint. Specifically, we can think of the frontal slices of [U, U, B] as the centroids representing the clusters, while the k-th row of A can be seen as an indicator vector encoding the cluster membership of Y_{::k}.\nThis observation leads us to another interesting finding. That is, if we constrain the non-zero elements of A and B to be all-ones and unconstrained, respectively, and we set R = MI, then GenClus becomes identical to k-means with the clustered data points being the frontal slices of Y. To see this, note that in this case for a fixed A there always exist U and B in (3.10) such that \\sum_{k=1}^K A_{km}Y_{::k}/||A_{:m}|| = ||A_{:m}|| U^{(m)} D_B^{(m)} U^{(m)T} for all m. Therefore, for a fixed A, the m-th frontal slice of the optimal [U, U, B] will be  A_{km}Y_{::k}/||A_{:m}||^2. In turn, this can be simplified to   Y_{::k}/|S_m| with \\diggS_m being the set of indices of the views assigned to cluster m, which is exactly the centroid calculation step of k-means. Thus, in the more general case where we do not set R = MI, roughly speaking GenClus can be seen as a version of k-means where each cluster assignment step is based on the denoised low-rank centroids {[U, U, B]_{::m}}_{m=1}^M instead of the raw centroids {Z^{(m)}}_{m=1}^M"}, {"title": "4 Experiments", "content": "In this section, we perform an in-depth experimental exploration of the behavior of GenClus both quantitatively and qualitatively. First, in subsection 4.1 we perform quantitative clustering quality comparisons with other baseline methods on artificially generated multi-view graphs with known ground truth labels. Then, in subsection 4.2 we present a qualitative case study which demonstrates the ability of GenClus to generate meaningful clusterings on real-world multi-structure multi-view graphs. Lastly, in subsection 4.3 we perform an experimental comparison of the time complexity of all methods. These experiments were conducted using Multi-Graph Explorer [17] whose source code can be found in [18].\nLastly, note that in all methods discussed so far, the clustering process can be divided into four segments: data preprocessing, embedding calculation, embedding postprocessing and embedding clustering. However, embedding calculation is arguably the central novelty in both the existing methods and our proposed method. Therefore, we will consider both the clustering schemes as proposed in the original papers and enhanced versions where the best combination of the remaining segments is selected. We will call these \"original methods\" and \"enhanced methods\", respectively. For the precise details of the experiment setup, please refer to subsection A.8."}, {"title": "4.1 Clustering Quality on Artificial Data", "content": "We generate a directed unweighted multi-view graph with 120 nodes and 9 views, which leads to a tensor of size 120 \u00d7 120 \u00d7 9. The views form 3 clusters with 3 views each, and each view cluster contains to 3, 2 and 2 node clusters, respectively. Specifically, the node clusters corresponding to the first view cluster contain 60, 40 and 20 nodes, respectively, while in the second view cluster they contain 100 and 20 nodes, respectively, and in the third view cluster they contain 20 and 100 nodes, respectively. All node clusters are y-quasi-cliques [16] whose intra-community edge density, y, takes values in the set of the 8 equally spaced values from 15% to 1%. For a specific generated graph, all quasi-cliques have identical value of y. Then, we randomly select 1% of all pairs of nodes, and for each pair we remove their connecting edge if they are connected or introduce a new edge between them if they are not connected. For the complete details of the experiment setup, please refer to subsubsection A.8.2."}, {"title": "4.1.1 Results Analysis", "content": "First, we make comparisons of the original methods as shown in Figure 3a. Here we see that GenClus offers superior performance compared to all baselines, and that it is in fact the only method that manages, in the median, to perfectly reconstruct the ground truth communities even with an intra-community edge density, \u03b3, as low as 11%. ComClus is the next best method that significantly outperforms both CMNC and Symmetric Richcom. Also, although ComClus slightly outperforms GenClus for very low values of y, it performs significantly worse than GenClus for higher values of \u03b3. CMNC outperforms Symmetric Richcom in the median, but due to its high variance we cannot confidently declare it as the clear winner.\nNow, we make comparisons of the enhanced meth-ods as shown Figure 3b. In this case, we observe that the enhanced versions of ComClus and Symmetric Rich-com present a significant performance uplift. In fact, ComClus now observably tends to perform better than GenClus. At the same time, Symmetric Richcom not only became significantly better than CMNC, but its performance is now closer to the performance of Gen-Clus than to CMNC. On the other hand, the enhanced versions of GenClus and CMNC barely show a perfor-mance improvement. However, at least for GenClus, this is a positive outcome, since it experimentally vali-dates our theoretical arguments from section 3 in favor of pairing GenClus with normalized Laplacians and with non-negative constraints for A and B."}, {"title": "4.2 Real-World Case Study", "content": "In this case study we are using a dataset of flight routes from 2012 available at [14], containing flights from a large number of airlines and airports around the world. Note that, to simplify the wording of our reasoning, we will refer to the Americas as if they were a single continent. For additional details, please refer to subsubsection A.8.4."}, {"title": "4.2.1 Results Analysis", "content": "Figure 4 shows the results of our proposed method on this dataset. As we can see, the clustering of views (airlines) generated by GenClus separates the majority of them based on their continent of origin. This aligns with our intuition that the flight patterns of airlines originating from the same continent are similar with each other and different from the flight patterns of airlines from different continents. Additionally, Figure 4b indicates that airlines from the Americas tend to have substantial presence in all three continents and these flights tend to be mostly within each continent instead of intercontinental which can be seen from the 3 airport clusters. On the other hand, in Figure 4c and Figure 4d which showcase the flights patterns of Asian and European airlines, respectively, we can see that these airlines tend to fly almost exclusively within their continent of origin."}, {"title": "4.3 Execution Time on Artificial Data", "content": "Here we experimentally compare the time complexity of all orig-"}, {"title": "5 Conclusions", "content": "In this work we devised a unifying framework for multiple existing graph clustering models, which is capable of modelling data as complex as multi-view graphs with multiple view structures. Then, we proposed GenClus which is a novel instance of this unifying framework. GenClus aims to have principled foundations by virtue of it being a generalization of the highly successful spectral clustering. Additionally, we conducted in-depth experiments on artificial data, in which we controlled for every aspect of the clustering workflow. These experiments showed that Gen Clus can have similar or better clustering performance than the baselines, while also being more computationally efficient as well. Lastly, we performed experiments on a real-world multi-view graph, which demonstrated that our method can properly model such complex datasets and uncover meaningful clusterings and insights."}, {"title": "A.1 Related Work", "content": "ComClus [12] operates on tensors whose frontal slices are symmetric adjacency ma-trices with non-negative elements. Specifically, it models the k-th view by approximating  X^{(k)} \\approx O^{(k)}UD(W) (O^{(k)}U)^T, where U and W are factor ma-trices of size  I\u00d7 R and K\u00d7 R, respectively, and D is defined as diag(W_{k:}). Also, each  O^{(k)} is a pre-defined indicator matrix that accounts for the fact that a node that is shared between different views may be represented by different rows and columns in the cor-responding adjacency matrices. ComClus additionally defines factor matrices A and B of sizes  K\u00d7 M and M\u00d7 R, respectively, which are then used to model W as  AB, where A is constrained to be an indicator matrix, and the rows of B are the latent representations of the views clusters. Also, for computational tractabil-ity reasons the authors relax the constraint of A and impose sparsity and non-negativity on  U, A and B, while W is constrained to be non-negative. Then, the authors calculate this model via the following optimiza-tion problem:\n\\\ninf_{U,W,A,B\u22650} \\sum_{k=1}^{K}||x^{(k)} \u2013 O^{(k)}UD(W) (O^{(k)}U)^T||^2 +\nr(U, W, A, B)\n\\\nwhere r(U, W, A,B) := \u03b2||W \u2013 AB||^2 + \u03c1(||U||_1 +||A||_1 + ||B||_1), and \u03b2 and pare penalty parameters that need to be defined by the user. Each factor matrix is updated individually via multiplicative updates in a block coordinate descent fashion [19], until the value of the objective function stops improving. Lastly, the authors assign the m-th view to the n-th cluster when the n-th element of the m-th row of A has the largest magnitude among all elements of that row. Then, they consider the embeddings of the nodes of the n-th calculated views cluster to be the rows of U diag(B_{n:}) and cluster the nodes in the same way.\nNote that the original formulation of ComClus includes two additional terms. Specifically, one term forces the latent representations of two views to be more orthogonal to each other as the number of their mutual nodes decreases, while the other term enables the user to perform semi-supervised learning when there is additional available information about how the various views relate to each other. While in this work we omit these terms, note that the effect of the term that imposes orthogonality can be achieved implicitly in a different way. That is, instead of considering an edge between two unshared nodes as unknown, we can consider it as known with weight zero. In this way, the larger the number of unshared nodes between two views is, the larger the number of elements which are non-zero in only one of the corresponding adjacency matrices of the networks will be. This implies that these adjacency matrices will tend to be orthogonal to each other, a property which will tend to hold for their latent representations as well. In fact, the original ComClus formulation can be interpreted as operating under the open world assumption [13], while our modification can be seen as operating under the closed world assumption."}, {"title": "A.1.2 Methods Based on Block Term Decomposition", "content": "Richcom Richcom [6] applies a rank-(Lm, Lm, 1) terms decomposition [3] on the data tensor and operates on tensors whose frontal slices can be arbitrary adja-cency matrices with non-negative elements. Specifically, it considers a tensor X of size  I\u00d7 J\u00d7K and models the graph by approximating X as   (U^{(m)}V^{(m)T}) \u00d7_3 a^{(m)}, where U^{(m)}, V^{(m)} and a^{(m)} are factors of sizes I\u00d7Rm,  J\u00d7 Rm and K, respectively. The authors im-plicitly assume that the ordering of the nodes is iden-tical for all views, and, therefore, no permutations sim-ilar to these of ComClus are required. Also, note that all adjacency matrices need to be of size I\u00d7 J, which can be interepreted as either that Richcom only works when all nodes exist in all views, or that it makes the closed world assumption. Note that all factor matrices are constrained to be sparse and non-negative. Then, the authors calculate their model via the following op-timization problem:\n\ninf_{U^{(m)},V^{(m)},}  ||X- \\sum_{m=1}^{M}(U^{(m)}V^{(m)T}) \u00d7_3 a^{(m)} ||^2+\na^{(m)} \\digg \u03a3 r (U^{(m)}, v^{(m)}, a^{(m)})\nwhere r (U^{(m)}, V^{(m)}, a^{(m)}) encodes the sparsity and non-negativity constraints. They solve (A.2) using the AO-ADMM framework [7] which solves for all  U^{(m)},"}, {"title": "A.1.3 Spectral Clustering", "content": "Spectral clustering [11],[18],[21],[8] has seen great success and devel-opments in the past decades thanks to its strong theoretical foundations and its ability to discover clusters of arbitrary shape. In fact, these developments have led to generalized versions for multi-view graphs [10], [20].\nConsider an adjacency matrix X of an arbitrary undirected graph with non-negative weights. If we de-fine D := diag (\\sum_{i=1}^{I} X_{i:}) then  L := D-X is called the Laplacian of X. It can be shown [18] that Lis positive semi-definite and that the number of connected components of the graph is equal to the multiplicity of the smallest eigenvalue of L, which is always 0. It can also be shown [11] that if U is a matrix whose columns form a basis for the eigenspace correspond-ing to the smallest eigenvalue of L, then two rows of U are collinear if the corresponding nodes belong to the same connected component, and orthogonal to each other if the corresponding nodes belong to different com-ponents. Spectral clustering algorithms then use these properties to cluster the rows of U and identify the communities of a graph in a principled manner. Ad-ditionally,  L_{sym} := D^{-\u00bd}LD^{-\u00bd} is called the normalized Laplacian and inherits all the aforementioned nice prop-erties of L. The usefulness of  L_{sym} is usually justified in the literature by associating it to a relaxation of the n-cut problem [18]. Lastly, note that the eigenspace of L_{sym} corresponding to an eigenvalue of 0 is identical to the eigenspace of S := I -  L_{sym} = D^{-\u00bd}XD^{-\u00bd} corresponding to its maximum eigenvalue which is 1.\nNote there is a more direct and intuitive justifica-tion for choosing  L_{sym} over L. Specifically, notice that since  L_{sym} is positive semi-definite, the eigenvalues of S := I-  L_{sym} = D^{-\u00bd}XD^{-\u00bd} will all be less than or equal to 1, and since all elements of S are non-negative, it can in turn be shown that its smallest eigenvalue will also be greater than or equal to -1. Therefore, the eigenvalues of  L_{sym} will be bounded between 0 and 2. This property can be especially useful when the differ-ent communities of the graph are not completely dis-connected from each other, in which case we evaluate the number of communities to be equal to the number of eigenvalues of  L_{sym} that are only approximately 0, or the number of eigenvalues of S that are only approx-imately 1. Another reason is that when communities are completely disconnected from each other, then  L_{sym} can be expressed as a block-diagonal matrix where each block corresponds to a different community. This im-plies that the set of the eigenvalues of  L_{sym} is the union of the eigenvalues of its blocks, and, therefore, the max-imum eigenvalue for all communities will be 2. In turn, this implies that when communities are not entirely dis-connected from each other, the decision of whether an eigenvalue is close enough to 0 does not have to involve the size of the corresponding community."}, {"title": "A.1.4 Multi-View Spectral Clustering", "content": "When a graph has multiple views such that all views are assumed to share a common underlying structure, i.e.  M = 1, then one of the multi-view spectral clustering models proposed in [10], [20] can be applied. In our work, we will be particularly interested in some of the models proposed in [10]. Specifically, we will study the MC-TR-I model,\n\nsup_{U^TU=I} \\sum_{k=1}^K Tr (U^TS^{(k)}U),\n\nwhich aims to perform spectral clustering jointly on all views in a way that the embeddings for all views are identical to each other. We will also consider its weighted variant,\n\nsup_{U^TU=I} \\sum_{k=1}^K Tr (U^Ta_kS^{(k)}U),\n\na\u22650,||a||=1\n\nwhich assigns a different weight to each view. To calcu-late these models, the authors proposed the MC-TR-I-EVD and MC-TR-I-EVDIT algorithms, respectively."}, {"title": "A.5 Derivation of (3.9)", "content": "\\\\\narginf \u03a3_m \u03a3_k ||Y_{::k"}, "A_{km}U^{(m)}U^{(m)T}||^2\nU^{(m) T}U^{(m)}=I\u2200m \u2208 I\nB_T\n= \\\\\narginf \u03a3_m \u03a3_k -2Tr (A_{km}U^{(m) T}U^{(m)} Y_{::k}) +\nU^{(m) T}U^{(m)}=I\u2200m\nB_T\n\\\nTr((A_{km}U^{(m) T}U^{(m) T})^2)\n= \\\narginf \u03a3"]}