{"title": "INSIGHTBUDDY-AI: Medication Extraction and Entity Linking using Large Language Models and Ensemble Learning", "authors": ["Pablo Romero", "Lifeng Han", "Goran Nenadic"], "abstract": "Medication Extraction and Mining play an important role in healthcare NLP research due to its practical applications in hospital settings, such as their mapping into standard clinical knowledge bases (SNOMED-CT, BNF, etc.). In this work, we investigate state-of-the-art LLMs in text mining tasks on medications and their related attributes such as dosage, route, strength, and adverse effects. In addition, we explore different ensemble learning methods (STACK-ENSEMBLE and VOTING-ENSEMBLE) to augment the model performances from individual LLMs. Our ensemble learning result demonstrated better performances than individually fine-tuned base models BERT, ROBERTa, ROBERTa-L, BioBERT, BioClinicalBERT, BioMedRoBERTa, ClinicalBERT, and PubMedBERT across general and specific domains. Finally, we build up an entity linking function to map extracted medical terminologies into the SNOMED-CT codes and the British National Formulary (BNF) codes, which are further mapped to the Dictionary of Medicines and Devices (dm+d), and ICD. Our model's toolkit and desktop applications are publicly available at https://github.com/HECTA-UoM/ensemble-NER.", "sections": [{"title": "Introduction", "content": "Information Extraction on Medications and their related attributes plays an important role in natural language processing (NLP) applications in the clinical domain to support digital healthcare. Clinicians and healthcare professionals have been doing manual clinical coding for quite a long time to map clinical events such as diseases, drugs, and treatments into the existing terminology knowledge base, for instance, ICD and SNOMED. The procedure can be time-consuming yet without a guarantee of total correctness due to human-introduced errors. With the process of automated information extraction on medications, it will be further possible to automatically map the extracted terms into the current terminology database, i.e. the automated clinical coding. Due to the promising future of this procedure, different NLP models have been deployed in medication mining and clinical coding in recent years. However, they are often studied separately. In this work, we investigate text mining of medications and their related attributes (dosage, route, strength, adverse effect, frequency, duration, form, and reason) together with automated clinical coding into one pipeline. In addition, we investigate the ensemble learning mechanisms (Stack and Voting) on a broad range of NLP models fine-tuned for named entity recognition (NER) tasks. These models include both general domain trained BERT, ROBERTa, RoBERTa-L, and domain-specific trained BioBERT, BioClinicalBERT, BioMedRoBERTa, ClinicalBERT, and PubMedBERT. In this way, users do not have to worry about which models to choose for clinical NER. Instead, they can just place the newer models into the ensemble-learning framework to test their performances. We offer desktop applications and web interfaces for the clinical NER, ensemble, and coding models we are developing."}, {"title": "Literature Review and Related Work", "content": ""}, {"title": "Clinical NER and Ensemble Learning", "content": "Named Entity Recognition (NER) is a critical task for extracting key information from unstructured text, like medical letters. The complexity and context-dependency of medical language pose significant challenges for accurate entity extraction. Traditional approaches to NER, such as rule-based systems, have shown limited success in capturing the nuanced contextual information crucial for clinical NER (Nadeau and Sekine, 2007).\nThe advent of deep learning methods, particularly Long Short-Term Memory (LSTM) networks, marked a significant improvement in NER performance (Graves and Schmidhuber, 2005). LSTMs demonstrated an ability to capture long-range dependencies in text. However, these models still struggled with rare entities and complex contextual relationships in clinical notes.\nThe introduction of BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) in 2018 revolutionised various Natural Language Processing (NLP) tasks, including NER. BERT's self-attention mechanism and bidirectional training allow it to capture nuanced contextual information over long pieces of text. The model's pre-training on a large corpus using a masked language modelling objective builds rich token representations. The model can then be later fine-tuned by adding a classification layer at the end of the network to make decisions over each individual token embedding.\nHowever, BERT's pre-training on general domain corpora (Wikipedia and books) limited its effectiveness on specialised medical texts. This limitation led to the development of domain-specific BERT variants. For example, BioBERT (Lee et al., 2019), pre-trained on large-scale biomedical corpora; ClinicalBERT (Wang et al., 2023), fine-tuned on EHR data from 3 million patients after pre-training on 1.2 billion words of diverse diseases, and other variants like Med-BERT (Rasmy et al., 2021) have demonstrated enhanced performance on medical NER tasks due to their specialised training on the medical domain.\nDespite these improvements, single-model approaches still struggle with the inherent complexity and variability of clinical text, as the comparative studies reported in (Belkadi et al., 2023) across different models using BERT, ClinicalBERT, BioBERT, and scratch-learned Transformers. Ensemble methods have emerged as a promising direction to address these challenges, they have proven useful in other fields, such as computer vision (Lee et al., 2018). By combining multiple models, ensembles can leverage the strengths of different models while mitigating their individual weaknesses. In the context of NER, ensembling has shown performance improvements, as shown by (Naderi et al., 2021), where an ensemble is used on a health and life science corpora for a significant improvement in performance over single models. Naderi et al. (2021) conducted max voting for word-level biology, chemistry, and medicine data. However, on clinical/medical NER, they only focused on French using the DEFT benchmark dataset; while for the other two domains of biology and chemistry, they tested on English data.\nThere are two commonly used ensemble methods, voting and stacked ensembles.\n\\bullet Maximum voting in ensembles where each model contributes equally to the final decision as used in the paper (Naderi et al., 2021) have proved effective.\n\\bullet We want to explore if training a network on the outputs of the ensemble enables it to capture more nuanced relationships. This is accomplished using a method called stacking introduced by Wolpert (1992). Stacking offers a more sophisticated approach by training a meta-model on the outputs of the base ensemble; the model is expected to learn more complex patterns from the outputs of the ensemble, leading to better predictions. This has proven effective in this paper (Saleh et al., 2022) where they use a stacked ensemble with a support vector machine for sentiment analysis. We use a simple feed-forward network from the outputs of the ensemble to the final labels. more examples on stacked ensemble can be found at (Mohammed and Kora, 2022; G\u00fcne\u015f et al., 2017).\nWhile ensemble methods have shown promise in other NER tasks, their effectiveness for clinical NER, particularly on challenging datasets like n2c2 2018 (Henry et al., 2020), remains unexplored. The n2c2 2018 dataset presents unique challenges due to its diverse entity types and the high degree of domain-specific knowledge required.\nThis work aims to address this gap by investigating whether stacked and voting ensembles can improve NER performance on clinical notes, potentially overcoming the limitations of current single-model approaches.\nHowever, it's important to acknowledge the limitations of ensemble methods, particularly in clinical settings. The increased computational resources required for training and inference present challenges in resource-constrained environments. The trade-off between improved performance and increased computational cost must be carefully evaluated in the context of clinical workflows and available infrastructure."}, {"title": "Clinical Entity Linking", "content": "Entity Linking in the clinical domain is often conducted as Clinical Coding, which traditionally requires human coders to map clinical events into dictionaries or knowledge bases such as ICD, SNOMED, and BNF. However, due to the time-consuming nature of such tasks, automated coding has been explored using different methods including rule-based ones such as (Farkas and Szarvas, 2008) and machine learning methods such as using neural networks CNN by Mullenbach et al. (2018), Transformers by Liu et al. (2022). There are also comparative studies of different methods with explainability (Glen et al., 2024). In our work, to make the extracted clinical events more valuable, we will integrate a clinical coding function into our platform by mapping into the BNF coding list, which is further mapped to ICD, SNOMED, and dm+d based on existing work."}, {"title": "Methodologies", "content": "The Overall framework of MEDNER is shown in Figure 1, which displays the base models we included from the general domain 1) BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and ROBERTa-Large, and 2) biomedical/clinical domains BioBERT (Lee et al., 2019), BioClinicalBERT (Alsentzer et al., 2019), BioMedROBERTa (Gururangan et al., 2020), ClinicalBERT (Wang et al., 2023), and PubMedBERT (Gu et al., 2020). The fine-tuning of eight models uses the same set of parameters (Section B for parameter selections) and the n2c2-2018 shared task training data with data pre-processing. The initial evaluation phase using n2c2-2018 testing set gives an overall idea of each model's performance. This is followed by ensemble learning on all the models' outputs. With the output from NER models, we add an entity linking function to map the extracted medical entities into the standard clinical terminology knowledge base (KB), using SNOMED-CT and BNF as our initial KB. Finally, we also offer user-friendly desktop applications and interfaces for the MEDNER project.\nFor data pre-processing, we chunk the sequence into a maximum of 128 tokens. If there is a full stop \".\" between the 100th and 128th tokens, it will be cut at the full stop. Regarding ensemble-learning strategy, we draw a MedNER Ensemble figure (Figure 3) to explain in detail. Firstly the initial output of eight individual fine-tuned NER models is tokenised, i.e. at the sub-word level, due to the model learning strategy, e.g. \u201cPara ##ce ##tam ##ol\" instead of \"Paracetamol\". What we need to do at the first step is to group the sub-word tokens into words for both practical application and voting purposes. However, each sub-word is labeled with predefined labels and these labels often do not agree with each other within the same words. We designed three group solutions, i.e. first-token voting/selection, max-token voting, and average voting. The first-token voting is to assign a word the same label as its first sub-word piece. For example, using this strategy, the word \"Paracetamol\" will be labeled as \"B-Drug\u201d if its first sub-word \"Para\" is labeled as \u201cB-Drug\u201d regardless of other labels from the subsequent sub-words. The max-token voting will assign a word the label that has the highest sub-word logit, this indicates that the model is more confident in that prediction, the higher the logit is. The average voting solution calculates the average logits across all sub-words predictions and then samples from this to get the label for the entire word.\nRegarding word-level ensemble learning, we investigate the classical voting strategy with modifications (two solutions). For the first solution \">=4 or O\", if there are more than half of the models agree on one label, we pick this label, i.e. >=4 such same labels. Otherwise, we assign the default \"O\" label to indicate it as context words, due to the models' disagreement. For the second solution, we use max-voting, i.e. the most agreed label regardless of how many models they are, e.g. 2, 3, 4, or more. In this case, if there are ties, e.g. (3, 3, 2) two labels are voted both three times from six models, we need to decide on the tied labels. There are two solutions for the selection, 1) alphabetical, and 2) fully randomised.\nWe also draw the STACKED-ENSEMBLE in Figure 4 and 5, where the model training and one-hot encoding are illustrated. In the training phase, we slit the real data into 80% and 20% for model training and testing. Model exports are conducted only if at least 2 models are predicting a label that is not \"O\"; otherwise \u201cO\u201d is the default option. For model deployment, output logis for each model are turned into a one-hot encoded vector, concatenated and saved along with the real label for each token. There are 8 one-hot encoded vectors from"}, {"title": "Experimental Evaluations", "content": "We use the n2c2-2018 shared task data on NER of adverse drug events and related medical attributes (Henry et al., 2020). The data is labels with the following list of labels: ADE, Dosage, Drug, Duration, Form, Frequency, Reason, Route, and Strength in BIO format. So overall we have 19 labels, 2 (B/I) x 9 + 1 (O). The original training and testing sets are 303 and 202 letters respectively. We divided the original training set into two parts (9:1 ratio) for our model selection purposes: our new training and validation set, following the data split from recent work by Belkadi et al. (2023).\nWe report Precision, Recall, and F1 score in two categories \"macro\u201d and \u201cweighted\", in addition to Accuracy. The \u201cmacro\u201d category treats each label class the same weight regardless of their occurrence rates, while the \u201cweighted\" category\" assigns each label class with a weight according to their occurrence in the data. We first report the individual model fine-tuning scores and compare them with related work (sub-word level); then we report the ensemble model evaluation with different ensemble solutions (word-level)."}, {"title": "Individual Models Finetuned: sub-word level", "content": "The performances of individual models after fine-tuning are reported in Figure 7, where it says that ROBERTa-L performs that best on macro Precision (0.8489), Recall (0.8606), and F1 (0.8538) score across general domain models, also winning domain-specific models. BioiMedRoBERTa wins the domain-specific category models producing macro Precision, Recall, and F1 scores (0.8482 0.8477 0.8468). In comparison to the NER work from Belkadi et al. (2023), who's macro avg scores are: 0.842, 0.834, 0.837 from ClinicalBERT-Apt, our fine-tuned ClinicalBERT has similar / comparable performances (0.848, 0.825, 0.834), which shows our fine-tuning was successful. However, our best domain-specific model BioMedRoBERTa produces higher scores: macro P/R/F1 (0.8482 0.8477 0.8468) and weighted P/R/F1 (0.9782 0.9775 0.9776) and Accuracy 0.9775 as in Figure 6. Furthermore, the fine-tuned ROBERTa-L even achieved higher scores of (0.8489 0.8606 0.8538) for macro P/R/F1 and Acc 0.9782 in Figure 13. Both fine-tuned BioMedROBERTa and ROBERTa-Large also win the best models reported by Belkadi et al. (2023) which is their ClinicalBERT-CRF model, macro avg (0.85, 0.829, 0.837), Acc 0.976. Afterwards, in this paper, we emphasis on word level instead of sub-word, which was focused on by Belkadi et al. (2023)."}, {"title": "Ensemble: word-level grouping max/first/avg-logit", "content": "We tried first logit voting, max voting, and average voting to group sub-words into words with corresponding labels. Their results are shown in Figure 12, in the upper cluster. First logit voting produced a higher Recall 0.8260 while Max logit voting produced a higher Precision 0.8261 resulting in higher F1 0.8232, i.e. Max logit > First logit > Average logit with macro F1 (0.8232, 0.8229, 0.8227). However, overall, their performance scores are very close, so we chose the first-logit voting output for the afterward word-level ensemble due to computational convenience."}, {"title": "Word-level vs Sub-word Level scores", "content": "From word-level ensemble result in Figure 13, it says that the ensembled model can achieve word-level evaluation scores 0.826, 0.826, and 0.823 for macro P/R/F1, which is close to sub-word level best model 0.847 F1. We can see that at word-level evaluation, there are 563,329 support tokens in Figure 13, vs sub-word level 756,014 tokens in Figure 8. Can we also report word-level eval without distinguishing B/I? i.e. as long as the label catergies are correct regardless it is B or I.\nWord-level ensemble voting, max-logit voting > first-logit > average-logit, as shown in Figure 12, with Macro F1 scores (0.8232, 0.8229, 0.8227) respectively, which are very close though. They have the same weighted average F1 and Accuracy scores (0.9798, 0.9796) respectively."}, {"title": "Ensemble: Voting vs Stacked", "content": "We use the Max logit stacked ensemble as an example, in figure 15, which shows that the Stacked Ensemble produced much lower evaluation scores macro avg (0.6863 0.7339 0.6592) than the voting mechanism macro avg (0.8261 0.8259 0.8232) for (P, R, F1). The corresponding confusion matrix from the stacked ensemble using the max logic is shown in Figure 16 with more errors spread in the image, the colored numbers outside the diagonal line."}, {"title": "Word-level: ensembles vs individual fine-tuned", "content": "BioMedROBERTa individual word level max logit grouping scores macro avg P/R/F1 (0.8065 0.8224 0.8122 563329) vs max logit ensemble voting P/R/F1 (0.8261 0.8259 0.8232), we can see that ensemble boosted P (0.8261-0.8065)/0.8065= 2.43%, and F1 (0.8232-0.8122)/0.8122= 1.35% which says the ensemble voting is successful. By increasing the Precision score, the ensembles reduce the false positive labels in the system output, while keeping the Recall at the same level, i.e. the true positive labels."}, {"title": "Ensemble Models: BIO-span vs non-strict word-level", "content": "So far, we have been reporting the evaluation scores on the BIO-strict label categorization, i.e. we distinguish between the label's beginning or the inner part of the label. For instance, a B-Drug will be different from an I-Drug and it will be marked as wrong if they are different from the reference. However, we think in practice, there are situations when users do not need the BIO, especially B and I difference. In Figure 12, we can see that, without considering the label difference of B and I, only focusing on the 9 label categories, word level ensemble model produced much higher Macro avg evaluations cores on Precision (0.8844) and Recall (0.8830) leading to higher F1 (0.8821), in comparison to BI-distinguished Macro F1 0.8232 (voting-max-logit) and F1 0.8156 (stacked-first-logit)."}, {"title": "Entity Linking: BNF and SNOMED", "content": "To map the identified named entities into the clinical knowledge base. We use the existing code mapping sheet from the British National Formulary (BNF) web between, SNOMED-CT, BNF, dm+d, and ICD. We preprocessed the SNOMED code from 377,834 to 10,804 to filter repeated examples between the mapping of SNOMED and BNF. We looked for non-drug words present in the text, then we filtered the drugs further by seeing if words like ['system', 'ostomy', 'bag', 'filter', 'piece', 'closure'] were present in the text and if so, it was discarded.\nFor SNOMED CT mapping, we applied fuzzy search on the cleaned mapping list with drug names. Then we SNOMED CT code will be added to the searching function on the SNOMED CT web, whenever there is a match.\nFor BNF mapping, the linking function uses keyword search to retrieve the BNF website with corresponding drugs, due to its different searching features in comparison to the SNOMED-CT web page. Users can select whichever is suitable to their preferences between the two clinical knowledge bases (KBs), Figure 18."}, {"title": "MEDNER Desktop Application and Interface: INSIGHT-BUDDY-AI", "content": "We offer the desktop application of individually fine-tuned models that we developed during this investigation.\nFigure 19 shows the options to choose either BNF or SNOMED-CT mapping for Medical Coding usage. Figure 20 demonstrates the NER outcomes using a synthetic clinical letter generated by Claude. Figure 21 shows the interface can load any Huggingface NER models and example output from Model https://huggingface.co/Xenova/bert-base-NER. Users can do inference on any NER model but they do need it to be .onnx in its folder structure. Figure 19 shows the features of INSIGHTBUDDY-AI with context-awareness using a window parameter around the recognised entity. We also recorded live demos on the desktop applications.\nThe applications currently support file types: PDF, DOCX, DOC, and TXT; operating systems: Both windows and Mac."}, {"title": "Discussion and Conclusion", "content": "In this paper, we investigated Stacked Ensemble and Voting Ensemble on medical named entity recognition tasks using eight pretrained LMs from both general and biomed/clinical domains. Our experiments show that our fine-tuned best individual models outperformed the state-of-the-art on standard shared task data n2c2-2018. The ensemble strategies further improved the individual model performances. We also offered desktop applications and user interfaces for individual fine-tuned models where we added the entity linking/normalisation function to BNF and SNOMED CT clinical knowledge base. We call the interface INSIGHTBUDDY-AI, which is publically available at."}, {"title": "Ethics", "content": "To use the n2c2 shared task data, the authors have carried out CITI training (https://physionet.org/settings/credentialing/) and gained the access to the data with user agreement."}]}