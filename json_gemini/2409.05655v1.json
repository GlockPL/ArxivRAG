{"title": "Interactive incremental learning of generalizable\nskills with local trajectory modulation", "authors": ["Markus Knauer", "Alin Albu-Sch\u00e4ffer", "Freek Stulp", "Jo\u00e3o Silv\u00e9rio"], "abstract": "The problem of generalization in learning from demonstration (LfD)\nhas received considerable attention over the years, particularly within the con-\ntext of movement primitives, where a number of approaches have emerged. Re-\ncently, two important approaches have gained recognition. While one leverages\nvia-points to adapt skills locally by modulating demonstrated trajectories, another\nrelies on so-called task-parameterized models that encode movements with re-\nspect to different coordinate systems, using a product of probabilities for gen-\neralization. While the former are well-suited to precise, local modulations, the\nlatter aim at generalizing over large regions of the workspace and often involve\nmultiple objects. Addressing the quality of generalization by leveraging both ap-\nproaches simultaneously has received little attention. In this work, we propose\nan interactive imitation learning framework that simultaneously leverages local\nand global modulations of trajectory distributions. Building on the kernelized\nmovement primitives (KMP) framework, we introduce novel mechanisms for skill\nmodulation from direct human corrective feedback. Our approach particularly ex-\nploits the concept of via-points to incrementally and interactively 1) improve the\nmodel accuracy locally, 2) add new objects to the task during execution and 3) ex-\ntend the skill into regions where demonstrations were not provided. We evaluate\nour method on a bearing ring-loading task using a torque-controlled, 7-DoF, DLR\nSARA robot [1] [2].", "sections": [{"title": "1 Introduction", "content": "Task-parameterized Gaussian mixture models (TP-\nGMMs) [3] are a popular approach to encoding the vari-\nations and correlations across multiple demonstrations,\nfacilitating skill generalization. Unlike earlier attempts\nsuch as dynamic movement primitives (DMPs) [4] and\nother probabilistic models for movement primitives like\nGaussian mixture models (GMMs) [5], TP-GMMs are\nbetter suited for adapting to new situations, including\nthose involving multiple objects [6]. TP-GMMs build lo-\ncal representations of demonstrated trajectories with re-\nspect to objects of interest, represented by their poses.\nThey then generalize the demonstrations to new situa-\ntions, by formulating generalization as a fusion problem,\nwhere each object's local model is weighed against the\nothers in a continuous fashion through a product of Gaus-\nsians, generating a trajectory distribution for the robot to track. Despite their adaptability, TP-GMMS\nare prone to modeling errors, particularly when imperfect demonstrations introduce ambiguity be-\ntween objects, which affects generalization when task conditions change, see Fig. 1. In addition,\nintroducing new objects into a learned skill (by defining new task parameters), requires providing\nnew demonstrations and training a new model, when often simple modulations of the original model\nwould suffice. In this paper, we propose to alleviate these issues via interactive imitation learning\n[7]. We do this by modulating local models from physical user feedback, both improving the gen-\neralization of taught skills - by locally correcting errors and permitting their incremental re-use.\nNotably, adaptations are applied only locally, with respect to objects of interest, yielding an updated\nskill model which accurately generalizes new behaviors to new situations."}, {"title": "2 Related work", "content": "Task-parameterized GMMs and variations. Several works have emerged both to address lim-\nitations of [3] and introduce new features. In [10] Huang et al. propose to associate confidence\nfactors to the different task parameters, allowing to regulate their influence during task execution.\nZhu et al. [11] introduce an algorithm to generate new data artificially by sampling from underly-\ning models and re-training the models to improve generalization. Building on [10], Sun et al. [12]\nand Sena et al. [13] introduce strategies to optimize frame relevance given task objectives. Similar\nre-optimization is required in incremental approaches for TP models, such as [14], which also relies\non an underlying GMM. Recently, Yao et al. [15], propose to replace the GMM representation by a\nProMP [16], introducing the idea of improving generalization by modulating TP models with via-\npoints. However, via-points are defined globally and projected locally on all frames. This creates\nambiguity when task parameters change, since all frames have high confidence on the via-point posi-\ntions, requiring the re-definition of via-points every time a task changes. Although these approaches"}, {"title": "3 Preliminaries", "content": "Let us denote a set of demonstrations by {{sh,m, Eh,m}=1}=1, where s \u2208 R and \u0118 \u2208RO\nrepresent input and output, and I, O, M, H, are the dimensions of input and output, number of\ndemonstrations and trajectory length, respectively. Similarly to many popular LfD approaches [5, 9,\n16], we focus on extracting the relationship between s and \u00a7 from demonstrations. Depending on\nthe task, s is often time or robot state while & represents desired poses or velocities. In Sec. 3 and\nSec. 4 we keep these generic except when concrete examples help explain new concepts.\nTask-parameterized movement models. In TP models [3] a frame p = 1,..., P is described\nby so-called task parameters b(p), A(p), which represent the position and orientation of an ob-\nject with respect to a common reference frame (e.g. the robot base)\u00b9, where demonstrations are"}, {"title": "4 Interactive local trajectory modulation with TP-KMP", "content": "Similarly to [9], we define a local KMP as a model (p) = {b(p), A(p), D(p)} with associated task\nparameters b(p), A(P) and D(p) = {sp), \u03bc(P), \u03a3(P)}=1. \u03bc(p), \u2211(p) are computed from output data\nprojected locally \u00a3(p), using a GMM. A TP-KMP is a set of P local KMPs: \u04e8 = {\u04e8(p)}=1, where\neach local KMP generates a distribution N(\u03bc(p), \u2211(p)), computed from (2)\u2013(3), which is used in\n(1). We introduce an approach to interactively add via-points to local KMPs at any moment of a\ntask, allowing users to intuitively improve models trained on sub-optimal demonstrations."}, {"title": "4.1 Interactive trajectory modulation with local via-points", "content": "In our approach, users add via-points via physical corrections locally, in different object frames,\nas opposed to in a common global frame. This enables the adaptation of robot behavior without\nretraining the model from scratch. Adding via-points in the relevant local frames has the advantage\nthat corrections 'move' with the objects when task conditions change, facilitating generalization.\nThe via-point mechanism of KMP entails the definition of a small covariance matrix, automatically\nassigning high importance in the Gaussian product (1) to local KMPs that receive new via-points.\nDefining and adding local via-points. To incorporate kinesthetic feedback from users during task\nexecution, we introduce via-points at specific inputs and outputs, where corrections are made. Let us\nassume a time-driven TP-KMP, where s(p) = t, and \u00a3(p) = x(p) is the end-effector position mapped\nto frame p. We further assume that the robot tracks a reference \u20a4t = \u03bct computed from (1) with a"}, {"title": "4.2 Uncertainty-aware skill extension in regions without demonstrations", "content": "Popular variable impedance schemes found in LfD regulate the robot stiffness by the inverse of\ncovariance matrices [27, 32]. When the latter represent the aleatoric uncertainty, they provide an\nefficient way for robots to be more precise, by being stiffer, where demonstrations showed less\nvariance, following a minimum intervention control principle [32, 35]. In the case of epistemic\nuncertainty, such schemes contribute to improved safety [27]. Kernel hyperparameters are typically\noptimized for the training data, but their choice influences the behavior of the model in regions\nwhere data was not shown. For instance, the kernel length depends on the scale of the input domain,\nbut it also dictates how quickly the epistemic uncertainty increases when moving away from the\ntraining data. Having ways to clearly distinguish between the two types of uncertainty enables\nusers to better design variable impedance strategies. Similarly to [34], the covariance prediction of\nKMP can be decomposed into two terms, corresponding to aleatoric and epistemic components (see\nA.3 for the derivation):"}, {"title": "5 Evaluation", "content": "We evaluate our approach on a torque-controlled 7-DoF robot in an industrial scenario where an\ninner ring of a ball bearing needs to be transferred between two boxes ('box 1' and 'box 2') placed at\ndifferent locations on a workbench. We provide M = 4 demonstrations with different box positions,\nsee Fig. 1-top and Appendix A.4.2. We use a time-driven representation with sh.m = th,m/Tm,\nwhere t is a time step, and learn the end-effector position \u00a7h,m = xh,m. In order to easily re-scale\nthe skill duration, we map all the inputs to the interval [0, 1] by dividing them by the duration of each\ndemonstration Tm. The experiments start with P = 2, with task parameters b(p), A(p) representing\nthe box positions and orientations, respectively. All local KMPs were initialized from GMMs with\n12 components, trained on locally projected data, and N = 500 inputs, equally spread over the\ninput space. We chose a Mat\u00e9rn kernel (v = 5/2) with length scale l = 0.1 and noise variance 1.0\n(see [36]). Other KMP hyperparameters were \u03bb\u2081 = 0.1, X2 = 1, a = 1, chosen empirically. For\ncompleteness, we provide an overview of hyperparameters and a KMP hyperparameter sensitivity\nanalysis in Sections A.4.3 and A.4.4, respectively. In all experiments, via-points are added with"}, {"title": "6 Discussion", "content": "Analysis of the results. Fig. 3\nshows that in Experiment 1 an\nexternal force trigger successfully\nallows for the definition of via-\npoints in the nearest frame (that\nof box 1). Thanks to a small via-\npoint variance, the via-points are\nmapped to the global frame by\n(1) improving generalization qual-\nity by avoiding a collision. Fig. 4\nExperiment 2 illustrates how the\ndefinition of a placeholder frame with large variance can be used in combination with the via-point\ninsertion mechanism from Experiment 1 to introduce behaviors with respect to objects that were\nnot present in the demonstrations. Particularly, one observes that the variance in frame 3 decreases\nafter the via-points are added, with the latter being successfully mapped to the global frame. Finally,\nExperiment 3 shows how our stiffness regulation approach leverages the epistemic uncertainty to\nenhance the robot's compliance beyond the initial set of demonstrations. While one could argue\nthat a similar effect could be achieved by manually lowering the stiffness at t > 1.0, this would\nrequire manually keeping track of the duration of demonstrations, as well as the exact locations of\nnewly added via-points. Our approach automates this by relying on the data properties, increasing\nthe epistemic uncertainty both before and after via-points as well (Fig. 6-right). For more results see\nFig. 13.\nLimitations. In our approach, via-points are added in all Cartesian DoFs, even though a correction\nmight only be required in a subset thereof (e.g. height in the first experiment). Since KMPs allow the\ndefinition of via-point covariances with different diagonal entries, one can, in principle, selectively\nset higher precision only on the DoFs that receive a corrective action (keeping the others as in the\ntraining data). This, however, requires a more complex interaction mechanism which extracts the\nuser intention, a topic that we plan to address in future research. Another possible limitation is"}, {"title": "7 Conclusion", "content": "We presented an interactive imitation learning framework that leverages both local and global mod-\nulations of trajectory distributions to address the problem of generalization in learning from demon-\nstration (LfD). To improve the generalization quality and incrementally add new features to a demon-\nstrated skill, the framework allows the interactive definition of local via-points. This is facilitated\nby a variable impedance scheme that leverages epistemic uncertainties to augment skills beyond\nthe demonstrations. Our results, evaluated on a ring-loading task using a torque-controlled, 7-DoF\nrobot, show that our framework permits users to incrementally build on an initial model of a skill by\ninteractively correcting errors and adding new behaviors in any phase of the task. This work has sig-\nnificant implications for the development of robots that can learn from demonstration and generalize\ntheir skills to new situations, making them more versatile and effective in real-world applications."}, {"title": "A Additional material", "content": "A.1 Key notations\nTable 1 summarizes the key notations used in our framework.\nIEN\n\u039f\u0395\u039d\n\u039c\u0395\u039d\nHEN\nNEN\nSERI\nERO\n{{Sh,m, &h,m}=1}=1\nX\nPEN\np = 1,..., P\nTable 1: Description of key notations\nInput dimension\nOutput dimension\nNumber of demonstrations\nNumber of data points per demonstration\nNumber of gaussians per KMP\nInput variable\nOutput variable\nSet of demonstrations\nEnd-effector position in Cartesian space\nNumber of frames in a TP-KMP\nFrame index"}, {"title": "A.2 Key acronyms", "content": "Table 2 shows a glossary of important acronyms in our approach.\nTable 2: Glossary of important acronyms\nLfD\nDoF\nTP\n(TP)-GMM\n(TP)-GMR\n(TP)-KMP\nDMP\nProMP\n||||||\nLearning from Demonstration\nDegrees of Freedom\nTask Parameterization\n(Task parameterized)- Gaussian Mixture Model [3]\n(Task parameterized)- Gaussian Mixture Regression [3]\n(Task parameterized)- Kernelized Movement Primitive [9]\nDynamic Movement Primitives [4]\nProbabilistic Movement Primitives [16]"}, {"title": "A.3 Decomposition of KMP covariance into a sum of epistemic and aleatoric terms", "content": "We here show that, similarly to [34], the covariance prediction of KMP can be decomposed into two\ndistinct terms, corresponding to aleatoric and epistemic components. Using the Woodbury identity\n(A+UBV)\u22121 = A\u22121 \u2013 A\u00af\u00b9U (B\u22121 + VA\u00af\u00b9U)\u00af\u00b9 VA\u2212\u00b9 with V = U = I, we can re-\nwrite (3) as (omitting a for the sake of the derivation):"}, {"title": "A.4 Experimental details and additional evaluations", "content": "A.4.1 Approach comparison\nIn Table 3 we compare our approach to other task-parameterized approaches on feature level. For\ncompleteness we also compare to non-task-parameterized approaches. However, as shown in [6]\ntask-parameterized approaches are more suitable for adapting to new situations, including those\ninvolving multiple objects. To the best of our knowledge, we are the first to present a task-\nparameterized approach allowing interactive modulations, extending of the skill with new frames\nand the application of local via-points, making it possible to move corrections with their corre-\nsponding objects. We are also unique in providing aleatoric as well as the epistemic uncertainties,\nwhich allow usage where those values are needed individually (like shown in Sec. 4.2). Since our\napproach offers the most features to adapt and extend motion primitives incrementally, we believe it\nshows the most potential for different application scenarios."}]}