{"title": "Audio-FLAN: A Preliminary Release", "authors": ["Liumeng Xue", "Ziya Zhou", "Jiahao Pan", "Zixuan Li", "Shuai Fan", "Yinghao Ma", "Sitong Cheng", "Dongchao Yang", "Haohan Guo", "Yujia Xia", "Xinsheng Wang", "Zixuan Shen", "Chuanbo Zhu", "Xinshen Zhang", "Tianchi Liu", "Ruibin Yuan", "Zeyue Tian", "Haohe Liu", "Emmanouil Benetos", "Ge Zhang", "Yike Guo", "Wei Xue"], "abstract": "Recent advancements in audio tokenization have significantly enhanced the integration of audio capabilities into large language models (LLMs). However, audio understanding and generation are often treated as distinct tasks, hindering the development of truly unified audio-language models. While instruction tuning has demonstrated remarkable success in improving generalization and zero-shot learning across text and vision, its application to audio remains largely unexplored. A major obstacle is the lack of comprehensive datasets that unify audio understanding and generation. To address this, we introduce Audio-FLAN, a large-scale instruction-tuning dataset covering 80 diverse tasks across speech, music, and sound domains, with over 100 million instances. Audio-FLAN lays the foundation for unified audio-language models that can seamlessly handle both understanding (e.g., transcription, comprehension) and generation (e.g., speech, music, sound) tasks across a wide range of audio domains in a zero-shot manner. The Audio-FLAN dataset is available on HuggingFace 1 and GitHub 2 and will be continuously updated.", "sections": [{"title": "Introduction", "content": "Recent advances in large language models and multimodal models have highlighted the effectiveness of instruction tuning for broad generalization [Ouyang et al., 2022, Touvron et al., 2023, Achiam et al., 2023]."}, {"title": null, "content": "Instruction-tuned models can generalize to unseen tasks far better than task-specific counterparts. In the text domain, models like FLAN (Finetuned Language Net) [Wei et al., 2021] demonstrate remarkable zero-shot and few-shot capabilities when fine-tuned on diverse instructions. For example, FLAN (137B parameters) was fine-tuned on 60 NLP tasks and outperformed even larger models, like the 175B GPT-3 [Brown et al., 2020], on many unseen tasks. Similarly, LIMA [Zhou et al., 2024], which used only 1,000 curated examples, achieved results preferred over much larger models, showing that minimal high-quality instruction data can significantly improve a model's ability to follow complex queries. In the vision domain, unified models like Chameleon [Team, 2024] and Janus-Pro 7B [Wu et al., 2024] have demonstrated strong performance by handling both understanding and generation tasks in a single system, outperforming specialized models in image captioning, visual question answering, and image generation. In contrast, the audio domain 3 still lags behind, with audio understanding and generation often treated as separate tasks.\nThis gap between modalities highlights a critical limitation: audio-language models still lack the unified modeling and generalization capabilities that are now common in NLP and computer vision. Despite the wide variety of audio tasks (such as speech transcription, speaker identification, emotion recognition, sound event recognition, music understanding, and text-to-speech generation), there is no \"audio GPT\" or \"audio foundation model\" that can seamlessly switch between understanding and generating audio across speech, music, and audio domains. For example, models like Musilingo [Deng et al., 2023] focus on music understanding, while LTU (Listen, Think, Understand) [Gong et al., 2023b] and Audio-Flamingo [Kong et al., 2024] focus on the audio domain. The SALMONN [Tang et al., 2023] and Qwen-Audio series [Chu et al., 2023] are designed for understanding speech, sound, and music, but lack generation capabilities. On the other hand, UniAudio [Yang et al., 2023] supports audio generation, but it is limited to 11 tasks spanning speech, sound, music, and singing, each with specific task identifiers.\nCurrently, no audio model exhibits the broad zero-shot generalization seen in text and vision models. Recent benchmarks highlight these limitations. Dynamic-SUPERB [yu Huang et al., 2024], a comprehensive benchmark with 33 speech tasks for speech models, shows that unlike text models, speech models remain confined to narrow tasks. It finds that systems perform well on seen tasks but struggle with unseen tasks, revealing poor zero-shot generalization. Dynamic-SUPERB Phase-2 [Huang et al., 2024], which has expanded to include 180 understanding tasks, reports that while recent models perform well on specific tasks, they struggle with generalization, underscoring the need for more research on developing universal models. Similarly, the MMAU benchmark [Sakshi et al., 2024], which covers speech, environmental sounds, and music, shows that even top models like Gemini-Pro v1.5 [Team et al., 2024] and Qwen2-Audio [Chu et al., 2024] only achieve about 52.97% accuracy. This stark contrast with text models underscores the underexplored potential of audio-language models for general auditory intelligence. Additionally, the lack of comprehensive evaluation frameworks further hinders progress. AIR-Bench [Yang et al., 2024], the first generative audio-language comprehension benchmark, reveals significant limitations in current models' ability to follow instructions across tasks. In summary, audio-language research is still in an early stage, similar to the pre-GPT-3/FLAN era of NLP: while there are task-specific models, there is no unified model with broad, zero-shot capabilities."}, {"title": null, "content": "A key challenge in the audio domain is the lack of large-scale, diverse instruction-tuning datasets tailored to audio-language tasks. While NLP has benefited from extensive multi-task instruction datasets like Super-NaturalInstructions [Wang et al., 2022a] with 1,616 tasks and vision-language models use resources like LLaVA [Liu et al., 2024] and InstructBLIP [Dai et al., 2023], the audio field lacks comparable datasets in scale or diversity. Some efforts, like GAMA [Ghosh et al., 2024] synthesize an instruction dataset, called CompA-R, for audio reasoning, but they focus mainly on narrow tasks like question-answering and captioning. Other works have used GPT-4 or LLMs to generate instruction data from existing speech corpora, e.g., LTU [Gong et al., 2023b] and DeSTA [Gong et al., 2023a], but these are fragmented, limited in scope, and often biased by the prompts used. No existing dataset spans the breadth of audio content, including speech, music, and sound, with instructions. In short, the audio domain lacks a \u201cFLAN\u201d equivalent\u2014a consolidated, high-quality instruction dataset to unify myriad audio tasks. This absence of data is a key reason we do not yet have audio models with the generalization of GPT-4 or Chameleon. Even as benchmarks like the Dynamic-SUPERB series and AIR-Bench call for instruction-following audio models, researchers struggle to train such models without a large, diverse training corpus tailored to audio-language understanding and generation.\nIn this work, we introduce Audio-FLAN, a preliminary attempt to bridge this data gap and enable truly unified audio-language modeling. Audio-FLAN (Preliminary Release) is a large-scale, diverse instruction-tuning dataset for both understanding and generation tasks across speech, music, and audio, constructed by collecting and standardizing nearly all publicly available academic audio datasets into a common instruction-based format. By normalizing the format of these heterogeneous datasets, we provide each audio sample with one or more accompanying instructions (or question/prompt) and the expected output (transcription, description, answer for understanding tasks, or an audio clip for generative tasks). Crucially, Audio-FLAN is designed to support both pre-training and supervised fine-tuning (SFT) of models for unified audio-language tasks. We envision that models trained on Audio-FLAN dataset will be capable of both audio understanding (e.g., transcribing and comprehending audio, answering questions about it) and audio generation (e.g., following instructions to produce speech, music and sounds) within one unified framework. In other words, Audio-FLAN lays the groundwork for an audio equivalent of multimodal foundation models an audio-language model that can listen, understand, speak, sing and compose in a general way.\nTo our knowledge, Audio-FLAN is the first comprehensive compilation that combines diverse audio datasets into a single, instruction-driven corpus of considerable scale. It includes approximately 80 tasks and over 100 million instances, significantly surpassing prior efforts in both quantity and diversity. We aim for Audio-FLAN to achieve for audio what FLAN and other instruction-tuned models have accomplished for text-enabling models to generalize across a wide range of audio tasks in a zero-shot manner and follow open-ended instructions related to audio content. The preliminary release of Audio-FLAN is only the beginning: we invite the research community to build on this resource, contribute new tasks (similar to Dynamic-SUPERB Phase-2), and explore unified models for speech, music, and audio. By unifying both audio understanding and generation, Audio-FLAN paves the way toward foundational models that can hear and generate audio as flexibly and broadly as language models process text."}, {"title": "Audio-FLAN Datasets Construction", "content": "Figure 1 illustrates the pipeline for constructing the Audio-FLAN dataset. We first collect the publicly released datasets and use their original labels, or manually processed labels, to determine the tasks that can be performed based on the task definitions. Next, instructions are generated and structured using task templates, which guide the format and content of instruction, input, and output. To increase the diversity of the instruction set, we apply a self-instruct-like method [Wang et al., 2023], where the instructions are varied through tools like LLaMA and GPT, which allow for the creation of multiple variations for each task and instance. These varied instructions are then validated to ensure they meet the required standards before being integrated into the dataset."}, {"title": "Task Category", "content": "We classify tasks into Major Tasks and Minor Tasks following a hierarchical structure based on the scope and specificity of the tasks within the broader domains of speech, music, and audio, as shown in Table 1.\n\u2022 Major Tasks represent broad categories that encompass a variety of related activities within each domain. For example, in the speech domain, major tasks include Speech Recognition, Speech Generation, and Speech Enhancement, which cover the general areas of recognizing spoken words, generating speech, and improving speech quality, respectively.\n\u2022 Minor Tasks are specific subcategories under each major task, providing more focused and detailed areas of work. For example, under Speech Recognition, the minor tasks include Automatic Speech Recognition, Dialect Automatic Speech Recognition, and Phonetic Recognition, each representing a specialized area within the overarching task of recognizing speech. Similarly, under Speech Generation, tasks like Text to Speech, Voice Conversion, and Speech to Speech Translation address more specific aspects of generating speech."}, {"title": "Task Instruction Template", "content": "The instruction data we aim to generate consists of a collection of instructions {Ii}, each describing a specific task i in natural language. For each task i, there are ni \u2265 1 input-output pairs {(Xt,i, Yt,i)}i=1. Once the tasks to be covered by the dataset are determined, we process the data into three core components: instruction, input, and output, all formatted in JSONL (JSON Lines) format. The instruction serves as a concise description of the task, guiding the model on the expected input and the type of output to generate. For tasks that involve understanding, the output is text, while for tasks focused on generation, the output is typically audio. The input can be audio, text, or a combination of both, depending on the task. Formally, given this structured data, a model M is expected to generate the appropriate output based on the task instruction and the corresponding input: M(Ii, Xt,i) = Yt,i, for i \u2208 {1, ..., ni}.\nIn the speech domain, the task of Speech-to-Text Translation involves both text and audio as input (e.g., an audio recording of speech and the corresponding transcription in target language), and the output is text, which is the translated text in a different language. In the music domain, the task of Text-guided Music Generation uses a combination of text and audio as input (e.g., a description of the type of music and a short melody clip), and the output is audio, which is a generated music track that matches the input description and melody. In the audio domain, tasks like Audio Super-resolution can take a combination of low-resolution audio and textual description of the expected quality improvements as input, and the output is high-resolution audio that enhances the quality of the input signal.\nTo generate the task instructions {Ii}, we initially employ template-based instructions. These instructions are human-written, task-specific descriptions that explicitly define the task. For example, the instruction for the Speech-to-Text Translation task could be \"Please translate the speech into the text in Chinese.\". For Text-guided Music Generation, the instruction might be \"Please continue the audio music prompt based on"}, {"title": "Instruction Variation", "content": "While fixed, template-based instructions provide consistency in task execution, they inherently constrain flexibility and creativity. This rigidity can hinder the model's ability to adapt to diverse and nuanced task descriptions. To mitigate these limitations and enhance the diversity and creativity of the instructions, we introduce an approach that expands template-based instructions into a broader set of variations using advanced language models, like LLaMA [Touvron et al., 2023]. By leveraging the generative power of these models, we can produce multiple distinct variations for each task instruction template, thereby augmenting the model's capacity to handle a wide array of task descriptions.\nThe process of instruction variation follows a three-step pipeline, inspired by the self-instruct approach [Wang et al., 2023], designed to systematically enhance instruction diversity. These steps include: (1) initializing the variation seed pool, (2) generating new diverse instructions, and (3) validating the generated instructions."}, {"title": "Audio-FLAN Dataset", "content": "Figure 2 illustrates the structure of the Audio-FLAN dataset, which spans a diverse range of tasks and instances. It is organized into 23 major tasks and 80 minor tasks from 52 released datasets, totaling 108.5M instances. These tasks are divided into two primary categories: understanding and generation.\n\u2022 Understanding: This category consists of 16 major tasks and 51 minor tasks with 51 open-sourced datasets, amounting to 62.44M instances. The understanding tasks are further divided into three domains:\nSpeech: 6 major tasks and 20 minor tasks, with 24 datasets and 57.42M instances.\nMusic: 5 major tasks and 21 minor tasks, with 19 datasets and 1.46M instances.\nAudio: 5 major tasks and 10 minor tasks, with 8 datasets and 3.56M instances.\n\u2022 Generation: This category includes 7 major tasks and 29 minor tasks with 31 publicly available datasets, with a total of 46.06M instances. The generation tasks are categorized as follows:\nSpeech: 2 major tasks, 14 minor tasks, with 12 datasets and 43M instances.\nMusic: 2 major tasks, 7 minor tasks, with 13 datasets and 0.71M instances."}, {"title": "Statistics of Task", "content": "The Audio-FLAN dataset, spanning across the speech, music, and audio domains, is summarized in Table 2. The dataset consists of 23 major tasks and 80 minor tasks across these domains, totaling 108.5M instances. These tasks cover a wide range of applications and modalities, integrating both understanding and generation tasks across various domains. The dataset's diversity is further enhanced by the variety of input-output formats, including audio, text, and multimodal combinations such as audio and text, allowing it to represent complex and realistic scenarios.\nSpeech Domain: The Speech domain encompasses 8 major tasks, including Speech Recognition, Speech Generation, and Paralinguistic Attribute Recognition, addressing both understanding and generation tasks. The Speech domain includes 34 minor tasks, with a total of 100.42M instances, showcasing a comprehensive"}, {"title": "Distribution of Audio Attributes", "content": "Each subdomain in the audio field encompasses a wide range of attributes. Specifically, the speech domain captures semantic content, speaker identity, and critical paralinguistic features such as emotion, language, accent, age, and more. The music domain contains a variety of musical attributes, including different instruments, timbres, techniques, and structures. Meanwhile, the audio domain covers diverse sounds, including events, animals, scenes, and even speech or music. To explore the different audio attributes in the Audio-FLAN dataset, we analyze the instance distribution of tasks related to these attributes across the speech, music, and audio domains, as shown in Figure 3.\nSpeech Domain: As shown in Figure 3 (a), in the speech domain, the most prominent features are content (35.5%) and language (32.1%). content-related tasks, like Automatic Speech Recognition (ASR), focus on transcribing spoken language into text, while language-related tasks, such as Language Identification and Speech to Text Translation, handle the translation and identification of speech across languages.\nAdditional tasks in the speech domain cover features like gender (8.8%), which identifies the speaker's gender, and age (5.7%), dialect (5.5%), and distortion (4.2%) tasks, such as Denoising and Dereverberation, which improve speech quality. Smaller, yet significant contributions come from tasks related to emotion, accent, and device (1.1%), contributing to a more nuanced understanding of speech signals.\nMusic Domain: As shown in Figure 3 (b), the music domain's most prominent features are instrumental (17.6%) and timbre (12.9%). instrumental tasks, like Instrument Classification and Beat-level Instrument Recognition, focus on identifying and analyzing different musical instruments. timbre is related to the tonal quality of sound, and tasks like Singing Voice Conversion capture the unique characteristics of sound sources.\nThe domain also includes ethnomusicology (12.3%), which helps the model understand diverse cultural music, and tasks like Text-to-Music Generation and Text-guided Music Continuation. vocals (19.4%) and melody (5.3%) tasks like Vocal Technique Classification and Melody Extraction focus on analyzing vocal and melodic elements in music. Additional tasks cover pitch (5.1%), key (4.9%), and chord (2.2%), focusing on musical structure and harmony.\nAudio Domain: As shown in Figure 3 (c), the audio domain is dominated by scene (33.4%), which represents environmental sounds, aiding in contextualizing audio. Tasks like Acoustic Scene Classification categorize different environments based on their audio characteristics. event (22.2%) and speech (20.3%) features involve tasks like Sound Event Recognition and Speech Detection, which identify specific events and speech elements in general soundscapes.\nAdditionally, the others category (24.1%) includes music (28.3%), object (26.1%), and human (25.3%) features, covering tasks like Audio Event Detection, Audio Source Separation, and Speech and Non-speech Detection, providing a comprehensive approach to general audio processing and recognition.\nIt is important to note that each instance may contain multiple features. As a result, the statistics presented reflect the frequency of feature occurrences rather than the absolute count of instances associated with each feature. This distribution highlights the rich diversity of attributes within both the speech, music and audio domains, encompassing foundational tasks such as speech recognition and speaker identification, as well as more specialized areas like noise reduction, environmental sound recognition, and music analysis. The broad range of features and tasks in these domains supports the development of unified models that can be generalized across various audio-language tasks. This diversity enables models to adapt to a wide variety of contexts, enhancing their zero-shot generalization capabilities across different types of audio with diverse attributes."}, {"title": "Conclusion and Discussion", "content": "The Audio-FLAN dataset represents a groundbreaking contribution to the audio domain by enabling instruction-tuning for both understanding and generation tasks across the speech, music, and audio domains. This pioneering dataset consists of 23 major tasks and 80 minor tasks, with 16 major tasks dedicated to understanding and 7 major tasks focused on generation, totaling 108.5 million instances. By covering a wide array of tasks from speech recognition and emotion detection to music generation and audio event recognition, the Audio-FLAN dataset provides a comprehensive foundation for developing unified models that can handle both understanding and generation across multiple audio domains. This dataset is designed to support instruction-tuning, empowering models to follow complex audio instructions with minimal task-specific data. It paves the way for zero-shot generalization, enabling models to perform well on unseen tasks within and across domains, much like the advancements seen in text and vision models.\nThe Audio-FLAN dataset, while a major step towards unifying understanding and generation tasks across the speech, music, and audio domains, exhibits an imbalance in instance distribution. Understanding tasks, particularly in the speech domain, dominate the dataset, benefiting from well-established datasets and easier labeling. In contrast, generation tasks, such as text-to-audio or music generation, are more complex and less represented. This imbalance results in a greater number of instances in the speech domain, while the music and audio domains have fewer. This skew may lead to models being biased toward understanding tasks, potentially impacting their generalization to generation tasks or underrepresented domains.\nFuture work should focus on balancing the distribution of tasks across domains, ensuring a more even representation between understanding and generation tasks, especially in the music and audio domains. Additionally, expanding the dataset to include more tasks and incorporating additional datasets will strengthen the audio domain's instruction-tuning capabilities, enhancing the development of unified models that can handle both understanding and generation tasks with improved zero-shot performance. Furthermore, integrating conversational data will be crucial for equipping models with the ability to engage in dynamic, real-time dialogue, broadening the dataset's applicability to intelligent virtual agents and multimodal interaction systems."}, {"title": "Appendix", "content": "A Appendix"}, {"title": "Task Definition", "content": "A.1 Task Definition\nSpeech Domain\nHere, we provide a detailed list of each minor task definition for the speech, music, and audio domains,\nrespectively.\nSpeech Recognition (3 minor tasks)\n1. Automatic Speech Recognition: transcribing speech into text.\n2. Dialect Automatic Speech Recognition: Automatic Speech Recognition adapted for dialectal variations.\n3. Phonetic Recognition: identifying and classifying the smallest units of sound in spoken language, known\nas phonemes.\nSpoken Language Understanding (2 minor tasks)\n1. Intent Classification: determining the purpose behind a user's spoken input.\n2. Speech to Text Translation: translating spoken language into written text in a different language.\nParalinguistic Attribute Recognition (7 minor tasks)\n1. Gender Recognition: classifying the biological gender of a speaker based on acoustic features of their\nvoice. This task leverages acoustic features of speech, such as pitch, formant frequencies, and speech patterns,\nwhich tend to differ between male and female speakers due to physiological differences in the vocal tract and\nlarynx.\n2. Age Prediction: estimating the age of a speaker based on the acoustic properties of their voice. This task\nutilizes various speech features, such as pitch, speaking rate, formant frequencies, and spectral characteristics,\nwhich can provide cues about the speaker's age.\n3. Emotion Recognition: identifying and classifying the emotional state of a speaker based on their vocal\nexpressions.\n4. Accent Recognition: identifying the regional or cultural accent of a speaker based on their speech\ncharacteristics.\n5. Spoken Paragraph Recognition: determining whether two audio recordings contain the same spoken\nparagraph by analyzing the linguistic content.\n6. Language Identification: determining the language spoken from a given audio sample.\n7. Dialect Identification: determining the specific dialect or regional variation of a language spoken in a\ngiven audio sample.\nSpeaker Recognition (4 minor tasks)"}, {"title": null, "content": "1. Speaker Verification: verifying a speaker's identity by comparing their voice to a pre-recorded voiceprint\n(voice model) of the claimed identity. This process is used to authenticate or verify a speaker's identity,\nensuring that the person speaking is who they claim to be. It includes text-independent and text-dependent\nspeaker verification.\n2. Speaker Diarization: identifying \"who spoke when\" in an audio recording containing multiple speakers.\nThis task segments an audio stream into homogeneous regions according to the speaker identity, effectively\nattributing each segment of speech to its corresponding speaker.\n3. Speaker Extraction: extracting the speech of a target speaker from a mixture of sounds that may include\nmultiple speakers and background noise.\n4. Speaker Identification: identifying a speaker from a set of known speakers based on their voice character-\nistics.\nSpeech Caption (1 minor task)\n1. Speech Caption: generating synchronized text captions from spoken language.\nSpeech Detection (3 minor tasks)\n1. Deepfake Detection: detecting whether an audio clip has been artificially manipulated or synthesized\nusing AI techniques, such as voice cloning or deepfake speech generation.\n2. Vocoder Type Classification: identifying and categorizing the type of vocoder used in a given speech\nsignal.\n3. Vocoder Type Classification: identifying the device used to record a given speech segment based on its\nacoustic features.\nSpeech Enhancement (5 minor tasks)\n1. Denoising: removing unwanted noise from an audio signal to enhance the clarity and quality of the speech.\nThis task involves distinguishing between the speech signal and the background noise, which can include\nsounds like traffic, machinery, conversations, or other environmental noises.\n2. Dereverberation: reducing or eliminating the effects of reverberation from an audio signal. Reverberation\noccurs when sound waves reflect off surfaces such as walls, ceilings, and floors, causing the original speech\nsignal to be combined with multiple delayed copies of itself.\n3. Declipping: restoring audio signals that have been distorted due to clipping. Clipping occurs when the\namplitude of an audio signal exceeds the maximum limit that a recording or playback system can handle,\ncausing the peaks of the waveform to be \"clipped\" off.\n4. Speech Bandwidth Extension: enhancing narrowband speech quality by extending its frequency range.\nNarrowband speech often lacks the higher frequencies that contribute to the naturalness and clarity of speech.\n5. Signal-to-noise Ratio Estimation: quantifying the ratio of the power of a signal to the power of background\nnoise. This task provides a quantitative measure of the quality of a signal.\nSpeech Generation (9 minor tasks)"}, {"title": null, "content": "1. Text to Speech: converting written text into spoken words. It involves synthesizing speech that is natural\nand understandable, enabling computers to \"read\" text aloud.\n2. Zero-shot Text to Speech/Voice Cloning: generating synthetic speech for voices or styles it has never\nencountered during training.\n3. Emotional Text to Speech: synthesizing speech with emotional nuances. The goal is to produce speech\nthat not only conveys the content of the text but also expresses specific emotions, making the synthetic voice\nmore engaging and human-like.\n4. Zero-shot Emotional Text to Speech: generating emotional speech that adapts to an unseen speaker's\nvoice while rendering specified emotions.\n5. Descriptive Speech Synthesis: generating synthetic speech that not only replicates the spoken content\nbut also conveys descriptive information about the context of the speech, such as emotions, tone, or other\nparalinguistic features.\n6. Spontaneous Text to Speech: generating synthetic speech that mimics the characteristics of spontaneous\nunscripted human speech. Spontaneous TTS aims to replicate the naturalness, variability, and informal\naspects of everyday conversational speech. This includes features such as hesitations, fillers (e.g., \"um,\"\n\"uh\"), varying speech rates, and natural prosody changes.\n7. Voice Conversion: converting one speaker's voice to resemble another's while preserving linguistic\ncontent and prosody.\n8. Emotion Conversion: transforming the emotional tone of a spoken utterance from one emotion to another\nwhile preserving the linguistic content.\n9. Speech to Speech Translation: converting spoken language in one language directly into spoken language\nin another language.\nMusic Domain\nGlobal MIR (10 minor tasks):\n1. Key Detection: recognizing the key signature of the given music.\n2. Scale Recognition: recognizing the scale of the given music.\n3. Music Tagging: assigning descriptive tags to audio files, such as genre, style, tempo, key, artist, and\nemotion.\n4. Genre Classification: categorizing the music into certain genres.\n5. Emotion Classification: recognizing emotion categories from the music.\n6. Pitch Classification: classifying the pitch of the given audio.\n7. Instrument Classification: identifying all existing instruments from the music.\n8. Vocal Technique Classification: detecting the playing techniques used in the vocal music."}, {"title": null, "content": "9. Instrumental Technique Classification: detecting the playing techniques used in the instrumental music.\n10 Artist Identification: identifying the relevant artists of a piece of music, given a set of artists as the options.\nSequential MIR (3 minor tasks)\n1. Beat Tracking: detecting and aligning beats of a music excerpt.\n2. Chord Estimation: estimating the chords sequence at each time step of a music excerpt.\n3. Progression Extraction: extracting the chord progression represented by chord number sequence.\nSingle Music Reasoning (2 minor tasks)\n1. Beat-level Instruments Recognition: recognizing the instruments from a certain beat or a certain segment.\n2. Beat-level Pitch Estimation: estimating the pitch of a certain beat or segment.\nMultiple Music Reasoning (5 minor tasks)\n1. Tempo Comparison: comparing the tempo characteristics between two music excerpts.\n2. Instruments Comparison: comparing instruments of two music excerpts.\n3. Key Comparison: comparing keys of two music excerpts.\n4. Instrumental Technique Comparison: comparing playing techniques of two music excerpts.\n5. Emotion Comparison: comparing emotions of two excerpts.\nMusic Caption (1 minor task)\n1. Music Caption: generating textual descriptions for a piece of music.\nMusic Separation (2 minor tasks)\n1. Melody Extraction: extracting the melody at each time step from a music excerpt.\n2. Text-guided Source Separation: separate certain tracks from a piece of mixed music with the text\ninstruction.\nMusic Generation (5 minor tasks)\n1. Text-to-Music Generation: generating the music given the text caption.\n2. Text-guided Music Continuation: extending a given initial audio segment based on a textual description\nof musical characteristics while ensuring continuity and coherence.\n3. Lyrics-to-song Generation: composing a song with the vocal track and instrumental track based on the\ngiven lyrics.\n4. Singing Voice Synthesis: synthesizing the voice given the pitches and lyrics sequence.\n5. Singing Voice Conversion: transforming the vocals (including the lyrics and melody) of singer A(source\nvocals) to sound like Singer B (target singer)."}, {"title": null, "content": "Audio Event Recognition (4 minor tasks)\nAudio Domain\n1. Sound Event Sequence Recognition: identifying and sequencing various sounds in an audio stream.\n2. Sound Event Recognition: detecting and identifying a particular sound in audio data.\n3. Sound Event Detection: determining when a specific sound occurs within an audio clip.\n4. Acoustic Scene Classification: classifying an audio clip according to the environment it represents (e.g.,\npark, street).\nAudio Caption (1 minor task)\n1. Audio Caption: generating natural language descriptions that summarize or explain the content of an\naudio clip.\nAudio Advanced Understanding (1 minor task)\n1. Sound Event Understanding: extracting meaningful information from multiple audio signals (e.g. What is\nhappening in the given audio).\nAudio Detection (2 minor tasks)\n1. Deepfake Audio Detection: identifying synthetic or manipulated audio content.\n2. Voice Activity Detection: identifying segments where human speech is present in the given audio.\nAudio Classification (2 minor tasks)\n1. Speech, Silence, Music and Noise Classification: distinguishing between music, speech, and various\ntypes of noise.\n2. Speech and Non-speech Detection: identifying segments which contain speech or non-speech of the given\naudio.\nAudio Enhancement (2 minor tasks)\n1. Audio Inpainting: filling in missing parts of an audio signal.\n2. Audio Super-resolution: improving the perceptual quality of an audio signal by increasing its resolution.\nAudio Separation (3 minor tasks)\n1. Text-guided Audio Source Separation: isolating specific sound sources from an audio clip based on text\ninput.\n2. Label-querying Sound Extraction: extracting sounds belonging to a predefined category from an audio\nmixture, given a textual label\n3. Audio-querying Sound Extraction: isolating sound sources from an audio mixture based on an example\naudio query.\nAudio Generation (3 minor tasks)"}, {"title": null, "content": "1. Text-guided Audio Generation: creating audio based on a textual description.\n2. Time-grounded Text-to-audio Generation: generating audio content that aligns with time-specific textual\ndescriptions.\n3. Audio Continuation: extending an audio clip by generating additional content that seamlessly continues\nthe original.\nA.2 Datasets for Each Task\nHere, we present the datasets associated with each minor task."}, {"title": null, "content": "A.3 Instruction Template\nHere we provide the complete task instruction template in JSONL format with all fields.\nSpeech-to-Text Translation\n{\"instruction\": \"Please translate the speech into the text in\nEnglish.\"", "n\"input\"": "<|SOA|>Speech_Audio<|EOA|>\"", "n\"output\"": "Nevertheless", "experiencing\n.\",\n\"uuid\"": "UUID\"", "n\"split\"": ["train\""], "n\"task_type\"": {"n},\n\"major\"": ["Spoken Language Understanding\""], "n\"minor\"": ["Speech-to-text Translation\""], "n\"U/G\"": ["understanding\""], "n\"unseen\"": false, "n},\n\"domain\"": "speech\"", "n\"source\"": ["unknown\""], "n\"other\"": null}, "Continuation\n{\"instruction\"": "Please continue the audio music prompt based on\nthe given text description\""}, {"n\"input\"": "This is a Carnatic music piece set in the atana raga. It\nfollows the 5/8 meter and is composed in the khandaChapu taala\n. The lead instrument featured in this performance is vocal", "prompt": "SOA|>Music_Audio<|EOA|>\"", "n\"output\"": "audio: <|SOA|>Musi_Audio<|EOA|>\"", "n\"uuid\"": "UUID\"", "n\"split\"": ["test\""], "n\"task_type\"": {"n\"major\"": ["Music Generation\""], "n\"minor\"": ["Text-guided Music Continuation\""], "n\"U/G\"": ["generation\""], "n\"unseen\"": false, "n},\n\"domain\"": "music\"", "n\"source\"": ["unknown\""], "n\"other\"": null}, "Super-resolution\n{\"instruction\"": "Please increase the resolution of the given audio\nsignal to 32k Hz.\""}, {"n\"input\"": "audio: <|SOA|>Sound_Audio<|EOA|>.\"", "n\"output\"": "n\"uuid\": \"UUID\"", "n\"split\"": ["train\""], "n\"task_type\"": {"n\"major\"": ["Sound Generation\""], "n\"minor\"": ["Sound Super-resolution\""], "n\"U/G\"": ["generation\""], "n\"unseen\"": false, "n},\n\"domain\"": "audio\",\n\"source"}}]}