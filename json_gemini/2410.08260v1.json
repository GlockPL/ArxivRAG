{"title": "KOALA-36M : A LARGE-SCALE VIDEO DATASET\nIMPROVING ConsistENCY BETWEEN FINE-GRAINED\nCONDITIONS AND VIDEO CONTENT", "authors": ["Qiuheng Wang", "Yukai Shi", "Jiahao Wang", "Jiarong Ou", "Rui Chen", "Ke Lin", "Boyuan Jiang", "Haotian Yang", "Mingwu Zheng", "Xin Tao", "Fei Yang", "Pengfei Wan", "Di Zhang"], "abstract": "As visual generation technologies continue to advance, the scale of video datasets\nhas expanded rapidly, and the quality of these datasets is critical to the perfor-\nmance of video generation models. We argue that temporal splitting, detailed\ncaptions, and video quality filtering are three key factors that determine dataset\nquality. However, existing datasets exhibit various limitations in these areas. To\naddress these challenges, we introduce Koala-36M, a large-scale, high-quality\nvideo dataset featuring accurate temporal splitting, detailed captions, and superior\nvideo quality. The core of our approach lies in improving the consistency be-\ntween fine-grained conditions and video content. Specifically, we employ a linear\nclassifier on probability distributions to enhance the accuracy of transition detec-\ntion, ensuring better temporal consistency. We then provide structured captions\nfor the splitted videos, with an average length of 200 words, to improve text-video\nalignment. Additionally, we develop a Video Training Suitability Score (VTSS)\nthat integrates multiple sub-metrics, allowing us to filter high-quality videos from\nthe original corpus. Finally, we incorporate several metrics into the training pro-\ncess of the generation model, further refining the fine-grained conditions. Our\nexperiments demonstrate the effectiveness of our data processing pipeline and the\nquality of the proposed Koala-36M dataset. Our dataset and code will be released\nat https://koala36m.github.io/.", "sections": [{"title": "INTRODUCTION", "content": "Generative AI, particularly video generation tasks, has recently garnered significant interest from\nresearchers. These tasks involve generating high-quality videos from textual descriptions or images.\nA critical factor in the success of these models is the quality of the datasets used for training. Several\nopen-source datasets (e.g. Panda-70M(Chen et al., 2024b), MiraData (Ju et al., 2024), OpenVid (Nan\net al., 2024), and VidGen (Tan et al., 2024)) have been introduced, each carefully selecting data\nsources and applying various evaluation metrics for video filtering. Moreover, innovative approaches\nhave been employed in the video captioning process, such as the multi-modal caption model (Chen\net al., 2024b) or structured captions (Ju et al., 2024).\nDespite the success of the data processing pipelines introduced by previous datasets, we argue that\nthe core challenge lies in establishing accurate and fine-grained conditioning for video data, which\nis crucial for both reducing the complexity of the training process and improving the quality of the\ngenerated outputs. To achieve this, we believe there are three key issues that need to be addressed:\nFirst, the alignment between text and video semantics is essential. Unlike video question answer-\ning tasks, where captions are primarily driven by specific question-based details, video generation\nrequires captions that are directly tied to the visual content itself. Due to the infinite granularity of\nvisual signals, this necessitates captions that are rich and detailed. Furthermore, raw video data often\ncontains complex transitions, adding additional challenges in ensuring the accuracy of captions.\nSecond, the effective evaluation and filtering of low-quality data remains underexplored. Low-\nquality video data, such as poor visual quality or excessive artificial effects, can impede the training\nprocess. However, accurately assessing and filtering such data presents an ongoing challenge. Ex-\nisting methods typically rely on manually selected quality metrics and heuristic threshold-based\nfiltering, which are often designed for other tasks and may not align with the specific requirements\nof video generation. As a result, these approaches may not effectively ensure the desired data quality\nfor training.\nThird, even with data filtering processes in place, the videos within the dataset still vary in quality,\nwith each video potentially exhibiting different strengths and weaknesses (e.g., one video may have\nlower clarity but better aesthetic appeal). Training with such heterogeneous data in the same manner\nmay introduce ambiguity for the model, hindering its ability to learn effectively.\nTo address these issues, we present Koala-36M, a large-scale high-quality video dataset with more\naccurate video splitting, detailed captions, better data filtering methods and metric conditions. As\nvideo content reaches considerable quality, the consistency between fine-grained conditions and\nvideo content determines the performance of generation models. We propose a more refined data\nprocessing pipeline based on this key insight. Since accurate video splitting leads to better temporal\nconsistency, we first employ a linear classifier on probability distributions to enhance the accuracy\nof transition detection. Then We generate structured captions for the splitted video clips, with an\naverage length of 200 words, to improve text-video alignment. Sequentially, to prevent the erroneous\ndeletion of high-quality data during filtering, we train a network to predict Video Training Suitability\nScore (VTSS) on human-aligned datasets to model the joint distribution of sub-metrics. This network\ntakes videos and sub-metrics as input, and outputs a single value called Video Training Suitability\nScore as the only metric to filter data. Additionally, we introduce data metrics as extra conditions\n(Metric Conditions) into the generation model during training, helping model distinguish data with\ndifferent quality and further improving the consistency between fine-grained conditions and video\ncontent, which results in better performance and controllability of the generation model.\nTo further validate Koala-36M and our data processing pipeline, we train video generation models\non different datasets. Both the dataset benchmark and the performance of the video generation\nmodel demonstrate the advantage of the Koala-36M dataset. We perform more ablation studies to\ndemonstrate effectiveness of our data processing pipeline.\nOur contributions can be summarized as follows:\n\u2022 We present a large-scale high-quality dataset called Koala-36M, with accurate video split-\nting, detailed captions and higher-quality video content.\n\u2022 We propose a refined data processing pipeline to further improve the consistency between\nfine-grained conditions and video content, including transition detection methods, struc-\ntured caption system, Video Training Suitability Score and metric conditions."}, {"title": "RELATED WORK", "content": "Recent advancements in diffusion models have driven the evolution of image generation models\ninto video generation models. In the field of text-to-video (T2V) generation, significant efforts have\nbeen made to develop large-scale T2V models, trained on extensive datasets using traditional U-Net-\nbased diffusion architectures (Zeng et al., 2024; Clark & Jaini, 2024; Ge et al., 2023; Yu et al., 2023;\nKhachatryan et al., 2023) and Transformer-based (DiT) architectures (Ma et al., 2024; Chen et al.,\n2023b; Lu et al., 2023; Chen et al., 2024a; Xing et al., 2024). The success of these video generation\nmodels heavily depends on the quality of the video-text datasets."}, {"title": "VIDEO DATASETS", "content": "While several video datasets (Caba Heilbron et al., 2015; Anne Hendricks et al., 2017; Rohrbach\net al., 2015; Zhou et al., 2018; Xu et al., 2016; Wang et al., 2023b; Sanabria et al., 2018; Wang\net al., 2023a; Chen et al., 2023a) have been applied to tasks such as action recognition, video un-\nderstanding, visual question answering (VQA), and video retrieval, there remains an urgent need for\na high-quality, open-source dataset specifically tailored for training video generation models, pro-\nviding rich video-text pairs. Datasets such as YouCook2 (Zhou et al., 2018), VATEX (Wang et al.,\n2019), and ActivityNet (Caba Heilbron et al., 2015) offer high-quality human caption annotations.\nAnother set of datasets, including Miradata (Ju et al., 2024), VidGen-1M (Tan et al., 2024), and\nOpenVid-1M (Nan et al., 2024), automatically generate high-quality captions and filter data using\nmanually selected thresholds on multiple dataset metrics.\nHowever, these datasets are insufficient in size to support the training of large models. Datasets, in-\ncluding YT-Temporal-180M (Zellers et al., 2021), HD-VILA-100M (Xue et al., 2022), ACAV (Lee\net al., 2021), etc., contain hundreds of millions of video-text pairs, but their captions are automati-\ncally generated via speech recognition, leading to subpar quality. Panda70M (Chen et al., 2024b),\nthe largest publicly accessible video-text dataset, has become a popular choice for video genera-\ntion due to its scale and considerable quality. However, its quality still needs further improvement.\nSpecifically, the captions in Panda-70M often provide simplistic, incomplete descriptions of video\ncontent, and the frequent transitions in the training videos can result in semantic inconsistencies,\npotentially leading to undesired or uncertain transitions in the generated videos."}, {"title": "VIDEO DATA CURATION", "content": "As models continue to scale up in size, effective data cu-\nration is of paramount importance (Zhou et al., 2023),\nparticularly in the formulation of a well-suited training\ndataset. This is crucial for enhancing model performance\nand improving training efficiency during both the pre-\ntraining and supervised fine-tuning phases. In the realm\nof large language models (LLMs), various data curation\napproaches have been proposed (Xie et al., 2023; Maha-\nrana et al., 2023; Tirumala et al., 2023), including op-\ntimizations for data quantity, data quality, and domain\ncomposition. However, there remains a lack of work\nexploring data curation strategies in the video domain.\nStable Video Diffusion (Blattmann et al., 2023) offers\na comprehensive overview of the curation of large-scale\nvideo datasets, including techniques such as video clip-\nping, captioning, and filtering. However, the dataset is not open-source. In this study, we propose\na novel data processing pipeline for video data and introduce a new video filtering metric. Unlike\ntraditional video quality assessment models (Wu et al., 2023; Zhao et al., 2023; Wu et al., 2022; Sun\net al., 2024), which focus primarily on the aesthetic and technical qualities of a video, our approach\nemphasizes the suitability of videos as training data."}, {"title": "KOALA-36M DATASET", "content": "Koala-36M is a large-scale high-quality video dataset with accurate video splitting, detailed captions\nand higher-quality video content. In summary, Koala-36M contains 36 million video clips with an\naverage duration of 13.75 seconds and a resolution of 720p, each captioned by a text description\naveraging 202 words in length. We compare Koala-36M dataset with previous video datasets in\nTab. 1. Koala-36M dataset simultaneously provides a large number of videos (over 10M) and high-\nquality fine-grained text captions (longer than 200 words), significantly improving the quality of\nlarge scale video datasets. Additionally, as shown in Fig. 2, we further compare Koala-36M with\nPanda-70M on a series of dataset metrics, such as aesthetic scores and clarity scores, demonstrating\na significant improvement in consistency between fine-grained conditions and video content. Since\nthese two datasets come from the same raw datasets, the superiority of Koala-36M dataset also prove\nthe effectiveness of our data processing pipeline."}, {"title": "METHOD", "content": "As shown in Fig. 3, we propose a refined data processing pipeline for Koala-36M dataset. Our\npipeline aims to further improve the consistency between fine-grained conditions and video content.\nOur main contributions are shown in the red box of Fig. 3. Specifically, we start from the same raw\ndata with Panda-70M (Chen et al., 2024b) dataset. First, we propose a more accurate and efficient\ntransition detection method for video splitting in section 4.1. Then we caption splitted videos with\nan average length of 200 words based on our structured caption system in section 4.2. Subsequently,\nwe train a Video Training Suitability Score (VTSS) for data filtering to prevent high-quality data\nfrom the erroneous deletion in section 4.3. Finally, we introduce multiple data sub-metrics as Metric\nConditions into the generation model to enrich the fine-grained conditions in section 4.4."}, {"title": "VIDEO SPLITTING", "content": "Splitting videos into temporal splits is crucial for creating video generation datasets. Transition-free\nvideo data enable more accurate alignment between text and video, while reducing the difficulty of\nmodel training and improving the temporal consistency of generated results. Current video splitting\nmethods Pyscenedetect (Castellano) typically detect transitions based on changes in image features\nbetween consecutive frames, relying on manually adjusted thresholds as criteria, but often overlook\ntemporal information. As a result, these methods struggle to distinguish between gradual transitions\nand fast-motion scenes, leading to missed detections in the former and incorrect detections in the\nlatter.\nTo address the above issues, we first propose a Color-Struct SVM (CSS) module that adopting\na learning-based approach for more accurate detection of changes between frames compared to\nthreshold-basd method. Then we leverage temporal smoothing and statistical features to differentiate\nbetween gradual transitions and fast-motion scenes."}, {"title": "VIDEO CAPTIONING", "content": "Detailed captions usually lead to better text-video consistency, which largely determines the gran-\nularity of semantic responses. To obtain more detailed captions, we propose a structured caption\nsystem, which consists of: (1) the subject, (2) actions of the subject, (3) the environment in which\nthe subject is located, (4) the visual language including style, composition, lighting, etc. (5) the cam-\nera language including camera movement, angles, focal length, shot sizes, etc. (6) world knowledge.\nWe generate these aspects separately, and merge them as the final caption."}, {"title": "DATA FILTERING", "content": "In the large-scale raw dataset, the quality of video content vary significantly. When the performance\nof the generation model is built upon videos with considerable content quality, it is necessary and\ncrucial to filter out low-quality data and remain high-quality data accurately. Traditional methods\noften use various sub-metrics to evaluate video quality and then manually set thresholds to filter\nthe desired data. Since these sub-metrics are not completely orthogonal with each other, the video\nquality is actually a joint distribution of all sub-metrics, which means these thresholds should have\nimplicit constraints with each other. However, existing methods neglect the joint distribution of sub-\nmetrics, resulting in inaccurate thresholds. Meanwhile, since multiple thresholds need to be set, the\ncumulative effect of inaccurate threshold lead to larger deviations during filtering. Therefore, not\nonly low-quality videos are not correctly filtered out as shown in Fig. 1, but also high-quality videos\nare mistakenly deleted as shown in Fig. 5."}, {"title": "NEW CRITERIA AND HUMAN EVALUATION", "content": "We have defined a new annotation criterion that assigns a score reflecting whether a video is suit-\nable as training data for video generation models. This criterion primarily considers the following\naspects of video quality: Dynamic Quality: A high-quality video should exhibit good dynamics,\nwhich are evaluated based on two factors: the extent of subject movement and the temporal stability"}, {"title": "TRAINING SUITABILITY ASSESSMENT NETWORK", "content": "As shown in Fig. 7, we propose a Training Suit-\nability Assessment Network, which takes videos\nand sub-metrics as input, and outputs a single\nvalue called Video Training Suitability Score\n(VTSS). Corresponding to the aforementioned\nannotation criteria, our network is divided into\ndynamic and static branches. Additionally, we\nretain various data labels from traditional data\nfiltering strategies and pass this extra informa-\ntion to the network model as a new branch. For\nthe features of different branches, the 3D Swin\nTransformer is used as the backbone for the\ndynamic branch, while the ConvNext network\nserves as the backbone for the static branch. To\nintegrate the features from different branches, we propose a Weight Cross-Gating Block (WCGB)\nto incorporate the information from the label branch into the other two branches. Since the label\nbranch inherently reflects various characteristics of the video, which are related to both dynamic and\nstatic features, we use label features to enhance the dynamic and static features. Given that different\nvideo labels focus on dynamic and static aspects to varying degrees, we learn a fusion weight to\nadjust the proportion of label features integrated with the two types of video features.\nAfter training Training Suitability Assessment Network on the human-aligned dataset, we employ it\nto predict Video Training Suitability Score (VTSS) for all videos, and obtain the score distribution as\nshown in Fig. 6. Since the VTSS distribution can roughly be divided into two Gaussian distributions,\nwe simply chose the decomposition value 2.5 as the VTSS threshold. Based on this threshold, we\nfiltered out a dataset containing a total of 36 million video clips with corresponding captions. And\nwe name the dataset as Koala-36M, which is the final dataset we present."}, {"title": "METRICS CONDITIONING", "content": "In previous pipelines, data metrics are simply used for data filtering. Meanwhile, the quality of\nthe filtered data still varied, making it difficult for the model to distinguish between high-quality\nand low-quality data. To address this issue, we propose a more fine-grained conditioning method\nto incorporate quality information of different videos into the generation model during training,\nleading to better consistency between conditions and video content. During inference, this method\nalso enables fine-grained control over the generated videos.\nSpecifically, during video diffusion training, we first\nencode data metrics such as motion score, aesthetic\nscore, and clarity score into frequency embeddings.\nSubsequently, frequency embeddings are passed\nthrough an MLP to obtain multiple embeddings,\nwhich are then directly added to the timestep embed-\ndings and incorporated into the transformer block\nusing Adaptive Layer Normalization (AdaLN). This\nmethod has two main advantages. First, it does\nnot increase the computational load of the diffusion\nmodel. Second, compared to adding conditions in\ncaptions like Open-sora (Zangwei et al., 2024), it al-\nlows for more precise control by being more sensi-\ntive to numerical scores, and posses a stronger ability to decouple control over different metrics.\nDuring the inference stage, we can set different feature scores, such as setting all scores to the\nhighest value, to generate high-quality videos."}, {"title": "EXPERIMENTS", "content": "To validate he superiority of Koala-36M dataset and the effectiveness of our data processing pipeline,\nwe train the same generation model from scratch on different datasets for comparison. Our text-to-\nvideo base model is based on a 3D attention-like Sora structure (Brooks et al., 2024), and the VAE\nemploys a causal convolution-based 3D VAE. Since the training was done from scratch, we set\nthe video duration to 2 seconds and the resolution to 256x256 for faster convergence. All models\nare trained on their respective datasets passing through 140M data samples in total. To evaluate the\nperformance of generation models, we conduct a comprehensive evaluation on the public benchmark\nVBench (Huang et al., 2023). Due to the domain gap between the captions provided by VBench and\ntraining set, we performed prompt expansion on the captions in VBench."}, {"title": "QUANTITATIVE RESULTS", "content": "Table 2: Quantitative results of text-to-video generation. We compare the performance of gen-\neration models trained on different datasets with VBench. The generation model trained on Koala-\n36M surpasses other models on both quality score and semantic score, with the highest total score.\nAs shown in Tab. 2, we comprehensively evaluate models trained on Panda-70M and our dataset\nat the same step. The generation model trained on Koala-36M surpasses other models on both\nquality score and semantic score, with the highest total score. Furthermore, we visualize the\nVBench metrics comparison in Fig. 9. Koala-36M significantly improves the generation model's\nperformance on aesthetic quality, object class, multi-objects, human action, and color."}, {"title": "QUALITATIVE RESULTS", "content": "We visualize the generated videos on VBench's prompts in Fig. 10. The generation model achieve\nthe optimal performance on Koala-36M, with both the best video quality and text-video consistency.\nKoala-36M outperform the larger Panda-70M dataset with only 36M data, indicating that our data\nquality far exceeds that of Panda-70M. See A.5 for more video generation results."}, {"title": "ABLATION EXPERIMENTS", "content": "We conduct extensive ablation experiments to demonstrate the superiority of our dataset and the\nentire pipeline. Specifically, we performed ablation experiments on different data processing and\ntraining strategies, divided into the following groups: (1) Panda-70M: baseline. (2) Koala-all:\nAll 48M data after video splitting and captioning. (3) Koala-37M: manually filtered data from\nKoala-all using multiple thresholds. (4) Koala-36M: filtered dataset from Koala-all using VTSS.\n(5) Koala-all-condition: Koala-all with metrics conditions. (6) Koala-36M-condition: Koala-36M\nwith metrics conditions.\nData Processing. Comparing the results of training from Panda-70M and Koala-all in Tab. 2 and\nFig. 10, we find that Koala-all produce better results, especially in temporal quality, such as subject\nconsistency, background consistency and temporal flickering. This indicates that our newly proposed\nre-splitting algorithm can more accurately segment transitions, reducing semantic inconsistencies\nbetween video segments. Additionally, our recaptioning algorithm provided more detailed video\ndescriptions, making it easier for the model to learn the relationship between visual and textual\ninformation. To further demonstrate the superiority of our splitting and captioning methods, we\nconducted extensive comparative experiments, detailed in the App. A.1.\nData Filtering. Comparing the results of training from Koala-all&Koala-36M and Koala-all-\ncondition&Koala-36M-condition, we find that the results from the latter one perform better than\nthat from the former datasets. This indicates that filtering out low-quality data and retaining high-\nquality data are necessary to prevent the model from learning biased distributions from low-quality\ndata. In addition, comparing the results of training from Koala-46&Koala-36M, it can be concluded\nthat our filtering method based on single VTSS results in better filtering performance, when more"}, {"title": "CONCLUSION", "content": "In this paper, we present a large-scale high-quality dataset called Koala-36M, with accurate video\nsplitting, detailed captions and higher quality video content. Koala-36M dataset is currently the only\nvideo dataset that simultaneously possesses a large number of videos (over 10M) and high-quality\nfine-grained text captions (longer than 200 words), significantly improving the quality of large scale\nvideo datasets. Additionally, we propose a refined data processing pipeline to further improve the\nconsistency between fine-grained conditions and video content, including better transition detection\nmethod, structured caption system, and data filtering method and fine-grained conditioning.\nLimitations. Despite all the strength above, Koala-36M is still insufficient to support the training of\nan extremely large video generation model with over 1B parameters. A larger-scale datasets need to\nbe further collected and processed. Meanwhile, the performance, generalization, and scaling laws\nof generation models on high-quality datasets need further exploration."}, {"title": "APPENDIX", "content": null}, {"title": "EFFECTIVENESS OF VIDEO SPLITTING METHODS", "content": "To validate the accuracy and efficiency of our proposed Color-Struct SVM (CSS) for scene transition\ndetection, we conduct the following experiments.\nWe annotate transitions in 10,000 video clips, creating a test set (approximately half of the videos\ncontain transitions). We then apply our proposed method and open-source methods to detect tran-\nsitions in the test set, recording the precision and recall of the detections. The open-source method\nis primarily based on Pyscenedetect (Castellano), and we test two versions: one that detects transi-\ntions based solely on HSL (Hue, Saturation, Lightness) and another that uses both HSL and edge\ndetection. The experimental results are shown in the Tab. 3. It can be observed that our transition de-\ntection algorithm outperforms the two pyscenedetect-based methods in terms of both precision and\nrecall. Notably, our algorithm achieves a high recall rate, indicating that it rarely misses transitions\nin videos."}, {"title": "ELIMINATION OF DEVIATIONS BETWEEN TRUE SCORES AND LABELED SCORES", "content": "After establishing the criteria, we randomly sample a batch of data and have it annotated by trained\nexperts, with each video being scored by eight experts on a scale of 1 to 5. To ensure that the anno-\ntations closely reflect the true suitability scores, we need to address two types of errors: Individual\nPreference Bias: As shown in the Fig. 12(a), we visualize the violin plots of scores given by dif-\nferent experts. The expert on the left tends to give lower scores, while the expert on the right tends\nto give higher scores. These individual preferences can cause the final scores of some videos to be\nlower or higher than their actual values. Therefore, we standardize the scores of each expert and\nthen scaled them using the mean and variance of the overall scores to eliminate the bias introduced\nby different experts. From the figure, it can be seen that the scores processed through our normaliza-\ntion and rescaling methods align more closely with the overall score distribution. Label Fluctuation\nBias: As shown in the Fig. 12(b), each video is annotated by eight experts, and different experts may\nassign different scores due to varying interpretations of the criteria. This leads to label fluctuations.\nWe use the mean score to reduce the error caused by these fluctuations."}, {"title": "ABLATION EXPERIMENTS OF TRAINING SUITABILITY ASSESSMENT NETWORK", "content": "Table 5: Performance Metrics for Different Combinations of Video, Image, and Feature\nWe conduct comprehensive ablation experiments on our Training Suitability Assessment Network.\nThe experimental results are shown in Tab 5. The baseline model utilizes only dynamic features.\nAdding the static branch enables the model to capture more static information, thereby improving\noverall performance. The inclusion of the feature branch allows the model to leverage additional la-\nbel information, further enhancing its performance. The WCGB module integrates label information\nwith dynamic and static features through a cross-gating mechanism, achieving optimal performance.\nEach module addition significantly boosts the model's performance."}, {"title": "COMPARISON OF RESULTS FROM DIFFERENT METRICS CONDITIONS", "content": "Figure 13: Comparison of results from different metrics conditions. Our method has more pre-\ncise control under the same normalized metrics score and stronger ability to decouple control over\ndifferent metrics, when the style of videos transfer with the motion score."}]}