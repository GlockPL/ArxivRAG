{"title": "Non-Uniform Parameter-Wise Model Merging", "authors": ["Albert M. Orozco Camacho", "Stefan Horoi", "Guy Wolf", "Eugene Belilovsky"], "abstract": "Combining multiple machine learning models has long been a technique for enhancing performance, particularly in distributed settings. Traditional approaches, such as model ensembles, work well, but are expensive in terms of memory and compute. Recently, methods based on averaging model parameters have achieved good results in some settings and have gained popularity. However, merging models initialized differently that do not share a part of their training trajectories can yield worse results than simply using the base models, even after aligning their neurons. In this paper, we introduce a novel approach, Non-uniform Parameter-wise Model Merging, or NP Merge, which merges models by learning the contribution of each parameter to the final model using gradient-based optimization. We empirically demonstrate the effectiveness of our method for merging models of various architectures in multiple settings, outperforming past methods. We also extend NP Merge to handle the merging of multiple models, showcasing its scalability and robustness.", "sections": [{"title": "I. INTRODUCTION", "content": "Combining multiple machine learning (ML) models has been historically a popular approach to improve model per- formance. Early works on model ensembles have shown that combining multiple models can lead to better generalization and robustness [1], [2]; in those settings, the focus has been on combining task predictions rather than the models themselves. On the other hand, combining model parameters brings the ad- vantage of reducing the computational and memory overhead of inference and storage. Furthermore, merging the models themselves is critical in federated learning (FL) settings to increase data privacy. In such settings, multiple models are trained independently on separate data instances that cannot be shared. Model ensembles are therefore not an option since this constraint prevents us from computing the predictions of the different models to then combine them. Directly merging the models' parameters, which can be shared, offers another way of combining the learnings from the different models. This parameter agregation has come to be known as model fusion or model merging, and previous works have demonstrated its efficacy in many settings [3]\u2013[14]. However, combining multiple models into a single one through model fusion is not a trivial task, and it is often difficult to achieve good predictive performance in this manner.\nIn the context of model merging, linear mode connectivity (LMC) is the property of two local minima of a deep learning (DL) model which are connected by a linear low loss path in the parameter space of the model. This is a desirable property since LMC between two models ensures that we can simply linearly interpolate between their parameters without significantly impacting performance. However, linear mode connectivity is hard to achieve and isn't guaranteed even for models trained on the same dataset and with a shared random initialization [15]. [16] recently conjectured that this difficulty in achieving LMC between models is largely due to the per- mutation invariance of deep learning models, i.e. the fact that we can change the order of the neurons in a given layer and the modified model would still represent the same mathematical function as long as the incoming and outgoing weights were permuted accordingly. Therefore most local minima of a given model, found through stochastic gradient descent, can become linearly mode-connected simply by finding the right permuta- tions to align one model's parameters to those of the second model. Recent works have since proposed numerous methods for finding adequate permutations or linear transformations that align one model, say model A, to another, model B [4]\u2013 [8], [17]. After aligning these models, their parameters can be combined through the equation $\\alpha P_A + (1 - \\alpha)P_B$ where $\\alpha \\in (0,1)$ and $P_A$ (resp. $P_B$) are the parameters of model A (resp. B). These approaches have successfully reduced the loss barrier on the linear path between different models. For most models however, a loss in accuracy is still incurred when the parameters of the models being merged are linearly combined uniformly, i.e. $\\alpha$ is a scalar that multiplies all the parameters in model A and B (through $1 - \\alpha$) uniformly. Recently, [18] argued that direct parameter averaging might fail to represent well either of the two mathematical functions learned by the two models being merged. They then propose a weighted merging of the models' parameters where the models' Fisher information determines the weights, a similar idea was presented in [13]\nWe continue this line of work, highlighting that while permutations might allow the alignment of different models in a way that makes LMC achievable, not all parameters"}, {"title": "II. RELATED WORK", "content": "Artificial neural networks have highly complex and non- convex loss landscapes. How we can train such models despite this fact has puzzled researchers for a long time. Furthermore, ANNs are often overparameterized for the tasks they solve and they can memorize the tasks they are trained on [19], however, they're still able to generalize and learn important patterns from the data.\nMultiple works have looked at the geometry of ANNs' loss landscapes [20], [21]. Of particular interest here, [22] theo- retically showed that minima of one-layered ReLU networks become connected by low loss paths in parameter space as the number of neurons increases, i.e. they have asymptotically connected level sets. [23] and [24] have continued this line of work empirically by providing algorithms to find non-linear low-loss paths between minima in ANNs' loss landscapes, this property of minima connected by such paths has been called mode connectivity. [23] used these insights to propose Fast Geometric Ensembling (FGE) where multiple local minima from a single training trajectory with cyclical learning rates are ensembled to improve performance. [15] have introduced the term linear mode connectivity (LMC) to describe the property of ANN minima connected by a linear low loss path in parameter space and have used it to study ANNs' stability to SGD noise, i.e. different data orders and augmentations. Specifically, they found that even models trained from the same initialization but with different SGD noise might not be linearly mode connected, however, sharing a part of the training trajectory helps models achieve LMC. We note that the concept of LMC is important here since it ensures that the models are already aligned to some extent, the Align function from Sec. III can then simply be the identity and the aggregation of the models parameters (Agg function from Sec. III) can consist of linearly interpolating the models' parameters.\n[16] conjectured that \"Most SGD solutions belong to a set S whose elements can be permuted in such a way that there is no barrier on the linear interpolation between any two permuted elements in S.\" In other words, most trained models found through standard ANN training are linearly mode connected, provided the right permutation is applied to align the two models' parameters. This conjecture is justified by the fact that one can apply any permutation to an ANN layer's neurons and their connections and the mathematical function described by the network will not have changed [25], [26].\na) Works focused on model alignment: Numerous recent works have been focused on finding these \"right\" permuta- tions to align different networks. [5] aligns networks by first computing the cross-correlation matrix between the neural activations of the two models and then solving the assignment"}, {"title": "III. METHODOLOGY", "content": "Let A and B denote two artificial neural networks (ANNS) with the same architecture, for simplicity purposes assume they're standard multi-layered perceptrons (MLPs) with linear layers. The output of the i-th layer (out of a total of L layers) of model $M \\in \\{A, B\\}$ in response to a given input $x_{M_{i-1}} \\in \\mathbb{R}^{n_{i-1}}$ is given by:\n$x_{M_{i}} = \\sigma(W_M x_{M_{i-1}} + b_M)$\nWhere $\\sigma$ is the non-linearity, often a ReLU and $W_i \\in \\mathbb{R}^{n_i \\times n_{i-1}}$, $b_i \\in \\mathbb{R}^{n_i}$ are that layer's parameters, respectively its weights and bias.\nModel merging aims to combine the learned features of both models by first aligning the two models and then aggregating their parameters. This can be seen as the successive application of two functions one for aligning and one for aggregation:\n$A', B' = Align(A, B)$\n$C = Agg(A', B')$\nWhere A', B' are the \u201caligned\" models and C = Agg o Align(A, B) is the final merged model. Concretely, the scope of the Align function is to make A and B linearly mode connected, it is implicitly understood that this operation also aligns the representation spaces of both models [27]. It is common practice to keep one of the models, say A, as being the same through the alignment, i.e. A' = A, and aligning the second model to it. Most model alignment methods have been motivated by ANN's invariance to permutations and have\ntherefore looked for permutations of the neurons of model B to align them \"optimally\" to those of model A. This is done in a layer-by-layer fashion. Specifically, we are looking for permutations $P_i \\in \\mathbb{R}^{n_i \\times n_i}$ to align the neurons at layer i of model B to those of model A. Since the layers are interconnected, we also need to apply the inverse permutation $P_i^{-1}$ at the input level of the following layer's parameters. After alignment, the output of layer i in model B' becomes:\n$x_{B'_i} = \\sigma(P_i (W_{B_i} P_i^{-1}) x_{A_{i-1}} + P_i b_{B_i})$\nWe note that some methods go beyond permutations as this methodology can be roughly extended to invertible linear transformations $T_i \\in \\mathbb{R}^{n_i \\times n_i}$ and their inverses $T_i^{-1}$ [17]. However, we note that ANN non-linearities might interfere with these merge/un-merge transformations in cases where they're not simple permutations.\nPrevious work on model merging has mainly focused on model alignment, we go over existing methods in Sec. II. Indeed, in most cases, once the models have been aligned, aggregating their parameters has simply been done through uniform linear combinations of the model's parameters, i.e. if $W_{B'_i} = P_i W_{B_i} P_i^{-1}$ are the weights at layer i of model B after being aligned to model A, the weights of the merged model are simply:\n$W_i = \\alpha W_{A_i} + (1 - \\alpha) W_{B'_i}$  (1)\nThe merging parameter, $\\alpha \\in (0,1)$, is a single scalar for the whole merging operation, i.e. a fixed value of $\\alpha$ is used for all parameters and all layers being merged, hence the term \"uniform\". The most commonly used value is $\\alpha = 0.5$.\nB. Non-uniform Parameter-wise Model Merging\nWhile this simple model aggregation function works rel- atively well in practice, it assigns equal importance to all parameters of the models being merged which might not be an appropriate assumption. Especially in situations where the models were trained on disjoint datasets, it is natural to think that one of the two models might have learned certain features to a greater or lesser extent than the other model. Therefore, assigning equal importance to all parameters by fixing $\\alpha = 0.5$ might be detrimental to the model being merged. Instead, we argue it might be beneficial to assign a different weight to each parameter. Mathematically this means that instead of having $\\alpha$ be a scalar (shared by all layers), we would have for each layer i a tensor $\\alpha_i \\in \\mathbb{R}^{n_i \\times n_{i-1}}$ of the same size as the models weight tensors at that layer $W_{M_i} \\in \\mathbb{R}^{n_i \\times n_{i-1}}$. The parameters of the merged models then become:\n$W_i = \\alpha_i \\odot W_{A_i} + (1 - \\alpha_i) \\odot W_{B'_i}$  (2)\nWhere $\\odot$ is the elementwise product and 1 is the matrix full of ones of the necessary size. Another way to visualize this is that we have an $\\alpha = \\cup_{i=1}^L \\alpha_i$ tensor, the same size as one of the two models' parameters, and for each parameter from the original models we have a scalar in $\\alpha$ which corresponds"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we experimentally validate the proposed model merging technique. We consider standard model merg- ing settings with similar and dissimilar training data distribu- tions for the base models [9], [17] as well as a setting where many models are merged.\nIn practical terms, we have implemented Pytorch [32] models whose weights are given by Eq. 2. The weights being merged, i.e. those of model A ($W_{A_i}$) and those of aligned model B ($W_{B'_i}$) are kept constant for all L layers. We can then backpropagate through these layers, compute the training loss gradient for $\\alpha$, and update them through gradient descent. We consider VGG11 [33] and ResNet20 models [34] of different widths [35] trained on CIFAR10 and CIFAR100 respectively. We also train pairs of ResNet20 models of different widths on three disjoint splits of CIFAR100:\nWe always evaluate and report the accuracy of the full test set of the task considered. Since NP Merge's optimization process involves training the $\\alpha$ parameters (of the same size as any given neural architecture), we report accuracies after 10 optimization epochs over the data. We use the ADAM [36] optimizer with a learning rate of 0.01 for CIFAR10 and CIFAR100. To avoid biasing the results towards any particular model, we initialized the $\\alpha$ parameters to 0.5 for all experiments. We compare the performance of our NP Merge method with the following:\nAs discussed in Sec. III-D, it is natural to compare our NP Merge method to full fine-tuning of the model post merging. Therefore we also compare NP Merge with standard fine- tuning of the merged model $\\frac{A+B'}{2}$, using the same data for both methods.\nWe note that since NP Merge is only concerned with the aggregation of parameters part of model merging, the alignment of the representation spaces needs to be done with one of the existing alignment methods. In all reported results for NP Merge we align the models using the permutations provided by the Permute method [5] and then do the proposed gradient-based optimization of the interpolation values for the parameter aggregation. We reset the Batch Norm statistics post merging to avoid variance collapse as suggested by [8]."}, {"title": "V. DISCUSSION AND CONCLUSION", "content": "In recent years, numerous methods have been proposed to align the representation spaces of deep learning models trained from different initializations that do not share parts of their training trajectories. While these methods have helped lower the loss barrier between merged models, achieving linear mode connectivity (LMC) is still challenging in some cases. Instead of focusing solely on model alignment as past works have done, we concentrated on the aggregation of models' parame- ters after alignment\u2014a research direction that has not received as much attention. Traditionally, merged model parameters are averaged or undergo uniform linear interpolation, which may not fully exploit the models' potential.\nOur experiments demonstrate that NP Merge consistently outperforms traditional merging methods and ensemble tech- niques. NP Merge significantly improves accuracy when merg- ing models trained on the same data distribution and non- uniform class distributions. It also proves robust with limited data, maintaining stable performance with small data subsets. Additionally, NP Merge shows scalability and effectiveness in complex scenarios like federated learning, handling increased computational complexity and memory requirements through successive pairwise merging.\nFuture work should explore the relationship between learned interpolation values from gradient descent and those from models' Fisher information matrices as suggested in [13], [18]. This line of work must also address the extra computational overhead issue: it is natural to assume that many distributed applications come with restricted computational budgets. Ad- ditionally, investigating the applicability of NP Merge in other domains and with different neural architectures could provide further insights into its versatility and robustness."}]}