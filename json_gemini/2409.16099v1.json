{"title": "Neuromorphic Drone Detection: an Event-RGB Multimodal Approach", "authors": ["Gabriele Magrini", "Federico Becattini", "Pietro Pala", "Alberto Del Bimbo", "Antonio Porta"], "abstract": "In recent years, drone detection has quickly become a subject of extreme interest: the potential for fast-moving objects of contained dimensions to be used for malicious intents or even terrorist attacks has posed attention to the necessity for precise and resilient systems for detecting and identifying such elements. While extensive literature and works exist on object detection based on RGB data, it is also critical to recognize the limits of such modality when applied to UAVs detection. Detecting drones indeed poses several challenges such as fast-moving objects and scenes with a high dynamic range or, even worse, scarce illumination levels. Neuromorphic cameras, on the other hand, can retain precise and rich spatio-temporal information in situations that are challenging for RGB cameras. They are resilient to both high-speed moving objects and scarce illumination settings, while prone to suffer a rapid loss of information when the objects in the scene are static. In this context, we present a novel model for integrating both domains together, leveraging multimodal data to take advantage of the best of both worlds. To this end, we also release NeRDD (Neuromorphic-RGB Drone Detection), a novel spatio-temporally synchronized Event-RGB Drone detection dataset of more than 3.5 hours of multimodal annotated recordings.", "sections": [{"title": "1 Introduction", "content": "Drones are versatile devices with a wide range of applications, including photography, videography, agriculture, search and rescue operations, environmental monitoring, infrastructure inspection, and more. However, concerns about privacy have also been raised due to the potential for drones to capture images, videos and audio of individuals without their consent [2]. In response to these concerns, many countries have enacted regulations governing the use of drones and addressing privacy issues. These regulations often include guidelines for where drones can be flown, how high they can fly, and restrictions on capturing images or videos of private property or individuals without permission4.\nAdvancements in technology are being developed to help mitigate potential privacy risks associated with drone use [30, 41]. For instance, geofencing technology creates virtual boundaries using GPS or RFID. Drones equipped with geofencing capabilities can be programmed to avoid restricted areas, such as private properties, government buildings, and sensitive locations. This ensures drones do not inadvertently capture images or data from these areas, thus protecting privacy. Additionally, to defend from both unintentional abuse and deliberate attack, a variety of methods and technologies have been proposed to detect drones without relying on their deliberate cooperation. Radio Frequency (RF) Analysis is widely used for drone detection and operates by detecting the radio signals used for drone communication and control [1]. However, this technology is of limited efficacy when the drone is equipped with processing modules enabling it to operate autonomously. High-resolution radar systems can detect and track drones by sending out signals and analyzing the reflections from objects in the sky [26]. However, traditional radar technology can struggle to detect increasingly miniaturized commercial drones, many of which have the size of a bird.\nEven if the radar system can detect very small objects it could not be able to distinguish a small drone from a bird. Acoustic Sensors use microphones to detect the unique sound signatures produced by drones' propellers and engines [33,33]. However, this technology does not work as well in noisy environments and it also has a very short operative range. Optical Sensors including RGB [39] and thermal imaging cameras [33], can visually detect drones [11]. Thermal cameras are particularly effective for drone detection as they can capture the heat signatures of drone propellers. However, only cooled thermal cameras are capable of capturing such signatures at a distance, which results in bulky and rather expensive devices. Furthermore, in strong sunlight, the ambient temperature of parts of the environment can rise significantly, causing non-target objects (like buildings, roads, and vegetation) to emit infrared radiation. This causes clutter and reduces the contrast between the temperature of the heated propellers and their surroundings, making it harder to detect them. Also, the use of RGB cameras for drone detection is particularly challenging when the drone is observed under a cluttered background. In such conditions, even state-of-the-art detection models, such as recent versions of the YOLO network, fail to adequately address the detection task [22].\nRecently, neuromorphic cameras, also referred to as event-based cameras or dynamic vision sensors (DVS), have been introduced to advance imaging technology in scenarios involving fast-moving objects and varying illumination conditions [12]. These cameras operate by detecting changes in the scene at the level of individual pixels asynchronously. This allows them to capture events with very high temporal resolution, often in the microsecond range, which is ideal for fast-moving objects. In addition, these cameras can handle a wide range of illumination conditions, from very low light to extremely bright environments,"}, {"title": "2 Related Work", "content": "Neuromorphic Object Detection Approaches to object detection with neuro-morphic cameras can be broadly grouped into two classes, depending on whether the stream of events is processed by preserving the spatio-temporal sparsity of the events or by first converting the stream to dense, pseudo-frame representations. Among the former approach, several methods have been proposed for object detection using Spiking Neural Networks [9, 17, 19, 24, 40, 42], biologically inspired networks composed of neurons that communicate using discrete and asynchronous spikes. Such an approach has been used in several fields, such as automotive for vehicle and pedestrian detection [9]. To improve detection rates, solutions like temporal-wise attention have been studied [40], as well as multi-camera processing involving two neuromorphic sensors [17]. Zhang et al. [42] adopted a spiking transformer network, STNet, to detect and track objects leveraging both spatial and temporal information."}, {"title": "3 Model Architecture", "content": "We propose a multimodal architecture for drone detection that merges information from neuromorphic and RGB frames. As a base model, we take inspiration from the DEtection TRansformer (DETR) model [7], motivated by its flexible modular structure and its effectiveness in common RGB object detection benchmarks. We first investigate the capabilities of a standard DETR model in both domains individually, highlighting the large performance gap between RGB-based and event-based models. We then analyze multimodal networks, to bridge the shortcomings of both modalities. To this end, we propose three different event-RGB fusion strategies, ranging from simple pooling layers to more complex attention-based strategies, to retain the best of both worlds and analyze the impact of both modalities. In the following, we assume to work with event and RGB frames, spatially aligned (i.e., overlappable with little or no misalignment) and temporally synchronized. Event frames $e_i$ are obtained by accumulating all events within temporal intervals of duration $\u2206t = 1/F$, where $F$ is the frame rate of the RGB video. The frame representation of events is made using the camera proprietary API from Prophesee6.\nIt must be noted that, in principle, two improvements could be made. First, the constraint binding the accumulation time $\u2206t$ to the inverse of the frame rate could be relaxed. Doing so would enable a more fine-grained analysis of motion patterns but would increase the computational burden and break the one-to-one pairing between event and RGB frames. Second, temporal dynamics could be modeled. For simplicity, in this work, we completely disregard time by processing frames individually. We leave the study of these aspects for future work.\nThe DETR model We choose the DETR model as both baseline and foundation to build on for comparing the effectiveness of drone detection by operating on each modality separately and in a multimodal approach, by leveraging on several fusion strategies. DETR combines a CNN backbone for feature extraction (such as Resnet-50 [14]) and a transformer architecture to detect bounding boxes. The distinguishing trait of DETR consists of the removal of traditional components like object proposals and anchor boxes with a set of learnable object"}, {"title": "5 Experimental Results", "content": "In this section, we report the results of our drone detection architectures, detailing also the experimental setting."}, {"title": "5.1 Implementation Details", "content": "We divided the dataset into train and test data, following a video-wise 80/20 split (92 videos for train, 23 for test) to avoid similar frames in both splits. All models have been fine-tuned on a pre-trained DETR model, changing the number of object queries and the classification head. In particular, since only a maximum of 2 contemporary drones are present in each frame, we opted to lower the number of object queries from 100 to 5. All the models have been trained for 30 epochs with a learning rate of $le-5$ with a decay of an order of magnitude every 15 epochs. The optimizer is AdamW as in standard DETR and the batch size is 8."}, {"title": "5.2 Evaluation", "content": "We present in Tab. 3 the results in terms of Average Precision (AP) with different intersection over union thresholds, namely 0.5, 0.75 and averaging the thresholds from 0.5 to 0.95 with a 0.05 step, as commonly done in datasets like COCO [18]. The gap between the event and the RGB base DETR models appears immediately clear, underlying the difficulty of detecting drones in RGB frames. On the contrary, event data proves to be very effective. The asymmetric modality injection fusion strategies still exhibit such a gap. Here, we refer to the two variants of the models as x-to-y, where x is the main modality and y the complementary one. At the same time though, injecting information from the other domain helps in significantly improving the AP of both models compared to their single modality counterparts. Quite surprisingly, we found that the symmetric fusion, which combines the two asymmetric injections in a single"}, {"title": "5.3 Ablation Study", "content": "We also carried out a set of ablation studies to get a better understanding of the differences between the architectures. In particular, we investigate both the impact of the number of queries and the effect of fusing RGB and event data in different parts of the architecture.\nEarly vs Late Fusion We investigated the effect of different variants of the same fusion strategy, by applying it picking different cut-off layers after which to apply the fusion module. We start by comparing our best performing model, based on pooling fusion after the encoder layer, against a similar approach, with an earlier fusion. In this case, we directly average the features that come out from the ResNet backbone and then feed the resulting features to the transformer block. The results are shown in Tab. 4. An early fusion in this case is detrimental to the performance, especially for AP75, which exhibits a drop of 10 points. Interestingly, the AP50 decreases by only 0.5 points, suggesting that the detector still keeps working, yet it becomes less precise in identifying the exact boundaries of the drones. Similarly, in Tab. 5, we change the fusion point for the symmetric fusion architecture. Instead of fusing the two modalities after the encoder layer, we tested a late-fusion approach, where the modalities are fused after the final decoder. In this case, the degradation is small but consistent across all metrics. To summarize, using two different modalities brings considerable improvements, yet picking the correct layer where to perform the modality fusion can have a significant impact on the overall capacity of the model. It appears that picking an intermediate layer yields the best results. This does not come as a surprise, as it offers a compromise between the number of parameters to be trained and the number of layers that can benefit from a joint training, sharing information across modalities."}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we studied the problem of detecting drones with an event camera. In particular, we focused on developing different modality fusion strategies, that can be summarized into three main categories: pooling-based fusion, asymmetric modality injection and symmetric fusion. We found that event-based models demonstrate large performance improvements compared to RGB counterparts, yet the two modalities combined can improve and bridge the limitations of both modalities. In order to carry out our experiments we also collected and presented NeRDD, a novel multimodal dataset comprising 3.5 hours of manually annotated and spatio-temporally synchronized event-RGB videos. We believe that publicly releasing the dataset will foster research in the field of neuromorphic object detection, in particular for drone detection. For future works, we plan to investigate how to leverage the temporal information contained in the event data for a more resilient detection, as well as explore more advanced applications such as tracking and forecasting."}]}