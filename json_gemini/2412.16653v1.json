{"title": "Internalized Self-Correction for Large Language Models", "authors": ["Nishanth Upadhyaya", "Raghavendra Sridharamurthy"], "abstract": "In this article, we introduce 'Internalized Self-Correction' (InSeC) for large language models (LLMs). While many approaches exist for self-reflection at inference time, we propose a novel method that combines ideas from negative sampling, self-reflection during training, and inference time. InSeC allows LLMs to correct themselves by introducing mistakes and their corresponding corrections during training, thereby converting the learning process into a true supervised learning task with both positive and negative examples. This approach can be extended to improve instruction following and correct hallucinations or incorrect sentences generated by LLMs.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) [1] have demonstrated impressive capabilities in various Natural Language Processing (NLP) tasks. However, a persistent challenge in LLM development is their tendency toward \"superficial alignment\u201d when trained using conventional Reinforcement Learning from Human Feedback (RLHF) methods. This phenomenon, characterized by models prioritizing stylistic changes over improvements in core abilities, highlights a critical need for more effective training strategies. In this paper, we introduce Internalized Self-Correction (InSeC), a novel approach designed to enhance LLMs' self-correction capabilities during training. By embedding this mechanism directly within the model, InSeC has the potential to mitigate superficial alignment and foster more robust learning."}, {"title": "Related Work", "content": "1. Lee et.al. [2] focus on improving LLM performance through error correction, leveraging external feedback and self-reflection to refine responses.\n2. Renze and Guven [3] analyse the effects of self-reflection by prompting LLMs to reflect on their errors in multiple-choice question-answering scenarios. In contrast InSeC uses self-reflection during the training phase in a much broader sense, to enhance the model's capability to generate grammatically correct sentences.\n3. Lingo et al. [4] proposes REAP, utilizing external processes like reflection, problem deconstruction, and advanced prompting to guide the LLM towards a solution. Again while the goal is similar, InSeC works in the training phase.\nSummary: While all the papers explore methods for improving LLM performance, their approaches and specific areas of focus differ. InSeC presents a unique angle by incorporating self-correction directly into the training process, potentially contrasting with the external feedback and reflection mechanisms employed in the other papers."}, {"title": "Contributions", "content": "Our contributions in this paper are as follows:\n1. Provide an opportunity for LLMs to correct themselves through InSeC. This method enables models to identify and rectify their own errors during training, potentially leading to a deeper understanding of the task beyond surface-level patterns.\n2. Address the inefficiency of typical self-supervised learning in LLMs. Traditional self-supervised training, akin to positive unlabeled learning, can be inefficient. InSeC introduces a novel approach by incorporating negative examples during training, potentially enhancing learning efficiency.\n3. Convert the learning process into a true supervised learning task with both positive and negative examples by introducing mistakes and their corrections during training. This approach, similar to negative sampling used in training word2vec [5], aims to provide the model with a more comprehensive understanding of correct and incorrect outputs.\n4. Extend InSeC to correct even paragraphs and improve instruction following. Our method demonstrates the potential to generalize beyond sentence-level correction, addressing a wider range of LLM applications.\n5. Ensure coherent and accurate output by easily removing special tokens and incorrect sentences after generation. We present a practical approach to maintain output quality by removing any artifacts introduced during the training process."}, {"title": "Implementation Details", "content": "To implement InSeC, we generate alternative continuations of the sequence (negative samples) alongside the true continuation (positive sample) within the training data. We artificially create incorrect (irrelevant) next sentences and use special tokens to self-correct.\nWe replace approximately 15% of sentences with incorrect sentences, followed by self-correction tags (and reasons). This approach generalizes the model, enabling it to correct any hallucination, incorrect sentences, or poor instruction following that might result from high temperature or other factors."}, {"title": "Results", "content": "We finetune a model (Meta LLama3.1 8B) on synthetic CoT data - with and without negative samples. We see that the model with negative samples can auto correct whenever an error is encountered. As an example we finetune on the below sample data and force an incorrect continuation. The finetuned model with negative samples is seen correcting itself, whereas the finetuned model without negative samples doesn't correct and produces incorrect final answer."}, {"title": "Potential Benefits", "content": "\u2022 Improved Sample Efficiency: The model learns from both positive and negative examples, leading to more efficient training.\n\u2022 Better Generalization: By explicitly learning to distinguish between correct and incorrect sequences, the model can better generalize to unseen data.\n\u2022 Reduced Overfitting: Negative samples can act as a form of regularization, reducing the risk of overfitting."}, {"title": "Challenges", "content": "\u2022 Increased Computational Complexity: Handling larger datasets and more complex models may require additional computational resources.\n\u2022 Selecting Informative Negative Samples: Ensuring negative samples are informative for complex language tasks is crucial for the success of this approach.\n\u2022 Nuanced Language Understanding: There is a risk of losing nuanced language understanding if negative samples are too simplistic or not representative of the task at hand."}, {"title": "Conclusion", "content": "Adapting negative sampling to LLMs represents an interesting area for research and experimentation. InSeC could potentially lead to more efficient training methods and improved model performance, especially in tasks that benefit from contrastive learning approaches. Further exploration of this technique may yield valuable insights into the development of more robust and accurate LLMs. In future, one can also explore the possibility of combining aspects of multiple approaches. For instance, one may leverage the fine-grained feedback mechanism used in RLRF, integrate it into the InSeC training process to provide more targeted self-correction."}, {"title": "Contributions", "content": "NU - conception, experiments, writing. RS - writing, review."}]}