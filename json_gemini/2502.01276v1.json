{"title": "HyperSHAP: Shapley Values and Interactions for Hyperparameter Importance", "authors": ["Marcel Wever", "Maximilian Muschalik", "Fabian Fumagalli", "Marius Lindauer"], "abstract": "Hyperparameter optimization (HPO) is a crucial\nstep in achieving strong predictive performance.\nHowever, the impact of individual hyperparam-\neters on model generalization is highly context-\ndependent, prohibiting a one-size-fits-all solution\nand requiring opaque automated machine learning\n(AutoML) systems to find optimal configurations.\nThe black-box nature of most AutoML systems\nundermines user trust and discourages adoption.\nTo address this, we propose a game-theoretic ex-\nplainability framework for HPO that is based on\nShapley values and interactions. Our approach\nprovides an additive decomposition of a perfor-\nmance measure across hyperparameters, enabling\nlocal and global explanations of hyperparame-\nter importance and interactions. The framework,\nnamed HYPERSHAP, offers insights into abla-\ntions, the tunability of learning algorithms, and\noptimizer behavior across different hyperparame-\nter spaces. We evaluate HYPERSHAP on various\nHPO benchmarks by analyzing the interaction\nstructure of the HPO problem. Our results show\nthat while higher-order interactions exist, most\nperformance improvements can be explained by\nfocusing on lower-order representations.", "sections": [{"title": "1. Introduction", "content": "Hyperparameter optimization (HPO) is an important step\nin the design process of machine learning (ML) applica-\ntions to achieve optimal performance for a given dataset\nand performance measure (Snoek et al., 2014; Bischl et al.,\n2023). This is particularly true for deep learning, where hy-\nperparameters describe the neural architecture and steer the\nlearning behavior, e.g., via the learning rate (Zimmer et al.,\n2021). Also, in the age of generative AI and fine-tuning of\nlarge foundation models, HPO is key for achieving state-of-\nthe-art results (Yin et al., 2021; Tribes et al., 2023; Wang\net al., 2023).\nHyperparameters affect the generalization performance of\nmodels in varied ways, with some having a more significant\nimpact on tuning than others (Bergstra & Bengio, 2012;\nHutter et al., 2014; Zimmer et al., 2021). The influence of\nhyperparameters on generalization performance is highly\ncontext-dependent, varying with the dataset characteristics\n(e.g., size, noise level) and the specific performance mea-\nsure being optimized (e.g., accuracy, F1 score) (Bergstra\n& Bengio, 2012; van Rijn & Hutter, 2018). This complex-\nity makes HPO particularly challenging, requiring opaque\nautomated machine learning (AutoML) systems to find opti-\nmal configurations within large search spaces (Feurer et al.,\n2015; Wever et al., 2021). Yet, even after arriving at an op-\ntimized hyperparameter configuration, understanding why\nit outperforms other configurations remains difficult due to\nintricate effects and interactions among hyperparameters.\nDespite their promise, AutoML systems have not fully per-\nmeated user groups such as domain experts, ML practi-\ntioners, and ML researchers (Lee et al., 2019; Bouthillier\n& Varoquaux, 2020; Hasebrook et al., 2023; Simon et al.,\n2023). This lack of adoption stems, in part, from the rigid-\nity of many AutoML systems, which are often difficult to\nadapt to specific use cases, but is also attributed to the lack of\ntransparency and interpretability (Wang et al., 2019; Drozdal\net al., 2020). Studies highlight that a concrete requirement\noften requested by AutoML system users is interpretability\n(Wang et al., 2019; Xin et al., 2021; Hasebrook et al., 2023;\nSun et al., 2023), and its lack has even led users to favor man-\nual development for high-stakes projects (Xin et al., 2021).\nFor ML researchers, explanations of HPO processes are\nparticularly relevant, as they often prioritize understanding\nthe effectiveness of individual ML components and require\ncontrol over key aspects of model behavior. Similarly, Au-\ntoML researchers need such kind of information to analyze\nAutoML systems' performance and behavior. Prior works\non hyperparameter importance analysis (Hutter et al., 2014;\nWatanabe et al., 2023; Theodorakopoulos et al., 2024) and\nhyperparameter effects (Moosbauer et al., 2021; Segel et al.,\n2023) show that addressing these interpretability gaps is\ncritical for building trust and enabling more effective use of\nAutoML systems in a synergetic way with ML experts and\ndata scientists (Lindauer et al., 2024)."}, {"title": "1.1. Contribution", "content": "In this paper, we formalize HYPERSHAP, a post-hoc expla-\nnation framework for hyperparameter importance:\n(1) We define a comprehensive set of 5 explanation games\nand interpret them using the Shapley value and interac-\ntions on three levels: specific configurations, hyperpa-\nrameter spaces, and optimizer bias.\n(2) With HYPERSHAP, we elicit hyperparameter impor-\ntance and interaction structures for various benchmarks,\nobserving the existence of higher-order interactions.\n(3) We apply HYPERSHAP to various explanation tasks\nand demonstrate its versatility."}, {"title": "1.2. Related Work", "content": "Hyperparameter importance (HPI) has gained significant\nattention in machine learning due to its crucial role in jus-\ntifying the need for HPO and in attributing performance\nimprovements to specific hyperparameters (Probst et al.,\n2019; Pushak & Hoos, 2020; 2022; Schneider et al., 2022).\nA variety of approaches have been developed to assess how\ndifferent hyperparameters affect the performance of result-\ning models, ranging from simple (surrogate-based) abla-\ntions (Fawcett & Hoos, 2016; Biedenkapp et al., 2017) to\nsensitivity analyses and eliciting interactions between hy-\nperparameters based on the functional ANOVA framework\n(Hutter et al., 2014; van Rijn & Hutter, 2018; Bahmani et al.,\n2021; Watanabe et al., 2023). In this work, we propose a\nnovel approach to quantifying HPI using Shapley values,\nwith a particular focus on capturing interactions between\nhyperparameters through Shapley interaction indices. We\nfocus on quantifying interactions since, in (Zimmer et al.,\n2021; Pushak & Hoos, 2022; Novello et al., 2023), it has\nbeen noticed that interaction is occasionally comparably\nlow, which could serve as a foundation for a new genera-\ntion of HPO methods that do not assume interactions to be\nomnipresent.\nBeyond quantifying HPI, to better understand the impact of\nhyperparameters and the tuning behavior of hyperparame-\nter optimizers, other approaches have been proposed, such\nas algorithm footprints (Smith-Miles & Tan, 2012), partial\ndependence plots for hyperparameter effects (Moosbauer\net al., 2021) or deriving symbolic explanations (Segel et al.,\n2023), providing an interpretable model for estimating the\nperformance of a learner from its hyperparameters. In this\nwork, we focus on quantifying the impact of tuning a hyper-\nparameter on the performance."}, {"title": "2. Hyperparameter Optimization", "content": "Hyperparameter optimization (HPO) is concerned with the\nproblem of finding the most suitable hyperparameter config-\nuration (HPC) of a learner for a given task, typically consist-\ning of some labeled dataset D and some performance mea-\nsure u quantifying the usefulness (Bischl et al., 2023). \u03a4\u03bf\nput it more formally, let X be an instance space and Y a label\nspace and suppose x \u2208 X are (non-deterministically) asso-\nciated with labels y \u2208 Y via a joint probability distribution\n$P(\\cdot, \\cdot)$. Then, a dataset $D = \\{(x^{(k)},y^{(k)})\\}_{k=1}^n \\subset X \\times Y$\nis a sample from that probability distribution. Furthermore,\na predictive performance measure $u : Y \\times P(Y) \\rightarrow \\mathbb{R}$ is a\nfunction mapping tuples consisting of a label and a proba-\nbility distribution over the label space to the reals. Given an\nHPC $X \\in \\Lambda$, a learner parameterized with $\\Lambda$ maps datasets\n$\\mathbb{D}$ from the dataset space $\\mathbb{D}$ to a corresponding hypothesis\n$h_{\\lambda,D} \\in \\mathcal{H} := \\{h \\mid h : X \\rightarrow P(V)\\}$.\nAs an HPC $X \\in \\Lambda$ typically affects the hypothesis space $\\mathcal{H}$\nand the learning behavior, it needs to be tuned to the given\ndataset and performance measure. The task of HPO is then\nto find an HPC yielding a hypothesis that generalizes well\nbeyond the data used for training. For a dataset $D \\in \\mathbb{D}$, the\nfollowing optimization problem needs to be solved:\n$\\lambda^* \\in \\arg \\max_{\\lambda \\in \\Lambda}  \\mathbb{E}_{(x,y) \\sim P(\\cdot,\\cdot)}[u(y, h_{\\lambda,D}(x))].$"}, {"title": "", "content": "As the true generalization performance is intractable, it is\nestimated by splitting the given dataset D into training DT\nand validation data Dv. Accordingly, we obtain\n$\\lambda^* \\in \\arg \\max_{\\lambda \\in \\Lambda} VAL_u(\\lambda, D)$, with $VAL_u(\\lambda, D) :=$\n$\\mathbb{E}_{(D_T,D_V) \\sim D} \\Big[ \\frac{1}{|D_V|} \\sum_{(x,y) \\in D_V} u(y, h_{\\lambda,D_T}(x)) \\Big]$.\nNaively, hyperparameter optimization can be approached by\ndiscretizing the domains of hyperparameters and conducting\na grid search or by a random search (Bergstra & Bengio,\n2012). State-of-the-art methods leverage Bayesian optimiza-\ntion and multi-fidelity optimization for higher efficiency and\neffectiveness (Bischl et al., 2023)."}, {"title": "3. Explainable AI (XAI) and Game Theory", "content": "Within the field of eXplainable AI (XAI), cooperative game\ntheory has been widely applied to assign contributions to\nentities, such as features or data points for a given task\n(Rozemberczki et al., 2022). Most prominently, to interpret\npredictions of black box models using feature attributions\n(Lundberg & Lee, 2017) and the Shapley Value (SV) (Shap-\nley, 1953). Shapley Interactions (SIs) (Grabisch & Roubens,\n1999) extend the SV by additionally assigning contributions\nto groups of entities, which reveal synergies and redundan-\ncies. Such feature interactions uncover additive structures in\npredictions, which are necessary to understand complex de-\ncisions (Sundararajan et al., 2020). Overall, explanations are\nsummarized by two components (Fumagalli et al., 2024a):\nFirst, an explanation game $v : 2^N \\rightarrow \\mathbb{R}$ is defined as a real-\nvalued set function over the powerset $2^N$ of the n features\nof interest indexed by $N = \\{1, ..., n\\}$. This explanation\ngame restricts the model prediction to subsets of features\nand evaluates a property of interest, e.g. the prediction or\nperformance. Second, given an explanation game, inter-\npretable main and interaction effects are constructed using\nthe SV and SIs. Analogously, in Section 4, we define ex-\nplanation games based on ablations of hyperparameters in\n$VAL_u$ and quantify importances with the SV and SIs.\nExplanation Games via Feature Imputations. Given\nthe prediction of a black box model $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ and an\ninstance $x \\in \\mathbb{R}^n$, baseline imputation with $b \\in \\mathbb{R}^n$ for a\ncoalition $S \\subset N$ is given by $\\oplus_S:\\mathbb{R}^n\\times \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ as\n$v_S^{(b)}(x) := f(x \\oplus_S b)$ with $x \\oplus_S b := \\begin{cases} x_i, & \\text{if } i \\in S, \\\\ b_i, & \\text{if } i \\notin S. \\end{cases}$\nBaseline imputation heavily depends on the choice of base-\nline (Sturmfels et al., 2020), and marginal and conditional"}, {"title": "", "content": "imputation extends the approach to an average over random-\nized baselines (Sundararajan & Najmi, 2020) as\n$v_v^{(p)}(S) := \\mathbb{E}_{b \\sim p(b)}[f(x \\oplus_S b)]$,\nwhere p(b) is either the marginal feature distribution or\nconditioned on $b = x \\oplus_S b$, respectively. Imputed model\npredictions define local explanation games that are used\nto explain predictions of single instances x. In contrast,\nglobal explanation games capture derived properties, such\nas the variance or performance of the imputed model predic-\ntion. It was shown that explanations derived from baseline,\nmarginal and conditional imputation are increasingly influ-\nenced by the feature distribution p (Fumagalli et al., 2024a)."}, {"title": "Shapley Value (SV) and Shapley Interaction (SI)", "content": "An\nexplanation game is additively decomposed by the M\u00f6bius\nInteractions (MIs) $m : 2^N \\rightarrow \\mathbb{R}$ (Muschalik et al., 2024a),\ni.e. the M\u00f6bius transform (Rota, 1964), for $T \\subset N$ as\n$v(T) = \\sum_{S \\subset T} m(S)$ with $m(S) := \\sum_{L \\subset S}(-1)^{|S\\setminus L|}v(L)$.\nThe MIs describe the pure main and interaction effects,\nbut contain $2^n$ non-trivial components in ML applications\n(Muschalik et al., 2024a). As a remedy, the SV and SIs\nsummarize the MIs into interpretable main and interaction\neffects of lower complexity. The SV assigns contributions\nto individuals and is uniquely characterized by four intu-\nitive axioms: linearity (contributions are linear for linear\ncombination of games), symmetry (players with equal con-\ntributions obtain equal payout), dummy (players that do\nnot change the payout receive zero payout), and efficiency\n(the sum of all payouts equals the joint payout). The SV\nsummarizes the MIs as $SV(i) = \\sum_{S \\subset N: i \\in S} m(S)$ for\nall $i \\in N$. In other words, each MI is equally distributed\namong the involved players. Yet, the SV does not yield\nany insights into interactions. Given an explanation order\n$k \\in \\{1, ..., n\\}$, the SIs $k$ extend the SV to assign contri-\nbutions to subsets of players up to size $k$. For $k = 1$ the SIs\nyield the SV, whereas for $k = n$ the MIs. While there exist\nmultiple variants of SIs, a positive interaction indicates a\nsynergistic effect, whereas a negative interaction indicates\nredundancies (on average) of the involved features. For in-\nstance, the Faithful Shapley Interaction Index (FSII) (Tsai\net al., 2023) is defined as the best k-additive approximation\n$\\hat{v}_k(S) := \\sum_{L \\subset S: |L| \\le k} \\Phi_k(L)$ of $v(S)$ across all subsets S\nweighted by the Shapley kernel, cf. Appendix D.2, which is\nuseful to analyze the degree of interaction. The SIs adjust ex-\nplanation expressivity and complexity based on practitioner\nneeds, a framework we now parallel in HPO."}, {"title": "4. HYPERSHAP: Attributing Importance to\nHyperparameters", "content": "In hyperparameter optimization (HPO), explanations are\nneeded on different levels of the HPO process, ranging\nfrom returned configurations to a qualitative comparison of\nentire HPO tools. Here, we limit ourselves to four areas,\ndubbed Ablation, Sensitivity, Tunability, and Optimizer\nBias. First, we introduce Ablation (Section 4.1), which we\nuse as the fundamental backbone of HYPERSHAP. Based\non Ablation, we discuss Sensitivity as an extension of the\nfunctional ANOVA framework by Hutter et al. (2014), and\ncompare it theoretically to our novel approach for Tunability\n(Section 4.2). Tunability is then used to discover Optimizer\nBias (Section 4.3), and we conclude with practical aspects of\nHYPERSHAP (Section 4.4). In the following, we let N be\nthe set of hyperparameters and quantify main and interaction\neffects based on the SV and SIs of the HPI games. All proofs\nare deferred to Appendix A."}, {"title": "4.1. Ablation of Hyperparameter Configurations", "content": "One common scenario for quantifying the HPI is to compare\na hyperparameter configuration (HPC) $\\lambda^*$ of interest to\nsome reference HPC $\\lambda^0$, e.g., the default parameterization\nof a learner as provided by its implementing library or a\ntuned default HPC that has proven effective for past tasks. In\nturn, $\\lambda^*$ can be any HPC obtained through HPO or a manual\nconfiguration. Given $\\lambda^*$ and $\\lambda^0$, the question now is how\nvalues of $\\lambda^*$ affect the performance of the learner relative\nto the reference HPC $\\lambda^0$. To this end, we can transition\nfrom the reference HPC to the HPC of interest by switching\nthe values of hyperparameters one by one from its value in\n$\\lambda^0$ to the value in $\\lambda^*$, which is also done in empirical ML\nstudies and referred to as ablations.\nAblation analysis has already been followed by Fawcett &\nHoos (2016) and Biedenkapp et al. (2017) but restricted to\nsingle hyperparameter ablations that are executed sequen-\ntially. Instead, we consider the HPI game of Ablation using\nall possible subsets, which allows us to capture interactions.\nDefinition 4.1 (HPI Game - Ablation). The Ablation HPI\ngame $V_{G_A} : 2^N \\rightarrow \\mathbb{R}$ is defined based on a tuple\n$G_A := (\\lambda^0, \\lambda^*, D, u)$,\nconsisting of a baseline (default) HPC $\\lambda^0$, an HPC of in-\nterest $\\lambda^*$, a dataset D, and a measure u. Given a coali-\ntion $S \\subset N$, we construct an intermediate HPC with\n$\\oplus : \\Lambda \\times \\Lambda \\rightarrow \\Lambda$ as\n$\\lambda^* \\oplus_S \\lambda^0 := \\begin{cases} \\lambda_i^*, & \\text{if } i \\in S, \\\\ \\lambda_i^0, & \\text{else}, \\end{cases}$\nand evaluate its worth with\n$V_{G_A}(S) := VAL_u(\\lambda^* \\oplus_S \\lambda^0, D).$"}, {"title": "", "content": "The Ablation game quantifies the worth of a coalition based\non the comparison with a default HPC $\\lambda^0$. In XAI termi-\nnology, this approach is known as baseline imputation, cf.\nSection 3. Natural extensions of the Ablation game capture\nthese ablations with respect to a distribution $\\lambda^0 \\sim p^0(\\lambda^0)$\nover the baseline HPC space $\\Lambda$ as\n$\\mathbb{E}_{\\lambda^0 \\sim p^0(\\lambda^0)} [VAL_u(\\lambda^* \\oplus_S \\lambda^0, D)]$,\nwhich relates to the marginal performance introduced by\nHutter et al. (2014). In XAI terminology, it is further dis-\ntinguished between distributions $p(\\lambda)$ that either depend\n(conditional) or do not depend (marginal) on the HPC of\ninterest $\\lambda^*$. While baseline imputation is mostly chosen for\ncomputational efficiency, it was argued that it also satisfies\nbeneficial properties (Sundararajan & Najmi, 2020). Nev-\nertheless, the choice of a baseline has a strong impact on\nthe explanation (Sturmfels et al., 2020). In HPO, we are\ntypically given a default HPC $\\lambda^0$, which we use for the Ab-\nlation game, but our methodology can be directly extended\nto the probabilistic setting."}, {"title": "4.2. Tunability of Learners", "content": "Zooming out from a specific configuration, we can ask to\nwhat extent it is worthwhile to tune hyperparameters. In the\nliterature, this question has been connected to the term of\ntunability (Probst et al., 2019). Tunability aims to quantify\nhow much performance improvements can be obtained by\ntuning a learner comparing against a baseline HPC, e.g.,\nan HPC that is known to work well across various datasets\n(Pushak & Hoos, 2020). In this context, we are interested\nin the importance of tuning specific hyperparameters. A\nclassical tool to quantify variable importance is sensitiv-\nity analysis (Owen, 2013), which measures the variance\ninduced by the variables of interest and decomposes their\ncontributions into main and interaction effects.\nDefinition 4.2 (HPI Game - Sensitivity). The Sensitivity\ngame $V_{G_V} : 2^N \\rightarrow \\mathbb{R}$ is defined based on a tuple\n$G_V := (\\lambda^0, \\Lambda, p^*, D, u)$,\nconsisting of a baseline HPC $\\lambda^0$, an HPC space of interest\n$\\Lambda$ equipped with a probability distribution $p^*$, a dataset D,\nand a measure u. The value function is given by\n$V_{G_V}(S) := Var_{\\lambda \\sim p^*(\\lambda)} [VAL_u(\\lambda \\oplus_S \\lambda^0, D)]$.\nA large value of a coalition $S \\subset N$ in the Sensitivity game\nindicates that these hyperparameters are important to be set\nto the right value. Hutter et al. (2014) implicitly rely on the\nSensitivity game and compute the functional ANOVA de-\ncomposition, quantifying pure main and interaction effects.\nIn game theory, this corresponds to the MIs of the Sensitivity\ngame, which can be summarized into interpretable represen-\ntations using the SV and SIs (Fumagalli et al., 2024a)."}, {"title": "", "content": "While sensitivity analysis is a suitable tool in XAI, it has\nsome drawbacks for Tunability. First, as illustrated below,\nthe total variance being decomposed $V_{G_V}(N)$ strongly de-\npends on the chosen probability distribution $p^*$ and the HPC\nspace $\\Lambda$. Moreover, it does not reflect the performance in-\ncrease expected when tuning all hyperparameters. Second,\nfor a coalition of hyperparameters $S \\subset N$, we expect that\nthe coalition's worth (performance) increases when tuning\nadditional hyperparameters, i.e., $v(S) \\le v(T)$, if $S \\subset T$.\nThis property is known as monotonicity (Fujimoto et al.,\n2006), but does not hold in general for the Sensitivity game\n$V_{G_V}$. For a simple example, we refer to Appendix A.3. In-\nstead, we now propose the monotone Tunability HPI game.\nDefinition 4.3 (HPI Game - Tunability). The Tunability\nHPI game is defined by a tuple\n$G_T := (\\lambda^0, \\Lambda, D, u)$,\nconsisting of a baseline HPC $\\lambda^0 \\in \\Lambda$, an HPC space $\\Lambda$, a\ndataset D, and a measure u. The value function is given by\n$V_{G_T}(S) := \\max_{\\lambda \\in \\Lambda} Val_u(\\lambda \\oplus_S \\lambda^0, D)$.\nThe Tunability game directly measures the performance\nobtained from tuning the hyperparameters of a coalition S\nwhile leaving the remaining hyperparameters at the default\nvalue $\\lambda^0$. The Tunability game is monotone, which yields\nthe following theorem."}, {"title": "Theorem 4.4", "content": "The Tunability game yields non-negative SVs\nand non-negative pure individual (main) effects obtained\nfrom functional ANOVA via the MIs.\nWhile the main effects obtained from the Tunability game\nare non-negative, interactions clearly can be negative, indi-\ncating redundancies of the involved hyperparameters."}, {"title": "Benefits of Tunability over Sensitivity", "content": "We now show-\ncase the benefits of the Tunability game over the Sensitiv-\nity game using a synthetic example. We consider a two-\ndimensional HPC space $\\Lambda := \\Lambda_1 \\times \\Lambda_2$ with discrete HPCs\n$\\Lambda_1 := \\{0,1\\}$ and $\\Lambda_2 := \\{0, ..., m\\}$ for $m > 1$. The opti-\nmal configuration is defined as $\\lambda^* := (1, m)$, and the perfor-\nmance is quantified by $VAL_u(\\Lambda, D) := 1_{\\lambda_1=\\lambda_1^*} + 1_{\\lambda_2=\\lambda_2^*}$,\nwhere 1 is the indicator function. That is, we observe an\nincrease of performance of 1 for each of the hyperparam-\neters set to the optimal HPC $\\lambda^*$. Lastly, we set the HPC\nbaseline to $\\lambda^0 := (0,0)$ or $\\lambda^0 := \\lambda^*$. Intuitively, we expect\nthat both hyperparameters obtain similar HPI scores, since\nthey both contribute equally to the optimal performance\n$VAL(\\lambda^*, D) = 2$. Moreover, if the baseline is set to the\noptimal HPC $\\lambda^*$, we expect the HPI to reflect that there is no\nbenefit of tuning. Since the hyperparameters independently\naffect the performance, we do not expect any interactions."}, {"title": "Theorem 4.5", "content": "The HPI scores of the Sensitivity and Tun-\nability game for the synthetic example are given by Table 1.\nBoth HPI scores correctly quantify the absence of interac-\ntion $\\lambda_1 \\times \\lambda_2$. In contrast to the Tunability game, the Sensitiv-\nity game assigns smaller HPI scores to the hyperparameter\n$\\lambda_2$ due to the larger domain $\\Lambda_2$. In fact, the Sensitivity HPI\nscore of $\\lambda_2$ roughly decreases with order $m^{-1}$. Moreover,\nthe Tunability HPI scores reflect the performance increase\nand decompose the difference between the optimal and the\ndefault performance. In contrast, the Sensitivity HPI scores\ndecompose the overall variance, which depends on $\\Lambda$ and\n$p^*$. Lastly, setting the default HPC $\\lambda^0$ to $\\lambda^*$ decreases the\nTunability HPI scores to zero, whereas the Sensitivity HPI\nscores remain unaffected. In summary, Sensitivity reflects\nthe variability in performance when changing the hyperpa-\nrameters, whereas Tunability reflects the benefit of tuning."}, {"title": "4.3. Optimizer Bias", "content": "The Tunability game aims to explain the importance of hy-\nperparameters being tuned, which can also be used to gain\ninsights into the capabilities of a hyperparameter optimizer.\nIn particular, by comparing the optimal performance with\nthe empirical performance of a single optimizer, we uncover\nbiases and point to specific hyperparameters that the opti-\nmizer fails to exploit. We define a hyperparameter optimizer\nas a function $O : \\mathbb{D} \\times 2^N \\rightarrow \\Lambda$, mapping from the space of\ndatasets and an HPC space to an HPC.\nDefinition 4.6 (HPI Game - Optimizer Bias). The Optimizer\nBias HPI game is defined as a tuple\n$G_O := (\\Lambda, \\lambda^0, O, D, u)$,\nconsisting of a HPC space $\\Lambda$, a baseline $\\lambda^0$, the hyperpa-\nrameter optimizer of interest O, a dataset D and a measure\nu. For $S \\subset N$, we construct $\\Lambda_S := \\{\\lambda \\oplus_S \\lambda^0 : \\lambda \\epsilon \\Lambda\\}$\nand define\n$V_{G_O}(S) := VAL_u(O(D, \\Lambda_S), D) - V_{G_T}(S).$"}, {"title": "", "content": "Intuitively speaking, the value function measures any devi-\nation from the performance of the actual best-performing\nHPC. In other words, with the help of Definition 4.6, we\ncan identify deficiencies of the hyperparameter optimizer O\nover the best-performing solution and, thereby for example,\nidentify whether an optimizer struggles to optimize certain\n(types of) hyperparameters."}, {"title": "4.4. Practical Aspects of HYPERSHAP", "content": "This section addresses practical aspects of HYPERSHAP to\nefficiently approximate the proposed games and generalize\nthem to multiple datasets.\nEfficient Approximation. Naively, to evaluate a single\ncoalition in Definition 4.3, we need to conduct one HPO run.\nWhile this can be costly, we argue that using surrogate mod-\nels that are, e.g., obtained through Bayesian optimization,\ncan be used to simulate HPO, rendering HYPERSHAP more\ntractable. This is similar to other surrogate-based explain-\nability methods for HPO (Hutter et al., 2014; Biedenkapp\net al., 2017; Moosbauer et al., 2021; Segel et al., 2023).\nIn contrast, to analyze Optimizer Bias, we propose to ap-\nproximate $V_{G_T}$ using a diverse ensemble of optimizers\n$\\mathbb{O} := \\{O_i\\}$, and choose the best result for $\\Lambda_S$ obtained\nthrough any optimizer from $\\mathbb{O}$, forming a virtual optimizer\nthat always yields the best-known value. This virtual best\nhyperparameter optimizer (VBO) approximates\n$V_{G_T}(S) \\approx \\max_{X^i=O(D,\\Lambda_S)} VAL(X^i, D)$.\nHPI Game Extensions across Multiple Datasets. In a\nmore general setting, we are also interested in the different\naspects of HYPERSHAP across multiple datasets, where\nwe present direct extensions of the previous games.\nDefinition 4.7 (HPI Game- Multi-Dataset Variants). Given\na collection of datasets $\\mathbb{D} := \\{D_1,...,D_M\\}$ and the\ncorresponding HPI games $v_{G_i}$ for $i \\in \\{1,..., M\\}$ with\n$G \\in \\{G_A, G_V, G_T, G_O\\}$, we define its multi-dataset vari-\nant with the value function\n$v^{\\oplus}(\\S) := \\bigoplus_{i=1}^M v_{G_i}^{D_i}(S)$,\nwhere $\\bigoplus$ denotes an aggregation operator, e.g. the mean, of\nthe game values obtained from the individual datasets $D_i$.\nConsidering HPI across datasets allows for a broader and\nmore comprehensive assessment of the impact of individual\nhyperparameters and how they interact with each other. By\naggregating the value of a coalition over datasets, we can\nevaluate the generalizability of HPI in contrast to identifying\nwhich hyperparameter is important for which dataset. For\ninstance, this can justify recommendations with respect to"}, {"title": "5. Experiments", "content": "In this section, we evaluate the effectiveness and applica-\nbility of HYPERSHAP across various scenarios. The ex-\nperiments demonstrate how our games help explain HPIs,\ninteractions, and biases in HPO. In all experiments we rely\non four HPO benchmarks; lcbench (Zimmer et al., 2021),\nrbv2_ranger (Pfisterer et al., 2022), PD1 (Wang et al.,\n2024), and JAHS-Bench-201 (Bansal et al., 2022). The\nimplementation is based around shapiq (Muschalik et al.,\n2024a). For more details regarding the experimental setup,\nwe refer to Appendix B. Additional results are contained in\nAppendix E and a guide for interpreting interaction-based vi-\nsualizations is presented in Appendix C. Generally, positive\ninteractions are colored in red and negative in blue."}, {"title": "5.1. Insights from Ablation and Tunability", "content": "First, we compare the results of the Ablation and the Tunabil-\nity game in terms of hyperparameter importance and interac-\ntions (cf. Figure 2). We retrieve an optimized configuration\nof PD1's lm1b_transformer scenario and explain it\nwith the Ablation game. HYPERSHAP's explanation shows\nthat the majority of the performance increase is attributed\nto the hyperparameter L-I, which is not surprising since the"}]}