{"title": "REVISITING MULTI-PERMUTATION EQUIVARIANCE THROUGH THE Lens OF IRREDUCIBLE REPRESENTATIONS", "authors": ["Yonatan Sverdlov", "Ido Springer", "Nadav Dym"], "abstract": "This paper explores the characterization of equivariant linear layers for representa- tions of permutations and related groups. Unlike traditional approaches, which ad- dress these problems using parameter-sharing, we consider an alternative method- ology based on irreducible representations and Schur's lemma. Using this method- ology, we obtain an alternative derivation for existing models like DeepSets, 2- IGN graph equivariant networks, and Deep Weight Space (DWS) networks. The derivation for DWS networks is significantly simpler than that of previous results. Next, we extend our approach to unaligned symmetric sets, where equivariance to the wreath product of groups is required. Previous works have addressed this problem in a rather restrictive setting, in which almost all wreath equivariant layers are Siamese. In contrast, we give a full characterization of layers in this case and show that there is a vast number of additional non-Siamese layers in some settings. We also show empirically that these additional non-Siamese layers can improve performance in tasks like graph anomaly detection, weight space alignment, and learning Wasserstein distances. Our code is available at GitHub.", "sections": [{"title": "1 INTRODUCTION", "content": "Learning with symmetries has recently attracted great attention in machine learning. In this learning setting, a group acts on an input space, and the hypothesis mappings are restricted to be equivariant with respect to the group action. Motivated by the structure of fully connected neural networks, group equivariant models are often defined by a composition of parametric linear equivariant func- tions and non-parametric non-linear activations functions (Cohen & Welling, 2016). Thus, character- izing all equivariant layers for a given group action is fundamental for understanding and designing equivariant deep neural networks. Indeed, this question has attracted a considerable amount of at- tention in various scenarios (e.g., Finzi et al. (2021); Kondor & Trivedi (2018); Cohen et al. (2019); Pearce-Crump (2023a;b)).\nFor permutation groups, the standard strategy for characterizing equivariant layers is through study- ing parameter-sharing (Ravanbakhsh et al., 2017). Perhaps the first result in this direction was given by Zaheer et al. (2017), who showed that only two parameters are required to characterize all per- mutation equivariant layers on a set of scalars. Another famous example is Maron et al. (2018), who described the specific parameter-sharing scheme for graph equivariant networks and other tensor rep- resentations of the symmetric group. Recently, parameter-sharing has been applied in Navon et al. (2023a); Zhou et al. (2024b) to characterize all equivariant layers of weight space neural functionals. These neural networks operate on weights of another neural network, a problem with a complex multi-permutation equivariant structure.\nIn this paper, we revisit equivariant linear layer characterization for permutation groups from the perspective of irreducible representations. Given a group G acting on a linear space V, we explore"}, {"title": "2 RELATED WORK", "content": "Learning sets of symmetric elements. When dealing with multiple occurrences of symmetric objects as a set, additional symmetries arise as the set elements can be permuted. In this case, the layers are required to be equivariant under the permutation of elements in addition to the original element group action. Maron et al. (2020) suggested a new layer called Deep Sets for Symmetric Elements (DSS) that operates on a set of symmetric objects. They showed that incorporating DSS layers is strictly more expressive than using Siamese networks. The DSS framework was also used in sub-graph aggregation networks (Bevilacqua et al., 2021), where the network should be invariant to the order of the sub-graphs. In the main setting of Maron et al. (2020), the same group element $g \\in G$ acts uniformly on all set objects. However, the more challenging setting is where we allow different elements $g_i \\in G$ to act on the objects. This corresponds to the action of the restricted wreath product $G \\wr S_k$ on the set. Maron et al. (2020) deal with this setting as well, but only when G is a subgroup of Sn and the action of G is transitive. Later, Wang et al. (2020) characterized all linear mappings for general wreath products, however they also assumed that the group actions are transitive.\nEquivariant Characterization via Irreducibles. Using irreducible representations to character- ize equivariant layers is a popular approach for rotation-equivariant networks (Thomas et al., 2018; Anderson et al., 2019; Dym & Maron). For permutation groups, the primary work which consid- ered this approach, to the best of our knowledge, is Henning Thiede et al. (2020). They suggest using the known characterization of permutation irreducible representations with Young diagrams (Fulton & Harris, 2013) to characterize permutation-equivariant layers and explain how to recon- struct the results from DeepSets and IGN. Our approach for these results is similar but gives a more explicit derivation of the DeepSet and 2-IGN layers. More importantly, our analysis cov-"}, {"title": "3 PRELIMINARIES", "content": "Let $V$ be a finite-dimensional vector space over the field $\\mathbb{F} = \\mathbb{C}$ or $\\mathbb{R}$. Let $G$ be a finite group acting linearly on $V$. We will sometimes say that $V$ is a representation of $G$. We will say $V$ is a trivial representation of $G$ if the action of $G$ on $V$ is the trivial action $gv = v$ for all $g \\in G$ and $v \\in V$.\nA subspace $U \\subseteq V$ is invariant to $G$ if $gu \\in U$ for all $g \\in G$ and $u \\in U$. We say an invariant subspace $U$ is irreducible if it does not strictly contain an invariant subspace except the zero space. By Maschke's theorem, each finite-dimensional space $V$ can be decomposed into a direct sum of irreducible invariant spaces (Fulton & Harris, 2013).\nA mapping $T : V \\rightarrow V$ is equivariant if $T(g \\cdot v) = g \\cdot T(v)$ for every $v \\in V, g \\in G$. Two representations $V, U$ of $G$ are isomorphic if there is an equivariant linear bijection $L_{V,U} : V \\rightarrow U$, which we notate by $V \\simeq U$.\nThe following lemma, named Schur's lemma, is a key component of our work. Schur's lemma describes equivariant mappings between irreducible representations:\nSchur's lemma. Let $V, W$ be finite-dimensional irreducible representations of $G$ over $\\mathbb{F}$, and let $T : V \\rightarrow W$ be a $G$-equivariant linear map \\footnote{The equivariant map is often referenced as a G-module.}. Then:\n*   Either $T$ is an isomorphism or $T = 0$.\n*   If $L : V \\rightarrow W$ is an isomorphism, and $\\mathbb{F} = \\mathbb{C}$, then $T = \\lambda L$ for $\\lambda \\in \\mathbb{C}$.\nIn this paper, we will only consider representations over $\\mathbb{R}$, which are the common setting in appli- cations. In all cases, we will discuss, our real irreducible representations are absolutely irreducible. This means that their natural extension to complex vector spaces is irreducible. In this case, if $L : V \\rightarrow W$ is an isomorphism, $\\mathbb{F} = \\mathbb{R}$, and $V$ is absolutely irreducible, then $T = \\lambda L$ for $\\lambda \\in \\mathbb{R}$ (Boardman). In Appendix B we elaborate more on this topic.\nLayer characterization using Schur's Lemma Equivariant layers from a representation $V$ to itself can be characterized using Schur's lemma via two steps. The first step is decomposition, where we identify the decomposition of $V$ into irreducible representations $V = \\bigoplus_{i=1}^{s} V_i$. Any $x \\in V$ can be written uniquely as a sum $x = \\sum_{i=1}^{s} x_i$ where each $x_i$ is in $V_i$. The decomposition step also requires an algorithm to compute this decomposition. Next, we will need to identify which of the space pairs $V_i$ and $V_j$ are isomorphic, and if they are, specify an isomorphism $L_{ij}$. Finally, since we are working over the reals, we will need to verify that all irreducible representations are absolutely irreducible.\nOnce the decomposition step is carried out, the second step uses Schur's lemma. Equivariant map- pings $T : V \\rightarrow V$ can be written as $T = \\sum_{i,j=1}^{s} T_{ij}$, where $T_{ij} : V_i \\rightarrow V_j$ are equivariant. By Schur's lemma if $V_i$ and $V_j$ are isomorphic then $T_{ij} = \\lambda_{ij} L_{ij}$ for some $\\lambda_{ij} \\in \\mathbb{R}$, and otherwise $T_{ij} = 0$. All in all, we obtain that $T$ is equivariant if and only if for some choice of the scalar $\\lambda_{ij}$ we have\n\\[T(x_1 + ... + x_s) = \\sum_{(i,j), V_i \\simeq V_j} \\lambda_{ij} L_{ij}(x_i).\\]\nThe number of parameters in the expression above depends on the isomorphism relations between the irreducibles. We can assume without loss of generality that the first $1 < t < s$ irreducible spaces are not isomorphic, and by identifying isomorphic irreducibles, we can rewrite the decomposition above as $V = \\bigoplus_{i=1}^{t} V_{a_i}$ (Where $V_{a_i}$ is defined to be the direct sum of $a_i$ copies of $V_i$) where $a_1+...+a_t = s$. The number of parameters $\\lambda_{ij}$ in the decomposition is then $\\sum_{i=1}^t a_i^2$. This process can easily be generalized to compute mappings between representations $V$ and $W$ by decomposing both spaces into irreducibles."}, {"title": "4 COMPUTING LINEAR EQUIVARIANT LAYERS", "content": "We now demonstrate how to derive the results in Zaheer et al. (2017); Maron et al. (2018); Navon et al. (2023a) via decomposition into irreducibles."}, {"title": "4.1 DEEP SETS", "content": "We begin with the simple case of the action of the permutation group $S_n$ on $V = \\mathbb{R}^n$ by permutation of elements (we assume $n \\geq 2$). Here, there are two non-trivial invariant spaces:\n\\[S = \\{ a \\cdot \\mathbf{1}_n | a \\in \\mathbb{R} \\}, \\quad V(n) = \\{ x \\in \\mathbb{R}^n : \\sum_{i=1}^n x_i = 0 \\}.\\]\nAs the first space is one-dimensional, it's irreducible. By Hinton et al. (2006), the second one is also irreducible. These two spaces are not isomorphic since the action of $S_n$ on $S$ is trivial, but the action of $S_n$ on $V(n)$ is not (also they do not have the same dimension when $n \\geq 3$).\nThe decomposition of $x \\in \\mathbb{R}^n$ to a sum of elements from the irreducible spaces can be computed as\n\\[x = \\overline{x} \\mathbf{1}_n + (x - \\overline{x} \\mathbf{1}_n)\\]\nwhere $\\overline{x}$ is the average of $x$, and $\\mathbf{1}_n$ is the all one vector. By Equation (1), Schur's lemma, and the fact that all irreducibles of the permutation group are absolutely irreducible (see Appendix B), the linear equivariant mappings $T : V \\rightarrow V$ are characterized by two parameters $a, b \\in \\mathbb{R}$, that is:\n\\[Tx = a \\overline{x} \\mathbf{1}_n + b (x - \\overline{x} \\mathbf{1}_n).\\]\nThis is exactly the result described in DeepSets (Zaheer et al., 2017)."}, {"title": "4.2 EQUIVARIANT GRAPH LAYERS", "content": "We next consider the setting where $V = \\mathbb{R}^{n \\times n}, G = S_n$ and the group action is defined by $(T_\\tau X)_{ij} = X_{\\tau^{-1}(i), \\tau^{-1}(j)}$, where $P_\\tau$ is the permutation matrix corresponding to $\\tau$. This setting is natural for graph neural networks, as two adjacency matrices $A, B \\in \\mathbb{R}^{n \\times n}$ represent isomorphic graphs if and only if $A = T_\\tau B$ for some permutation $\\tau$. This setting was considered in Maron et al. (2018) using a parameter sharing scheme, and we show how to obtain similar results using irreducibles (note that Maron et al. (2018) also discussed general mappings between $k$-order and $l$-order tensors. In contrast, we only discuss the $k = l = 2$ case).\nWe claim that (when $n \\geq 4$) the space $\\mathbb{R}^{n \\times n}$ can be written as a sum of seven irreducible permutation invariant sub-spaces.\nThe first two spaces are one-dimensional representations on which the action of $S_n$ is trivial: di- agonal matrices with identical diagonal entries and matrices with zero diagonal and identical off- diagonal entries:\n\\[V_0 = \\{ a \\cdot I_n | a \\in \\mathbb{R} \\}, \\quad V_1 = \\{ a \\cdot (\\mathbf{1}_{n \\times n} - I_n) | a \\in \\mathbb{R} \\}.\\]\nNext, we have three spaces of dimension $n - 1$ which are isomorphic to $V(n)$ from equation 2. The first space is the space of diagonal matrices whose diagonal sums to zero\n\\[V_2 = \\{ \\text{Diag}(a_1, .., a_n) | \\sum_{i=1}^n a_i = 0 \\}\\]\nand the next two $n - 1$ dimensional spaces are the space of matrices whose rows (respectively columns) are constant, and columns (respectively rows) sum to zero:\n\\[V_3 = \\{ \\mathbf{1} r^\\top | \\sum_{i=1}^n r_i = 0 \\}, \\quad V_4 = \\{ \\mathbf{1} c^\\top | \\sum_{i=1}^n c_i = 0 \\}\\]\nFinally, there are two larger irreducible spaces:\n\\[V_5 = \\{ A | A = -A^T, A \\mathbf{1}_n = 0 \\}, \\quad V_6 = \\{ A | A = A^T, A \\mathbf{1}_n = 0_n, A_{ii} = 0, \\forall i = 1, ..., n \\}\\]"}, {"title": "4.3 DEEP WEIGHT SPACES", "content": "Recently, there has been growing interest in devising neural operators: neural networks that operate on an input, which is itself a neural network. This type of problem is of interest for various tasks involving post-processing and synthesis of multiple trained neural networks, as well as for process- ing Implicit Neural Representations (INRs), which are a popular alternative for representing certain standard data structures. (see e.g. Kalogeropoulos et al. (2024) for further discussion).\nA key concept in the design of 'neural operators' has been the requirement that they are equivariant to the input neural data (Kalogeropoulos et al., 2024; Kofinas et al., 2024; Zhou et al., 2024a; 2023; Lim et al., 2024) In this section we consider the setting discussed in Navon et al. (2023a); Zhou et al. (2024b), where the neural data is a collection of weights and biases $(W_m, b_m)_{m=1}^M$ representing an MLP of depth M, and a neural operator is constructed by composing standard activation with linear mappings which are equivariant with respect to the multi-permutation action we will now describe:\nThe output of an MLP architecture is invariant to the permutation of its hidden neurons. For example, the MLP defined by $W_2 \\cdot \\text{ReLU} \\cdot (W_1 x)$ will remain the same function if we replace weights $W_2, W_1$ with the weights $W_2 P, P^T W_1$.\nTo define the symmetries of learning on MLPs in full generality, we will adapt the notation from Navon et al. (2023a). We consider the space of MLP parameters with a given fixed depth M and layer dimensions $d_0, ..., d_{M+1}$ (M and all $d_j$ are assumed to be larger than one). These are param- eterized by the vector space $V = \\bigoplus_{m=1}^M (W_m \\oplus B_m)$ where $W_m := \\mathbb{R}^{d_m \\times d_{m-1}}$ and $B_m := \\mathbb{R}^{d_m}$ represent the weights and biases of the m-th layer. The symmetry group of the weight space is the direct product of symmetric groups $G = S_{d_1} \\times \\cdots \\times S_{d_{M-1}}$. An element $g = (\\tau_1, ..., \\tau_{M-1})$ in the group acts on an element $v = [W_m, b_m]_{m \\in [M]}$ from $V$ as follows:\n\\begin{align*}\np(g)v &= [W'_m, b'_m]_{m \\in [M]}, \\\\\nW'_1 &= P_{\\tau_1} W_1, \\quad b'_1 = P_{\\tau_1} b_1, \\\\\nW'_m &= P_{\\tau_m} W_m P_{\\tau_{m-1}}^T, \\quad b'_m = P_{\\tau_m} b_m, \\quad m \\in [2, M - 1] \\\\\nW'_M &= W_M P_{\\tau_{M-1}}^T, \\quad b'_M = b_M.\n\\end{align*}\nwhere $P_{\\tau_m} \\in \\mathbb{R}^{d_m \\times d_{m}}$ is the permutation matrix of $\\tau_m \\in S_{d_m}$.\nPrevious work (Navon et al., 2023a; Zhou et al., 2024b) has already characterized all linear equiv- ariant functions from $V$ to itself. However, this characterization requires tedious bookkeeping and"}, {"title": "5 SETS OF UNALIGNED SYMMETRIC ELEMENTS", "content": "Next, we consider the setting where our data is a $k$-tuple of 'unaligned objects' $(v_1,..., v_k)$, each coming from a representation $V$ of $G$, and we want to learn functions which are equivariant with respect to the joint action of a $k$-tuple of group elements $(g_1,..., g_k)$ on each coordinate indepen- dently, and to a permutation $\\tau \\in S_k$ of the $k$-tuple. This action is given by\n\\[(\\tau, g_1, ..., g_k) \\cdot (v_1, .., v_k) = (g_{\\tau(1)} \\cdot v_{\\tau(1)}, ..., g_{\\tau(k)} \\cdot v_{\\tau(k)})\\]\nThe group for which this action is defined is the semi-direct product of the group $G^k := G \\times G \\times ... \\times G$ and $S_k$, also called the restricted wreath product of $G$ and $S_k$ and denoted by $G \\wr S_k$. For more details see Wang et al. (2020).\nThis type of 'wreath-equivariant-structure' arises in several settings. One is 'alignment problems', where our goal is, given a pair of elements $(v_1, v_2) \\in V^k, k = 2$, to find the group element $g^* = g^*(v_1, v_2)$ which makes $v_1$ 'as similar as possible' to $v_2$. This task is equivariant to application of G elements to each coordinate because if $g^*v_1 \\approx v_2$ then $g_2 g^* g_1^{-1}(g_1v_1) \\approx g_2 v_2$. Similarly, this task is equivariant to permuting $v_1$ and $v_2$, because if $g^*v_1 \\approx v_2$ then $(g^*)^{-1}v_2 \\approx v_1$. For a more detailed derivation, see Chen & Wang (2024); Navon et al. (2023b), which discussed these problems for sets and weight spaces, respectively. Additional examples of 'wreath-equivariant-problems' are the anomaly detection problem discussed in the experimental section, and problems with hierarchical structures as discussed in Wang et al. (2020).\nWreath-equivariant layers. Our aim is to characterize all $G \\wr S_k$ equivariant mappings from $V^k$ to itself. We note that any linear $G$-equivariant mapping $\\widehat{L} : V \\rightarrow V$ induces a 'Siamese' $G \\wr S_k$ equivariant mapping defined by\n\\[L(v_1, ..., vk) = (\\widehat{L}(v_1), ..., \\widehat{L}(v_k)).\\]\nThe interesting question is how many additional mappings are present.\nThis problem was previously studied in Maron et al. (2020); Wang et al. (2020) when G is a finite group acting on $\\mathbb{R}^n$ transitively by permutations (this means that the action of G on [n] has a single orbit). In this setting, the equivariant mappings are composed of the Siamese mappings and a single additional non-Siamese mapping. However, the transitivity assumption does not hold in many exam- ples of interest, such as the graph and weight space examples discussed in this paper. In our analysis, we will release the transitivity assumption, and allow G to be a general finite group. In some cases, this will lead to a substantial number of non-Siamese networks.\nTo characterize $G \\wr S_k$ equivariant functions, we first aim to characterize all invariant irreducible sub-spaces of $V^k$, assuming we know all irreducible sub-spaces of $V$. Important for this decomposition"}, {"title": "Theorem 5.1.", "content": "Let $V$ be a real representation of $G$, with irreducible decomposition\n\\[V = (\\bigoplus_{i=1}^{s} S_i) \\oplus (\\bigoplus_{i=1}^{t} V_i)\\]\nwhere $S_i$ are trivial representations and $V_i$ are not. Then an irreducible decomposition for $V^k$ with respect to the action of $G \\wr S_k$ is given by\n\\[V^k = (\\bigoplus_{i=1}^{s} S_0^i) \\oplus (\\bigoplus_{i=1}^{s} S_1^i) \\oplus (\\bigoplus_{i=1}^{t} V_i^k)\\]\nwhere\n\\[S_0^i = \\{ (s_1, ..., s_k) \\in S^i | \\sum_{i=1}^k s_i = 0 \\}, \\quad S_1^i = \\{ (s, ..., s) \\in S^i \\}\\]\nProof. The fact that $V^i$ is $G$ invariant implies easily that $V_i^k$ is $G^k$ invariant. In the appendix E.1, we will show that the action of $G$ on $V^i$ is not trivial implies that $V_i^k$ is irreducible.\nIn contrast, for the spaces $S^i$ the action of $G$ is trivial, so this representation can be identified with the standard representation $\\mathbb{R}^k$ of $S_k$. The decomposition to $S_0^i$ and $S_1^i$ then follows from the 'DeepSets decomposition' discussed in Section 4.1.\nTo count the number of linear equivariant mappings $L : V^k \\rightarrow V^k$, we note that\n\\[S_i^0 \\simeq S_j^0, \\quad S_i^1 \\simeq S_j^1, \\quad S_i^0 \\ncong S_j^1, \\quad \\forall 1 \\leq i < j \\leq k\\]\nThus, the number of linear equivariant mappings from the 'trivial part' of $V^k$, that is $(\\bigoplus_{i=1}^{s} S_0^i) \\oplus (\\bigoplus_{i=1}^{s} S_1^i)$, to istelf, is $2s^2$, while the number of linear equivariant mappings from the 'trivial part' of $V$ to itself (which is equal to the number of Siamese layers), is $s^2$. As a result, we obtain $s^2$ non-Siamese $G \\wr S_k$ equivariant maps.\nExamples. We found that the number of non-Siamese layers is $s^2$, where $s$ is the number of trivial representations in $V$. Let us consider the implications for the three examples we have discussed earlier:"}, {"title": "Theorem 5.2.", "content": "Let $V$ be a real representation of a finite group G. and let $e_1, ..., e_s$ be a basis to the subspace $V_{\\text{fixed}}$. Let $\\langle \\cdot, \\cdot \\rangle$ be a $G$ invariant inner product on $V$. Then every linear equivariant map $L : V^k \\rightarrow V^k$ is of the form\n\\[L(v_1,..., v_k) = \\sum_{i,j=1}^s \\alpha_{ij} \\left(\\left\\langle v, e_i \\right\\rangle e_j,..., \\sum_{l=1}^k \\left\\langle v, e_i \\right\\rangle e_j\\right) + (\\widehat{L}(v_1), ..., \\widehat{L}(v_k))\\]\nwhere $\\widehat{L} : V \\rightarrow V$ is a linear equivariant map, and $\\alpha_{ij}$ are real numbers. Conversely, every linear mapping of the form defined in equation 9 is equivariant."}, {"title": "6 EXPERIMENTS", "content": "In this section, we consider problems with a wreath-equivariant structure. We compare networks implementing the complete basis of equivariant layers that we have found, with networks that only use Siamese layers or combine a partial list of non-Siamese layers suggested in previous works. Implementation details of all experiments are described in Appendix A.\nGraph Anomaly Detection. We begin with a synthetic wreath-equivariant task in which Siamese networks will fail by design: we consider a graph anomaly detection problem, where the input is k graphs with n nodes, $(G_1,..., G_k)$, where most graphs are similar, and one is an 'anomaly.' The output is a k dimensional probability vector, where the i-th entry denotes the probability that $G_i$ is an anomaly. This task is $G \\wr S_k$ equivariant (where $G = S_n$), where the action on the input space is as in equation 7, and the action on the output space is just permuting the entries of the probability vector. We generate data for this problem as follows: We randomly generate two graphs $G$ and $\\widehat{G}$ using the Erdos-Reyni distribution. We take k 1 copies of the graphs $(G_1,..., G_{k-1})$ to be permuted copies of $G$, and one of them to be $\\widehat{G}$ and insert it in a random location. We also add some noise with variance $\\eta$ to all graphs.\nWe consider equivariant models for this task, composed of several linear wreath-equivariant layers and point-wise ReLU activations. The final layer is a point-wise summation of each of the k graphs to obtain a final vector in $\\mathbb{R}^k$, followed by a softmax layer to obtain a probability vector. For the linear wreath-equivariant layers, we consider several alternatives: Siamese layers only, adding the single additional non-Siamese layer suggested in Wang et al. (2020); Maron et al. (2020), and the full model we suggest, which has four non-Siamese layers (we name our model SchurNet). The Siamese layers are implemented using the decomposition computed in Subsection 4.2.\nThe results of this experiment are shown in Table 1. As\nwe can see, Siamese networks attain 50% accuracy with-\nout noise (and even lower accuracy with noise). This is to\nbe expected since Siamese features can be useful to differ-\nentiate between G and \u011c, but not to determine how many\ntimes each one of them occurs in a k-tuple. In contrast,\nnetworks with non-Siamese layers attain significantly bet-\nter performance, whereas our method, which contains the\nmaximal number of non-Siamese layers,\nattains the best performance."}, {"title": "5.1.", "content": "We note that equation 6 includes almost all information required to compute linear equivariant maps from V to itself. All is left is the decomposition algorithm to write each $(W_m, b_m)_{m \\in [M]}$ as a direct sum of elements in the irreducible decomposition. This can be done independently for each subspace $W_m$ and $B_m$. The decomposition for $W_m$ in equation 6d is not immediate and we will explain it in Appendix D.1.\nThe decomposition in equation 6 and equation 5 also provides substantial additional information. For example, we can immediately see that there are $d_0$ invariant maps from V to $\\mathbb{R} = S$, which correspond to the number of copies of the trivial representation S in V. Moreover, if we are interested in the equivariant maps from a bias space $B_1$ (or weight space $W_1) to another bias space B_j (or weight space $W_j)$, we can easily infer the equivariant mappings from the decomposition in equation 6. For example, when $i = j = M$ there will be $d_1$ mappings since $B_m$ consists of $d_1$ isomorphic representations, and when $i < j = M$ there will be $d_M$ mappings since S appears a single time in $B_i$ and $d_M$ times in $B_M$. Continuing in this way we can reconstruct all the different bias-to-bias, bias-to- weight, weight-to-bias, and weight-to-weight cases analyzed in Tables 5-8 of Navon et al. (2023a), and Tables 8-11 in Zhou et al. (2024b). More importantly, these tables which were necessary for implementing weight space layers in previous work, are not necessary when implementing these layers using Schur's lemma as we suggest."}, {"title": "7 ACKNOWLEDGMENTS", "content": "We would like to extend our thanks to Eitan Rosen for the valuable discussions. We also sincerely thank Idan Tankel for his technical support, as without your help, none of this would have worked as it has."}]}