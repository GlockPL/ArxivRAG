{"title": "WHY GRADIENT SUBSPACE? IDENTIFYING AND MITIGATING LORA'S BOTTLENECKS IN FEDERATED FINE-TUNING OF LARGE LANGUAGE MODELS", "authors": ["Navyansh Mahla", "Ganesh Ramakrishnan"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, particularly\nin task generalization for both text and vision data. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data that is distributed and cannot be\nshared due to privacy concerns. Federated Learning (FL) offers a promising solution for collaborative training\nwithout direct data sharing. However, many parameter-efficient fine-tuning strategies for LLMs in FL, particularly\nthose based on Low-Rank Adaptation (LoRA), face limitations. In this paper, we critically analyze the convergence\nand performance guarantees of popular FL frameworks that utilize LoRA, highlighting its suboptimal nature\ndue to constrained subspace learning of low-rank matrices. We argue that this limitation hinders the effective\nfine-tuning of LLMs in federated settings. Through rigorous analytical and empirical evaluations, we demonstrate\nthat direct weight averaging outperforms LoRA-based strategies, resulting in superior performance for fine-tuned\nmodels. Our comprehensive comparison exposes the inefficiencies inherent in LoRA approaches and underscores\nthe advantages of full-rank weight aggregation. We also extend our analysis to low-rank gradient-based optimizers,\nsuch as GaLore, used during local training steps. Our findings demonstrate that GaLore is a more effective\nalternative, outperforming federated LoRA methods like FlexLoRA and FFA-LORA across both text and image\nmodalities. While privacy considerations remain paramount in FL discourse, our primary focus lies in assessing\nthe performance outcomes of federated fine-tuned models and evaluating various FL frameworks from both\ntheoretical and empirical perspectives. Our findings advocate reassessing the reliance on LoRA within FL contexts,\npaving the way for more efficient training methodologies.", "sections": [{"title": "1 INTRODUCTION", "content": "The past few years have witnessed unprecedented advance-\nments in Large Language Models (LLMs) (Brown et al.,\n2020; OpenAI, 2023; Du et al., 2024; Touvron et al., 2023;\nZeng et al., 2022; Zhang et al., 2022). These language\nmodels (LMs) are powered by Transformer (Vaswani et al.,\n2017) neural network architecture. Since the transformer\nmodels have extensive pre-trained context, they exhibit en-\nhanced generalization capabilities, providing them effective\nfew-shot learning capabilities (Brown et al., 2020). The in-\ntroduction of Vision Transformers (ViTs) (Dosovitskiy et al.,\n2021) has extended the capabilities of transformers to pro-\ncess image modalities. Like their counterparts in language\nprocessing, ViTs are pre-trained on vast image datasets, en-\nabling them to achieve robust contextual understanding of\nvisual content. Models such as CLIP (Contrastive Learning\nImage Pre-training) (Radford et al., 2021a) have facilitated\nthe integration of text and image modalities by aligning\ntheir representations into a shared subspace. This has led\nto the development of Vision-Language Models (VLMs)\nand Large Multimodal Model (LMM) (Liu et al., 2023)\narchitectures, enabling transformer networks to interpret\nand reason over text and image prompts simultaneously,\nthereby enhancing their capabilities in visual question an-\nswering. These pre-trained language models can be further\nfine-tuned to improve performance on specific downstream\ntasks (Radford et al., 2021a). However, the good-quality\ndatasets required for fine-tuning can be distributed and may\nnot be shared directly due to privacy concerns. Researchers\nhave turned to Federated Learning (FL) (McMahan et al.,\n2017) as a means to fine-tune LLMs without compromis-\ning the data privacy (Qin et al., 2024; Zhang et al., 2024a;\nBai et al., 2024; Babakniya et al., 2023). In these settings,\nparameter-efficient fine-tuning methods (Ding et al., 2023)\nlike LORA (Hu et al., 2022) are utilized to minimize compu-\ntational overhead. We analyze the most-recent SOTA LORA\napproaches FlexLoRA (Bai et al., 2024) and FFA-LORA\n(Sun et al., 2024b), and identify potential bottlenecks result-\ning in their performance degradation.\nWe demonstrate that the direct weight aggregation strategy\neffectively addresses the limitations of LoRA in federated"}, {"title": "2 RELATED WORKS", "content": "learning contexts to some extent. Building on this analysis,\nwe theoretically establish that gradient low-rank optimiz-\ners, such as GaLore (Zhao et al., 2024), represent a more\neffective fine-tuning approach for both large language mod-\nels (LLMs) and vision transformers (ViTs) in federated\nsettings. Our theoretical insights are validated through ex-\nperiments that reveal the suboptimal performance of LoRA-\nbased methods in FL environments. Notably, combining\ndirect weight aggregation with GaLore as an optimizer for\nlocal training steps significantly outperforms leading state-\nof-the-art LORA methods like FlexLORA and FFA-LORA.\nWe list the contributions of our paper below:\n\u2022 We highlight the sub-optimal nature of the most recent\nLORA-based SOTA methods like FlexLORA (Bai et al.,\n2024) and FFA-LORA (Sun et al., 2024b) in FL.\n\u2022 We provide theoretical and empirical evidence that\neven with a straightforward aggregation algorithm like\nFedAvg, a framework utilizing full-rank weight ag-\ngregation can yield superior performance in federated\nfine-tuning of LLMs and ViTs.\n\u2022 We also provide analytical and theoretical evidence\nshowing that LLMs and ViTs fine-tuned with the Ga-\nLore optimizer for local training steps when aggregated\nusing full-rank weights with FedAvg, outperform meth-\nods like LoRA. Our study demonstrates this through\na simple experimental setup that employs the GaLore\noptimizer for parameter updates and implements direct\nweight aggregation.\n\u2022 We present our results on text and image modalities us-\ning both vision and language transformers across mul-\ntiple clients and configurations surpassing the current\nSOTA LORA methods and validating our theoretical\nanalysis of the sub-optimal nature of LoRA."}, {"title": "2.1 Parameter Efficient Fine-Tuning of LLMs", "content": "Pre-trained transformers, trained on extensive text or image\ndatasets, gain broad contextual understanding. Fine-tuning\nthese models improve their performance on targeted down-\nstream tasks (Radford et al., 2021b). However, fine-tuning\non consumer-grade GPUs is challenging due to their large\nparameter count, which demands significant GPU memory.\nTo address this computational complexity, researchers have\nproposed Parameter Efficient Fine-Tuning (PEFT) (Ding\net al., 2023) techniques like prompt tuning (Lester et al.,\n2021), adapter tuning (He et al., 2021; Houlsby et al.,\n2019), and Low-Rank Adaptation (LoRA) (Hu et al., 2022).\nAdapter learning techniques add trainable parameters to the\nmodel sequentially while keeping other components frozen.\nThis reduces the overall number of trainable parameters,\nenabling fine-tuning on smaller GPUs. In LoRA, adapters\nare applied in parallel and decomposed into lower-rank ma-\ntrices, further optimizing parameter efficiency. Similarly,\nprefix and prompt tuning involve applying modules in par-\nallel to attention heads (Li & Liang, 2021) or embeddings\n(Lester et al., 2021) to achieve efficient fine-tuning. The\nmodel's fine-tuning performance greatly depends on the\ndataset's size and quality (Sun et al., 2024a). Good-quality\ndatasets are often distributed across multiple parties, as a\nsingle source may be insufficient for effective fine-tuning.\nPrivacy concerns and regulations like the General Data Pro-\ntection Regulation (GDPR) further prevent centralizing data\nfor collaborative efforts, especially under the new EU Act\n(Woisetschl\u00e4ger et al., 2024)."}, {"title": "2.2 PEFT in Federated Learning", "content": "Federated Learning (FL) (McMahan et al., 2017) addresses\nthis issue by enabling collaborative neural network training\nwithout data centralization. It has been successfully applied\nto various architectures, including transformers (Bai et al.,\n2024; Qin et al., 2024; Zhang et al., 2024a; Babakniya et al.,\n2023; Kuang et al., 2023; Cho et al., 2023; Ansell et al.,\n2022; Alam et al., 2022; Niu et al., 2022; Diao et al., 2021).\nHowever, the massive size of large language models de-\nmands substantial resources for inter-client communication\nand local training, highlighting the need for more compute\nand communication-efficient paradigms in FL fine-tuning.\nDue to their efficiency in fine-tuning, PEFT approaches\nare well-suited for FL schemes to solve such problems.\nLORA has been prominently utilized in fine-tuning language\nmodels in FL settings as discussed in several studies (Bai\net al., 2024; Qin et al., 2024; Zhang et al., 2024a; Cho et al.,\n2023; Kuang et al., 2023; Babakniya et al., 2023). Many\nof these studies adopt FedAvg (McMahan et al., 2017) as\nthe aggregation algorithm to aggregate client-side param-\neters onto a server model. For instance, FedIT (Zhang\net al., 2024a) fine-tunes models in decentralized settings by\nsharing and aggregating LoRA matrices (let B be the zero-\ninitialized LoRA matrix and A be the Gaussian initialized\nLORA matrix) separately. SLORA (Babakniya et al., 2023)\nintroduces a two-stage sparse fine-tuning approach with\nimproved LORA matrix initialization for federated learn-\ning. Following the improved initialization, a mechanism\nsimilar to that of FedIT is adopted for distributed training.\nFlexLoRA (Bai et al., 2024) enhances previous methods\nby allowing diverse LoRA weight mixtures across clients,\nclaiming superior performance compared to SLORA in ho-\nmogeneous settings and HETLORA (Cho et al., 2023) in\nheterogeneous settings. Instead of aggregating LoRA ma-\ntrices separately, FlexLoRA multiplies matrices B and A\nbefore aggregation, then decomposes the resulting matrix\ninto low-rank components via truncated SVD, with the de-"}, {"title": "2.3 GaLore in LLMs and ViTs", "content": "GaLore is a subspace gradient learning approach designed\nfor memory-efficient training of LLMs. This method re-\nduces the memory footprint of optimizer states by projecting\ngradient matrices into a low-rank subspace before apply-\ning optimization techniques such as AdamW (Loshchilov\n& Hutter, 2019) or SGD (Ruder, 2017). Recent studies,\nsuch as MedSAGa (Mahla et al., 2024), have empirically\nvalidated GaLore's effectiveness for tasks like image seg-\nmentation. Authors fine-tuned ViT-based Segment Anything\nModel (SAM) (Kirillov et al., 2023) using GaLore, showcas-\ning its applicability beyond traditional language modeling\ntasks."}, {"title": "3 METHODOLOGY", "content": ""}, {"title": "3.1 Problem Setting", "content": "We frame our problem within a multi-silo environment\nwhere each silo (client) hosts the same Large Language\nModel (LLM) M to fine-tune along with a dataset $D_i =$\n$\\{(x, y)_i\\}_{i=1}^n\\}$ (assuming all the N clients have the same\nsample size n) that is non-independent and identically dis-\ntributed (non-IID) compared to datasets of other clients. $D_i^T$\nand $D_i^E$ respectively represent the train and eval split of\nthe dataset $D_i$. A single server facilitates the global aggre-\ngation of client parameters of N clients. Due to privacy\nconcerns, these clients cannot share the data with the server\nfor centralized training. $ \\theta_i \\forall t \\in \\{1, 2, ..., N\\}$ are the model\nparameters at client i and $ \\theta_g $ are the model parameters at the\nserver. We define the distributed learning objective as:\n$\\min_{\\theta_g} \\frac{1}{N} \\sum_{i=1}^{N} L_i(\\theta_i) := \\frac{1}{N} \\sum_{(x, y) \\in D_i^E} l(M(x; \\theta_g), y)$ (1)\nWhere l is the convex loss function. Each client performs\nits own local training step:\n$L_i(\\theta_i) = \\frac{1}{|D_i^T|} \\sum_{(x, y) \\in D_i^T} l(M(x; \\theta_i), y)$ (2)\nAfter some local iterations $T_{agg}$, the parameters are sent to\nthe server for aggregation using FedAvg:\n$\\theta_g := \\frac{1}{\\sum_{i=1}^{N} |D_i^T|} \\sum_{i=1}^{N} |D_i^T| \\theta_i$\nHere, $|D_i^T|$ is the sum of the size of the train split of the\ndatasets of all the clients. These aggregated parameters are\nsubsequently copied back to each client model to resume\ntraining. This allows local iterations to proceed with the\nmodel parameters $ \\theta_g $, which were aggregated during the\nlatest global aggregation step. In our problem setting, we\nassume the dataset sample size to be the same across all\nthe clients i.e. $|D_i^T| = |D_j^T|$ and $|D_i^E| = |D_j^E|$ for all\ni = 1: N."}, {"title": "3.2 Why Not LoRA?", "content": "In this section, we analyze the use of LoRA in FL from\nthe perspective of two of the most recent SOTA LORA FL\nschemes: FlexLoRA (Bai et al., 2024) and FFA-LORA (Sun\net al., 2024b). Both of these methods have presented their\nown analysis of the vanilla LORA FL scheme FedIT (Zhang\net al., 2024a). The authors of FlexLoRA question LoRA's\nefficiency in highly heterogeneous tasks across different\nclients. They discuss the infeasibility of existing LoRA\nsolutions like FedIT in FL settings due to the bucket effect\ncaused by different intrinsic ranks at each client because\nof heterogeneous datasets. On the other hand, authors of\nFFA-LORA perform a rigorous analysis of vanilla LoRA\nmethod like FedIT in the \"client-drift\" (Karimireddy et al.,\n2020) scenario and the amplification of noise in FL settings\nwith DP-SGD (Abadi et al., 2016) due to semi-quadratic\nstructure of LoRA. With thorough analysis, both of these\nmethods were able to outperform vanilla LoRA FL method.\nHere, we present our analysis of why the SOTA LORA\nframeworks like FlexLoRA and FFA-LORA are sub-optimal\nunder multi-client FL settings.\nThe core idea of LoRA is to constrain the weight update on\nthe model by a low-rank decomposition:\n$W_o + \\Delta W = W_o + BA$ (3)\nHere $W_o \\in \\mathbb{R}^{d\\times k}$ is the pre-trained weight matrix which is\nfrozen during the training process. Updates are performed\non $A \\in \\mathbb{R}^{r\\times k}$ and $B \\in \\mathbb{R}^{d\\times r}$. B is initialized as zero\nwhile A uses random Gaussian initialization. Here, $r \\ll$\n$\\min(d, k)$. This reduces the number of training parameters\nby a factor of $ \\frac{r \\times (k + d)}{d \\times k}$ compared to full parameter fine-\ntuning.\nProposition 1 In FL scenarios like FlexLoRA, where pa-\nrameter change matrices $\\Delta W_i$ from N clients are aggre-\ngated with each client i having an intrinsic rank $r_i$, the\nglobally aggregated parameter matrix exhibits rank infla-\ntion following each global aggregation step. Specifically, in\na scenario where all clients have identical ranks $r_i = r$ for\nsimplicity, the rate of rank inflation, denoted by \u03b7 satisfies\n$1 \\leq \\eta \\leq N$ per global aggregation step.\nThe equality on the left holds true when the ranks of all\nmatrices share the same subspace, a condition that becomes\nunattainable when data samples are non-IID across clients."}, {"title": "3.3 How can we improve?", "content": "The previous section offers valuable insights into the sub-\noptimal behavior of LoRA in federated settings. In partic-\nular, the use of low-rank adapters hinders efficient learn-\ning, as the low-rank subspace expands with each global\nFedAvg aggregation step. This is particularly suboptimal\nfor FlexLoRA, while for FFA-LORA, the bounds on excess\nrisk are quadratic. Therefore, we recommend against using\nlow-rank adapters due to their negative impact on perfor-\nmance. Instead, we advocate for the direct averaging of\nparameters in federated settings. The following theorem\ndelineates the excess risk bounds associated with direct\naveraging.\nTheorem 2 For a convex loss function L, let $\\Delta W^*$ denote\nthe optimal weight matrix and $ \\alpha $ represent the learning rate.\nAssuming that the L2 norm of the gradient is bounded, specif-\nically $|\\nabla_w C_i(\\Delta W)||_2 \\leq D$, the excess risk, defined\nas |L(AWagg) - L(AW*)\\, for the aggregated weights\nafter a total of S direct FedAvg aggregations having oc-\ncurred every $t_{agg}$ local training iterations can be expressed\nas follows:\n$|L(\\Delta W_{agg}) - L(\\Delta W^*)| < \\alpha D^2 St_{agg} + c = O(St_{agg})$ (6)\nHere, c is a scalar constant."}, {"title": "3.4 Direct Weight Aggregation and GaLore", "content": "One of the primary challenges associated with direct weight\naggregation, compared to Low-Rank Adaptation (LoRA),\nis its higher computational demand. LoRA provides a\nparameter-efficient fine-tuning approach for large language\nmodels (LLMs), significantly reducing computational re-\nquirements. While direct weight aggregation offers theo-\nretically superior performance bounds, its practical imple-\nmentation can lead to inefficient GPU memory usage. This\nconcern drives our exploration of more parameter-efficient\nand computation-friendly alternatives that maintain perfor-\nmance standards. A straightforward way to enhance pa-\nrameter efficiency is through the use of adapters. However,\nas noted, employing low-rank adapters may result in sub-\noptimal performance for fine-tuned transformer models in\nfederated settings due to their constrained subspace learn-\ning. Therefore, we propose the use of GaLore as a more\neffective solution. GaLore is an optimization method that\nwe adopt in contrary to the existing optimizers since it adds\nmemory efficiency and also provides better generalisation\ncompared to other optimizers (Theorem 3). GaLore projects\nthe weight gradients to a low dimensional sub-space where\nthe optimization algorithm is applied:\n$W_r = W_0 + \\eta \\sum_{t=0}^{T-1} \\hat{G}_t, \\qquad  \\hat{G}_t = P_t \\pi_t (P_t^T G_t Q_t) Q_t^T$ (7)\nWhere $P_t \\in \\mathbb{R}^{d\\times r}$ and $Q_t \\in \\mathbb{R}^{r\\times k}$ are the projection ma-\ntrices of gradient $G_t \\in \\mathbb{R}^{d\\times k}$. $\\pi_t$ is an entry-wise state-\nful gradient regularizer like Adam. The state of $\\pi_t$ can\nbe memory intensive. By projecting the gradients into a\nlow-dimensional space, GaLore significantly reduces the\noptimizer state memory.\nOur analytical comparison indicates that using GaLore as an\noptimizer for federated fine-tuning of large language models\n(LLMs) and vision transformers (ViTs) provides advantages\nbeyond mere memory efficiency, which will be the focus of\nthis paper. GaLore achieves the same bounds on excess risk\nas direct weight averaging (as outlined in Theorem 2), with\ndetailed proofs in the supplementary material. Theorem 3\nfurther demonstrates that direct averaging with GaLore as\nan optimizer results in better generalization of the trained\nmodel compared to direct weight averaging methods that\nemploy full-rank gradient-based optimizers like Adam or\nAdamW. The concept of gradients becoming low-rank is\nspecifically applicable to the Feed-Forward Neural Network\n(FFN) layers. In the context of transformer neural networks,\nthe GaLore paper demonstrates that the low-rank projection\nof gradient matrices can be effectively applied to its lower\nMLP layers like project-up (see Lemma B.6 from GaLore\npaper) whose gradients become low-rank during training.\nConsequently, unless stated otherwise, we conduct all our\nanalyses and experiments on the lower MLP layers.\nTheorem 3 Let N denote the number of clients, each pos-\nsessing a dataset with n samples. We consider the weights\nof a lower-level MLP layer represented by $W \\in \\mathbb{R}^{d\\times k}$. Un-\nder the assumption that the loss function of each client is\n\u03c3-sub-Gaussian with respect to the data distribution of that\nclient with respect to the corresponding weight matrix, we\nderive the following generalization error bounds for weight\naggregation:\na) For direct weight aggregation using FedAvg with standard\noptimizers (such as Adam, AdamW, etc.), the generalization\nerror bound is given by:\n$E_1 \\leq \\frac{1}{N} \\sum_{i=1}^{N} \\frac{2\\sigma^2}{n} (dk)$ (8)\nb) or direct weight aggregation using FedAvg with a low-\nrank gradient-based optimizer, such as GaLore, the general-\nization error bound can be expressed as:\n$E_2 \\leq \\frac{1}{N} \\sum_{i=1}^{N} \\sqrt{\\frac{2\\sigma^2}{n} \\sum_{j=1}^{d} \\sum_{l=1}^{k} (\\frac{E_\\tau (\\hat{W}^{(j)}_{il,t})}{log (f(\\hat{W}^{(j)}_{il,t}))})}$ (9)\nwhere $\\hat{W}^{(i)}_t[l]$ represents the element of lth index in\nthe jth row of the matrix $W_i$. Here, $f(\\hat{W}^{(i)}_t[l])$ = $\\frac{exp (\\hat{W}^{(j)}_{il}))}{\\sum_il exp (\\hat{W}^{(j)}_{il}))}$\nbe the function to represent the ratio\n(probability) in a succinct manner such that as $t \\rightarrow \\infty$,\ni.e. as the federated training continues $f(\\hat{W}^{(i)}_t[l]) \\rightarrow \\frac{1}{k}$ and as a result, the generalization error E2 at the end of the\ntraining becomes upper bounded by $\\sum_{i=1}^N \\sqrt{\\frac{2\\sigma^2d log k}{n}}$ which is much smaller than that of the part (a) ($E_1 \\leq \\frac{1}{N} \\sum_{i=1}^{N} \\frac{2\\sigma^2}{n} qdk$).\nDirect weight aggregation with GaLore for local training\niterations demonstrates significantly improved upper bounds\ncompared to standard direct weight aggregation. As train-\ning progresses to convergence ($t \\rightarrow \\infty$), the GaLore-based\napproach achieves superior generalization error compared\nto conventional methods. A notable observation is the de-\ncrease in overall entropy during GaLore-based direct Fe-\ndAvg aggregation, indicating increasingly structured weight\nmatrices. This structural improvement directly correlates\nwith enhanced generalization performance. The integra-\ntion of GaLore optimizer with FedAvg for local training\nresults in an effective rank reduction (under fixed quanti-\nzation bits q), constraining the learning process to a more"}, {"title": "3.5 Federated Fine-Tuning Using GaLore (FedFTG)", "content": "even a straightforward aggregation algorithm like FedAvg\ncan lead to significantly improved results if paired with a\nmore efficient framework.\nBased on our previous analysis, we have identified limita-\ntions in LoRA and the low-rank adapter tuning approach for\nLLMs in federated settings. To address these shortcomings,\nwe advocate for direct averaging of model parameters, thus\nmaintaining a near full-rank structure without the involve-\nment of adapters. To make the method more parameter-\nefficient we follow upon the results from the JoMA paper\n(Tian et al., 2024) that states that we do not need to explic-\nitly update the self-attention parameters since it is already\nimplicitly incorporated in the lower layer of MLP weight.\nConsequently, we focus on fine-tuning the lower MLP lay-\ners (project-up) of the transformer neural network. (See\nFigure). This is an important insight from Theorem 1 of\nthe JOMA paper which we list below:\n$\\frac{1}{2} z_m(t) = \\sum_k \\frac{1}{2} v_k^2(t) - ||v_k(t)||^2 b_m + c$ (10)\nwhere $z_m(t)$ are the attention logits, and $v_k(t) := U_k \\vec{w}_k$\nwhere $\\vec{w}_k$ are the weights for the kth node of lower MLP\nlayer. Since the information on attention logits is implicitly\ncontained in the MLP weights, we do not need to fine-tune\nthe attention modules separately. As a result, we advocate\nfor the use of lower MLP layers (project-up) for fine-tuning\nand aggregation. This aligns with our Theorem 3, which\nspecifically addresses the lower MLP layers. Since the infor-\nmation from the self-attention layer is implicitly represented\nwithin these lower MLP layers, and they tend to exhibit\nlower generalization error in federated settings, we recom-\nmend focusing on fine-tuning only the lower MLP layers.\nTo update the parameters in FedFTG, we employ GaLore,\nas discussed in the previous section. This approach of-\nfers improved memory efficiency and demonstrates tighter\nbounds on excess risk compared to LoRA. Additionally, it\nfacilitates efficient weight matrix updates in a low-rank sub-\nspace for the layer right after the self-attention layer (lower\nlevel of MLP layer). Clients locally fine-tune their respec-\ntive models on their own datasets using GaLore. Global\nweight aggregation occurs after a specified number of local\niterations. Due to the parameter-efficient nature of this fine-\ntuning approach, only the non-frozen parameters (i.e., the\nparameters that are updated) are transmitted to the server for\nglobal aggregation. These weights are then aggregated at the\nserver using the FedAvg algorithm. This approach reduces\ncommunication overhead by transferring only a subset of\nparameters. Additionally, the reduction in the number of\nparameters, coupled with the use of GaLore, enables clients\nwith limited computational resources to run local training\niterations of these LLMs. The complete FedFTG algorithm\npipeline is illustrated in Algorithm. FedFTG is an experi-\nmental setup that aims to show the empirical evidence for\nour theoretical analysis in comparison to the current SOTA\nLORA strategies. It successfully prevents the constrained\nlow-rank learning of LoRA methods like FlexLoRA and\nshows stable training."}, {"title": "4 EXPERIMENTS", "content": "In this section, we aim to demonstrate the convergence, sta-\nbility, and efficiency of using GaLore as an optimizer in the\nfederated fine-tuning of LLMs. We do this by comparing the\nperformance of model trained using FedFTG with the model\ntrained using current SOTA LORA methods like FlexLoRA\nand FFA-LORA. We evaluate its performance across both\ntext and image modalities to highlight its effectiveness on\nLLMs and ViTs. For text modality, experiments are con-\nducted with 3 and 4 clients, while for vision, experiments\nare conducted with 3, 4, and 5 clients. Each client hosts\na non-IID dataset distinct from those of other clients. We\nperformed all of our experiments on a cluster of Nvidia\nA6000. Details regarding the cluster setup are discussed in\nthe supplementary material."}, {"title": "4.1 Datasets", "content": "We conduct experiments on both text and image datasets,\nincorporating various downstream tasks. For text, we use\nthe MedQuAD (Ben Abacha & Demner-Fushman, 2019)\nand Databricks Dolly 15k (Conover et al., 2023) datasets.\nMedQuAD is a medical question-answering dataset con-\ntaining 47,457 question-answer pairs sourced from 12 NIH\nwebsites, covering 39 question types related to diseases,\ndrugs, and other medical entities. However, owing to the\nMedlinePlus copyright, answers from the 3 subsets were re-\nmoved. Databricks Dolly-15k contains 15,000 high-quality\nhuman-generated prompt/response pairs designed for in-\nstruction tuning LLMs. It includes various categories like\nbrainstorming, classification, summarization, and question\nanswering. For experiments related to vision modality, we\nutilize the Brain Tumour classification dataset (Cheng, 2017)\nwhich comprises of 3,064 T1-weighted contrast-enhanced\nMRI images from 233 patients, categorized into three tumor\ntypes: meningioma (708 slices), glioma (1,426 slices), and\npituitary tumor (930 slices). The task is to identify the cor-\nrect tumor type by having the MRI image fed as an input to"}, {"title": "4.2 Non-IID Data Preparation", "content": "To simulate non-IID conditions, we used Dirichlet Allo-\ncation to partition each dataset into several non-IID splits\nsimilar to (Zhang et al., 2024a; Hsu et al., 2019). For the\ntext datasets (MedQuAD and Dolly 15k), we generated 4\nsplits, whereas for the Brain Tumor Classification dataset,\nwe created 5 splits. The concentration parameter a was set\nto 0.1, resulting in highly skewed distributions across clients\nthat replicate non-IID behavior in FL settings. Splits are\ncreated based on labels. For MedQuAD dataset, the label is\nquestion_type while for Dolly-15k it is category. Class label\ndistribution across each non-IID shard for both MedQuAD\nand Dolly-15k datasets can be found in Figure 7b and 7a.\nSimilar class label distribution for the Brain Tumour Clas-\nsification Dataset is shown in figure 8a. A more detailed\ndescription of dataset pre-processing and non-IID sharding\ncan be found in the supplementary material."}, {"title": "4.3 Models and Hyperparameters", "content": "For experiments on text datasets (MedQuAD and Dolly-\n15k), we utilize Gemma-2B (Team et al., 2024) and TinyL-\nlama (Zhang et al., 2024b). In FedFTG, we fine-tuned the\nup-proj MLP layer, which follows the self-attention mod-\nule. Conversely, FlexLoRA and FFA-LORA focused on\nfine-tuning the attention modules (query, key, value) with a\nLORA rank of 8 and a scaling factor of 16, in line with their\noriginal experimental setups. For vision modality experi-\nments, we fine-tuned SigLIP (Zhai et al., 2023). In FedFTG\napplied on SigLIP, we fine-tuned the MLP layer immedi-\nately after the attention module and the classifier layer (see\nsupplementary material for more details). FlexLoRA and\nFFA-LORA, on the other hand, fine-tuned the attention pa-\nrameters (query, key, value) with a LoRA rank of 8 and a\nscaling factor of 32."}, {"title": "4.4 Training and Evaluation", "content": "We conducted experiments with multiple client configura-\ntions. For text datasets, we evaluated FedFTG with 3 and\n4 clients. With 3 clients, shards 1, 2, and 3 of each dataset\nwere used, while with 4 clients, all shards were utilized.\nSimilarly, for the Brain Tumor Classification Dataset, we\napplied the same shard assignments. Each shard was split\ninto training and test sets, with 1% of samples reserved for\ntesting in the text datasets and 5% for the vision dataset.\nThe global evaluation set was created by combining the\ntest sets from each shard, containing unseen samples from\ndifferent non-IID shards, which helps assess the model's\ngeneralization ability in distributed non-IID settings. Text\ndatasets were trained with a batch size of 1, while the image\ndataset used a batch size of 2, with all models trained for 3\nepochs."}, {"title": "4.5 Experiment Results", "content": "Table 2 shows the results on text datasets. We report the\nROUGE_L F1 (Longest common subsequence ROUGE)\nscore (Lin, 2004) and the BLEU-4 (4-gram) (Papineni et al.,\n2002) score for text datasets. Out of 3 epochs, we report the\nresults of the epoch which has the best result. This is done\now"}]}