{"title": "A STEP TOWARDS MIXTURE OF GRADER: STATISTICAL\nANALYSIS OF EXISTING AUTOMATIC EVALUATION METRICS", "authors": ["Yun Joon Soh", "Jishen Zhao"], "abstract": "The explosion of open-sourced models and Question-Answering (QA) datasets emphasizes the\nimportance of automated QA evaluation. We studied the statistics of the existing evaluation metrics\nfor a better understanding of their limitations. By measuring the correlation coefficients of each\nevaluation metric concerning human-like evaluation score, we observed the following: (1) existing\nmetrics have a high correlation among them concerning the question type (e.g., single word, single\nphrase, etc.), (2) no single metric can adequately estimate the human-like evaluation. As a potential\nsolution, we discuss how a Mixture Of Grader could potentially improve the auto QA evaluator\nquality.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) are widely adopted across various tasks including Question-Answering (QA) tasks.\nMore and more models including the fine-tuned models and the dataset used for fine-tuning are being released daily.\nThis explosion in the number of models, and datasets emphasizes the importance of accurate automatic evaluation for\nout-of-the-model language model training as well as gauging their QA capabilities.\nHowever, varying question types (short-form, long-form, open-ended, etc.) and ambiguity in the grading rubric make\nit difficult to properly gauge each model's capability objectively for QA tasks. No single existing evaluation metric\ncan capture the language model's QA answer capability for multiple quality types. For example, Exact Match (EM)\nis a widely adopted all-or-nothing evaluation metric that shows a high correlation with human-evaluated scores for\nshort-form QA tasks but is too strict to give credit for any semantically identical answer. The lack of an objective\ngrading rubric for varying QA types creates a bias in summary statistics. For example, half credit for an open-ended\nquestion is regarded equally as half credit for a simple factual question.\nIn this paper, we (1) deploy statistical approaches to characterize various existing evaluation metrics, (2) the effectiveness\nof recent ChatGPT-01-preview model [6] as QA grader, and (3) potential solution, a Mixture Of Grader (MOG), which\nfirst classifies each (question, gold answer) pair into one of the predefined QA type class and select the appropriate\nevaluation metric accordingly for an advanced automatic evaluation that better \"correlates\u201d to human evaluator."}, {"title": "Background and Related Work", "content": "Both industry and academia released various fine-tuned language models along with the dataset used for fine-tuning.\nAs some reports [7] warn of the depletion of trainable data soon, generating quality datasets via LLMs is also gaining\nattention [3]. With the explosion of the open-sourced models and datasets, the need for good quality QA evaluation\nmetrics surfaces [1]."}, {"title": "Problem Statement", "content": "We define the problem as follows; Given a list of \u201cgold\u201d answers (typically aggregated list of multiple human annotations)\nand an attempted answer, a QA evaluation metric returns a real number, score, where 0 < score < 1 and a score close\nto 0 would typically indicate that the attempted answer is likely to be incorrect."}, {"title": "Automatic Evaluation Metrics", "content": "Several automatic evaluation metrics have been proposed ranging from token-counting to agentic evaluation approach.\nWe briefly describe them and their advantages and disadvantages.\nToken Counting Exact Match (EM) is a discrete metric (either 0 or 1) that is often too strict for semantically\ncorrect answers. As a complementing metric, many works report token-level F1 Score. F1 score, a harmonic mean of\nprecision and recall, is more permissive than EM giving a partial score (score \u2208 [0, 1]). On the other hand, F1 scores\nare misleading when a model includes an additional explanation as part of the attempted answer because the additional\ntokens reduce the precision and thus the F1 score. For this reason, several papers included just the token-level recall as\nan evaluation metric. The downside of the recall metric is that naively repeating the context for contextual QA would\nyield a high score as it does not get penalized for lengthy answers.\nN-gram Metrics Bilingual Evaluation Understudy (BLEU) measures the n-gram overlap between the predicted\nanswer and the gold answer. It is calculated as the geometric mean of n-gram precision (i.e., true positive count /\n(true positive + false positive count)). Unlike naive token counting, BLEU rewards a sequence of token overlaps\ncapturing the partial correctness more objectively than token-level metrics. Recall-Oriented Understudy for Gisting\nEvaluation (ROUGE) also leverages n-grams and measures how many n-grams in the correct answer are also present\nin the generated answer. Many variants have been proposed and we use the most widely adopted version, which takes\nthe Longest Common String instead of a naive n-gram. The benefit of ROUGE-L is that it captures the sentence-level\nstructural correctness, which may be difficult for BLEU.\nMachine Learned Evaluation Metric Recent works leverage the machine-learned method as a QA grader [4, 8, 2, 5].\nDespite their ability to better understand the evaluation instructions and return a score with logical justification,\nthe evaluation quality fluctuates from model to model and prompt to prompt. Furthermore, agentic evaluators are\ncomputationally expensive which may impact the overall training performance if the QA task evaluation is included as\npart of the training loop."}, {"title": "Proposal", "content": "Instead of searching for a single metric that can handle all QA types properly, we propose a Mixture Of Grader\n(MOG), which first classifies the question and answer pair and selects the appropriate evaluation metric based on the\nclassification outcome. As the first step, we study the strengths and weaknesses of existing metrics. With the help of the\nChagGPT-01-preview model, we devised the QA types including a single word, numerical, paragraph, code snippet,\nsentence, equation, phrase, name, boolean, list, symbol, single character, formula, long paragraph, essay, and short\nparagraph."}, {"title": "Preliminary Results", "content": "We discuss the experiment setups, visualize the statistics result, and share our interpretation of the result."}, {"title": "Experiment Setup", "content": "We assume that the ChatGPT-01-preview model performs human-like evaluation on a (question, gold answer, attempted\nanswer) tuple and study the correlation between the evaluation metric and the ChatGPT's evaluation score.\nDataset We synthesized the dataset by asking the ChatGPT-01-preview model to generate data samples of (question,\ngold answer, attempted answer, score, justification, question type, answer type) tuple. A total of 359 data were generated\nwith an average score of 0.42 and a standard deviation of 0.42. Each data has 6 fields and we count the number of\nunique entries per field in Table 1."}, {"title": "Main Result - Pedant is the winner", "content": "To quantify, how well each evaluation metric \"estimates\u201d the human-like evaluation, we measure the arithmetic mean of\nthe absolute distance.\nMean of Score Delta = $\\frac{\\sum_{i=1}^n abs(s_i - S_i)}{n}$"}, {"title": "Comparison of Evaluation Metric", "content": "To better understand the difference between each evaluation metric, we measured the correlation of each evaluation\nmetric score to the human-like evaluation score (Figure 3). The key observation is that the Pedant score showed the\nhighest correlation (0.77) with the human-like score. This result aligns with the main result in (Figure 2), where Pedant\nshowed the least deviation from the human-like evaluation. The observation also implies that a trained solution may\nwork better than existing evaluation metrics and better estimate human-like evaluations.\nFrom the correlation matrix, we further observed that many of these metrics show high correlations. For example,\nthe F1 Score showed a high resemblance to most evaluation metrics, supporting its popularity in various works. As\nexpected, the random scores were uncorrelated with any of the existing metrics."}, {"title": "Per Answer Type Analysis", "content": "For deeper understanding, we measured per answer type (e.g., single word, paragraph, etc.) correlation coefficients.\nFigure 4 shows the Pearson Correlation Coefficient per answer type across multiple evaluation methods. We make the\nfollowing observations.\n\u2022 Exact Match (EM) was particularly strong for short-form answers such as single word, numerical, name, list,\nand formula, but not for long-forms such as essay, long paragraph, paragraph, short paragraph, or sentences.\n\u2022 F1 Score showed a slightly lower coefficient than EM, but higher for answer types where EM suffered to\ncorrectly evaluate.\n\u2022 Pedant performed worse than EM for short-forms but had a better correlation for long-form answers (essay or\nlong paragraph) resulting in a better correlation in general (Figure 3).\nFrom the observation, we conclude that there is no \"one-size fits all\" evaluation metric. A better direction for a more\nconcrete automatic evaluator would be to first classify the question and answer types and use different evaluation\nmetrics for each type. We left the details of question and answer type classification and per-type evaluation metric\nselection for future work."}, {"title": "Discussion and Limitation", "content": "We articulate the limitations of the current study.\n\u2022 The generated dataset has a limited number of samples for different answer types. This was mainly due to the\nlimited query count to ChatGPT-01-preview.\n\u2022 The number of samples for each answer type varies significantly. We left the proportion to be non-uniform as\nit may reflect the real-world scenario (i.e., users may ask a question that can be answered with a single word\nmuch more often than asking a question with a symbol as an answer).\n\u2022 The TF-IDF tokenizer can be useful for many grading metrics but cannot properly vectorize symbols or\nequations. Trying with an alternative embedding model is left for future work.\n\u2022 Several additional automatic evaluators [8, 2] were recently proposed but left for future work."}, {"title": "Conclusion and Future Work", "content": "We explored the pros and cons of existing QA evaluation metrics concerning the score correlation to state-of-the-art\nLLM evaluator. We show that each metric has strengths and weaknesses based on QA type and motivate a QA type\nclassifier and QA type-based metric selection."}, {"title": "ChatGPT-01-preview's Justification for Scores", "content": "Table 2 shows some samples of ChatGPT's justification for giving the score. It resembles how an actual human evaluator\nwould give a partial score."}, {"title": "Score Distribution Details", "content": "Figure 5 shows the score score distribution per question type."}, {"title": "Per Answer Type Mean Delta Score", "content": "Figure 6 is a per-answer type detailed version of Figure 2."}, {"title": "Additional Correlation Coefficient", "content": "Although some prior works [3, 1] used the Spearman and/or Kendall's correlation coefficient, we moved these results\n(Figure 7 and Figure 8, respectively) to appendix because these ranking based statistics were more suitable for ELO\nranking based score as detailed in the prior work [3]."}]}