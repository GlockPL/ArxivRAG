{"title": "Mixed-State Quantum Denoising Diffusion Probabilistic Model", "authors": ["Gino Kwun", "Bingzhi Zhang", "Quntao Zhuang"], "abstract": "Generative quantum machine learning has gained significant attention for its ability to produce quantum states with desired distributions. Among various quantum generative models, quantum denoising diffusion probabilistic models (QuDDPMs) [Phys. Rev. Lett. 132, 100602 (2024)] provide a promising approach with stepwise learning that resolves the training issues. However, the requirement of high-fidelity scrambling unitaries in QuDDPM poses a challenge in near-term implementation. We propose the mixed-state quantum denoising diffusion probabilistic model (MSQuDDPM) to eliminate the need for scrambling unitaries. Our approach focuses on adapting the quantum noise channels to the model architecture, which integrates depolarizing noise channels in the forward diffusion process and parameterized quantum circuits with projective measurements in the backward denoising steps. We also introduce several techniques to improve MSQuDDPM, including a cosine-exponent schedule of noise interpolation, the use of single-qubit random ancilla, and superfidelity-based cost functions to enhance the convergence. We evaluate MSQuDDPM on quantum ensemble generation tasks, demonstrating its successful performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Quantum machine learning (QML), integrating quantum mechanical principles such as entanglement and superposition to optimize and approximate complex functional behavior, has demonstrated superior performance compared to classical approaches in certain optimization problems and potential efficiency in specific contexts [1-4]. Variational quantum algorithms (VQAs), designed using parameterized quantum circuits (PQCs) or variational quantum circuits (VQCs) with classical optimization, have enabled the development of QML algorithms in conjunction with various applications on noisy intermediate-scale quantum (NISQ) devices [5-7].\nClassical deep generative models (DGMs) such as generative adversarial network (GAN) [8], variational autoencoder (VAE) [9], and attention-based models like Transformers [10] have substantially advanced machine learning by producing realistic samples across various domains such as vision [11], language [12], speech [13], and audio generation [14]. Among the plethora of versatile generative models, denoising diffusion probabilistic models (DDPMs) and their variations [15-17], which represent a branch of hierarchical VAEs incorporating a thermodynamic approach in deep learning [18, 19], have provided another way to create realistic samples. The model includes two main components. The forward process gradually injects Gaussian noise into the data and transforms it into complete Gaussian white noise. The backward process learns from intermediate forward samples to effectively remove noise using deep neural network.\nInspired by the success of classical DGMs, quantum generative models, including quantum generative adversarial network (QuGAN) [20-22], quantum variational autoencoder (QVAE) [23], tensor network [24], and diffusion-based quantum generative model [25-29] have been presented and studied extensively for their potential in quantum data generation and their applications in real-world tasks. In particular, the quantum denoising diffusion probabilistic models (QuDDPMs) [25] adopt quantum scrambling as the forward process and VQCs with projective measurements as the backward process. Via the forward scrambling process creating interpolation between target and noise, QuDDPM resolves the training issue that plagues VQC-based algorithms [30, 31]. However, the scrambling-based forward process requires multiple steps of high-fidelity unitaries, creating a challenge in near-term implementation. In addition, the implementations of QuDDPM in Ref. [25] are restricted to pure states, leaving the general mixed state unexplored.\nTo address the limitations, we propose the mixed-state quantum denoising diffusion probabilistic model (MSQuDDPM) as a generalization of the QuDDPM framework. This model adopts the depolarizing noise channels [32] as the forward process and maintains PQCs with projective measurements as the backward process [25]. MSQuDDPM extends the capability of QuDDPMs to produce both pure and mixed quantum state ensembles and provides computationally efficient training and sample generation. Compared to the original QuDDPM designed for pure state generation, the protocol in this work achieves comparable performance in the same tasks while reducing the implementation complexity. By utilizing superfidelity-based cost functions, adapting single-qubit"}, {"title": "II. FORMULATION OF MSQUDDPM", "content": "In this section, we describe the architecture of MSQuDDPM, focusing on its training strategy and cost functions. MSQuDDPM contains two main components: a forward diffusion process and a backward denoising process, as illustrated in Fig. 1 (b). Similar to classical DDPMS, MSQuDDPM gradually injects noise to convert the original distribution at t = 0 into simple but noisy distributions, such as Haar random states and maximally mixed states at t = T [15, 25]. The backward process uses PQCs to sequentially approximate the recovery of the noise induced by the forward diffusion from t = T to t = 0."}, {"title": "A. Forward diffusion process", "content": "The forward diffusion process begins with an n-qubit initial quantum state ensemble {$ \\{ \\rho_0 \\} $}, consisting of $n_{data}$ individual quantum states (pure or mixed), sampled from an unknown distribution. Throughout the process, the model iteratively applies T discrete quantum channels $ \\Phi_1, \\Phi_2, ..., \\Phi_T $, generating incremental noisy ensembles {$ \\{ \\rho_1 \\} $}, {$ \\{ \\rho_2 \\} $},..., {$ \\{ \\rho_T \\} $}. We employ a depolarizing channel as a noise provider, which leads the state ensemble toward a totally mixed state [32, 33]. In detail, at time t, the relation between the states $ \\rho_t^{(i)} \\in \\{ \\rho_t \\} $ and $ \\rho_{t+1}^{(i)} \\in \\{ \\rho_{t+1} \\} $ is given by\n$$\\rho_{t+1}^{(i)} = \\Phi_{t+1}(\\rho_t^{(i)}) = (1 - q_{t+1})\\rho_t^{(i)} + q_{t+1}I/d,$$\nwhere $q_{t+1} \\in (0,1] $ is the depolarizing parameter, and I/d represents the maximally mixed state.\nFor the noise scheduling of depolarizing parameters, we adopt linear [15] and cosine scheduling [34] from classical DDPM. However, in the multi-qubit system, the forward samples rapidly converge to maximally mixed states, precluding meaningful training during the early stages of the backward process. Thus, we propose the cosine-exponent schedule to amplify the preservation effect of the initial state ensemble, denoted by\n$$q_t = \\frac{f(t)}{f(0)},$$\nwhere\n$$f(t) = cos^k(\\frac{t/T + \\epsilon}{1 + \\epsilon} \\frac{\\pi}{2})^2,$$\nk is a constant, and $ \\epsilon $ is a small offset. In this work, we use cosine and cosine square scheduling, corresponding to k = 1 and k = 2, respectively. Generation of the final state $ \\rho_T^{(i)} \\in \\{ \\rho_T \\} $ can be achieved by repetitively applying the channels, i.e., $ \\rho_T^{(i)} = \\Phi_T \\circ \\Phi_{T-1} \\circ ... \\circ \\Phi_2 \\circ \\Phi_1(\\rho_0^{(i)}) $."}, {"title": "B. Backward denoising process", "content": "The backward denoising process begins with an ensemble {$ \\{ \\rho_T \\} $} of $n_{train}$ copies of n-qubit maximally mixed states and gradually produces {$ \\{ \\rho_0 \\} $} that resembles the original state ensemble {$ \\{ \\rho_0 \\} $}. This process utilizes a VQA with T total chunks of sequentially trainable VQCs and projective measurements with $n_a$ ancillary qubits [25]. We considered two initialization methods for the ancillary qubits, one uses all zero states, ($ |0\\rangle^{n_a} $), and the other replaces a single qubit with a Haar random pure state, ($|Haar\\rangle \\otimes |0\\rangle^{(n_a-1)} $), to introduce randomness and therefore enhance the generative power. We sample the $n_{train}$ Haar random states for each diffusion step and maintain these ancillary states throughout the individual training process. After training, we freshly draw"}, {"title": "C. Cost function", "content": "The cost function of MSQuDDPM is based on the similarity between two quantum state ensembles. For individual states, this similarity can be quantified using fidelity [35]. Fidelity F($ \\rho, \\sigma $), measuring the similarity between two n-qubit density matrices $ \\rho $ and $ \\sigma $, is defined as\n$$\\mathcal{F}(\\rho, \\sigma) = [Tr(\\sqrt{\\sqrt{\\rho}\\sigma\\sqrt{\\rho}})]^2,$$\nwhere Tr denotes the trace operation. However, fidelity-based cost functions cannot be effective in MSQuDDPM because they may require full tomography of mixed states, which would require a large number of copies of the state, making it impractical in high-dimensional systems [36]. To address the limitations of fidelity, we use superfidelity, an upper-bound approximation of fidelity, due to its computational feasibility in mixed states, requiring fewer state copies and alleviating potential inefficiencies [37]. The superfidelity between two density matrices $ \\rho $ and $ \\sigma $ is computed by\n$$\\mathcal{G}(\\rho, \\sigma) = Tr(\\rho\\sigma) + \\sqrt{[1 - Tr(\\rho^2)][1 - Tr(\\sigma^2)]}.$$\nExtending this notion to quantum state ensembles, we define the mean superfidelity $ \\mathcal{G}(\\{ \\rho_1 \\}, \\{ \\rho_2 \\}) $ of two quantum state ensembles {$ \\{ \\rho_1 \\} $} and {$ \\{ \\rho_2 \\} $} as\n$$\\mathcal{G}(\\{ \\rho_1 \\}, \\{ \\rho_2 \\}) = E_{\\rho \\sim \\{ \\rho_1 \\}, \\sigma \\sim \\{ \\rho_2 \\}} [\\mathcal{G}(\\rho, \\sigma)],$$\nand leverage this metric to measure the discrepancy between two quantum state ensembles. Here, we assume that $ \\rho $ and $ \\sigma $ are randomly sampled from ensembles {$ \\{ \\rho_1 \\} $} and {$ \\{ \\rho_2 \\} $}, respectively.\nThe loss functions are extensions of Ref. [25] to mixed states, which can be maximum mean discrepancy (MMD) [38] and Wasserstein distance [39] with superfidelity as a kernel function and the distance measure, respectively.\nIn the learning cycle at t = m+1 in Fig. 1 (a), the model constructs two ensembles, one from the backward model outputs {$ \\{ \\rho_m \\} $} and the other from the target states {$ \\{ \\rho_m \\} $}. Then, the training algorithm minimizes the (squared) MMD distance between the two quantum state ensembles, defined as\n$$D_{MMD}(\\{ \\rho_m \\}, \\{ \\hat{\\rho}_m \\}) = \\mathcal{G}(\\{ \\rho_m \\}, \\{ \\rho_m \\}) + \\mathcal{G}(\\{ \\hat{\\rho}_m \\}, \\{ \\hat{\\rho}_m \\}) - 2\\mathcal{G}(\\{ \\rho_m \\}, \\{ \\hat{\\rho}_m \\}).$$\nThe Wasserstein distance is presented as an enhancement for generating complex quantum state ensembles where it is infeasible to distinguish between two state ensembles with MMD distance [25]. After reducing Kantorovich's formulation of the optimal transport problem into linear programming through discretizing and limiting the mass distributions of quantum state ensembles {$ \\{ \\rho_i \\}_{i=1}^{v} $} and {$ \\{ \\sigma_j \\}_{j=1}^{w} $} with sizes v and w, respectively, we provide the transport plan cost as v \u00d7 w pairwise superfidelity-based cost measure $C_{i,j} = 1 - \\mathcal{G}(\\rho_i, \\sigma_j)$ between two quantum states from each ensemble. The minimum total distance of the corresponding optimal transport problem is expressed as\n$$Wass(\\{ \\rho \\}, \\{ \\sigma \\}) = \\min_P \\langle P, C\\rangle,$$\ns.t.\n$$P1_w = r,$$\n$$P^T1_v = s,$$\n$$P \\ge 0,$$\nwhere $ \\langle , \\rangle $ denotes the inner product, P is the transport plan, $ 1_v $ and $ 1_w $ are all-ones vectors, and r and s are the probability vectors corresponding to {$ \\{ \\rho \\} $} and {$ \\{ \\sigma \\} $}, respectively."}, {"title": "III. APPLICATIONS", "content": "There are several tasks utilizing MSQuDDPM to generate the desired quantum state ensembles. In this section, we present these tasks, their results, and interpretations. Since the initial data for training and testing of the backward process are all maximally mixed states, we use the forward diffusion and backward testing samples for visualization. Details of the simulations are provided in Table I."}, {"title": "A. Clustered states", "content": "In the clustered state generation task, the initial state ensemble is the set of density matrices {$ \\{ \\rho_0 \\} $}, where each $ \\rho_0 $ is defined as $ \\rho_0 = (1 - q_0)|\\psi_0\\rangle\\langle\\psi_0| + q_0I/d $. Here, $q_0 \\in [0,0.01) $ is the depolarizing parameter, I/d represents the maximally mixed state, and $ |\\psi_0\\rangle = |0\\rangle + \\epsilon_0 c_0|1\\rangle $ up to normalization coefficient with $ \\epsilon_0 = 0.08 $, and Re($c_0$), Im($c_0$) ~N(0,1). We employ two ancillary"}, {"title": "B. Circular states", "content": "The initial state ensemble for the circular state case forms a depolarized circle in the X-Z plane of the Bloch sphere, defined by the single-qubit density matrices {$ \\{ \\rho_0 \\} $}, where each $ \\rho_0 $ is represented as $ \\rho_0 = (1 - q_0)|\\psi_0\\rangle\\langle\\psi_0| + q_0I/d $. The depolarizing parameter $q_0$ is sampled from the interval [0, 0.04), and each $ |\\psi_0\\rangle $ is initialized as $ |\\psi_0\\rangle = R_{Y\\theta_0}|0\\rangle $, where $ \\theta_0 \\in [0, 2\\pi) $ indicates a rotation around the Y-axis.\nIn the backward process, we utilize two ancillary qubits in the state |00) and employ the Wasserstein distance with superfidelity to reconstruct quantum states in a ring formation. As depicted in Fig. 2(a) and (b), the generated samples resemble the forward samples as the backward denoising proceeds. Furthermore, Fig. 2(c) and (d) show the decay of the average purity and the Wasserstein distance, maintaining similar trends between the depolarizing and reverse processes."}, {"title": "C. Many-body phase learning", "content": "To test our model under more complex conditions, we applied MSQuDDPM to the transverse-field Ising model (TFIM) in condensed matter physics [40]. The Hamiltonian of TFIM is given by: $H_{TFIM} = -(\\sum_i Z_iZ_{i+1} + g\\sum_i X_i)$, where $Z_i$ and $X_i$ denote the Pauli-Z and Pauli-X operations acting only on qubit i. As the parameter g increases from zero, a phase transition occurs in the system from an ordered phase (|g| < 1) to a disordered phase (|g| > 1), with the critical point at g = 1. We consider the 4-qubit case (n = 4) and sample initial states from the ground states of $H_{TFIM}$ in the disordered phase, which are paramagnetic along the X-axis, where g is uniformly sampled from the range [1.8, 2.2). To evaluate the model's performance, we calculate the X-axis magnetization, $M_x(\\rho) = Tr(\\rho \\sum_i X_i)/n $, of the generated samples to determine the phase of the density matrix. As shown in Fig. 3, the majority of the generated states can recover their original magnetization, in contrast to the magnetization of totally mixed states."}, {"title": "IV. STRATEGIES FOR IMPROVING MSQUDDPM", "content": "We propose several suggestions that we have gained from MSQuDDPM simulations for better model utilization. Our findings focus on three aspects of MSQuDDPM. First, we address the design of MSQuDDPM and compare the effects of splitting diffusion steps with increasing the number of ancillary qubits for generating high-quality samples and robust scaling. Moreover, we consider the noise scheduling in the forward process of MSQuDDPM for multi-qubit ensemble production. Finally, we investigate the advantage of using single-qubit"}, {"title": "A. Design principle: more diffusion steps or more ancillary qubits", "content": "Although the optimal number of diffusion steps may depend on the problem and hyperparameters, our results show that dividing the training process into multiple diffusion sequences is beneficial for efficiency and performance by reducing the actual qubits required for the backward VQC. The advantage of incorporating intermediate steps in MSQuDDPM is apparent in complex pattern-learning tasks. In Fig. 4, we compare the performance of MSQuDDPM with different diffusion steps and the number of ancillary qubits in the many-body phase learning task. We benchmark a model against our proposed architecture with n = 4 qubits, T = 6 diffusion steps, L = 12 layers in a single PQC, and $n_a$ = 2 ancillary qubits. The configuration of the benchmark model is n = 4, T = 2, L = 21, and $n_a$ = 6, which employs more auxiliary qubits but fewer diffusion steps than the proposed model. To ensure a fair comparison, we maintain a similar total number of parameters across all models and use the total parameter updates, calculated as the product of iterations and the number of trainable parameters per step. As shown in Fig. 4, the benchmark model that requires ten actual qubits presents training issues that preclude it from producing samples in the target distribution, resulting in a final MMD distance of 0.9325. In contrast, our proposed model with T = 6 stages uses six actual qubits, considering that we can reset the auxiliary qubits after measurement and reuse them in the subsequent circuits. Our model better captures the target distribution than the benchmark, achieving the terminal MMD distance of 0.0061. Although both models utilize a total of 12 ancillary qubits, the benchmark model would not be robust enough for scaling to larger qubit systems, as the necessity of numerous qubits in a single circuit would make the model vulnerable to training problems and limit its capacity, especially when increasing the circuit depth [41]. While the relation between the model's capacity and hyperparameters, such as the number of ancillary qubits and diffusion steps, requires further investigation, increasing the diffusion steps can allow comparable or better performance in applying MSQuDDPM to the complex quantum state pattern generation."}, {"title": "B. Noise scheduling in MSQuDDPM", "content": "In multi-qubit systems, the outputs of the MSQuDDPM forward process with linear or cosine schedules tend to quickly lose their original characteristics, such as purity or magnetization. Fig. 5(a) illustrates the decay of average purity, defined as $ \\sum_{\\rho_t \\in \\{ \\rho_t \\}} Tr(\\rho_t^2)/|\\{ \\rho_t \\}| $, during the forward depolarizing process under different forward scheduling methods. The purities of linear and cosine schedules rapidly converge to the maximally mixed states by t = 3. Fast forgetting of the original form exacerbates the MSQuDDPM's performance, as it makes not only the later training rounds hard to converge but also the earlier training steps ineffective because the target state ensemble gathers close to the maximally mixed states instead of spreading out for better optimization. Thus, maintaining the original state is crucial when handling multi-qubit en-"}, {"title": "C. Haar random state as an ancillary qubit", "content": "Although the initial ensemble for the backward PQC of MSQuDDPM, represented by the totally mixed state, can be viewed as having maximal noise, starting from identical states limits the model's ability to generate diverse samples in the early stages of the backward process. The number of possible outcomes at step m (where 1 < m < T) in the backward process, with $n_a$ auxiliary qubits initialized in $|0\\rangle^{n_a}$, is approximately $(2^{n_a})^{T-m+1} = 2^{n_a(T-m+1)}$, as measurements on the auxiliary qubits cause state branching. Consequently, the earlier rounds of backward learning (i.e., steps close to T) suffer from a capacity limit, as seen in the backward"}, {"title": "V. DISCUSSIONS", "content": "This work presents MSQuDDPM as a general and resource-efficient method for mixed quantum state ensemble generation. We demonstrate that this step-by-step denoising approach is beneficial to various quantum generative machine learning tasks. Through numerical experiments, we found that increasing the diffusion steps rather than the number of auxiliary qubits of MSQuDDPM results in better performance and robustness. We proposed cosine-exponent scheduling, especially cosine-square scheduling, which better preserves the original quantum states and is crucial for learning multi-qubit distributions. Moreover, Haar random states can serve as an additional ancilla option in VQC, as circuits with Haar ancillas are trainable, and the approach can further improve the model's capability.\nWe discuss several interesting future directions for MSQuDDPM. Further exploration of the architectures and hyperparameters in MSQuDDPM is an important direction for future research. Our numerical experiment on forward scheduling aligns with its classical counterpart, demonstrating that forward scheduling significantly influences the convergence of diffusion models [42]. Hence, exploring the impact of cosine-exponent scheduling and its relation to other factors or approaches in MSQuDDPM would be a promising avenue for research. Also, the cost function of the proposed MSQuDDPM structure requires the estimation of superfidelity between two mixed-state ensembles, which introduces overhead in quantum experiments. Alternative and more efficiently measurable cost functions remain to be developed.\nIn addition, the current architecture does not consider components such as positional encoding and the attention mechanism, which have been utilized in the DDPM [15] and latent diffusion model [43]. Combining these elements with backward VQC through their quantum counterparts could enable even more complex quantum state generation. Furthermore, while Haar random states have shown their potential as auxiliary qubits in MSQuDDPM training, further investigation into their theoretical foundations and applications in other quantum optimization"}, {"title": "Appendix A: Circuit layout for forward and backward process", "content": "In practice, we can construct the depolarizing channel of the forward process using a p-SWAP gate, as described in Fig. 7 (a). The p-SWAP gate performs a SWAP operation between two n-qubit inputs, $ \\rho_m^{(i)} $, and the fully mixed state I/d, with probability $q_{m+1}$, and applies the identity operator (I) with probability $1 - q_{m+1}$.\nFig. 7 (b) depicts the detailed circuit diagram of the backward process. At each timestep t, the parameterized quantum circuit $ \\hat{U}_t(\\theta_t, P_t, |\\xi_t\\rangle) $ incorporates $n_a$ ancillary qubits, $ |\\xi_t\\rangle $. The tensor product of $ |\\xi_t\\rangle $ and $ \\rho_t^{(i)} \\in \\{ \\rho_t \\} $ forms the complete input to a single PQC followed by applying L layers of parameterized universal unitaries with hardware-efficient quantum ansatz [25, 44] and entangling qubits. After the gates, Z-axis projective measurements are performed on the auxiliary qubits and the remaining density matrices form the output ensemble {$ \\{ \\rho_{t-1} \\} $}, which is used as the input ensemble for the next timestep, ignoring the measurement outcomes."}, {"title": "Appendix B: Implementation details", "content": "We implemented the MSQuDDPM using Pytorch, a widely-used machine learning library in Python [45]. For the quantum circuit implementation and automatic gradient calculation, we utilized the TensorCircuit library [46], and the Wasserstein loss calculation was based on the POT package [47]. We visualized the outcomes in the Bloch sphere with QuTip [48].\nFor optimization, we employed Adam optimizer [49] with exponential learning rate decays since some proposals suggest that tuning the learning rate for Adam may be helpful for performance [50, 51]. In the parameter initialization of the backward PQC, we considered normal and Xavier initialization methods [52], which include n input qubits and $n_a$ ancillary qubits. For normal initialization, the parameters at training step t are drawn from the standard Gaussian distribution, $ \\theta_t \\sim N(0, 1) $. In Xavier initialization, weights are assigned as $ \\theta_t \\sim N(0, \\frac{2}{n_{in}+n_{out}}) $, using the total number of input and output qubits $n_{in}$ == n + $n_a$ in $N(0, \\frac{2}{n_{in}+n_{out}})$. Note that even with Xavier initialization, we applied normal initialization to the ancillary qubits to facilitate random results in their projective measurements."}]}