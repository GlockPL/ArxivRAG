{"title": "The Performance of the LSTM-based Code Generated by Large Language Models (LLMs) in Forecasting Time Series Data", "authors": ["Saroj Gopalia", "Sima Siami-Namini", "Faranak Abri", "Akbar Siami Namina"], "abstract": "Generative AI, and in particular Large Language Models (LLMs), have gained substantial momentum due to their wide applications in various disciplines. While the use of these game changing technologies in generating textual information has already been demonstrated in several application domains, their abilities in generating complex models and executable codes need to be explored. As an intriguing case is the goodness of the machine and deep learning models generated by these LLMs in conducting automated scientific data analysis, where a data analyst may not have enough expertise in manually coding and optimizing complex deep learning models and codes and thus may opt to leverage LLMs to generate the required models. This paper investigates and compares the performance of the mainstream LLMs, such as ChatGPT, PaLM, LLama, and Falcon, in generating deep learning models for analyzing time series data, an important and popular data type with its prevalent applications in many application domains including financial and stock market. This research conducts a set of controlled experiments where the prompts for generating deep learning-based models are controlled with respect to sensitivity levels of four criteria including 1) Clarify and Specificity, 2) Objective and Intent, 3) Contextual Information, and 4) Format and Style. While the results are relatively mix, we observe some distinct patterns. We notice that using LLMs, we are able to generate deep learning-based models with executable codes for each dataset seperatly whose performance are comparable with the manually crafted and optimized LSTM models for predicting the whole time series dataset. We also noticed that ChatGPT outperforms the other LLMs in generating more accurate models. Furthermore, we observed that the goodness of the generated models vary with respect to the \"temperature\" parameter used in configuring LLMS. The results can be beneficial for data analysts and practitioners who would like to leverage generative Als to produce good prediction models with acceptable goodness.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) such as ChatGPT [15], LLaMa [22], Falcon [16], and PaLM [4] are gaining popularity on a regular basis and for a variety of reasons. These generative models are already playing an integral role in assisting people with their day-to-day duties, such as generating code, writing emails, assisting with projects, and many more. As a result, a wider range of users is involved in dealing with LLMs. According to a report published by Markets and Markets Research Pvt. Ltd. [13], the global market for generative AI is expected to achieve at a Compound Annual Growth Rate (CAGR) of 35.6% between 2023 and 2028, indicating significant potential opportunities. The CAGR was valued at $11.3 billion in 2023 and it is expected to rise to almost $51.8 billion by 2028. Looking ahead, industry forecasts estimate that the value of generative AI market might reach $191.8 billion by 2032.\nRecent research suggests that generative AI-based applications could contribute an annual value ranging from $2.6 trillion to $4.4 trillion across various use cases, surpassing the 2021 GDP of the United Kingdom at $3.1 trillion [5]. Such integration could enhance the overall influence of artificial intelligence by up to 40% with the possibility of further doubling this estimate by incorporating generative AI into existing software applications used for tasks beyond the initially analyzed use cases.\nAs per Salesforce's findings [20], 61% of employees already employ or intend to utilize generative AI for accomplishing their tasks. Additionally, 68% of employees believe that generative AI can enhance their ability to serve customers better. Moreover, 67% of employees feel that it can amplify the benefits derived from other technological investments. These insights highlight the growing adoption of LLMs in various professional settings.\nIn this research work, we have studied the performance of the following large language models:\n\u2022 GPT-3.5-Turbo 1 is an efficient model that are designed for natural languages and code comprehension.\n\u2022 Falcon [16], is one of the most effective and optimized language models based on high quality, large scale training data."}, {"title": "2. Related Work", "content": "In November 2022, OpenAI released ChatGPT [15]. In February 2023, Meta released LLaMa [22] followed by Technology Innovation Institute (TII) introducing \"Falcon LLM\" [16], a foundational Large Language Model (LLM) with 40 billion parameters that was introduced in March 2023. In May 2023, Google joined the race and announced PaLM [4]. Moreover, Meta continued to release models, offering a set of models in July 2023 under the name Llama 2 [23], with parameter counts ranging from 7 billion to 70 billion. Since then major high tech companies continue improving their LLMs by adding additional features and capabilities.\nThe idea of leveraging generative Als in building executable codes and models has been discussed in several research papers. Vaithilingam et al. [24] conducted a study with 24 volunteers to evaluate the usability of GitHub Copilot, a code generation tool that employs sophisticated language models. Participants in the research completed Python programming tasks using both Copilot and a controled condition that used VSCode's default IntelliSense functionality. The research sought to ascertain the influence of these tools on programming experience, error detection, problem-solving tactics, and barriers to their adoption. According to quantitative research by the authors, there was no significant difference in job completion times between Copilot and IntelliSense controlled groups. However, it was discovered that Copilot customers had more failures, which were mostly related to Copilot's incorrect advice. Despite this, the majority of participants (19 out of 24) preferred Copilot because of its ability to give a useful and informative starting point that eliminate the needs for frequent Web searches. However, several participants had difficulty comprehending and debugging the code generated by Copilot.\nDestefanis et al. [7] studied and compared the performance of two AI models: GPT-3.5 and Bard, in generating code for Java functions. The Java functions and their descriptions were sourced from CodingBat Website, a platform for practicing programming problems. The evaluation of the Java code generated by the models was based on correctness, which was further verified using CodingBat's test cases. The results of the evaluation showed that GPT-3.5 outperformed Bard in code generation, producing accurate code for around 90.6% of the functions, while Bard achieved correctness for only 53.1% of the functions. Both AI models displayed strengths and weaknesses. GPT-3.5 consistently performed better across most problem categories, except for functional programming, where both models showed similar performance.\nLiu et al. [12] proposed EvalPlus, a framwork for rigorously evaluating the functional correctness of code generated by large language models (LLMs). The framework solved the issue of insufficient test coverage in current coding benchmarks like as HUMANEVAL, which employ only a few manually written test cases and consequently miss numerous problems in LLM-generated code. EvalPlus is built around an automated test input generator that combine LLM and mutation-based methods. It begins by using ChatGPT to generate high-quality seed inputs focusing at edge situations. The seeds are then changed using type-aware operators to produce a large number of new test cases quickly. The findings showed that inadequate benchmark testing could have a significant impact on claimed performance. EvalPlus also found flaws in 11% of the original HUMANEVAL solutions. Through automated testing, the study points in the direction of thoroughly analyzing and refining programming benchmarks for LLM-based code creation.\nNi et al. [14] proposed LEVER, a method for improving language-to-code generation by Code Language Models (LLMs) utilizing trained verifiers, as proposed in their work. They trained different verifier models based on plain language input, program code, and execution outcomes to determine the validity of created programs. LEVER was tested on four language-to-code datasets: Spider, WikiTable-Questions, GSM8k, and MBPP, in the fields of semantic parsing, table quality assurance, arithmetic reasoning, and basic Python programming. LEVER enhanced execution accuracy over strong baselines by 4.6 \u2013 10.9% when paired with Codex and achieved new state-of-the-art outcomes on all datasets. The relevance of execution results in verification became clear through ablation study, and the technique kept its strong performance even in circumstances with limited resources and without supervision. The findings showed that using benchmark datasets to train compact verifiers increased the performance of various LLMs in the field of language-to-code generation.\nDenny et al. [6] proposed \"Prompt Problems\u201d, a unique educational idea aimed to educate students on how to create effective natural language prompts for large language models (LLMs) with the objective of generating executable codes. The authors created Promptly, a web-based application that allows students to iteratively tweak prompts based on test case output until the LLM produces accurate code. They used Promptly in classroom research with 54 beginning Python students and discovered that the tool teaches students to new programming structures and promotes computational thinking, despite the fact that some students were hesitant to utilize LLMs. The research looked at prompt duration and iteration counts, as well as student opinions based on open-ended feedback. Overall, the work presents preliminary evidence that quick Problems warrant more investigation as an approach to developing the growing ability of quick engineering.\nBecker et al. [2] investigated the revolutionary impact of Al-driven code generation tools like OpenAI Codex, DeepMind AlphaCode, and Amazon CodeWhisperer. These tools possess the remarkable ability to translate natural language prompts into functional code, heralding a potential revolution in the realm of programming education. While admitting their potential, the authors argue for urgent talks within the computer science education community in order to overcome difficulties and properly utilize these technologies. The study provided an overview of important code generation models-Codex, AlphaCode, and CodeWhisperer-that were trained on massive public code repositories. These models excel at creating code in several programming languages and go beyond coding by providing features such as code explanations and language translation. From examples, answers, and different problem-solving methodologies to scalable learning materials and an emphasis on higher-level topics, code-generating tools provide potential in education. The authors underline the need for educators proactively integrate these technologies, anticipating ethical concerns and a trend toward code analysis.\nZamfrescu-Pereira et al. [26] conducted a study whose findings shed some light on the difficulties that non-AI specialists have when attempting to provide effective prompts for large language models like GPT-3. These individuals frequently use a more impromptu and ad hoc approach rather than a systematic one, which is hampered by a tendency to overgeneralize from limited experiences and is based on human-human communication conventions. The authors developed BotDesigner, a no-code chatbot design tool for iterative fast development and assessment. This tool helps with a variety of tasks, including dialogue formulation, error detection, and fast alteration testing. Participants in a user research adjusted prompts and assessed modifications well, but with limited systematic testing and issues in prompt efficacy understanding. These difficulties originate from a tendency to overgeneralize and predict human-like behaviors. Through patterns and cause analysis, the study proposed potential for further training and tool development to encourage systematic testing, moderate expectations, and give assistance, while noting persistent uncertainty regarding generalizability and social bias consequences. This experiment highlights the difficulties that non-experts have in rapid engineering and suggests to opportunities for more accessible language model tools.\nZhou el al. [27] introduce a novel approach called the Automatic Prompt Engineer (APE) designed to facilitate the automatic generation and selection of effective natural language prompts. The primary goal is to guide large language models (LLMs) towards desired behaviors. APE tackles this challenge by framing prompt generation as a natural language program synthesis problem. It treats LLMs as black box computers capable of proposing and evaluating prompt candidates. The APE method leverages LLMs in three distinct roles: 1) as inference models for suggesting prompt candidates, 2) as scoring models to assess these candidates, and 3) as execution models to test the selected prompts. Prompt candidates are generated either directly through inference or recursively by creating variations of highly-rated prompts. The final selection of the most suitable prompt is determined by maximizing metrics such as execution accuracy on a separate validation set. Importantly, APE achieves these outcomes without the need for gradient access or fine-tuning, relying instead on a direct search within the discrete prompt space. In the experimental phase, APE was put to the test across a range of tasks. It successfully addressed 24 instruction induction tasks, exhibiting performance on par with or surpassing human capabilities across all of them. Additionally, APE demonstrated its effectiveness on a subset of 21 BIG-Bench tasks, outperforming human prompts in 17 out of 21 cases."}, {"title": "3. Large Language Models Studied", "content": "This paper compares the performance of four LLMs including GPT, Falcon, LLama-2, and PaLM."}, {"title": "3.1. GPT-3.5-Turbo", "content": "GPT-3.5-Turbo is an OpenAI-developed variant of the Generative Pre-trained Transformer 3. GPT-3.5 models include a wide variety of capabilities including natural language and code comprehension and creation. GPT-3.5-Turbo is the standout model in this series known for its exceptional capabilities and low cost of ownership. The GPT-3.5 model, designed for chat interactions, boasts exceptional capabilities while being remarkably cost-effective, priced at only one-tenth of the cost of the text-davinci-003 model."}, {"title": "3.2. Falcon", "content": "The Technology Innovation Institute located in Abu Dhabi created the Falcon LLM [16], a significant advancement in AI language processing that has revolutionized its potential. Within the Falcon series, namely Falcon-40B and Falcon-7B, distinct versions, each possessing specific merits, contribute to making Falcon LLM an inventive and adaptable solution suitable for diverse uses.\nFalcon's creation involved tailored tools and a distinctive data flow approach. This system extracts valuable Web information for customized training, differing from methods by NVIDIA, Microsoft, and HuggingFace. Focus on large-scale data quality was critical, recognizing LLMs' sensitivity to data excellence. Thus, an adept pipeline is built for rapid processing and quality content from Web sources. Falcon's architecture was meticulously optimized for efficiency. Coupled with high-caliber data, this enables Falcon to notably surpass GPT-3, utilizing fewer resources.\nFalcon is a decoder-only model with 40 billion parameters trained with 1 trillion tokens. The training took two months and made use of 384 GPUs on AWS. After rigorous filtration and de-duplication of data from CommonCrawl, the model's pretraining dataset was generated using web crawls with roughly five trillion tokens. Falcon's capabilities were also expanded by incorporating certain sources such as academic papers and social media debates. The model's performance was then evaluated using open-source benchmarks such as EAI Harness, HELM, and BigBench."}, {"title": "3.3. LLama-2", "content": "Meta AI created lama 2 [23], a new family of pretrained and fine-tuned large language models (LLMs). Llama 2 has characteristics ranging from 7 billion to 70 billion arameters. The pre-trained models are designed for a wide range of natural language activities, whilst the fine-tuned versions known as Llama 2-Chat are designed for discourse. Llama 2 was pretrained on 2 trillion publically accessible tokens utilizing an improved transformer architecture with advantages like as extended context and grouped-query attention. On knowledge, reasoning, and code benchmarks, Llama 2 surpassed other open-source pretrained models such as Llama 1, Falcon, and MPT. Llama 2-Chat aligns the models to be helpful and safe in discourse by using supervised fine-tuning and reinforcement learning with human feedback (RLHF). Over 1 million fresh human preference annotations were collected in order to train and fine-tune reward models. To increase multi-turn discourse consistency, techniques such as Ghost Attention were created. Ghost Attention (GAtt) is a straightforward technique influenced by Context Distillation [1]. GAtt manipulates the fine-tuning stage to guide attention concentration through a step-by-step approach."}, {"title": "3.4. PaLM", "content": "The Pathways Language Model (PaLM) [4] is a Transformer based language model built by Google with 540 billion parameters. PaLM implements a traditional Transformer decoder structure with modifications such as SwiGLU activation and parallel layers for faster training. A total of 780 billion tokens of training data from natural language sources such as books, Web material, Wikipedia, GitHub code, and conversations were employed. The Pathways system, which allows for efficient distributed training on accelerators, was used to train on 6144 TPU v4 processors. This allows the training of such a big model without the need for pipeline parallelism.\nThe PaLM is evaluated over a wide range of tasks and datasets, proving its strong performance across several domains. The PaLM 540B achieved an outstanding score of 92.6 in the SuperGLUE test after fine tuning, essentially putting it with top models such as the T5-11B. In the field of question answering, PaLM 540B outperformed previous models by earning F1 scores of 81.4 on the Natural Questions and TriviaQA datasets in a few-shot setting. The model's abilities extended to mathematical thinking, where it achieved an astounding 58% accuracy on the difficult GSM8K math word problem dataset using chain-of-thought cues. PaLM-Coder 540B has been elevated even further, reaching 88.4% success with a pass@100 criterion on HumanEval and 80.8% success with a pass@80 criterion on MBPP.\nPaLM 540B excels at translation, earning a notable BLEU score of 38.4 in zero shot translation on the WMT English-French dataset, outperforming other significant language models. The model's responsible behavior is obvious in toxicity evaluations, with a RealToxicity dataset average toxicity probability of 0.46. Finally, using the WinoGrande coreference dataset, PaLM 540B achieved an accuracy of 85.1%, demonstrating its capacity to mitigate gender bias. These extensive findings highlight the PaLM model's adaptability and efficacy across a wide range of language-related tasks."}, {"title": "4. Research Questions", "content": "Recent advances in large language models like GPT-3 (Brown et al.[3]) have demonstrated impressive text generation capabilities when provided with well-designed prompt instructions. However, best practices for prompt engineering are still developing. As Reynolds and McDonell [19] discuss, prompt programming, the importance of being aware of prompts fitting within the concept of natural language.\nThis study will perform a systematic sensitivity analysis to identify the most sensitive prompt components for text generation using large language models. Following the workflow outlined by Saltelli et al. [21], each input factor will be varied individually while holding others constant to isolate its impacts. Text outputs will be analyzed to measure sensitivity.\nFindings will provide prompt engineers with guidance on precision tuning. In line with recommendations by P\u00e9rez et al. [17], this research aims to demystify prompts through empirical testing, allowing LM prompts and a few shots with hyperparameters."}, {"title": "5. Experimental Setup", "content": "The experiment was conducted in two parts. First, the LLM models were run in Google Colab Pro using a GPU with high RAM. Second, the outputs from the LLM models were run on a Macbook Pro with an M1 Max chip with 64 GB Memory."}, {"title": "5.1. Dataset", "content": "The daily financial time series data from January 01, 2022, through April 23, 2022, were collected from Yahoo Finance2. The dataset, described in Table 1, includes a diverse selection of stocks and indices. Each dataset is a stock or index with a number of data points, date range, sector and country of origin. Stocks were chosen based on market capitalization and sector representation. As this table 1 shows, the selected datasets represent a variety of sectors (indices, technology, e-commerce and automakers), and countries (USA, Japan, Hong Kong, China). This table 1 provides a basis to test the LLM-generated models in datasets in terms of variety and geographical diversity.\nIn aaddition, major indices like the S&P 500 (GSPC) and Dow Jones Industrial Average (DJI), Nasdaq Composite (IXIC), Nikkei 225 (N225) and Hang Seng Index (HSI) were included. Giant hightech companies such as Apple (AAPL) and Microsoft (MSFT) were also included in the dataset. furthermore, E-commerce giants such as Amazon (AMZN) and Alibaba (BABA) were incorporated in the dataset to make the dataset more diverse. Lastly, Automakers like Tesla (TSLA) were also added with the goal of investigating the performance of each model generated by LLMs for each industry sector."}, {"title": "5.2. LLMs Configuration", "content": "The granularity and diversity of responses generated by LLMs can be controlled via several configuration parameters:\n1. Temperature control randomization of the responses generated by a large language model where temperature score close to 1 indicates increases in the randomness;"}, {"title": "5.3. Prompt Engineering with Sensitivity Levels", "content": "The purpose of this research paper is to investigate 1) whether the deep learning-based models generated by LLMs for forecasting time series data are good enough (i.e., comparable with fine-tuned models manually produced by expert data analysts; and 2) whether it is feasible to enhance the goodness of models by controlling the criteria for effective prompt engineering including 1) clarity and specificity, 2) objectives and intents, 3) contextual information, and 4) format and stylealong with sensitive levels of low, medium, and high.\nTo perform such controlled experiment, the study consider the following criteria in crafting prompts along with additional controlled imposed by sensitivity levels considered.\nI) Clarity and Specificity (CS) of prompts provided to LLMs.\n1. Low. Use of vague terms, lack of specific details, and general language that does not clearly convey the desired tasks.\n2. Medium. Check for clear tasks with some specific details, but might have some rooms for further clarification.\n3. High. Identify prompts with well-defined, precise tasks that leave little ambiguity.\nII) Objectives and Intents (OI) of prompts provided to LLMs.\n1. Low. The prompt's exact objectives and intents are unclear due to the use of ambiguous language or a lack of clear purpose.\n2. Medium. The prompt states a general objective but does not provide a clear context for the desirable task.\n3. High. The prompts are crafted with clear statements of objective and intent, providing a well-defined context.\nIII) Contextual Information (CI) of prompts provided to LLMs.\n1. Low. The prompts are provided with minimal contextual information about the data, the problem domain, or the use case, making it hard to understand the request.\n2. Medium. Identify prompts that offer some context but might lack crucial information about the scenario.\n3. High. The prompts provide detailed context, including metadata, problem domain, and use case information.\nIV) Format and Style (FS) of prompts provided to LLMs.\n1. Low. The prompts are written with unclear or inconsistent format explaining the desired code or model.\n2. Medium. Check for prompts that are generally structured but might have some issues with clarity, style, or terminology.\n3. High. The prompts are crafted with well-structured language, proper format for desirable code, and appropriate use of terminology."}, {"title": "5.4. Performance Metric", "content": "Throughout the experiment, the performance metric utilized is the Root-Mean-Square Error (RMSE). This metric serves to assess the output generated by all LLMs in response to the provided input prompts. The RMSE essentially computes the square root of the mean of the squared differences between the model's predictions and the true values. A lower RMSE value indicates that the underlying models had achieved better performances. More specifically, RMSE values indicate that the model's predictions are closer to the actual ground target values on average."}, {"title": "6. Experimental Procedure", "content": "To assess the performance of LLMs in generating good deep learning-based models for analyzing time series data, we crafted a set of prompts according to the criteria and sensitivity levels discussed earlier. The prompts are designed and provided to mainstream LLMs as inputs for various levels of sensitivity. We will then provide these prompts to each LLM, take the response of each LLM, compile and execute the code generated by LLMs and capture RMSE for comparison purposes."}, {"title": "6.1. Sensitivity Analysis for Designing Prompts", "content": "We crafted eleven prompts ranging from easy to complex sensitivity. The designing of these eleven prompts was based on pair-wise sensitivity analysis where a factor is changing, and remaining factors are kept constant. Pair-wise analysis, also known as pairwise comparison, is a method for comparing and evaluating many items or criteria by comparing each item to every other item in a methodical and systematic manner. The phrase \"pair-wise analysis\" refers to the process of analyzing and comparing the distinct criterion levels (i.e., Low, Medium, and High) against each other for each individual element in the context of the information and determining their impact on the results.\nThe pair-wise analysis helps in evaluating the quality, significance, or applicability of multiple characteristics by directly comparing them to one another, allowing for a more systematic and thorough review process.\nTo help trace the sensitivity levels, a coloring scheme is employed where the green, orange, and red colors in Table 3 represent sensitivity level of high, medium, and low, respectively."}, {"title": "6.2. Manual Creation and Optimization a Model", "content": "The experiments execute on Apple M1 MAX, Memory of 64 with GPU. The dataset split into 80% for training and 20% testing for testing. In the preprocessing, the data is scaled using MinMaxscaler, which transforms the feature range from 0 to 1 where the data linearly scales down. After the scaling, the data is prepared into sequences of length 5 to predict the next day's (1) data and feed into the model for training. The manual creation of the model consists of the LSTM architecture with tensorflow in the backend. The model contains One LSTM layer with 50 Units with 'relu' activation function. The model trains with 100 epochs with a batch size of 1. The model employs 'adam' as an optimizer and 'mse' as loss function. The hyperparameters were chosen with various observations during the experiments. The preprocessing steps and building model are only relevant for the manual creation, as LLMs are provided the raw data for code generation."}, {"title": "7. Results", "content": "Table 4 reports the results the performance of the deep learning-based models generated by LLMs for time series data analysis. Each model is evaluated using RMSE values for each stock data. The PaLM model achieved the lowest RMSE value of 0.0023 for BABA ticker while the Falcon achieved the lowest RMSE value of 0.0041 for the GSPC ticker. The LLama2 did not achieve the lowest RMSE across all tickers, whereas the GPT 3.5 has the lowest RMSE for eight tickers. However, the manually developed and optimized model achieved the lowest RMSE compared to LLM generated model across all tickers.\nIn Table 4, the best RMSE values obtained for each language model is highlighted with gray color. Moreover, the best RMSE values among all models for each stock data and for all 11 different prompts are highlighted in dark color. The cells with NA indicate that the models generated by the underlying LLM were meaningless and the underlying LLMs produced some other types of models such as regression models instead of deep learning-based models for forecasting time series data. In other words, the NA values represent the output of the LLMs with no code related to the LSTM model or code not related to predicting time series. This might be due to hallucination problem known in language models where the underlying LLM confuses leveraging its trained data to properly responding to queries and prompts. A noticeable case is the falcon case where the number of NA (the irrelevant response to prompts) is outnumbering the expected responses. This may indicate that the falcon large language model is suffering from the hallucination problem more than the other LLMs.\nI) The Performance of Generated Models Across LLMs. As Table 4 indicates, on average (i.e., the last rows of each stock data) there is no clear winner among the language models for the eleven prompts studied. The deep learning-based models generated by each LLM are rather comparably competitive. However, as Table 4 shows, the models generated by GPT 3.5 on prompt 9 and 10 outperform the other generated models (i.e., the dar cells in the table) except for the GPSC and BABA stock data.\nII) The Performance of Generated Models Across Prompts.\nWe observe that for the case of GPT 3.5, the best models with minimal RMSE values are produced by prompts 8, 9, and 10 where three criteria as 1) clarify and specificity, 2) objective and intent, and 3) Format and Style are set high.\nFor the case of LLama 2, we observe that the language model generates the best model using prompt 8 where where three criteria as 1) clarify and specificity, 2) objective and intent, and 3) Format and Style are set high (in most cases). We also observe a similar pattern for models generated by PaLM through prompts 7, 8, and 9. For the models generated by falcon, there is no clear pattern whether any prompt standout in the comparison where the results are mixed.\nWhile the results and performance of models and prompts are dispersed, we observe a clear pattern where prompts 8 and 9 seem to produce the best results in generating more accurate models for forecasting time series models where three criteria as 1) clarify and specificity, 2) objective and intent, and 3) Format and Style are set high.\nIII) The Performance of Generated Models Across the Time Series Datasets. As Table 4 and the black cells indicate, the best results across different dataset is produced by GPT 3.5 mostly by prompts 8, 9, and 10. This may indicate that, at least for GPT 3.5, the more clear and specific (CS), and the more objectively crafted prompt with clear intention (OI), and a clear expression regarding the desired output and format (FS) in the prompts will yield creating better and more accurate models for forecasting time series data.\nIV) The Performance of Generated Models and Manually Developed and Optimized Model. The most important observation is the accuracy of models created and optimized manually in comparison with the models generated by prompts.\nIt is important to note that the manually created and optimized model was created based on all data and thus there is only one single manually created and optimized model to compare the results with. More specifically, we did not manually craft and optimize separate deep learning-based models for each dataset. We created a single optimized model for all dataset all together. Figure 1 depicts the RMSE values of the manually crafted and optimized single model obtained for each dataset. In Figure 1, the RMSE values of the manually implemented LSTM model on HSI achieved the highest RMSE of value 0.0354 and N225 achieved the lowest RMSE value of 0.0034.\nWhile the manually crafted and optimized model outperforms on three sets of stock data, the models generated by LLMs are also outperforming the manually crafted and optimized models for the seven sets of stock data. More specifically, we observe that the manually created and optimized model outperforms models generated by LLMs for DJI, N225, and AMZN; whereas, the models created through prompts outperform manually created and optimized model for GSPC, IXIC, HSI, AAPL, MSFT, BABA, and TSLA.\nIt is important to note that the results are compared based on the best results obtained by the prompts and the variances of RMSE values among different prompts and LLMs are still playing an important indicator in making the final judgment. However, given that prompts 8, 9, and 10 outperform the other prompts in most cases, one case conclude that to generate a comparably good model it is better to set Clarity and Specificity (CS), Objective and Intent (OI), and Format and Style (FS) high and use GPT 3.5 language model to generate the deep learning-based models that can be comparable with manually crafted and optimized model for forecasting time series data."}, {"title": "7.1. Fixed/Consistent Configurations of LLMs", "content": "The results reported in Table 4 are based on the configuration and settings of LLMs parameters listed in Table 2 where each model wes fine-tuned empirically to obtain the best results. To investigate whether various configuration and parameter settings for LLMs have any effect on the results, we replicate the study with fixed and consistent parameter settings for all LLMs.\nThe result in Table 5 are obtained using the same set of parameters but with consistent and fix values as follows: 1) temperature= 0.1, 2) max token_size= 1,024 and 3) top_p= 0.6 in all models including GPT 3.5 Turbo, Falcon, Llama-2 and PaLM. This setting primarily means reducing randomness in generating responses to queries or prompts.\nAs Table 5 indicates that the reduction in randomness through minimizing the value of the temperature value has some impacts on the performance of each prompt. The table demonstrates that the GPT 3.5 model achieved the lowest RMESE for nine tickers whereas Palm achieved the lowest RMSE value of 0.0357 for the MSFT ticker.\nI) The Performance of Generated Models Across LLMs. A detailed view on both Tables 4 and 5 indicates that lower values of temperature makes the accuracy of models slightly better. In particular, we observe that GPT still outperforms other LLMs.\nII) The Performance of Generated Models Across Prompts.\nWe observe that the the best models generated by GPT are the ones generated by simpler prompts such as Prompt 2, 3, and 4 where the criteria (i.e., Clarity and Specificity, Objective and Intent, Contextual Information, and Format and Style) are all kept consistent at the level of either low or medium or high.\nIII) The Performance of Generated Models Across the Time Series Datasets. A similar pattern is observed. A mixed results, but consistent with the results observed in Table 4.\nIV) The Performance of Generated Models and Manually Developed and Optimized Model. As shown in both Tables 4 and 5, we observe a slightly better models for the case where the temperature parameter is kept low.\nTables 4 and 5 clearly demonstrated that the Falcon model generates more valid and correct models when the temperature parameter is configured at 0.7 (high) compared to 0.1 (low). The results show the number of invalid models labeled with \"NA\" is lower than the number of invalid models generated by higher temperature 0.7 which leads to more exploration in the model's predictions. By increasing the temperature, the model is encouraged to introduce more randomness into its predictions, reducing the likelihood of exhibiting the hallucinated phenomenon.\nIn contrast, for GPT 3.5 Turbo model the number of invalid models (i.e., \"NA\") is lower with temperature parameter set to low (i.e., 0.1) instead of high (i.e., 0.7). The simpler prompts with lower temperature yield better results because the model produce more coherent and relevant responses. In case of complex prompts with higher temperature the GPT 3.5 model explores a wider range of possibilities and generate more diverse responses because of higher randomness. Complex prompts may contain unclear information, making it difficult for the model to provide appropriate outputs with high confidence. In such circumstances, greater temperatures allow the model to experiment with different variations of the prompt, resulting in responses that represent the input's nuances."}, {"title": "7.2. Model Architecture of Generated Models", "content": "Given the variation in performance of the models generated by LLMs, it is of important to investigate the cause of such differences. One of the key factors in deep learning-based models including LSTM, which plays an important role in the performance, is the architecture (e.g., number of layers and nodes) of the generated models. To compare the architecture of the models generated by LLMs and the architecture of our manually created and optimized LSTM model, this section reports the architecture metadata of all models.\nTable 6 reports configuration of the models generated by LLMs using the prompts listed in Table 3. The configurations are set differently for each LSTM model with a number of hyperparameters to analyze. The configuration consists of 1) the number of the number of LSTM layers, 2) number of units"}]}