{"title": "Words2Contact: Identifying Support Contacts from Verbal Instructions Using Foundation Models", "authors": ["Dionis Totsila", "Quentin Rouxel", "Jean-Baptiste Mouret", "Serena Ivaldi"], "abstract": "This paper presents Words2Contact, a language-guided multi-contact placement pipeline leveraging large language models and vision language models. Our method is a key component for language-assisted teleoperation and human-robot cooperation, where human operators can instruct the robots where to place their support contacts before whole-body reaching or manipulation using natural language. Words2Contact transforms the verbal instructions of a human operator into contact placement predictions; it also deals with iterative corrections, until the human is satisfied with the contact location identified in the robot's field of view. We benchmark state-of-the-art LLMs and VLMs for size and performance in contact prediction. We demonstrate the effectiveness of the iterative correction process, showing that users, even naive, quickly learn how to instruct the system to obtain accurate locations. Finally, we validate Words2Contact in real-world experiments with the Talos humanoid robot, instructed by human operators to place support contacts on different locations and surfaces to avoid falling when reaching for distant objects.", "sections": [{"title": "I. INTRODUCTION", "content": "Humanoid robots can use various body parts to create support contacts to help balance when reaching for difficult positions. For example, they can use their right hand as a support on a table, bend forward and reach a cup that would otherwise be out of reach (Fig. 1); or they can lean on the counter with their left hand to reach for a dish in the bottom rack of a dishwasher to prevent falling. Solving these tasks autonomously is usually done with multi-contact whole-body planners and controllers [1, 2].\nRecent advances in whole-body control using quadratic programming have shown that both torque-controlled robots [3] and position-controlled robots with force/torque sensors [4] can effectively utilize additional contact points to increase their manipulability and improve their balance, but these control methods require the prior knowledge of the contact locations. This information is usually the output of a contact planning algorithm, where typically a planner decides a sequence of contact locations that enable the robot to solve its task (e.g., walking, manipulating a complex object) [5]. Contacts computation usually relies on visual or 3D perception and environment models to look first for suitable contact surfaces, before deciding whether they are kinematically feasible for the robot. For example, it is common to look for flat areas to place the footsteps in humanoid walking [6].\nUnfortunately, selecting contacts, especially when applying forces, often requires an understanding of the world that can hardly be modeled. Some surfaces might be flat, but too fragile for support, like a glass window. Other surfaces might be off-limits for safety reasons, like the wing of an aircraft, or they might be slippery, dirty, or unstable. Overall, in many real-world situations, the choice of support contacts is likely to require human expertise at some point to be deployed outside of a laboratory. Giving the power to human experts to guide the robot and choose the contact locations for them is therefore a very desirable feature.\nHuman guidance in contact selection is ideal for teleoperated robots in remote maintenance or hazardous scenarios and for collaborative robots cooperating and working side-by-side with humans. For example, a remote operator could instruct the robot to place one end-effector on a wall to lift one foot, and a factory worker could instruct the robot to reach a handle with one end-effector and take a fallen tool with the other one. In these situations, language-based instructions provide a natural communication channel and free the hands of the operator, nor do constrain the human worker to use computer interfaces to instruct the robot on what to do.\nGiving instructions in natural language has long been a dream of the robotics community [7, 8, 9]. For years, this goal eluded researchers due to two main challenges: (1) understanding what a sentence means requires a good intuition of the context and the implicit knowledge, that"}, {"title": null, "content": "is, some \"common sense\" (2); there are countless ways of expressing the same instruction, which prevents the use of simple keywords. To give an illustration, some people might refer to the \"Handbook of Robotics\" of Fig. 1 as \"the big red book\", \"the book\", \"the book next to your right hand\", \"the red thing\", \"the big thing in front of you\", and so on.\nLarge Language Models (LLMs) [10] might be on the verge of solving these challenges for robotics [11], providing a way to give natural and general instructions to robots. Trained on billions of human-written texts, LLMs exhibit a form of \"common sense\" that allows them to interpret instructions with their most likely meaning. They are also, perhaps surprisingly, highly versatile, as they are capable of handling instructions or situations not anticipated by the robot designers. Additionally, LLMs naturally process the many ways humans express similar concepts, as they are represented by similar \"embeddings\". Visual Language Models (VLMs) are equally appealing, as they can link text to images and vice-versa.\nIn this paper, we harness the power of Foundation Models (LLMs and VLMs) to instruct a humanoid robot about desired contact locations for increased support in whole-body reaching, an essential skill for solving several downstream tasks.\nThe robotics community has been working intensely on integrating LLMs with robots since the first demonstrations of ChatGPT (2022). The key challenge is connecting perception, which is continuous, structured, and high-dimensional, to language, which is linear and loosely structured, and then to actions, which are also continuous and depend on the specific robot. While many approaches have been proposed (see Sec. II-B), there is currently no consensus on how to establish this link in the general case.\nWe present the following key contributions:\n\u2022 Words2Contact: a novel pipeline integrating LLMs and VLMs with a multi-contact whole-body controller to identify support contacts from verbal instructions.\n\u2022 A benchmark of state-of-the-art LLMs and VLMs for contact prediction.\n\u2022 A pilot study showing that users quickly learn to use our system to identify accurate contact locations.\n\u2022 Validation of our system on a real Humanoid Robot.\nTo the best of our knowledge, this paper is the first to address support contact identification from verbal instructions using Foundation Models and demonstrate it with a humanoid robot."}, {"title": "II. RELATED WORK", "content": ""}, {"title": "A. Multi-contact whole-body Control", "content": "Whole-body controllers are typically formalized as a QP problem with the minimization of a weighted sum of quadratic functions each time-step t [12], under several constraints:\n$q^{*}(t) = \\underset{q(t)}{\\operatorname{argmin}} \\sum w_{i}||c_{i}(t)||^{2}$ s.t. (dynamics/kinematics joint, torque, velocity limits\nwhere each $c_i(t)$ is a cost function i at time-step t (often called a task), for instance the Euclidean distance between the"}, {"title": null, "content": "left hand and its desired position, the distance of the center of mass to the desired position, the head's orientation etc.\nDepending on the robot's control formulation, the optimization variables q can be the joints' torque, velocity, or angular position. When multiple contacts are involved, torque control is ideal to directly set the interaction forces and wrenches [3]. Nevertheless, torque control requires both a torque-controlled robot, and an accurate dynamic model. Impedance control can be an alternative to distribute the forces among contacts, but it requires modeling the actuators and lot of parameter tuning [13].\nIn our recent work, we introduced a unified formulation for whole-body multi-contact control [4], for position-controlled robots to distribute and control the contact forces. The core idea is to utilize the flexibility that stems both the mechanical bending of the robot and the non-perfect joint position controllers. By modeling this flexibility, the joint position commands and the contact forces can be connected, allowing the controller to regulate these forces by incorporating them in the quadratic programming formulation. In this paper, we use this multi-contact whole-body controller to perform experiments with the Talos humanoid robot [14]."}, {"title": "B. Robotics and Language Models", "content": "Prior to the introduction of LLMs, numerous approaches in both language comprehension and generation were explored in robotics [8, 15]. However, these early methods were limited due to their reliance on rigid, rule-based systems and predefined vocabularies [9].\nThanks to their training on a very large dataset, LLMs can answer to a very large set of natural language queries without having been trained on any specific domain.\nIn particular they can provide high-level plans with some \"common sense\" by inferring many pieces of context, thus bypassing most of the \"frame problem\" [16]. In robotics, by using well-designed \"prompts\" that explain the problem to be solved in natural language and the kind of expected output, LLMs were used to find a sequence of pre-learned behaviors [17, 18], generate Python code to be executed by the robot [19, 20], or cost functions for a model-based controllers [21]. For example, \"Inner Monologue\" [22] uses the ability of LLMs to generate task plans and explores embodied reasoning through self-dialogue. \"Code-as-Policies\u201d [20] uses the code generation abilities of LLMs to inform robotic policies directly from natural language descriptions without the need for further training.\nIn some cases where the desired structure of the output is in a form that LLMs are not inherently able to generate, or if the nature of the problem requires more complex responses, additional fine-tuning may be useful [23, 24, 25]. For example, in \"BTGenBot\", behavior trees are generated through LLMS that have been fine-tuned on specialized datasets. The key takeaway is that a well-structured output is beneficial to transition from non-structured high-level instructions to low-level control commands. The drawback of fine-tuning is that it requires large amounts of data, which is time-consuming"}, {"title": null, "content": "and resource-intensive to collect, and may introduce biases based on how the data are collected or generated [26].\nEven though the generated plans are often successful, the ability to use language-based corrections to fix the generated plans generated with minor adjustments during task execution can be very useful. For example, Sharma et al. [27] present a model that integrates natural language and visual feedback to adjust robot planning costs in real-time, enabling more dynamic and responsive adaptation to new tasks. LILAC [28] proposes a shared autonomy paradigm that updates the control space in response to continuous user corrections. DROC [29] further advances this paradigm by enabling LLM-based robot policies to respond to, remember, and retrieve feedback efficiently, significantly improving adaptability to natural language instructions. Overall, a correction mechanism that understands general and abstract corrections, such as \"a bit to the right\", is essential to ensure the reliability and effectiveness of robotic systems guided by LLMs.\nInstead of relying on LLMs solely trained on language, an alternative idea is to use the same learning architectures as LLMs (transformers), but train them on multi-modal robotics data instead of pure text, like in the Robotics Transformers (RT) line of work [30]. A more popular and less compute-intensive approach is to use similar large-scale robotics datasets and incorporate pre-trained language and vision models with a few trained layers to connect the components. OpenVLA [31] uses this approach with small open-source models (7-billion parameters), highlighting the potential for substantial achievements with smaller models.\nRegarding humanoid robots, recent research has focused on generating human-like motions from text descriptions, specifically through animation using simulated human-like articulated models [32, 33]. However, for the problem of multi-contact planning, we are only aware of a traditional (pre-LLMs) language processing-based approach, where an n-gram language model is employed. The goal of this model is to learn motion as a sequence of transitions, where each word represents a shape pose, and each sentence represents a motion [9]."}, {"title": "III. METHODS", "content": "The Words2Contact pipeline (Fig. 2) unfolds as follows: visual feedback is streamed to the user, who starts by instructing the robot to place a contact in a specified location. The initial prediction resulting from this instruction is displayed to the user. If dissatisfied, the user can either correct the prediction or provide a different instruction until they confirm satisfaction with the updated predicted target. Once the contact location is confirmed, the robot proceeds to execute the contact placement at the specified point using the SEIKO controller.\nTo achieve this, we split the contact prediction task into three sub-modules, each responsible for a specific sub-task: Prediction, Correction, and Confirmation. This split is crucial for ensuring that even small models will be able to effectively handle each stage of the pipeline."}, {"title": "A. Prompting LLMs", "content": "We use a single LLM and dynamically adjust the system prompt (Fig. 3) at each step of the pipeline. Furthermore, outputs from the LLM are constrained to JSON format to ensure a desired structure that simplifies information extraction, in the open source models we enforce this constraint with grammar-based token sampling and acceptance [34], whereas for the proprietary model we follow the documentation instructions\u00b9.\nFor readers unfamiliar with LLMs, we want to stress the difference between the system prompt and the user prompt. The system prompt is an instruction or message given to the LLM to guide its responses. This prompt sets the tone, context, and boundaries for the conversation, helping the model understand its role and what is expected of it. Considering the constraints of the task, each module has its own system prompt for the LLM. Examples of system prompts are shown in Fig. 3. The user prompt, instead, is the input or query provided by the user to the LLM in natural language."}, {"title": "B. Module Selector", "content": "The Module Selector (Fig. 2) interprets the user's natural language prompt and classifies it into one of three categories: Prediction, Correction, or Confirmation.\nThis classification is achieved by combining two key techniques: few-shot prompting [35] and logits bias\u00b2. Few-shot prompting involves providing a system prompt that describes the task that the LLM has to perform, accompanied by examples to guide it in classifying new inputs correctly. For this, and all the following modules, we use 5-shot prompting, i.e., we provide five examples. Logits bias is a technique used to adjust the output probabilities of logits, specifically for terms such as 'Prediction', 'Correction', and 'Confirmation'. This adjustment aims to prioritize the correct classification of inputs into one of these three categories."}, {"title": "C. Prediction Module", "content": "To interpret the desired location implied by the user, we need to have a system that leverages both natural language instructions and visual state feedback. The Prediction module (Fig. 4) combines both Vision Language (VLMs) and a Large Language Model (LLM). We assume that there are two cases of positions that the user might refer to:\n1) Absolute Positions: For prompts specifying a contact that is on an object (e.g., \"place your hand on the book\").\n2) Relative Positions: For prompts where the contact is expressed in terms of its spatial relation to the object(s) (e.g., \"left from the box\", \"between the cup and the bowl\").\nThe Prompt Analyzer is responsible for (a) identifying which of the two scenarios the prompt is relevant to and (b)"}, {"title": null, "content": "isolating the object's descriptions mentioned in the prompt so that they can be passed to the VLMs. For complex tasks that involve common sense and math reasoning, chain-of-thought prompting, where the LLM is asked to provide its thought process before reaching a conclusion, has proven to be beneficial [36]. We combine few-shot prompting and chain-of-thought reasoning, to achieve better results (see the prompt on Fig. 3-a).\nIn the case of Absolute Positions (Case 1 in Fig. 4), we use the capability of language-grounded segmentation models to segment images based on natural language descriptions. This allows the system to detect the object regardless of how they are referenced by the user, overcoming the limitations of classic pre-trained segmentation models that either retrieve a mask based on a pre-trained set of labels or return a segmentation of an image without any labeling. CLIPSeg [37], for example, addresses this problem by extending a CLIP model [38] with a transformer-based decoder. Once we obtain the segmentation heatmap for the requested object, we determine the coordinates of the contact point in image space using the following metric: $[i_{max},\u0130_{max}] = \\operatorname{argmax}_{i,j} H_{ij}$, where H is the heatmap produced by the language-grounded segmentation model, up-scaled to the size of the original image. While a more sophisticated point sampling technique could be chosen to ensure sufficient space coverage, such considerations are beyond the scope of this work.\nIn the case of Relative Positions (Case 2 in Fig. 4), we utilize spatial relationships derived from the visual scene and the verbal instruction to determine the contact location. To achieve this, following the same intuition as in the first case, we extract the bounding box(es) using pre-trained open-set object detection [39]. Similarly to grounded segmentation, these models are trained using bounding box annotations and aim at detecting arbitrary classes with the help of language generalization. The representation of a bounding box using natural language is straightforward and thus motivates our approach. For instance, in case 2 of Fig. 4, after we receive a bounding box for the cup, we build the following prompt: \"Cup is at [100,150] with width=120 and height=90. Place your hand left from the cup.\" The system prompt (Fig. 3-b) contains some basic information about the representation we are following. Additionally, we provide a few examples to ensure that the LLM will accurately interpret the spatial instruction accurately, and will calculate the final contact position successfully. Furthermore, instead of directly outputting a numerical value, the LLM outputs a mathematical expression which is then parsed and calculated using a Python parser. This choice was made because, in preliminary experiments, we noticed a performance increase of around 10% when using this approach instead of having the LLM perform the computation on its own."}, {"title": "D. Correction Module", "content": "When dealing with humanoid robots and contacts, precision is of high importance. The Correction module (Fig. 5) enhances the Words2Contact pipeline by allowing for both minor and major corrections. Similarly to the second scenario of the Prediction Module, we detect the object(s)"}, {"title": "E. Control Module", "content": "Once the user confirms that they are satisfied with the current contact point shown to them in image space, we query the LLM one final time, with the end-effector selector prompt (Fig. 3-c), to select the robot's end-effector that will be used for contact (e.g., right or left hand) and the task type (e.g., support contact or reaching). Then, we extract the 3D position, in camera frame $p_{cam} =[x,y,z]_{cam}$, from the point cloud. The Control Module then relies on the SEIKO Retargeting [40, 41] and Controller [4] to figure out the commanded motion and smoothly establish the new contact.\nThe SEIKO Controller takes as input the discrete selection of the end-effector (EE), as well as the 3D position of the selected contact point [i,j] in camera frame $p_{cam} = [x,y,z]_{cam}$. It transforms it into a position in the robot's world frame: $p_{rob} = r_{orb} + R_{cam} p_{cam}$. A spline-based Cartesian trajectory, starting from the current position $X_{EE}^t$ at time t, brings the end-effector EE to the desired contact point $p_{rob}$. SEIKO Retargeting uses a model-based Sequential Quadratic Programming (SQP) optimization to compute feasible whole-body configurations (joint positions and contact forces) that track the Cartesian effector pose commands. The contact forces are automatically determined by the optimization problem of SEIKO, so only the contact locations are necessary, to enforce safety and balance constraints\nSEIKO Controller integrates an explicit model of joint flexibility and employs an SQP whole-body admittance formulation to regulate contact forces on our position-controlled humanoid robot. This controller is crucial for performing the distant reaching tasks that challenge the system's balance, as it enhances robustness against model errors."}, {"title": "IV. EXPERIMENTS & RESULTS", "content": ""}, {"title": "A. Evaluation of contact prediction using pre-trained models", "content": "In this experiment, we benchmark several state-of-the-art pre-trained models (VLMs and LLMs): the goal is to select the best combination for our pipeline, evaluating the impact of"}, {"title": "B. Pilot Study - Evaluation of the correction mechanism and usability of the pipeline", "content": "To evaluate the performance and usability of our system, we conducted a pilot study with 11 volunteer participants (9 male, 2 female, aged 26.27\u00b11.8 y.o., min 24, max 30). All"}, {"title": "C. Real Robot experiment", "content": "We evaluated Words2Contact with the Talos humanoid robot [14] in four distinct whole-body reaching settings (Fig. 9), with and without corrections, with the following user prompts p:"}, {"title": "V. CONCLUSIONS & FUTURE WORK", "content": "We introduce Words2Contact, a pipeline for language-guided multi-contact placement for humanoid robots that leverages VLMs and LLMs to interpret verbal commands: this is a powerful tool for teleoperated and collaborative robots. Words2Contact is effective even with small open-source models without fine-tuning. In a pilot study, we show that humans quickly learn to use the system; real robot experiments show that the system provides satisfactory contacts even in difficult environments, thanks to the iterative correction mechanism.\nOngoing work is focused on evaluating the impact of the prediction errors on downstream tasks. Future work will improve the contact prediction using insights from Visual Question Answering (VQA) to better handle abstract spatial concepts (\"a bit, a little more\") and link corrections to physical quantities. Additionally, we plan to inform the Contact Prediction with the robot's dynamics model:"}]}