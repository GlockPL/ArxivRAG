{"title": "Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation", "authors": ["Chung Park", "Taesan Kim", "Hyungjun Yoon", "Junui Hong", "Yelim Yu", "Mincheol Cho", "Minsung Choi", "Jaegul Choo"], "abstract": "Cross-Domain Sequential Recommendation (CDSR) improves recommendation performance by utilizing information from multiple domains, which contrasts with Single-Domain Sequential Recommendation (SDSR) that relies on a historical interaction within a specific domain. However, CDSR may underperform compared to the SDSR approach in certain domains due to negative transfer, which occurs when there is a lack of relation between domains or different levels of data sparsity. To address the issue of negative transfer, our proposed CDSR model estimates the degree of negative transfer of each domain and adaptively assigns it as a weight factor to the prediction loss, to control gradient flows through domains with significant negative transfer. To this end, our model compares the performance of a model trained on multiple domains (CDSR) with a model trained solely on the specific domain (SDSR) to evaluate the negative transfer of each domain using our asymmetric cooperative network. In addition, to facilitate the transfer of valuable cues between the SDSR and CDSR tasks, we developed an auxiliary loss that maximizes the mutual information between the representation pairs from both tasks on a per-domain basis. This cooperative learning between SDSR and CDSR tasks is similar to the collaborative dynamics between pacers and runners in a marathon. Our model outperformed numerous previous works in extensive experiments on two real-world industrial datasets across ten service domains. We also have deployed our model in the recommendation system of our personal assistant app service, resulting in 21.4% increase in click-through rate compared to existing models, which is valuable to real-world business.", "sections": [{"title": "1 INTRODUCTION", "content": "In most real-world recommender systems, various business domains require understanding of the different interests and needs of users. A practical approach to address this involves Single Domain Sequential Recommendation (SDSR), which focuses on recommending the next item within a specific domain using only the single-domain sequence and builds many domain-specific models [9, 25, 32, 36]. Another solution is Cross-Domain Sequential Recommendation (CDSR): it predicts the next item a user will interact with by leveraging their historical interaction sequences across multiple domains [3, 11, 20]. In particular, CDSR has proven to be a promising approach to improve the performance of sequential recommendations across multiple domains simultaneously with one model [11, 13, 16, 20]. Note that the difference between CDSR and SDSR tasks for each domain lies in whether or not they use information from other domains in addition to the specific domain. However, in certain domains, CDSR significantly underperforms relative to the SDSR approach, while in other domains it may outperform the SDSR approach. This is because transferring knowledge from other weakly related domains or domains with different levels of data sparsity can have a negative impact on a particular domain: this is known as negative transfer [20]. To verify this experimentally, we measured the difference in the model performance between the CDSR and the SDSR approaches on a per-domain basis. As illustrated in Fig. 1, the recommendation performance of the Book and Clothing domains in the Amazon dataset, as well as the Call domain"}, {"title": "2 RELATED WORK", "content": "2.1 Single-Domain Sequential Recommendation\nThe SDSR framework is designed to model the temporal dynamics in user-item interactions, effectively capturing how user preferences evolve over time within a specific domain. For example, GRU4Rec [8], STAMP [15], and NARM [12] were introduced to predict the user's next item using GRU-based models. Moreover, in SASRec [9], BERT4Rec [25], SINE [27], and LightSANs [4], attention mechanisms are utilized to encapsulate sequential patterns, thereby addressing both short- and long-term dependencies. In addition, other machine learning architectures such as a convolutional network (e.g., NextItNet [31]) or Markov Chain (e.g., TransRec [6]) are adopted in the SDSR task. Meanwhile, FDSA [32], S\u00b3Rec [36], and CARCA [22] utilize item features (e.g., text description) in addition to item sequences to demonstrate their efficiency. Nonetheless, studies on the SDSR framework continue to encounter issues of negative transfer in CDSR task.\n2.2 Cross-Domain Sequential Recommendation\nThe goal of Cross-Domain Recommendation (CDR) is to improve recommendations in a given domain by leveraging information from various other domains. For example, CMF [24] and CLFM [5] modeled the matrix factorization of user-item interactions across multiple domains, capturing their shared patterns. Other approaches, DTCDR [37], DeepAPF [30], and BiTGCF [14], modeled user-item relationships in paired domains by a multi-task learning method. However, these methods modeled pairwise relationships between domains for the CDR task, a strategy that may be less effective in scenarios involving a large number of domains. To fill this research gap, CAT-ART [11] simultaneously modeled user and item representations in five domains using a matrix factorization approach. However, these studies have not taken into account the sequential nature of user interaction sequence. Meanwhile, the CDSR aims to enhance the task of sequential recommendation, focusing on items that span multiple domains. \u03c0-Net [17] introduced gating mechanisms designed to transfer information from a single domain to another paired domain. Meanwhile, C2DSR [3] employed a self-attention based encoder and graph neural network to model both single- and cross-domain representations. MIFN [16] introduced the concept of mixed information flow, which reflects the knowledge flows between multiple domains. MAN [13] designed group-prototype attention mechanisms to capture domain-specific and cross-domain relationships. However, these studies only modeled relationships between pairs of domains. In scenarios involving more than three domains, this approach would require handling an impractically large number of domain pairs, potentially limiting its feasibility in real-world applications [11]. To address this limitation, CGRec [20] proposed a CDSR model that deals with more than three domains at once. They approximated the negative transfer for each domain using Shapley values, and applied them to penalize items in domains with high negative transfer. However, even in CGRec, we encountered problems with negative transfer,"}, {"title": "3 PRELIMINARY", "content": "We consider an expanding CDSR task characterized by interaction sequences spanning more than three domains. These domains may encompass diverse service platforms such as e-commerce or video platforms [11, 38]. Formally, the set of domains is denoted as $D = \\{A, B, C, ...\\}$, where $|D| \\geq 3$. We also consider the notation $d \\in D$ as a specific domain $d$. The notation $V_d$ is introduced to represent a set of items specific to the domain $d$. Then, $V$ indicates the total item set across all domains.\nDefinition 1. Single- and Cross-Domain Sequential Recommendation: The single-domain sequences of domain $d$ are represented by $X_d = [\\text{(SOS)}, x_1^d, x_2^d, ..., x_{|X_d|-1}^d]$, where each element $x_t^d$ signifies an interaction occurring at time $t$ and $\\text{(SOS)}$ is a start token of the sequence. We then define $X = (X_A, X_B, X_C, ...)$ as the cross-domain sequence where the $|D|$ single-domain sequences are merged and rearranged in chronological order. Conversely, the cross-domain sequence $X$ can be split into $|D|$ of $X_d$ (Fig. 2a). For example, consider a scenario where a user engages with three domains, labeled \\{A, B, C\\}. The cross-domain sequence for these domains is $X = [\\text{(SOS)}, x_1^A, x_2^A, x_3^A, x_1^B, x_2^B, x_4^C, x_8^C, x_9^C]$, and is then split into $X_A = [\\text{(SOS)}, x_1^A, x_2^A, x_3^A]$, $X_B = [\\text{(SOS)}, x_1^B, x_2^B]$, and $X_C = [\\text{(SOS)}, x_4^C, x_8^C, x_9^C]$ from the split process.\nNote that the SDSR solely focuses on recommending the next item within a specific domain $d$ using the single-domain sequence"}, {"title": "4 MODEL", "content": "We illustrate the overall architecture of our model in Fig. 2."}, {"title": "4.1 Shared Embedding Layer", "content": "During the embedding look-up phase, we acquire initialized representations of items for $|D|$ single-domain sequences (i.e., $X_d$) and one cross-domain sequence (i.e., $X$). For each domain $d$, we define an item embedding matrix $M^d$ represented by $\\mathbb{R}^{|V_d| \\times r}$, where $|V_d|$ indicates the total number of items in domain $d$, and $r$ is the embedding dimension. We then concatenate the embedding matrices of all $|D|$ domains to form an aggregated embedding matrix $M \\in \\mathbb{R}^{|V| \\times r}$, where $|V|$ is the total number of item sets across all domains.\nThe single- and cross-domain sequences (i.e., $X_d$ and $X$) are subsequently converted into a fixed length, $T$. If the sequence length is shorter than $T$, special tokens (PAD) are added to the left side, serving as placeholders for past interactions on $X_d$ and $X$. Conversely, if the sequence length exceeds $T$, only the $T$ most recent actions are preserved. As shown in Fig. 2b, the final embedding matrix $M$ is shared by the single- and cross-domain sequence by a look-up operation to extract the initialized sequences representation $E^d \\in \\mathbb{R}^{T \\times r}$ and $E \\in \\mathbb{R}^{T \\times r}$, respectively (Fig. 2(c-1) and 2(c-2)). The aggregation $|D|$ of $E^d$ in chronological order can be described as $E_{\\text{single}}$ (Fig. 2(c-1)). Additionally, a learnable position embedding matrix $P \\in \\mathbb{R}^{T \\times r}$ is integrated into $E^d$ and $E$ to encapsulate the sequential nature of user interaction ($E^d \\leftarrow E^d + P$, $E \\leftarrow E + P$) [28]. The embedding representations of $E^d$ and $E$ at the $t$-th step are denoted as $e^d_t$ and $e_t$, respectively."}, {"title": "4.2 Asymmetric Cooperative Network with Mixture-of-Sequential Experts (ACMOE)", "content": "Referring to previous studies [29, 33], the degree of the negative transfer of a specific domain can be defined as the difference between the loss of the SDSR and \u2461 the loss of CDSR tasks for the domain (i.e., \u2460 - \u2461). This value is called the negative transfer gap (NTG), and helps determine whether samples from other domains are beneficial or detrimental to the specific domain. The degree of negative transfer in a specific domain increases as the NTG decreases, and therefore, we assign this value as the weight for the prediction loss in the domain, resulting in a smaller gradient flow. To efficiently calculate this, we designed an asymmetric cooperative network that concurrently executes SDSR and CDSR tasks for each domain in a multi-task learning manner. For our multi-task learning, we utilized the multi-gate Mixture of Sequential Experts (MoE) architecture [21]. This architecture explicitly models the relationships between different tasks and learns task-specific functionalities, enabling it to effectively leverage shared representations. During this phase, we employed a decoupled mechanism allowing expert networks to be specialized in either SDSR or CDSR tasks, ensuring both tasks are implemented without mutual interference. For this, in the decoupled mechanism, a stop-gradient operation is adopted on some experts for the SDSR tasks and on other experts for the CDSR task. This approach enables the losses for both tasks to be computed independently, leading to a more precise evaluation of the NTG. We used Transformer [28] as the expert to handle sequential data.\n4.2.1 Architecture. Mathematically, given the initialized representations of single- and cross-domain sequences $E^d$ and $E$ from the"}, {"title": "4.2.2 Transformer Experts", "content": "We adopted the self-attention module introduced in Transformer [28] as the sequential expert layer $\\text{f}_{\\text{TRM}}$ (Fig. 2(c-3)), which consists of two distinct sub-layers as follows: Multi-head Self-Attention (MSA) At each MSA head, the inputs $Z \\in \\mathbb{R}^{T \\times r}$ undergo a linear transformation resulting in three hidden representations, i.e., queries $Q_i \\in \\mathbb{R}^{T \\times r/p}$, keys $K_i \\in \\mathbb{R}^{T \\times r/p}$, and values $V_i \\in \\mathbb{R}^{T \\times r/p}$, where $i$ indicates a specific head, $p$ is the number of heads, and $r$ is the dimension of the input. Using these three hidden representations, the scaled dot-product attention (Attn) is then computed as follows:\n$\\text{Attn}(Q_i, K_i, V_i) = \\text{softmax}(\\frac{Q_iK_i^\\top}{\\sqrt{r/p}})V_i,$ (5)\n$Q_i = ZW_i^Q, K_i = ZW_i^K, V_i = ZW_i^V,$\nwhere $W_i^Q, W_i^K, W_i^V \\in \\mathbb{R}^{r \\times r/p}$ are the trainable parameters. In the MSA, the above Attn operation is implemented $p$ times in parallel, and then involves concatenating the outputs of each head and linearly projecting them to extract the final output $H \\in \\mathbb{R}^{T \\times r}$:\n$H = \\text{MSA}(Z) = [\\text{Attn}(Q_1, K_1, V_1)||...||\\text{Attn}(Q_p, K_p, V_p)]W^F$ (6)\nwhere $||$ is the concatenate operation and $W^F \\in \\mathbb{R}^{r \\times r}$ is the trainable parameters. For the sequential recommendation, only information from prior time steps can be utilized. Therefore, a masking operation is applied to the output of the MSA layer, effectively eliminating all connections between $Q_i$ and $K_j$ whenever $j > i$. Point-wise Feed-Forward Network (FFN) The FFN is then adopted into $H$ to introduce nonlinearity and enable interactions among various latent subspaces using a fully-connected layer (FC), as follows:\n$\\text{FFN}(H) = [\\text{FC}(H_1)||\\text{FC}(H_2)||, ..., ||\\text{FC}(H_T)],$\n$\\text{FC}(H_t) = \\text{GELU}(H_tW_1 + b_1)W_2 + b_2,$\n(7)\nwhere $H_t$ is the $t$-th representation of $H$, $W_1 \\in \\mathbb{R}^{r \\times r}$, $b_1 \\in \\mathbb{R}^{r \\times 1}$, $W_2 \\in \\mathbb{R}^{r \\times r}$, and $b_2 \\in \\mathbb{R}^{r \\times 1}$ are learnable parameters, and GELU is the gelu activation [7].\nTherefore, the transformer-based sequential expert $\\text{f}_{\\text{TRM}}$ in Eq 2 and 4 is described as follows: $\\text{f}_{\\text{TRM}}(\\cdot) = \\text{FFN}^*(\\text{MSA}^*(\\cdot))$."}, {"title": "4.3 Loss Correction with Negative Transfer Gap (LC-NTG)", "content": "From our ACMOE layer, we can explicitly measure the negative transfer gap (NTG) of each domain as described in Section 3, and use the NTG to adaptively assign lower weights to the loss of items in domains that exhibit significant negative transfer during the training stage (Fig. 2(e-3)).\n4.3.1 Single-Domain Item Prediction. As presented in Fig. 2(e-1), given the single-domain sequence $X_d$, and its expected next item $x_{t+1}^d$, we use the pairwise ranking loss [23] to optimize our model as follows:\n$\\ell^d_t = \\text{logo}( \\sigma( P(x_{t+1}^d = x_{t+1}^{d+} | X_{1:t} ) - P(x_{t+1}^d = x_{t+1}^{d-} | X_{1:t} ) ), \\quad L_{\\text{single}} = \\sum_{t=1}^T \\ell^d_t,$\n(8)"}, {"title": "4.3.2 Cross-Domain Item Prediction", "content": "Similar to the single-domain item prediction, the CDSR task using the cross-domain sequence $X_{1:t}$ is performed with the following objective (Fig. 2(e-2)):\n$\\ell_t = \\text{logo}( \\sigma( P(x_{t+1}^d = x_{t+1}^{d+} | X_{1:t} ) - P(x_{t+1}^d = x_{t+1}^{d-} | X_{1:t} ) ), \\quad L_{\\text{cross}} = \\sum_{t=1}^T \\ell_t,$\n(9)"}, {"title": "4.3.3 Calculating the Negative Transfer Gap", "content": "We can describe the negative transfer gap (NTG) using the losses of Eq. 8 and Eq. 9 as follows:\n$\\phi_\\pi(d) = \\sum_{t=1}^T (\\ell_t^d - \\ell_t),$\n(10)"}, {"title": "4.3.4 Loss Correction", "content": "The relative NTG serves as a weight for the cross-domain item prediction loss computed in Eq. 9. This loss is re-aggregated by multiplying the relative NTG for each domain separately as follows (Fig. 2(e-3)):\n$L_{\\text{cross}} = \\sum_{t=1}^T \\sum_{d=1}^{|D|} \\lambda_d \\text{logo} \\sigma (P(x_{t+1}^d = x_{t+1}^{d+} | X_{1:t} ) - P(x_{t+1}^d = x_{t+1}^{d-} | X_{1:t} )$ (12)\nThis adaptive regulation of the gradient diminishes its flow in domains where there is significant negative transfer, thereby alleviating negative transfer effects across all domains."}, {"title": "4.4 Single-Cross Mutual Information Maximization (SC-MIM)", "content": "To improve the transfer of valuable information between SDSR and CDSR tasks, we developed the Single-Cross Mutual Information Maximization (SC-MIM), an auxiliary loss that maximizes the mutual information between the representation pairs from the SDSR and CDSR tasks on a per-domain basis. The SC-MIM extracts the correlation signals between the intrinsic characteristics of the both tasks for a specific domain. This module is an application of the"}, {"title": "4.5 Model Training and Evaluation", "content": "The total training loss function of our model is as follows:\n$L = \\eta \\sum_{d=1}^{|D|} (L_{\\text{single}} + L_{\\text{cross}}) + (1-\\eta) L_{\\text{SC-MIM}}$ (18)\nwhere $\\eta$ is the harmonic factor. In the evaluation stage, we only used cross-domain representations to make predictions. For example, given the latest representations $(y_1^t, y_2^t, ..., y_{|D|}^t)_{\\text{cross}}$, the next recommended item is selected based on the highest prediction score in domain $d$:\n$\\underset{x_d \\in V_d}{\\text{argmax}} ((y_t^d)^{\\text{cross}}) \\cdot M(x_d)),$ (19)"}, {"title": "5 EXPERIMENTS", "content": "The experiments are designed to answer the following research questions:\n(RQ1): Does the performance of our model surpass the current state-of-the-art baselines in practical applications that involve more than three domains?\n(RQ2): Can our model effectively address the challenge of negative transfer across all domains in the CDSR task?\n(RQ3): What is the impact of various components of our model on its performance in CDSR tasks?\n(RQ4): How do variations in hyper-parameter settings influence the performance of our model?\n(RQ5): How does the model perform when deployed online?"}, {"title": "5.1 Datasets", "content": "5.1.1 Amazon Dataset. Amazon review datasets [18] was used in our experiments, encompassing five domains: Books, Clothing Shoes and Jewelry, Video and Games, Toys and Games, and Sports and Outdoors (Table 1). These domains are abbreviated as Books, Clothing, Video, Toys, and Sports, respectively.\n5.1.2 Industrial Dataset (Telco). We collected user logs from diverse real-world applications operated by a leading global telecommunications company. The dataset comprised customers who consented to the collection and analysis of their data. The data contain five domains: Application Usage (APP-Use), Call Record (Call-Use), Navigation Service (Navi), e-commerce service (e-comm), and Discount Coupon Usage (Coupon-Use). We identified 99,936 users, and there are 48,887 non-overlapping users across five domains."}, {"title": "5.2 Experimental Settings", "content": "5.2.1 Baseline Models. The performance of our model was evaluated with four categories of baseline models: (1) General Recommendation (GR), (2) Single-Domain Sequential Recommendation (SR), (3) Cross-Domain Recommendation (CDR), and (4) Cross-Domain Sequential Recommendation (CDSR), as shown in Tables 2 and 3. The baselines in the GR include BPRMF [23] and GCMC [2]. BPRMF is a classical approach that develops a pairwise loss function for modeling the relative preferences of users. GCMC proposes a graph autoencoder framework to address the matrix completion. Refer to Section 2 for a description of SR, CDR, and CDSR baselines. The RecBole framework [34, 35] was used to implement all CDR baselines and some SR models. CARCA [22], S\u00b3Rec [36], C2DSR [3],"}, {"title": "5.2.2 Evaluation Settings and Metrics", "content": "In line with prior studies [9, 22, 36], we employed the leave-one-out approach for evaluating recommendation performance. Specifically, each user interaction sequence was divided into three parts: the last item designated as test data, the penultimate item reserved for validation, and all preceding items used as training data. Performance for each domain was assessed independently, depending on the domain of the last item in the test data, resulting in domain-specific performance metrics derived from different user groups.\nGiven the vast number of items, using all items as test candidates was impractical due to time constraints. Consequently, a widely adopted method was implemented, where the ground truth item (positive sample) is paired with 99 randomly chosen items (negative samples) that the user had not engaged with before. We then reported the performance of Top-k recommendations, which is based on a ranked list of 100 items. The assessment focused on several key metrics: Hit Ratio (HR), Normalized Discounted Cumulative Gain (NDCG), and Mean Reciprocal Rank (MRR). Note that the performance of GR, SR, CDR, and CDSR baselines including ours was measured using cross-domain sequences on the CDSR scenario.\n5.2.3 Implementation Details. The size of the embedding (r), training batch, and maximum sequence length (T) were all set to 128. The number of experts in ACMOE is set to four for the Amazon"}, {"title": "5.3 Performance Evaluation (RQ1)", "content": "We evaluated the performance of predicting the next item among all baselines and our model. The results for the two datasets are reported in Table 2 (Amazon) and Table 3 (Telco). Based on these results, we can draw the following conclusions.\n(1) The effectiveness of our model can be observed. Our model outperforms most baseline models on two real-world datasets. Our model improved upon the best baseline model (second best for each domain) by +3.13% (Book), +1.18% (Clothing), +7.29% (Video), +9.65% (Toys), and +5.28% (Sports) for HR@5 on the Amazon dataset, and +11.96% (Call-Use), +4.65% (Navi), +6.47% (Coupon-Use), and +38.81% (e-comm) on the Telco dataset. In the App-Use domain, our model did not achieve the top performance, but the difference from the best-performing baseline (i.e., CMF) was minimal, at only -0.63%. Our model thus performs consistently well across all domains, unlike some other models such as CMF and DeepAFP which show high performance in certain domains but significantly lower performance in others.\n(2) Integrating information from all domains simultaneously in a model can improve performance in each domain compared to modeling a pairwise domain-domain relationship. CDSR baselines that modeled five domains simultaneously, such as CAT-ART, CGRec, and our model, outperformed other baselines that only trained on domain pairs (e.g., C2DSR, MAN, MIFN, and \u03c0-net). Furthermore, extending these domain pair-wise CDSR models to a multi-domain CDSR scenario with |D| domains would require managing at least $\\binom{5}{2}$ pairs of relations, which becomes impractical when the number of domains is substantial. As a result, we confirmed the effectiveness of combining information from various domains at once for the CDSR task."}, {"title": "5.4 Discussion of the negative transfer (RQ2)", "content": "The phenomenon of negative transfer becomes apparent when comparing the performance of the domain-specific model, trained exclusively on single-domain sequence, with the performance of baseline models trained on cross-domain sequences. For the domain-specific model, we used CGRec [20], a state-of-the-art CDSR model that can also be trained on a single-domain sequence. As shown in Table 4, all models, except for ours, performed worse than the SDSR approach for two to nine of the ten domains in both datasets. Our model outperformed the SDSR approach for all domains, particularly in the Clothing, Toys, and Call-Use domains, where other models performed poorly. In conclusion, our model significantly mitigates the negative transfer problem compared to other CDSR baselines for all domains."}, {"title": "5.5 Discussion of Model Variants (RQ3)", "content": "To examine the validity of each component, we introduce various variants, each aligned with their specific design purpose (Table 5):\n(A) w/o LC-NTG: The LC-NTG module is removed in this model variant. Therefore, there is no gradient explicit controller in the training for high NTG domains in the model.\n(B) w/o SC-MIM: This model variant removes the SC-MIM objective, which means it does not take into account the correlation between single- and cross-domain representations for each domain.\n(C) w/o ACMoE: The ACMOE layer is replaced with the vanilla MoE layer without the stop gradient operation. This variant does not explicitly require experts to be specialized in SDSR and CDSR tasks. Therefore, it is not possible to compute the pure losses of SDSR and CDSR tasks, and as a result, the exact negative transfer in LC-NTG cannot be obtained.\n(1) Each of the three main components of our model contributes to improved performance. The prediction performance of our model and its three variants was compared, and it was discovered that removing or replacing any key component resulted in significant performance degradation. In particular, the performance gap between full SyNCRec and the (A) w/o LC-NTG variant indicates the advantage of a penalty for the high NTG domain in the training process. The comparison of full SyNCRec with (B) w/o SC-MIM shows that the SC-MIM module led to performance improvements in most domains, demonstrating the advantage of"}, {"title": "5.6 Hyper-parameter Analysis (RQ4)", "content": "As shown in Fig. 3, to assess how different hyper-parameter configurations affect our model, we performed experiments on two datasets using various configurations of crucial hyper-parameters, the model dimensionality (r) and the number of experts (K) in ACMoE. The other hyperparameters are set to the same values as those mentioned in Section 5.2.3.\n(1) Model dimensionality r: We observed that when r rises from 8 to 16, there is an improvement in the average performance of all domains on the Amazon dataset. For the Telco dataset, there is an improvement in the average performance of all domains when r rises from 8 to 64. From the results, we confirmed that the threshold for r depends on the characteristics of the datasets. The number of interactions of Telco is larger than that of the Amazon dataset, and thus a higher dimensionality is required for the Telco (i.e., 64) than for the Amazon dataset (i.e., 16) to achieve the best performance.\n(2) Number of Sequential Experts K: We experimented with a range of experts in ACMoE, from 2 to 6, and the results are presented in Fig. 3. Under all tested settings, our approach consistently surpassed the best baseline model (second best for each domain), in most domains of the two datasets. This result underscores the robustness of our approach with respect to the configuration of the mixture of experts.\nWe also verified the effects of the number of decoupled experts (j) for CDSR in ACMOE (Section 4.2.1). In both datasets, when three (j) out of five experts (K) are trained by the CDSR task (i.e., the stop gradient is applied to two experts for the CDSR task), the highest performance is achieved for all domains. This indicates that assigning more experts to the challenging CDSR task improves model performance. Due to limited space, a detailed analysis of j cannot be provided in this section."}, {"title": "6 ONLINE A/B TEST (RQ 5)", "content": "We deployed our recommendation model in the personal assistant service of our company. Online experiments were conducted within an A/B testing framework from October 2023 to January 2024. The"}, {"title": "7 CONCLUSION", "content": "We introduce a CDSR framework that tackles negative transfer by dynamically applying a weight to the prediction loss, which is determined based on the estimated evaluation of negative transfer. Additionally, we developed an auxiliary loss to enhance the exchange of valuable information between SDSR and CDSR tasks on a per-domain basis. Our model outperformed existing models in extensive tests on two real-world datasets with ten domains. Its implementation in our personal assistant app's recommendation system yielded a significant increase in click-through rate, demonstrating substantial business value."}]}