{"title": "Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation", "authors": ["Chung Park", "Taesan Kim", "Hyungjun Yoon", "Junui Hong", "Yelim Yu", "Mincheol Cho", "Minsung Choi", "Jaegul Choo"], "abstract": "Cross-Domain Sequential Recommendation (CDSR) improves recommendation performance by utilizing information from multiple domains, which contrasts with Single-Domain Sequential Recommendation (SDSR) that relies on a historical interaction within a specific domain. However, CDSR may underperform compared to the SDSR approach in certain domains due to negative transfer, which occurs when there is a lack of relation between domains or different levels of data sparsity. To address the issue of negative transfer, our proposed CDSR model estimates the degree of negative transfer of each domain and adaptively assigns it as a weight factor to the prediction loss, to control gradient flows through domains with significant negative transfer. To this end, our model compares the performance of a model trained on multiple domains (CDSR) with a model trained solely on the specific domain (SDSR) to evaluate the negative transfer of each domain using our asymmetric cooperative network. In addition, to facilitate the transfer of valuable cues between the SDSR and CDSR tasks, we developed an auxiliary loss that maximizes the mutual information between the representation pairs from both tasks on a per-domain basis. This cooperative learning between SDSR and CDSR tasks is similar to the collaborative dynamics between pacers and runners in a marathon. Our model outperformed numerous previous works in extensive experiments on two real-world industrial datasets across ten service domains. We also have deployed our model in the recommendation system of our personal assistant app service, resulting in 21.4% increase in click-through rate compared to existing models, which is valuable to real-world business.", "sections": [{"title": "1\nINTRODUCTION", "content": "In most real-world recommender systems, various business domains require understanding of the different interests and needs of users. A practical approach to address this involves Single Domain Sequential Recommendation (SDSR), which focuses on recommending the next item within a specific domain using only the single-domain sequence and builds many domain-specific models [9, 25, 32, 36]. Another solution is Cross-Domain Sequential Recommendation (CDSR): it predicts the next item a user will interact with by leveraging their historical interaction sequences across multiple domains [3, 11, 20]. In particular, CDSR has proven to be a promising approach to improve the performance of sequential recommendations across multiple domains simultaneously with one model [11, 13, 16, 20]. Note that the difference between CDSR and SDSR tasks for each domain lies in whether or not they use information from other domains in addition to the specific domain.\nHowever, in certain domains, CDSR significantly underperforms relative to the SDSR approach, while in other domains it may outperform the SDSR approach. This is because transferring knowledge from other weakly related domains or domains with different levels of data sparsity can have a negative impact on a particular domain: this is known as negative transfer [20]. To verify this experimentally, we measured the difference in the model performance between the CDSR and the SDSR approaches on a per-domain basis. As illustrated in Fig. 1, the recommendation performance of the Book and Clothing domains in the Amazon dataset, as well as the Call domain"}, {"title": "2\nRELATED WORK", "content": ""}, {"title": "2.1 Single-Domain Sequential Recommendation", "content": "The SDSR framework is designed to model the temporal dynamics in user-item interactions, effectively capturing how user preferences evolve over time within a specific domain. For example, GRU4Rec [8], STAMP [15], and NARM [12] were introduced to predict the user's next item using GRU-based models. Moreover, in SASRec [9], BERT4Rec [25], SINE [27], and LightSANs [4], attention mechanisms are utilized to encapsulate sequential patterns, thereby addressing both short- and long-term dependencies. In addition, other machine learning architectures such as a convolutional network (e.g., NextItNet [31]) or Markov Chain (e.g., TransRec [6]) are adopted in the SDSR task. Meanwhile, FDSA [32], S\u00b3Rec [36], and CARCA [22] utilize item features (e.g., text description) in addition to item sequences to demonstrate their efficiency. Nonetheless, studies on the SDSR framework continue to encounter issues of negative transfer in CDSR task."}, {"title": "2.2 Cross-Domain Sequential Recommendation", "content": "The goal of Cross-Domain Recommendation (CDR) is to improve recommendations in a given domain by leveraging information from various other domains. For example, CMF [24] and CLFM [5] modeled the matrix factorization of user-item interactions across multiple domains, capturing their shared patterns. Other approaches, DTCDR [37], DeepAPF [30], and BiTGCF [14], modeled user-item relationships in paired domains by a multi-task learning method. However, these methods modeled pairwise relationships between domains for the CDR task, a strategy that may be less effective in scenarios involving a large number of domains. To fill this research gap, CAT-ART [11] simultaneously modeled user and item representations in five domains using a matrix factorization approach. However, these studies have not taken into account the sequential nature of user interaction sequence.\nMeanwhile, the CDSR aims to enhance the task of sequential recommendation, focusing on items that span multiple domains. \u03c0-Net [17] introduced gating mechanisms designed to transfer information from a single domain to another paired domain. Meanwhile, C2DSR [3] employed a self-attention based encoder and graph neural network to model both single- and cross-domain representations. MIFN [16] introduced the concept of mixed information flow, which reflects the knowledge flows between multiple domains. MAN [13] designed group-prototype attention mechanisms to capture domain-specific and cross-domain relationships. However, these studies only modeled relationships between pairs of domains. In scenarios involving more than three domains, this approach would require handling an impractically large number of domain pairs, potentially limiting its feasibility in real-world applications [11]. To address this limitation, CGRec [20] proposed a CDSR model that deals with more than three domains at once. They approximated the negative transfer for each domain using Shapley values, and applied them to penalize items in domains with high negative transfer. However, even in CGRec, we encountered problems with negative transfer,"}, {"title": "3\nPRELIMINARY", "content": "We consider an expanding CDSR task characterized by interaction sequences spanning more than three domains. These domains may encompass diverse service platforms such as e-commerce or video platforms [11, 38]. Formally, the set of domains is denoted as $D = {A, B, C, ...}$, where $|D| \\ge 3$. We also consider the notation $d \\in D$ as a specific domain d. The notation $V_d$ is introduced to represent a set of items specific to the domain d. Then, $V$ indicates the total item set across all domains.\nDefinition 1. Single- and Cross-Domain Sequential Recommendation: The single-domain sequences of domain d are represented by $X^d = [(SOS), x_1^d, x_2^d, ..., x_{|X^d|-1}^d]$, where each element $x_t^d$ signifies an interaction occurring at time t and (SOS) is a start token of the sequence. We then define $X = (X^A, X^B, X^C, ...)$ as the cross-domain sequence where the |D| single-domain sequences are merged and rearranged in chronological order. Conversely, the cross-domain sequence X can be split into |D| of $X^d$ (Fig. 2a). For example, consider a scenario where a user engages with three domains, labeled {A, B, C). The cross-domain sequence for these domains is $X = [(SOS), x_1^A, x_2^A, x_3^A, x_1^B, x_2^B, x_4^C, x_8^C, x_9^C]$, and is then split into $X^A = [(SOS), x_1^A, x_2^A, x_3^A]$, $X^B = [(SOS), x_1^B, x_2^B]$, and $X^C = [(SOS), x_4^C, x_8^C, x_9^C]$ from the split process.\nNote that the SDSR solely focuses on recommending the next item within a specific domain d using the single-domain sequence of domain d (i.e., $X^d$) while the CDSR is to recommend the next item of domain d based on the cross-domain sequence (i.e., X).\nDefinition 2. Negative Transfer Gap: Based on previous studies [29, 33], we quantitatively measured the degree of negative transfer of each domain in the context of the CDSR task, referred to as the Negative Transfer Gap (NTG). Suppose that $L_{\\pi}^d$ is the empirical loss of the sequential recommendation task using a model \u03c0 for a specific domain d, which takes the user's single-domain sequence $X^d$ (i.e., SDSR task) or the cross-domain sequence X (i.e., CDSR task). We can then define the negative transfer gap $\\varphi_{\\pi}(d)$ of the domain d as a quantifiable measure of negative transfer: $\\varphi_{\\pi}(d) = L_{\\pi}(X^d) - L_{\\pi}(X)$, and negative transfer is identified when this value is negative. In Section 4.3, we introduce the module that explicitly calculates the negative transfer gap $\\varphi_{\\pi}(d)$ and we use it as the weighting factor for the domain-specific loss function.\nProblem Statement: Given the historical cross-domain sequences $X_{1:t}$ observed up to the time step t, our goal is to predict the next item $x_{t+1}^d$:\n$$\\operatorname{argmax}_{x \\in V_d} P(x_{t+1}^d | X_{1:t}),$$(1)\nwhere $P(x_{t+1}^d | X_{1:t})$ is the probability that the target item $x_{t+1}^d$ in domain d will be interacted with by the given item sequences. This goal is achieved through the SDSR and the CDSR tasks simultaneously. In other words, there are |D| single-domain sequences for the SDSR and one cross-domain sequence for the CDSR task, and therefore |D|+1 next item prediction tasks are performed in one model in a multi-task learning manner."}, {"title": "4\nMODEL", "content": "We illustrate the overall architecture of our model in Fig. 2."}, {"title": "4.1 Shared Embedding Layer", "content": "During the embedding look-up phase, we acquire initialized representations of items for |D| single-domain sequences (i.e., $X^d$) and one cross-domain sequence (i.e., X). For each domain d, we define an item embedding matrix $M_d$ represented by $R^{|V_d|\\times r}$, where $|V_d|$ indicates the total number of items in domain d, and r is the embedding dimension. We then concatenate the embedding matrices of all |D| domains to form an aggregated embedding matrix $M \\in R^{|V|\\times r}$, where |V| is the total number of item sets across all domains.\nThe single- and cross-domain sequences (i.e., $X^d$ and X) are subsequently converted into a fixed length, T. If the sequence length is shorter than T, special tokens (PAD) are added to the left side, serving as placeholders for past interactions on $X^d$ and X. Conversely, if the sequence length exceeds T, only the T most recent actions are preserved. As shown in Fig. 2b, the final embedding matrix M is shared by the single- and cross-domain sequence by a look-up operation to extract the initialized sequences representation $E^d \\in R^{T\\times r}$ and $E \\in R^{T\\times r}$, respectively (Fig. 2(c-1) and 2(c-2)). The aggregation |D| of $E^d$ in chronological order can be described as $E_{single}$ (Fig. 2(c-1)). Additionally, a learnable position embedding matrix $P\\in R^{T\\times r}$ is integrated into $E^d$ and E to encapsulate the sequential nature of user interaction ($E^d \\leftarrow E^d + P$, $E \\leftarrow E + P$) [28]. The embedding representations of $E^d$ and E at the t-th step are denoted as $e_t^d$ and $e_t$, respectively."}, {"title": "4.2 Asymmetric Cooperative Network with Mixture-of-Sequential Experts (ACMOE)", "content": "Referring to previous studies [29, 33], the degree of the negative transfer of a specific domain can be defined as the difference between \u2460 the loss of the SDSR and \u2461 the loss of CDSR tasks for the domain (i.e., \u2460 - \u2461). This value is called the negative transfer gap (NTG), and helps determine whether samples from other domains are beneficial or detrimental to the specific domain. The degree of negative transfer in a specific domain increases as the NTG decreases, and therefore, we assign this value as the weight for the prediction loss in the domain, resulting in a smaller gradient flow. To efficiently calculate this, we designed an asymmetric cooperative network that concurrently executes SDSR and CDSR tasks for each domain in a multi-task learning manner. For our multi-task learning, we utilized the multi-gate Mixture of Sequential Experts (MoE) architecture [21]. This architecture explicitly models the relationships between different tasks and learns task-specific functionalities, enabling it to effectively leverage shared representations. During this phase, we employed a decoupled mechanism allowing expert networks to be specialized in either SDSR or CDSR tasks, ensuring both tasks are implemented without mutual interference. For this, in the decoupled mechanism, a stop-gradient operation is adopted on some experts for the SDSR tasks and on other experts for the CDSR task. This approach enables the losses for both tasks to be computed independently, leading to a more precise evaluation of the NTG. We used Transformer [28] as the expert to handle sequential data.\n4.2.1 Architecture. Mathematically, given the initialized representations of single- and cross-domain sequences $E^d$ and E from the shared embedding layer, we perform many-to-many sequence learning in each sequential expert. In the case of single-domain sequences, for a given T-length inputs $E^d$, we can formulate the output of domain d for this module as given below:\n$$(Y^d)_{single} = h_d (f_d (E^d)),$$$$f_d (E^d) = \\sum_{k=1}^{j} g_d^k(E^d) SG(F_{TRM}^k(E^d)) + \\sum_{k=j+1}^{K}  g_d^k(E^d) F_{TRM}^k(E^d)$$(2)\nwhere $h_d$ is the tower network for domain d (Fig. 2(c-7)), $f_d$ is the multi-gated mixture of the sequential experts layer, SG(.) is the stop-gradient operation (Fig. 2(c-4)), and $F_{TRM}^k$ is the k-th transformer-based [28] sequential expert (Fig. 2(c-3); there are K experts in total). The tower network $h_d$ consists of a feed-forward network with layer normalization [1]. The stop-gradient operation SG(.) serves as an identity function during the forward pass, but it drops the gradient for variables enclosed within it during the backward pass. This operation is only used in the output from the 1 to the j-th expert network (j < K). Therefore, these experts cannot be updated by the single-domain sequences, but experts from j+1 to K can learn the unique sequential pattern of single-domain sequences. In other words, we can say that K-j represents the number of experts trained by the SDSR task with the gradient descent algorithm. We set j to be 0.2 times the total number of experts K, and report a sensitivity analysis on the hyper-parameter j in Section 5.6. In addition, $g_d$ is the gating network for domain d (Fig. 2(c-6)), which converts the input into a distribution across the K experts:\n$$g_d^k (E^d) = softmax(W_dE^d)$$(3)\nwhere $W_d \\in R^{K \\times dT}$ is the trainable fully-connected layer. The aggregation |D| of $(Y^d)_{single}$ in chronological order is described as $Y_{single}$ (Fig. 2(d-1)). The t-th element of $Y_{single}$ is then $(y_t^d)_{single}$.\nWe also perform the CDSR task using a similar scheme to that above for the single-domain case. For a given T-length input E, we represent the output of ACMOE module as follows:\n$$Y_{cross} = h_{cross} (f_{cross} (E)),$$$$f_{cross} (E) = \\sum_{k=1}^{j} g_{cross}^k(E) F_{TRM}^k (E) + \\sum_{k=j+1}^{K} g_{cross}^k(E) SG(F_{TRM}^k(E))$$(4)\nwhere $h_{cross}$ is the tower network (Fig. 2(c-9)) and $f_{cross}$ is the multi-gated mixture of sequential experts layer for a cross-domain sequence. The stop-gradient operation SG(.) (Fig. 2(c-5)) is only used in the output from j+1 to K-th $F_{TRM}$. From this operation, certain experts (from 1 to j) are able to learn the distinct sequential patterns present in cross-domain sequences. In other words, we can say that j is the number of experts trained by the CDSR task. The gating network $g_{cross}^k(E) = softmax(W_{cross}E)$ for the cross-domain sequence maps the input to a distribution over the K experts (Fig. 2(c-8)). We can then formulate the output for this module as $Y_{cross} = (Y_1, Y_2, ..., Y_n)_{cross}$. Note that $(y_t^d)_{single}$ and $(y_t)_{cross}$ are two representations of different views for the same item (i.e., also same domain d) at time step t, depending on whether it was encoded based on the single- or cross-domain sequence."}, {"title": "4.2.2 Transformer Experts", "content": "We adopted the self-attention module introduced in Transformer [28] as the sequential expert layer $F_{TRM}$ (Fig. 2(c-3)), which consists of two distinct sub-layers as follows: Multi-head Self-Attention (MSA) At each MSA head, the inputs $Z \\in R^{T\\times r}$ undergo a linear transformation resulting in three hidden representations, i.e., queries $Q_i \\in R^{T\\times r/p}$, keys $K_i \\in R^{T\\times r/p}$, and values $V_i \\in R^{T\\times r/p}$, where i indicates a specific head, p is the number of heads, and r is the dimension of the input. Using these three hidden representations, the scaled dot-product attention (Attn) is then computed as follows:\n$$Attn(Q_i, K_i, V_i) = softmax(\\frac{Q_iK_i^T}{\\sqrt{r/p}})V_i,$$(5)\n$$Q_i = ZW_i^Q, K_i = ZW_i^K, V_i = ZW_i^V,$$\nwhere $W_i^Q, W_i^K, W_i^V \\in R^{r\\times r/p}$ are the trainable parameters. In the MSA, the above Attn operation is implemented p times in parallel, and then involves concatenating the outputs of each head and linearly projecting them to extract the final output $H \\in R^{T\\times r}$:\n$$H = MSA(Z) = [Attn(Q_1, K_1, V_1)||...||Attn(Q_p, K_p, V_p)]W^F$$(6)\nwhere || is the concatenate operation and $W^F \\in R^{r\\times r}$ is the trainable parameters. For the sequential recommendation, only information from prior time steps can be utilized. Therefore, a masking operation is applied to the output of the MSA layer, effectively eliminating all connections between $Q_i$ and $K_j$ whenever j > i. Point-wise Feed-Forward Network (FFN) The FFN is then adopted into H to introduce nonlinearity and enable interactions among various latent subspaces using a fully-connected layer (FC), as follows:\n$$FFN(H) = [FC(H_1)||FC(H_2)||, ..., ||FC(H_T)],$$$$FC(H_t) = GELU (H_tW_1 + b_1)W_2 + b_2,$$(7)\nwhere $H_t$ is the t-th representation of H, $W_1 \\in R^{r\\times r}$, $b_1 \\in R^{r\\times 1},W_2 \\in R^{r\\times r}$, and $b_2 \\in R^{r\\times 1}$ are learnable parameters, and GELU is the gelu activation [7].\nTherefore, the transformer-based sequential expert $F_{TRM}$ in Eq 2 and 4 is described as follows: $F_{TRM}(\\cdot) = FFN*(MSA*(\\cdot))$."}, {"title": "4.3 Loss Correction with Negative Transfer Gap (LC-NTG)", "content": "From our ACMOE layer, we can explicitly measure the negative transfer gap (NTG) of each domain as described in Section 3, and use the NTG to adaptively assign lower weights to the loss of items in domains that exhibit significant negative transfer during the training stage (Fig. 2(e-3)).\n4.3.1 Single-Domain Item Prediction. As presented in Fig. 2(e-1), given the single-domain sequence $X_{1:t}^d$, and its expected next item $x_{t+1}^d$, we use the pairwise ranking loss [23] to optimize our model as follows:\n$$l_t^d = logo(\\sigma (P(x_{t+1}^d = x_{t+1}^{d+} |X_{1:t}^d) - P(x_{t+1}^d = x_{t+1}^{d-} |X_{1:t}^d))), L_{single} = \\sum_{t=1}^T l_t^d,$$(8)\nwhere $x_{t+1}^{d+}$ is the ground-truth item paired with a negative item $x_{t+1}^{d-}$ sampled from a uniform distribution, and $\\sigma$ indicates the sigmoid activation function. Note that $P(x_{t+1}^d = x |X_{1:t}^d)$ is calculated as $\\sigma((y_t^d)_{single} . M(x^d))$, where \u00b7 is the dot-product operation.\n4.3.2 Cross-Domain Item Prediction. Similar to the single-domain item prediction, the CDSR task using the cross-domain sequence $X_{1:t}$ is performed with the following objective (Fig. 2(e-2)):\n$$l_t = logo(\\sigma (P(x_{t+1}^d = x_{t+1}^{d+} |X_{1:t}) - P(x_{t+1}^d = x_{t+1}^{d-} |X_{1:t})), L_{cross} = \\sum_{t=1}^T l_t,$$(9)\nwhere $P(x_{t+1}^d = x |X_{1:t})$ is obtained by $\\sigma((y_t)_{cross} . M(x^d))$.\n4.3.3 Calculating the Negative Transfer Gap. We can describe the negative transfer gap (NTG) using the losses of Eq. 8 and Eq. 9 as follows:\n$$\\varphi_{\\pi}(d) = \\sum_{t=1}^T (l_t^d - l_t)$$(10)\nwhere $l_t^d$ and $l_t$ are losses of the SDSR and CDSR tasks in time step t for the domain d, respectively, calculated with our model \u03c0. Note that in the cross-domain item prediction loss $l_t$, only the loss values corresponding to domain d are used in Eq. 10.\nWe then calculated the trainable relative NTG in each domain. Let $\\lambda_d$ represent the relative NTG of domain d, and $\\lambda = (\\lambda_1, \\lambda_2, ..., \\lambda_{|D|})$ be the vector of the relative NTG. The vector of the relative NTG is initialized to be 0.From Eq. 10, we obtain $\\varphi_{\\pi}(d)$ for all domains at each training batch. $\\lambda$ is then updated in every batch as follows:\n$$\\lambda_d \\leftarrow softmax(\\alpha*\\lambda_d + \\beta * \\varphi_{\\pi}(d); \\delta); \\forall d \\in D,$$(11)\nwhere softmax(; \u03b4) is the softmax function with the temperature \u03b4 [26], and \u03b1 and \u03b2 are learnable parameters.\n4.3.4 Loss Correction. The relative NTG serves as a weight for the cross-domain item prediction loss computed in Eq. 9. This loss is re-aggregated by multiplying the relative NTG for each domain separately as follows (Fig. 2(e-3)):\n$$L_{cross} = \\sum_{t=1}^T \\sum_{d=1}^{|D|} \\lambda_d logo (P(x_{t+1}^d = x_{t+1}^{d+} |X_{1:t}) - P(x_{t+1}^d = x_{t+1}^{d-} |X_{1:t})$$(12)\nThis adaptive regulation of the gradient diminishes its flow in domains where there is significant negative transfer, thereby alleviating negative transfer effects across all domains."}, {"title": "4.4 Single-Cross Mutual Information Maximization (SC-MIM)", "content": "To improve the transfer of valuable information between SDSR and CDSR tasks, we developed the Single-Cross Mutual Information Maximization (SC-MIM), an auxiliary loss that maximizes the mutual information between the representation pairs from the SDSR and CDSR tasks on a per-domain basis. The SC-MIM extracts the correlation signals between the intrinsic characteristics of the both tasks for a specific domain. This module is an application of the concept of mutual information, which reflects how knowledge of one random variable reduces uncertainty in the other random variable. Mathematically, the mutual information I between random variables X and Y is given as follows:\n$$I(X, Y) = D_{KL}(p(X,Y)|lp(X)p(Y) = \\sum_{x,y} p(x,y) log \\frac{p(X, Y)}{p(X)p(Y)}$$(13)\nwhere $D_{KL} (||)$ is the Kullback-Leibler divergence. However, maximizing mutual information in high-dimensional spaces can be a challenging task. Therefore, in practice, it is common to maximize a tractable lower bound on I(X, Y). A specific lower bound that has demonstrated effective practical results is InfoNCE [10, 36], which is defined as follows:\n$$I(X, Y) \\ge E_{p(X,Y)} [\\rho_{\\theta}(x, y) - E_{q(\\hat{Y})} (log \\sum_{\\hat{y} \\in \\hat{Y}} exp \\rho_{\\theta}(x, \\hat{y}))] + log |\\hat{Y}|,$$(14)\nwhere x and y are two distinct viewpoints of the same input, and $\\rho_{\\theta}$ represents a parameterized similarity function with \u03b8. And $\\hat{Y}$ is a set of samples chosen from a proposal distribution $q(\\hat{Y})$, which includes a positive sample y and $|\\hat{Y}|-1$ negative samples. Refer to the detailed derivations of InfoNCE (Eq. 14) as a lower bound for mutual information in Oord et al. [19]. If $\\hat{Y}$ contains all possible outcomes of the uniformly distributed random variable Y, then maximizing InfoNCE is equal to maximizing the standard cross-entropy loss:\n$$E_{p(x,y)} [\\rho_{\\theta}(x, y) - log \\sum_{\\hat{y} \\in \\hat{Y}} exp \\rho_{\\theta}(x, \\hat{y})].$$(15)\nIn our problem, we focus on maximizing the mutual information of single- and cross-domain representations (i.e., $Y_{single}$ and $Y_{cross}$), which are different views of the user sequence. For this, we split the cross-domain representation $Y_{cross}$ into a subsequence per domain $(Y^d)_{cross}$ (Fig. 2(d-2)). Then, as shown in Fig. 2(d-3), we can describe the mutual information between single- and cross-domain sequences for domain d as follows:\n$$L_{SC-MIM} = - log \\frac{exp(\\rho( (Y^d)_{single}, (Y^d)_{cross} ))}{\\sum_{u^-} exp(\\rho( (Y^d)_{single}^u, (Y^d)_{cross}^u ))},$$(16)\nwhere u- is the other users in a training batch, and $(Y^d)_{single}^u$ is the subsequence of domain d of user u-. In addition, $\\rho(\\cdot,\\cdot)$ is implemented according to the following equation:\n$$\\rho(U, V) = \\sigma(U^T \\cdot W^H \\cdot V),$$(17)\nwhere \u03c3 is the sigmoid function and $W^H \\in R^{r\\times r}$ is a trainable parameter matrix."}, {"title": "4.5 Model Training and Evaluation", "content": "The total training loss function of our model is as follows:\n$$L = \\eta \\sum_{d=1}^{|D|} (L_{single}^d + L_{cross}) + (1-\\eta) \\sum_{d=1}^{|D|} L_{SC-MIM}$$(18)\nwhere \u03b7 is the harmonic factor. In the evaluation stage, we only used cross-domain representations to make predictions. For example, given the latest representations $(y_t^d)_{cross}$, the next recommended item is selected based on the highest prediction score in domain d:\n$$\\operatorname{argmax}_{x \\in V_d} (\\sigma ((\\mathbf{y_t^d})_{cross} ) . M(x^d)),$$(19)\nwhere $V_d$ is the item set of domain d and M is the embedding layer."}, {"title": "5 EXPERIMENTS", "content": "The experiments are designed to answer the following research questions:\n(RQ1): Does the performance of our model surpass the current state-of-the-art baselines in practical applications that involve more than three domains?\n(RQ2): Can our model effectively address the challenge of negative transfer across all domains in the CDSR task?\n(RQ3): What is the impact of various components of our model on its performance in CDSR tasks?\n(RQ4): How do variations in hyper-parameter settings influence the performance of our model?\n(RQ5): How does the model perform when deployed online?"}, {"title": "5.1 Datasets", "content": "5.1.1 Amazon Dataset. Amazon review datasets [18] was used in our experiments, encompassing five domains: Books, Clothing Shoes and Jewelry, Video and Games, Toys and Games, and Sports and Outdoors (Table 1). These domains are abbreviated as Books, Clothing, Video, Toys, and Sports, respectively.\n5.1.2 Industrial Dataset (Telco). We collected user logs from diverse real-world applications operated by a leading global telecommunications company. The dataset comprised customers who consented to the collection and analysis of their data. The data contain five domains: Application Usage (APP-Use), Call Record (Call-Use), Navigation Service (Navi), e-commerce service (e-comm), and Discount Coupon Usage (Coupon-Use). We identified 99,936 users, and there are 48,887 non-overlapping users across five domains."}, {"title": "5.2 Experimental Settings", "content": "5.2.1 Baseline Models. The performance of our model was evaluated with four categories of baseline models: (1) General Recommendation (GR), (2) Single-Domain Sequential Recommendation (SR), (3) Cross-Domain Recommendation (CDR), and (4) Cross-Domain Sequential Recommendation (CDSR), as shown in Tables 2 and 3. The baselines in the GR include BPRMF [23] and GCMC [2]. BPRMF is a classical approach that develops a pairwise loss function for modeling the relative preferences of users. GCMC proposes a graph autoencoder framework to address the matrix completion. Refer to Section 2 for a description of SR, CDR, and CDSR baselines. The RecBole framework [34, 35] was used to implement all CDR baselines and some SR models. CARCA [22], S\u00b3Rec [36], C2DSR [3],"}, {"title": "5.3 Performance Evaluation (RQ1)", "content": "We evaluated the performance of predicting the next item among all baselines and our model. The results for the two datasets are reported in Table 2 (Amazon) and Table 3 (Telco). Based on these results, we can draw the following conclusions.\n(1) The effectiveness of our model can be observed. Our model outperforms most baseline models on two real-world datasets.\u00b2 Our model improved upon the best baseline model (second best for each domain) by +3.13% (Book), +1.18% (Clothing), +7.29% (Video), +9.65% (Toys), and +5.28% (Sports) for HR@5 on the Amazon dataset, and +11.96% (Call-Use), +4.65% (Navi), +6.47% (Coupon-Use), and +38.81% (e-comm) on the Telco dataset. In the App-Use domain, our model did not achieve the top performance, but the difference from the best-performing baseline (i.e., CMF) was minimal, at only -0.63%. Our model thus performs consistently well across all domains, unlike some other models such as CMF and DeepAFP which show high performance in certain domains but significantly lower performance in others.\n(2) Integrating information from all domains simultaneously in a model can improve performance in each domain com-pared to modeling a pairwise domain-domain relationship. CDSR baselines that modeled five domains simultaneously, such as CAT-ART, CGRec, and our model, outperformed other baselines that only trained on domain pairs (e.g., C2DSR, MAN, MIFN, and \u03c0-net). Furthermore, extending these domain pair-wise CDSR models to a multi-domain CDSR scenario with |D| domains would require managing at least $\\binom{D}{2}$ pairs of relations, which becomes impractical when the number of domains is substantial (See footnote"}]}