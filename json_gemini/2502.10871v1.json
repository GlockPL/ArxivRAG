{"title": "THE REPRESENTATION AND RECALL OF INTERWOVEN STRUCTURED KNOWLEDGE IN LLMS: A GEOMETRIC AND LAYERED ANALYSIS", "authors": ["Ge Lei", "Samuel J. Cooper"], "abstract": "This study investigates how large language models (LLMs) represent and recall multi-associated attributes across transformer layers. We show that intermediate layers encode factual knowledge by superimposing related attributes in overlapping spaces, along with effective recall even when attributes are not explicitly prompted. In contrast, later layers refine linguistic patterns and progressively separate attribute representations, optimizing task-specific outputs while appropriately narrowing attribute recall. We identify diverse encoding patterns including, for the first time, the observation of 3D spiral structures when exploring information related to the periodic table of elements. Our findings reveal a dynamic transition in attribute representations across layers, contributing to mechanistic interpretability and providing insights for understanding how LLMs handle complex, interrelated knowledge.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) excel in tasks like translation, text generation, and comprehension 5,18,27, yet their mechanisms for storing and processing information remain poorly understood. In human cognition, complex networks of interrelated knowledge enable efficient generalization, inference, and creativity by integrating new information into existing frameworks 3,21. But how do LLMs represent multi-related attributes and interconnected knowledge? For example, when knowing a chemical element like hydrogen, are attributes such as 'the lightest element' and 'atomic number 1' stored independently, or do they influence each other? When one attribute is activated, does it trigger related concepts through a network of connections? If these attributes interact, how are they organized\u2014linearly, cyclically, or in more complex structures? Clarifying these mechanisms is vital for aligning LLMs with human values 16, enhancing their design, and broadening their applications.\nMechanistic interpretability offers a pathway to answer these questions, aiming to reverse-engineer neural networks into human-understandable algorithms 4,6,30. A proposed research direction is the linear representation theory, suggesting that LLMs encode knowledge as one-dimensional lines, with model states formed by sparse combinations of these representations. Evidence supports this for spatial, temporal, and sentiment knowledge 12,33. However, emerging evidence points to more intricate structures, such as circular encodings for periodic concepts like days or months 10, challenging the simplicity of the linear model.\nBuilding on prior work that primarily examined single-attribute representations, we delve deeper into how LLMs encode and recall complex, interwoven knowledge. Our study explores the interaction and independence of linguistic and factual representations, as well as their underlying non-linear geometric relationships. The key findings of our study are:\n1. In LLMs, intermediate layers tend to focus on factual knowledge, while later layers shift toward linguistic patterns and task-specific outputs (Sec.3).\n2. LLMs can recall associative knowledge about related attributes, even when not explicitly mentioned in the prompt, with the strongest recall observed in intermediate layers, diminishing in deeper layers (Sec.4.1)."}, {"title": "2 Preliminaries", "content": "Our study only focuses on how reliably acquired knowledge (i.e. things we're confident the model knows) is represented within LLMs, and excludes hallucinations or information not in the training set. We use the properties of chemical elements in the periodic table as a case study due to their frequent occurrence in training data, well-defined attributes, and quantifiable properties, making them an ideal subject for this investigation. We adopt Llama series models in this study."}, {"title": "2.1 Activation collection", "content": "Generating prompts To study how LLMs represent attributes across layers, we construct a prompt dataset based on a set of attributes ($A = \\{A_j\\}_{j=1}^{M}$, such as 'atomic number' or 'group') and a set of elements ($X = \\{X_i\\}_{i=1}^{N}$, such as \u2018Mg' or 'Al'). For linguistic diversity, we incorporate predefined template sets: $T_{cont} = \\{T_{cont}^k\\}_{k=1}^{11}$ for continuation-style prompts and $T_{ques} = \\{T_{ques}^k\\}_{k=1}^{11}$ for question-style prompts, with 11 templates in each.\nIn the continuation-style templates, the next output token would be the factual knowledge directly such as:\n$T_1^{cont}(A_j, X_i) = \u2018The A_j of X_i is \u2018$\n$T_2^{cont}(A_j, X_i) = \u2018X_i\u2019s A_j is \u2018$\nIn question-style templates, the next output token is typically a syntactic word like 'The', which ensures the grammatical structure is correct, such as:\n$T_1^{ques}(A_j, X_i) = \u2018What is the A_j of X_i?\u2019$\n$T_2^{ques}(A_j, X_i) = \u2018Which value represents X_i\u2019s A_j?\u201d$\nBy substituting each element and attribute (Xi, Aj) into these templates, we generate prompts:\n$P_{i,j,k} = T_k(X_i, A_j)$\nEach prompt Pi,j,k will then be fed into LLMs to study the corresponding activations at different layers.\nCollecting last-token activations Last-token activations capture the full prompt context in decoder-only models with masked attention, as they integrate information from all preceding tokens. For each layer l, we collect last-token activations $h_{i,j,k}^{(l)}$ from prompts $P_{i,j,k}$ across all elements and templates:\n$h_{i,j,k}^{(l)} = f_l(D(P_{i,j,k}) \\in R^{T \\times d},$\nwhere $f_l()$ denotes the layer-$l$ transformation, T is the token length, and d is the hidden dimension. The initial activation $h_{i,j,k}^{(0)}$ is obtained by embedding the prompt through an embedding layer $E_0$, followed by processing through L Transformer layers. Each layer applies multi-head attention and a feedforward network with residual connections and layer normalization:\n$h_{i,j,k}^{(l)} = LayerNorm (h_{i,j,k}^{(l-1)}) + Attention(Q, K, V))$\n$h_{i,j,k}^{(l)} = LayerNorm (h_{i,j,k}^{(l)}) + FFN(h_{i,j,k}^{(l)})).$\nHere, Q, K, and V represent the query, key, and value matrices used in multi-head attention to compute token-to-token interactions. Finally, $h_{i,j,k}^{(L)}$ is mapped to the vocabulary space using the vocabulary head $W_{vocab}$ to produce logits:\n$logits_{i, j,k} = h_{i,j,k}^{(L)} W_{vocab}$\nBy analyzing last-token activations $h_{i,j,k}^{(l)}$ across layers, we investigate how attributes are represented in the model's hidden states."}, {"title": "2.2 Activation distribution", "content": "We start with a preliminary visualization of the distribution of last-token activations for the 'atomic number' attribute. Activations from each transformer layer l were collected for the atomic number attribute across the first 50 elements using 11 continuation-style templates, forming the set $H_{atomic number}^{(l)}$. To enable informative plots to be produced efficiently, PCA was applied to $H_{atomic number}^{(l)}$ and then t-SNE was use to project the first 50 principle components into 2D. Fig.1 shows the resulting distributions for Meta-Llama-3.1-70B, with points colored by atomic number and other attributes, revealing their associations to atomic number.\nThe first column of the figure colors activations by true atomic number values (explicitly mentioned in the prompt). In early layers, prompts with similar vocabulary cluster together irrespective of atomic number, reflecting token-level similarity. In the intermediate layer, the activations are hierarchically clustered. The small clusters each contain activations for the 11 prompts"}, {"title": "3 Intermediate layers encode knowledge, later layers shape language", "content": "To understand how complex interrelated knowledge emerges, we first investigate how individual attributes are represented and evolve across layers. As a preliminary step, we analyzed attention maps, revealing that intermediate layers focus more on tokens with a significant impact on the output, while later layers distribute attention more evenly, suggesting a transition from capturing specific relationships to integrating broader context for cohesive outputs. Detailed findings are provided in the appendix (see Fig.A1). The following sections quantitatively examine the roles of intermediate and later layers in representing attributes within LLMs."}, {"title": "3.1 Intermediate layers know factual knowledge. Language? Not yet!", "content": "We used linear probing, with 5-fold cross-validation, to train linear Support Vector Regression (SVR) models for each layer, l, and attribute, Aj, based on the activation dataset $H_j(l)$. The ground truths correspond to attribute values $y_{i,j}$ for each element $X_i$. To focus on numerical factual knowledge while minimizing the influence of language patterns, we used continuation-style prompts. Specifically, SVR maps each activation vector $h_{i,j}^{(l)}$ from layer l to the target attribute $y_{i,j}$ using the following linear function:\n$\\hat{y}_{i,j} = w_j^{(l)} h_{i,j}^{(l)} + b,$\nwhere $w_j^{(l)} \\in R^d$ is the weight and $b \\in R$ is the bias.\nThe $R^2$ score trends of the test set for each attribute across layers are shown in Fig.2 with the detailed results for the best layer provided in Appendix D.2. Both the intermediate and last layers exhibit high R2 scores (note that is not 1; see Appendix D.1). The last layer's"}, {"title": "3.2 Later layers shift focus from factual knowledge to language patterns", "content": "The previous experiment shows that later layers begin to transform factual knowledge into language patterns. To further validate this, we compared question-style and continuation-style prompts using linear probing, following the procedure in Sec.3.1. The attribute 'group' was used as an example, with results shown in Fig.4."}, {"title": "4 Recall peaks at intermediate layers", "content": "The previous section focused on the behavior of single attributes across layers. Here, we investigate whether related attributes are interconnected by examining the recall ability of LLMs-their capacity to retrieve attributes related to, but not explicitly mentioned in the prompt. Additionally, we analyze the geometric mechanisms underlying this recall process."}, {"title": "4.1 Recalling knowledge ability peaks at intermediate layers and declines afterward", "content": "We conducted an experiment to explore the relationship between different attributes using misaligned linear probing. Specifically, we generated activation datasets from prompts about $H_{atomic number}^{(l)}$ and $H_{group}^{(l)}$. Separate probes were trained on each activation dataset, but in both cases, only $y_{group}$ (group labels) were used as the target during training. This approach examines whether activations related to one attribute (e.g., atomic number) encode information about another attribute (e.g., group). Continuation prompts were used, and the probing process followed the method described in Sec.3.1. The results are shown in Fig.5.\nThe results showed that in the early and intermediate layers, R\u00b2 scores for recalling group information were consistent, regardless of whether activations came from prompts about atomic number or group. However, in the later layers, activations from atomic number prompts showed a significant drop in R2 scores for group recall compared to those from group prompts. This suggests that intermediate layers enable LLMs to recall related attribute knowledge, indicating a capacity for broader knowledge representation. The significant drop in the later layers suggests that knowledge not immediately necessary for generating the next token is not activated. The relationships between different attribute representations are further explored in Sec.5."}, {"title": "4.2 Attribute geometric interrelationship", "content": "LLMs can recall knowledge in the early to intermediate layers, but how do these attributes interact? We hypothesize that attributes in LLMs exist in a high-dimensional"}, {"title": "5 Relationship in attributes representation: from superposition to separation", "content": "The last section explores whether one attribute's representation can recall related attributes without being mentioned, while this section explores the relationships between attribute representations across layers."}, {"title": "5.1 Attribute representations overlap in intermediate layers but become distinct later", "content": "As outlined in Sec.3.1, we trained a linear model for each attribute Aj at each layer l, yielding a weight vector $w_j^{(l)}$ that represents how attribute Aj is stored in the activation space of layer l. To analyze attribute relationships across layers, we computed the cosine similarity between weight vectors of different attributes using continuation-style activation sets to minimize language pattern influence.\nFig.9 illustrates the cosine similarity across 80 layers of Meta-Llama-3.1-70B. Notably, in high-dimensional spaces, random vector pairs typically approach orthogonality due to the 'blessing of dimensionality'. To illustrate this, we randomly sampled vector pairs in an 8129-dimensional space (the activation vector size of Meta-Llama-3.1-70B) and calculated their cosine similarity, with the 99.9% confidence interval (CI) shown in gray. Cosine similarity outside this interval indicates meaningful relationships between attributes. See Appendix C for more details.\nIn the early layers, high similarity reflects token-level processing rather than semantic understanding. As layers deepen, similarity decreases as the model begins capturing semantics. In the intermediate layers, similarity rises, indicating shared representation of correlated attributes. Finally, in the later layers, similarity drops again as the model separates features for refined decision-making."}, {"title": "5.2 Attribute representations exhibit linear relationships", "content": "To directly capture relationships between attributes, we map the representation of $A_{j1}$ to $A_{j2}$ at each layer by training a linear model on the last activation of a fixed prompt template (with PCA reducing the activation dimension to 20), e.g., \u2018In the periodic table, the Aj of Xi is.' Mapping performance is evaluated by R\u00b2 scores of 5-fold cross-validation.\nFig.10 shows R\u00b2 scores across layers for attribute pairs. High scores in early layers reflect token-level differences due to shared prompt template. As layers deepen, R\u00b2 decreases, indicating a shift to semantic representations. In the intermediate layers, R\u00b2 rises, revealing attribute overlap, exhibiting linear relationships. In the later layers, R\u00b2 drops as the model separates attributes for task-specific outputs."}, {"title": "7 Discussion and conclusions", "content": "Building on prior research into how LLMs represent individual entities, our study systematically investigates how LLMs encode and recall interwoven structured knowledge across transformer layers. Intermediate layers emerge as pivotal for encoding factual knowledge and maintaining superimposed representations of related attributes, enabling effective recall of associated features even when not explicitly prompted. In contrast, later layers refine these representations into more distinct and task-specific outputs, prioritizing linguistic coherence. In addition, we uncovered geometric patterns, such as 3D spirals, reflecting relationships like the periodicity in chemical elements. This suggests LLMs encode both linear and non-linear representations aligned with the real geometry of knowledge. The findings provide insights that could inform the development of more interpretable and efficient models, with potential applications in scientific discovery and trustworthy AI systems.\nLimitations. Our prompts are focused on chemical elements, while ideal for their structured attributes, may not extend to domains with more abstract features. The hypothesis-driven validation of geometric structures may oversimplify LLMs' non-linear interactions."}, {"title": "Impact Statement", "content": "We believe interpretability in LLMs is essential for AI safety, reducing unintended behaviors and building trust. Understanding how knowledge is stored and recalled across layers can inspire more interpretable, efficient models, advance knowledge editing and scientific discovery."}, {"title": "Code Availability", "content": "The code required to reproduce the results presented in this paper is available at https://github.com/tldr-group/LLM-knowledge-representation with an MIT license agreement."}, {"title": "Appendix", "content": "A Attention map detailed results\nTo investigate how the model prioritizes different parts of the input text, we conducted a preliminary analysis using the 32-layer Meta-Llama-3-8B model. We adopted the attribute Aj, Period and Group, and iterated over Xi, consisting of 50 elements, using the prompt template: \u2018In the periodic table of elements, the Aj of Xi is.' These prompts were input into the language model, and we analyzed the average attention across all attention heads in each transformer layer from the token 'is' to all other tokens. The averaged results across different prompts are presented in Fig.A1.\nThe results indicate that in the intermediate layers, where entropy is relatively high, there is a noticeable concentration of attention from the token 'is' to attribute and element tokens. This suggests that these intermediate layers focus more on tokens within the sequence that have a significant impact on the output. In contrast, the later layers, which exhibit lower entropy (with the exception of the final layer), show a more evenly distributed attention pattern. This pattern implies that the model transitions from focusing on specific token relationships to integrating broader context, thereby finalizing its interpretation for a cohesive output."}, {"title": "B Intervention outcomes in geometric recall", "content": "B.1 Layer-wise performance evaluation\nFig.B1 illustrates the prediction error across layers when the activation of the last token across layers is replaced with the predicted activation derived from the geometric space $f(r, g, p) = (rcos \u03b8, r sin \u03b8, r)$. In the early layers, errors gradually decrease because the model has not yet captured semantic information, and the geometric space is still being constructed. The continuous decline in error reflects the model's growing ability to capture semantic information and progressively build a coherent geometric representation. By layer 20, the error stabilizes, indicating that these layers effectively encode the periodic and geometric relationships between atomic properties such as atomic number, group, and period."}, {"title": "C Blessing of dimensionality", "content": "When the dimensionality is very high, the most of random vector pairs approach orthogonality. We illustrate this by sampling pairs of vectors in an 8129-dimensional space (corresponding to the activation vector dimension of Meta-Llama-3.1-70B) and computing their cosine similarities. The 99.9% confidence interval (CI) provides an estimate of the expected cosine similarity range at each dimensionality:\n$C_{199.9\\%} = \\mu \\pm z \\frac{\\sigma}{\\sqrt{n}},$\nwhere u is the sample mean, o is the sample standard deviation, n is the number of sampled pairs, and z \u2248 3.29 for a 99.9% confidence level."}]}