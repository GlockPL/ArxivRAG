{"title": "PERCUL: A Story-Driven Cultural Evaluation of LLMs in Persian", "authors": ["Erfan Moosavi Monazzah", "Vahid Rahimzadeh", "Yadollah Yaghoobzadeh", "Azadeh Shakery", "Mohammad Taher Pilehvar"], "abstract": "Large language models predominantly reflect\nWestern cultures, largely due to the domi-\nnance of English-centric training data. This\nimbalance presents a significant challenge, as\nLLMs are increasingly used across diverse con-\ntexts without adequate evaluation of their cul-\ntural competence in non-English languages,\nincluding Persian. To address this gap, we\nintroduce PERCUL, a carefully constructed\ndataset designed to assess the sensitivity of\nLLMs toward Persian culture. PERCUL fea-\ntures story-based, multiple-choice questions\nthat capture culturally nuanced scenarios. Un-\nlike existing benchmarks, PERCUL is cu-\nrated with input from native Persian annota-\ntors to ensure authenticity and to prevent the\nuse of translation as a shortcut. We eval-\nuate several state-of-the-art multilingual and\nPersian-specific LLMs, establishing a foun-\ndation for future research in cross-cultural\nNLP evaluation. Our experiments demon-\nstrate a 11.3% gap between best closed source\nmodel and layperson baseline while the gap in-\ncreases to 21.3% by using the best open-weight\nmodel.", "sections": [{"title": "1 Introduction", "content": "Effective interactions between users from diverse\nbackgrounds and LLMs are contingent on outputs\nthat are culturally relevant (Bhatt and Diaz, 2024).\nAs the use of generative artificial intelligence in-\ncreases to expedite and automate personal and pro-\nfessional tasks, the cultural values embedded in AI\nmodels may inadvertently bias people's authentic\nexpression and perpetuate the dominance of cer-\ntain cultures (Tao et al., 2024), particularly West-\nern culture, which is over-represented in English-\ndominated training data (Li et al., 2024; Naous\net al., 2024). This highlights the importance of cre-\nating culture-specific benchmarking tools to assess"}, {"title": "2 Related Work", "content": "LLM evaluation has expanded significantly in re-\ncent years, covering aspects such as reasoning (Suz-\ngun et al., 2023; Sprague et al., 2024), knowl-\nedge and language understanding (Rein et al.,\n2023; Wang et al., 2024b), and instruction follow-\ning (Zhou et al., 2023; Qin et al., 2024). As LLMs\nhave dramatically improved in capability, the focus\nof benchmarking has shifted towards more chal-\nlenging tasks, such as cultural awareness. Despite\nnumerous attempts to develop cultural benchmarks\nfor English (Wang et al., 2024a,c; Rao et al., 2024;\nChiu et al., 2024) and other widely spoken lan-\nguages (Myung et al., 2024; AlKhamissi et al.,\n2024; Fung et al., 2024; Kim et al., 2024; Huang\net al., 2024; Masoud et al., 2023), a gap remains in\nevaluations of less-studied languages and cultures,\nsuch as Persian.\nMost existing Persian benchmarks focus on lan-\nguage understanding tasks such as textual entail-\nment and question answering (Amirkhani et al.,\n2023; Darvishi et al., 2023; Abadani et al., 2021;\nKhashabi et al., 2021), or the evaluation of fac-\ntual/scientific knowledge of LLMs (Ghahroodi\net al., 2024; Abaskohi et al., 2024). For instance,\nthe Khayyam-Challenge (Ghahroodi et al., 2024)\nproposes a set of 20K Persian questions divided\ninto 38 tasks, but these tasks are mainly school-\nlevel examinations, primarily covering mathemati-\ncal and scientific subjects. Although this is useful\nfor evaluating the capabilities of LLMs to solve sci-\nentific problems in Persian, it fails to assess LLMs'\nunderstanding of Persian culture. This also applies\nto the work of (Abaskohi et al., 2024) which intro-\nduces two new datasets to evaluate LLM abilities\nin solving Persian mathematical and scientific ques-\ntions.\nAmong works on Persian culture, PSN (Saf-\nfari et al., 2024) provides pairs of social norms\nand contexts along with a label for each pair de-\nscribing the appropriateness of each pair. How-\never, it is limited to social norms, leaving out\nother important aspects such as Visible Behavior\nor Rituals. BLEnD (Myung et al., 2024) and Cul-\nturalBench (Chiu et al., 2024) are multi-cultural\ndatasets that despite the inclusion of certain ques-\ntions about Persian culture, present crucial limi-\ntations. BLEnD primarily features questions that\nfocus on non-Persian cultural events and traditions,\nsuch as Thanksgiving or Christmas, making it less\nrelevant for assessing cultures where these events\nare not celebrated, such as Persian. CulturalBench,\nwhile contains question relevant to Persian culture,\nis small in size."}, {"title": "3 PERCUL Construction", "content": "The process of creating PERCUL consists of mul-\ntiple steps, as shown in Figure 2. Briefly, the cre-\nation process begins (1) by identifying cultural cat-\negories based on Hall's Triad of Culture (Katan and\nTaibi, 2021). (2) Then, native annotators generate"}, {"title": "3.1 Base Theory", "content": "To effectively assess cultural understanding of\nLLMs, we must first establish a clear definition\nof culture. One widely accepted definition is Ed-\nward T. Hall's Triad of Culture, commonly known\nas Cultural Iceberg Theory. This model, which is\nfrequently used by intercultural scholars and train-\ners (Katan and Taibi, 2021; Thier, 2013; Manrai\net al., 2019), has recently gained traction among\nNLP researchers (Singh et al., 2024). Hall's the-\nory classifies culture into three levels: technical,\nformal, and informal. The technical level is charac-\nterized by empirical facts and precise definitions,\ntypical in scientific discourse. The formal level con-\nsists of traditions and social norms that shape ev-\neryday life, often going unnoticed unless violated.\nThe informal level, meanwhile, encompasses un-\nconscious, emotionally driven behaviors absorbed\nthrough socialization (Katan and Taibi, 2021). Our\nanalysis centers on the technical and formal level\n(see Figure 3). We choose not to include the infor-\nmal category due to the difficulty of capturing these"}, {"title": "3.2 Seed Topic Collection", "content": "To represent Persian culture across all categories\ndepicted in Figure 3, we collected 709 cultural seed\ntopics through human annotation (see Table 1). A\ngroup of native Persian speakers from diverse cul-\ntural backgrounds (see Appendix A) contributed\ntheir perspectives while following the guidelines in\nAppendix B. To ensure the quality of the seed top-\nics, an inter-agreement assessment was conducted"}, {"title": "3.3 Metadata Collection", "content": "To enhance narrative generation and prevent hal-\nlucination issues when incorporating LLMs, we\ncollect a set of carefully annotated data from na-\ntive Persians to ground the generated storylines in\nnext steps in factual infromation. First, for each\ncategory, we collect human-annotated facets (see\nTable 1). These facets were expected to effectively\ndescribe the characteristics of the seed topics in\nthat category. They were also required to provide\nsufficient clues and factual information, allowing"}, {"title": "3.4 Instance Generation", "content": "Using seed topics and their corresponding meta-\ndata, we conduct a semi-automatic process by\nprompting two state-of-the-art LLMs, GPT 40\n(Team, 2024) and Claude Sonnet 3.5 (Anthropic,\n2024), to generate short storylines leveraging the\nprovided prompts (see Appendix C). These sto-\nrylines imply the respective seed topic using its\nmetadata as clues. Table 2 shows two samples\n(translated) from the dataset. To ensure cultural\nauthenticity and accuracy, two human annotators\nreview and revise the model-generated stories with-\nout knowing their source models. Using the user in-"}, {"title": "3.5 Distractor Options", "content": "We develop six heuristic rules to guide comprehen-\nsion option generation and use GPT 40 and Sonnet\n3.5 to create 24 options per question (2 models\n\u00d7 6 rules \u00d7 2 options). The options undergo a\nthree-stage selection process:\n1.  Initial Selection:  Human annotators evalu-\nate 4 options per heuristic rule (2 from each\nmodel) and select the 2 best options that match\nthe rule's intended objective. Model names\nare hidden to prevent bias.\n2.  Focused Pruning:  From the remaining\noptions, annotators select 6 options per story,\nallowing up to 2 options from the same rule."}, {"title": "3. Final Refinement:", "content": "Annotators select 3 final\noptions, prioritizing contextual relevance and\nstory alignment.\nEach stage includes inter-agreement assessment to\nvalidate annotator consistency (see Appendix C).\nExample of resulting distractors and stories are\nshown in Table 2."}, {"title": "3.6 Data Statistics", "content": "The final dataset is a comprehensive collection of\n592 multiple-choice question-answer pairs, care-\nfully designed to assess story comprehension while\nsubtly incorporating cultural seed topics through\nshort stories without explicit mention. The dis-\ntribution of these stories across various cultural\ncategories is presented in Table 1, providing a de-\ntailed breakdown of the dataset's composition. Fur-\nthermore, the dataset is accompanied by a set of\nmetadata, which will be made available to ensure\na comprehensive understanding of the data and its\ncultural nuances. This metadata will serve as an in-\nvaluable resource for researchers and practitioners\nworking with the dataset."}, {"title": "4 Experiments", "content": "We perform a comprehensive series of evalua-\ntions on our dataset using state-of-the-art closed-\nsource and open-weight models, as depicted in Fig-\nure 4. We also assess two Persian open-weight\nLLMs, namely PersianMind-v1.0 (Rostami et al.,\n2024) and PartAI Dorna-Llama3-8B-Instruct (Par-\ntAI, 2024), which are aimed to enhance Persian\nlanguage and cultural understanding by further pre-\ntraining & fine-tuning on corpora with dominant\nPersian data. To ensure the reproducibility of our\nexperiments, all models utilize zero temperature\nand allowed to generate up to their maximum gener-\nation length. We employ the same prompts for each\nquestion across all models, which can be found in\nAppendix C. The results are presented in Table 3."}, {"title": "Accuracy and model size.", "content": "Figure 4 shows the\nperformance variation of models with their size (in\nterms of parameters). As can be seen, there is a\nclear positive correlation between the number of\nparameters and the accuracy of models within each\nmodel family. However, this relationship does not\nhold across different families. For instance, LLaMa\n3.1 405B, while being the top model in the LLaMa\nfamily, its performance is close to Command-R-\nPlus, despite being nearly four times larger. Sim-\nilarly, Gemma 2 9B's, which is comparable in ac-\ncuracy to models 10x and 40x larger in terms of\nparameter count. These differences imply that ar-\nchitectural advancements and data efficiency might\nhave a more substantial impact than mere size. As\nPersian was not the target language for any of these\nmultilingual LLMs, this outcome is expected due\nto the unknown quantity and quality of Persian data\nin their training set."}, {"title": "Persian (fine-tuned) models.", "content": "Another interest-\ning observation, as depicted in Figure 6, is the\nnegative impact of fine-tuning on Persian models.\nBoth Persian models exhibited lower performance\ncompared to their corresponding base models. Al-\nthough it's fair to note that PersianMind refused\nto answer most of the questions by stating \"The\nanswer is not available in the provided options\".\nThis could be attributed to the quality of the fine-\ntuning data which may have introduced noise or\ncaused overfitting, resulting in a decline in the mod-\nels' ability to generalize effectively. This finding\nprompts further investigation into the quality and\nrelevance of the fine-tuning data, which we propose\nas a direction for future research."}, {"title": "4.1 Accuracy per category", "content": "Figure 5 shows performance of the models across\nthe 11 cultural categories. Among these, music\nproves to be the most challenging, with the re-\nspective accuracies of 56.2% and 59.4% for 3.5-\nSonnet and the best performing model on that\ncategory (Gemini Pro 1.5). In contrast, rituals\nis the least difficult, with three models crossing"}, {"title": "4.2 Impact of Translation", "content": "Since Persian is not a target language for most of\nthese multilingual LLMs and their training corpora\nis predominantly English-based, one might assume\nthat translating our stories into English could en-\nhance these models' performances. We investigate\nthe impact of translation on model performance to\ndetermine if they rely on translation as a proxy for\nunderstanding or have directly learned the concepts\nin the target language. To ensure the translation\nquality, we experiment with both Google Trans-\nlate API and GPT 40. After careful investigation,\nwe opted for GPT 40 given that it provided higher\nquality translations from Persian to English. Fig-\nure 7 displays the results for the best model in top-\nperforming families, as evaluated using the English\ntranslation of the dataset. As can be seen, the accu-\nracy of these models decreases by 6.6% to 14.5%\non the translated dataset. To delve deeper into the\nreasons for this decline, we manually examine the\nresults of Sonnet 3.5 (the best performing model) in\nboth the original Persian and the translated samples.\nHaving two sets of answers for these models, let's\ndenote the set of correct answers in Persian as P\nand that for English as E. Then, P \u2013 E represents\na set of answers where the model initially provided\nthe correct answer but failed when the question was\ntranslated. To investigate the cause, we categorized\nthe items in this set into three classes:"}, {"title": "4.3 Distraction Analysis", "content": "To gain deeper insights into common failure pat-\nterns of LLMs concerning their understanding of\nPersian culture, we examine the distraction choices\nin PERCUL and their success in deceiving the mod-\nels. For this analysis, we evaluate how heuristic\nrules are distributed within each category (refer to\nAppendix E for full model distributions) and con-\nsider how often a rule is chosen for its category as\na measure of its effectiveness.\nThe effectiveness, distribution of heuristic rules\nwithin each category, of distractor options created\nby different heuristic rules in misleading models\nover different cultural categories.\nOur analysis across cultural categories (Figure 8)\nshows that heuristic rule 1 (Partial Correctness)\nwas consistently the most effective in misleading\nmodels. R1 creates options that are either partially"}, {"title": "5 Conclusion", "content": "In this paper, we introduced PERCUL, a carefully\ncurated dataset designed to assess LLMs' sensi-\ntivity towards Persian culture. Our dataset is non-\ntrivial, as it employs implied concepts within con-\nversations or story scenarios, rendering translation\nineffective for solving our benchmark. The experi-\nments demonstrated a significant performance gap\nbetween open-weight and closed-source LLMs for\nPersian culture. We also showed that the knowl-\nedge of Persian culture in LLMs is not dependent\non the number of parameters when comparing inter-\nfamily models, whereas parameter count plays a\ncrucial role in intra-family models. Lastly, our\nexperiments revealed that current state-of-the-art\nPersian-specific LLMs fall short and even degrade\nin performance, when compared to their original\nbase models, emphasizing the need for more effec-\ntive methods, models, and higher-quality datasets\nto train Persian-specific LLMs. For future research,\nwe suggest studying LLMs based on the final level\nof culture, namely informal level, where categories\nare more subjective. One potential approach could\ninvolve assigning personalities to each LLM and\nobserving their behavior in a simulated environ-\nment to evaluate the third level of cultural under-\nstanding."}, {"title": "Limitations", "content": "During our research, we aimed to include annota-\ntors from diverse backgrounds and cities, but the\nmajority were university students, which may intro-\nduce bias towards the Persian academic community\nand potentially limit the cultural knowledge cap-\ntured in the dataset. Due to the inability to host\nmost state-of-the-art LLMs locally, we relied on\nAPIs to benchmark these models, restricting us to\na specific set of models. While we managed to\nbenchmark many SOTA models, the list is not ex-\nhaustive. Despite our efforts to encompass various\naspects of Persian culture, there remain untapped\nareas such as individualism and communication\nthat are not addressed in this work. These informal\naspects of culture are inherently subjective and are\nhard to capture in the medium of text."}, {"title": "Ethics Statement", "content": "This work presents various aspects of Persian cul-\nture through illustrative situations. While these\naspects and their examples are gathered by a di-\nverse group of Persian annotators and validated by\nanother group, adhering to a carefully crafted man-\nifesto, it is not entirely free from bias. Some sec-\ntions of the dataset, particularly those concerning\nsocial norms and behaviors, contain information\nthat mirrors the current state of Persian culture, re-\ngardless of whether it is unpleasant or criticized by\nnew social movements. We included such content\nfor the sake of comprehensiveness, and it does not\nnecessarily reflect the authors' opinions on these\nmatters."}]}