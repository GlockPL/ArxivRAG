{"title": "External Model Motivated Agents: Reinforcement Learning for Enhanced Environment Sampling", "authors": ["Rishav Bhagat", "Zhiyu Lin", "Mark Riedl", "Jonathan Balloch", "Julia Kim"], "abstract": "Unlike reinforcement learning (RL) agents, humans remain capable multitaskers in changing environments. In spite of only experiencing the world through their own observations and interactions, people know how to balance focusing on tasks with learning about how changes may affect their understanding of the world. This is possible by choosing to solve tasks in ways that are interesting and generally in-formative beyond just the current task. Motivated by this, we propose an agent influence framework for RL agents to improve the adaptation efficiency of external models in changing environments without any changes to the agent's rewards. Our formulation is composed of two self-contained modules: interest fields and behavior shaping via interest fields. We implement an uncertainty-based interest field algorithm as well as a skill-sampling-based behavior-shaping algorithm to use in testing this framework. Our results show that our method outperforms the baselines in terms of external model adaptation on metrics that measure both efficiency and performance.", "sections": [{"title": "1 Introduction", "content": "Deep reinforcement learning (RL) (Sutton & Barto, 2018) has proven useful in solving tasks in many fields, including playing games (Mnih et al., 2015; Silver et al., 2016), robotics (Gu et al., 2017), and traditional control applications such as power management (Mao et al., 2016; Zhang et al., 2019). In some of these applications, the RL agent whose primary objective is defined with task rewards may additionally need to explicitly model phenomena not directly related to that primary task. These models that train on data acquired while the agent performs the task, which we refer to as \"external models\", are often present in complex systems such as robots or smart devices.\nExternal models are used to estimate descriptive statistics, distinguish safe vs unsafe areas, and model environment dynamics, and can be used to inform third-parties like humans or prepare the agent itself for downstream tasks. Consider an autonomous underground mining robot whose pri-mary task is to dig and search for minerals, but that is also being used to model and monitor whether the environment is safe for humans, looking for dangers such as toxic air or intense tem-peratures (Nanda et al., 2010). Although a temperature safety model may not be relevant to the robot's mining task since temperatures can pose a danger to humans long before the robot, it is critical to human safety that the temperature model remains accurate. While many use-cases might not require the task policy and an external model to be trained simultaneously, if an environment transfer occurs during deployment, such as moving to a new depth underground or damage to the robot, both the task policy and external model must be quickly adapt to the change.\nThese external models generally exist outside of the reinforcement learning loop and therefore have no conventional way to dictate to the policy what new data is needed. In the case of environment change, we hypothesize that explicitly identifying and acquiring more informative data for external model adaptation is crucial to improving the performance and efficiency of the external model. While non-greedy behavior in RL is conventionally used to compensate for the shortcomings of greedy policy learning, we argue that, in the presence of some environment change, intelligent agents supporting external models will need to use non-greedy behavior to update those models as well. Inaccurate external models could result in increased risk if high-stakes decisions, such as exploring an area the model predicts is safe, are made using them. Thus, this work examines how an RL agent in a changing environment can balance updating its task policy with improving the efficiency and performance of external model adaptation.\nThe key idea for this work is that there are points of interest that, if observed, would benefit adaptation of the external model. Our main contributions are the following: (1) a framework to motivate agents in a task reward agnostic way using two modules: a definition of \"interest\" and a method to steer agents by leveraging interest; (2) an exemplar implementation of both modules to motivate the agent with the secondary objective of enhanced environment sampling for external model learning; (3) experiments and results demonstrating that our interest-based agent motivation implementations improve the adaptation of external models in the presence of environment change compared to the standard PPO and online DIAYN baselines."}, {"title": "2 Related Work", "content": "Multi-Objective Reinforcement Learning. The core technical challenge of this work focuses on influencing the behavior of an RL agent in a changed environment according to a model with an objective unrelated to the agent's reinforcement task. As single-objective RL is poorly suited to handle multiple objectives scenarios like these (Hayes et al., 2022), multi-objective reinforcement learning (MORL) is often used to optimize one or more policies according to multiple objectives simultaneously (Roijers et al., 2013; Moffaert & Now\u00e9, 2014). Our work is related to, but different from, multi-objective RL because our secondary objective of improving external model adaptation is not an RL task. As a result, existing multi-objective RL methods do not fit this problem.\nInfluencing agent behavior according to a complex set of contraints or objectives, however, is not limited to MORL. Policy shaping Griffith et al. (2013) enables an oracle policy to influence a learned policy, while reward shaping (Ng et al., 1999; Knox & Stone, 2009) and intrinsic rewards (Chentanez et al., 2004; Aubret et al., 2019; Linke et al., 2020) steer agent behavior by changing the reward function with factors separate from the task. However, we do not assume access to an oracle policy, and reward shaping only has a delayed impact on the policy and often produces brittle policies that are highly sensitive to changes in the environment (Fu et al., 2018). Goal-conditioned (Nair et al., 2018; Liu et al., 2022; Hafner et al., 2022) and skill-conditioned (Eysenbach et al., 2019; Sharma et al., 2019) reinforcement learning has become a popular alternative that influences behavior by learning a policy conditioned on a certain goal or skill vector. Related to options in hierarchical RL (Sutton et al., 1999; Stolle & Precup, 2002; Pateria et al., 2021), by learning an embedding space of skills or goals, policy conditioning can be used to immediately influence agent behavior depending on how the goal or skill is selected. As such, our work uses skill-conditioned policies to trade-off objectives for our unique multi-objective problem setting.\nInteresting Points for Learning. In this work, skills are used to direct the policy toward sam-ples that are most beneficial, i.e. \"interesting,\" to the external model's transfer learning process. Adapting to the changed environment with transfer learning allows the model to improve learning"}, {"title": "3 Approach", "content": "In this work, we developed an approach, which we call External Model Motivated Agent (EMMA), to influence an agent towards more interesting samples for an external model while performing its task. The EMMA approach formulates the solution to this unique multi-objective problem as a modular framework for reward-agnostic agent motivation, described in Section 3.1. Additionally, we designed exemplar implementations of the framework's interest and influence modules, which are described in Sections 3.2 and 3.3.\n3.1 Reward-Agnostic Agent Motivation Framework\nOur agent motivation framework consists of two separate modules: Point of Interest Field (Interest Field) and Point of Interest Influence (POI Influence). The interest field is a scalar field over the observation space $\\mathcal{O}$ that defines how \"interesting\" an observation is for the policy to visit. The POI influence module uses the interest field to shape the agent's behavior to collect more \"interest\" throughout the on-policy rollout. Together, these modules form a task reward-agnostic motivation framework. Figure 1 shows the complete framework and illustrates how the Interest Field and P\u039f\u0399 Influence modules interact.\nThe first module is the interest field: $f_{\\text{POI}}: \\mathcal{O} \\rightarrow \\mathbb{R}$. This field defines how interesting a given observation is to the external model. For our methods, we require the scalar field to be defined at every observation in the observation space, allowing it to be queried for any observation $o \\in \\mathcal{O}$, irrespective of the agent's current state. These constraints form a standard interface for the P\u039f\u0399 influence module to use that allows for global task-agnostic interest information to be queried.\nOnce an interest field is calculated across the observation space, it is still not trivial to shape agent behavior using such a field for a few reasons. The interest field usually has nothing to do with the task and may provide conflicting goals for the agent to perform. The interest field may often be highly non-stationary, so using models to estimate a \"value\" for the interest field in an RL sense may be challenging, as they would be chasing a moving target. Balancing these goals of providing information to the external model while still learning to complete the task is an essential part of our method. The goal of any of our POI influence algorithms is to maximize the interest our agent \"collects\" as it learns to complete the task at hand.\nRecalling the earlier example of an autonomous underground mining robot modeling safety for humans, our approach would use reward-agnostic behavior shaping to guide the agent towards areas that provide informative samples to the safety model after environment changes. Upon reaching a deeper level, where the temperature distribution changes and temperature concerns become more prevalent, a framework like the EMMA would enhance the human safety model by being able to define areas of unexplored temperature readings as interesting and then influence the robot to execute its task near those high-interest locations.\n3.2 Exemplar Interest Field: Monte Carlo Dropout Disagreement\nThe primary goal of our agent motivation strategy is to enhance the information gain of the external model $\\mathcal{M}$ throughout policy episodes. Fundamentally, we can maximize the information gained by the external model by minimizing the uncertainty in the predictions (MacKay, 1992). As such, areas of high model uncertainty are areas that the agent should prioritize visiting.\nVarious methods exist for quantifying model uncertainty (Abdar et al., 2021). For our experiments, we employ Monte Carlo dropout (Gal & Ghahramani, 2016), a technique suited for parameterized external models. This method involves estimating uncertainty by computing the variance of model predictions using multiple iterations of dropout. Formally, the interest field $f_{\\text{POI}}: \\mathcal{O} \\rightarrow \\mathbb{R}$ for an observation $o$ is defined as:\n$f_{\\text{POI}}(o) = \\text{Var}[\\mathcal{M}(o, d^{(1)}), \\mathcal{M}(o, d^{(2)}), ..., \\mathcal{M}(o, d^{(N)})]$\nwhere $d^{(i)}$ represents the $i$-th dropout sample and $N$ denotes the number of dropout samples used. This function can be computed for any observation $o$ without the agent currently observing $o$ or having access to any ground truth information about the underlying state, as required. If the model has multiple outputs, then the variance across each output should be computed and then the variances should be summed or averaged.\nIt is worth noting that while Monte Carlo dropout serves as our uncertainty quantification method, alternative uncertainty metrics (Abdar et al., 2021) can be seamlessly incorporated into our method-ology without compromising its effectiveness.\n3.3 Exemplar POI Influence: Interest-Valued Discrete Skill Sampling\nConsider using a discrete skill-conditioned agent $\\pi_{\\theta}(a | o, z)$ along with a skill classifier $q_{\\phi}(z | o)$ that classifies any state to one of a finite set of skills. This agent can be directed towards paths with higher interest by biasing the skill sampling every episode. Using a VAE (Kingma & Welling, 2014) to sample observations (see Appendix D.2.2) these observations can then be passed to the skill classifier. The interest value is computed for each of these observations and assigned to the skill that was classified for that observation. Then a skill is sampled from the set based on the average interest per observation of the skills. We introduce $\\eta$ to weight a uniform prior along with the interest biased prior to improve task convergence across the skills. This algorithm for biased skill sampling per episode is outlined in Algorithm 1."}, {"title": "4 Experiments", "content": "Procedure. In our experiments, we train an RL agent using a chosen interest field and P\u039f\u0399 influence algorithm along with proximal policy optimization (PPO) as the backbone RL algorithm. After some predefined number of steps (unknown to the agent), an environment transfer is injected. The agent and the external model must both adapt to the new post-transfer environment as efficiently as possible. The external model $\\mathcal{M}$ and the policy are both trained on the rollout data at the end of a rollout. For the external model, we train 8 epochs per rollout; we found that increasing the number of epochs per rollout for the model greatly increased the sample efficiency of the method (see Appendix A for a sensitivity study of epochs per rollout).\nEnvironment. We use a task from NovGrid (Balloch et al., 2022) called DoorKeyChange, a grid world environment in which there exists one red key, one blue key, and a red locked door with the goal past the door. Pre-transfer, the red key opens the red door as expected; post-transfer, the blue key opens the red door, and the red key stops working.\nExternal Model Specification. For our experiments, we define the external model specification as correct key distance prediction, in which a neural network is trained to predict the Euclidean distance to the key that currently opens the door. If the key is not within the agents view, the model should predict 14 (a distance that ensures the key is outside the agent's view). This model is a good model to test our algorithms since the transfer specified impacts the outputs of this model and efficient adaptation of this model could potentially be used to aid agent adaptation."}, {"title": "5 Results", "content": "We ran experiments with the setup discussed in the Section 4 (using hyperparameters shown in Appendix E) using the correct key distance prediction external model specification, to test if our interest based methods are more sample efficient and performant for external model learning post-transfer while also solving the task. These methods aim to have lower adaptive efficiency values (more efficient) and lower adaptive performance value (more performant) compared to the PPO and online DIAYN baselines. For ease of comparison, we normalize these metrics by PPO performance.\nWe present results for the Monte Carlo dropout uncertainty interest fields along with the follow-ing POI influence algorithms: POI DIAYN and POI intrinsic reward with global POI embedding (included for comparison, see Appendix D.2.2 for details). We compare these methods with two POI-free baselines: Vanilla PPO and Online DIAYN with PPO. This experiment aims to show how these different POI influence algorithms perform with uncertainty-based interest compared to our baselines. Our POI DIAYN method converged to the task pretty consistently (more than 80% of the time), so we were able to use an $\\eta$ of 0.\nThe quantitative comparison of metrics in Figure 3 shows that the external model adapts most efficiently when using POI DIAYN, which can also be seen in the graphs in Figure 2. These results confirm our hypothesis that uncertainty-based interest paired with an interest biased skill sampling method yields better environment sampling for external model learning over both a pure task agent and a diversity-driven task agent."}, {"title": "6 Discussion and Conclusions", "content": "The experimental results described in Section 5 provide much insight into our interest-based meth-ods for learning external models via agent rollouts. As noted in Figure 3, interest-based methods yield performance gains in both sample efficiency and asymptotic performance post-transfer. The improvement in adaptive performance is especially apparent on the evaluations using the random agent rollouts; evaluations on the random agent are challenging for the baselines because the exter-nal model must generalize beyond just the on-policy greedy rollouts of the trained, RL task-only agent. By learning from preferred environment samples acquired by the interest-influenced agent, the external model likely receives the data it needs to generalize more efficiently over more of the observation space. This shows that the EMMA framework can be used to help external models influence agents for faster convergence. Furthermore, these results confirm that an interest field based on external model uncertainty estimates using Monte Carlo dropout disagreement is a good choice to motivate the agent, and that POI DIAYN can be used as a way to steer agent behavior towards high-interest areas.\nIn our work, we investigated whether using interest fields to motivate an agent with the purpose of learning an external model more effectively yields better external model adaptation when faced with environment transfer in terms of sample efficiency and asymptotic performance. While our experiments focused on key distance prediction, the EMMA framework can be extended to a variety of external models including but not limited to world models, policy models, and safety prediction models (see Appendix C). These models are widely used in reinforcement learning, particularly in environments where novelties can arise. Our results demonstrate the potential of interest-driven behavior shaping to improve the adaptation of external models and open up avenues for iteration on our methods through the behavior shaping framework.\nOur findings suggest that interest-driven methods can serve as a powerful tool for shaping agent behavior agnostic to rewards. In our use-case, we motivated the agent to aid in the learning of an external model, but our algorithms work with any definition of interest. This flexibility opens avenues for a wide range of investigations into the interactions between interest signals and agent behavior. For example, using recency-based interest may yield more patrolling behaviors to visit stale states. We aim for this work inspires future research to further explore the applications of this paradigm, leading to more versatile and adaptable agents in complex environments with transfers."}, {"title": "A Learning Repeats Sensitivity Study", "content": "This sensitivity study examines the effects of varying the number of epochs per rollout on the relative efficiency of different interest-driven methods and baselines.\nA.1 4 epochs per rollout\nIn addition to our results with 8 epochs per rollout, we also ran an experiment using the same setup described in Section 4 with only 4 epochs per rollout. As seen by Figure 4, the interest based methods outperform the baselines. However, interestingly our secondary method described in Appendix D.2.2 does better than POI DIAYN in this experiment. Nevertheless, POI DIAYN still outperforms the baselines in all of our metrics by a wide margin even with 4 epochs per rollout.\nA.2 1 epoch per rollout\nWe briefly present the results of an experiment with the same setup as in Section 4 except now only using 1 epoch per rollout of data. With only one epoch of training per rollout in the environment, it is likely that the model did not fully learn everything it could from that rollout of data. Thus, our methods will likely not work as well.\nAs seen in Figure 5, the performance gains of our methods are lost when only using a single epoch per rollout. This result aligns with the expectation as each rollout is not nearly fully learned, so actively seeking specific rollouts will not help.\nA.3 Analysis\nThe discrepency of the results using different values for epochs per rollout is unsurprising as the enhanced environment sampling provided by our algorithms can be better used when more of the information from each sample is captured. If a sample is not fully learned before it is thrown out, then the next sample doesn't necessarily need to contain new information in it to optimally aid in external model training as even giving the same sample again would result information gain to the external model."}, {"title": "B Impact on Task Performance", "content": "While the main motivation for our work is to improve the external model performance while the agent is learning a task, it is important to note on the changes to task performance when applying external model motivated agents.\nFigure 6 shows the reward learning curves for each of the methods in each experiment post-transfer. As seen by the graphs, our methods do not impact task learning too much as in all of the experiments included within the interquartile mean, the agent converged on the task. Our experiments ran with around at least an 80% convergence rate with POI DIAYN and at least a 70% with POI instrinsic reward w/ a global POI embedding. Additionally, the graphs show that the efficiency of the agent is impacted to some extent by using a diverse skill based method like DIAYN, but this is more due to the diverse nature of online DIAYN rather than our contributions.\nThese results are also quite noisy because if the interquartile seeds end up including one poorly per-forming run (as we see for POI DIAYN in Figure 6a and DIAYN in Figure 6b), the IQM significantly gets decreased. Some loss of performance is expected as the agent is now attempting to optimizing two potentially competing objectives of solving the task while also learning the external model well, and these results show that the loss of performance on the task is not too much."}, {"title": "C Additional External Model Specifications", "content": "We provide code to run experiments using the correct key distance specification or any of the external models listed in this section (see Appendix F) on a variety of tasks. Further, these additional external model specifications listed below are not unique to MiniGrid (unlike the one used in our main experiments), or even discrete action spaces, so they can be run with Mujoco or any other environment.\nPolicy Model. Setting the external model in our algorithms to the policy model effectively con-verts all of our methods into traditional exploration methods. To calculate uncertainty of the policy model, we must add dropout to the policy and use the different dropout samples as an ensemble the same way our other external models were used. We used the same dropout hyperparameters used for correct key distance prediction. Additionally, to calculate the disagreement of a skil condi-tioned policy, we need a skill in addition to the observation. To solve this problem, we use the skill classifier and condition the policy with the skill that is most probable if the agent was in the given observation.\nWe present some preliminary results in 7 using this interest-based exploration method on the door key change task with higher learning repeats (16) than used on the experiments shown previously in this work (4) (see Appendix E for a full list of hyperparameters).\nWith this specification, task performance becomes the single objective and our interest methods are pure exploration methods, thus we evaluate our methods using task rewards. Our POI DIAYN method does not show significant improvement over the two baselines provided in this work. It should also be noted that improvement over vanilla PPO is less of an accomplishment in the context of exploration since there already exists plenty of methods that outperform vanilla PPO in terms of exploration (Seo et al., 2021; Yuan et al., 2022b; Yu et al., 2020; Yuan et al., 2022a; Ladosz et al., 2022). While this interest field didn't show improvement with POI DIAYN, another potential interest field could be the entropy of the policies output given an observation since this is another indicator for uncertainty. This removes the need for using a dropout ensemble for the policy. Additionally, further tuning for using policy models as external models likely needs to be done to use our methods in this way.\nForward Dynamics Model. Another possible external model specification is a forward dynamics model that takes the current observation and an action and then predicts the next observation. It should be noted that our algorithms presume the model takes only an observation, but this can be remedied by producing an action that makes the most sense to use when calculating interest. For our skill based method, we pass the observation through the policy to produce the action as that is what the agent will do at that observation. For the intrinsic reward based methods, we use the action that the agent actually took for the reward.\nLearning a forward dynamics model efficiently is something prior work in model-based reinforcement learning has attempted (Pathak et al., 2019; Yao et al., 2021); however, this is still an active field of research. Further, these methods generally do not specifically aim for solely learning good dynamics models as a separate objective and instead aim for optimal policy performance, differentiating them from our methods."}, {"title": "D Additional Methods", "content": "Here we present some additional methods that overall performed worse that POI DIAYN, but may still be more applicable for other use cases. Further, these methods likely can be developed further to perform well on tasks that are more tightly specified.\nD.1 Interest Fields\nD.1.1 Model Parameter Gradient Fields\nAnother proxy that can be used for information gain is the amount the model changes from seeing an observation. Consider basic gradient descent on a single observation $o$ paired with its ground truth $y$ for an external model $M_\\theta : \\mathcal{O} \\rightarrow \\mathbb{R}^d$ parameterized by $\\theta \\in \\mathbb{R}^p$. The general update rule looks like (with learning rate $\\alpha$),\n$\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta L(y, M_\\theta(o))$\nSo, the model parameter change is $\\alpha||\\nabla_\\theta L(y, M_\\theta(o))||$. For the following derivation, we use $z$ to denote the vector gradient (of shape length of $\\mathcal{F} \\times 1$) and $J_\\theta$ to denote the Jacobian matrix of vector valued function (of shape length of $f \\times$ length of $\\theta$). By chain rule,\n$\\nabla_o L(y, M_\\theta(o)) = J_\\theta[M_\\theta(o)]^T \\nabla_{M_\\theta(o)} L(y, M_\\theta(o))$\nNow, by the Cauchy-Schwartz inequality (Wu & Wu, 2009),\n$||\\nabla_o L(y, M_\\theta(o))|| \\le ||J_\\theta[M_\\theta(o)]^T||_F ||\\nabla_{M_\\theta(o)} L(y, M_\\theta(o))|| = ||J_\\theta[M_\\theta(o)]||_F ||\\nabla_{M_\\theta(o)} L(y, M_\\theta(o))||$\nwhere $||A||_F$ denotes the Frobenius norm of $A$ and $||z||$ is the euclidean norm of $z$. Thus, the upper bound of the model change is proportional to $||J_\\theta[M_\\theta(o)]||_F$, which only depends on $\\theta$ and $o$, so we can set this to the interest function without breaking the constraints of being able to calculate it while the agent does not have access to $y$.\n$f_{\\text{POI}}(o) = ||J_\\theta[M_\\theta(o)]||_F$\nWhile potential model change seems like a decent indicator for interest, it can be quite slow to calculate as it requires a gradient (backward) calculate over the network for each calculation of POI. Thus, it should not be used along with a POI influence algorithm that makes frequent calls to the POI function. For this reason, the focus of our experiments are on other interest fields.\nD.1.2 Transfer Aware Fields\nFor certain tasks, there may be some priors or assumptions that can be made about the novelties that may occur in the environment. Consider knowing that each step there is a probability $p$ that a transfer occurs in a specific position in the environment. Then, it may be useful to use a \"staleness\" interest field that incentivises the agent to feed the external model the recent data for the full position space.\nDue to the generic framework and separation of the interest field and the POI influence algorithm that uses this field, task specific interest fields can be designed and easily integrated with our exploration methods.\nD.2 Behavior Shaping via Interest Fields\nD.2.1 Intrinsic Rewards\nOne simple method for POI influence via these points of interest is to use the interest value as an intrinsic reward signal. There is a plethora of prior work on designing intrinsic rewards that optimize exploration for policy learning (Burda et al., 2018; Yuan et al., 2022b; Yu et al., 2020; Yuan et al., 2022a; Seo et al., 2021); however, we directly use interest values as these values motivate the agent to optimize for external model learning. That is, we reformulate the reward signal to be:\n$r = r^{(i)} + r^{(e)}$\nwhere $r^{(e)}$ is the task reward provided by the environment and $r^{(i)} = f_{\\text{POI}}(o)$ where $o$ is the observation that the agent is observing while receiving this reward.\nThis method is relatively straightforward in terms of both implementation and theory, but it has a few problems. One potential drawback of the fact that this intrinsic reward may be highly non-stationary making it non-trivial (Cheung et al., 2020) for the RL algorithm to learn to optimize for this POI without any sense of global POI information across the observation space. Another drawback of this method is that it requires a call to the POI function every step, which can be computationally costly.\nD.2.2 Intrinsic Rewards w/ an interest embedding\nOne of the main drawbacks to just using the POI signal as an intrinsic reward is that the agent will likely learn too slow to adapt to changes in the POI signal, which can be extremely dynamic depending on the definition of interest. Thus, it may be useful for the policy $\\pi: \\mathcal{O} \\rightarrow \\mathcal{A}$, where $\\mathcal{A}$ is the action space of the agent, to be conditioned on some sort of global POI information. However, for continuous or large observations spaces, it is not possible to just enumerate the possible observations and their respective interest values. Thus, we attempt to construct a vector $e_{\\text{POI}}$ that contains some sort of semi-global interest information while maintaining the same intrinsic reward from above.\nObservation Space Sampling. First, as the observation space may be large, we need a way to sample observations that can be directly passed into the interest field to produce POI values. In our experiments, we used a variational autoencoder (VAE) (Kingma & Welling, 2014), with encoder $E_{\\phi} : \\mathcal{O} \\rightarrow \\mathbb{R}^l$ and decoder $D_{\\phi} : \\mathbb{R}^l \\rightarrow \\mathcal{O}$ where $l$ is the size of the latent space. After each rollout of the policy, this VAE is trained using the variation ELBO loss. Although we use VAEs, any observation sampler can work, so if one is provided with the environment it should be used. Further, sampling states from a replay buffer is also a valid option.\nEmbedding Learning. Now, we attempt to create an embedding $e_{\\text{POI}}$ that contains the informa-tion about the global state of interest across the observation space. Logically, if a model can \"query\" this vector with an observation and produce the POI of that observation, then the information is contained in that vector (or only depends on the obserservation and wont change over time)."}, {"title": "E Experiment Hyperparameters", "content": "In this appendix we provide the set of hyperparameters used to generate our results. Any parameters not provided here were ommited for brevity, but can be found in the configs folder within the code implementation (see Appendix F). We held any parameters that were agnostic to our methods constant between the different methods to allow for fair comparisons."}, {"title": "F Code Implementation", "content": "We additionally provide the code used to generate the results in our work in this repository. Our implementation mainly uses the PyTorch, Stable Baselines 3 (Raffin et al., 2021), Gymnasium, and ExperimentLab libraries to run our experiments."}]}