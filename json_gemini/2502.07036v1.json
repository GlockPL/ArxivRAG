{"title": "Automated Consistency Analysis of LLMs", "authors": ["Aditya Patwardhan", "Vivek Vaidya", "Ashish Kundu"], "abstract": "Generative AI (Gen AI) with large language models (LLMs) are being widely adopted across the industry, academia and government. Cybersecurity is one of the key sectors where LLMs can be and/or are already being used. There are a number of problems that inhibit the adoption of trustworthy Gen AI and LLMs in cybersecurity and such other critical areas. One of the key challenge to the trustworthiness and reliability of LLMs is: how consistent an LLM is in its responses?\nIn this paper, we have analyzed and developed a formal definition of consistency of responses of LLMs. We have formally defined what is consistency of responses and then develop a framework for consistency evaluation. The paper proposes two approaches to validate consistency: self-validation, and validation across multiple LLMs. We have carried out extensive experiments for several LLMs such as GPT40Mini, GPT3.5, Gemini, Cohere, and Llama3, on a security benchmark consisting of several cybersecurity questions: informational and situational. Our experiments corroborate the fact that even though these LLMs are being considered and/or already being used for several cybersecurity tasks today, they are often inconsistent in their responses, and thus are untrustworthy and unreliable for cybersecurity.", "sections": [{"title": "I. INTRODUCTION", "content": "Over the last couple of years, the rapid development of Generative AI, and wide adoption of Large Language Models (LLMs) have revolutionized the field of natural language processing, and automated generation of content in several languages, in multi modal manner such as text, audio, and video. These machine learning and AI models, related AI agents [28] are also being widely used for automation of tasks across the industry and several sectors. One such important sector is cybersecurity. LLMs have already been used to assist with cybersecurity processes (SecOps) [4], in security operation centers (SoC) [24], in security analysis of code and configurations [31], [18], in generation of secure code [32], [25], in security and penetration testing [27] among many others.\nTrustworthiness of such LLMs and generative AI is a critical factor in whether and how we use it in cybersecurity as well as other real-world applications and systems. LLMs suffer from security issues [34], [22], and there are risks of code generation using LLMs [32]. LLMs also hallucinate [35] and such hallucinations often increase the risk of reduction of utility of the models, or even risk of failure during the task execution with LLMs as judges [39], or autonomous LLM agents [28]. Consistency has a strong relation to hallucination.\nIn order to be able to trust the large language models, one key factor is consistency. Studies of LLM consistency lead to the following questions: (1) are they reliably consistent, in that can we rely on the responses are they semantically consistent? And that leads to the following questions further: (2) What is consistency, and (3) how can consistency be evaluated in an automated manner. In this paper, we study these research questions and attempt to address them. Our focus is on semantic consistency of responses by one LLM or a set of LLMs. There are other types of consistency requirements such as: syntactic consistency, structural consistency, which are often in the context of copyrights, and are out of the scope of this paper.\nOur Contributions; We have studied the problem of consistency of responses by LLMs especially in the context of cybersecurity. That is because, in such a domain, consistency of responses is quite important for making correct and effective decisions, otherwise inconsistency may lead to security vulnerabilities, weaknesses and/or liabilities and harm to systems, users and to enterprises. We have formally defined what is consistency of responses and then develop a framework for consistency evaluation. The paper proposes two approaches to validate consistency: self-validation, and validation across multiple LLMs. We have carried out extensive experiments for several LLMs such as ChatGPT 40 Mini, GPT3.5, Gemini, Cohere, and Llama3, on a security benchmark consisting of both informational and situational cybersecurity questions. Our experiments corroborate the fact that even though these LLMs are being considered and/or already being used for several cybersecurity tasks today, they are often inconsistent in their responses. Thus unless the consistency of LLMs in their responses is improved to a reliable level, they cannot be trusted to an extent that they can be used in enterprise-level cybersecurity operations.\nOrganization of the paper: Section II defines consistency of LLM responses formally and analyzes its relationship with accuracy and hallucination. Section III describes the frame-work of self-validation and cross-validation of consistency and the algorithms. In Section IV we have presented the experimental benchmark, settings and analyzed the experimental results. Section V discusses the related work, and Section VI concludes the paper with future work."}, {"title": "II. CONSISTENCY IN THE CONTEXT OF LLMS", "content": "One key question arises on the use of LLMs is can we trust an LLM's responses? Reliability of an LLM is dependent on its consistency.\nConsistency of LLMs is defined based on how consistent an LLM's responses to different prompts are. It is about whether the responses that an LLM returns to the same or semantically identical prompts are sufficiently similar or identical to each other. Such responses may be syntactically or structurally different but semantically identical. If a set of responses are not sufficiently similar or identical to each other then we refer to it as \"inconsistent responses\".\nThe prompts are processed by the LLM over a duration of time \u0394t, during which the LLM model is assumed to remain stable and unchanged. In this paper, by the clause - \"responses of an LLM to a prompt\" means \"responses of an LLM to the same prompt or semantically identical prompts issued multiple times\". Each issuance of a prompt to an LLM is called as a query."}, {"title": "B. Consistency and other Properties", "content": "Some of the key implications of consistency and inconsistency in the context of LLMs are as follows:\n\u2022 Accuracy A \u21d2 Consistency C: if an LLM is stated to be accurate in its responses to a prompt, then such responses will be consistent with respect to each other. In turn, lack of consistency implies lack of accuracy: \u00acC \u21d2 \u00acA.\n\u2022 Consistency C\u21d2 Accuracy A: Semantic consistency of responses does not always imply accuracy of the responses, primarily because an LLM may respond with the semantically equivalent responses for the same prompt or semantically equivalent prompts each time it is queried, but guaranteeing the semantic accuracy of the responses is beyond the problem of maintaining consistency.\n\u2022 Consistency of LLMs are related to the hallucination of LLMs [17]: The more inconsistent an LLM is in its responses, the more the LLM may hallucinate simply because the LLM is responding with semantically different responses to the same prompt over multiple queries. Section IV-C3 presents some related findings on hallucination. However, a detailed discussion and analysis of such a relationship is beyond the scope of this paper."}, {"title": "C. Formal Definition", "content": "Let Li refer to a large language model (LLM). Let pk refer to a prompt in one or more languages that is/are supported by an Li. Let sj represent an active user session of a u'th user entity uw. Let ti represent the point in time at which a query is made to an LLM or the time at which an LLM responds to a query such that the response is complete (not in the process of incremental output). Let rv \u2190 qv(Li, sj, pk, ti, uw) refer to a unique query the user u using prompt pj to Li leading to response rv. The terms \"Prompt\" and \"Query\" are used interchangeably throughout this paper.\nLet Li \u2208 L, which is a set of LLMs, where Li \u2208 L, refer to a large language model (LLM). Let R\u2082 \u2190 Qv(Li, sj, pk,t\u0131, uw) refer to a unique set of queries the user u using prompt pj to a subset of LLMs Li \u2286 L leading to a set of responses R that are received by the set of users Uw, over a set of active sessions Sj, where there is an one-to-one mapping of all the elements across the sets of responses, queries, LLMs, sessions, prompts, timestamps of completion of queries, and users.\nPrompts px and py are semantically equivalent, i.e. they are identical, sufficiently identical or sufficiently similar to each other semantically, which is represented by px \u2261 py. Responses rv and rw are semantically equivalent, i.e. they are identical, sufficiently identical, or sufficiently similar to each other semantically, which is represented by rv \u2261 rw.\nDefinition 1: Consistency of Responses \u2013 One LLM. Given two queries r\u2081 \u2190 q1(L1,s1, px, t1, u1) and r2 \u2190 q2(L1, s1, py, t3, u1), where px \u2261 py, the responses are consistent if r\u2081 \u2261 r2.\nDefinition 2: Consistency of Responses \u2013 Multiple LLMs. Given two queries R1 \u2190 Q1(L1, S1, Px, T1, 41) and R2 \u2190 Q2(L1, S1, Py, T3, 41), where px \u2261 py, the responses are consistent if for all pairs < xv, ev >, where dv \u2208 R1 and ev \u2208 R2, xv = \u03c8\u03c5\u00b7\nIn the rest of the paper, on the basis of this formal model, we will study and carry out experiments on black-box LLMs."}, {"title": "III. CONSISTENCY VALIDATION FRAMEWORK", "content": "In this paper, we propose a comprehensive evaluation framework that incorporates multiple algorithms for evaluating consistency and accuracy, providing a holistic metric of how trustworthy an LLM is. In this paper, we measure LLM consistency in the context of cybersecurity applications.\nThe rest of the paper follows the formal model and definitions. However, the notations used may differ in order to provide more readability for the algorithms and discussion."}, {"title": "A. Consistency", "content": "To be trustworthy, an LLM has to return a similar answer every time it's prompted with the same question, so different users don't get different answers or explanations to answers when researching the same topic. Our consistency algorithm gives an LLM the same prompt n times and evaluates the similarity between responses using multiple metrics such as Jaccard Index [7], Cosine Similarity [26], Sequence Matcher [21], and Levenshtein distance [13], all standardized to a scale of 0 to 100.\nThe Consistency algorithm (Algorithm 1) operates in three modes: low, medium, and high, where higher settings require progressively greater consistency in the metrics for the model to be considered consistent. For each question, the algorithm"}, {"title": "B. Agreement", "content": "To be trustworthy an LLM has to return the correct answer to a question. To determine if the LLMs agree on whether a certain answer is correct or not, our framework uses two algorithms. The first is Self-Validation, where an LLM checks it's own answer to a question. The second is Cross-Validation, where an LLM's answer to a question is checked by every other LLM. An LLM must be considered accurate by both these algorithms to be considered trustworthy in terms of information accuracy.\n1) Self-Validation: Figure 1 illustrates the Self-Validation framework for Large Language Models (LLMs). In this process, the LLM generates a \"Response List\" with repeated responses to the same question. That same LLM is then asked whether the generated responses are the correct answer to the original query. If it agrees with enough of its own responses, it is considered factually consistent by self-validation.\nThe Self-Validation Algorithm (Algorithm 2) has an LLM to evaluate the accuracy of its answers as shown in 1. It prompts the LLM with its answer to a question and asks if it is the correct answer to that question. This is done k times for every question. If the LLM responds \"yes\" 80% of the time, that question is considered correct by this metric. If the number of correct questions divided by the total number of questions is greater than qthreshold, the LLM is considered accurate overall by this metric.\n2) Cross-Validation: 2 illustrates the cross-validation framework for evaluating the consistency of a Large Language Model (LLM). This framework fact checks LLM responses with other LLMs. Each LLM generates a response to a prompt,"}, {"title": "IV. EMPIRICAL ANALYSIS ON LLMS USING THIS FRAMEWORK", "content": "To test the LLMs, we developed a benchmark on 40 Cybersecurity interview questions from a popular list of cybersecurity interview questions and answers [6]. In the benchmark, the questions are divided into two types as follows.\n1) Information Questions: The first 33 questions are basic information questions, with a well-known correct answer. For example, \"what is cryptography\", or \"what is the CIA triad in"}, {"title": "B. Consistency", "content": "Gemini [29] and Bloom [33] are both deterministic models, and return the exact same response when the same prompt is passed. Therefore, they are perfectly consistent for all thresholds. Since their behavior is deterministic, the consistency check does not provide any useful information and they aren't included in the plots.\nFigures 3-5 depict the results for the low, medium, and high thresholds respectively for the 33 information questions. Figures 6-8 provide the corresponding results for the 7 situational questions. In each figure, there are 4 sets of bars corresponding to each LLM, providing the results of the consistency analysis for the cases where 1 or more, 2 or more, 3 or more, or all 4 of the consistency metrics evaluate to true.\nLooking at the results for Low threshold with information questions (Figure 3), the majority of LLMs seem to pass when 1-3 similarity metrics are sufficient to pass for a response pair to be considered consistent. When all 4 similarity metrics are needed, the percentage of questions that pass drops drastically, and not a single model is above 80%. Comparing this with the situational questions (Figure 6), all the results are noticeably lower, as expected. As opposed to all LLMs passing at least once under the Low threshold, none pass under the High threshold. With the information questions (Figure 5), GPT 40 mini performs the best once again, with GPT 3.5 not far behind. 3.5 and 40 are the only ones to have any questions pass when questions have to pass 3/4 metrics, and 40 mini is the only one to have any questions pass when all 4 metrics are used. The difference between the situation questions (Figure 5) and the information questions is also greatest under the High threshold. Interestingly, GPT 3.5 outperforms 40 mini here, and it and Gemini are the only models to have any questions pass when only 1/4 of metrics are used.\nThe Medium threshold has the most interesting results. With the information questions (Figure 4), GPT 40 Mini and GPT 3.5 are the only models to pass when 2/4 metrics are used. With 1 metric, Gemini passes as well. With the situation questions (Figure 7) GPT 40 Mini was the only model to pass when only 1/4 metrics are used.\nNot counting Bloom and Meta OPT [37] since they're inherently 100% consistent, the most consistent models are GPT 40 Mini, GPT 3.5, and Google Gemini, in that order. When putting the average metric scores into tables, in both the regular questions (Table I) and the situation questions (Table I) GPT 40 Mini consistently scores higher in Sequence Matcher and Levenshtein Distance, while 3.5 consistently scores higher on Jaccard Index and Cosine similarity. Sequence and Levenshtein take order into account, so it can be inferred that 40 mini has more variation in the way it words its responses as compared to the older 3.5. Along with that, there is very little variation in the Cosine Similarity and to a lesser degree Jaccard Index scores. This increases a bit on the situation questions but is still considerably lower than the other two. This shows that these two metrics are less useful for comparing LLMs to each other."}, {"title": "C. Agreement", "content": "For agreement, Meta OPT [38]'s responses had to be left out, because it refuses to give a yes or no answer on whether a response is correct or not, usually responding with \"I think it's a good question\", or just answering the question again, no matter what prompt we used. This means it cannot be used for Self-Validation, and cannot Cross-Validate the other models. So it isn't included in the Self-Validation plots, and on the Cross-Validation plots every other model only has 5 other LLMs to check its answer instead of 6, Meta OPT is not included.\n1) Self-Validation: For Self-Validation, whether it's informational (Figure 9) or situational questions (Figure 10), most of the models agree with themselves more than 80% of the time. Bloom is the only model that is critical of itself. Interestingly, it appears to agree with itself more on the situational questions than the informational questions.\n2) Cross-Validation: For Cross-Validation, the plots are organized by the number of LLMs that agree with one LLM's response. The first bar depicts the case when 2 of the 5 LLMS agree with a response, and the last bar depicts the case when all 6 agree. The last bar is only applicable for Meta OPT since Meta OPT itself cannot be used to cross-validate the other models, they all have 5 models checking their responses, while Meta OPT has all 6.\nInterestingly, despite being the only 100% consistent models, Meta OPT and Bloom score the lowest in both the informational questions (Figure 11) and the situational questions (Figure 12). They actually score higher on the situational questions than the informational ones. Along with Gemini, they're the only models to behave this way.\n3) Model Hallucination and Validation: When asked to \"Explain CIA triad in Cybersecurity\u201d, Meta OPT provided an incorrect answer, talking about the Central Intelligence Agency instead of the CIA triad. In the Cross-Validation step, 4 of the 6 LLMs responded that the answer was incorrect, which marks the overall question as incorrect by our parameters. While that showcases the strengths of Cross-Validation with other LLMs, two LLMs still got the validation wrong, those being GPT 3.5 and Cohere. More interestingly, Cohere answered \"Yes, you explained the CIA triad, which stands for confidentiality, integrity, and availability, in cybersecurity\", generating the correct answer itself but wrongfully believing it was provided in the prompt. Future work on refining the effectiveness of the Cross-Validation algorithm could be on marking the LLMs which are most accurate when validating answers and removing those that are the least.\nDetecting a hallucination is more difficult with situational questions. When prompted with \"A friend of yours sends an e-card to your mail. You have to click on the attachment to get the card. What do you do? Justify your answer\", Meta OPT simply responded with \"I have to click on the attachment to get the card\", where the correct answer would be to confirm that it is not malicious first. In the cross-validation step, only GPT 40 Mini identified this as the incorrect answer. Despite Gemini, GPT 3.5, Cohere and Llama3 all giving the correct answer when directly responding to the question, they still believed the incorrect answer was correct despite correctly identifying the security risks in their own responses. This shows the weakness in using LLM agreement to double-check responses for more abstract questions, as an LLM's response to a situation can be a possible course of action, but not necessarily a correct one.\""}, {"title": "V. RELATED WORK", "content": "Our paper focuses on how to analyze the consistency of responses by an LLM. Hamman et al [5] defines the measure of consistency for models predicting tabular data. Our work relies on general large language models that are not specifically fine-tuned for tabular data.\nConsistency analysis is an important aspect of hallucination analysis and detection. There is an entire body of work on hallucination [14]. There are other works related to self-reflection [9], distributed LLMs [36], and mixture-of-experts [3]. [8] performed experiments to examine the consistency of ChatGPT's [19], [30] responses, uncovering situations where its language comprehension abilities don't always lead to logically sound predictions.\nLLMs suffer from security issues [34], [22], and there are risks of code generation using LLMs [32]. LLMs hallucinate [35] and such hallucinations often increase the risk of reduction of utility of the models, or even risk of failure during task execution, with LLMs as judges [39], or autonomous LLM agents [28].\nLLMs can be used in enabling cybersecurity processes (Sec-Ops) [4]. Aljanabi et al. [1] explored ChatGPT's relevance in the cybersecurity domain, demonstrating its utility for security professionals in identifying potential vulnerabilities. Khoury et al. [11] conducted experiments to assess the safety of programs generated by ChatGPT and sought security assessments. Peng et al. [20] introduced a new model that enhances black-box LLMs with plug-and-play modules, outlining an iterative strategy to improve model feedback. LLMs are also being proposed to be used in security operation centers (SoC) [24]. Secure code copilots and secure generation of code is an important area of investigation, in security analysis of code and configurations [31], [18], in generation of secure code [32], [25], in security and penetration testing [27].\nExisting studies have focused on the broader implications and uses of ChatGPT [16], exploring the transformative potential of ChatGPT in academia and libraries, and discussing its benefits for search, discovery, cataloging, and information services. Meanwhile, discussions on the impact on higher education [23] highlight concerns about its ability to generate text that resembles human-authored content. [2] examined the pros and cons of incorporating ChatGPT into teaching and learning, adding to discussions about educational technology, while [10] investigates how well ChatGPT can translate across multiple languages. [12] presents an innovative technique for validating and querying LLMs using standard regular expressions. Liu et al. [15] conducted a thorough study that included trend analysis, word cloud representations, and domain-wise distribution analysis. This investigation shed light on the model's capabilities, ethical considerations, and future directions."}, {"title": "VI. CONCLUSIONS AND FUTURE WORK", "content": "In this paper, we ask the following question \u2013 \"how consistent are LLM responses, both in the information provided and factually\" especially in the context of their use in cybersecurity. LLMs are expected to be used for various security operations in the industry [4]. Before we put significant reliance on the black-box LLMs (which the industry has already started), can we evaluate such models on how consistent they are in their responses, and can security stakeholders such as CISOS make decisions on whether and how to use which LLM for tasks as important as cybersecurity operations?\nWe have carried out an extensive set of experiments and analyzed the consistency of LLMs for their responses against a benchmark of cybersecurity questions. Our experiments demonstrate that LLMs have made significant strides in improving consistency and reducing hallucination in the past couple of years, as newer models like GPT 40 Mini and Meta Llama3 outperform older ones like Meta OPT and Bloom. Despite that, LLMs still have quite a way to go before they become usable for important cybersecurity operations. When confronted with more abstract situational questions, there is a clear drop in consistency and agreement between LLMs. While our self and cross-validation algorithms have been effective at detecting LLM hallucination, they become less reliable the more abstract a question is. In the future, we plan to conduct a more supervised analysis of how accurate LLMs are to specifically select the ones with the best track record for response validation.\nWe plan to explore the relations between consistency and hallucination in detail, as well as carry out further experiments on classifying LLMs to different cybersecurity tasks. Further understanding of how inconsistent are the responses and how they are generated based on an analysis of the internal states of the models and attention layers may help us fine-tune the models better."}]}