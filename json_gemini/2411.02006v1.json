{"title": "Foundations and Recent Trends in Multimodal Mobile Agents: A Survey", "authors": ["Biao Wu", "Yanda Li", "Meng Fang", "Zirui Song", "Zhiwei Zhang", "Yunchao Wei", "Ling Chen"], "abstract": "Mobile agents are essential for automating tasks in complex and dynamic mobile environments. As foundation models evolve, the demands for agents that can adapt in real time and process multimodal data have grown. This survey provides a comprehensive review of mobile agent technologies, focusing on recent advancements that enhance real-time adaptability and multimodal interaction. Recent evaluation benchmarks have been developed to better capture the static and interactive environments of mobile tasks, offering more accurate assessments of agents' performance. We then categorize these advancements into two main approaches: prompt-based methods, which utilize large language models (LLMs) for instruction-based task execution, and training-based methods, which fine-tune multimodal models for mobile-specific applications. Additionally, we explore complementary technologies that augment agent performance. By discussing key challenges and outlining future research directions, this survey offers valuable insights for advancing mobile agent technologies. A comprehensive resource list is available at https://github.com/aialt/awesome-mobile-agents", "sections": [{"title": "1 Introduction", "content": "Mobile agents have achieved notable success in handling complex mobile environments, enabling the automation of task execution across various applications with minimal human intervention (Zhang et al., 2023a; Li et al., 2024; Bai et al., 2024). These agents are designed to perceive, plan, and execute in dynamic environments, making them highly suitable for mobile platforms that demand real-time adaptability. Over the years, research on mobile agents has evolved significantly, advancing from simple rule-based systems to more sophisticated models capable of handling complex tasks in multimodal and dynamic setting (Shi et al., 2017; Rawles et al., 2023).\nIn their initial stages, mobile agents were predominantly focused on executing predefined workflows through lightweight, rule-based systems tailored for specific tasks on mobile devices. These early agents were often limited by the computational and memory constraints of the hardware, relying heavily on basic interaction patterns and static processes. However, the rapid advancement of mobile technologies has paved the way for more advanced agent architectures, enabling richer task execution capabilities.\nEvaluating mobile agents presents unique challenges, as traditional static evaluation methods often fail to capture the dynamic and interactive nature of real-world mobile tasks. To address this, recent benchmarks such as AndroidEnv (Toyama et al., 2021) and Mobile-Env (Zhang et al., 2023a) offer interactive environments that assess agents' adaptability and performance under realistic conditions. These benchmarks not only measure task completion but also evaluate how well agents respond to changing mobile environments, thus providing a more comprehensive assessment of their capabilities.\nRecent advancements in mobile agent research can be categorized into two approaches: prompt-based methods and training-based methods. Prompt-based methods leverage large language models (LLMs), such as ChatGPT (OpenAI, 2023) and GPT-4 (OpenAI, 2023), to handle complex tasks by using instruction prompting and chain-of-thought (CoT) reasoning. Notable works such as OmniAct (Kapoor et al., 2024) and AppAgent (Yang et al., 2023) have demonstrated the potential of prompt-based systems in interactive mobile environments, although scalability and robustness remain ongoing challenges. On the other hand, training-based methods focus on fine-tuning multimodal models, such as LLaVA (Liu et al.,"}, {"title": "2 Benchmarks for Mobile Agents", "content": "Benchmarks establish a standardized testing environment for evaluating and comparing the performance of mobile agents across both static and interactive settings, covering areas such as user interface automation, task completion, and real-world application scenarios.\nCurrently, many benchmarks for GUI interaction rely on static datasets (Sun et al., 2022; Deng et al., 2024; Niu et al., 2024; Ro\u00dfner et al., 2020), which provide fixed ground-truth annotations and evaluate models by comparing their action sequences to predefined solutions. This method is problematic, as it penalizes alternative valid approaches, marking them as failures even if the task is successfully completed. Interactive benchmarks, such as AndroidArena (Xing et al., 2024), also use action sequence similarity as a primary evaluation metric, resulting in an inadequate assessment of agent performance. While recent studies on LLM-based GUI agents (Yang et al., 2023; Wang et al., 2024a; Zhang et al., 2024a) incorporate LLMs or human evaluations, these experiments are often conducted in uncontrolled open environments, leading to issues with reproducibility and comparability of results."}, {"title": "2.1 Static Datasets", "content": "Static datasets provide a controlled and predefined set of tasks with annotated ground-truth solutions, making them essential for evaluating the performance of mobile agents in fixed environments. These datasets are primarily used to assess task automation, where agents are required to follow predetermined actions or commands to complete specific tasks.\nEarly research link referring expressions to UI elements on a screen, with each instance containing a screen, a low-level command, and the corresponding UI element. For example, the RicoSCA dataset (Deka et al., 2017) uses synthetic commands, while MiniWoB++ (Liu et al., 2018) includes sequences of low-level commands for multi-step tasks.\nRecent research has shifted towards task-oriented instructions, where each episode contains action-observation pairs, including screenshots and tree-structured representations like Android's View Hierarchy or the Document Object Model in web environments. For instance, the PixelHelp (Li et al., 2020a) dataset contains 187 high-level task goals with step-by-step instructions from Pixel Phone Help pages, while the UGIF (Venkatesh et al., 2022) dataset extends similar queries to multiple languages. Meanwhile, MoTIF(Burns et al., 2021), includes 4.7k task demonstrations, with an average of 6.5 steps per task and 276 unique task instructions. AITW (Rawles et al., 2024b) is much larger, featuring 715,142 episodes and 30,378 unique prompts, some inspired by other datasets."}, {"title": "2.2 Interactive Environment", "content": "Interactive environments provide dynamic platforms where agents engage with the environment in real time, receiving feedback and adjusting their actions accordingly. Unlike static datasets, these environments allow for continuous, adaptive interactions, making them critical for evaluating agents in more complex, evolving scenarios.\nBefore the rise of LLM-based agents, research primarily focused on reinforcement learning (RL)-based agents. A prominent example is AndroidEnv (Toyama et al., 2021), which provided RL agents with an environment to interact with mobile applications via predefined actions and rewards. However, with advancements in LLMs, the focus has shifted towards agents that can use natural language understanding and generation to perform more flexible and adaptive tasks (Liu et al., 2024; Sun et al., 2024b,a).\nClosed Environments are a key focus in current research on LLM-based agents, particularly in their ability to explore decision paths autonomously through interactions with the environment (Liu et al., 2024; Sun et al., 2024b,a). In mobile settings, these agents are designed to handle complex, multi-step tasks and simulate human-like behaviors for app automation (Wen et al., 2023a,b; Liu et al., 2023c; Yao et al., 2022a; Shvo et al., 2021). A notable example is Mobile-Env (Zhang et al., 2023a), created to evaluate how well agents manage multi-step interactions in mobile environments. Ultimately, this research aims to improve the adaptability and flexibility of LLM-based agents, allowing them to function in dynamic, real-world environments with minimal reliance on predefined scripts or manual input.\nOpen-world Environments present a significant opportunity to address one of the main limitations of closed-reinforcement learning settings: their inability to fully capture the complexity and variability of real-world interactions. While controlled environments are useful for training and testing agents, they often miss the dynamic elements of real-world scenarios, where factors like changing content, unpredictable user behavior, and diverse device configurations are crucial. To overcome these challenges, researchers are increasingly exploring open, real-world environments for LLM-based GUI agents, enabling them to learn and adapt to the intricacies of live systems and evolving situations (Gao et al., 2023a; Wang et al., 2024b; Zhang et al., 2024a; Yang et al., 2023). However, deploying agents in open-world settings introduces several risks. These include safety concerns, irreproducible results, and the potential for unfair comparisons. To mitigate these issues and ensure fair, reproducible evaluations, researchers advocate for strategies such as fixing dynamic online content and employing replay mechanisms during evaluation (Liu et al., 2018; Shi et al., 2017; Zhou et al., 2023). These methods help create a more controlled testing environment, even within the broader scope of open-world deployments."}, {"title": "2.3 Evaluation Methods", "content": "In evaluating agent performance, trajectory evaluation, and outcome evaluation are two main methods. Trajectory evaluation focuses on how well agent actions align with predefined paths. In contrast, outcome evaluation emphasizes whether the agent achieves its final goals, focusing on results rather than the specific process. The following sections will explore recent research advancements in these two areas, highlighting how more comprehensive evaluation strategies can enhance our understanding of agent performance in complex environments.\nProcess Evaluation has significantly improved in recent GUI interaction benchmarks, with a focus on step-by-step assessments that compare predicted actions to reference action trajectories for evaluating agent performance effectiveness (Rawles et al., 2024b; Zhang et al., 2021). While this approach is effective in many cases, task completion often has multiple valid solutions, and agents might explore different paths that do not necessarily follow the predefined trajectories. To enhance the flexibility and robustness of these evaluations, a greater emphasis could be placed on the final outcomes rather than the process itself (Zhang et al., 2023a).\nOutcome Evaluation determines an agent's success by assessing whether it reaches the desired final state, viewing task goals as subsets of hidden states, regardless of the path taken to achieve them. These final states can be identified through various system signals. Relying on a single signal type may not capture all relevant state transitions, as certain actions, such as form submissions, may only be visible in the GUI and not in system logs (Toyama et al., 2021) or databases (Rawles et al., 2024a). Shifting to outcome-based evaluation and using multiple signals can make GUI interaction benchmarks more reliable and adaptable, allowing agents to show their full abilities in various scenarios (Wang et al., 2024c; Rawles et al., 2024a)."}, {"title": "3 The Components of Mobile Agents", "content": "As shown in Fig 1, This section outlines the four fundamental components of mobile agents: perception, planning, action, and memory. Together, these components enable agents to perceive, reason, and execute within dynamic mobile environments, adapting their behavior dynamically to improve task efficiency and robustness."}, {"title": "3.1 Perception", "content": "Perception is the process through which mobile agents gather and interpret multimodal information from their surroundings. In mobile agents, the perception component focuses on handling multimodal information from different environments, extracting relevant information to aid in planning and task execution.\nEarly research on mobile agents (Zhang et al., 2021; Sunkara et al., 2022; Song et al., 2023) primarily relied on simple models or tools to convert images or audio into text descriptions. However, these approaches often generate irrelevant and redundant information, hampering effective task planning and execution, especially in content-heavy interfaces. Additionally, the input length limitations of LLMs further amplified these challenges, making it difficult for agents to filter and prioritize information during task processing. Existing visual encoders, mostly pre-trained on general data, are not sensitive to interactive elements in mobile data. To address this, recent studies by Seeclick (Cheng et al., 2024) and CogAgent (Hong et al., 2024) have introduced mobile-specific datasets that enhance visual encoders' ability to detect and process key interactive elements, such as icons, within mobile environments.\nIn contexts where API calls are accessible, Mind2Web (Deng et al., 2024) introduces a method for processing HTML-based information. This method ranks key elements of HTML data and filters crucial details to improve LLM perception of interactive components (Li et al., 2024). Meanwhile, Octopus v2 (Chen and Li, 2024) leverages specialized functional tokens to streamline function calls, significantly enhancing on-device language model efficiency and reducing computational overhead."}, {"title": "3.2 Planning", "content": "Planning is central to mobile agents, enabling them to formulate action strategies based on task objectives and dynamic environments. Unlike agents in static settings, mobile agents must adapt to ever-changing inputs while processing multimodal information.\nPlanning in mobile agents can be done either programmatically or using natural language. Programmatic formats, like those in AiTW (Rawles et al., 2024b), are ideal for precise system execution. On the other hand, natural language formats, as seen in CoCo-Agent (Ma et al., 2024), bridge the gap between task instructions and the agent's existing conversational skills, making it easier for the agent to adapt and generalize to tasks in different domain.\nPlanning strategies can be categorized as dynamic or static. In dynamic planning, agents break down tasks into sub-goals but do not re-plan if errors occur (Zhang et al., 2024b). In contrast, static planning adjusts the plan based on real-time feedback, enabling agents to revert to earlier states and re-plan (Gao et al., 2023b; Wang et al., 2024a).\nRecent advances in prompt engineering have further enhanced mobile agent planning. OmniAct (Kapoor et al., 2024) employs prompt-based techniques to structure multimodal inputs and improve reasoning capabilities. This approach allows agents to integrate external tools and adjust output formats dynamically and efficiently."}, {"title": "3.3 Action", "content": "The action component demonstrates how agents execute tasks in a mobile environment by utilizing three key aspects: screen interactions, API calls, and agent interactions. Through screen interactions, agents tap, swipe, or type on GUIs, imitating human behavior to navigate apps. They also make API calls to access deeper system functions, such as issuing commands to automate tasks beyond the GUI. Additionally, by collaborating with other agents, they enhance their ability to adapt to complex tasks, ensuring efficient task execution across diverse environments.\nScreen Interactions In mobile environments, interactions often involve actions like tapping, swiping, or typing on virtual interfaces. Agents, such as those in AiTW, AITZ, and AMEX (Rawles et al., 2024b; Chai et al., 2024; Zhang et al., 2024b), perform GUI-based actions by mimicking human interactions, ensuring they work smoothly with native apps. These actions go beyond simple gestures, including complex multi-step processes requiring agents to dynamically adapt to changes or new inputs (Lee et al., 2021; Wang et al., 2022).\nAPI Calls are essential for mobile agents as they interact with GUIs and perform tasks that require deep integration with the mobile operating system (Chen and Li, 2024; Kapoor et al., 2024). Besides API calls, agents can also use HTML and XML data to access underlying functions, modify device settings, retrieve sensor data, and automate app navigation without relying solely on GUI-based inputs (Chai et al., 2024; Deng et al., 2024; Li et al., 2024). By combining these approaches, agents can efficiently complete tasks while comprehensively understanding the environment.\nAgent Interactions go beyond basic screen actions and API calls, requiring decision-making, environmental adaptation, and multitasking. Mobile agents, like Octo-planner (Chen et al., 2024c) working with action agents such as Octopus V2, need to handle tasks dynamically, such as interpreting user commands, managing app states, and adapting to changing inputs. By separating planning from execution, Octo-planner enhances both specialization and flexibility."}, {"title": "3.4 Memory", "content": "Memory mechanisms are crucial for mobile agents, allowing them to retain and use information across tasks. Current research maps in-context learning to short-term and long-term memory to external vector stores.\nShort-term Memory involves temporarily storing information and reasoning about it, similar to human working memory, which enables it to manage task continuity and adaptation effectively. Recent advancements have focused on enhancing the memory capabilities of mobile agents. For instance, Auto-UI (Zhan and Zhang, 2023) incorporates historical text information to improve decision-making by retaining past context, while UI-VLM (Dorka et al., 2024) adopts image-based memory storage. Unlike single-modality agents, multimodal agents need to manage short-term memory across various types of data, including text, images, and interactions. This ensures that important information from different sources is kept.\nLong-term Memory is more complex. While external vector stores allow retrieval of past experiences, their function differs significantly from human long-term memory, which is structured and highly interconnected. Currently, a combination of parametric memory and vector databases can mimic human long-term memory, with parametric memory holding implicit and semantic memories, while vector databases store more recent semantic and episodic memories. To address the need for efficient memory management, some approaches convert multimodal inputs into a unified text format for storage, simplifying retrieval and integration during task execution(Yang et al., 2023; Wang et al., 2024b; Wen et al., 2024)."}, {"title": "4 The Taxonomy of Mobile Agents", "content": "This section introduces a taxonomy of mobile agents, categorizing them into two primary types: prompt-based methods and training-based methods. Prompt-based agents leverage the advancements in LLMs to interpret and execute instructions through natural language processing, often focusing on tasks that require dynamic interaction with GUIs. On the other hand, training-based methods involve fine-tuning models or applying reinforcement learning to enhance agents' decision-making and adaptability over time."}, {"title": "4.1 Prompt-based Methods", "content": "Recent advancements in LLMs have demonstrated significant potential in developing autonomous GUI agents, particularly in tasks that require instruction following (Sanh et al., 2022; Taori et al., 2023; Chiang et al., 2023) and chain-of-thought (CoT) prompting (Nye et al., 2022; Wei et al., 2022). CoT prompting (Wei et al., 2022; Kojima et al., 2022; Zhang et al., 2023d), in particular, has proven effective in enabling LLMs to handle step-by-step processes, make decisions, and execute actions. These capabilities have shown to be highly beneficial in tasks involving GUI control (Rawles et al., 2023).\nGUI Tools are essential for enabling LLMs to interact with graphical user interfaces, as these models are primarily designed to process natural language rather than visual elements. To bridge this gap, GUI tools convert visual elements into text-based formats that LLMs can interpret. This multimodal integration significantly boosts the efficiency and flexibility of mobile agents in complex environments. Techniques like icon recognition and OCR (Zhang et al., 2021; Sunkara et al., 2022; Song et al., 2023) are used to parse GUI elements, which then converts the parsed elements into HTML layouts. However, this method relies heavily on external tools (Rawles et al., 2023; Wen et al., 2023a) and app-specific APIs (Zhou et al., 2023; Gur et al., 2023), often resulting in inefficiencies and errors during inference. Although some research has investigated multimodal architectures to process different types of inputs (Sun et al., 2022; Yan et al., 2023), these approaches still depend on detailed environment parsing for optimal performance. Given the importance of accurate GUI grounding, newer studies (Cheng et al., 2024; Hong et al., 2023) have begun exploring pre-training methods to improve agent performance in GUI tasks.\nMemory Mechanism plays a critical role in enhancing task execution within prompt-based methods. In agents like AppAgent (Yang et al., 2023), the agent employs an exploration phase for memory, allowing it to learn and adapt to new applications by storing interactions from prior explorations. This approach enables the agent to retain knowledge without needing additional training data. Mobile-Agent (Wang et al., 2024b,a) automates mobile app operations by analyzing screenshots with visual tools, avoiding reliance on system code. It plans tasks and corrects errors during operation using a self-reflection mechanism. Omniact (Kapoor et al., 2024) enhances perception by converting images into text and creating multimodal spaces for better reasoning.\nComplex Reasoning in agent systems refers to the ability of models to process, analyze, and integrate information from multiple sources to solve intricate tasks. This capability enhances decision-making, planning, and adaptability by enabling agents to draw connections between different data inputs, evaluate various outcomes, and execute informed actions in dynamic environments. COAT (Zhang et al., 2024b) enhances GUI agent performance by integrating semantic information"}, {"title": "4.2 Training-based Methods", "content": "In contrast to prompt-based methods, training-based approaches involve explicit model optimization. These agents fine-tune large language models like Llama (Zhang et al., 2023b) or multimodal models such as LLaVA (Liu et al., 2023a) by collecting multimodal instruction-following data or accessing API to obtain instruction information. This enhancement allows these models to function as the core \"brain\" for reasoning and planning and to execute these plans.\nPre-trained VLMS have become powerful tools for decision-making and interaction in mobile environments. Models like LLaVA (Liu et al., 2023a) and Qwen-VL (Bai et al., 2023), pre-trained on large-scale general datasets, capture both visual and language information effectively. However, their applicability in mobile settings is limited by the lack of sensitivity to interactive elements specific to mobile data. To improve the responsiveness of pre-trained models to interactive elements in mobile data, CogAgent (Hong et al., 2023) collected a large-scale mobile dataset for pre-training representations. CogAgent (Hong et al., 2023) integrates visual and textual inputs for GUI agents, improving interaction with complex mobile UIs using VLMs. Spotlight (Li and Li, 2022) is a vision-language model for mobile UI tasks, relying solely on screenshots and specific regions, supporting multi-task and few-shot learning, trained on a large-scale dataset. VUT (Li et al., 2021) employs a dual-tower Transformer for multi-task UI modeling, achieving competitive performance with fewer models and reduced computational costs.\nFine-Tuning pre-trained VLMs with common-sense reasoning capabilities has been facilitated by large-scale mobile datasets, such as AitW (Rawles et al., 2024b), through the Visual Instruction Tuning approach. Mobile data is highly structured and information-rich, making it challenging to accurately identify the position of a specific element, especially in densely packed images. ScreenAI (Baechler et al., 2024) uses LLMs to generate synthetic data for screen annotation, identifying UI elements' types and locations to create large datasets for tasks like question answering and UI navigation. In contrast, AMEX (Chai et al., 2024) employs multi-level annotations, including GUI element grounding, functionality descriptions, and complex natural language instructions, offering more detailed training data for mobile AI agents. Both methods enhance model performance by fine-tuning with the use of constructed synthetic datasets.\nAuto-GUI (Zhan and Zhang, 2023) introduces autonomous GUI control through direct interface interaction, using a chain-of-action technique for improved prediction. UI-VLM (Dorka et al., 2024) leverages multimodal data to generate image-text sequences for enhanced task performance. COCO-Agent (Ma et al., 2024) simplifies grounding tasks with modified instructions and element layouts. Octo-planner (Chen et al., 2024c) separates planning and execution, while AutoDroid (Wen et al., 2024) automates tasks by converting app exploration data into actionable knowledge, enhancing automation with fine-tuning and functionality matching.\nReinforcement Learning offers a dynamic approach to training mobile agents by allowing them to learn from interactions with real environments. This method is particularly effective in scenarios where the agent must adapt to changing contexts or optimize its actions based on rewards.\nThe WoB (Shi et al., 2017) platform enables reinforcement learning in real web environments by allowing agents to interact with websites using human-like actions. This work (Shi et al., 2017) converts web tasks into question-answering tasks through crowdsourcing, improving task generalization across different environments. Mini-WoB++ (Liu et al., 2018) introduces workflow-guided exploration, which integrates expert workflows with task-specific actions, accelerating learning and improving task efficiency in web-based tasks. DigiRL (Bai et al., 2024) combines offline and online reinforcement learning to train device control agents. It scales online training using a VLM-based evaluator that supports real-time interaction with 64 Android emulators, enhancing the efficiency of RL-based agent training."}, {"title": "5 Future Work", "content": "In this survey, we have presented the latest advancements in the field of mobile agents. While significant progress has been made, several challenges remain unresolved. Based on the current state of research, we propose the following future research directions:\n\u2022 Security and Privacy: Mobile agents face security risks in open environments. Future work should prioritize stronger security mechanisms to guard against malicious behavior and data breaches. Privacy-preserving techniques must also be developed to ensure sensitive data remains secure during agent interactions.\n\u2022 Adaptability to Dynamic Environments: Enhancing mobile agents' ability to adapt to dynamic and unpredictable environments is crucial. Future research should explore methods for real-time behavioral adjustments to changing conditions and resource availability.\n\u2022 Multi-agent Collaboration: Improving collaboration among multiple mobile agents remains a key challenge. Future research should focus on efficient communication and collaborative mechanisms that enable agents to dynamically form coalitions and complete tasks more effectively."}, {"title": "6 Conclusion", "content": "This survey provides a comprehensive overview of mobile agent technologies. Firstly, we reviewed advancements in mobile agents' benchmarks, which improve mobile agent assessments but still require more comprehensive methods to capture real-world dynamics. Next, we discussed the core components-perception, planning, action, and memory-that enable mobile agents to adapt to their environments, forming the foundation of their functionality. We then presented a taxonomy of mobile agents, differentiating between prompt-based and training-based methods, each with strengths and challenges in scalability and adaptability. Finally, we highlighted future research directions, focusing on security, adaptability, and multi-agent collaboration to advance mobile agent capabilities."}, {"title": "7 Limitations", "content": "This survey focuses on recent advancements in LLM-based mobile agents but provides limited coverage of traditional, non-LLM-based systems. The lack of discussion on older rule-based agents may limit the broader context of mobile agent technology development."}, {"title": "A.1 Complementary Technologies", "content": "Effective complementary technologies are vital for enhancing the performance and usability of mobile agents, in addition to key components like benchmarks, VLM models, fine-tuning methods, and advanced reasoning skills. These technologies facilitate seamless interactions with mobile environments, allowing agents to adapt, learn, and perform complex tasks efficiently.\nUIED (Xie et al., 2020) detects and classifies GUI elements using computer vision and deep learning, supporting interactive editing. WebGPT (Nakano et al., 2021) fine-tunes GPT-3 for web-based question answering using imitation learning and human feedback. WebVLN (Chen et al., 2024b) trains AI agents to navigate websites with question-based instructions, incorporating HTML for deeper understanding."}, {"title": "A.2 Available related technologies", "content": "Additionally, OmniACT (Kapoor et al., 2024) offers a comprehensive platform for evaluating task automation across various desktop applications and natural language tasks. WebVoyager (He et al., 2024) introduces an automated evaluation protocol using GPT-4V, capturing screenshots during navigation and achieving an 85.3% agreement with human judgments. Furthermore, Widget Captioning (Li et al., 2020b) sets a benchmark for improving UI accessibility and interaction by providing 162,859 human-annotated phrases that describe UI elements from multimodal inputs, paving the way for advancements in natural language generation tasks. Above all, leveraging a diverse set of system signals provides a more comprehensive and accurate assessment of an agent's performance (Xie et al., 2024).\nOn desktop platforms, research has focused on evaluating how well LLM-based agents utilize APIs and software tools to complete tasks such as file management and presentations (Qin et al., 2023; Guo et al., 2023). AgentBench (Liu et al., 2023b) offers a flexible, scalable framework for evaluating agent tasks, while PPTC Benchmark (Guo et al., 2023) targets the evaluation of LLM-based agents' performance in PowerPoint-related tasks."}]}