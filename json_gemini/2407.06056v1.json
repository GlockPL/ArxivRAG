{"title": "Stranger Danger! Identifying and Avoiding Unpredictable Pedestrians in RL-based Social Robot Navigation", "authors": ["Sara Pohland", "Alvin Tan", "Prabal Dutta", "Claire Tomlin"], "abstract": "Reinforcement learning (RL) methods for social robot navigation show great success navigating robots through large crowds of people, but the performance of these learning-based methods tends to degrade in particularly challenging or unfamiliar situations due to the models' dependency on representative training data. To ensure human safety and comfort, it is critical that these algorithms handle uncommon cases appropriately, but the low frequency and wide diversity of such situations present a significant challenge for these data-driven methods. To overcome this challenge, we propose modifications to the learning process that encourage these RL policies to maintain additional caution in unfamiliar situations. Specifically, we improve the Socially Attentive Reinforcement Learning (SARL) policy by (1) modifying the training process to systematically introduce deviations into a pedestrian model, (2) updating the value network to estimate and utilize pedestrian-unpredictability features, and (3) implementing a reward function to learn an effective response to pedestrian unpredictability. Compared to the original SARL policy, our modified policy maintains similar navigation times and path lengths, while reducing the number of collisions by 82% and reducing the proportion of time spent in the pedestrians' personal space by up to 19 percentage points for the most difficult cases. We also describe how to apply these modifications to other RL policies and demonstrate that some key high-level behaviors of our approach transfer to a physical robot.", "sections": [{"title": "I. INTRODUCTION", "content": "While robot navigation has been explored extensively, smooth integration of mobile robots into human-populated spaces is yet to be achieved. Robots that interact with people are expected to navigate in a way that is predictable and unobtrusive, maintaining both the safety and comfort of surrounding people [1]. The social robot navigation field is seeing a growing number of RL-based approaches that implicitly predict human motion and plan robot paths without explicit models of human behavior [2]. These RL-based approaches have achieved great success in enabling effective navigation around large crowds of people, outperforming traditional approaches [3]\u2013[6]. However, the performance of RL policies is contingent on having representative training data, so these policies are sensitive to differences in pedestrian behavior seen during deployment versus training (a problem generally referred to as domain shift). Domain shift is always a concern with learning-based methods but is of particular importance in social robot navigation because of the wide range of human behavior and the potential physical hazards and psychological risks associated with mobile robots operating in close proximity to humans [7].\nThus, to widely deploy RL-based approaches for social robot navigation, these methods should recognize their own level of uncertainty in situations in which pedestrians behave unpredictably from the perspective of the RL policy. We say that pedestrians behave unpredictably if their behavior deviates significantly from their normal or expected behavior as defined by the RL policy's implicit model of human behavior. Once an RL policy recognizes that it is in an unfamiliar situation and cannot accurately predict the behavior of nearby pedestrians, it should respond with appropriate caution (Figure 1). Thus, it must distinguish between pedestrians who exhibit predictable behavior seen during training and those whose behavior is unpredictable, and learn to navigate efficiently around predictable pedestrians while maintaining caution toward unpredictable ones. This would allow such policies to generalize well to arbitrary pedestrian behavior.\nTo explore this idea, we incorporate uncertainty-awareness into an existing RL policy called SARL [6] by (1) modifying the training process to systematically inject significant deviations into a model of pedestrian behavior, (2) augmenting the observation space of the value network algorithm to recognize and quantify deviations of pedestrian behavior from the assumed model, and (3) adding a term to the reward function to encourage caution toward progressively more unpredictable pedestrians while navigating normally around predictable ones. We conduct ablation studies in simulation to understand the cumulative impact of these modifications and find that they substantially improve the performance of SARL around particularly difficult and previously unseen pedestrian behavior. Compared to the original policy, our modified policy maintains similar navigation times and path lengths while notably reducing the number of collisions and the proportion of time spent in the simulated pedestrians' personal space. We then describe how the same modifications"}, {"title": "II. RELATED WORK", "content": "We first note high-level procedural novelties of our work and then distinguish our approach from closely related work.\nTraditional approaches to social robot navigation explicitly predict human trajectories and then plan paths around them. Because these approaches use explicit human models, they can be adapted to detect when a pedestrian consistently deviates from these models, and then respond by calculating a conservative path that still maintains pedestrian comfort and safety [8]\u2013[15]. Some RL-based policies also incorporate explicit pedestrian trajectory predictions [16], [17], but the vast majority do not. Our work extends the anomaly detection and response process to RL-based policies that implicitly model agent interactions and are thus not directly amenable to techniques designed for explicit human models.\nThe evaluation procedures in most prior work in RL-based social robot navigation are ill-suited for determining policy performance under significant deviations from the assumed pedestrian model (i.e., under domain shift) because (i) the evaluations are conducted using the same pedestrian model as was used during training, albeit in randomly generated scenarios and (ii) they usually only report average performance values, which reveal very little about policy performance in particularly difficult and unfamiliar situations [18]\u2013[24]. To better evaluate and quantify policy performance under domain shift, we evaluate our policies on pedestrian models that are outside of the training distribution, and we report Conditional Value at Risk (CVaR) values, which describe expected performance on the hardest cases [25].\nOne approach to addressing the domain shift problem in RL-based social robot navigation could be to train on more realistic data (e.g., higher-fidelity pedestrian models or real-world pedestrian data) [26], [27]. While this approach would expand what is included in the training distribution, there are undoubtedly myriad situations and behaviors that still lie outside the training distribution, so the need to identify and account for these unfamiliar situations still persists. In our paper, we attempt to express this gap in realism by training on a relatively simple and homogeneous pedestrian model and testing on scenarios that include a mix of three different pedestrian models with randomized parameters.\nA collection of papers manage pedestrian unpredictability by training the robot to avoid regions around pedestrians called \"Danger Zones\u201d or \u201cWarning Zones\" that comprise all their physically plausible next states [28]\u2013[30]. The size and shape of these Zones depend on pedestrian velocity and observed demographic (e.g., child vs. adult). In our approach, we directly adjust each pedestrian's discomfort distance instead of defining additional Zones, and these"}, {"title": "III. OUR UNCERTAINTY-AWARE RL POLICY", "content": "To reduce SARL's dependence on its training data, allow the policy to recognize when it is in an unfamiliar situation, and improve the ability of the policy to generalize to novel scenarios, we modify the training process (\u00a7III-A), the model architecture (\u00a7III-B), and the reward function (\u00a7III-C).\n### A. Training Process\nOur uncertainty-aware RL policy is trained in a modified CrowdSim environment [6], where we generate arbitrarily many pedestrians with randomized initial positions and goals."}, {"title": "A. Training Process", "content": "Our uncertainty-aware RL policy is trained in a modified CrowdSim environment [6], where we generate arbitrarily\nmany pedestrians with randomized initial positions and goals.\nBy default, pedestrians choose their action at each time step\nbased on the ORCA policy [32] a navigation strategy\ncommonly used to model human navigation behavior [2].\nFor a pedestrian with a preferred velocity of $v_{pref}$, an ORCA\naction, $a_{orca} \\in {v \\in \\mathbb{R}^2 : ||v||_2 \\leq V_{pref}}$, comprises x and y\nvelocities and makes progress towards a goal while avoiding\ncollisions with other agents. To generate quantifiable devi-\nations from this policy and systematically produce highly-\nheterogenous pedestrians for training, we augment the policy\nwith Gaussian noise. Each pedestrian is instantiated with\na deviation value $p \\sim U(0,P_{max})$ for $P_{max} \\in [0,1]$, which\nrepresents how much the pedestrian deviates from the default\nORCA policy. At each time step, the pedestrian takes an ac-\ntion $\\tilde{a} = (1 - p)\\tilde{a}_{ORCA}+ p\\tilde{a}_{rand}$, where $\\tilde{a}_{rand} \\sim N(O_2, V_{pref}I_2)$\nis a 2D Gaussian-random action. We call this noisy policy\nNoisy ORCA to differentiate it from the standard ORCA\npolicy. The left plot in Figure 2 provides one example of\nNoisy ORCA pedestrians. We intentionally do not ensure\nthat this action is collision-free and rational, as real people\nmay take actions that appear irrational and result in collision.\nWe found that successfully training an RL policy on\nNoisy ORCA pedestrians is not trivial. These pedestrians\ngenerate spurious signals from their random motion, making\nit difficult for the robot to simultaneously learn how to\nexploit behavioral patterns in ORCA while also avoiding\nthe unpredictable deviations from ORCA. To overcome this\nproblem, we trained the RL policies using curriculum train-\ning, starting with standard ORCA pedestrians and gradually\nincreasing the difficulty of the navigation scenario throughout\nthe training process. Specifically, we increased the maxi-\nmum deviation value ($p_{max}$) of pedestrians by 0.1 every\n2,000 training episodes. We ran 12,000 episodes, concluding\ntraining with a maximum deviation value of $P_{max} = 0.5$."}, {"title": "B. Model Architecture", "content": "We assume the robot has access to its own position, veloc-\nity, radius, orientation, preferred velocity, and goal position.\nWe also assume the robot has access to the position, velocity,\nand radius of each pedestrian that has been observed by the\nrobot over time. We choose to use only these observations\nbecause they can be readily obtained by physical robots\nnavigating around people in the real world.\nOur RL policy is trained using a value iteration algorithm\nwith the value network shown in Figure 3. Notably, we add\na layer of multi-layer perceptrons ($MLP_1$), which we train\nseparately from the rest of the network such that $MLP_1$\npredicts deviation values $\\hat{p}$ for each pedestrian while the\nrest of the network estimates the value function using ground\ntruth p values. We train these components separately so the\nvalue network learns how to utilize the quantified deviation\nof each pedestrian from the ORCA policy, as opposed to\nsimply learning latent features of Noisy ORCA pedestrians.\nThe purpose of $MLP_1$, which we refer to as the uncertainty\nestimation network, is to quantify deviation of any observed\npedestrian (not just Noisy ORCA ones) from the ORCA\npolicy. See Figure 2b for sample unpredictability estimations\nfor pedestrians operating under previously unseen policies."}, {"title": "C. Reward Function", "content": "The reward function used to train our RL policies en-\ncourages the robot to reach its goal while maintaining social\nnorms and avoiding collisions with people. In an environment\nwith n pedestrians, where $d_i$ is the distance from the robot\nto the ith person and $d_g$ is the distance from the robot to its\ngoal, the default (i.e., p-independent) reward is:\n$r = k_{succ}H(-d_g)+k_{coll} \\sum_{i=1}^n H(-d_i) + k_{disc} \\sum_{i=1}^n min{0, d_i-d_{disc}}$,\nwhere H is the step function and $d_{disc} = 0.1$ is a constant\nreferred to as discomfort distance. In this function, the first\nterm rewards the robot for reaching its goal, the second pe-\nnalizes it for colliding with a person, and the third encourages\nit to maintain a comfortable distance from each person.\nTo incorporate the intuition of avoiding close interac-\ntions with unpredictable pedestrians while freely navigating\naround predictable ones, we modify the discomfort distance\nin the reward function to be p-dependent. Given a deviation\nvalue $p_i$ for the ith pedestrian, their discomfort distance in\nthe modified function is $d_{disc}(p_i) = ap_i+b$, where a = 1.0\nand b = 0.2 for our experiments. Everything else from the\ninitial reward function remains unchanged. We call this p-\ndependent reward function the modified reward function."}, {"title": "IV. EXPERIMENTAL EVALUATION", "content": "We conduct two simulated ablation studies of our RL pol-\nicy to analyze how the policy behaves in various situations.\nWe also implement our RL policy on a physical robot and\ndiscuss some takeaways from our hardware experiment."}, {"title": "A. Simulation Experimental Setups", "content": "We evaluate our RL policies in a modified CrowdSim\nenvironment by conducting randomized episodes across six\ndistinct categories of robot-pedestrian interactions (circle\nand perpendicular crossing, oncoming and outgoing flow,\nsingle and perpetual random goals) comprising a superset"}, {"title": "B. Performance Metrics", "content": "To compare navigation policies, we evaluate the robot's\nability to efficiently navigate to its goal while preserving the\nsafety and comfort of surrounding pedestrians using the fol-\nlowing metrics: (1) Success rate: percentage of trials where\nthe robot successfully reaches its goal within 30 seconds.\n(2) Timeout rate: percentage of trials where the robot fails\nto reach the goal in the allotted time. (3) Collision rate:\npercentage of trials where the robot collides with at least\none pedestrian. (4) Relative navigation time: time required to\nnavigate to the goal (relative to the fastest time). (5) Relative\npath length: distance traveled by the robot to its goal (relative\nto the shortest path). (6) Number of collisions: total number\nof collisions between the robot and any pedestrian across all\ntrials. (7) Personal space cost: overall personal space cost\nincurred by the robot (as defined by [35] with parameters\nfrom [36]). (8) Personal space violation: percentage of time\nspent within the personal space of a pedestrian (as defined\nby [37]). (9) Intimate space violation: percentage of time\nspent within the intimate space of a pedestrian (as defined\nby [37]). Metrics 1 \u2013 5 describe robot path efficiency, while\nmetrics 6 - 9 quantify the comfort of nearby pedestrians."}, {"title": "C. Simulation Results & Analysis", "content": "Our ablation studies analyze the cumulative impact of\nmodifying the training process (\u00a7III-A), the model architec-\nture (\u00a7III-B), and the reward function (\u00a7III-C) of the original\nSARL policy. Since these modifications have sequential\ndependencies, we define our ablation study as follows: the\noriginal policy with no modification is referred to as SARL,\nthe policy with only the modified training process is referred\nto as Training, the policy with both the modified training\nprocess and model architecture is referred to as Model, and\nthe policy with all three modifications (training process,\nmodel architecture, and reward function) is referred to as\nReward or \"our full uncertainty-aware policy.\" We show that\nthe combination of these three policy modifications improves\npolicy performance on particularly complex scenarios.\n1) Ablation Study on Noisy ORCA Pedestrians: We run\n500 trials with increasingly noisy pedestrians to quantify\nthe policies' performance under domain shift. In the left\nplot of Figure 4, we see that ORCA and all variations of\nthe socially-aware RL policy perform comparably when the\npedestrians navigate with only small deviations from ORCA\n(i.e., $P_{max} < 0.3$). However, as the maximum randomness\nof the pedestrians increases, the performance of all of the\npolicies drops significantly except for that of Reward. We\nsee the most significant performance drops for ORCA and\nstandard SARL, which is expected because they implicitly\nexpect pedestrians to behave according to the original ORCA\npolicy. We also see a reasonably large drop for Training and\nModel, which indicates that the modified reward function in\nReward is crucial for learning robot responses that generalize\nwell to significant changes in pedestrian behavior.\n2) Comparing Different Discomfort Distances: Since the\ngreatest performance improvement comes from using a p-\ndependent discomfort distance in the reward function, it\nis natural to suspect that we can improve performance by\nsimply tuning the constant p-independent distance in the\noriginal reward function. We explore this in the right plot of\nFigure 4. The policy trained with a discomfort distance of Om\nfrequently collides with pedestrians, while policies trained"}, {"title": "V. EXTENSIONS TO OTHER RL POLICIES", "content": "In our work, we demonstrate the need to train and evaluate\nRL-based social navigation policies with the consideration of\ndomain shift. While we focus on modifying SARL [6], our\nproposed modifications are not restricted to this particular\npolicy. Even given significant advances in model architec-\ntures for RL-based social navigation, the overall framework\nof many policies remains conducive to these modifications.\nTraining process: We develop a curriculum training pro-\ncess, where the RL policy is initially trained as normal. As\ntraining progresses, Gaussian noise is increasingly added to\nthe pedestrians' actions. While there are many approaches\nfor modeling pedestrian behavior during RL policy training,\nthe training process of any policy can be modified in this\nway as long as the action values for pedestrians in the\nenvironment are accessible. Regardless of how pedestrian\nactions are determined, each pedestrian can be initialized\nwith a corresponding deviation value p and their actions\nadjusted with p-dependent Gaussian random noise.\nModel architecture: We propose a modification to the\nRL policy model architecture by (1) training an uncertainty\nestimation network and (2) incorporating the uncertainty\nestimations as agent-level features in the observation space.\nThe uncertainty estimation network can be developed entirely\nindependently from the RL policy, so this component is\ncompletely policy-agnostic. For the uncertainty estimations\nto be seamlessly incorporated into the original RL policy,\nthe observation space of the original policy must contain\nagent-level features (e.g., position and velocity values for\neach nearby pedestrian). This is true for many existing\npolicies. Additional modifications would have to be made for\nend-to-end RL policies that operate directly on raw sensor\nmeasurements (e.g., images or 2D lidar).\nReward function: We propose a modification to the\nreward function that encourages the robot to maintain ad-\nditional space around pedestrians that deviate from the\nassumed model of pedestrian behavior. To make this same\nmodification in other RL policies, their reward function\nsimply must contain some notion of \"safety space,\" \"dis-\ncomfort distance,\u201d or \u201cclearance\" that captures a sense of\nmaintaining proper distance from pedestrians. If this is the\ncase, the modification of increasing each agent's discomfort\ndistance based on their deviation value p is subsequently\nstraightforward, though specific constants may need to be\ntuned for the particular model of interest."}, {"title": "VI. CONCLUSIONS", "content": "In this work, we articulate the domain shift problem\nfor RL policies in social robot navigation and present an\napproach that improves generalizability of RL policies to\nnovel scenarios while maintaining their efficiency in familiar\nones. We find that SARL [6] generalizes poorly to significant\ndeviations in pedestrian behavior, thereby presenting serious\nconcerns for pedestrian safety and comfort in a real-world\ndeployment. We posit that for socially-aware RL policies\nto be viable in real-world mobile robots, these policies\nmust recognize when people deviate from the (implicitly)\nassumed pedestrian model and take appropriate caution. We\npresent effective methods for modifying the training process,\nthe model architecture, and the reward function of SARL\nthat substantially improve the generalizability of the policy.\nComparing our modified policy to the original SARL policy\non randomized scenarios containing realistic ORCA and non-\nORCA human policies, our modifications reduce the number\nof collisions by 82% and reduce the proportion of time spent\nin the pedestrians' personal space by 16 percentage points for\nthe hardest 10% of all trials and by 19 percentage points for\nthe hardest 5% of trials. This increase in pedestrian comfort\nis achieved while maintaining similar navigation times and\npath lengths. We also discuss how these same modifications\ncan be applied to other socially-aware RL policies.\nWhile we believe our work takes an important step toward\nenabling the deployment of socially-aware RL policies on\nmobile robots, there are some limitations that should be\naddressed. First, we modify the reward function to encourage\nthe robot to maintain greater space between itself and\nunpredictable pedestrians. While this heuristic for caution\nis reasonable in many situations, it is less effective in tight\nspaces, where the robot is unable to maintain such a distance.\nIt would be interesting to explore other heuristics, such as\nslowing down, speeding up, or some combination of ad-\njusting distance and speed when approaching unpredictable\npedestrians. Another limitation is that we use the ORCA\npolicy as our primary pedestrian model for this study because\nthis is the model commonly used in other RL-based social\nnavigation work. However, this model is relatively simplistic.\nIt would be interesting to train RL policies using other\nmodels of pedestrian behavior and evaluate generalizability\nto even more diverse and realistic pedestrian scenarios."}, {"title": "VII. APPENDIX", "content": "### A. Additional RL Policy Details\n1) Observation Space: The observation space of the RL\npolicy consists of the full state of the robot and the observ-\nable state of each of the n pedestrians. The full state of the\nrobot is given by\n$o^{(r)} = [(p_x^{(r)}, p_y^{(r)}, v_x^{(r)}, v_y^{(r)}, r^{(r)}, g_x^{(r)}, g_y^{(r)}, v_{pref}^{(r)}, \\theta^{(r)}]$\nand the observable state of the ith pedestrian is given by\n$o^{(hi)} = [(p_x^{(hi)}, p_y^{(hi)}, v_x^{(hi)}, v_y^{(hi)}, r^{(hi)}, g_x^{(hi)}, g_y^{(hi)}, v_{pref}^{(hi)}, \\theta^{(hi)}]$\nwhere $(p_x,p_y)$ is the current position, $(v_x, v_y)$ is the velocity,\nr is the radius, $(g_x,g_y)$ is the goal position, $v_{pref}$ is the pre-\nferred velocity, and $\\theta$ is the turning angle corresponding to\nthe robot (r) or ith human (hi). The current position, velocity,\ngoal position, and turning angle are all measured with respect\nto a fixed world frame. The radius is measured assuming\nthat the robot and each pedestrian can be represented using\na circle with a fixed radius. The preferred velocity is the\nmaximum possible speed of each agent, which is the speed\nit would travel at if nothing is obstructing its path.\n2) Action Space: The robot receives linear and rotational\nvelocity commands as actions under the assumption that the\nvelocity of the robot can be achieved instantly after the action\ncommand is received. The action space is discretized into\n16 rotations evenly spaced in the range $[0,2\\pi)$ and 5 speeds\nexponentially spaced in the range $(0, v_{pref}^{(r)}]$ such that the full\nset of speeds of the robot is\n$\\{ \\frac{e^{i/5} - 1}{e-1}v_{pref}^{(r)}, i={1,5}\\}$.\nThe robot is also able to receive a stop command, resulting\nin 81 possible discrete actions.\n3) Value Network Architecture: The parameters used in\nour model architecture are given in Table III. For the\nuncertainty estimation network (MLP1), we conducted a\nparametric depth study (from 0 to 18 hidden layers) to see\nhow the size of the network improved inference performance.\nRegarding the number of hidden layers, the models' perfor-\nmance improved as the number of hidden layers increased\nfrom 0 to 5, but degenerated with more than 9 hidden layers.\n4) Network Input Preprocessing: We preprocess the\npedestrian observations ($o_1$ through $o_n$ in Figure 3) before\nusing them as input to the value network. For the uncertainty\nestimation network (MLP\u2081), we explored a number of dif-\nferent possible input features. In particular, we considered"}]}