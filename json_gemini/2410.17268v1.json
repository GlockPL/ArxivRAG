{"title": "SPIKE-SSM: A SPARSE, PRECISE, AND EFFICIENT\nSPIKING STATE SPACE MODEL FOR LONG SEQUENCES\nLEARNING", "authors": ["Yan Zhong", "Ruoyu Zhao", "Chao Wang", "Qinghai Guo", "Jianguo Zhang", "Zhichao Lu", "Luziwei Leng"], "abstract": "Spiking neural networks (SNNs) provide a low-power, energy-efficient solution by\nutilizing the spike-based and sparse nature of biological systems. Since the advent\nof Transformers, SNNs have struggled to compete with artificial networks on long\nsequential tasks, until the recent emergence of state space models (SSMs), which\noffer superior computational efficiency and modeling capability. However, apply-\ning the highly capable SSMs to SNNs for long sequences learning poses three\nmajor challenges: \u25cf The membrane potential is determined by the past spiking\nhistory of the neuron, leading to reduced efficiency for sequence modeling in par-\nallel computing scenarios. Complex dynamics of biological spiking neurons are\ncrucial for functionality but challenging to simulate and exploit effectively in large\nnetworks. It is arduous to maintain high sparsity while achieving high accuracy\nfor spiking neurons without resorting to dense computing, as utilized in artificial\nneuron-based SSMs. To address these challenges, we propose a sparse, precise\nand efficient spiking SSM framework, termed SPikE-SSM. For 0, we propose a\nboundary compression strategy (PMBC) to accelerate the inference of the spiking\nneuron model, enabling parallel processing for long sequence learning. For , we\npropose a novel and concise neuron model incorporating reset-refractory mech-\nanism to leverage the inherent temporal dimension for dynamic computing with\nbiological interpretability. For , we hierarchically integrate the proposed neuron\nmodel to the original SSM block, and enhance the dynamics of SPikE-SSM by\nincorporating trainable thresholds and refractory magnitudes to balance accuracy\nand sparsity. Extensive experiments illustrate the effectiveness and robustness\nof SPikE-SSM on the long range arena benchmarks and large language dataset\nWikiText-103, showing the potential of dynamic spiking neurons in efficient long\nsequence learning. The code will be publicly available.", "sections": [{"title": "1 INTRODUCTION", "content": "Spiking neural networks (SNNs) recently emerged as a competitive paradigm to improve AI en-\nergy efficiency. SNNs transmit information as binary spikes between synapses to perform sparse\nand event-driven computation. Despite being increasingly more competitive with artificial neural\nnetworks (ANNs) in vision tasks, SNNs still struggle with long-sequence modeling a critical task\nfor a wide range of temporal or sequential data-driven machine learning applications, such as text\ncomprehending, electroencephalograms spanning, etc."}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 LONG SEQUENCES LEARNING MODELS", "content": "Long sequence modeling has gained significant attention recently due to its widespread applica-\ntion across different domains such as text comprehending, computer vision and electroencephalograms spanning. The key\nchallenge in long-sequence modeling lies in efficiently compressing context into a manageable state\nwhile capturing information spread across observations separated by thousands of timesteps. To\naddress them, Transformer and Attention are\nproposed to retain the entire context during auto-regressive inference, which is effective but requires\nquadratic-time computational complexity. Although some Transformer variants are proposed to reduce the compute and memory requirements, their\nperformances on long-range reasoning remain considerably suboptimal. Inspired\nby RNNs, RWKV combines the parallel training of transformers with the effi-\ncient inference of RNNs. Similarly, other recurrent models aim to compress context into a finite\nstate, offering constant-time inference and linear-time training, but their effectiveness is limited by\nthe quality of compression and a fixed representation space. More recently, SSM-\nbased methods have emerged as a promising\nalternative to sequence models such as RNNs and Transformers. For example, HiPPO pioneered compressing long inputs into dynamic representations using orthogonal polynomi-\nals, while S4 advanced this with low-rank corrections for stable diagonalization\nand simplified Cauchy kernel operations. Mamba focuses on selective state\nrepresentations to optimize efficiency and effectiveness, using a selection mechanism and hardware-\noptimized algorithms to maintain robust contextual information capture. All above methods are\nbased on artificial neurons with analog-valued output, resulting in dense vector-matrix multipli-\ncation (VMM) and huge computational costs. In contrast, the proposed SPikE-SSM utilizes the"}, {"title": "2.2 SNNS-BASED SEQUENCE MODELING AND APPLICATIONS", "content": "SNNs have gained attention as a compelling bio-plausible and\ncomputational efficient substitute for traditional artificial neural networks (ANNs) in many vision\ntasks. However, SNNs have struggled to make significant progress in long-sequence modeling tasks\ndue to the inherent serial computing nature. Therefore, to train SNNs in parallel, PSN simplifies spiking neuron by omitting the reset mechanism, leading to reduced sparsity. To\nhandle this issue, a probabilistic reset mechanism is proposed in PSU to achieve\nparallel computing with elevated sparsity by decoupling the integration-spiking-resetting process,\nwhich comes at the expense of higher computational complexity. With the recent resurgence of\nSSMs, there has been a renewed focus on applying efficient parallel computing to SNNs. For ex-\nample, SpikeS4 integrates LIF neurons with S4 layers for speech learning. Binary\nS4D builds a binary SSM by applying a spiking activation directly to the sum of hidden states,\nenabling parallel training but neglecting neuronal dynamics. To further en-\nhance sparsity, a stochastic spiking neuron is proposed in S6-based SNN, which is trained with stochastic noises in gradients, resulting in accuracy degradation. More re-\ncently, SpikingSSMs utilizes a surrogate dynamic network (SDN) to approxi-\nmate the dynamics of LIF neurons, which extremely accelerates the training and inference by par-\nallel computing. However, the pre-training requirement of SDN could constrain its application on\nmore general dynamic spiking neurons which are hard to approximate. Due to the effectiveness of\nspike-based sequence learning, some SNNs-based language models are proposed for more efficient\nlanguage modeling, such as SpikeGPT and SpikeBERT. In con-\ntrast to existing spiking SSMs, SPikE-SSM proposed in this paper realizes comprehensive parallel\nacceleration with trainable temporal dynamics, efficiently achieving both high sparsity and excel-\nlent accuracy for long-range dependencies learning, which possesses the potential and prospects for\nconstructing low-energy language models and enabling widespread applications."}, {"title": "3 \u041c\u0415\u0422\u041dOD", "content": ""}, {"title": "3.1 PRELIMINARIES OF SSMS AND LIF NEURON", "content": "SSMs. According to and, SSMs provide a framework for\nlong sequences modeling with lower computational complexity, which aims to transform an input\nsequence $x(t) = (x_o,\u2026\u2026,x_{L-1}) \\in \\mathbb{R}^{1\\times{L}}$ into an output sequence $y(t)(y_o,\u2026\u2026, y_{L-1}) \\in \\mathbb{R}^{1\\times{L}}$,\nwhere L is the length of sequence. This transformation occurs with the aid of an implicit latent state\n$h(t) \\in \\mathbb{R}^{N\\times{1}}$, which captures the underlying dynamics and relationships between the input and\noutput sequences. The continuous representation of this model is formulated as:\n$\\frac{dh(t)}{dt} = h'(t) = Ah(t) + Bx(t), y(t) = Ch(t),$ (1)\nwhere the state matrix $A \\in \\mathbb{R}^{N\\times{N}}$ and vectors $B \\in \\mathbb{R}^{N\\times{1}}, C\\in \\mathbb{R}^{1\\times{N}}$ are the parameters. To\nadapt SSM to real-world discrete data, one can discretize the continuous formulation Eq. (1) with\ndiscretization rules such as zero-order hold. Then x(t) can\nbe mapped to y(t) in a recurrent view:\n$\\overline{A} = e^{\\Delta{A}}, \\overline{B} = A^{-1}(\\overline{A} \u2013 I)B, C = C \\\\h_t=\\overline{A}h_{t-1}+ \\overline{B}x_t, Y_t = Ch_t,$ (2)\nwhere $\u2206 \\in \\mathbb{R}^+$ is the sample time, and $h_{-1} = 0$ for convenience. Note that the recurrence operation\nin Eq. (2) can be explicitly unrolled as a kernel view:\n$Y_k = \\sum_{j=0}^{k}K_jX_{k-j}, \\\\K = (CB,CAB,...,CA^{L-1}B) \\in \\mathbb{R}^{1\\times{L}},$ (3)\nwhich requires O(L2) multiplications despite all the elements of y can be expediently computed in\nparallel by computing the kernel K first. Fortunately, Eq. (3) can be accelerated by Fast Fourier\nTransform (FFT) with time complexity O(Llog L)"}, {"title": "3.2 PARALLEL MAX-MIN BOUNDARY COMPRESSION (PMBC)", "content": "This subsection aims to address Challenge 1. According to discretizing the LIF neuron with the\nsoft reset mechanism in Eq. (5) combined with a decoupling reset magnitude $U_t{th}$, we can obtain the\nfollowing formula:\n$U_t = \\tau{U_{t-1}} \u2013 S_{t-1}U_{th} + I_t, S_t = H_s (U_t \u2013 U_{th}).$ (6)\nThe output membrane voltage u is iteratively computed by Eq. (6) since $U_t$ depends on the spiking\nhistory from the previous time steps, notwithstanding the input current I can be obtained in parallel.\nThis leads to a significant reduction in computational efficiency, especially for long sequence inputs.\nTo solve this problem, we propose the following assertion, which lays the foundation for subsequent\nparallel computation to accelerate training and inference (See Appendix A.1 for the proof).\nAssertion 3.1. The historical input signal I and spiking informations are deconstructed in the\niteration process of Eq. (6), which is equivalent to:\n$U_t = K_t \u2013 M_t + U_{th}, S_t = H_s (K_t \u2013 M_t),$ (7)\nwhere $K_t = \\sum_{i=1}^{t} \\tau^{t-i}I_i, M_t = U_{th} \\sum_{i=1}^{t-1} S_i + U_{th}.$ (8)"}, {"title": "3.3 REFRACTORY LIF NEURON MODEL", "content": "In biological neurons, spiking is usually followed by a refractory period during which new spiking is\nmore difficult. This mechanism improves the overall sparsity of the network and could substantially\nreduce its energy consumption. Therefore, to simulate the intrinsic temporal dynamics of realistic\nneurons and further improve network sparsity, we introduce an innovative refractory LIF neuron\nmodel based on the soft reset mechanism, which effectively addresses Challenge . The LIF neuron\nwith a refractory period can be mathematically described as:\n$U_t = \\tau{U_{t-1}} + I_t \u2013 R_tU_{th}, S_t = H_s(U_t \u2013 U_{th}),$ (9)\nwhere $R_t = T_rR_{t-1} + S_{t-1},$ (10)\nIn our refractory neuron model, $T_r$ is the refractory magnitude. $R_t$ denotes the refractory period-\nbased sliding pulse, which is determined by both spiking signal $S_{t\u22121}$ and $R_{t-1}$ in the last time step.\nFrom Eq. (10) we can observe that the larger the value of the previous sliding pulse $R_{t-1}$, the greater\n$R_t$ becomes, causing membrane voltage $U_t$ to decrease accordingly, which makes it harder for the\nneuron to spike again during the refractory period. Similar to Assertion 3.1, we have the following\nresults for the proposed refractory neuron model (see Appendix A.3 for the proof):"}, {"title": "3.4 THE BLOCK OF SPIKE-SSM", "content": "For Challenge , due to the exceptional long sequence modeling capability of SSMs, we integrate\nthe proposed refractory neuron with soft reset mechanism and PMBC to the inherent SSM block,\nwhich aims to maintain both the high sparsity and excellent accuracy in the inference progress.\nIn the proposed SPikE-SSM, we choose the original block of S4D model as the\nbackbone since it can achieve pragmatic simplification to enhance model efficiency as the latest\ndiagonal version of SSM. Then the output y of the S4D block is activated by the proposed refractory\nneuron, hence Eq. (9) is rewritten as follows with Eq. (10) unchanged:\n$Y_t U_t = \\tau{U_{t-1}} + Y_t S_t = Ch_t, - R_tU_{th} H_s(u_t \u2013 U_{th}).$ (13)\nInspired from, we render $U_{th}$ and $U_{th}$ as trainable parameters within the SPikE-\nSSM block. This approach is motivated by their pivotal role in regulating the neuron\u2019s spiking\nrate, thereby not only bolstering the SPikE-SSM\u2019s capability to attain exceptional performance and\nexpedite the convergence of PMBC, but also serving as a further stepping stone to tackle Challenge\nwith greater efficacy. The results of Eq. (13) are fed into a linear layer that comprises a Conv1D\noperation followed by a GLU activation function. The Conv1D enables\nefficient local feature extraction, while the GLU activation selectively gates the information flow,\nimproving the model\u2019s ability to capture critical patterns in sparse binary data. Since the Heaviside\nfunction H is non-differentiable at x = 0, we adopt the surrogate gradient (SG) method in SPikE-\nSSM. The details of SG in our method are provided in the Appendix B."}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct extensive experiments to validate the superiority of our method, including\nthe testing of long-range modeling capabilities on the sequential LRA and WikiText-103 tasks, with\nablation studies and other related analyses. More experiments are shown in the Appendix \u0421."}, {"title": "4.1 DATASETS AND EXPERIMENTAL SETTINGS", "content": "Datasets. In this paper, we perform experiments on extensive long sequence databases, including\nsequential MNIST (sMNIST), LRA benchmarks (comprising six tasks) and WikiText-103 (one large Wikipedia text data). The Details of these\ndatasets are shown in the Appendix C.1.\nImplementation Details. The hyper-parameters \u03c4 and \u03c4\u03b7 are set to 0.1 and 0.9, respectively. To\nensure the threshold and refractory magnitude are positive during training, the trainable parame-\nters $U_t{th}$ and $U_t{th}$ are computed by exp(uth) and exp(Uth) with zero initialization (i.e. exp(uth)\nand exp(Uth) are initialized as 1). Other parameters of SPikE-SSM blocks are initialized same as\nS4D-Lin. SPikE-SSM is trained with Pytorch library using AdamW optimiza-\ntion. For sCIFAR10, sMNIST, psMNIST and LRA benchmarks, the model is\ntrained by the cross-entropy loss with accuracy (Acc) results reported, while the\nPerplexity results are reported for WikiText-103. The division of training and test data is consistent\nwith. The details of settings on nine different tasks are described in Table 7 in\nAppendix C.2, including six LRA benchmarks, three sequential vision tasks, and a large text dataset\n(WikiText-103)."}, {"title": "4.2 PERFORMANCES COMPARISONS", "content": "Results on LRA Benchmarks. Table 2 compares SPikE-SSM with both non-spiking and spiking\nnetworks using Transformer or SSM architectures. While maintaining accuracy comparable to the\noriginal model, SPikE-SSM achieves an average network sparsity of less than 10%. Additionally,"}, {"title": "4.3 ABLATION STUDY", "content": "We conduct ablation studies to verify the design rationality of SPikE-SSM following the same ex-\nperimental setups as Table 2. The variants with different levels of biological interpretability include:\n\u2022 ANN-S4D. ANN-based SSM (S4D) model.\n\u2022 Spiking-S4D. LIF-based spiking SSM without reset mechanism and refractory period.\n\u2022 SPikE-SSM-SR. Only the soft reset mechanism is considered in the LIF neuron of SPikE-\nSSM block with PMBC, as shown in Eq. (6).\n\u2022 SPikE-SSM-SRR. Both the soft reset mechanism and refractory period are considered in\nthe LIF neuron of SPikE-SSM block with PMBC, as shown in Eq. (9-10).\n\u2022 SPikE-SSM-SRT. Only the soft reset mechanism is considered in the LIF neuron of SPikE-\nSSM block with PMBC. Uth and $V_t{th}$ are trainable.\n\u2022 SPikE-SSM-Full. Both the soft reset mechanism and refractory period are considered in\nthe LIF neuron of SPikE-SSM block with PMBC. Uth and $V_t{th}$ are trainable.\nNote that Uth and $V_t{th}$ are trainable only in SPikE-SSM-Full and SPikE-SSM-SRT. We compare\nthe performances of different variants of SPikE-SSMs on sMNIST, psMNIST and sCIFAR10. The\nresults are shown in Table 4, from which we can observe that each component designed for three\nChallenges is effective in SPikE-SSM. Specifically, the proposed refractory neuron model with the\nsoft reset mechanism can optimize both high accuracy and pronounced sparsity with the thresholds"}, {"title": "4.4 TRAINING SPEED AND COMPUTATION COST ANALYSE", "content": "The Superiority of PMBC on Training Speed. We compare the training speed of SPikE-SSM,\nenhanced by our PMBC strategy, against traditional methods based on iterative LIF neurons, includ-\ning Back-Propagation Through Time (BPTT) and the more recent Spatial Learning\nThrough Time (SLTT), which uses an optimized computational graph. The input\nconsists of randomly generated 1-D sequences with various lengths of L = 1K, 2K, 4K, and 8K,\nand a batch size of 64. As shown in Table 5, the speedup ratio using PMBC increases with sequence\nlength, achieving a nearly two-order acceleration at 8K.\nThe Energy-efficiency of SPikE-SSM. We compare the energy costs of the proposed SPikE-\nSSM and its corresponding ANN-based version on WikiText-103, the sequence length of which\nis L = 8192. Spiking networks are considered energy-efficient due to sparse binary activation. The\nmultiplication between a binary activation and a floating-point weight can be performed using only\naddition operations in some neuromorphic chips. As a result, the primary oper-\nation in SNNs, synaptic accumulation (AC), incurs lower energy costs compared to the multiply-\nand-accumulate (MAC) operation in traditional ANNs. Although the hardware specifics and neuron\ndynamics are not considered here, a theoretical analysis can provide an estimate of SNN efficiency.\nFollowing previous studies, we assume the energy cost of an MAC\noperation is $E_{MAC}$ = 4.6pJ, while an AC operation costs $E_{AC}$ = 0.9pJ. In this\npart of the experiment, our model is set to comprise 16 layers, including a linear layer that projects\nspikes from d = 1024 to d = 2048. For specific quantitative comparison, we first report the spiking\nrates across different layers of SPikE-SSM in Figure 4. Then we report the MAC, AC, and energy\nconsumption in these feature-mix layers since they occupy the majority of parameters and computa-\ntions. Specifically, if these projections were fully computed via floating-point multiplications (SSM\nwith ANN-based settings), they would require 275.2G MACs, consuming approximately 1.265J.\nHowever, in our model (SPikE-SSM with SNN-based settings), the inputs to these layers are binary,\nwith an average spiking rate of less than 25%. Based on the spiking rates in Figure 4, our model per-\nforms 67.42G ACs, consuming 60.68mJ. The results are summarized in Table 6, which illustrate\nthe high energy efficiency of SPikE-SSM compared with ANN-based SSM and SpikingSSM."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduced SPikE-SSM, a novel spiking state space model designed to address key\nchallenges in long-sequence learning with SNNs. Specifically, we innovatively address the conflict\nof event-driven neuronal dynamics with parallel computing in long sequence modeling by the PMBC\nmethod, enabling explicit and efficient training of neuronal dynamics. Subsequently, a concise reset-\nrefractory neuron model is proposed to exploit the functionality of biological-plausible temporal\ndynamics. Its effective integration with the SSM block and incorporation of trainable thresholds\nand refractory magnitudes realize a balance between sparsity and accuracy. Extensive experiments\non sequential vision tasks, LRA benchmarks, and WikiText-103 language modeling validate the\nsuperior efficiency, accuracy, and sparsity of SPikE-SSM. Our work shows the potential of dynamic\nspiking neurons in efficient long sequence learning."}]}