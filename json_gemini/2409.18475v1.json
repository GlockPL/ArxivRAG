{"title": "Data Analysis in the Era of Generative AI", "authors": ["Jeevana Priya Inala", "Chenglong Wang", "Steven Drucker", "Gonzalo Ramos", "Victor Dibia", "Nathalie Riche", "Dave Brown", "Dan Marshall", "Jianfeng Gao"], "abstract": "This paper explores the potential of AI-powered tools to reshape data analysis, focusing on design considerations and challenges. We explore how the emergence of large language and multimodal models offers new opportunities to enhance various stages of data analysis workflow by translating high-level user intentions into executable code, charts, and insights. We then examine human-centered design principles that facilitate intuitive interactions, build user trust, and streamline the AI-assisted analysis workflow across multiple apps. Finally, we discuss the research challenges that impede the development of these AI-based systems such as enhancing model capabilities, evaluating and benchmarking, and understanding end-user needs.", "sections": [{"title": "1. Introduction", "content": "Data-driven decisions and insights are crucial across a wide range of industries and for everyday end-users. Businesses rely on data to make strategic decisions, healthcare providers track trends to improve patient outcomes, and journalists craft data-based news stories that inform the public. On a more personal level, individuals can use data to manage their finances, monitor their health, optimize their schedules, and even for shopping and making travel plans. However, the cost of current data analysis is high, meaning that only a select group of experts-data analysts have the tools and skills to ask data-related questions, analyze information, and generate reports/insights based on these insights. For the rest of the population, this process is largely inaccessible; they must accept the insights provided by others, without the ability to ask their own questions or conduct data analysis for personal tasks.\nDemocratizing data analysis has the potential to revolution-ize how we operate. Imagine a world where anyone, not"}, {"title": "The Challenge of Data Analysis: A Complex and Iter-ative Process.", "content": "Deriving insights from data is a complex process. Data analysts need to iterate among task formu-lation, data collection, exploratory analysis, and creating visual representations to discover and validate data insights before drawing conclusions, and they further need to doc-ument the analysis as reports for sharing (Figure 1). To achieve these diverse analysis steps, the analysts need not only conceptual knowledge (e.g., data sense-making, do-main, statistics, visualization design knowledge) to make right analysis decisions, they also need tool expertise and programming skills to take actions. Furthermore, because the analysis process is rarely linear and different steps are segregated across various tools (e.g., once the analysts dis-cover missing data based on their current visualizations, they may need to backtrack and revisit data collection or clean-ing steps), analysts face considerable overhead switching between tools and managing branching analysis steps.\nTo address these challenges, many interactive and automated tools and solutions have been developed, both to enhance analysts' ability to understand and explore data and to close the gulf of execution. These include interactive visualization platforms like Tableau and Power BI, automated data prepa-ration platforms like Alteryx or Trifacta, and programming environments like Jupyter Notebooks where data analysis can be integrated with data cleaning and visualization. How-ever, these systems often have to trade-off between the flexi-bility (and the expressiveness) of tasks that can be achieved with the system and the ease of learning and specifying the tasks. For example, compare Tableau, which provides user-friendly interface and drag-and-drop chart creation, with Python's Matplotlib library, which offers extensive cus-tomization and fine control over chart details but requires coding expertise."}, {"title": "The Generative AI (GenAI) revolution.", "content": "The emergence of large language and multimodal models presents new op-portunities to develop tools that are both expressive and user-friendly. These AI models possess the ability to per-form human-like reasoning and transform high-level user specifications into low-level executable steps (Bubeck et al., 2023). This reduces the user's need to learn new tools and languages while assisting with a variety of tasks. Founda-tion models like ChatGPT, GPT-4 (Achiam et al., 2023), Claude (Anthropic, 2024), Phi-3 (Abdin et al., 2024) have advanced proficiency in language understanding, conversa-tion ability, and planning; they encode knowledge across various fields such as medicine and finance, along with code generation capabilities. Multimodal models like GPT-40 and Phi-3-Vision (Abdin et al., 2024) further augment these abilities by integrating language and vision modalities, en-hancing reasoning across diverse input forms. With these abilities, we are in the era where AI-powered tools can em-power novice users to perform data analysis tasks previously out of reach and even significantly boost the productivity of experienced data analysts."}, {"title": "Uniqueness of AI-powered tools for data analysis.", "content": "AI-powered tools are being developed for many domains, in-cluding video and image generation, gaming, medicine, and software engineering. So, why does the data analysis do-main require special treatment? Unlike generation tasks, data analysis involves planning and exploration over mul-tiple steps. It also requires working with multiple modali-ties-natural language, structured data, code, images, and charts. Moreover, the task is fairly complicated that even non-AI solutions are spread across multiple apps and tools (for e.g. a user might clean data in Excel, generate charts in PowerBI, and then create a report in PowerPoint). These complexities create interesting challenges for existing Gen-erative AI models and systems, highlighting the need for better models and algorithms. Moreover, solving these tasks may require multiple AI tools and require novel techniques to avoid fragmented and disconnected experience for users.\nData analysis is inherently iterative. Users must be involved throughout the process because the full specification of the task is usually unknown at the beginning. As users see initial results, they may want to follow up with new questions or adjust their goals. This iterative and multimodal nature of data analysis means that current AI tools, which often rely on natural language interfaces and static modalities such as images and file upload, may not be well-suited for this domain. The data analysis domain inspires invention of new interfaces and experiences that allow humans to collaborate with AI tools more effectively.\nMoreover, data analysis is a sensitive domain. Errors can have serious consequences, especially in fields like health-care or finance. Imagine an LLM that incorrectly identifies"}, {"title": "Unlocking the potential of GenAI in data analysis.", "content": "The goal of this paper is to explore how we can unlock the poten-tial of generative AI tools in the data analysis domain. We aim to provide a view into parts of the vast design space of AI-powered data analysis tools that AI and HCI practition-ers can use to organize existing AI systems/tools as well as identify areas requiring further attention. Our focus centers on three key themes.\n\u2022 GenAI opportunities in data analysis. We explore how GenAI systems can support each individual stage of the data analysis process. While the utilization of LLMs for code generation in tasks like data cleaning, transformation, and visualization is well-known, we delve into additional avenues such as aiding users in finding required data, ensuring analyses maintain sta-tistical rigor, assisting in hypothesis exploration and refinement, and personalized report generation (see section 4).\n\u2022 Human-driven design considerations. The design of these Al-powered tools and their user interfaces can have a huge impact on user experiences. For instance, some interfaces facilitate a more intuitive approach for users to specify intents such as selecting the color of a chart using a color picker widget vs. using nat-ural language description. Similarly, distinguishing between seemingly correct but analytically different charts poses a challenge for users, especially without additional interface showing provenance analysis. Con-cretely, we discuss various design considerations for reducing users' intent specification efforts, enhancing users' ability to understand and verify system outputs, and streamlining the analysis workflow to reduce it-eration overhead across multiple steps and tools (see section 5).\n\u2022 Research challenges ahead of us. Finally, we con-clude the paper by discussing the research challenges that must be addressed to implement AI-powered data analysis systems. These challenges include enhancing existing models' capabilities, addressing the scarcity of training and evaluation data, ensuring system reli-ability and stability of the data analysis process, and conducting user research to align system designs to meet users' cognitive abilities and practical needs (see Figure 6)."}, {"title": "2. Background", "content": ""}, {"title": "2.1. The data analysis process", "content": "Figure 1 and Table 2 show some of the common steps in-volved in deriving insights from data. This process has mul-tiple steps and often involves iterating back and forth among these steps. The process usually starts with task formulation, where the problem, question, or decision to be addressed is identified (e.g. What's the trend of renewable energy adop-tion over the years for different countries?). This step also involves operationalizing the task into sub-tasks (such as figuring out global trends over time, ranking of countries, and compare trends among top countries). Then comes the task of collecting relevant data to answer the above ques-tions. This might involve actions like querying databases, instrumenting applications, web scraping, data cleaning, and integration from multiple sources. The sequence of these steps can vary; analysts may already possess data and seek insights or may identify data gaps or data cleaning needs later in the analysis.\nFollowing data collection, the exploration phase begins, where analysts delve into the dataset to gain familiarity and identify potential patterns or relationships. This involves forming further sub-tasks such as generating descriptive statistics (e.g., mean, median, and count), visualizing the relationship between several variables, trends, and corre-lations, and generating hypotheses to explore. Then, they transform data or generate visualization artifacts to help sup-port their hypotheses. Once artifacts (transformed data and charts) are generated, one has to understand them and val-idate them to ensure the reliability and accuracy of their findings. This may involve statistical analysis, domain knowledge-based validation, or comparison with external sources. The insights gathered from these artifacts are then used to refine the hypotheses accordingly.\nFinally, analysts communicate their findings and generate the final results. This may involve creating reports, dash-boards, or presentations to effectively communicate insights to stakeholders. Analysts may iterate on their communica-tion based on the feedback received, further refining their presentation of the data and insights.\nThe data analysis process is inherently complex and multi-faceted, characterized by its iterative nature and the diverse set of skills required at each stage. Each step demands a dis-tinct set of domain knowledge, statistical expertise, coding proficiency, and familiarity with various tools. Moreover, the fragmented nature of the process requires analysts to continuously juggle between different tools and methodolo-gies to derive meaningful insights.\nPlease note that the steps outlined above are not exhaustive. They do not cover areas such as model building, automated machine learning, collaborative authoring, or accessibility. While these topics are crucial to data science and analy-sis, they require specialized attention beyond the scope of this discussion. Instead, we focus on the iterative and ex-ploratory aspects of deriving insights from data and how generative AI can influence this process."}, {"title": "2.2. LLMs, LMMs, and agents", "content": "The research on LLMs, LMMs, and the AI systems that are built using these models has advanced drastically in recent years. On the base level, we have several foundation models starting with language models (ChatGPT, GPT4 (Achiam et al., 2023), Claude (Anthropic, 2024), and Llama (Touvron et al., 2023)) and then recently multimodal models (GPT-40 (OpenAI, 2024), LLava (Liu et al., 2024a), and Phi-3-Vision (Abdin et al., 2024)). These models are well-known for their ability to generate and understand code and have been shown to have good performance in generat-ing algorithms and solutions to interview/competition prob-lems (Li et al., 2022), write real-world software with GitHub copilot (Nguyen & Nadi, 2022), and various data trans-formations and visualization authoring tasks (Dibia, 2023; Narayan et al., 2022). These models are also instruction-tuned to understand and converse with people in a chat-like environment (Ouyang et al., 2022).\nUsing foundation models as a basis, researchers have devel-oped AI agents or AI assistants tailored to different domains. These agents act as interfaces around foundation models, embedding any application-specific knowledge through sys-tem prompts or leveraging external tools like code execu-tion engines, search engines, or calculators to respond to user queries (Lu et al., 2024). For example, a code inter-preter (OpenAI, 2023) is an AI assistant that can generate code, execute it in a sandbox environment, and use the exe-cution results to produce further text or code.\nOn top of all of these, there are also multi-agent AI systems that have a series of agents, each supposed to be specialized in a particular subtask, with all agents acting together to collaborate and respond to a user query (Wu et al., 2023b;"}, {"title": "3. Case study: User experiences with AI-tools for visualization authoring", "content": "Data visualizations are prevalent in different data analysis stages: data analysts create visualizations to assess data quality issues, explore relations between data fields, under-stand data trends and statistical attributes, and communicate their insights from data to the audience. Although mod-ern visualization tools and libraries have greatly eased the requirements of user expertise and effort, the authoring pro-cess remains challenging since the analysts not only need to learn to use visualization tools to implement charts, they also need to transform data into the right format for their chart design.\nWith the emergence of LLMs, AI-powered visualization authoring tools have been developed to reduce the author-ing barrier (Dibia, 2023; Wang et al., 2023a; Maddigan & Susnjak, 2023; Vaithilingam et al., 2024). Here, we use an example visualization task to compare user experiences with (1) traditional programming tools, (2) direct conversational interaction with LLMs via a chat-based interface, and (3) LLM-powered interactive data-analysis tools, highlighting various human-driven design principles which will be dis-cussed later in this paper. We center our comparison around the user experience for (1) specifying the initial intent, (2) consuming the AI system's output, (3) editing, (4) iterating, (5) verifying AI system's output, and (6) the workflow in the larger data analysis context."}, {"title": "3.1. Example task and the traditional experience", "content": "Table 1 shows a sample dataset that shows the CO2 and the electricity generated from different sources for 20 countries between the years 2000 and 2020. Let us assume the user is at a stage in the data analysis pipeline where they want to investigate the trend of percentage of electricity from renewable energy sources over time for the five countries with the highest CO2 emissions.\nThe traditional (non-AI) approach to this task involves two steps: (a) data transformation, where the user creates new columns (e.g., \"percentage of renewable electricity\") and performs operations like grouping, summing, and filtering. More complex tasks may require pivoting, unpivoting, and"}, {"title": "3.2. Conversational LLM interface (ChatGPT with GPT-40 and CodeInterpreter)", "content": "LLMs have made the above process easier for a wide range of users with different experience levels. For this running example, we use ChatGPT's interface with GPT-40 model as the conversational LLM together with CodeInterpreter assistance capability. In this conversational experience, the user loads the dataset and expresses their intent as a natu-ral language command, as shown in Figure 2(a). ChatGPT processes the request and performs several steps such as loading and understanding the dataset, figuring out subtasks, writing code for the subtasks, and executing the code to generate the final chart. ChatGPT shows its steps as code snippets, natural language descriptions, and renders the fi-nal chart for the user (Figure 2(b)). For this task, ChatGPT requires multiple invocations of the underlying model for the different sub-tasks, leading to long wait times for users. Moreover, repeated model invocations increase the likeli-hood of failure, and the system can sometimes get stuck on one of the steps, making it difficult to recover.\nIn conversational LLM interface, users can edit and iterate on their visualizations through follow-up instructions in a conversational manner (see Figure 3 (a)). The chat interface also allows users to backtrack to a previous conversation and edit from there. However, there is no easy way for"}, {"title": "3.3. LLM-powered interactive data analysis tools", "content": "Now, we describe a different user experience for the above case study using a few selected interactive LLM-powered data analysis tools. Some systems and experiences rely on the capabilities of LLMs to support data analysis, while departing from the types of chat-like experience described above. First, for specifying the user's intent, Data Formula-tor (Wang et al., 2023a) has devised a multi-modal UI that allows users to specify the intent using a combination of field-encodings and natural language (NL) instructions (Fig-ure 2(c)). Based on the user's multi-modal inputs, an LLM model generates the necessary data transformations to create a new table. Using this table and the user's specified chart encodings, a Vega-Lite specification can then be straight-forwardly generated to produce the chart. The chart is then rendered along with options for users to read/edit code, read code explanations, and view the newly transformed data ta-ble (Figure 2 (d)). This multi-model UI allows users to have more control on the type/orientation/high-level details of the chart they want to create but also offload low-level tasks, such as performing the necessary transformations needed for a new concept, to the LLM. From the multi-modal in-put, the model also has more information to ground the user's instruction for better code generation. For instance, the system can generate a Vega-Lite specification without requiring a separate model invocation. Moreover, this AI system is tailored for data visualization tasks, incorporating components for data processing and summarization, along with domain-specific prompts and instructions for LLMs. These features enhance reliability and reduce latency in the AI-driven data visualization authoring process compared to the experience with the ChatGPT interface.\nDynaVis (Vaithilingam et al., 2024) is another interactive AI tool that provides a multi-modal UI for users to specify their chart editing intents. Based on the high-level intent specified in natural language, such as \u201cI want to change the colors of this chart,\u201d the system dynamically generates widgets (e.g., color pickers) on the fly, allowing users to try out several similar options (see Figure 3(b)). The system only calls the LLM when initially generating the widget, not for every change requested by the widget, enabling users to experiment with many variants and receive instant feedback for their edits. These intent-based widgets help users specify intents that are difficult to express in natural language and give users a sense of control and trust by allowing them to easily explore various chart editing options.\nData Formulator2 (Wang et al., 2024a) extends Data Formu-lator (Wang et al., 2023a) by supporting iterative exploration as a first-class interaction. It allows users to pick any ex-isting visualization and express their modification intent again through the multi-modal UI from Figure 2 (c). The system's agents reuse previously generated code and data to generate new results, thus giving a consistent experience to users. The system organizes the user's interaction history as data threads (see Figure 3 (c)) to help the user manage the analysis session. Data threads enable users to easily locate existing plots for refinement, branch out from previous steps to explore alternatives, or backtrack to correct mistakes."}, {"title": "3.4. Remarks", "content": "In our comparative analysis of user experiences with vari-ous AI tools for visualization authoring, we observe that the design of these tools significantly impacts users' ability to create the desired visualizations. These design considera-tions are critical not only for visualization authoring tools but also for AI tools across the different stages of data analy-sis. In section 5, we elaborate on these design principles and explore various strategies-beyond those examined in this case study-to alleviate user specification burdens, enhance trust and verification strategies, and facilitate workflows across multiple stages and tools.\nBefore delving into these design principles, section 4 will investigate additional AI opportunities within the broader data analysis landscape, extending beyond visualization authoring."}, {"title": "4. Opportunities for AI systems in the data analysis domain", "content": "In this section, we explore how AI systems can help users navigate the complexities of data analysis workflows. Each stage of the analysis process presents unique challenges, from complex reasoning tasks and iterative processes to the need for diverse skills in coding, domain knowledge, and scientific methodologies across multiple tools. To address these challenges, AI systems offer numerous opportunities to streamline and enhance these workflows.\nThe goal of this section is to establish some concrete prob-lem definitions for a few potential AI-driven workflows in the data-analysis domain (i.e. establishing user's require-ments and expectations from the AI system). While there is an aspiration to automate the entire analysis process, it is imperative to dissect the process into its sub-tasks to identify areas where AI systems can make a significant impact and where they may fall short. This way of solving a complex problem by breaking into multiple sub-tasks also helps users evaluate the intermediate outputs of the AI systems and to intervene and iterate when necessary, thus helping to ensure the reliability and effectiveness of the overall process.\nOnce we establish the opportunity and its impact on users, we do a literature review of studies that are related or useful for that particular opportunity. This is not an exhaustive set of opportunities or related works, but rather an preliminary overview aimed at establishing a foundation on which future researchers can build upon."}, {"title": "4.1. Closing the skills gap: Empowering users in data analysis", "content": "A data analysis task can require diverse skills including domain knowledge, data science knowledge, coding, as well as knowledge of and how to use tools. The need to possess this diverse skill set often acts as a barrier for non-expert individuals, hindering their ability to engage in effective data analysis and fully realize its benefits. LLMs bring new opportunities to mitigate this barrier by supplementing the existing expertise of users with complementary skills.\nLow code/no code experiences. During a data analysis process, once users identify a specific next step, such as visualizing temporal trends, they still face the additional challenge of writing code or using specialized tools to ac-complish this objective. For non-programmers, LLMs can alleviate this burden by aiding users in seamlessly transi-tioning from intent to automatically generating the required code.\nLLMs have shown great proficiency in generating code snip-pets for data analysis from natural language descriptions, especially in languages such as Python. There are several AI systems for taking a user's specification in natural language or other forms and generating code for various sub-stages such as data cleaning, transformations, querying databases, and visualization authoring.\nLIDA (Dibia, 2023) is an open-source tool that enables goal-based data visualization generation by generating, refin-ing, filtering, and executing code to produce visualizations. It also includes an infographer module that creates data-faithful, stylized graphics using image generation models. The system supports multiple Python libraries, including Matplotlib, Seaborn, and Altair, for visualization genera-tion. ChatGPT's code interpreter API (Cheng et al., 2023) lets users upload their dataset and pose queries in natural language (e.g., analyze the trend of renewable energy over time). Then the model generates a code snippet for this task and executes it to generate a plot. Chat2Vis (Maddigan & Susnjak, 2023) and ChartGPT (Tian et al., 2024) are some"}, {"title": "4.2. Potential AI workflows for the different stages of data analysis process", "content": "Task formulation. One of the first steps of the analysis process is to formulate high-level goals for the analysis task and refine them into specific actionable tasks. Often, people face a cold-start problem, where they are unsure of what kind of analysis to perform or what insights they want to gain from their dataset. In such cases, AI systems can assist by leveraging domain-knowledge to understand the dataset and formulate meaningful questions. LIDA's goal explorer module (Dibia, 2023) addresses this problem by generating high-level goals grounded in data and an optional persona. Each hypothesis is generated as a triplet - goal (e.g., Understand trends in CO2) a visualization that addresses the hypothesis (e.g., Line chart of CO2 per year) and a rationale on why the goal is informative.\nLLMs can also assist in translating high-level user require-ments (e.g., evaluating a company's performance) into spe-cific and actionable tasks (e.g., assessing revenue, employee satisfaction, etc.) again by leveraging the domain knowl-edge. Some AI systems to refine fuzzy specifications include InsightPilot (Ma et al., 2023) takes a vague user specifica-tion and uses an LLM to issue concrete analysis actions that are then given to a dedicated insight engine to explore data and generate insights. NL4DV Toolkit (Narechania et al., 2020) is another tool that generates analytic specifi-cations for data visualization from natural language queries by breaking down an input query into attributes, tasks, and visualizations.\nIn addition to assisting with goal formulation and refining user tasks, AI systems can further enhance the analysis pro-cess by drawing inspiration from existing examples. People sometimes get inspiration from existing charts and data anal-ysis (Bako et al., 2022). Similarly, AI systems can also assist by identifying similar datasets and questions, and their data analysis tasks and use them to guide the user into formulat-ing their own task for their dataset. This draws some sim-ilarities to Retrieval Augmented Generation (RAG) based approaches that are commonly used to overcome a model's limited prompt window size by selecting relevant sources from a vast body of external knowledge (Lewis et al., 2020; Izacard et al., 2022; Peng et al., 2023; Lu et al., 2024) and also few-shot learning approaches with examples chosen based on a similarity metric (Poesia et al., 2022; Li et al., 2023b; Liu et al., 2021).\nHypothesis exploration. Another avenue for AI systems to assist is by reducing the risk of biased data analysis through the support for multiverse analysis, wherein various analytical approaches are systematically examined. This helps users evaluate the strength of conclusions and im-proves transparency in decision-making by presenting all potential outcomes. Boba (Liu et al., 2020) is an early pre-LLM system to simplify multiverse analysis in data science by letting analysts write shared code with local variations, generating multiple analysis paths.\nAutomated hypothesis exploration, while highly beneficial, poses significant challenges due to the need to mitigate exponential blow-up of hypothesis space. Hence, it is an interesting research opportunity for AI systems to carefully combine domain knowledge and iterative exploration and at the same time avoid LLM's own inherent biases.\nExecution + Authoring. Given a concrete goal, the next step is to generate the code necessary for any data transfor-mations and for configuring the charts for the visualization. There are numerous works on automating this process with LLMs and we refer to (Dibia, 2023; Wang et al., 2023a; Maddigan & Susnjak, 2023; Shen et al., 2022) for a more thorough exploration of these works.\nValidation, insight generation, and refinement. Once data analysis artifacts (charts and new data representations) are created, it is now time to inspect them carefully. This in-spection serves two primary purposes. The first is to validate the analysis and assumptions against general expectations derived from domain knowledge. The second is to extract insights that can inform downstream iterative analysis and decision-making processes. Vision language models such as GPT-V and Phi-3-Vision, have shown a lot of potential in analyzing images, charts, and figures. This gives us the opportunity to use these models to reason about the visual-izations and generate insights from them.\nLLMs and LMMs have been used to evaluate visualizations (or their representations). LIDA (Dibia, 2023) uses an LLM to examine the generated code for a visualization and judge it based on a set of predefined criteria. ChartQA (Masry et al., 2022), PlotQA (Methani et al., 2020) datasets are established to test LMM performance in answering charts-based questions. There are several models (Han et al., 2023; Masry et al., 2023) and approaches (Obeid & Hoque, 2020; Huang et al., 2024b) in the literature for using LLMs and LMMs to understand and gather insights from charts. LLMs have also been instrumental in consolidating insights from various analysis steps to construct a cohesive and compre-hensive narrative (Zhao et al., 2021; Lin et al., 2023; Weng et al., 2024).\nFinally, these chart understanding capabilities of vision mod-els can be used to automatically iterate and refine hypothe-ses. For example, for regression analysis, the AI system can fit various curves to the scatter plot of the data, analyze the goodness of fit for each, and try other types of curves based on the initial analysis. This also draws similarity to LLM-based self-repairing for code (Olausson et al., 2023; Le et al., 2023) works where LLMs reason about their own generated code and refine them further.\nIt is important to note that the user might still play an in-strumental role in validation of the AI assistant's output. Gu et al., (Gu et al., 2024b) found that providing a combina-tion of a high-level explanation of the steps that the AI is taking, the actual code, as well as intermediate data arti-facts is important to help resolve ambiguous statements, AI misunderstandings of the data, conceptual errors, or simply erroneous Al generated code. We discuss more on user-verification strategies in subsection 5.2.\nData discovery. In many data analysis workflows that leverage AI, it is often assumed that users have access to all the necessary data. However, ongoing data analysis may reveal the need for additional data to enhance the depth and accuracy of the analysis. For example, an analysis fo-cusing on county-based statistics may require population data to perform proper averaging. Alternatively, some users"}, {"title": "Personalized reports, sharing, and creativity.", "content": "One of the final steps of the data analysis workflow is to create dashboards, reports, and presentations to communicate the results and insights to stakeholders. Current tools like PowerBI, Tableau, and Powerpoint require significant pro-ficiency and development effort. Some ways AI systems can assist here are by facilitating the generation of dash-boards and what-if analyses with minimal developer involve-ment (Pandey et al., 2022; Wu et al., 2023a; Sultanum et al., 2021). AI systems also bring the opportunity to transform static elements, such as papers and documents, into inter-active versions (Dragicevic et al., 2019; Heer et al., 2023). Another example is Microsoft Office copilots that answer questions on the side about the document in an interactive manner.\nFurthermore, AI systems, particularly LLMs, can tailor the content and format of reports to suit various audiences and devices by dynamically adjusting tone, detail level, and em-phasis. Another aspect in which LLMs and other types of GenAI models such as DALL-E and Stable Diffusion can help is by creating artistic and aesthetically pleasing charts and infographics (Dibia, 2023; Schetinger et al., 2023). By interpreting the underlying patterns and insights in a dataset, GenAI models can craft visually representative charts that not only communicate information effectively but also evoke a sense of creativity and design. For example, for a chart regarding global warming trends, we can create an info-graphic with a melting ice metaphor to convey the urgency and severity of climate change.\nIn general, creating interactive and personalized artifacts for communication is challenging, as it involves linking various forms of media such as static text, images, videos, and animations, as well as employing diverse interaction techniques such as details-on-demand, drag-and-drop func-tionality, scroll-based interactions, hover effects, responsive design, and gamification to enrich communication (Hohman et al., 2020). The vast space of potential interactive de-signs presents an interesting problem for AI-based systems, which could generate these artifacts by understanding user preferences and intentions and executing them accordingly."}, {"title": "5. Human-driven design considerations for AI-based data analysis systems", "content": "In the previous section, we explored how AI systems can impact various workflows within the data analysis pipeline. However, the user experience of these AI-powered work-flows varies significantly based on several design considera-tions. As Hutchins et al., (Hutchins et al., 1985) highlighted, two main challenges users must overcome when interacting with technology are (a) Execution: taking action to accom-plish a particular goal, and (b) Evaluation: understanding the state of the system. While GenAI alters the notions of"}, {"title": "5.1. Enhancing user-AI interaction: For natural intent communication", "content": "Relying solely on natural language based interfaces can pose challenges for users of AI-based data analysis systems, as expressing complex intents can become lengthy or difficult due to limited expressiveness. For instance, altering the legend placement in a chart requires users to acquire the literacy to talk about charts (in this case, learn the word \"legend\") so that an AI agent would unambiguously under-stand. Similarly, for complex tasks, users might find it necessary/beneficial to interact with intermediate artifacts, explore variations, and integrate them gradually, rather than specifying everything at once. Below we discuss some po-tential ways AI systems can enable natural modes \u00b9 for users to communicate their intent.\nMulti-modal inputs. Beyond chat and natural language interaction, there are other modalities like mouse-based operations, pen and touch, GUI based interactions, audio, and gestures that could provide a more powerful and easier way for users to provide their intents.\nFor example, Data Formulator (Wang et al., 2023a) is an AI system that supports multi-modal inputs. Data Formulator combines graphical widgets and natural language inputs to let users specify their intent more clearly with less over-head compared to using just natural language interface (Fig-ure 2(c)). Data Formulator provides a shelf-configuration user interface (Grammel et al., 2013) for the user to spec-ify visual encodings, and the user only needs to provide short descriptions to augment UI inputs to explain chart semantics. In Data Formulator, the shelf is pre-populated based on the current columns in the dataset, but one could also imagine auto-generating additional concepts on the fly based on the task and context and let users choose the right concepts rather than fully specifying them in natural lan-guage. DirectGPT (Masson et al., 2024) is another system that allows users to specify parts of the intent by clicking, highlighting, and manipulating its canvas. This type of di-rect manipulation has been previously explored for image generation (Dang et al., 2022). InternGPT (Liu et al., 2023d) combines chatbots like ChatGPT with non-verbal instruc-tions like pointing movements that enable users to directly manipulate images or videos on the screen.\nMultimodality can also surface as demonstrations from the users. Pure demonstration-based inputs have been explored before (Barman et al., 2016), but combining demonstra-tions with natural language can be complex. For example, ONYX (Ruoff et al., 2023) is an intelligent agent that in-teractively learns to interpret new natural language utter-ances by combining natural language programming and programming-by-demonstration. Sketching or in"}]}