{"title": "Trustworthiness in Retrieval-Augmented\nGeneration Systems: A Survey", "authors": ["Yujia Zhou", "Yan Liu", "Xiaoxi Li", "Jiajie Jin", "Hongjin Qian", "Zheng Liu", "Chaozhuo Li", "Zhicheng Dou", "Tsung-Yi Ho", "Philip S. Yu"], "abstract": "Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal paradigm in the development of Large Language\nModels (LLMs). While much of the current research in this field focuses on performance optimization, particularly in terms of accuracy\nand efficiency, the trustworthiness of RAG systems remains an area still under exploration. From a positive perspective, RAG systems\nare promising to enhance LLMs by providing them with useful and up-to-date knowledge from vast external databases, thereby mitigating\nthe long-standing problem of hallucination. While from a negative perspective, RAG systems are at the risk of generating undesirable\ncontents if the retrieved information is either inappropriate or poorly utilized. To address these concerns, we propose a unified\nframework that assesses the trustworthiness of RAG systems across six key dimensions: factuality, robustness, fairness, transparency,\naccountability, and privacy. Within this framework, we thoroughly review the existing literature on each dimension. Additionally, we create\nthe evaluation benchmark regarding the six dimensions and conduct comprehensive evaluations for a variety of proprietary and open-\nsource models. Finally, we identify the potential challenges for future research based on our investigation results. Through this work,\nwe aim to lay a structured foundation for future investigations and provide practical insights for enhancing the trustworthiness of RAG\nsystems in real-world applications.", "sections": [{"title": "1 INTRODUCTION", "content": "THE emergence of Large Language Models (LLMs) repre-\nsents a significant advancement in artificial intelligence,\nparticularly in natural language processing (NLP) and com-\nprehension. Over time, these models have evolved from\nsimple rule-based systems to sophisticated deep learning\narchitectures, driven by innovations like the transformer\narchitecture [1], extensive pre-training on diverse datasets,\nand advanced fine-tuning techniques [2]. These advance-\nments have greatly enhanced LLM capabilities, impacting\napplications such as automated content generation [3] and\nadvanced language translation [4], thereby transforming\nmachine interpretation and generation of human language.\nDespite these advancements, LLMs face the persistent\nchallenge of hallucination, where models produce plausible\nbut incorrect or nonsensical information [5, 6]. Hallucina-\ntions arise from factors such as biases in training data [7] and\nthe probabilistic nature of language models [8]. This issue is\ncritical in contexts requiring high precision and reliability,\nsuch as medical and legal applications [9]. To mitigate this,\nRetrieval-Augmented Generation (RAG) systems have been\ndeveloped [10]. RAG systems integrate external information\nretrieval mechanisms to ensure that generated content is\nbased on factual data, thus improving the accuracy and\ncredibility of LLM outputs [11].\nThe trustworthiness of LLMs has become a critical con-\ncern as these systems are increasingly integrated into appli-\ncations such as financial systems [12] and healthcare [13].\nTrustworthiness, as outlined in various frameworks, is eval-\nuated across multiple key dimensions, including truthful-\nness, safety, fairness, robustness, privacy, machine ethics,\ntransparency, and accountability [14]. These dimensions\nensure that LLMs provide accurate, unbiased, and safe\noutputs while protecting user privacy and aligning with\nethical standards [15]. Techniques like reinforcement learn-\ning from human feedback (RLHF)[16], data filtering[17], and\nadversarial training [18] have been employed to improve\ntrustworthiness, with proprietary models such as GPT-4\ngenerally outperforming open-source alternatives in certain\nhigh-stakes applications [19]. As LLMs continue to influence\nkey societal functions, ongoing research and transparent,\ncollaborative efforts between academia and industry are\nessential to ensure their reliable and ethical deployment [20].\nHowever, research on RAG systems predominantly fo-\ncuses on optimizing the retriever and generator compo-\nnents, as well as refining their interaction strategies [3, 21].\nThere is a significant gap in the attention given to the\ntrustworthiness of these systems [22]. Trustworthiness is\ncrucial for the practical deployment of RAG systems, es-\npecially in high-stakes or sensitive applications like legal\nadvising or healthcare, where errors could have serious con-\nsequences [23]. Therefore, it is essential to identify the key\nelements that define the trustworthiness of RAG systems"}, {"title": "1", "content": "and to develop methodologies to evaluate trustworthiness\nacross these dimensions [24]. Two main challenges arise\nin this context: (1) Defining a comprehensive framework\nthat captures all relevant aspects of trustworthiness in RAG\nsystems, and (2) Designing practical and robust evaluation\nmethodologies that can effectively measure trustworthiness\nacross these identified dimensions [25].\nTo address these challenges, we propose a unified frame-\nwork that supports a comprehensive analysis of trustworthi-\nness in RAG systems, including three key parts:\nDefination of six key dimensions of trustworthiness in\nthe RAG context: As shown in Figure 1, we define trust-\nworthiness across six dimensions: (1) Factuality: Ensuring\nthe accuracy and truthfulness of generated information by\nverifying it against reliable sources. (2) Robustness: Ensur-\ning the system's reliability against errors, adversarial at-\ntacks, and other external threats. (3) Fairness: Minimizing\nbiases in retrieval and generation stages to ensure fair out-\ncomes. (4) Transparency: Making RAG system processes\nand decisions clear and understandable to users, fostering\ntrust and accountability. (5) Accountability: Implementing\nmechanisms to ensure the system's actions and outputs\nare responsible and traceable. (6) Privacy: Protecting per-\nsonal data and user privacy throughout retrieval and\ngeneration processes.\nSurvey of existing work: We involves a thorough review\nof the current literature and research efforts related to\ntrustworthiness in RAG systems. We analyze various ap-\nproaches, methodologies, and techniques that have been\nproposed or implemented to enhance trustworthiness\nacross the six key dimensions.\nBenchmarking and assessment on various LLMs: To pro-\nvide a practical evaluation of trustworthiness in RAG sys-\ntems, we construct a benchmark and establish a compre-\nhsive evaluation framework. This framework assesses\nthe trustworthiness of 10 different LLMs, including both\nproprietary and open-source models covering various\nmodel sizes and training strategies. This benchmark offers\nvaluable insights into the performance on trustworthiness\nof different models in real-world applications.\nThe contributions of this survey are threefold: (1) We\nintroduce a unified framework which defines six key dimen-\nsions of trustworthiness in RAG systems. (2) We present a\ndetailed review for the existing literature on RAG trustwor-\nthiness, identifying gaps and highlighting promising ap-\nproaches. (3) We establish a practical benchmarking frame-\nwork and make comprehensive evaluation for 10 LLMs,\noffering actionable insights and guidelines for improving\ntrustworthiness in future RAG system developments."}, {"title": "2 BACKGROUND AND PRELIMINARIES", "content": "In this section, we will introduce the background of RAG\nsystems and the concept of trustworthiness in LLMs.\n2.1 Retrieval-augmented Generation System\nRAG is proposed to enhance generation quality by lever-\naging external knowledge bases. As research progresses,\nRAG technology has undergone three major developmental\nstages: Naive RAG, Advanced RAG, and Modular RAG.\nNaive RAG. Typically, naive RAG follows a \u201cRetrieval-\nthen-Read\" process [21, 26, 27], consisting of a simple re-\ntriever and a pre-trained language model as the genera-\ntor. Its workflow involves two simple steps: (1) retrieving\nrelevant passages from a pre-constructed knowledge base\nbased on the user query, and (2) combining the retrieved\ninformation with the input query to generate a response.\nEarly works primarily focused on optimizing the inte-\ngration of retrievers and generators, including end-to-end\njoint training of retrievers and generators [21, 26], separately\ntraining generators to better utilize retrieved documents\nwith frozen retrievers [3, 28], and modifying the model's\ndecoding methods [10, 29]. With the emergence of LLMs,\nthe capabilities of generative models have significantly ad-\nvanced. To further enhance the quality of generated con-\ntext, prompt engineering have been proposed to optimize\nmodel outputs without additional training. To enhance\nthe model's reasoning capabilities and the robustness of"}, {"title": "3", "content": "responses, various prompting techniques such as Chain-\nof-Thought (CoT)[30], Tree-of-Thought (ToT) [31], and Self-\nConsistency [32] have been proposed. These methods ex-\ntend the number of LLM's reasoning paths, thereby improv-\ning the likelihood of arriving at the correct result during the\ndecoding process. However, Naive RAG also faces certain\nlimitations. Firstly, the retrieved documents may contain\nnoise or irrelevant information, which can interfere with\nthe model's responses [5, 33]. Secondly, the high reasoning\ncost inherent to large models is further exacerbated in the\nRAG process; the inclusion of lengthy retrieved documents\ncan slow down the generation process and consume more\ncomputational resources.\nAdvanced RAG. To tackle the issues discussed earlier,\nadditional components have been added to the RAG pro-\ncess, making it more complex. These enhanced systems,\nknown as Advanced RAG, introduce specialized modules\nat different stages of the retrieval and generation pipeline,\nwhich can be categorized as pre-retrieval and post-retrieval\ncomponents. In the pre-retrieval stage, a common issue is\nthat the original query may be too short or vague, resulting\nin irrelevant retrieval results. To address this, a rewriter is\nintroduced to clarify or expand the query. Rewriting meth-\nods include directly prompting the LLM [34, 35] or training\na rewriter model using feedback from the generator [36]. In\nthe post-retrieval stage, the generator often faces challenges\ndue to the length or noise of the retrieved content, which\ncan affect the generation quality [33, 37]. To mitigate this,\na reranker is used to reorder the retrieval results [38].\nRerankers, often using cross-encoder architectures, better\nmeasure the similarity between the query and retrieved\ndocuments, pushing more relevant documents forward and\nremoving less relevant ones. Another optimization com-\nponent is the refiner, which summarizes or compresses\nretrieved content using techniques like prompting the LLM\nto summarize [39, 40], or training a summarizer through\nsupervised fine-tuning or reinforcement learning [41-43].\nDespite the flexibility of Advanced RAG, its sequential\nstructure limits adaptability in complex scenarios, such as\nqueries requiring step-by-step reasoning.\nModular RAG. As RAG research evolves, it has entered\nthe modular RAG stage, where components are treated as\nflexible modules that can be combined to create customized\npipelines for different scenarios, offering greater flexibil-\nity and adaptability. Research now focuses on optimizing\nthese pipelines, which come in four main types: Sequen-\ntial, Conditional, Branching, and Loop. Sequential Pipelines\nprocess queries linearly, similar to advanced RAG, with\npre-retrieval and post-retrieval stages. Conditional Pipelines\nroute queries along different execution paths based on their\ntype. For instance, SKR [44] identifies queries that the LLM\ncan answer without retrieval, while Adaptive-RAG [45]\nclassifies queries as simple or complex, using multi-round\nretrieval for complex ones. Branching Pipelines execute\nmultiple paths simultaneously for a query, combining the\nresults to form the final output. This can involve aggregating\ngeneration probabilities [10] or generating multiple answers\nand selecting the best [40]. This helps address instability\nin single-path reasoning. Loop Pipelines, the most complex,\ninvolve multiple rounds of interaction between the retriever\nand generator. Techniques like ReAct [46] use prompts to"}, {"title": "3", "content": "generate reasoning paths and search requests, while Self-\nAsk [47] allows the LLM to ask and answer intermediate\nquestions. IRCOT [48] introduces repeated retrieval during\nthe CoT path generation. Other approaches involve models\ndeciding when to retrieve information [49], use external\ntools [50], or access a browser [4]. These modular pipelines,\nwith features like iterative and multi-round retrieval and\nself-correction, create a more intelligent RAG process.\n2.2 Trustworthiness in Large Language Models\nThe rapid development of LLMs has ignited the revolution\nof various industries and domains, such as automatic article\nwriting [51], drug development [9], and even coding [52].\nAs various applications based on LLMs gradually permeate\ndifferent aspects of life, especially critical fields like health-\ncare [13] and finance [12], the trustworthiness of LLMs has\naroused increasing concern and attention. Since LLMs are\ntrained on vast amounts of data collected from sources such\nas the internet [53], and due to the inherent limitations\nof probabilistic models, they have been found to exhibit\nserious issues such as hallucination [54], discrimination [55],\nprivacy breaches [56], and so on. Once applied to real-\nlife situations, these issues with LLMs could lead to very\nserious or even catastrophic consequences, such as further\nexacerbating social injustice or causing harm to property\nand personal safety [57].\nEssentially, these issues of LLM can be attributed from\ntwo perspectives: data and algorithms. From the data per-\nspective, due to the pre-training data coming from multiple\ndata sources, the data quality is uneven and cannot be\nthoroughly cleaned, resulting in LLM remembering incor-\nrect or harmful information during the training process.\nThe pre-training data for LLMs typically come from a va-\nriety of sources to ensure a broad and diverse coverage\nof language content. Main sources include: (1) web data\nwhich are scraped from the internet, including news articles,\nblogs, and forum posts; (2) books that encompass a range of\ngenres such as fiction, non-fiction, textbooks, and technical\nmanuals; (3) wikipedia that covers numerous topics; (4)\nsocial media contents that are collected from social media\nplatforms (like Twitter and Reddit); (5) code repositories\nthat include code and documentation from repositories\nlike GitHub; (6) QA Platforms that aid LLMs in learning\ndialogue and problem-solving skills (like Quora and Stack\nOverflow). Due to the mixed sources of this data, it contains\na lot of harmful information and social biases, including\neven profanity and expressions that insult others. What's\nmore concerning is that some harmful information is not\npresented directly but expressed in a subtle manner, making\nit more difficult to filter out. Additionally, the sheer volume\nof training data makes comprehensive data cleansing im-\npossible. As a result, the model inevitably learns harmful\ninformation from the training data. From the algorithms\nperspective, existing LLMs all use the Transformer archi-\ntecture, with attention mechanisms [1] at its core. Large\nmodels employing this algorithmic structure tend to learn\nshallow correlations during training. For example, they may\nincorrectly associate religious people with terrorist attacks,\nleading to the erroneous generations that view all religious\nindividuals as dangerous people. Due to inherent algorith-\nmic limitations, preventing models from learning harmful"}, {"title": "4", "content": "correlations is a significant challenge for LLMs that use\nattention mechanisms. Additionally, since large models are\nessentially probability prediction models, they often do not\nrespond based on factual situations. Instead, they tend to\ngenerate high-probability statements learned during train-\ning, leading to the issue of hallucinations in LLMs.\nIn addition to these two major root causes of harmful\nbehaviors in LLMs, the technologies derived from applying\nLLMs in real-world scenarios have introduced new chal-\nlenges [57] to the trustworthiness of them. Taking the RAG\ntechnology discussed in this paper as an example, RAG\nretrieves additional knowledge from external databases.\nWhile this process provides the model with more informa-\ntion, it also reintroduces safety issues such as information\nleakage and unfairness. For example, if the information\nretrieved by RAG contains personal privacy information,\nthe augmented output is highly likely to include this sen-\nsitive information, leading to potential information leakage.\nTherefore, in this paper, we focus on the trustworthiness\nproblem of LLMs caused by RAG. We provide a detailed\nanalysis and discussion from six different aspects (factu-\nality, robustness, fairness, transparency, accountability, and\nprivacy), aiming to raise awareness of this critical problem.\n3 TRUSTWORTHY RAG SYSTEM\nA complete RAG system involves three main stages: the\ninjection of external knowledge into the generator, the gen-\neration of answers by the generator, and the evaluation\nof the generated answers. Each of these stages presents\nchallenges related to trustworthiness. During the external\nknowledge injection phase, there is a risk of injecting noisy\nor private information. In the answer generation phase,\nthe introduction of external knowledge may lead to biased\nreasoning and compromise the alignment achieved through\nRLHF. Finally, during the answer evaluation phase, the gen-\nerated answers may contain factual errors or lack sufficient\ngrounding in the external knowledge.\nAs illustrated in Figure 2, we identify six essential di-\nmensions of trustworthiness in a RAG system: Robustness,\nFairness, Factuality, Privacy, Transparency, and Account-\nability. For each of these dimensions, we will explore the\nfollowing aspects: a general definition applicable to LLMs,\na specific definition within the RAG context, and a thorough\nliterature review. To provide a clearer categorization and\nsummary of the relevant research, we first present a timeline\nof these studies in Figure 3 to identify trends in the field.\nThen, in Table 1, we categorize each study based on three\ncriteria: dimension of trustworthiness, method type, and ob-\nject. The following sections will delve into each dimension\nof trustworthiness in greater detail.\n3.1 Factuality\n3.1.1 General Definition for LLMs\nFactuality is the most critical capability of language models,\ndirectly determining the reliability and usability of their out-\nputs. In the context of LLMs, factuality refers to whether the\nmodel's output containing accurate facts and information.\nThe key aspects of factuality include:\nTruthfulness: The generated information must align-\ning with real-world facts, figures, and events, and the"}, {"title": "4", "content": "model should avoide providing any fiction or misinfor-\nmation into response.\nLogical Consistency: The content should maintain logi-\ncal correctness, ensuring coherence within and between\nsentences, preventing self-contradictions and errors.\nFor example, if a hypothesis is mentioned in the previ-\nous content, the following content needs to be written\nunder this hypothesis and cannot be contradictory.\nTemporal Awareness: It should account for temporal\nchanges in given information and it's own knowledge,\nand reflect the latest or specified state of facts at a given\ntime. If the knowledge can only be provided at a certain\npoint in time, special explanations are needed to avoid\nmisleading users.\nConsistency with instructions: Model responses must\nadhere to the provided instructions, avoiding irrelevant\ninformation, even if correct.\nSince the applications of LLMs are mostly based on a\nfactual and reliable output, substantial research works have\nbeen proposed to evaluating and enhancing the factuality. In\nfacutality evaluation, studies have introduced benchmarks\nspecifically designed for assessing factuality, along with au-\ntomated evaluation methods. To improve LLMs' factuality,\nsome approaches optimize the training process, including\npretraining and supervised fine-tuning stages. There are\nalso some works that further optimize the model after train-\ning, leveraging knowledge editing or specialized decoding\ntechniques to augment the factual accuracy of generated\ncontent.\n3.1.2 Factuality in RAG Systems\nIn vanilla generation processes, LLMs rely on the inter-\nnal knowledge they've learned during training to gener-\nate response, making factuality a direct measure of the\nmodel's own knowledge. However, in RAG scenarios, a\nlarge amount of retrieved content is fed into the input,\nwhich results in additional implications and challenges for\nLLMs. This expanded definition of factuality requires the\nmodel to synthesize both internal and external knowledge\nto produce factually responses. Under these circumstances,\nunique challenges arise:\nConflicts Between Internal and External Knowledge:\nThe model's internal knowledge is based on patterns\nlearned from the training data, while retrieved external\nknowledge comes directly from reliable documents.\nWhen these sources provide conflicting information on\nthe same topic, the model must discern and prioritize\nthe more accurate source. Failing to do so can result\nin factual inaccuracies or logitic errors in the gener-\nated content. For example, for current events or news\nthat evolve over time information, the model's internal\nknowledge may be outdated, necessitating the use of\nupdated external knowledge.\nNoise in Retrieved Documents: Since retrieval sys-\ntems are imperfect, retrieved documents often contain\nconsiderable noise, such as outdated information, con-\ntextually mismatched irrelevant details, or differently\nphrased redundant information. Such noise can erro-\nneously steer the model's responses, directly affecting\nthe accuracy of the generation and mislead the model's\noutput."}, {"title": "5", "content": "Handling Long Contexts In RAG settings, models con-\nfront substantial hurdles in deeply understanding and\nreasoning over extensive, structurally complex long-\ncontext information. Longer documents demand en-\nhanced information filtering and comprehension capa-\nbilities from the model to avoid missing crucial details.\nMoreover, long texts typically involve intricate contexts\nand multiple documents, requiring the model to not\nonly understand individual sentences but also grasp the\noverall logic and inter-document information. In multi-\nhop questions, ensuring the accuracy of the generated\nfacts necessitates inference based on multiple pieces of\ninformation.\nAddressing these challenges is crucial for improving\nthe factuality of LLMs in RAG scenarios, ensuring that\nthey can reliably generate accurate, coherent, and up-to-\ndate information even when faced with complex inputs\nand external knowledge sources. This require advancements\nin how models handle and integrate diverse information,\nmanage contradictions, and filter out noise to produce high-\nquality outputs.\n3.1.3 Representative Studies\nTo address the issues outlined earlier, recent studies have\nfocused on two primary areas to improve the factuality of\nresponses generated in RAG environments:\nBetter Integration of Internal and External Knowledge:\nThe separation between retrieval systems and generative\nmodels can lead to conflicts between internal and external\nknowledge, hindering the model's ability to understand and\nutilize external information effectively. Early works attempt\nto mitigate this issue through optimizing the generative\nmodel or jointly training both components. RETRO [28]\nintroduces a chunked cross-attention architecture designed\nto better integrate information from retrieval documents\nwith the instruction and internal parameters of the model.\nAtlas [21] co-trains the retriever and generator, optimizes\nthe retriever using supervision signals from the language\nmodel. Fusion-in-decoder [3] techniques allows document"}, {"title": "5", "content": "attention scores to feedback into the retriever's ranking\nmechanism, demonstrating that specialized pre-training en-\nables models to leverage external knowledge efficiently with\nminimal training examples.\nAs LLMs have grown in size, previous retrieval-\nenhanced paradigms have become inefficient. SAIL [58]\nexplores instruction-tuning to fine-tune generative models\nfor enhanced factuality. By instruction-tuning on search-\naugmented prompts, models can distinguish between mis-\nleading and relevant information within complex retrieval\ndocuments, significantly boosting factual accuracy. Their ex-\nperiments show that smaller models trained in this manner\ncan outperform commercial models like ChatGPT in terms\nof factual generation.\nReplug [10] explores a novel method for black-box\nmodels. It separately concatenates each search document\nwith the query one by one to create different generation\npaths. Then, it merges the token distributions from these\npaths to produce the final output. This approach avoids\nthe challenges of handling multiple documents at once and\nbypasses context limitations in LLMs.\nPeng et al. [59] introduces a plug-and-play module to\nenhance the factual accuracy of model responses, evaluating\nthe response's reliability and providing feedback for refine-\nment. Zhang et al. [8], Yu et al. [60] prompt LLMs to generate\nrelated documents based on their own knowledge, explicitly\nextracting internal knowledge to facilitate conflict resolution\nand information fusion.\nAdaptive Retrieval: Traditional RAG methods often\nstruggle with insufficiently refined queries that fail to re-\ntrieve highly relevant documents. Adaptive retrieval strate-\ngies have been proposed to dynamically fetch necessary\ncontent.\nSelf-Ask [47] employs prompts to progressively decom-\npose complex queries into subqueries, and addressing each\none through retrieval and response. This method ensures\nmore precise knowledge retrieval, reducing noise and sim-\nplifying the model's task of answering complex questions.\nReAct [46] treats the generative model as an agent capable"}, {"title": "6", "content": "of dynamically choosing thoughts and actions. Through\nprompting, the model generates an expanded query and\nplans subsequent steps, capitalizing on its own query design\nabilities for flexibility throughout the process.\nFLARE [25] adapts retrieval based on model output\nconfidence. The system will do retrieve when confidence\nis low to enhance factual accuracy, while relying on internal\nknowledge to generate when confidence is high. This has\nproven effective in long-form qa, ensuring sentence-level\nfactuality.\nIRCOT [48] integrates chain-of-thought reasoning with\nthe retrieval process, guiding the model to sequentially\ngenerate a reasoning path and determine what knowledge is\nneeded at each step. Self-RAG [49] combines self-reflection\nwith dynamic retrieval, generating tokens to indicate re-\ntrieval necessity and selecting the most informative docu-\nment autonomously, avoiding the introduction of irrelevant\ndocuments. Experimental results demonstrate the genera-\ntion improvements in factual accuracy and response quality\nacross various tasks.\nThese advancements aim to refine RAG systems' abil-\nity to generate factually accurate responses by improving\nintegration and utilization of external knowledge and dy-\nnamically adapting retrieval strategies to better meet the\ndemands of complex information-seeking tasks.\n3.2 Robustness\n3.2.1 General Definition for LLMs\nRobustness in the context of LLMs refers to their capacity\nto maintain stable and reliable performance across diverse\ninput conditions and operational environments. Key aspects\nof robustness for LLMs include:\nInput Diversity: The ability of LLMs to interpret and\nrespond accurately to a wide range of inputs that vary\nin style, structure, and complexity.\nNoise Tolerance: The capacity of the model to under-\nstand and process inputs that include errors, irrelevant\ninformation, or distortions without significant degrada-\ntion in performance.\nAdversarial Resistance: The capability to withstand in-\ntentional manipulations or attacks designed to deceive\nor mislead the model.\nData Distribution Shifts: The need for LLMs to per-\nform reliably when encountering data that differ sig-\nificantly from the training set, reflecting real-world\nscenarios where data characteristics can evolve over\ntime.\nPrevious studies have extensively researched the robust-\nness of traditional language models, focusing on how to\nevaluate and enhance their robustness [88-90]. In recent\nyears, many studies have specifically explored the robust\nness of LLMs [18, 91, 92]. These studies highlight that most\nexisting LLMs struggle to resist adversarial prompts, under-\nscoring the need for continued research and development in\nthis area."}, {"title": "6", "content": "3.2.2 Robustness in RAG Systems\nIn the context of RAG, robustness refers to the ability of\nLLMs to consistently extract and utilize relevant knowledge\nwhen presented with varying retrieval information inputs.\nSpecifically, we define the robustness of LLMs in RAG\nscenarios through the following three dimensions:\nSignal-to-Noise Ratio in Retrieved Information: Ro-\nbustness in RAG involves the model's ability to dis-\ntinguish and prioritize relevant information from re-\ntrieved documents that may contain a mix of useful\ndata and noise. The model should effectively filter out\nirrelevant content and focus on relevant information to\ngenerate accurate and coherent responses.\nGranularity of Retrieved Information: This dimension\nexamines how well the LLM can handle information at\ndifferent levels of detail. Robust models should seam-\nlessly integrate fine-grained details and broader contex-\ntual information from retrieved documents, adapting\ntheir responses based on the required specificity.\nOrder of Retrieved Information: Robust LLMs should\nmaintain performance regardless of the sequence in\nwhich the information is retrieved. The ability to pro-\ncess and synthesize information accurately, irrespective\nof its order, is crucial for ensuring the reliability of\ngenerated content in dynamic retrieval scenarios.\nMisinformation in Retrieved Content: Robustness in\nRAG systems requires the ability to detect and manage\nmisinformation within retrieved documents. The model\nshould effectively identify and exclude inaccurate or\nmisleading information from its responses, ensuring the\ngenerated content remains accurate and trustworthy.\nBuilding on the general definition of robustness for\nLLMs, these dimensions emphasize the model's capacity to\nhandle diverse, noisy, and variably ordered inputs, which\nare typical in real-world RAG applications.\n3.2.3 Representative Studies\nCorruption Attacks. In recent years, the increasing so-\nphistication of misinformation attacks has posed significant\nchallenges to the robustness of automated fact-checking and\nRAG systems. These attacks exploit vulnerabilities in natu-\nral language generation and LLMs to degrade the perfor-\nmance and reliability of information-intensive applications.\nDu et al. [93] explores the vulnerability of automated\nfact-checking systems to synthetic adversarial evidence, in-\ntroducing Adversarial Addition and Adversarial Modifi-\ncation scenarios. The study demonstrates significant per-\nformance drops in fact-checking models across multiple\nbenchmarks, highlighting the threat posed by advanced\nNLG systems capable of producing coherent disinformation.\nPan et al. [62] and Pan et al. [65] investigate the mis-\nuse potential of LLMs for generating credible-sounding\nmisinformation and its impact on Open-Domain Question\nAnswering (ODQA) systems. They establish threat models\nand simulate misuse scenarios, revealing that LLMs can\nsignificantly degrade ODQA performance. The authors pro-\npose defense strategies such as misinformation detection,\nvigilant prompting, and reader ensemble, emphasizing the\nneed for ongoing research to mitigate these threats.\nZhong et al. [64] and Zou et al. [79] examine the vul-\nnerabilities of dense retrieval systems and RAG systems\nto misinformation and knowledge poisoning attacks. They\nintroduce novel attack methods that generate adversarial\npassages and poisoned texts, showing high attack success"}, {"title": "7", "content": "rates. The studies highlight the need for robust defenses to\nprotect these systems from such vulnerabilities.\nAbdelnabi et al. [66] explores Indirect Prompt Injection\n(IPI) attacks, where adversaries inject prompts into data\nsources likely to be retrieved during inference, remotely\ncontrolling the LLM without direct access. The study cat-\negorizes various threats posed by these attacks and demon-\nstrates their practical viability on real-world systems, ad-\nvocating for improved safety evaluations and mitigation\nstrategies.\nCho et al. [63] addresses the robustness of RAG sys-\ntems against low-level textual perturbations, such as typos,\nthrough a novel adversarial attack method called Genetic\nAttack on RAG (GARAG). The study reveals significant\nvulnerabilities in RAG systems, showing that even small\nperturbations can drastically reduce performance.\nIn conclusion, the evolving landscape of misinformation\nattacks poses a severe threat to the reliability and accu-\nracy of RAG and related systems. Various attack strategies,\nfrom adversarial document additions to indirect prompt\ninjections, can significantly undermine system performance.\nThe necessity for robust defenses, including misinformation\ndetection, vigilant prompting, and misinformation-aware\nQA systems, is clear. Ongoing research and collaboration are\nessential to develop effective mitigation strategies, ensuring\nthe safe and reliable use of these advanced technologies in\nreal-world applications.\nDefenses Against Attacks. Defending against these so-\nphisticated attacks requires a multifaceted approach, includ-\ning enhancing model robustness, improving data verifica-\ntion processes, and developing new defensive strategies.\nHong et al. [68] investigates the vulnerability of retrieval-\naugmented language models to counterfactual and mis-\nleading information within retrieved documents. The study\nproposes fine-tuning a discriminator alongside the retrieval-\naugmented model and prompting GPT-3.5 to elicit its dis-\ncriminative capabilities, demonstrating significant improve-\nments in model robustness against noise.\nWeller et al. [67] addresses the challenge of defending\nODQA systems against adversarial poisoning attacks. The\nauthors propose a defense mechanism based on query\naugmentation and a novel confidence method called Con-\nfidence from Answer Redundancy (CAR). Experimental\nresults show that this approach can improve exact match\nscores by nearly 20% across various levels of data poisoning,\nenhancing the system's resilience to such attacks.\nXiang et al. [69] proposes RobustRAG, a defense frame-\nwork for protecting RAG systems from retrieval corruption\nattacks. RobustRAG utilizes an isolate-then-aggregate strat-\negy, computing LLM responses for each passage in isolation\nand then securely aggregating these responses to ensure\nrobustness. The framework demonstrates its effectiveness\nacross various tasks and datasets, showcasing its generaliz-\nability and potential for real-world applications.\nThe landscape of misinformation attacks is continuously\nevolving, posing significant threats to the reliability of RAG\nsystems. The research highlights a range of attack strate-\ngies and underscores the importance of developing robust\ndefenses to mitigate these threats. Continuous research and\ncollaborative efforts are essential to ensure the safe and ef-"}, {"title": "8", "content": "fective use of advanced technologies in information retrieval\nand generation.\n3.3 Fairness\nWith the rapid development of LLMs, the corresponding\nfairness study has gained increasing importance. As the\ncapabilities of LLMs continue to grow, a wide variety of ap-\nplications are gradually entering and impacting the lives of\ncountless people. However, LLMs have been acknowledged\nto contain harmful and discriminatory information towards\nmarginalized social groups [94, 95"}]}