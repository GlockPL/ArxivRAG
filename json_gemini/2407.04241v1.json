{"title": "ANYSR: REALIZING IMAGE SUPER-RESOLUTION AS ANY-SCALE, ANY-RESOURCE", "authors": ["Wengyi Zhan", "Mingbao Lin", "Chia-Wen Lin", "Rongrong Ji"], "abstract": "In an effort to improve the efficiency and scalability of single-image super-resolution (SISR) ap- plications, we introduce AnySR, to rebuild existing arbitrary-scale SR methods into any-scale, any-resource implementation. As a contrast to off-the-shelf methods that solve SR tasks across vari- ous scales with the same computing costs, our AnySR innovates in: 1) building arbitrary-scale tasks as any-resource implementation, reducing resource requirements for smaller scales without additional parameters; 2) enhancing any-scale performance in a feature-interweaving fashion, inserting scale pairs into features at regular intervals and ensuring correct feature/scale processing. The efficacy of our AnySR is fully demonstrated by rebuilding most existing arbitrary-scale SISR methods and validating on five popular SISR test datasets. The results show that our AnySR implements SISR tasks in a computing-more-efficient fashion, and performs on par with existing arbitrary-scale SISR methods. For the first time, we realize SISR tasks as not only any-scale in literature, but also as any-resource. Code is available at https://github.com/CrispyFeSo4/AnySR.", "sections": [{"title": "1 Introduction", "content": "Single image super-resolution (SISR) is the process of reconstructing a low-resolution (LR) image into a high-resolution (HR) one rich in detail. The ill-posed nature has made SISR one of the most challenging tasks in low-level computer vision. The past decades have witnessed the advent of many classic studies that continuously address such a challenge. Typical works such as EDSR [1], RCAN [2], and RDN [3], serving as network backbones, effectively extract both texture and semantic information from LR images and lay the foundation for HR reconstruction. The pixel-shuffle upsampling method [4] takes a step further to enhance the image reconstruction quality. Standing upon the shoulders of these pioneering works, the majority of SISR methods [1, 2, 3] adheres to a straightforward and efficient paradigm in which features are extracted by feature extractors, followed by upscaling modules for reconstruction.\nAlbeit the encouraging achievement, most of the above methods are confined to addressing the image reconstruction of a fixed upsampling scale and are therefore stuck in repeatedly rebuilding an SR network when the fully trained model cannot accommodate the resolutions in demand. Actually, there is a significant demand for multi-scale SR tasks. Training individual models for respective resolutions seems to be a great waste of computing resources and fails to adapt to real-time online deployment. Reflecting on this, many recent researchers have shifted their focus to integrating any-scale tasks into only one integrated SR model. For example, Meta-SR [9] learns to predict coordinates and scale-oriented convolutional weights to generate HR images. Taking into consideration the neighborhoods in the LR image and a scale-related cell size, the subsequent LIIF [10] utilizes an MLP to predict the pixel values of the queried coordinates in an HR one. To enhance the representational capabilities, LTE [11] further introduces a local texture estimator that transforms coordinate information into the Fourier domain. The very recent SRNO [12] and OPE-SR [13] enable HR tasks of continuous scales by respectively incorporating Neural Operators [14] and introducing an orthogonal position encoding upscale module."}, {"title": "2 Related Work", "content": "2.1 Arbitrary-Scale Super-Resolution\nOver the past few years, many classic convolutional neural networks (CNNs) based methods, such as SRCNN [22], SRResNet [23], EDSR [1], and RDN [3], have been proposed and shown commendable promise in SR tasks. To further improve SR performance, more methods incorporate residual blocks [24, 6, 25], dense blocks [26, 3, 27], and other techniques [28, 29, 30, 31, 32, 33]. Additionally, certain SR approaches leverage attention mechanisms, such as self-attention [34, 35], channel attention [36, 37, 38], and non-local attention [39, 40, 41]. Yet, most of these methods are tailored to particular scales, constraining their versatility and adaptability.\nTherefore, recent researchers have turned their attention to arbitrary-scale SR tasks, aiming to tackle SR problems with arbitrary scales within a unified model. For example, Meta-SR [9] innovatively introduces the first arbitrary-scale meta-upscale module, predicting convolutional filters based on coordinates and scales to generate HR images. Subsequently, employing implicit neural representation, LIIF [10] predicts the RGB value at an arbitrary query coordinate by incorporating image coordinates and features from the backbone around that point. Further enhancements have been achieved by techniques like LTE [11], which introduces a local texture estimator characterizing image textures in Fourier space. Additionally, recent advancements in the SR field have introduced novel structures. For instance, SRNO [12] incorporates Neural Operators, and OPE-SR [13] introduces orthogonal position encoding (OPE), an extension of position encoding. These innovations have demonstrated substantial performance improvements.\n2.2 Efficient Image Super-Resolution\nIn recent years, numerous approaches [42, 43, 44, 45, 46] have emerged to develop efficient super-resolution networks. Building upon the pioneering work of SRCNN [22], FSRCNN [42] substantially accelerates the SISR networks. This acceleration is achieved by taking the original LR image as input without bicubic interpolation, using smaller sizes of convolution kernels, and incorporating a deconvolutional layer at the final stage of the network for upsampling. LapSRN [47] incrementally reconstructs the sub-band residuals of HR images through the utilization of the Laplacian pyramid. CARN [43] achieves improved efficiency by ingeniously designing cascading residual networks with group convolution. IMDN [48] presents information multi-distillation blocks featuring a contrast-aware channel attention layer. In parallel, RFDN [44] optimizes the architecture using a feature distillation mechanism through the proposed residual feature distillation block. In the wake of RFDN, RLFN [49] adds more channels to compensate for discarded feature distillation branches and introduces the residual local feature block, resulting in enhanced inference speed and superior performance with fewer parameters. On the other hand, FMEN [45] introduces the re-parameterization technique, expanding the optimization space during training through re-parameterizable building blocks [50], without incurring additional inference time."}, {"title": "3 Methodology", "content": "3.1 Preliminaries\nThe general pipeline of our AnySR method acts in accordance with most existing methods [1, 51, 38] as in the upper half of Fig. 2, typically highlighted with 1) A 3\u00d73 convolutional layer to convert a given LR image $I_{LR} \\in R^{H \\times W \\times 3}$ into a shallow feature $f_s \\in R^{H \\times W \\times C_{in}}$ where $C_{in}$ represents the channel number. 2) N consecutive blocks for deep feature extraction, standing out as the \"AnySR\" block in this paper, along with a common convolutional layer and a global residual connection to the shadow feature $f_s$. 3) An upsampler $U(\\cdot)$ to reconstruct an HR version of the LR image, denoted as $I_{HR} \\in R^{H' \\times W' \\times 3}$ where $H' > H$ and $W' > W$. For ease of the following representation, we use $F(\\cdot)$ to denote feature extraction operations in 1) and 2).\n3.2 Any-Resource Implementation\nThe upper part of Fig. 2 manifests our any-resource implementation. Considering the close relationship between scaling factor $(s_h, s_w)$ and reconstruction difficulty, we rearrange scale set $S$ in an ascending order and partition it into $T$ groups:\n$S_1 \\cup S_2 \\cup ... \\cup S_T = S \\quad and \\quad S_1 \\cap S_2 \\cap ... \\cap S_T = \\emptyset$.\nCorrespondingly, $T$ distinct feature extraction networks ${F_t}_{t=1}^T$ with various complexities are then constructed. Each network $F_t$ is affiliated with parameters of a particular size as $\\Theta_{F_t}$, indicating the complexity. We have $|F_{t+1}| > |\\Theta_{F_t}|$.\nA natural approach is to utilize network $F_t$ to process the SR tasks within group $S_t$ such that 1) Smaller-scale tasks can be efficiently fulfilled through a structurally simple network; 2) Larger-scale ones benefit from complex networks. Thus, the process of reconstructing group $S_t$ tasks becomes\n$I_{HR} = U (F_t(I_{LR}, S_t; \\Theta_{F_t}); \\Theta_U)$.\nAll-in-One Training. Although the above process greatly conserves computational resources, it defects in that 1) $T$ networks have to be repeatedly trained like traditional SR methods, and that 2) additional parameters are introduced compared with arbitrary-scale SR methods. To solve this issue, we follow [20, 21] to train and infer all networks in a parameter-sharing manner by the following set of constraints:\n$\\Theta_{F_1} = \\Theta_F[1 : |\\Theta_{F_1}|], \\Theta_{F_2} = \\Theta_F[1 : |\\Theta_{F_2}|], ..., \\Theta_{F_T} = \\Theta_F[1 : |\\Theta_{F_T} |] = \\Theta_F,$\nwhere $\\Theta_F [1: |\\Theta_{F_1} |]$ represents the first $| \\Theta_{F_1} |$ filters of $\\Theta_F$, which indeed makes $F_1$ a subnet of $F$ as $\\Theta_{F_1} \\subset \\Theta_{F_2} \\subset ... \\subset \\Theta_{F_T} \\subset \\Theta_F$.\n3.3 Any-Scale Enhancement\nThough any-resource implementation boosts the scalability and efficiency of SR tasks, its parameter-sharing puts the performance at risk, due to fewer parameters for smaller scales and mutual weight influence among different scales. It is of great necessity to enhance the reconstruction results of smaller networks and bring them closer to the performance of the original arbitrary-scale network.\nPrevious research [51] has proven beneficial from linking extracted features with scales, ultimately optimizing the performance of the upsampling module. We realize scale information can be well excavated and ameliorated under our any-resource implementation for the following reasons: 1) The weights $\\Theta_{F[1 : |\\Theta_F|]}$ are particularly trained to deal with the scale set $S_t$, leading to scale-aware features; 2) Partial weights are shared among different subnets, injecting information of other scales. Therefore, we achieve any-scale enhancement by emphasizing the significance of customized handling for features across different scales. Specifically, as shown in Fig. 2, inside each \u201cAnySR\u201d block lies two sub-blocks with the first/second reducing/increasing the channels. We choose to inject better scale information, in a feature-interweaving fashion as illustrated in Fig. 3(a), into the outputs of the first sub-block, denoted as $f_t \\in R^{H \\times W \\times ([C_{in} \\cdot w_t])}$, where $w_t = \\frac{|F_t|}{\\Theta_F}$, and $[\\cdot]$ denotes the floor function.\nFeature-Interweaving. Features $f_t \\in R^{H \\times W \\times ([C_{in} \\cdot w_t])}$ from $F_t$ would first go through a global average pooling (GAP) to get a variant $\\overline{f_t} \\in R ^{[C_{in} \\cdot w_t]}$ where the scale information $(s_h, s_w) \\in S_t$ is formally injected.\nTo this end, one naive fashion, as illustrated in the upper half of Fig. 3(b), is to follow existing methods [10, 52] which simply concatenate $(s_h, s_w)$ at the rear of $\\overline{f_t}$ to form $\\overline{f_t} = [\\overline{f_t}, s_h, s_w] \\in R^{[C_{in} \\cdot w_t]+2}$. Then, a two-layer MLP, with weights $W_1 \\in R^{2 \\cdot C_{in} \\times (C_{in}+2)}$, $W_2 \\in R^{C_{in} \\times 2 \\cdot C_{in}}$ and a ReLU layer inserted between, is created, followed by a Sigmoid function to weight the original features $f_t$ as\n$f'_t = f_t \\bigodot Sigmoid(W_2[1 : [C_{in} \\cdot w_t], :] \\cdot ReLU(W_1[:, 1 : [C_{in} \\cdot w_t] +2] \\cdot \\overline{f_t})),$\nwhere $W_1[:, 1 : [C_{in} \\cdot w_t] + 2] \\in R^{2 \\cdot C_{in} \\times ([C_{in} \\cdot w_t])+2}$ and $W_2[1 : [C_{in} \\cdot w_t], :] \\in R^{[C_{in} \\cdot w_t] \\times 2 \\cdot C_{in}}$ are the shared MLP weights for the network $F_t$. $f'_t$ then serves as the input of the second sub-block in the \u201cAnySR\u201d block.\nThough this naive approach facilitates the interaction between features and scale information, two notable limitations, as we analyze, arise: 1) Insufficient scale information. Compared to a total of $[C_{in} \\cdot w_t]$ channels in image features, where $[C_{in} \\cdot w_t] \\gg 2$ signifies a limited influence of scale information on the weighted features, the scale information takes up only 2 dimensions. 2) Inappropriate scale processing. For the MLP weights $W_1 \\in R^{2 \\cdot C_{in} \\times (C_{in}+2)}$, the sub-weights $W_1[:, [C_{in} \\cdot w_t] + 1 : [C_{in} \\cdot w_t] + 2] \\in R^{2 \\cdot C_{in} \\times 2}$ are used to process the scale information $(s_h, s_w) \\in S_t$. For $(s_h, s_w) \\in S_{t+1}$, the weights to process scales are $W_1[:, [C_{in} \\cdot w_{t+1}] + 1 : [C_{in} \\cdot w_{t+1}] + 2] \\in R^{2 \\cdot C_{in} \\times 2}$ and\nTo better align features with scale information, for network $F_t$, our feature-interweaving inserts scale pair $(s_h, s_w)$ for $\\Lambda$ times into positions $(\\lfloor C_{in} \\cdot i/\\lambda \\rfloor + 2i - 1, \\lfloor C_{in} \\cdot i/\\lambda \\rfloor + 2i)$ where $i = 1, 2, ..., [\\Lambda \\cdot w_t]$, of the pooled features $\\overline{f_t} \\in R^{[C_{in} \\cdot w_t]}$, which leads to the following concatenated features $f_t \\in R^{[C_{in} \\cdot w_t]+2 \\cdot [\\Lambda \\cdot w_t]}:$\n$f_t = [\\overline{f_t} [1: [C_{in} \\cdot 1/\\lambda]], s_h, s_w, \\overline{f_t}[[C_{in} \\cdot 1/\\lambda]+1 : [C_{in} \\cdot 2/\\lambda]], s_h, s_w, ..., \\overline{f_t}[[C_{in} \\cdot (i - 1)/\\lambda] + 1 : [C_{in} \\cdot i/\\lambda]], s_h, s_w, \\overline{f_t} [[C_{in} \\cdot [\\Lambda \\cdot w_t]/\\lambda] + 1 :]].$\nThen, weights $W_1$ in the two-layer MLP becomes $W_1 \\in R^{2 \\cdot C_{in} \\times (C_{in}+2 \\cdot [\\Lambda \\cdot w_t])}$, and the weighted features in (5) becomes:\n$f'_t = f_t \\bigodot Sigmoid (W_2[1 : [C_{in} \\cdot w_t], : ] \\cdot ReLU(W_1 [ :, 1 : [C_{in} \\cdot w_t] + 2 \\cdot [\\Lambda \\cdot w_t]] \\cdot \\overline{f_t})).$\nAs a consequence, the weights $W_1 [ :, [C_{in} \\cdot i/\\lambda] + 2i - 1 : [C_{in} \\cdot i/\\lambda] +2i] \\in [R^{2 \\cdot C_{in} \\times 2}, in which $i = 1, 2, ..., [\\Lambda \\cdot w_t]$ consistently deal with the scale information whatever the specific value of $t$. In conclusion, our feature-interweaving overcomes the issues of traditional methods [10, 52] by taking into account two key operations: 1) repeating the scale pair $(s_h, s_w)$ $\\Lambda$ times to ensure sufficient scale information for network $F_t$; 2) inserting scale pairs into features in regular intervals to ensure the correct feature/scale processing. The efficacy of our feature-interweaving will be shown in Sec. 4."}, {"title": "4 Experimental Results", "content": "4.1 Experiment Settings\nTraining Details. We primarily rely on existing well-established arbitrary-scale SR models, including Meta-SR [9], LIIF [10], ArbSR [51], and SRNO [12], for AnySR rebuilding. Without loss of generality, experiments on top of two famous feature extraction backbones, including EDSR [1] and RDN [3], are conducted to validate the universality of our AnySR. During specific implementation, we configure the network number $T = 4$ and ${w_t}_{t=1}^4 = {0.5, 0.7, 0.9, 1.0}$, that is, the smallest scale group $S_1$ enables a reduction in inference cost by up to 50% compared with the original. The reset probability $p$ in Line 4 of Algorithm 1 is configured at 0.6, and the hyper-parameter $\\lambda$ in feature-interweaving is set as 4 for EDSR and 8 for RDN. Accordingly, the scale groups $S_1 = {1.1, 1.2, ..., 1.7}, S_2 = {1.8, 1.9, ..., 2.5}, S_3 = {2.6, 2.7, ..., 3.2}$ and $S_4 = {3.3, 3.4, ..., 4.0}$. To ensure a fair performance evaluation, we keep identical settings and configurations used for the original models in our training process. The patch size is 50 \u00d7 50 for Meta-SR and ArbSR, 48 \u00d7 48 for LIIF, and 128 \u00d7 128 for SRNO, with a batch size of 8 per GPU for EDSR and 4 per GPU for\n4.2 Main Results\nQuantitative Results. Table 1 and Table 2 show the PSNR performances of off-the-shelf arbitrary-scale methods, compared to that of AnySR variants with different subnets dealing with individual scales and that of AnySR-retrained largest network for all scales. Fig. 4 presents a bar graph detailing the PSNR performances across all 30 scales (\u00d71.1~\u00d74.0) for the three entities mentioned above. Considering the unavailability of some existing models (Meta-SR: EDSR; ArbSR: EDSR, RDN), we re-implement and re-train them following the experiment settings outlined in the papers [9, 51].\nIn Table 1, our AnySR approach markedly decreases computational expenses while preserving visual qualities, thereby improving network efficiency and scalability. In particular, when selecting the subnetwork for inference, there is an average performance drop of 0.15 dB on benchmark datasets, primarily observed at smaller scales. Considering the smaller subnetwork reduces computational costs by up to 50%, we find this performance drop acceptable. Additionally, Table 2 shows that, when using the entire network, the average performance drop is only 0.05 dB. This indicates that AnySR-retrained models can effectively preserve the original model's performance during the rebuilding process, which can also be visually observed in Fig. 4. Such good performance is closely related to the reset probability $p$ in Line 4 of Algorithm 1, which will be further investigated in Sec. 4.4. Overall, as evidenced by the results, our AnySR succeeds in dynamically selecting the reconstruction network according to the availability of computing resources, thereby making\n4.3 Complexity Analysis\nWe perform a thorough analysis and comparison on SR tasks at different scales, encompassing the number of parameters, PSNR performance, and FLOPs, for SRNO [12] and its AnySR variants of different subnets to solve different scales and the entire network to solve all scales. All results are evaluated on top of the EDSR [1] backbone network on the Set14 dataset [54] as shown in Table 3.\nIn terms of parameters, our AnySR variant introduces only 0.28 M additional weights (22.95% of the original 1.22 M parameters), primarily dedicated to any-scale enhancement. In relation to computational costs, when performing\n4.4 Ablation Studies\nWe conduct ablation studies to validate the effectiveness of individual components of AnySR. All ablation experiments are performed on SRNO [12], using EDSR [1] as the backbone. We compare our AnySR with two variants: 1) \"w/o ASE\": AnySR without any-scale enhancement; 2) \u201cw/o FI\u201d: AnySR replacing feature-interweaving with a simple concatenation. Also, we vary the value of reset probability $p$ in Line 4 of Algorithm 1, to show its importance.\nAny-Scale Enhancement. Any-scale enhancement constitutes one of the fundamental branches encompassed within our overarching research framework. By injecting, excavating, and ameliorating sufficient scale information, we realize customized handling for features at different scales. In order to validate the effectiveness of ASE, we train the network by removing this component, and the results are presented in Table 4. It is evident that the absence of ASE (i.e., \"w/o ASE\") leads to a certain performance drop.\nFeature-Interweaving. Feature-interweaving considers the non-uniformity of shared weights and overcomes the mutual weight influence across different scales by repeating and inserting scale pairs into features at regular intervals. We study the contribution of this mechanism by substituting the feature-interweaving fashion with a simple concatenation in earlier methods [10, 52]. By introducing feature-interweaving, a better performance is achieved in Table 4."}, {"title": "5 Conclusion and Future Work", "content": "In this paper, we presented AnySR, a simple yet versatile approach to transform existing arbitrary-scale super-resolution methods into implementations that adapt to any scale and resource availability. By rebuilding arbitrary-scale tasks into an any-resource implementation, we enable the completion of smaller-scale SISR tasks with reduced computational resources and no additional parameters. To maintain performance, we enhance any-scale features through a feature-interweaving fashion, ensuring sufficient scale information and correct feature/scale processing. Extensive experiments on benchmark datasets demonstrate the efficiency and scalability of our AnySR method in arbitrary-scale SISR applications.\nAn alternative approach is resorting to a more complex NAS (Neural Architecture Search), which may achieve better performance and will be our major future exploration."}]}