{"title": "ANYSR: REALIZING IMAGE SUPER-RESOLUTION AS\nANY-SCALE, ANY-RESOURCE", "authors": ["Wengyi Zhan", "Mingbao Lin", "Chia-Wen Lin", "Rongrong Ji"], "abstract": "In an effort to improve the efficiency and scalability of single-image super-resolution (SISR) ap-\nplications, we introduce AnySR, to rebuild existing arbitrary-scale SR methods into any-scale,\nany-resource implementation. As a contrast to off-the-shelf methods that solve SR tasks across vari-\nous scales with the same computing costs, our AnySR innovates in: 1) building arbitrary-scale tasks\nas any-resource implementation, reducing resource requirements for smaller scales without additional\nparameters; 2) enhancing any-scale performance in a feature-interweaving fashion, inserting scale\npairs into features at regular intervals and ensuring correct feature/scale processing. The efficacy\nof our AnySR is fully demonstrated by rebuilding most existing arbitrary-scale SISR methods and\nvalidating on five popular SISR test datasets. The results show that our AnySR implements SISR\ntasks in a computing-more-efficient fashion, and performs on par with existing arbitrary-scale SISR\nmethods. For the first time, we realize SISR tasks as not only any-scale in literature, but also as\nany-resource. Code is available at https://github.com/CrispyFeSo4/AnySR.", "sections": [{"title": "1 Introduction", "content": "Single image super-resolution (SISR) is the process of reconstructing a low-resolution (LR) image into a high-resolution\n(HR) one rich in detail. The ill-posed nature has made SISR one of the most challenging tasks in low-level computer\nvision. The past decades have witnessed the advent of many classic studies that continuously address such a challenge.\nTypical works such as EDSR [1], RCAN [2], and RDN [3], serving as network backbones, effectively extract both\ntexture and semantic information from LR images and lay the foundation for HR reconstruction. The pixel-shuffle\nupsampling method [4] takes a step further to enhance the image reconstruction quality. Standing upon the shoulders of\nthese pioneering works, the majority of SISR methods [1, 2, 3] adheres to a straightforward and efficient paradigm in\nwhich features are extracted by feature extractors, followed by upscaling modules for reconstruction.\nAlbeit the encouraging achievement, most of the above methods are confined to addressing the image reconstruction\nof a fixed upsampling scale and are therefore stuck in repeatedly rebuilding an SR network when the fully trained\nmodel cannot accommodate the resolutions in demand. Actually, there is a significant demand for multi-scale SR\ntasks. Training individual models for respective resolutions seems to be a great waste of computing resources and\nfails to adapt to real-time online deployment. Reflecting on this, many recent researchers have shifted their focus to\nintegrating any-scale tasks into only one integrated SR model. For example, Meta-SR [9] learns to predict coordinates\nand scale-oriented convolutional weights to generate HR images. Taking into consideration the neighborhoods in the\nLR image and a scale-related cell size, the subsequent LIIF [10] utilizes an MLP to predict the pixel values of the\nqueried coordinates in an HR one. To enhance the representational capabilities, LTE [11] further introduces a local\ntexture estimator that transforms coordinate information into the Fourier domain. The very recent SRNO [12] and\nOPE-SR [13] enable HR tasks of continuous scales by respectively incorporating Neural Operators [14] and introducing\nan orthogonal position encoding upscale module."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Arbitrary-Scale Super-Resolution", "content": "Over the past few years, many classic convolutional neural networks (CNNs) based methods, such as SRCNN [22],\nSRResNet [23], EDSR [1], and RDN [3], have been proposed and shown commendable promise in SR tasks. To\nfurther improve SR performance, more methods incorporate residual blocks [24, 6, 25], dense blocks [26, 3, 27], and\nother techniques [28, 29, 30, 31, 32, 33]. Additionally, certain SR approaches leverage attention mechanisms, such as\nself-attention [34, 35], channel attention [36, 37, 38], and non-local attention [39, 40, 41]. Yet, most of these methods\nare tailored to particular scales, constraining their versatility and adaptability.\nTherefore, recent researchers have turned their attention to arbitrary-scale SR tasks, aiming to tackle SR problems\nwith arbitrary scales within a unified model. For example, Meta-SR [9] innovatively introduces the first arbitrary-\nscale meta-upscale module, predicting convolutional filters based on coordinates and scales to generate HR images.\nSubsequently, employing implicit neural representation, LIIF [10] predicts the RGB value at an arbitrary query\ncoordinate by incorporating image coordinates and features from the backbone around that point. Further enhancements\nhave been achieved by techniques like LTE [11], which introduces a local texture estimator characterizing image\ntextures in Fourier space. Additionally, recent advancements in the SR field have introduced novel structures. For\ninstance, SRNO [12] incorporates Neural Operators, and OPE-SR [13] introduces orthogonal position encoding (OPE),\nan extension of position encoding. These innovations have demonstrated substantial performance improvements."}, {"title": "2.2 Efficient Image Super-Resolution", "content": "In recent years, numerous approaches [42, 43, 44, 45, 46] have emerged to develop efficient super-resolution networks.\nBuilding upon the pioneering work of SRCNN [22], FSRCNN [42] substantially accelerates the SISR networks. This\nacceleration is achieved by taking the original LR image as input without bicubic interpolation, using smaller sizes\nof convolution kernels, and incorporating a deconvolutional layer at the final stage of the network for upsampling.\nLapSRN [47] incrementally reconstructs the sub-band residuals of HR images through the utilization of the Laplacian\npyramid. CARN [43] achieves improved efficiency by ingeniously designing cascading residual networks with group\nconvolution. IMDN [48] presents information multi-distillation blocks featuring a contrast-aware channel attention\nlayer. In parallel, RFDN [44] optimizes the architecture using a feature distillation mechanism through the proposed\nresidual feature distillation block. In the wake of RFDN, RLFN [49] adds more channels to compensate for discarded\nfeature distillation branches and introduces the residual local feature block, resulting in enhanced inference speed\nand superior performance with fewer parameters. On the other hand, FMEN [45] introduces the re-parameterization\ntechnique, expanding the optimization space during training through re-parameterizable building blocks [50], without\nincurring additional inference time."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Preliminaries", "content": "The general pipeline of our AnySR method acts in accordance with most existing methods [1, 51, 38] as in the upper\nhalf of Fig. 2, typically highlighted with 1) A 3 \u00d7 3 convolutional layer to convert a given LR image $I_{LR} \\in \\mathbb{R}^{H\\times W \\times 3}$\ninto a shallow feature $f_s \\in \\mathbb{R}^{H\\times W \\times C_{in}}$ where $C_{in}$ represents the channel number. 2) N consecutive blocks for deep\nfeature extraction, standing out as the \"AnySR\" block in this paper, along with a common convolutional layer and a\nglobal residual connection to the shadow feature $f_s$. 3) An upsampler $U(\\cdot)$ to reconstruct an HR version of the LR\nimage, denoted as $I_{HR} \\in \\mathbb{R}^{H' \\times W' \\times 3}$ where $H' > H$ and $W' > W$. For ease of the following representation, we use\n$F(\\cdot)$ to denote feature extraction operations in 1) and 2)."}, {"title": "3.2 Any-Resource Implementation", "content": "The upper part of Fig. 2 manifests our any-resource implementation. Considering the close relationship between scaling\nfactor $(s_h, s_w)$ and reconstruction difficulty, we rearrange scale set $S$ in an ascending order and partition it into $T$\ngroups:\n$S_1 \\cup S_2 \\cup ... \\cup S_T = S$ and $S_1 \\cap S_2 \\cap ... \\cap S_T = \\O$.\nCorrespondingly, $T$ distinct feature extraction networks ${F_t}_{t=1}^T$ with various complexities are then constructed. Each\nnetwork $F_t$ is affiliated with parameters of a particular size as $\\Theta_{F_t}$, indicating the complexity. We have $|F_{t+1}| > |\\Theta_{F_t}|$.\nA natural approach is to utilize network $F_t$ to process the SR tasks within group $S_t$ such that 1) Smaller-scale tasks\ncan be efficiently fulfilled through a structurally simple network; 2) Larger-scale ones benefit from complex networks.\nThus, the process of reconstructing group $S_t$ tasks becomes\n$I_{HR} = U (F_t(I_{LR}, S_t; \\Theta_{F_t}); \\Theta_U)$.\nAll-in-One Training. Although the above process greatly conserves computational resources, it defects in that 1) $T$\nnetworks have to be repeatedly trained like traditional SR methods, and that 2) additional parameters are introduced\ncompared with arbitrary-scale SR methods. To solve this issue, we follow [20, 21] to train and infer all networks in a\nparameter-sharing manner by the following set of constraints:\n$\\Theta_{F_1} = \\Theta_F[1 : |\\Theta_{F_1}|], \\Theta_{F_2} = \\Theta_F[1 : |\\Theta_{F_2}|], ..., \\Theta_{F_T} = \\Theta_F[1 : |\\Theta_{F_T} |] = \\Theta_F,$\nwhere $\\Theta_F [1: |\\Theta_{F_1} |]$ represents the first $| \\Theta_{F_1} |$ filters of $\\Theta_F$, which indeed makes $F_1$ a subnet of $F$ as $\\Theta_{F_1} \\subset \\Theta_{F_2} \\subset ... \\subset \\Theta_{F_T} \\subset \\Theta_F$."}, {"title": "3.3 Any-Scale Enhancement", "content": "Though any-resource implementation boosts the scalability and efficiency of SR tasks, its parameter-sharing puts the\nperformance at risk, due to fewer parameters for smaller scales and mutual weight influence among different scales. It\nis of great necessity to enhance the reconstruction results of smaller networks and bring them closer to the performance\nof the original arbitrary-scale network.\nPrevious research [51] has proven beneficial from linking extracted features with scales, ultimately optimizing the\nperformance of the upsampling module. We realize scale information can be well excavated and ameliorated under\nour any-resource implementation for the following reasons: 1) The weights $\\Theta_{F[1 : |\\Theta_{F_t}|]}$ are particularly trained\nto deal with the scale set $S_t$, leading to scale-aware features; 2) Partial weights are shared among different subnets,\ninjecting information of other scales. Therefore, we achieve any-scale enhancement by emphasizing the significance\nof customized handling for features across different scales. Specifically, as shown in Fig. 2, inside each \u201cAnySR\u201d\nblock lies two sub-blocks with the first/second reducing/increasing the channels. We choose to inject better scale\ninformation, in a feature-interweaving fashion as illustrated in Fig. 3(a), into the outputs of the first sub-block, denoted\nas $f_t \\in \\mathbb{R}^{H \\times W \\times ([C_{in} \\cdot w_t])}$, where $w_t = \\frac{F_t}{F}$, and $[\\cdot]$ denotes the floor function.\nFeature-Interweaving. Features $f_t \\in \\mathbb{R}^{H \\times W \\times ([C_{in} \\cdot w_t])}$ from $F_t$ would first go through a global average pooling\n(GAP) to get a variant $\\bar{f}_t \\in \\mathbb{R}^{[C_{in} \\cdot w_t]}$ where the scale information $(s_h, s_w) \\in S_t$ is formally injected.\nTo this end, one naive fashion, as illustrated in the upper half of Fig. 3(b), is to follow existing methods [10, 52] which\nsimply concatenate $(s_h, s_w)$ at the rear of $\\bar{f}_t$ to form $f'_t = [\\bar{f}_t, s_h, s_w] \\in \\mathbb{R}^{[C_{in} \\cdot w_t]+2}$. Then, a two-layer MLP, with\nweights $W_1 \\in \\mathbb{R}^{2 \\cdot C_{in} \\times (C_{in}+2)}$, $W_2 \\in \\mathbb{R}^{C_{in} \\times 2 \\cdot C_{in}}$ and a ReLU layer inserted between, is created, followed by a Sigmoid\nfunction to weight the original features $\\bar{f}_t$ as\n$f_f = \\bar{f}_t \\bigodot \\text{Sigmoid}(W_2[1 : [C_{in} \\cdot w_t], :] \\cdot \\text{ReLU}(W_1[:, 1 : [C_{in} \\cdot w_t] +2] \\cdot f'_t)),$\nwhere $W_1[:, 1 : [C_{in} \\cdot w_t] + 2] \\in \\mathbb{R}^{2 \\cdot C_{in} \\times ([C_{in} \\cdot w_t])+2}$ and $W_2[1 : [C_{in} \\cdot w_t], :] \\in \\mathbb{R}^{[C_{in} \\cdot w_t] \\times 2 \\cdot C_{in}}$ are the shared MLP\nweights for the network $F_t$. $f_f$ then serves as the input of the second sub-block in the \u201cAnySR\u201d block.\nThough this naive approach facilitates the interaction between features and scale information, two notable limitations,\nas we analyze, arise: 1) Insufficient scale information. Compared to a total of $[C_{in} \\cdot w_t]$ channels in image features,\nwhere $[C_{in} \\cdot w_t] \\gg 2$ signifies a limited influence of scale information on the weighted features, the scale information\ntakes up only 2 dimensions. 2) Inappropriate scale processing. For the MLP weights $W_1 \\in \\mathbb{R}^{2 \\cdot C_{in} \\times (C_{in}+2)}$, the\nsub-weights $W_1[:, [C_{in} \\cdot w_t] + 1 : [C_{in} \\cdot w_t] + 2] \\in \\mathbb{R}^{2 \\cdot C_{in} \\times 2}$ are used to process the scale information $(s_h, s_w) \\in S_t$.\nFor $(s_h, s_w) \\in S_{t+1}$, the weights to process scales are $W_1[:, [C_{in} \\cdot w_{t+1}] + 1 : [C_{in} \\cdot w_{t+1}] + 2] \\in \\mathbb{R}^{2 \\cdot C_{in} \\times 2}$ and"}, {"title": "4 Experimental Results", "content": ""}, {"title": "4.1 Experiment Settings", "content": "Training Details. We primarily rely on existing well-established arbitrary-scale SR models, including Meta-SR [9],\nLIIF [10], ArbSR [51], and SRNO [12], for AnySR rebuilding. Without loss of generality, experiments on top of two\nfamous feature extraction backbones, including EDSR [1] and RDN [3], are conducted to validate the universality of our\nAnySR. During specific implementation, we configure the network number $T = 4$ and ${w_i}_{i=1}^T = {0.5, 0.7, 0.9, 1.0}$,\nthat is, the smallest scale group $S_1$ enables a reduction in inference cost by up to 50% compared with the original. The\nreset probability $p$ in Line 4 of Algorithm 1 is configured at 0.6, and the hyper-parameter $\\lambda$ in feature-interweaving\nis set as 4 for EDSR and 8 for RDN. Accordingly, the scale groups $S_1 = {1.1, 1.2, ..., 1.7}, S_2 = {1.8, 1.9, ..., 2.5},$\n$S_3 = {2.6, 2.7, ..., 3.2}$ and $S_4 = {3.3, 3.4, ..., 4.0}$. To ensure a fair performance evaluation, we keep identical\nsettings and configurations used for the original models in our training process. The patch size is 50 \u00d7 50 for Meta-SR\nand ArbSR, 48 \u00d7 48 for LIIF, and 128 \u00d7 128 for SRNO, with a batch size of 8 per GPU for EDSR and 4 per GPU for"}, {"title": "4.2 Main Results", "content": "Quantitative Results. Table 1 and Table 2 show the PSNR performances of off-the-shelf arbitrary-scale methods,\ncompared to that of AnySR variants with different subnets dealing with individual scales and that of AnySR-retrained\nlargest network for all scales. Considering the unavailability of some existing models (Meta-SR:\nEDSR; ArbSR: EDSR, RDN), we re-implement and re-train them following the experiment settings outlined in the\npapers [9, 51].\nIn Table 1, our AnySR approach markedly decreases computational expenses while preserving visual qualities, thereby\nimproving network efficiency and scalability. In particular, when selecting the subnetwork for inference, there is an\naverage performance drop of 0.15 dB on benchmark datasets, primarily observed at smaller scales. Considering the\nsmaller subnetwork reduces computational costs by up to 50%, we find this performance drop acceptable. Additionally\nshows that, when using the entire network, the average performance drop is only 0.05 dB. This indicates that\nAnySR-retrained models can effectively preserve the original model's performance during the rebuilding process, which\ncan also be visually observed in Overall, as evidenced by the results, our AnySR succeeds in\ndynamically selecting the reconstruction network according to the availability of computing resources, thereby making"}, {"title": "4.3 Complexity Analysis", "content": "We perform a thorough analysis and comparison on SR tasks at different scales, encompassing the number of parameters,\nPSNR performance, and FLOPs, for SRNO [12] and its AnySR variants of different subnets to solve different scales\nand the entire network to solve all scales. All results are evaluated on top of the EDSR [1] backbone network on the\nSet14 dataset [54] as shown in Table 3.\nIn terms of parameters, our AnySR variant introduces only 0.28 M additional weights (22.95% of the original 1.22\nM parameters), primarily dedicated to any-scale enhancement. In relation to computational costs, when performing"}, {"title": "4.4 Ablation Studies", "content": "We conduct ablation studies to validate the effectiveness of individual components of AnySR. All ablation experiments\nare performed on SRNO [12], using EDSR [1] as the backbone. We compare our AnySR with two variants: 1) \"w/o\nASE\": AnySR without any-scale enhancement; 2) \u201cw/o FI\u201d: AnySR replacing feature-interweaving with a simple\nconcatenation. Also, we vary the value of reset probability $p$ in Line 4 of Algorithm 1, to show its importance.\nAny-Scale Enhancement. Any-scale enhancement constitutes one of the fundamental branches encompassed within\nour overarching research framework. By injecting, excavating, and ameliorating sufficient scale information, we realize\ncustomized handling for features at different scales. In order to validate the effectiveness of ASE, we train the network\nby removing this component, and the results are presented in It is evident that the absence of ASE (i.e., \"w/o\nASE\") leads to a certain performance drop.\nFeature-Interweaving. Feature-interweaving considers the non-uniformity of shared weights and overcomes the mutual\nweight influence across different scales by repeating and inserting scale pairs into features at regular intervals. We study\nthe contribution of this mechanism by substituting the feature-interweaving fashion with a simple concatenation in\nearlier methods [10, 52]. By introducing feature-interweaving, a better performance is achieved in"}, {"title": "5 Conclusion and Future Work", "content": "In this paper, we presented AnySR, a simple yet versatile approach to transform existing arbitrary-scale super-resolution\nmethods into implementations that adapt to any scale and resource availability. By rebuilding arbitrary-scale tasks into\nan any-resource implementation, we enable the completion of smaller-scale SISR tasks with reduced computational\nresources and no additional parameters. To maintain performance, we enhance any-scale features through a feature-\ninterweaving fashion, ensuring sufficient scale information and correct feature/scale processing. Extensive experiments\non benchmark datasets demonstrate the efficiency and scalability of our AnySR method in arbitrary-scale SISR\napplications.\nAn alternative approach is resorting to a more complex NAS (Neural Architecture Search), which may achieve better\nperformance and will be our major future exploration."}]}