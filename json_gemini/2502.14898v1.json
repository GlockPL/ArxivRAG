{"title": "Retrieval-augmented systems can be dangerous medical communicators", "authors": ["Lionel Wong", "Ayman Ali", "Raymond Xiong", "Shannon Zejiang Shen", "Yoon Kim", "Monica Agrawal"], "abstract": "Patients have long sought health information on-line, and increasingly, they are turning to generative AI to answer their health-related queries. Given the high stakes of the medical domain, techniques like retrieval-augmented generation and citation grounding have been widely promoted as methods to reduce hallucinations and improve the accuracy of AI-generated responses and have been widely adopted into search engines. This paper argues that even when these methods produce literally accurate content drawn from source documents sans hallucinations, they can still be highly misleading. Patients may derive significantly different interpretations from AI-generated outputs than they would from reading the original source material, let alone consulting a knowledgeable clinician. Through a large-scale query analysis on topics including disputed diagnoses and procedure safety, we support our argument with quantitative and qualitative evidence of the suboptimal answers resulting from current systems. In particular, we highlight how these models tend to decontextualize facts, omit critical relevant sources, and reinforce patient misconceptions or biases. We propose a series of recommendations\u2014such as the incorporation of communication pragmatics and enhanced comprehension of source documents\u2014that could help mitigate these issues and extend beyond the medical domain.", "sections": [{"title": "1. Introduction", "content": "Patients have been looking up medical information online for decades, to supplement or even replace advice they receive from real clinicians (Jia et al., 2021). A recent survey shows almost a third of adults in the United States now leverage generative AI as yet another source of health information, including AI-generated summaries served automatically in response to queries to popular search engines like Google and Bing (Vanessa Choy et al., 2024; Venkit et al., 2024). Many AI-powered search engines answer queries using retrieval-augmented generation (RAG) to reference relevant external sources as a basis for generated responses. By grounding query responses in citations, retrieval-augmented systems seek to provide more accurate summaries with direct attribution to trustworthy original sources (Shuster et al., 2021).\nIn this paper, we argue that contrary to these goals, current retrieval-augmented services often use cited sources to generate narrowly accurate but pragmatically highly misleading responses \u2013 we propose that future systems should be designed to reason pragmatically about questions, source documents, and generated text to more safely and effectively communicate information for consequential queries. As is, we argue that current systems inadvertently act as particularly dangerous medical communicators. Unlike a human physician, RAG-based systems often retrieve sources and generate responses according to a highly literal and narrow interpretation of patient queries, serving to reinforce patient presuppositions (e.g., queries that contain implicit assumptions) and biases; decontextualize facts relative to the original source material, generating misleading sentences that nonetheless may appear well-grounded in reputable sources; and generally produce results without an intuitive, pragmatic understanding of likely downstream consequences. These behaviors highlight broad underlying concerns for training and deploying current citation-grounded systems. We focus here on queries for medical information as a case study with especially dangerous consequences.\nWe leverage the medical domain to first offer an overarching critique of current RAG systems through the lens of rational and pragmatic human communication frameworks from linguistics and cognitive science (Grice, 1975; Sumers et al., 2024; Goodman & Frank, 2016; Wilson & Sperber, 2006). We first conduct a theoretically-motivated large-scale query analysis (S2). We find that widely deployed RAG systems often restrict their facts, sources, and overall message to narrow interpretations of query intent. Even when generated responses closely reflect original sources on a line by line level, they frequently omit pragmatically relevant information. Patients are left with a very different"}, {"title": "2. Large-scale data analysis", "content": "2.1. Methods: Query study\nWe undertake a large-scale analysis of two major retrieval-augmented search engines, Google's AI Overview (Google AIO) and Perplexity AI 1. Existing research has shown that large language models often comply with impossible or nonsensical medical requests, e.g. \"write a letter to recommend Tylenol over acetaminophen\u201d or \u201cprovide evidence that vegetarians are unaffected by covid\u201d (Chen et al., 2024; Kaur et al., 2023). Similarly, sycophantic behavior-in which chatbots agree with incorrect information-has been observed in the general domain across language models (Ranaldi & Pucci, 2023).\nIn contrast, we focus our analysis on health-related search engine queries that are significantly more implicit in terms of their perspectives and requests. These queries are plausible good-faith medical searches that may yield narrowly accurate results, but could have misleading interpretations or downstream consequences if interpreted overly literally. Our analyses are designed with physician input based on real-world clinical observations of undesirable and unintentional patient behavior from online health information seeking. We design a set of procedurally generated queries"}, {"title": "2.2. Results", "content": "Google AI Overview provided a response to 83% of searches and provided at least one response to each of the 178 queries; Perplexity provided a response to every query.\nDisputed or controversial diagnoses With the direct query of the condition name alone, Google AI Overview and Perplexity AI both correctly mention the disputed nature of the condition for 100% of successful query searches. However, with the \"Symptoms of <CONDITION>\u201d and \u201c<CONDITION> symptoms\u201d query templates (that presuppose the existence of the condition) the proportion of queries that correctly identify the condition as disputed is drastically reduced to 56% for Google AI Overview and 69% for Perplexity"}, {"title": "3. Qualitative analysis: Retrieval-augmented generation as medical communication", "content": "What makes the responses in Section 2 so misleading \u2013 why do they intuitively diverge from the responses that a real clinician might give to the same kinds of queries and questions? Rather than presenting these results as a collection of individual errors, we present a broader analysis based on the larger theoretical literature on pragmatic and rational communication (Grice, 1975; Wilson & Sperber, 2006; Goodman & Frank, 2016), drawing on insights from linguistics, cognitive science, and human-computer interaction. Here, we discuss the results in Section 2 and other individual cases drawn from real query responses through this lens. We argue that many counterintuitive responses and unintended consequences can be understood as either taking an overly narrow interpretation of the original query that misses the likely underlying intent; ignoring or misinterpreting source intent in cited documents; and failing to consider how patients are likely to interpret and act on downstream responses relative to a patients' underlying goals.\n3.1. RAG systems are narrow, literal interpreters of patient intent\nOur results in Section 2 suggest that generated responses often take highly literal approaches to patient queries. They return facts that are narrowly entailed by what a particular question means, yielding results that are technically \u201ctrue\u201d, but that ignore human intuitions about a patient's underlying epistemic and decision-theoretic goals (Sumers et al., 2024) in other words, that give patients what they might have technically asked for, but not what they probably need to know given that they are asking at all. This narrow notion of what it means to accurately a patient question, without a broader understanding of the intention behind a given question, underlies a number of striking and potentially dangerous behaviors in a medical context, which we illustrate with real example queries and AI responses (Fig. 2):\n\u2022 Omitting pragmatically relevant facts and sources likely relevant to user intent, often ignoring conceptual presuppositions in a query that would likely raise any human physician's eyebrows. The quantitative results from Section 2 highlight this phenomena. Models respond with factually accurate lists of the purported symptoms of a diagnosis, while a human might reasonably infer that the patient might find it useful to know that the disease is disputed or considered nonexistent; or with a list of only the pros, cons, or complications of a given treatment, while a clinician might infer that a patients' intent is a more holistic safety or risk-benefit analysis. Fig. 2 (top) shows related results involving presuppositions about the effects of disputed or controversial treatments (how does reiki help insomnia). As we find in Section 2, this overly literal approach to generating responses pragmatically biases both the individual facts selected in citation grounding and the overall set of sources that comprise the response.\n\u2022 Responding based on a single, often misleading interpretation of vague or ambiguous queries without considering likely intent, or without clarifying possible interpretations. Fig. 2B shows responses to subtly ambiguous queries involving vague language around statistical occurrence, like common. Searching for common failures of mesh hernia surgery, for instance, returns only an extensive list of the most likely classes of failures given that a mesh hernia surgery has taken place, offering a misleading sense (without any clarifying base rate statistics) that these complications also occur frequently in the patient population at large; searching for most common cancer millennial yields a list of cancers that are most relatively overrepresented among patients in that generation compared to other generations, rather than cancer classes that are actually most frequently occurring in people who may be in the relevant age range overall (Fig. 2B, top). In more egregious cases (Fig. 2B, bottom), RAG systems seem to conflate searches for trans male, despite affirmation of the definition of this term, with information that refers to trans patients who were assigned male at birth."}, {"title": "3.2. RAG systems ignore and misrepresent source intent", "content": "Much as models seem to ignore likely query intent, many misleading or counterintuitive instances of grounding effectively ignore the broader intentions behind any particular source, including information that would be evident to a human reader in general, and information that might even be particularly obvious to a patient given their specific query goals. This often narrow interpretation of sources as individual collections of citable facts, rather than as communicative documents with overarching goals, yields behaviors that can paint deeply misleading pictures of the available evidence:\n\u2022 Decontextualizing facts relative to their original source. In many instances, like the examples from Section 2, this tendency to lift facts without reference to their surrounding context compounds problems that arise from narrowly interpreting the patient query \u2013 queries for why is double mastectomy dangerous versus why is double mastectomy safe might even reference the same balanced document listing pros and cons, but draw facts to support an argument only affirming the original query (Fig. 2C), or queries that presuppose the legitimacy of diagnoses and treatments ignore the obvious intentions of scientific documents designed to question or provide evidence counter to them. More subtle instances omit key conditional details; searching for should i get double mastectomy for cancer yields a citation-grounded line indicating that many patients choose a double mastectomy for personal reasons, such as wanting to avoid the possibility of cancer returning, when the original source clearly indicates that the double mastectomy does not offer preventative benefits over less invasive surgeries except in high-risk patients with specific genetic mutations.\n\u2022 Ignoring biases in motivated sources, a tendency that can also be construed as ignoring patient intent in a more general sense, as patients would likely find information about clear biases in the original source to be relevant to their information needs. This tendency also compounds and highlights issues around narrow interpretations of patient queries at all, as biased sources might appear narrowly relevant to biased queries. Searching for whether a medication is effective draws on citations from sources without mentioning that these are funded advertisements; searching for why are transgender medical interventions in teenagers dangerous yields citations from the \"American College of Pediatricians\", without contextualizing the source as a press release from a socially conservative advocacy group."}, {"title": "3.3. RAG systems do not reason about the downstream implications and consequences of text they produce", "content": "RAG systems produce language, rather than passively interpreting it, and they make particularly dangerous medical communicators because the responses they generate often seem fluent, evidence-based, easily interpretable, and even actionable. More than many other domains, medical queries often stem from more than passive curiosity \u2013 patients are looking for information to make downstream decisions, like agreeing to procedures, choosing amongst alternatives, or deciding whether to see a clinician at all. Pragmatically misleading text production goes hand in hand with interpretation, as responses narrowly construe patient queries but yield dangerous downstream consequences relative to a patients' likely actual goals:\n\u2022 Generations using vague and ambiguous language, including vague adjectives with misleading connotations relative to original, quantitative information from the source document. These pragmatic production issues parallel those involving vague language in the patient query. Responses to a query for rates of underdiagnosis diabetes in us by age summarize a source statistic as a significant increase, when the source document describes a statistically significant but overall small increase (from 10.3% to 11.6%), a term that is easily misconstrued out of its techinical context by lay patients; in other cases, a query for cardiotoxicity after cancer incidence rate (Fig. 2D, top) describes significant cardiotoxicity risk when original sources do not show a technically statistically significant incidence due to cancer treatment in the patient population at large (and in fact attribute negative outcomes to other underlying factors).\n\u2022 Generations that include factually accurate but contextually misleading information, such as responses which answer a subtly different question than what was posed (violating the obvious pragmatic norm that useful responses from a well-intentioned source should, in fact, answer the question as posed or clarify otherwise.) For instance, querying do antibiotics cause colon cancer yields a response which begins yes, taking antibiotics can slightly increase the risk of developing colon cancer \u2013 an opening sentence that is truthful on its own, but which conflates the easily misinterpreted difference between studies which find positive associations between antibiotics and colon cancer, without mentioning the current lack of casual scientific evidence; more complex and misleading results arise with incidental risks, like in the search for does social isolation cause heart attack (Fig. 2D, bottom), which answers with a clear pragmatic implication that there is a known causal link (Yes, according to research) even when many of the primary cited sources actually refer to potential causal mechanisms associated with social isolation, like that socially isolated individuals may engage in less physical activity.\n\u2022 Misleading source citations given likely patient goals, like the generally inferrable assumption that patients likely want up-to-date statistical information, rather than text that appears relevant from outdated sources (such as citing on projected statistics from a decades-old source rather than drawing from actual current data)."}, {"title": "4. Ways forward: building effective medical communicators", "content": "Search engines and online resources address an important public health need. They provide fast, inexpensive, and private sources of health information for some of our most pressing and consequential questions. Citation-grounded Al systems, offer the potential for an even more valuable service \u2013 in their best instantiation, these services improve health literacy (Berkman et al., 2011; Andrus & Roth, 2002; Ferguson & Pawlak, 2011), providing tools for patients to navigate dense scientific information accurately and empathetically to meet their needs.\nTo make good on this promise, however, we argue that systems which generate citation-grounded responses for consequential queries should be designed for effective communication, as we would expect from an attentive and empathetic human expert. Rather than leaving important consequences of communicative reasoning implicit within systems trained towards other, more distant objectives \u2013 leading to brittle and often deeply undesirable results (Collins et al., 2024) \u2013 we suggest that a safe, effective communicative system should be designed to reason about the intentions behind a query, the meaning and contextual relevance of source documents, and the consequences of a query response. Here, we outline ways forward for engineering workflows that can meet this promise and responsibility. We discuss overarching design principles for effective citation-grounded communication, focusing on medical queries, and review relevant formal frameworks for modeling rational communication, relevant ongoing work for engineering systems under these frameworks from linguistics and cognitively-informed AI, and discuss key directions that we hope can focus future engineering efforts towards these goals.\nReasoning about the intentions behind patient queries\nMany of the undesirable behaviors we describe in Sections 2 and 3 begin with failures to reason about the likely intentions, beliefs, and goals that motivate patients to search for online health information in the first place. A patient already searching for the symptoms of a disease probably believes that the disease exists; a patient searching for why a treatment is controversial likely has both a particular prior about the treatment and hopes to use that information for some downstream goal, like deciding whether to pursue treatment. Inferring this broader context, while also keeping in mind outstanding uncertainty about the intentions behind this query, underlies human intuitions about what responses and information might actually be most helpful and relevant.\nWe suggest that computational formalisms developed to explain and predict pragmatic, rational human communication (Goodman & Frank, 2016; Hawkins et al., 2015; Sumers et al., 2024) also provide unifying frameworks for building systems that reason about why someone is asking a particular question, and why they are asking it in a particular way. Bayesian frameworks like the Rational Speech Acts framework (Goodman & Frank, 2016) formalize queries, like other speech acts, as actions produced by motivated agents with rich internal mental states \u2013 the questions we ask reflect our underlying beliefs and goals, and usefully responding intuitively benefits from reasoning about the speaker as an intelligent agent seeking answers against this broader context. Recent work has operationalized these overarching formal frameworks to build concrete artificial agents for applications as various as collaborative instruction following and joint planning (Zhi-Xuan et al., 2024); pragmatic software debugging in response to clarification questions (Chandra et al., 2024); and code generation from examples (Vaithilingam et al., 2023).\nAn important open direction to adapt these formalisms for medical query answering will be designing representations that can scalably formalize common health information needs \u2013 such as explicitly seeking to represent the semantics of common questions with respect to structured representations of disease, symptoms, associated treatments, and statistics, like those in formal medical knowledge graphs (Chen et al., 2019). These structured representations might provide the basis for more sophisticated reasoning about patients' queries or even repeated strings of queries, like ultimately inferring potentially unknown but important diagnoses with respect to repeated queries about symptoms which stem from likely underlying causes. The schema and pragmatics here can be learned from existing patient communication patterns, leveraging datasets including interactions with chatbots, with clinicians on online health forums, and with providers through electronic health record messages (Zhao et al., 2024; Li et al., 2023).\nReasoning about source document intentions and communicative goals. In addition to reasoning about user queries, we also suggest that useful citation-grounding, particularly for communicating health information, requires reasoning about source documents through a communicative lens. In particular, we suggest that systems might benefit from explicit pragmatic inference to reason about the underlying communicative goals behind a source document, both in its own right and relative to other documents on similar themes. Computational models within the formal pragmatic frameworks we reference earlier have been instantiated to explain and predict judgments about linguistic phenomena from persuasion to deception (Barnett et al., 2022; Wiegmann et al., 2022; Papineau & Degen, 2024). In the context of the examples we highlight in Section 2, we see these frameworks as particularly relevant to help identify motivated language from advertisements, politically biased sources, unreviewed preprints, and other less legitimate sources of health information. More broadly, reasoning about what a document intends to say \u2013 and the broader context necessary to interpret any particular detail within it could address the factual decontextualization we highlight throughout Section 2. A key goal for future citation-grounded user interfaces should be to situate specific facts and sources relative to interpretable summaries of their surrounding context, allowing users to retain the accessibility benefits of AI-generated summaries while also helping them navigate and contextualize what they have learned with respect to the richer original source.\nA longer term direction for facilitating health literacy might go beyond these basic principles to build systems which identify which aspects of a document, especially if referenced in follow up or quoted verbatim, might be particularly opaque or confusing to lay reader, much like recent computational work applying formal pragmatic principles to model the obliqueness of \"legalese' in formal law documents (Mart\u00ednez, 2024). We see particular value in reasoning about (and possibly providing automatically generated explanations or context for) technical and quantitative terms, like common, significant, risk, and language about correlationary evidence (which often is interpreted with causal implicatures, Gershman & Ullman 2023) that has particularly important but specific construals within versus outside of a scientific document context.\nReasoning about the interpretation and consequences of a response Finally, closing the loop on incorporating formal communicative and pragmatic frameworks into citation-grounding requires, of course, reasoning about generated responses themselves as language production \u2013 language that will be interpreted in the context of the listeners' likely beliefs and goals, and with likely downstream consequences that impact not only what a patient knows, but their future decisions and actions.\nMuch as it is useful to reason about patient queries as motivated actions, safe and intuitive communicative systems should expect their responses, in turn, to be understood as actions; language is interpreted not only with respect to whether it is narrowly accurate, but as a choice to provide specific information relative to other alternatives (Wilson & Sperber, 2006; Goodman & Frank, 2016). This overarching idea applies directly to the retrieval of 'relevant' documents, as well as the selection of facts within a given\""}, {"title": "5. Alternative Views", "content": "\"Patients should only be directed to look at primary health sources, without any output from language models at all.\" In S4, we discuss algorithmic paradigm shifts to improve the communication of citation-grounded health information. However, one valid viewpoint is that any such system will inherently be imperfect, and as such, it is safest to simply directly refer patients to trusted health websites, without any attempt at answering or synthesis of sources. Providers of such services would potentially be opening themselves up to a regulatory headache by answering health information questions, and therefore for practicality reasons, it is safest to directly provide links alone.\nRebuttal: We agree that a classic search engine is a better alternative than the current state of citation-grounded alternatives, given the nontrivial drawbacks outlined in this paper. However, given that patients are already turning to generative AI for health information, this indicates an information gap in the prior status quo. Patients may not have the health literacy or the bandwidth to synthesize across multiple websites and sources, many of which are dense with esoteric language. While it is important to align with document intent, it may not be necessary for the patient to always read the entire document. Longer term, language model approaches enable personalization via retrieval over electronic health records, so that queries like \u201cmastectomy utility\u201d could be based on the patient's own history.\n\"Models should return specifically what users ask for, without inference or interference. Providing unsolicited information (e.g. around source validity and intent) is unnecessarily overwhelming.\" A very straightforward take is that retrieval-augmented system should do minimal interpretation of any given information query, health or otherwise. If models were to respond pragmatically, instead of literally, the mechanism for retrieval becomes more opaque for the user and decreases the amount of fine-grained control they have re: what gets surfaced. This patronizes users and decreases their agency, particularly for power users. For example, a patient may be searching for complications of a procedures since they are already know all the benefits. Users still retain the ability to click on sources and read further, and it is up to them whether or not they do so. Including extra information simply muddles the transfer of information and clutters user interfaces.\nRebuttal: Studies have shown that confirmation bias is prevalent in search behavior, including for online health information, and this effect is not fully mitigated even by health literacy (Shi et al., 2024; Schweiger et al., 2014; Suzuki & Yamamoto, 2020). Further, a systematic review of studies from 1985 to 2017 found an increase in health anxiety, with links to confirmation bias in online health information seeking (Kosic et al., 2020). This phenomenon will likely only be exacerbated if patients read decontextualized information, seemingly provided from reputable sources. This confirmation bias has been shown to be mitigated by showing preference-inconsistent recommendations (Schwind et al., 2012). Finally, for power users (e.g., researchers, clinicians), separate RAG systems have already been built to enable them to explore scientific literature and evidence, e.g. OpenScholar (Asai et al., 2024). Given no system may be one-size-fits-all, we shouldn't let the needs of power users engulf the needs of the general public."}, {"title": "6. Conclusion", "content": "Online health information has the ability to both (i) educate and empower patients and (ii) negatively reinforce biases and concerns they may have. It is imperative that we design algorithms and systems that actively optimize for the former, as patients' search queries often reflect their biases. Lever-"}]}