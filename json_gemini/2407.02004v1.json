{"title": "SAVE: Segment Audio-Visual Easy way using Segment Anything Model", "authors": ["Khanh-Binh Nguyen", "Chae Jung Park"], "abstract": "The primary aim of Audio-Visual Segmentation (AVS) is to precisely identify and locate auditory elements within visual scenes by accurately predicting segmentation masks at the pixel level. Achieving this involves comprehensively consid-ering data and model aspects to address this task effectively. This study presents a lightweight approach, SAVE, which effi-ciently adapts the pre-trained segment anything model (SAM) to the AVS task. By incorporating an image encoder adapter into the transformer blocks to better capture the distinct dataset information and proposing a residual audio encoder adapter to encode the audio features as a sparse prompt, our proposed model achieves effective audio-visual fusion and interaction during the encoding stage. Our proposed method accelerates the training and inference speed by reducing the input reso-lution from 1024 to 256 pixels while achieving higher per-formance compared with the previous SOTA. Extensive ex-perimentation validates our approach, demonstrating that our proposed model outperforms other SOTA methods signifi-cantly. Moreover, leveraging the pre-trained model on syn-thetic data enhances performance on real AVSBench data, achieving 84.59 mIoU on the S4 (V1S) subset and 70.28 mIoU on the MS3 (V1M) set with only 256 pixels for input images. This increases up to 86.16 mIoU on the S4 (V1S) and 70.83 mIoU on the MS3 (V1M) with inputs of 1024 pixels.", "sections": [{"title": "Introduction", "content": "The task of audio-visual segmentation (AVS) focuses on identifying and delineating auditory elements corresponding to audio cues within video frames. An ideal AVS model re-quires dual-level recognition: semantic and instance levels. However, in practical scenarios, semantic categorization often suffices for localizing sounding objects, achievable by train-ing on artificially constructed data with semantic-consistent image-audio pairs. Prior studies have approached this through self-supervised learning, drawing upon audio-visual signals for training objectives (Chen et al. 2021; Hu et al. 2020, 2021; Lin et al. 2023b; Liu et al. 2022b; Shaharabany et al. 2023). However, these methods lack precise pixel-level supervision, resulting in coarse segmentation. This limitation hampers AVS applications in scenarios demanding accurate segmenta-tion, such as video surveillance, multi-modal video editing, and robotics. Zhou et al. (2022) recently addressed AVS via supervised learning, manually annotating a video dataset with pixel-level segmentation for sound-related objects. Duan et al. (2024) leverages audio and visual modalities as soft prompts to dynamically adjust the parameters of pre-trained mod-els based on the current multi-modal input features. Chen et al. (2023c) proposes a new cost-effective strategy to build challenging and relatively unbiased high-quality audio-visual segmentation benchmarks.\nOn the other hand, Mo and Tian (2023) attempts to lever-age the Segment Anything Model (Kirillov et al. 2023) to perform AVS tasks. However, the basic SAM fails to estab-lish semantic connections between audio and images during the pre-training phase for the encoder. Even when integrat-ing audio prompts in the decoder phase later on, the fusion of audio-visual features remains superficial, proving inadequate for supporting the AVS task due to the lightweight design of the SAM decoder. In contrast, Liu et al. (2024) suggests a fu-sion of the visual and audio features by employing an adapter that injects the audio features into each transformer block in the image encoder. Despite demonstrating some effective-ness, SAMA-AVS overlooked the significance of the sparse prompt towards the mask decoder of SAM, a crucial element for achieving superior performance. Another notable work is GAVS (Wang et al. 2023b), which addresses this downside of SAMA-AVS by employing a Semantic-aware Audio Prompt (SAP) to assist the visual foundation model in focusing on sounding objects. However, it also neglected the importance of the fusion of visual and audio features. Moreover, (Mo and Tian 2023; Liu et al. 2024; Wang et al. 2023b) are ineffi-cient methods since they require an input of 1024 pixels with the ViT huge architecture (Dosovitskiy et al. 2020), which induces a large number of computational.\nConsequently, this study seeks to overcome these limita-tions by introducing two innovative modules: 1) an image encoder adapter and 2) a residual audio encoder adapter. In contrast to the SAMA-AVS adapter, our design employs a residual connection in the residual audio encoder adapter to preserve the information introduced into each transformer block. Subsequently, our image encoder adapter is designed to enhance the audio-visual fused features in both channel and spatial dimensions. The output from the final audio resid-ual audio encoder adapter serves as the sparse prompt for SAM\u2019s mask decoder, containing the audio-agnostic infor-"}, {"title": "Related Work", "content": "In recent studies, researchers have delved into a spectrum of audio-visual tasks to gain a comprehensive grasp of multime-dia resources. These tasks span audio-visual sound separation (Gao and Grauman 2021; Tzinis et al. 2022), visual sound source localization (Chen et al. 2021; Hu et al. 2021; Lin et al. 2023b; Liu et al. 2022b; Qian et al. 2020; Song et al. 2022), and audio-visual video understanding (Lee et al. 2020a; Tian, Li, and Xu 2020). Visual sound source localization (VSSL), also known as audio-visual segmentation, aims to pinpoint and segment regions of sounding objects based on their au-dio signals. Current approaches (Chen et al. 2021; Liu et al. 2022b) primarily hinge on the co-occurrence of audio and visual signals, providing weak instance-level supervision, which poses challenges in predicting precise pixel-level seg-mentation masks.\nTo address this limitation, Zhou et al. (2022) introduced a benchmark dataset annotated with masks for sounding objects in video frames, marking a significant step in supervised AVS tasks. Segment Anything (SAM) (Kirillov et al. 2023), the pioneering model for image segmentation, was pre-trained on a vast dataset comprising millions of images associated with billions of segmentation masks. The versatility of SAM ex-tends to various prompts, including points, boxes, masks, and texts, making it applicable across diverse visual problems, such as medical image segmentation (Cheng et al. 2023; Gao et al. 2023b; Ma and Wang 2023; Shaharabany et al. 2023; Wang et al. 2023a; Wu et al. 2023), weakly-supervised se-mantic segmentation (Chen et al. 2023a; He et al. 2023; Jiang and Yang 2023), few-shot segmentation (Liu et al. 2023e; Zhang et al. 2023), 3D vision (Chen et al. 2021; Liu et al. 2023d), shadow detection (Chen et al. 2023b; Wang et al. 2023c; Zhang, Gu, and Zhu 2023), and camouflaged object segmentation (Chen et al. 2023b; Ji et al. 2023a,b).\nWhile some works (Mo and Tian 2023; Liu et al. 2024; Wang et al. 2023b) have explored SAM for the AVS task, their outcomes have been less satisfactory. In this paper, we evaluate the performance of vanilla SAM and introduce SAM-based models specifically designed for AVS tasks. Adapters, recognized as an efficient method in parameter-efficient trans-fer learning, introduce minimal parameters to effectively har-ness knowledge from pre-trained models and apply it to re-"}, {"title": "Method", "content": "Image Encoder Adapter module\nThe most resource-demanding part of SAM is the global update of the image encoder during the fine-tuning process, which leads to significant computational costs. To infuse knowledge specific to the dataset into the image encoder in a cost-effective manner, we introduce an image encoder adapter layer. Specifically, during fine-tuning, we freeze all parameters of the original image encoder and add a proposed image encoder adapter to each transformer block, as shown in Figure 1. We modify the image encoder across both the channel and spatial dimensions. For the channel dimension, we first reduce the resolution of the input feature map to $C\\times 1 \\times 1$ using global average pooling. Then, a linear layer is employed to shrink the channel embeddings and another linear layer to expand them, keeping a compression ratio of 0.25. Next, we calculate the weights for the channel dimen-sion using a sigmoid function and multiply them with the input feature map to produce the input for the next layer. For the spatial dimension, we downsample the spatial resolution of the feature map by a factor of two using a convolutional layer and restore the spatial resolution using a transposed convolution, maintaining the same number of channels as the input. The overall function for the transformer block with the proposed image encoder adapter is defined as:\n$x' = MHSA(LN(X_{i-1})) + X_{i-1}$\n$x_i = MLP(LN(x')) + Adapter(LN(x')) + x'$   (1)\nwhere MHSA, MLP, LN, and Adapter are the Multi-head Self-Attention, Feed-Forward Network, Layer Norm, and our proposed Adapter layer, respectively. $x$ denotes the input, and $i$ indicates the i-th element.\nResidual Audio Encoder Adapter module\nThe residual audio encoder adapter is a composition of multiple audio encoder adapters, interconnected via a residual connection. Each audio encoder adapter (PE) consists of a 2-layer MLP and a residual connection. The quantity of these encoders corresponds to the number of transformer blocks in the image encoder, as we inject the audio features subsequent to each transformer block. The output of the residual audio encoder adapter is subsequently utilized as the sparse prompt. However, due to a misalignment in dimensions, we employ an MLP to map the feature from the image encoder embed-ding dimension ($C \\times H \\times W$) into the prompt embedding dimension (256 \u00d7 H \u00d7 W). Finally, this sparse prompt is input into the mask decoder and fine-tuned with the gradients from the audio-visual fused features following SAM procedure.\n$\\begin{aligned}\\text{prompt}\\_{\\text{out}} = PE\\_N(F\\_A + PE\\_{N-1}(F\\_A))\\\\ \\quad + PE\\_{N-1}(F\\_A + PE\\_{N-2}(F\\_A)) + \\cdots + PE\\_1(F\\_A)\\\\PE\\_i(F\\_A) = MLP(F\\_A) + F\\_A\\end{aligned}$   (2)\nwhere $\\text{prompt}\\_{\\text{out}}$ is the sparse prompt to use for the mask decoder and $PE\\_i$ is the i-th audio encoder adapter.\nFine-tuning strategy\nThere are several issues of SAM-based models in terms of computational burden when using ViT-H architecture and an input image resolution of 1024 \u00d7 1024 pixels. To ease these drawbacks, we resize the input image to 256 \u00d7 256 pixels. This resizing strategy enables training on low-memory GPUs such as the NVIDIA GeForce RTX 3090, which only has 24GB of memory. In addition, using lower-resolution input also accelerates the training with a larger batch size and reduces the training as well as inference time.\nSpecifically, we modify the Relative Position Encoding token from the pre-trained SAM by applying bilinear inter-polation to 256 \u00d7 256 pixels. However, such interpolation notably diminishes performance. Therefore, employing an adapter for the image encoder is the best approach to mitigat-ing this performance degradation.\nLearning Objectives\nSegmentation Loss. During the training process of the model, we employ the binary cross-entropy loss, denoted as $BCE()$, to quantify the discrepancy between the mask predicted by the model and the actual mask as:\n$\\mathcal{L}\\_{\\text{seg}} = BCE(M\\_{\\text{pred}}, M\\_{\\text{gt}})$   (3)\nwhere $M\\_{\\text{pred}}$ and $M\\_{\\text{gt}}$ are predicted masks and ground truth masks, respectively.\nSemantic Loss. We adopt a simple IoU (Intersection over Union) loss to optimize the IoU between predicted masks and the ground truth masks following Liu et al. (2024).\nTotal Loss. The final loss function is calculated as the sum of the two aforementioned losses:\n$\\mathcal{L}\\_{\\text{seg}} = \\mathcal{L}\\_{\\text{seg}} + \\mathcal{L}\\_{\\text{IOU}}$   (4)"}, {"title": "Experiments", "content": "Datasets\nAVSBench dataset The AVSBench dataset (Zhou et al. 2022) is a newly launched dataset for audio-visual segmen-tation, which comes with carefully human-annotated mask annotations. This dataset is divided into two unique subsets: the semi-supervised Single Sound Source Segmentation (S4) and the fully supervised Multiple Sound Source Segmenta-tion (MS3).\nAVSBench-S4 subset. The AVSBench S4 (V1S) dataset includes videos that contain at most one sounding object. During the training phase, only the first frame of each video sequence is annotated with a ground-truth segmentation bi-nary mask. The goal during inference is to segment sound-ing objects in all frames of the video. This subset contains 3,452/740/740 videos for the training, validation, and test sets, respectively, amounting to 10,852 annotated frames.\nAVSBench-MS3 subset. On the other hand, the AVSBench MS3 (V1M) dataset is made up of videos that may contain multiple-sounding objects. All frames in these videos are annotated with masks for both the training and evaluation stages. This subset has 296/64/64 videos for training, val-idation, and testing, respectively, totaling 2,120 annotated frames.\nAVSBench-Semantic dataset The AVSBench-Semantic (AVSS) dataset is an enriched dataset by adding a third Semantic-labels subset that provides semantic segmenta-tion maps as labels. AVSS contains S4 and MS3 subsets, namely V1S and V1M, respectively. AVSS is enriched in video amount and audio-visual scene categories. We only use the additional subset, V2, to evaluate our model. The V2 subset contains 6,000 videos in 70 categories, which are split into 4,750/500/750 for training, validation, and testing, respectively. For each video, 10 frames are extracted, which creates 60,000 frames in total.\nAVS-Synthetic dataset The AVS-Synthetic dataset (Liu et al. 2024) is introduced, encompassing 62,609 instances of sounding objects across 46 common categories. The size of the training set is 52,609, and both the validation and test sets contain 5,000 instances. The process of constructing this dataset utilizes existing image segmentation datasets such as LVIS and Open Images, and audio classification datasets such as VGGSound.\nImplementation Details\nOur approach is built using PyTorch and is trained on 2 NVIDIA RTX A6000 GPUs. Except for the resolution of 224 used for the V2 subset, all experiments are conducted using resolutions of 224 and 1024. We employ the AdamW optimizer with a starting learning rate of 2 \u00d7 10e-4 and carry out training over 80 epochs, using cosine decay. During the training phase, all images are resized to resolutions of 256 \u00d7 256 (224 \u00d7 224) and 1024 \u00d7 1024. Our strategy for resizing includes zero-padding the edges for images where both the width and height are less than 256 (224) and using bilinear interpolation for resizing images in other scenarios. The loss function that oversees the mask predictions is a blend of binary cross entropy and IoU loss, as indicated in Section. The batch size is established at 2 for a resolution of 1024, and 16 for a resolution of 256 and 224 per GPU.\nWe measure the segmentation quality by employing estab-lished segmentation metrics detailed in (Zhou et al. 2022). Specifically, we denoted $M_J$ as the mean Intersection-over-Union (mIoU) between ground-truth binary masks and pre-dicted masks. Additionally, we use $M_F$, referred to as the F-score, representing the harmonic mean of precision and"}, {"title": "Qualitative Examples", "content": "Figure 2 presents a qualitative comparison of the results generated by SAMA-AVS and our proposed SAVE on AVS-Bench. The results demonstrate that the segmentation masks produced by SAVE align more closely with the ground truths across both subsets. Our model is capable of accurately seg-menting the entire body of the sounding objects without omitting any parts. It also outperforms SAMA-AVS in cap-turing the edges and intricate details of the sounding target, as evidenced by the clear depiction of the body of a cat and the gun ammo in Figure 2a.\nInterestingly, SAMA-AVS struggled to differentiate be-tween the sounding object and the associated object, such as the gun and the ammo. In contrast, SAVE accomplished this task without generating incorrect masks. In the MS3 set-ting, our model also demonstrated its ability to localize and segment multiple targets from the sound accurately. Even in scenarios where multiple objects overlap or are absent in the ground truth, SAVE provides accurate segmentation masks, as illustrated in Figure 2b."}]}