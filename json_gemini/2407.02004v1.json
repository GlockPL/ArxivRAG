{"title": "SAVE: Segment Audio-Visual Easy way using Segment Anything Model", "authors": ["Khanh-Binh Nguyen", "Chae Jung Park"], "abstract": "The primary aim of Audio-Visual Segmentation (AVS) is to precisely identify and locate auditory elements within visual scenes by accurately predicting segmentation masks at the pixel level. Achieving this involves comprehensively consid-ering data and model aspects to address this task effectively. This study presents a lightweight approach, SAVE, which effi-ciently adapts the pre-trained segment anything model (SAM) to the AVS task. By incorporating an image encoder adapter into the transformer blocks to better capture the distinct dataset information and proposing a residual audio encoder adapter to encode the audio features as a sparse prompt, our proposed model achieves effective audio-visual fusion and interaction during the encoding stage. Our proposed method accelerates the training and inference speed by reducing the input reso-lution from 1024 to 256 pixels while achieving higher per-formance compared with the previous SOTA. Extensive ex-perimentation validates our approach, demonstrating that our proposed model outperforms other SOTA methods signifi-cantly. Moreover, leveraging the pre-trained model on syn-thetic data enhances performance on real AVSBench data, achieving 84.59 mIoU on the S4 (V1S) subset and 70.28 mIoU on the MS3 (V1M) set with only 256 pixels for input images. This increases up to 86.16 mIoU on the S4 (V1S) and 70.83 mIoU on the MS3 (V1M) with inputs of 1024 pixels.", "sections": [{"title": "Introduction", "content": "The task of audio-visual segmentation (AVS) focuses on identifying and delineating auditory elements corresponding to audio cues within video frames. An ideal AVS model re-quires dual-level recognition: semantic and instance levels. However, in practical scenarios, semantic categorization often suffices for localizing sounding objects, achievable by train-ing on artificially constructed data with semantic-consistent image-audio pairs. Prior studies have approached this through self-supervised learning, drawing upon audio-visual signals for training objectives (Chen et al. 2021; Hu et al. 2020, 2021; Lin et al. 2023b; Liu et al. 2022b; Shaharabany et al. 2023). However, these methods lack precise pixel-level supervision, resulting in coarse segmentation. This limitation hampers AVS applications in scenarios demanding accurate segmenta-tion, such as video surveillance, multi-modal video editing, and robotics. Zhou et al. (2022) recently addressed AVS via supervised learning, manually annotating a video dataset with pixel-level segmentation for sound-related objects. Duan et al. (2024) leverages audio and visual modalities as soft prompts to dynamically adjust the parameters of pre-trained mod-els based on the current multi-modal input features. Chen et al. (2023c) proposes a new cost-effective strategy to build challenging and relatively unbiased high-quality audio-visual segmentation benchmarks.\nOn the other hand, Mo and Tian (2023) attempts to lever-age the Segment Anything Model (Kirillov et al. 2023) to perform AVS tasks. However, the basic SAM fails to estab-lish semantic connections between audio and images during the pre-training phase for the encoder. Even when integrat-ing audio prompts in the decoder phase later on, the fusion of audio-visual features remains superficial, proving inadequate for supporting the AVS task due to the lightweight design of the SAM decoder. In contrast, Liu et al. (2024) suggests a fu-sion of the visual and audio features by employing an adapter that injects the audio features into each transformer block in the image encoder. Despite demonstrating some effective-ness, SAMA-AVS overlooked the significance of the sparse prompt towards the mask decoder of SAM, a crucial element for achieving superior performance. Another notable work is GAVS (Wang et al. 2023b), which addresses this downside of SAMA-AVS by employing a Semantic-aware Audio Prompt (SAP) to assist the visual foundation model in focusing on sounding objects. However, it also neglected the importance of the fusion of visual and audio features. Moreover, (Mo and Tian 2023; Liu et al. 2024; Wang et al. 2023b) are ineffi-cient methods since they require an input of 1024 pixels with the ViT huge architecture (Dosovitskiy et al. 2020), which induces a large number of computational.\nConsequently, this study seeks to overcome these limita-tions by introducing two innovative modules: 1) an image encoder adapter and 2) a residual audio encoder adapter. In contrast to the SAMA-AVS adapter, our design employs a residual connection in the residual audio encoder adapter to preserve the information introduced into each transformer block. Subsequently, our image encoder adapter is designed to enhance the audio-visual fused features in both channel and spatial dimensions. The output from the final audio resid-ual audio encoder adapter serves as the sparse prompt for SAM's mask decoder, containing the audio-agnostic infor-"}, {"title": "Related Work", "content": "In recent studies, researchers have delved into a spectrum of audio-visual tasks to gain a comprehensive grasp of multime-dia resources. These tasks span audio-visual sound separation (Gao and Grauman 2021; Tzinis et al. 2022), visual sound source localization (Chen et al. 2021; Hu et al. 2021; Lin et al. 2023b; Liu et al. 2022b; Qian et al. 2020; Song et al. 2022), and audio-visual video understanding (Lee et al. 2020a; Tian, Li, and Xu 2020). Visual sound source localization (VSSL), also known as audio-visual segmentation, aims to pinpoint and segment regions of sounding objects based on their au-dio signals. Current approaches (Chen et al. 2021; Liu et al. 2022b) primarily hinge on the co-occurrence of audio and visual signals, providing weak instance-level supervision, which poses challenges in predicting precise pixel-level seg-mentation masks.\nTo address this limitation, Zhou et al. (2022) introduced a benchmark dataset annotated with masks for sounding objects in video frames, marking a significant step in supervised AVS tasks. Segment Anything (SAM) (Kirillov et al. 2023), the pioneering model for image segmentation, was pre-trained on a vast dataset comprising millions of images associated with billions of segmentation masks. The versatility of SAM ex-tends to various prompts, including points, boxes, masks, and texts, making it applicable across diverse visual problems, such as medical image segmentation (Cheng et al. 2023; Gao et al. 2023b; Ma and Wang 2023; Shaharabany et al. 2023; Wang et al. 2023a; Wu et al. 2023), weakly-supervised se-mantic segmentation (Chen et al. 2023a; He et al. 2023; Jiang and Yang 2023), few-shot segmentation (Liu et al. 2023e; Zhang et al. 2023), 3D vision (Chen et al. 2021; Liu et al. 2023d), shadow detection (Chen et al. 2023b; Wang et al. 2023c; Zhang, Gu, and Zhu 2023), and camouflaged object segmentation (Chen et al. 2023b; Ji et al. 2023a,b).\nWhile some works (Mo and Tian 2023; Liu et al. 2024; Wang et al. 2023b) have explored SAM for the AVS task, their outcomes have been less satisfactory. In this paper, we evaluate the performance of vanilla SAM and introduce SAM-based models specifically designed for AVS tasks. Adapters, recognized as an efficient method in parameter-efficient trans-fer learning, introduce minimal parameters to effectively har-ness knowledge from pre-trained models and apply it to re-"}, {"title": "Method", "content": "Image Encoder Adapter module\nThe most resource-demanding part of SAM is the global update of the image encoder during the fine-tuning process, which leads to significant computational costs. To infuse knowledge specific to the dataset into the image encoder in a cost-effective manner, we introduce an image encoder adapter layer. Specifically, during fine-tuning, we freeze all parameters of the original image encoder and add a proposed image encoder adapter to each transformer block, as shown in Figure 1. We modify the image encoder across both the channel and spatial dimensions. For the channel dimension, we first reduce the resolution of the input feature map to $C\\times 1 \\times 1$ using global average pooling. Then, a linear layer is employed to shrink the channel embeddings and another linear layer to expand them, keeping a compression ratio of 0.25. Next, we calculate the weights for the channel dimen-sion using a sigmoid function and multiply them with the input feature map to produce the input for the next layer. For the spatial dimension, we downsample the spatial resolution of the feature map by a factor of two using a convolutional layer and restore the spatial resolution using a transposed convolution, maintaining the same number of channels as the input. The overall function for the transformer block with the proposed image encoder adapter is defined as:\n$x = MHSA(LN(X_{i-1})) + X_{i-1}$\n$x_i = MLP(LN(x)) + Adapter(LN(x)) + x$\nwhere MHSA, MLP, LN, and Adapter are the Multi-head Self-Attention, Feed-Forward Network, Layer Norm, and our proposed Adapter layer, respectively. x denotes the input, and i indicates the i-th element.\nResidual Audio Encoder Adapter module\nThe residual audio encoder adapter is a composition of multiple audio encoder adapters, interconnected via a residual connection. Each audio encoder adapter (PE) consists of a 2-layer MLP and a residual connection. The quantity of these encoders corresponds to the number of transformer blocks in the image encoder, as we inject the audio features subsequent to each transformer block. The output of the residual audio encoder adapter is subsequently utilized as the sparse prompt. However, due to a misalignment in dimensions, we employ an MLP to map the feature from the image encoder embed-ding dimension ($C \\times H \\times W$) into the prompt embedding dimension ($256 \\times H \\times W$). Finally, this sparse prompt is input into the mask decoder and fine-tuned with the gradients from the audio-visual fused features following SAM procedure.\npromptout = PEN(FA + PEN\u22121(FA))\n+ PEN-1(FA + PEN\u22122(FA)) + \u00b7\u00b7\u00b7 + PE1(FA)\nPE\u00bf(FA) = MLP(FA) + FA\nwhere promptout is the sparse prompt to use for the mask decoder and PE\u00bf is the i-th audio encoder adapter.\nFine-tuning strategy\nThere are several issues of SAM-based models in terms of computational burden when using ViT-H architecture and an input image resolution of 1024 \u00d7 1024 pixels. To ease these drawbacks, we resize the input image to 256 \u00d7 256 pixels. This resizing strategy enables training on low-memory GPUs such as the NVIDIA GeForce RTX 3090, which only has 24GB of memory. In addition, using lower-resolution input also accelerates the training with a larger batch size and reduces the training as well as inference time.\nSpecifically, we modify the Relative Position Encoding token from the pre-trained SAM by applying bilinear inter-polation to 256 \u00d7 256 pixels. However, such interpolation notably diminishes performance. Therefore, employing an adapter for the image encoder is the best approach to mitigat-ing this performance degradation.\nLearning Objectives\nSegmentation Loss. During the training process of the model, we employ the binary cross-entropy loss, denoted as BCE(), to quantify the discrepancy between the mask predicted by the model and the actual mask as:\nLseg = BCE(Mpred, Mgt)\nwhere Mpred and Mgt are predicted masks and ground truth masks, respectively.\nSemantic Loss. We adopt a simple IoU (Intersection over Union) loss to optimize the IoU between predicted masks and the ground truth masks following Liu et al. (2024).\nTotal Loss. The final loss function is calculated as the sum of the two aforementioned losses:\nLseg = Lseg + LIOU"}, {"title": "Experiments", "content": "Datasets\nAVSBench dataset The AVSBench dataset (Zhou et al. 2022) is a newly launched dataset for audio-visual segmen-tation, which comes with carefully human-annotated mask annotations. This dataset is divided into two unique subsets: the semi-supervised Single Sound Source Segmentation (S4) and the fully supervised Multiple Sound Source Segmenta-tion (MS3).\nAVSBench-S4 subset. The AVSBench S4 (V1S) dataset includes videos that contain at most one sounding object. During the training phase, only the first frame of each video sequence is annotated with a ground-truth segmentation bi-nary mask. The goal during inference is to segment sound-ing objects in all frames of the video. This subset contains 3,452/740/740 videos for the training, validation, and test sets, respectively, amounting to 10,852 annotated frames.\nAVSBench-MS3 subset. On the other hand, the AVSBench MS3 (V1M) dataset is made up of videos that may contain multiple-sounding objects. All frames in these videos are annotated with masks for both the training and evaluation stages. This subset has 296/64/64 videos for training, val-idation, and testing, respectively, totaling 2,120 annotated frames.\nAVSBench-Semantic dataset The AVSBench-Semantic (AVSS) dataset is an enriched dataset by adding a third Semantic-labels subset that provides semantic segmenta-tion maps as labels. AVSS contains S4 and MS3 subsets, namely V1S and V1M, respectively. AVSS is enriched in video amount and audio-visual scene categories. We only use the additional subset, V2, to evaluate our model. The V2 subset contains 6,000 videos in 70 categories, which are split into 4,750/500/750 for training, validation, and testing, respectively. For each video, 10 frames are extracted, which creates 60,000 frames in total.\nAVS-Synthetic dataset The AVS-Synthetic dataset (Liu et al. 2024) is introduced, encompassing 62,609 instances of sounding objects across 46 common categories. The size of the training set is 52,609, and both the validation and test sets contain 5,000 instances. The process of constructing this dataset utilizes existing image segmentation datasets such as LVIS and Open Images, and audio classification datasets such as VGGSound.\nImplementation Details\nOur approach is built using PyTorch and is trained on 2 NVIDIA RTX A6000 GPUs. Except for the resolution of 224 used for the V2 subset, all experiments are conducted using resolutions of 224 and 1024. We employ the AdamW optimizer with a starting learning rate of 2 \u00d7 10e-4 and carry out training over 80 epochs, using cosine decay. During the training phase, all images are resized to resolutions of 256 \u00d7 256 (224 \u00d7 224) and 1024 \u00d7 1024. Our strategy for resizing includes zero-padding the edges for images where both the width and height are less than 256 (224) and using bilinear interpolation for resizing images in other scenarios. The loss function that oversees the mask predictions is a blend of binary cross entropy and IoU loss, as indicated in Section. The batch size is established at 2 for a resolution of 1024, and 16 for a resolution of 256 and 224 per GPU. We measure the segmentation quality by employing estab-lished segmentation metrics detailed in (Zhou et al. 2022). Specifically, we denoted My as the mean Intersection-over-Union (mIoU) between ground-truth binary masks and pre-dicted masks. Additionally, we use MF, referred to as the F-score, representing the harmonic mean of precision and"}, {"title": "Comparison with SAM-based methods", "content": "We have assessed the performance of various methods on AVSBench, which include SAM (Kirillov et al. 2023), AV-SAM (Mo and Tian 2023), AP-SAM (Liu et al. 2024), SAMA-AVS (Liu et al. 2024), and GAVS (Wang et al. 2023b). For the SAM baseline, we utilized the pretrained model weights of the ViT-H SAM model without any additional training and adopted the maximum segmentation evaluation. The results from AV-SAM (Mo and Tian 2023), AP-SAM (Liu et al. 2024), SAMA-AVS (Liu et al. 2024), and GAVS (Wang et al. 2023b) are cited directly from their original papers. The results, as displayed in Table 1, reveal that our proposed SAVE method significantly outperforms all the SAM-based methods by a substantial margin across both subsets. Notably, on the S4 subset, SAVE exceeds the pre-vious state-of-the-art, SAMA-AVS, with an improvement of 2.53 M\u1ef9 while only using input images of 256 pixels and 3.58 M\u1ef9 when using 1024 pixels. This underscores the effectiveness of our proposed image encoder adapter."}, {"title": "Comparison with other SOTA methods on AVSBench S4 and MS3 subsets", "content": "Our proposed SAVE method has been compared with the latest AVS techniques (Ling et al. 2023; Gao et al. 2023a; Liu et al. 2023a,b; Mao et al. 2023; Zhou et al. 2022; Huang et al. 2023), as well as other related audio-visual methods. These include sound source localization (SSL) methods such as LVS (Chen et al. 2021) and MSSL (Qian et al. 2020), video object segmentation (VOS) methods like 3DC (Mahadevan et al. 2020) and SST (Duke et al. 2021), and salient object detection (SOD) methods such as iGAN (Mao et al. 2021) and LGVT (Zhang et al. 2021). As per the results in Table 2, our approach surpasses exist-ing methods across all metrics in both subsets. For instance, it"}, {"title": "Comparison with other SOTA methods on AVSS V2", "content": "As shown in Table 3, we evaluate the capabilities of our model on the AVSS V2 subset. As we have defined in Section, the V2 subset has more data as well as many categories, which makes it more difficult than the S4 and MS3 subsets. For the V2 subset, we resize the input image into 224 \u00d7 224 pixels for SAVE and SAMA-AVS (Liu et al. 2024) for a fair comparison with other methods. Other training hyperparameters are kept the same as defined in Section.\nThe proposed method, SAVE, achieves the result of 71.28 for the M\u1ef9 and 0.773 for the MF, which significantly out-performs the SOTA method - GAVS (Wang et al. 2023b) by a large margin of 3.58 M\u1ef9. Interestingly, when we apply our fine-tuning strategy with SAMA-AVS (Liu et al. 2024) for this subset, it can achieve a high performance of 70.48 and 0.770 for M\u1ef9 and MF, respectively. This hinders the SAM-based methods, which could benefit from the fine-tuning strategy rather than the high resolution. This indicates that the fine-tuned model learned specific knowledge of the do-main, and fine-tuning with low cost is an effective and fea-sible method to reduce domain differences. Based on this observation, we further study the fine-tuning strategy for SAMA-AVS in Section."}, {"title": "Comparison with other SOTA methods for unseen classes on AVSS V3 subset", "content": "In line with the approach proposed by Wang et al. (2023b), we utilize their suggested V3 subset to evaluate the ability of models to generalize to unseen classes for the AVS task. This evaluation is conducted under four different settings: 0-shot, 1-shot, 3-shot, and 5-shot. For the zero-shot test, objects in the test set are not included in the training or validation stages. In contrast, for the other settings, N = [1, 3, 5] data samples are selected for each object and incorporated into the training process to facilitate few-shot learning.\nAs indicated in Table 4, our model delivers the highest performance in the 0-shot setting, demonstrating superior generalization capabilities when faced with unseen objects. Remarkably, the 0-shot performance of SAVE is already on par with the 5-shot performance of GAVS (Wang et al. 2023b), and it surpasses the best 0-shot result by a signifi-"}, {"title": "Ablation study", "content": "Zero-shot results on S4 subset\nIn this section, we evaluate the pretrained model using syn-thetic data on the S4 subset (Zhou et al. 2022). Table 5 quan-titatively showcases the results for the overlapping categories between AVS-Synthetic and AVSBench. As can be seen, SAVE achieves remarkable results across the categories by a large gap compared with SAMA-AVS under the same set-tings. Especially in certain categories, such as \"ambulance\", \"cat\", \"dog\", \"horse\", and \"guitar\", SAVE exceeds SAMA-AVS when only using the synthetic data for training, which demonstrates the effectiveness of SAVE for zero-shot transfer to real data. Furthermore, the model trained with both real and synthetic data outmatches the others across almost all categories.\nFurthermore, the results presented in Table 6 compare the performance of SAVE and SAMA-AVS under zero-shot and full-shot training on the S4 subset, with or without pre-training on synthetic data. Although the performance of SAVE is lower when both pre-trained synthetic data and 0% real data are used, this testing scheme (without training) does not reflect real-world applications. This is due to the additional learnable image encoder adapter, which requires training to obtain the information from the data.\nNevertheless, SAVE consistently delivers high perfor-mance, whether pre-trained on synthetic data or not. Notably, the model that is solely trained on AVS-Synthetic achieves a mean Intersection over Union (mIoU) of 69.85 (My) with-out the use of any real data. When 100% real data is used, SAVE enhances its performance by 3.53 M\u1ef9 and 1.99 M\u1ef9, achieving scores of 85.16 M\u1ef9 and 84.59 M\u1ef9 with and without the weights of the AVS-Synthetic pre-trained model, respectively."}, {"title": "Effectiveness of each module toward the results on MS3 (V1M) and S4 (V1S) subsets", "content": "In this section, we experiment with each additional module (image encoder adapter and residual audio encoder adapter) to show the improvement from each of them. The results are shown in Table 7. For a fair comparison, we also fine-tune the SAMA-AVS with 256 input image resolution. As we can see, inference SAMA-AVS directly with 256 pixels has a significant drop in performance. Contrasting with it, fine-tuning the mask decoder for the input resolution of 256 shows comparable performance to its 1024 pixels version on S4 and lower performance on the MS3 subset. This strongly proves that SAM-based methods do not depend heavily on the input resolution to achieve high performance. With proper fine-tuning, one can have as high performance as using 1024 \u00d7 1024 pixels while only using 256 \u00d7 256 pixels.\nSAVE, on the other hand, consistently improves perfor-mance. Specifically, SAVE-256 using only a residual audio encoder adapter with a fine-tuning strategy shows much per-formance compared with SAMA-AVS using 1024 pixels. With the additional image encoder adapter, the performance outperforms the SAMA-AVS significantly on both subsets. In-terestingly, SAVE-256 performs better on the S4 subset than SAVE-1024, and vice versa, SAVE-1024 provides a much better segmentation mask on the MS3 subset. This shows that we could consider the trade-off between efficiency and performance when dealing with single/multiple objects."}, {"title": "Ablation for memory and training time efficiency", "content": "We benchmark the batch size and inference FPS for both SAVE and SAMA-AVS (Liu et al. 2024). SAVE-256 enables the training with a higher batch size of 16 per GPU as well as increasing the inference FPS to 35, while SAVE-1024 and SAMA-AVS only allow a batch size of 2 and an FPS of 8."}, {"title": "Ablation for audio encoder adapter", "content": "In this section, we perform the study for choosing the number of layers in MLP of an audio encoder adapter. The results are recorded in Table 9.\nAs we can see, using 2-layer MLP gives better results for both resolutions that we tested (256 and 1024). Thus, for the rest of our experiments, all of the MLP is a 2-layer MLP."}, {"title": "Conclusion", "content": "In this study, we present SAVE, a potent audio-visual seg-mentation model that adheres to the encoder-prompt-decoder paradigm. This model addresses the growing need for precise localization in real-world scenarios where annotated data is limited. Our proposed method excels at achieving general-izable cross-modal segmentation. It generates high-quality segmentation masks using only low-resolution input images. Furthermore, SAVE facilitates faster training and serves as a memory-efficient framework, delivering high performance. The extensive experiments show that SAVE outperforms the previous SOTA by a large margin across benchmark scenar-ios, from fully supervised to zero-shot and few-shot ones."}, {"title": "Qualitative Examples", "content": "Figure 2 presents a qualitative comparison of the results generated by SAMA-AVS and our proposed SAVE on AVS-Bench. The results demonstrate that the segmentation masks produced by SAVE align more closely with the ground truths across both subsets. Our model is capable of accurately seg-menting the entire body of the sounding objects without omitting any parts. It also outperforms SAMA-AVS in cap-turing the edges and intricate details of the sounding target, as evidenced by the clear depiction of the body of a cat and the gun ammo in Figure 2a.\nInterestingly, SAMA-AVS struggled to differentiate be-tween the sounding object and the associated object, such as the gun and the ammo. In contrast, SAVE accomplished this task without generating incorrect masks. In the MS3 set-ting, our model also demonstrated its ability to localize and segment multiple targets from the sound accurately. Even in scenarios where multiple objects overlap or are absent in the ground truth, SAVE provides accurate segmentation masks, as illustrated in Figure 2b."}]}