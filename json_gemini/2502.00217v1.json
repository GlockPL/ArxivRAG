{"title": "Fantastic Multi-Task Gradient Updates and How to Find Them In a Cone", "authors": ["Negar Hassanpour", "Muhammad Kamran Janjua", "Kunlin Zhang", "Sepehr Lavasani", "Xiaowen Zhang", "Chunhua Zhou", "Chao Gao"], "abstract": "Balancing competing objectives remains a fundamental challenge in multi-task learning (MTL), primarily due to conflicting gradients across individual tasks. A common solution relies on computing a dynamic gradient update vector that balances competing tasks as optimization progresses. Building on this idea, we propose CONICGRAD, a principled, scalable, and robust MTL approach formulated as a constrained optimization problem. Our method introduces an angular constraint to dynamically regulate gradient update directions, confining them within a cone centered on the reference gradient of the overall objective. By balancing task-specific gradients without over-constraining their direction or magnitude, CONICGRAD effectively resolves inter-task gradient conflicts. Moreover, our framework ensures computational efficiency and scalability to high-dimensional parameter spaces. We conduct extensive experiments on standard supervised learning and reinforcement learning MTL benchmarks, and demonstrate that CONICGRAD achieves state-of-the-art performance across diverse tasks.", "sections": [{"title": "1. Introduction", "content": "In many real-world machine learning applications, resources such as data, computation, and memory are limited (Navon et al., 2022; Yu et al., 2020a; Xiao et al., 2024). Therefore, instead of Single-Task Learning (STL) (Long et al., 2015; He et al., 2017) that trains an independent model for each downstream task, it is often advantageous to share parts of the model structure across multiple tasks, a paradigm known as Multi-Task Learning (MTL) (Vandenhende et al., 2021). MTL aims to learn a shared representation while simultaneously optimizing for several different tasks, thereby improving efficiency and generalization.\nMTL approaches can be broadly categorized into multi-task architectures (Lin et al., 2025; Zhang et al., 2025) and optimization strategies (Navon et al., 2022; Yu et al., 2020a; Xiao et al., 2024; Liu et al., 2023). Architectural methods leverage parameter sharing to reduce redundancy and maximize learning across tasks. However, even with efficient architectures, optimizing multiple losses concurrently remains challenging, as naive strategies such as utilizing the reference objective gradient $g_0$ (i.e., uniformly weighted average of all task gradients) throughout the entire training often lead to sub-optimal performance (Liu et al., 2021a).\nOne of the primary reasons for this challenge is the potential conflicts between task gradients (i.e., gradients pointing in opposing directions) which can impede the concurrent optimization of multiple losses (Yu et al., 2020a). These conflicts often hinder convergence and negatively impact overall performance. Recent research efforts have focused on optimization strategies that balance task gradients and/or resolve conflicts via computing and utilizing a dynamic gradient update vector $d$ at each optimization step.\nA foundational approach in this direction is Multiple-Gradient Descent Algorithm (MGDA) (D\u00e9sid\u00e9ri, 2012), originally proposed to address Multi-Objective Optimization (MOO). Sener & Koltun (2018) apply MGDA specifically for MTL. FAMO (Liu et al., 2023) offers an efficient solution (constant space and time) for the log of MGDA objective. However, it may trade off some performance for speed, particularly when compared with methods like NashMTL (Navon et al., 2022) and IMTL-G (Liu et al., 2021b). Additionally, approaches such as MGDA and FAMO only guarantee finding Pareto-stationary points\u00b9 rather than truly optimal solutions.\nOther prior works address this by imposing directional constraints on the update vector. CAGrad (Liu et al., 2021a), for instance, seeks the update vector within a Euclidean ball centered at $g_0$, while SDMGrad (Xiao et al., 2024) restricts the update direction to be near $g_0$ through inner-product"}, {"title": "2. Preliminaries", "content": "Consider a multi-task model parameterized by $\u03b8 \\in R^M$ with number of tasks $K > 2$. Each task has its own objective function, or loss, $L_i(\u03b8)$. A trivial / reference objective in MTL is optimizing for the uniform average over all the losses, i.e.,\n$\u03b8^* = \\underset{\u03b8 \\in R^M}{argmin} {L_0(\u03b8) = \\frac{1}{K} \\sum_{i=1}^{K}L_i(\u03b8)}$    (1)\nThe parameters can then be updated as $\u03b8' \u2190 \u03b8 \u2013 \u03b7g_0$, where $g_0 = \\frac{1}{K} \\sum_{i=1}^{K} \\nabla_\u03b8L_i$ is the gradient of the reference objective in Equation (1) and $\u03b7$ is the learning rate. This approach however, is known to be sub-optimal, due to the potential conflicts between task gradients that may occur during training (Liu et al., 2021a).\nTo address these conflicts, we aim to find an alternative update vector $d$ that not only decreases the average loss $L_0$, but also every individual loss. This can be framed as maximizing the minimum decrease rate across all tasks:\n$\\underset{d \\in R^M}{max} \\underset{i \\in [K]}{min} (L_i(\u03b8) - L_i(\u03b8 \u2013 \u03b7d)) \u2248 \\underset{d \\in R^M}{max} \\underset{i \\in [K]}{min} (g_i, d)$,  (2)\nwhere the approximation relies on a first-order Taylor approximation, which is accurate when $\u03b7$ is small, as is often the case (Liu et al., 2021a).\nThe optimization problem expressed in Equation (2) can be subjected to various constraints, such as $||d|| \u2264 1$ in MGDA (Sener & Koltun, 2018) and FAMO (Liu et al., 2023), or $||d - g_0|| \u2264 c||g_0||$ in CAGrad (Liu et al., 2021a), which indirectly controls the alignment between $d$ and $g_0$ by limiting the deviation in Euclidean space. As mentioned earlier, in the case of the former constraint, the respective algorithms can only reach a Pareto-stationary point, while for the constraints such as the latter (that incorporate the reference objective gradient $g_0$), there exist guarantees that the algorithm can converge to an optimum."}, {"title": "3. Multi-Task Learning with CONICGRAD", "content": "Our goal is to dynamically compute a gradient update vector $d$ at each optimization step. This vector should balance task gradients and mitigate their potential conflicts, while ensuring convergence towards the optimum of the reference objective in Equation (1). To this end, we propose CONICGRAD, which enforces an angular constraint that restricts $d$ within a cone centered around $g_0$. Formally, this can be expressed as the following constrained optimization problem\n$\\underset{d \\in R^M}{max} \\underset{i \\in [K]}{min} (g_i, d) s.t. \\frac{\\langle g_0, d \\rangle}{||g_0|| ||d||} \u2265 c$,   (3)\nwhere $c \u2208 [-1, 1]$, and in practice we restrict it to $c \u2208 (0,1]$ to avoid negative correlation (see Appendix B.1).\nThis approach provides an interpretable formulation that maintains sufficient alignment with $g_0$ without imposing overly rigid restrictions on the gradient update. Specifically, the advantages include: (i) explicit control over the update direction, ensuring that $d$ remains geometrically aligned with the reference objective gradient $g_0$; and, (ii) decoupling of magnitude and direction, unlike common directional constraints, which allows $||d||$ to adopt any task-specific criteria.\nTo convert Equation (3) to an unconstrained optimization problem, the Lagrangian in Equation (3) is\n$\\underset{d \\in R^M}{max} \\underset{i \\in [K]}{min} (g_i, d) - \\frac{\u03bb}{2} (c^2 ||g_0||^2 ||d||^2 \u2013 ||g_0 d||^2)$.   (4)\nNote that $\\underset{i \\in [K]}{min} (g_i, d) = \\underset{w \\in W}{min} (g_w, d)$, where $g_w = \\sum_{i=1}^{K} w_i g_i$ and $w$ is on the probability simplex, i.e., $\\forall i w_i > 0$ and $\\sum_{i=1}^{K} W_i = 1$. The objective is concave in $d$, and we find that when $c < 1$ Slater condition holds (see proof in Proposition A.4). We get strong duality since the duality gap is 0, and swap the max and min operators. The dual objective of the primal problem in Equation (4) is\n$\\underset{\u03bb\u22650}{min} \\underset{d\u2208R^K}{max} g_w^T g_w d - \\frac{\u03bb}{2} (c^2 ||g_0||^2 ||d||^2 \u2013 ||g_0 d||^2)$.   (5)\nProposition 3.1. Given the optimization problem in Equation (3), its Lagrangian in Equation (4), and assuming the Slater condition holds, the dual of the primal problem in Equation (5), the optimal gradient update $d^*$ is given by\n$d^* = \\frac{1}{\u03bb}(c^2 ||g_0||^2I - g_0g_0^T)^{-1} g_\u03c9,   (6)\nwhere I is a M \u00d7 M identity matrix.\nEquation (5) is now simplified to\n$\\underset{\u03bb>0}{min} g_\u03c9^T d^* g_\u03c9 d^* - \\frac{\u03bb}{2} (c^2 ||g_0||^2 ||d^*||^2 \u2013 ||g_\u03c9 d^* ||^2)$.   (7)\nWe empirically find that $\u03bb = 1$ works well in practice (see Appendix A.2 for a detailed derivation and discussion), and therefore, Equation (6) is simplified to\n$d^* = (c^2 ||g_0||^2I - g_0g_0^T)^{-1} g_\u03c9$.   (8)"}, {"title": "3.1. Efficient Computation of $d^*$", "content": "Note that Equation (8) requires inverting an M \u00d7 M matrix (where M is the number of model parameters, which can be huge) and computing it may be impractical. However, due to its specific structure, it is possible to use the Sherman-Morrison-Woodbury (SMW) formula (Higham, 2002) (also known as the matrix inversion lemma) to reformulate it as a 1 x 1 (i.e., scalar) inversion instead.\nFor the sake of brevity, let $Z$ denote $c^2 ||g_0||^2I - g_0g_0^T$, where $g_0$ and $g_w$ are both vectors \u2208 $R^{M\u00d71}$, and I is the M \u00d7 M identity matrix. Note that the term $g_0g_0^T$ is low rank (in fact rank-1). Therefore, Z can be interpreted as a rank-1 perturbation of a scaled identity matrix $c^2 ||g_0||^2I$. This is a well-suited setting for the SMW formula, which states\n$(A+UCV)^{-1} = A^{-1}-A^{-1}U(C^{-1}+VA^{-1}U)^{-1}VA^{-1}$.   (10)\nHere we have $A = c^2||g_0||^2I$ (diagonal) and $C^{-1} = -I$, $U = g_0$, and $V = g_0^T$. We first compute $A^{-1} = \\frac{1}{c^2||g_0||^2}I$. Then substitute $U = V = g_0$ and $A^{-1}$ to form\n$C^{-1} + V A^{-1} U = -I + \\frac{1}{c^2 || g_0 ||^2}g_0^T g_0$.    (11)\nThis yields an scalar, which we invert as\n$D=(-I+\\frac{1}{c^2 || g_0 ||^2}g_0^T g_0)^{-1}.   (12)"}, {"title": "Algorithm 1 CONICGRAD", "content": "1: Input: Initial model parameters $\u03b8_0$, Differentiable task losses {$L_i$}$_1^K$, Learning rates $\u03b7_1$ and $\u03b7_2$, Decay $\u03b3$, Cosine similarity constraint $c$.\n2: Initialize uniform weights: $\\forall i : w_i = \\frac{1}{K}$\n3: for $t = 1$ to $T$ do\n4:    Compute $g_0 = \\sum_{i=1}^{K} w_i \\nabla_\u03b8L_i$\n5:    Compute $g_\u03c9 = \\sum_{i=1}^{K} w_i \\nabla_\u03b8L_i$\n6:    Compute $d^* = (c^2||g_0||^2I \u2013 g_0g_0^T)^{-1}g_\u03c9$\n7:    Update the weights $w_{t+1}$ using $\u03b7_2$ via Equation (9)\n8:    Recompute $d^*$ based on $w_{t+1}$\n9:    Update the model parameters $\u03b8_{t+1}$ using $\u03b7_1$\n10: end for"}, {"title": "3.2. Convergence Analysis", "content": "We analyze the convergence property of CONICGRAD.\nTheorem 3.2. Assume individual loss functions $L_0, L_1,\u2026L_K$ are differentiable on $R^M$ and their gradients $L_i(\u03b8)$ are all L-Lipschitz with $L > 0$, i.e., $||\u2207L_i(x) \u2013 \u2207L_i(y)|| \u2264 L||x \u2212 y||$ for $i = 0,1,\u2026, K$, where $L\u2208 (0,\u221e)$, and $L_0$ is bounded from below i.e., $L_0 = \\underset{\u03b8\u2208R^M}{inf} L_0(\u03b8) > \u2212\u221e$. Then, with a fixed step-size $a$ satisfying $0 < a < \\frac{1}{\u03ba}$, and in case of \u22121 < c < 1, CONICGRAD satisfies the inequality\n$\\sum_{t=0}^{T}||g_0(0)||^2 \u2264 \\frac{2(L_0(0) - L)}{a(2Kc - 1)(T+1)}$.  (15)"}, {"title": "3.3. Geometric Intuition of CONICGRAD", "content": "The angular constraint in CONICGRAD provides greater flexibility in selecting the gradient update vector $d$ compared to the directional constraint of CAGrad (Liu et al., 2021a). To illustrate this, we present a geometric interpretation in a toy"}, {"title": "4. Experimental Results and Discussions", "content": "In this section, we present a comprehensive evaluation of CONICGRAD across several standard MTL benchmarks, report its performance, compare it to the existing methods, and analyze the results in detail. We consider two commonly used metrics for evaluating MTL methods (Liu et al., 2023; Navon et al., 2022): (i) \u2206 m% which measures the average per-task performance drop of a method relative to the single task baseline (STL), i.e.,\n$\u0394 m% = \\frac{1}{K}\\sum_{k=1}^{K}(\u03b4_k (\\frac{M_{m,k} - M_{STL,k}}{M_{STL,k}}) * 100$,\nwhere $M_{STL,k}$ refers to the value of STL baseline for some metric M of task k, while $M_{m,k}$ denotes the value of the method being evaluated for the same metric, and dk is a binary indicator if a metric is better when higher ($\u03b4_\u03ba$ = 1) or lower ($\u03b4_\u03ba$ = 0). (ii) Mean Rank (MR) which measures the average rank of each method across different tasks (e.g., MR = 1 when the method ranks first for every task)."}, {"title": "4.1. Toy Example", "content": "Given the standard practices in MTL evaluation, we assess CONICGRAD on a toy 2-task example (Liu et al., 2021a). This setup consists of two competing objectives that define the overall objective ($(L_1(\u03b8) + L_2(\u03b8)$), mimicking scenarios where optimization methods must balance conflicting gradients effectively in order to reach the global minimum. Failure to do so often results in getting stuck in either of the two suboptimal local minima. More details on the setup is provided in Appendix C.\nUsing five commonly studied initialization points init = {(-8.5, 7.5), (-8.5, -5), (9, 9), (-7.5, -0.5), (9, -1)}, we compare CONICGRAD with the following leading methods: FAMO (Liu et al., 2023), CAGrad (Liu et al., 2021a), and NashMTL (Navon et al., 2022). Figure 1 visualizes each method's optimization trajectories, illustrating how they handle conflicts in task gradients, as well as their final optimization outcomes.\nThe results highlight key differences among the methods. NashMTL struggles with two initialization points near the two local minima. FAMO consistently converges to the Pareto front, but cannot achieve the global minimum. This can be explained by its lack of an aligning mechanism with the reference objective gradient $g_0$. In contrast, both CAGrad and CONICGRAD successfully reach the global minimum ( on the Pareto front) for all initialization points.\nNotably, CONICGRAD achieves the global minimum significantly faster than CAGrad, as evidenced by its respective learning curve on the far-right of Figure 1. While FAMO and CONICGRAD demonstrate comparable speeds in reaching the Pareto front, only CONICGRAD consistently converges to the global minimum. This highlights its effectiveness in balancing objectives and optimizing the overall performance, surpassing competing methods in both convergence speed and outcome."}, {"title": "4.2. Multi-Task Supervised Learning", "content": "In the supervised MTL setting, we evaluate CONICGRAD on three widely used benchmarks, namely CityScapes (Cordts et al., 2016), CelebA (Liu et al., 2015), and NYUv2 (Silberman et al., 2012), following (Liu et al., 2023; Xiao et al., 2024; Navon et al., 2022; Liu et al., 2021a). Cityscapes includes two tasks: segmentation and depth estimation. It comprises 5000 RGBD images of urban street scenes, each annotated with per-pixel labels. NYUv2 is another vision-based dataset that involves three tasks: segmentation, depth prediction, and surface normal prediction. It contains 1449 RGBD images of indoor scenes, with corresponding dense annotations. CelebA dramatically increases the number of tasks to 40. It features approximately 200K images of 10K celebrities, where each face is annotated with 40 different binary attributes. The task is to classify the presence or absence of these facial attributes for each image.\nNote that CityScapes and NYUv2 are dense prediction tasks, whereas CelebA is a classification task, offering a diverse set of challenges for evaluating MTL methods. Also note that, while each benchmark has its own set of performance metrics, the primary metrics for evaluating MTL methods are MR (Mean Rank) and Am% (lower is better for both).\nWe compare CONICGRAD against 12 multi-task optimization methods and a single-task baseline (STL), where a separate model is trained for each task. The comparison includes widely recognized MTL approaches such as MGDA (Sener & Koltun, 2018; D\u00e9sid\u00e9ri, 2012), PCGrad (Yu et al., 2020a), GradDrop (Chen et al., 2020), CAGrad (Liu et al., 2021a), IMTL-G (Liu et al., 2021b), NashMTL (Navon et al., 2022), FAMO (Liu et al., 2023), and SDMGrad (Xiao et al., 2024). Three established methods on gradient manipulation are also evaluated: DWA (Liu et al., 2019), RLW (Lin et al., 2021), UW (Kendall et al., 2018), Additionally, we consider two baseline methods commonly used in MTL literature: Linear Scalarization (LS), which minimizes $L^0$, and Scale-Invariant (SI), which minimizes $\\sum log L_k (\u03b8)$."}, {"title": "4.3. Multi-Task Reinforcement Learning", "content": "In addition to supervised multi-task learning benchmarks, we evaluate CONICGRAD in a multi-task Reinforcement Learning (RL) (Sutton, 2018) setting. Gradient conflicts are particularly prevalent in RL due to the inherent stochasticity of the paradigm (Yu et al., 2020a), making it an ideal testbed for optimization strategies that handle such conflicts effectively. Following prior works (Yu et al., 2020a; Liu et al., 2021a; 2023; Xiao et al., 2024), we benchmark CONICGRAD on MetaWorld MT10 (Yu et al., 2020b), a widely used MTRL benchmark consisting of 10 robot manipulation tasks, each with a distinct reward function.\nIn accordance with the literature, our base RL algorithm is Soft Actor-Critic (SAC) (Haarnoja et al., 2018). We adopt LS (i.e., a joint SAC model) as our baseline and STL (i.e., ten independent SACs, one for each task) as a proxy for skyline. Other methods we compare to include PCGrad (Yu et al., 2020a), CAGrad (Liu et al., 2021a), NashMTL (Navon et al., 2022), FAMO (Liu et al., 2023), and SDMGrad (Xiao et al., 2024). We also compare to an architectural approach to multi-task learning, Soft Modularization (Yang et al., 2020), wherein a routing mechanism is designed to estimate different routing strategies and all routes are softly combined to form different policies.\nResults. Table 3 summarizes the results on the MT10 benchmark. While NashMTL reports strong results, (Liu et al., 2023) could not reproduce the same performance. CONICGRAD achieves a success rate of 0.89 with a standard error of 0.02, outperforming all the contending methods and approaching the STL upper bound (0.90 \u00b1 0.03). Notably, its also exhibits the lowest standard error among all methods which indicates more stable and consistent performance."}, {"title": "4.4. Scalability Analysis for Larger Models", "content": "We conduct scaling experiments to evaluate the computational overhead incurred by MTL methods as model size increases. This analysis is crucial since most of these methods involve direct manipulation of gradients associated with the parameters of the model. We compare CAGrad (Liu et al., 2021a), NashMTL (Navon et al., 2022), SDMGrad (Xiao et al., 2024), and CONICGRAD along with STL. CelebA (Liu et al., 2015) is chosen as the evaluation benchmark due to its high task count (i.e., 40). The base model has 5.2M parameters (measured using the ptflops package (Sovrasov, 2018-2024)), and we create two scaled variants with 26.71M and 34.41M parameters, representing roughly 5\u00d7 and 7\u00d7 the base model size, by increasing the number of layers and neurons. For each method and model size, we measure the average time per epoch over two runs.\nFigure 4 illustrates the per-epoch time (in hours) for each algorithm as the underlying model size increases. Our method, CONICGRAD, remains computationally efficient as the number of parameters grows, while other methods experience slowdowns. In particular, SDMGrad (Xiao et al., 2024) requires significantly more time due to its reliance on multiple forward passes for gradient estimation."}, {"title": "4.5. Ablation Study", "content": "We examine the effect of two key hyperparameters of CONICGRAD, namely c which controls the maximum permissible angle between the gradient update vector d and the reference gradient vector $g_0$, and the regularization coefficient \u03b3. We explore \u03b3 \u2208 [0.001,0.01] and \u0441\u2208 {0.1, 0.25, 0.5, 0.75, 0.9} as the space of admissible values. In Figure 5, we observe that smaller values of \u03b3 are preferred for the CityScapes and CelebA benchmarks, while a larger \u03b3 enhances performance on NYUv2. As for the conic constraint, c \u2265 0.5 is generally preferred for CelebA and NYUv2 benchmarks, enforcing the angle between d and $g_0$ to remain below 60\u00b0. In contrast, for CityScapes, a smaller c (i.e., c = 0.25) leads to better performance, suggesting that the update vector d should be allowed to deviate more from $g_0$ in this benchmark."}, {"title": "5. Conclusion", "content": "In this work, we explored Multi-Task Learning (MTL) through the lens of Multi-Objective Optimization (MOO) and introduced CONICGRAD. A fundamental challenge in MTL is gradient conflicts, where task gradients may point in opposing directions, making it difficult to find a unique gradient update vector $d$ that improves all tasks simultaneously. To address this, CONICGRAD analyzes the evolving relationships between task-specific gradients as optimization progresses, and dynamically computes d at each training step. CONICGRAD offers a geometrically interpretable solution by enforcing an angular constraint, ensuring that d remains within a cone defined by an angle of at most arccos(c) relative to the reference objective gradient $g_0$. This formulation preserves alignment with $g_0$ while still permitting adaptive adjustments to task-specific contributions. Additionally, we demonstrate that not only CONICGRAD is computationally efficient, but also scales effectively to high-dimensional parameter spaces. Evaluations on standard supervised and reinforcement learning benchmarks demonstrate that CONICGRAD consistently outperforms state-of-the-art methods in most cases, while remaining competitive in others.\nLimitations and Future Work. Our method relies on the cone angle parameter c, which influences the alignment"}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Proofs", "content": ""}, {"title": "A.1. Optimal Gradient Direction", "content": "Proposition A.1. Given the optimization problem in Equation (3), its Lagrangian in Equation (4), and assuming the Slater condition holds, the dual of the primal problem in Equation (5), then the optimal update direction $d^*$ is given by\n$d^* = \\frac{1}{\u03bb}(c^2 ||g_0||^2I - g_0g_0^T)^{-1} g_\u03c9$\nProof. We re-produce the Lagrangian from Equation (4) as\n$\\underset{d \u2208 R^M}{max} \\underset{i \u2208 [K]}{min} (g_i, d) - \\frac{\u03bb}{2} (c^2 ||g_0||^2 ||d||^2 \u2013 ||g_\u03c9 d||^2)$.   (16)\nWe take the partial derivative of Equation (16) with respect to d while keeping \u03bb and w fixed, i.e., $\\frac{\u2202}{\u2202d}$, and get\n$\\frac{\u2202}{\u2202d}g_\u03c9 - \u03bbc^2 ||g_0||^2 d \u2013 g_0g_0^T d = 0$,   (17)\n$\\frac{\u2202}{\u2202d}g_\u03c9 = \u03bbc^2 ||g_0||^2 d + \u03bbg_0g_0^T d = 0$.   (18)\nSince we want to find $d^*$, we collect all the terms dependent on d and re-arrange as\n$g_\u03c9 = \u03bbc^2 ||g_0||^2d \u2013 \u03bbg_0g_0^T d$,   (19)\n$g_\u03c9 = \u03bbd(c^2 ||g_0||^2I \u2013 g_0g_0^T)$.   (20)\nNow, we can write $d^*$ as\n$d^* = \\frac{g_\u03c9}{\u03bb} (c^2 ||g_0||^2I \u2013 g_0g_0^T)^{-1}$,   (21)\n$d^* = \\frac{1}{\u03bb}(c^2 ||g_0||^2I \u2013 g_0g_0^T)^{-1} g_\u03c9$,   (22)\nand we arrive at the equation we set out to prove."}, {"title": "A.2. Optimizing for \u03bb", "content": "To optimize \u03bb, we first substitute $d^*$ from Equation (8) into Equation (7). To simplify the resulting expression, we define Z := $(c^2 ||g_0||^2I - g_0g_0^T)$, which is independent of \u03bb, following a similar approach as in Section 3.1, then\n$E(\u03bb) = \\frac{1}{\u03bb} (g_\u03c9^T Z^{-1} g_\u03c9) - \\frac{\u03bb}{2} (c^2 ||g_0||^2 ||Z^{-1} g_\u03c9||^2 \u2013 ||\\frac{g_\u03c9}{\u03bb} Z^{-1} g_\u03c9||^2)$.\nThis further simplifies to\n$E(\u03bb) = \\frac{g_\u03c9^T Z^{-1} g_\u03c9}{\u03bb} - \\frac{\u03bb}{2} \\frac{1}{c^2 ||g_0||^2} ||Z^{-1} g_\u03c9||^2 \u2013 (g_\u03c9 Z^{-1} g_\u03c9)^2]$,\nTerm 1\nTerm 2\nDerivative w.r.t. \u03bb Let the constant part (independent of \u03bb) be denoted by C, then we can write\n$C = g_\u03c9^T Z^{-1} g_\u03c9 - \\frac{1}{2} [c^2 ||g_0||^2 ||Z^{-1} g_\u03c9||^2 \u2013 (g_\u03c9 Z^{-1} g_\u03c9)^2]$."}, {"title": "A.3. Convergence Analysis of CONICGRAD", "content": "We borrow from (Liu et al., 2021a) the style and language for the purpose of analyzing convergence rate of CONICGRAD. We abuse the notation and assume that $L_0$ denotes a general function which has associated gradient $g_0 = \\nabla L_0$.\nTheorem A.2. Assume individual loss functions $L_0, L_1,\u2026L_K$ are differentiable on $R^M$ and their gradients $\u2207L_i(\u03b8)$ are all L \u2013 Lipschitz, i.e., $||\u2207L_i(x) \u2013 \u2207L_i(y)|| \u2264 L||x \u2212 y||$ for $i = 0,1,\u2026\u2026, K$, where $L \u2208 (0,\u221e)$, and $L_0$ is bounded from below i.e., $L = \\underset{\u03b8\u2208R^M}{inf} L_0(\u03b8) > \u2212\u221e$. Then, with a fixed step-size $a$ satisfying $0 < a < \\frac{1}{\u03ba}$, and in case of \u22121 < c < 1, CONICGRAD satisfies\n$\\sum_{t=0}^{T}||g_0(0)||^2 \u2264 \\frac{2(L_0(0) - L)}{a(2Kc - 1)(T+1)}$.   (23)\nProof. Consider the optimization step to be tth and let $d^* (0_t)$ be the update direction obtained by solving Equation (3), then we can write\n$L_0(0_{t+1}) - L_0(0_t) \u2264 L_0(0_t \u2013 ad^*(0_t)) \u2013 L_0(0_t)$\n$L_0(0_{t+1}) \u2013 L_0(0_t) \u2264 L_0(0_t \u2013 ad^*(0_t)) \u2013 L_0(0_t) \u2264 -ag_0(0_t)^T d^* (0_t) + \\frac{L}{2}||ad^*(0_t)||^2$ by smoothness\n$\u2264 -ag_0(0_t)^T d^* (0_t) + \\frac{La^2}{2}||d^*(0_t)||^2$ re-arranging\n$\u2264 -ag_0(0_t)^T d^* (0_t) + \\frac{La^2}{2\u03ba^2}||g_0||^2 by $a\u2264 \\frac{1}{\u03ba}$\n$\u2264 -ac\u03ba||g_0(0_t)||||d^*(0_t)||+ \\frac{La^2}{2\u03ba^2}||d^*(0_t)||^2 by constraint in Equation (3)\n$= -ac\u03ba||g_0(0_t)||||d^*(0_t)||+ \\frac{La^2}{2\u03ba^2}(||d^*(0_t)||)^2 because we enforce $||d^*(0_t)|| \u2248 \u03ba||g_0(0)||$\n$= \u2212\u03b1c\u03ba\u03b5||g_0(\u03b8_t)||^2 + \\frac{L\u03b1^2}{2\u03ba^2} \u03ba^2 ||g_0(\u03b8_t) ||^2$\n$= -(\u03b1\u03bac \u2013 \\frac{L\u03b1^2}{2} ) || g_0 (\u03b8_t) ||^2$\n$= -(\\frac{L\u03b1^2}{2\u03ba}-\u03b1\u03bac) || g_0 (\u03b8_t) ||^2,$\nwhere \u03ba is some constant that $||d^* (0_t)||$ approximates $||g_0(0)||$ with. Note that this follows from the normalization term $d = d^* \\frac{||g_0||}{||d^*||}$. Using telescopic sums, we have $L_0(\u03b8_{T+1}) \u2013 L_0(0) = \\sum_{t=0}^{T}L_0(0_{T+1}) L_0(0)$. Therefore,\n$\\underset{t0$ or $c < 1$. Hence, we see that $d_s$ strictly satisfies the inequality $\\frac{\\langle g_0, d \\rangle}{||g_"}]}