{"title": "FROM CODE TO CORRECTNESS: CLOSING THE LAST MILE OF Code GenNERATION WITH HIERARCHICAL DEBUGGING", "authors": ["Yuling Shi", "Songsong Wang", "Chengcheng Wan", "Xiaodong Gu"], "abstract": "While large language models have made significant strides in code generation,\nthe pass rate of the generated code is bottlenecked on subtle errors, often requir-\ning human intervention to pass tests, especially for complex problems. Existing\nLLM-based debugging systems treat generated programs as monolithic units, fail-\ning to address bugs at multiple levels of granularity, from low-level syntax errors\nto high-level algorithmic flaws. In this paper, we introduce Multi-Granularity\nDebugger (MGDebugger), a hierarchical code debugger by isolating, identify-\ning, and resolving bugs at various levels of granularity. MGDebugger decom-\nposes problematic code into a hierarchical tree structure of subfunctions, with\neach level representing a particular granularity of error. During debugging, it an-\nalyzes each subfunction and iteratively resolves bugs in a bottom-up manner. To\neffectively test each subfunction, we propose an LLM-simulated Python executor,\nwhich traces code execution and tracks important variable states to pinpoint errors\naccurately. Extensive experiments demonstrate that MGDebugger outperforms\nexisting debugging systems, achieving an 18.9% improvement in accuracy over\nseed generations in HumanEval and a 97.6% repair success rate in HumanEval-\nFix. Furthermore, MGDebugger effectively fixes bugs across different categories\nand difficulty levels, demonstrating its robustness and effectiveness.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) such as GPT-4 (OpenAI, 2023), LLaMA (Touvron et al., 2023),\nand DeepSeek-Coder (Zhu et al., 2024) have made significant advances in AI-assisted coding\ntasks (Chen et al., 2021; Lu et al., 2021; Li et al., 2022). Trained on vast corpora of text and\ncode, LLMs can understand and generate code snippets for various programming tasks, ranging\nfrom simple data structures to complex algorithmic problems (Li et al., 2022). These models have\ndemonstrated proficiency in tasks such as code completion, bug detection, and even tackling com-\npetitive programming challenges.\nWhile the code generated by large models generally meets the requirements, it often contains critical\nerrors that require human intervention to pass tests (Liu et al., 2023b; Dou et al., 2024). This has\ngradually led to a new development paradigm: large models generate the code, while humans fix it.\nTherefore, the \u201clast mile\u201d, as well as the most crucial step, of code generation is how to efficiently\nrepair the code generated by large models.\nNumerous efforts have been made to debug LLM-generated code. The most popular way is to reuse\nthe LLM generator to debug the generated code with the feedback from test case execution (Chen\net al., 2023b; Zhong et al., 2024; Hu et al., 2024). While these methods increase the pass rates,\nthey treat the erroneous program as a holistic set of statements (Chen et al., 2023b; Shinn et al.,\n2023; Zhong et al., 2024; Ding et al., 2024) regardless of the varying types and levels of failures.\nFailures of test cases arise from different levels of factors, from low-level syntactic errors to high-"}, {"title": "2 RELATED WORK", "content": "Code Generation with LLMs Recent models such as GPT4 (OpenAI, 2023), Codestral (Mistral\nAI team, 2024), and DeepSeek-Coder (Zhu et al., 2024) have advanced code generation through\ninstruction tuning and RLHF with mixed code and natural language data (Ziegler et al., 2020; Hu-\nsain et al., 2020; Rafailov et al., 2023). Code generation with LLMs has been enhanced by various\ntechniques. Some approaches focus on improving the quality of generated code using planning algo-\nrithms, transitioning from outlines to detailed implementations (Zhang et al., 2022; Yao et al., 2023;\nZelikman et al., 2023; Zhou et al., 2023; Zheng et al., 2023). Other methods sample multiple pro-\ngrams from the same LLM and rank them to identify the best one (Chen et al., 2023a; 2022; Ni et al.,\n2023). Additionally, some works leverage multi-agent collaboration frameworks to enhance code\ngeneration quality (Zhang et al., 2024; Huang et al., 2023a; Dong et al., 2024). These approaches\naim to optimize the production of correct code from the outset. By contrast, MGDebugger targets\nthe post-generation phase, focusing on debugging and fixing errors that inevitably arise during the\ncode generation process.\nRepairing LLM-Generated Code Program repair is a critical aspect of software development,\naiming to automatically identify and fix bugs in code (Just et al., 2014; Gupta et al., 2020; Yasunaga\n& Liang, 2021). There are two main streams of research in repairing code generated by LLMs: (1)\ntraining models to repair code (Huang et al., 2023b; Jiang et al., 2024; Ding et al., 2024; Zheng\net al., 2024; Moon et al., 2024; Kumar et al., 2024) and (2) providing external feedback to the raw\npretrained models to fix code (Jiang et al., 2023; Chen et al., 2023b; Olausson et al., 2023; Zhong\net al., 2024; Hu et al., 2024). By contrast to previous work that trains separate models for code\nrepair (Ding et al., 2024; Zheng et al., 2024; Moon et al., 2024), MGDebugger does not require\ntask-specific retraining but takes advantage of the inherent capabilities of pretrained LLMs. This\nflexibility allows MGDebugger to operate in zero-shot settings, offering a lightweight and scalable\nalternative. And exploring the ability of LLMs to fix their own code is a promising direction for\nself-improvement training of the LLMs (Wang et al., 2023; Burns et al., 2023).\nMGDebugger falls under the category of work that leverages pretrained models to fix code by rea-\nsoning with external feedback. Several recent methods (Zhang et al., 2023; Olausson et al., 2023;\nBouzenia et al., 2024; Lee et al., 2024; Xia & Zhang, 2023) utilize execution results from test cases\nto guide LLMs in code correction. More recent works have explored advanced debugging tech-\nniques utilizing LLM's reasoning ability. Reflexion (Shinn et al., 2023) prompts LLMs to reflect on\nthe generated code and uses a memory buffer for iterative refinement. Self-Debugging (Chen et al.,"}, {"title": "3 \u041c\u0415\u0422\u041dODOLOGY", "content": "3.1 OVERVIEW\nWe present MGDebugger, a novel bottom-up hierarchical debugging method for repairing LLM-\ngenerated code. The overall workflow of MGDebugger is illustrated in Figure 1, while the detailed\ndebugging process for each subfunction is depicted in Figure 2.\nAs shown in Figure 1, MGDebugger begins with Hierarchical Code Decomposition (Section 3.2),\nwhich decomposes the input buggy code into a hierarchical structure of subfunctions. This enables\nsystematic identification and resolution of bugs at various levels of granularity. For each subfunction,\nMGDebugger Generates Test Case Generation for Subfunctions (Section 3.3), deriving private test\ncases from public test cases of the main function, as illustrated in Figure 2. MGDebugger then\nexecutes these test cases and Debugs Subfunction with LLM-Simulated Execution (Section 3.4). The\nLLM simulates step-by-step code execution for failed test cases, monitoring critical variables and\nstate changes to pinpoint the cause of errors. Once a subfunction has been fixed, MGDebugger\nupdates it in the hierarchical structure and propagates the changes to dependent functions through\nBottom-up Debugging (Section 3.5). This hierarchical debugging approach not only tackles different"}, {"title": "3.2 HIERARCHICAL CODE DECOMPOSITION", "content": "Modularizing and decomposing complex code into smaller helper subfunctions has been proven to\nbe helpful especially for large functions that are difficult to understand (Jain et al., 2023; Zelikman\net al., 2023). To enable hierarchical debugging, we need to transform the input code into a tree-like\nstructure of subfunctions.\nSpecifically, given an LLM-generated function $f$, we decompose it into a hierarchical structure\nof subfunctions denoted as $(f_1,..., f_n)$. These subfunctions can be organized as a tree $f_{root} =$\nTREE($f_{root}$, CHILD($f_{root}$)), where $f_{root}$ represents the main function and CHILD($f$) denotes the set\nof subfunctions directly called by $f$. We leverage an LLM for the decomposition, adhering to three\nprinciples: (1) each subfunction represents the minimal reusable unit of code with a specific purpose,\n(2) higher-level functions call lower-level functions to achieve complex functionality, and (3) the\noverall structure facilitates isolated testing and debugging. As illustrated in Figure 1, the resulting\ntree-like structure allows us to isolate logical units of the code, enabling more focused debugging\nefforts across different levels of granularity (Woodfield et al., 1981; Isazadeh et al., 2017). The\nprompt template used for code decomposition is provided in Appendix G.1."}, {"title": "3.3 GENERATING TEST CASES FOR SUBFUNCTIONS", "content": "Having obtained the hierarchy of subfunctions, we aim to verify the correctness of each subfunc-\ntion. For this purpose, we generate test cases for each subfunction leveraging automatic unit test\ngeneration techniques (Wang et al., 2021; Sch\u00e4fer et al., 2024; Liu et al., 2023a). For each sub-\nfunction $f_i \\in f_{root}$, we generate a set of test cases $T_i$. Following the problem settings from Chen\net al. (2023b) and Zhong et al. (2024), we assume that the public test cases for the main function\n$T_{pub}$ have been provided, which is common in most code generation benchmarks (Chen et al., 2021;\nHendrycks et al., 2021; Muennighoff et al., 2023). We can leverage these test cases to derive a set\nof corresponding test cases for each subfunction.\nWe employ the same LLM for the test case generation. For each $f_i \\in f_{root}$. The LLM is now\nprompted to perform the following steps: (1) analyze how the subfunction is used within the main\nfunction and how it contributes to the expected outputs in the public test cases; (2) for each public\ntest case, reason through the overall code structure step by step to figure out the input and expected"}, {"title": "3.4 DEBUGGING SUBFUNCTIONS WITH LLM-SIMULATED EXECUTION", "content": "With the generated test cases, we debug each subfunction by running them on the test case inputs,\nobtaining the results, and comparing these results against the expected outcomes in the test cases.\nWhen a failed test case is identified, we fix the corresponding subfunction and produce a corrected\nversion.\nOne straightforward way to implement this process is to use an external Python executor to mon-\nitor runtime variable values (Zhong et al., 2024). However, when debugging high-level functions,\ntracking variable values within lower-level subfunctions is often unnecessary, as their correctness is\nensured by the bottom-up debugging methodology. Furthermore, directly collecting all execution\ntraces from the external debugger can add unnecessary overhead and complexity to the process.\nInspired by the methodology in Li et al. (2023), we propose an LLM-simulated code executor, which\nprompts LLM to act as a Python interpreter and track the code execution. As shown in Figure 2,\nwe request the LLM to simulate the execution process, reasoning about key variables and their\nstates at each step, and thoroughly analyzing the failed test cases. This eliminates the need for an\nexternal debugger, offering a more flexible and efficient debugging solution. In addition, the LLM\ncan accurately identify where errors occur and grasp their surrounding context. The LLM prompt\nfor the debugging process is detailed in Appendix G.3."}, {"title": "3.5 BOTTOM-UP DEBUGGING", "content": "Having introduced code decomposition and the debugging process for each subfunction, we now\noutline the overall debugging workflow.\nWe initiate the process by calling MGDebugger on the main function with the decomposed code $f_{root}$\nand the set of public test cases $T_{pub}$. MGDebugger traverses the hierarchical structure in a depth-first\nmanner, recursively debugging each subfunction before moving on to the higher-level functions.\nFor each specific subfunction, MGDebugger generates relevant test cases and debugs the function\nbased on the results. When a fix is identified, MGDebugger updates the function and propagates\nthe changes to the dependent functions. This recursive, bottom-up strategy systematically addresses\nbugs, beginning with the most granular levels and progressively advancing through the function\nhierarchy. This method accommodates various types of bugs at different abstraction levels, from"}, {"title": "4 EXPERIMENTS", "content": "4.1 SETUP\nModels We select three state-of-the-art LLMs ranging from 7B to 22B parameters as backbones for\ncode generation and debugging: CodeQwen1.5 (7B) (Bai et al., 2023), DeepSeek-Coder-V2-Lite\n(16B) (Zhu et al., 2024), and Codestral (22B) (Mistral AI team, 2024). Please refer to Appendix C\nfor our implementation details.\nDatasets We conduct experiments on three datasets. HumanEval (Chen et al., 2021) and\nMBPP (Austin et al., 2021) are two widely used benchmarks for evaluating code generation systems\nwith 164 and 500 problems, respectively. The HumanEvalFix dataset (Muennighoff et al., 2023)\nconsists of 164 buggy functions with six different bug categories: value misuse, missing logic, ex-\ncess logic, operator misuse, variable misuse, and function misuse. The detailed explanations and\ndistribution of bug categories can be found in Appendix B.\nMetrics We adopt two metrics to evaluate our method: 1) Accuracy (Chen et al., 2023b; Zhong\net al., 2024), which measures the overall proportion of correct code samples among all generated\ncode samples after debugging. A code is correct iff it passes all private test cases assigned to it. 2)\nRepair Success Rate (RSR) (Yasunaga & Liang, 2021), which refers to the proportion of fixed code\nsamples to the total number of buggy code samples.\nBaselines We compare MGDebugger with eight state-of-the-art methods for debugging LLM-\ngenerated code. 1) Simple Feedback is a basic baseline that informs the LLM that the code is\nincorrect and asks it to fix the issue. 2) Self-Edit (Zhang et al., 2023) prompts the LLM to edit the\ncode based on the execution results of the test cases. 3) Self-Debugging (Chen et al., 2023b) has\ntwo variants: Self-Debugging (Expl.) prompts the LLM to explain the generated code line-by-line,\nwhile Self-Debugging (Trace) asks the LLM to dry run the code for debugging. 4) LDB (Zhong\net al., 2024) segments the code into basic blocks, functions or lines, and tracks variable values dur-\ning runtime after each block to verify correctness against the task description. 5) Reflexion (Shinn\net al., 2023) asks the LLM to reflect on the previous code given execution results and uses a memory\nbuffer to enable iterative refinement."}, {"title": "4.2 MAIN RESULTS", "content": "The results in Table 1 show that MGDebugger consistently outperforms existing approaches across\nall models and datasets. Specifically, MGDebugger achieves the highest accuracy improvements,\nwith gains of +15.3% to +18.9% on HumanEval and +11.4% to +13.4% on MBPP. These improve-\nments are particularly notable when compared to baseline methods such as Self-Debugging (Expl.)\nand Reflexion, which also incorporate external feedback but exhibit lower gains in accuracy and\nRSR. The strong results across models of varying sizes highlight the adaptability of MGDebugger\nto different LLM architectures.\nMoreover, MGDebugger demonstrates remarkable debugging capabilities, particularly with\nDeepSeek-Coder-V2-Lite (16B) and Codestral (22B), where it achieves an accuracy of 94.5% on\nthe HumanEval dataset, the highest score among all methods. This is especially impressive con-\nsidering that MGDebugger operates in a zero-shot setting without task-specific retraining. This\nresult illustrates the inherent debugging ability of larger LLMs with MGDebugger. Additionally, the\nmethod's performance on MBPP, achieving an RSR of up to 41.1% with smaller models like Code-\nQwen1.5 (7B), further underscores its robustness. In general, these results validate MGDebugger as\na highly effective and scalable debugging method for LLM-generated code."}, {"title": "4.3 ABLATION STUDY", "content": "To understand the contribution of each component in MGDebugger and validate our design choices,\nwe conduct an ablation study by systematically removing key components of our method: hierar-"}, {"title": "4.4 DEBUGGING DIFFERENT TYPES OF BUGS", "content": "To assess the versatility and effectiveness of MGDebugger across various bug categories, we carry\nout experiments using the HumanEvalFix dataset, which is specifically designed to evaluate code\ndebugging performance. The dataset involves six distinct bug categories: value misuse, missing\nlogic, excess logic, operator misuse, variable misuse, and function misuse, allowing us to examine\nhow effectively MGDebugger addresses different types of programming errors compared to existing\nmethods. The detailed explanations of each bug category are available in Appendix B.\nTable 3 presents the RSRs across various bug categories. We observe that MGDebugger consistently\noutperforms other methods with significantly higher overall accuracies. And MGDebugger achieves\na remarkable repair success rate of 97.6% using DeepSeek-Coder, with 100% success rates in all bug\ncategories except for value misuse. This is particularly notable given the complexity and diversity\nof the bugs in the dataset. This highlights the effectiveness of the hierarchical debugging strategy.\nLooking into details of different bug categories, MGDebugger shows a strong advantage in debug-\nging bottom-level bugs, such as missing logic and excess logic. Missing logic refers to situations\nwhere essential code is omitted, preventing the solution from functioning correctly. Excess logic,\non the other hand, involves unnecessary code that can lead to mistakes and confusion (Muennighoff\net al., 2023). Other methods often struggle to identify and address these underlying issues because\nthey treat the code holistically. This can lead to confusion over bottom-level details when dealing\nwith complex logical errors. By contrast, the hierarchical decomposition in MGDebugger allows\nit to focus on different levels of code granularity. This enables more effective identification and\ncorrection of bugs. These results demonstrate the robustness and versatility of MGDebugger across\nvarious bug types."}, {"title": "4.5 DEBUGGING CODE WITH VARYING LENGTH", "content": "We further assess the versatility of MGDebugger in debugging code of varing lengths (i.e., number of\ntokens), since code length often correlates with complexity and debugging challenges. We categorize\ncode snippets from the HumanEvalFix dataset into short, medium, and long groups, ensuring equal\nsample sizes. We subsequently analyze the RSR scores obtained by MGDebugger and baselines\nwhen using DeepSeek-Coder as the backbone LLM.\nThe results are presented in Figure 3. We can observe that as the code length increases, most\nmethods experience an obvious decrease in performance due to the increased complexity. We note\nthat MGDebugger consistently outperforms other methods in different code lengths and especially\nexcels in debugging longer and more complex code snippets. This showcases the scalability and\nrobustness of MGDebugger in handling code of varying lengths and complexities. The results on\nother two datasets are available in Appendix D, where MGDebugger also consistently outperforms\nother methods across different code lengths."}, {"title": "4.6 IMPACT OF DEBUG ATTEMPTS", "content": "Another important factor for LLM-based debugging is the number of debugging attempts. Itera-\ntive debugging allows LLMs to refine their corrections over multiple passes, potentially leading to\nbetter outcomes. We aim to assess MGDebugger's ability to improve over successive iterations.\nFollowing Zhong et al. (2024), we vary the number of debugging attempts from 1 to 10 using the\nHumanEvalFix dataset and DeepSeek-Coder.\nThe results in Figure 4 show that MGDebugger achieves the highest cumulative RSR score among\nall methods, highlighting its ability to continually refine its debugging over multiple attempts. In par-\nticular, while most methods plateau after the first few debug attempts, MGDebugger and Reflexion\ncontinue to improve with more iterations. This result underscores the great potential of MGDe-\nbugger for iterative and comprehensive debugging, making it a promising solution for complex and\nchallenging code repair tasks. The results on the other two datasets are available in Appendix E,\nwhere MGDebugger outperforms other methods from the first attempt and continues to improve\nwith great potential."}, {"title": "4.7 CASE STUDY", "content": "We perform a qualitative analysis of how MGDebugger effectively identifies and corrects buggy\nparts compared to baseline methods. Figure 5 shows an example of debugging code snippets from\nthe HumanEvalFix dataset using MGDebugger and other representative methods, with DeepSeek-"}, {"title": "5 CONCLUSION", "content": "In this paper, we introduced MGDebugger, a novel hierarchical code debugging framework that\nsystematically fixes bugs at multiple levels of granularity. By decomposing complex code into\na hierarchical structure, generating targeted test cases and employing LLM-simulated execution,\nMGDebugger effectively identifies and fixes bugs ranging from syntax errors to logical flaws in a\nbottom-up manner. Experiments across various models and datasets demonstrate MGDebugger's\nsuperior performance over existing methods, particularly in handling complex logical errors and\nlonger code snippets.\nFuture work can build upon this foundation to develop more advanced code generation and debug-\nging methodologies. One direction is to extend MGDebugger to handle more complex bugs and code\nstructures, such as multi-file projects and codebase with multiple dependencies. Another direction\nis to explore the collaboration of hierarchical code generation approaches such as Parsel (Zelikman\net al., 2023) with hierarchical debugging, enabling end-to-end code generation and debugging sys-\ntems. Furthermore, integrating MGDebugger into self-training systems to correct outputs from base\nmodels, then retraining the base models with the corrected data, could potentially improve their\nperformance iteratively (Gulcehre et al., 2023)."}, {"title": "A DETAILED HIERARCHICAL DECOMPOSITION EXAMPLE", "content": "We provide the detailed illustration of the hierarchical decomposition process in MGDebugger, as\nshown in Figure 6, which has been simplified for illustration in Figure 1. For the original function\n\"make_palindrome\", we decompose it into three minimal reusable subfunctions. And the relation-\nships between subfunctions are naturally captured in the hierarchical structure based on the function\ncalls. This hierarchical decomposition allows MGDebugger to systematically analyze and debug the\ncode at different levels of granularity, leading to more effective identification and correction of bugs."}, {"title": "B HUMANEVALFIX DATASET", "content": "To access the ability of MGDebugger in debugging code with different types of bugs, we use the\nHumanEvalFix dataset (Muennighoff et al., 2023), which consists of 164 buggy functions across six\nprogramming languages, each provided with solutions and unit tests. For our experiments, we focus\non the Python subset of the dataset. The buggy functions are categorized into six types of bugs:\nvalue misuse, missing logic, excess logic, operator misuse, variable misuse, and function misuse.\nTable 4 shows the distribution and explanations of these bug types within the HumanEvalFix dataset."}, {"title": "C IMPLEMENTATION DETAILS", "content": "We generate seed programs for HumanEval and MBPP using the BigCode Evaluation Harness\nframework\u00b3. The specific versions of models used in our experiments are DeepSeek-Coder-V2-\nLite-Instruct, CodeQwen1.5-7B-Cha\u0165, and Codestral-22B-v0.16. All experiments are conducted\non NVIDIA A100 GPUs with 80GB memory. During debugging, we use the vLLM engine\u00b9 to\nserve the LLMs, setting the maximum token length according to each LLM's max length. Follow-\ning Zhong et al. (2024), we limit the maximum number of debugging iterations to 10 for all methods.\nAdditionally, the sampling temperature is set to 0.8 in MGDebugger.\nTo obtain visible test cases for HumanEval and HumanEvalFix, we extract the given visible test\ncases from the task description. For MBPP, we use the first test case of each problem as the visible\ntest case and use the rest as hidden test cases, in line with the settings referenced from Chen et al.\n(2023b) and Zhong et al. (2024)."}, {"title": "D DEBUGGING CODE WITH VARYING LENGTHS ON HUMANEVAL AND MBPP", "content": "To further demonstrate the robustness of MGDebugger in handling code of varying lengths, we\npresent examples from MBPP and HumanEval. Similar to the examples provided for HumanEval-\nFix, we categorize the problems into short, medium, and long groups based on their code lengths,\nand we measure the repair success rates of MGDebugger and other baseline methods. All methods\nare built upon DeepSeek-Coder-V2-Lite. As is observed in Figure 7 and Figure 8, MGDebugger\nconsistently outperforms other methods across different code lengths, especially in longer codes.\nThis result demonstrates the scalability and robustness of MGDebugger in handling code of varying\nlengths and complexities again."}, {"title": "E IMPACT OF DEBUG ATTEMPTS ON HUMANEVAL AND MBPP", "content": "We also investigate the impact of debug attempts on the cumulative repair success rate of MGDe-\nbugger and other methods on HumanEval and MBPP. As shown in Figure 9 and Figure 10, MGDe-\nbugger continues to improve with more debug attempts and achieves the highest success rate among\nall methods. Different from the results on HumanEvalFix that MGDebugger starts to ourperform\nother methods after the first attempt, MGDebugger significantly outperforms other methods from the\nbeginning to the end on HumanEval and MBPP. This result highlights the effectiveness of MGDe-\nbugger in iterative and comprehensive debugging, making it a promising solution for complex and\nchallenging code repair tasks."}, {"title": "F EXAMPLES", "content": "We provide example code repairs for HumanEval, MBPP, and HumanEvalFix with DeepSeek-\nCoder-V2-Lite as the base model. The results of MGDebugger and baselines: Simple Feedback,\nSelf-Edit, LDB (Block), LDB (Line), LDB (Function), Self-Debugging (Expl.), Self-Debugging\n(Trace) and Reflexion, are shown in the following tables. The buggy part in the original code is\nhighlighted in yellow, and the repaired code is compared with the original buggy code, with changes\nhighlighted in green if the repair passes the final test cases and in red if it fails. The functional com-\nments in the solution code have been replaced with placeholders for brevity, as they are the same as\nthose in the problem description.\nThe success of code repair often depends on initial solutions, as other methods typically change only\na few lines of the original code, keeping the overall structure the same. This tendency to keep the\nstructure of the initial solution may cause other methods to miss important parts of the code that are\nactually flawed. By contrast, by breaking down the code into smaller parts and looking at different\nlevels of detail, our approach makes these seemingly correct but actually wrong parts easier to spot,\nas seen in the example of HumanEval. Also, the core of the code is often found in just a few lines,"}, {"title": "G PROMPT TEMPLATES FOR MGDEBUGGER", "content": "We provide prompt templates for the three main components of MGDebugger: hierarchical decom-\nposition, test case generation, and sub-function debugging with LLM-simulated execution. These\nprompts are designed to guide the language model in generating the desired outputs for each step of\nthe debugging process. They have been slightly modified for clarity and brevity, please refer to our\ncodes if you need the exact prompt templates 8."}]}