{"title": "Bridging the Gap between Expert and Language Models: Concept-guided Chess Commentary Generation and Evaluation", "authors": ["Jaechang Kim", "Jinmin Goh", "Inseok Hwang", "Jaewoong Cho", "Jungseul Ok"], "abstract": "Deep learning-based expert models have reached superhuman performance in decision-making domains such as chess and Go. However, it is under-explored to explain or comment on given decisions although it is important for human education and model explainability. The outputs of expert models are accurate, but yet difficult to interpret for humans. On the other hand, large language models (LLMs) produce fluent commentary but are prone to hallucinations due to their limited decision-making capabilities. To bridge this gap between expert models and LLMs, we focus on chess commentary as a representative case of explaining complex decision-making processes through language and address both the generation and evaluation of commentary. We introduce Concept-guided Chess Commentary generation (CCC) for producing commentary and GPT-based Chess Commentary Evaluation (GCC-Eval) for assessing it. CCC integrates the decision-making strengths of expert models with the linguistic fluency of LLMs through prioritized, concept-based explanations. GCC-Eval leverages expert knowledge to evaluate chess commentary based on informativeness and linguistic quality. Experimental results, validated by both human judges and GCC-Eval, demonstrate that CCC generates commentary that is accurate, informative, and fluent.", "sections": [{"title": "Introduction", "content": "Artificial intelligence (AI) has achieved superhuman performance in various decision-making tasks, particularly in abstract strategy games like chess and Go. Milestones such as Deep Blue's victory over the world chess champion and AlphaGo's defeat of top human Go players highlight AI's capabilities in solving complex problems. While these expert models deliver highly accurate decisions, they often lack interpretability, which is critical for hu-"}, {"title": "Related work", "content": "Chess commentary generation Chess commentary generation is generating a comment for a chess move. first address the task by utilizing web-crawled data to form a chess commentary dataset, framing commentary generation as a sequence prediction problem. Building on this, incorporate domain-specific chess knowledge using internal chess models, improving quality and contextual relevance of generated comments. , integrate BART and an external chess engine for more reliable move evaluation. However, their system classifies moves into predefined categories (e.g., excellent, good, inaccuracy, mistake, blunder), without deeper understanding of the model decision-making process. In contrast, we leverage concept-based explanation to extract chess concepts from an expert model to understand the rationale behind the decision.\nNot limited to chess commentary, fine-tune an LLM on chess-related data, to leverage chess skills, not only the linguistic ability. However, we demonstrate that its understanding of chess knowledge is inferior to GPT-40 (OpenAI, 2023) (Section 4.4).\nConcept-based explanation in chess Concepts are high-level abstractions commonly shared within a community, enabling efficient communication. In chess, concepts such as \"king safety\" (i.e., all potential threats against the king) condense complex strategies into understandable terms, allowing players to communicate effectively without lengthy explanations. These concepts are understandable to both humans and language models, serving as a bridge between human intuition and neural networks. Concept-based explanations aim to make a model interpretable by aligning its internal decision-making process with these shared concepts, assuming that such concepts are linearly embedded in the representation space. This assumption is validated in chess domains for chess expert models like Stockfish, AlphaZero"}, {"title": "Method: generation and evaluation", "content": "We propose two methods to address chess commentary generation (Section 3.1) and chess commentary evaluation (Section 3.2)."}, {"title": "Concept-guided commentary generation", "content": "We propose Concept-guided Chess Commentary generation (CCC), which is a method for generating chess commentary by leveraging a chess expert model and its concept-based explanations. The method involves two key steps: 1) extracting concept vectors from a chess expert model (Section 3.1.1); and 2) generating commentary via an LLM using prioritized concepts that explain the given position and movement (Section 3.1.2). Figure 2 provides an overview of the proposed method."}, {"title": "Concept vector extraction", "content": "To make a chess expert model interpretable, we extract concept vectors that correspond to key concepts in chess. We follow a common approach involving two steps: preparing a dataset for concept learning and extracting concept vectors by training a linear classifier. The concepts we focus on are adopted from Stockfish 8, a classical chess engine that can evaluate positions for their relevance to specific concepts (see Table 6). We collect 200,000 chess positions from the Lichess open database and use Stockfish 8 to assign a score reflecting how strongly each position relates to these concepts. We then label the top 5% of positions with the highest scores as positive samples and the bottom 5% with the lowest scores as negative samples. This process results in a dataset of 20,000 positions for each concept, split equally between positive and negative samples. We employ LeelaChessZero T78, an open-source neural network-based chess model similar to AlphaZero for extracting concept vectors. For the representation space, we use the final layer before policy and value heads (layer 40). We then train a linear Support Vector Machine (SVM) to classify these samples. The resulting normal vector of the SVM classification boundary serves as the concept vector, and the distance from this boundary determines the concept score for any"}, {"title": "Chess comment generation with an expert model and extracted concepts", "content": "Given a chess position and a specific move, our goal is to identify the concepts most relevant to explaining that movement. For the chess position, we compute the score for each concept by taking the dot product between the expert model representation of the position and the extracted concept vectors. These concept scores reflect how strongly each concept is reflected in the current position. To prioritize concepts, we compare the concept scores before and after the move. By analyzing the differences between pre-move and post-move scores, we identify which concepts are most influenced by the move. This allows us to assign priority to the concepts that explain the impact of the move.\nCommentary generation via LLM We generate chess commentary using an LLM and a chess expert model. Although a language model understands chess-specific notations and terms, it lacks the ability to perform chess-specific reasoning and complex analysis, which can result in hallucination. By integrating chess expert model output, the LLM determines whether to focus on advantageous aspects or disadvantageous aspects. However, since the chess expert model output is based on scalar values, it still generates incorrect comments. Concept-based explanation guides the LLM"}, {"title": "Automatic evaluation of commentary", "content": "Our evaluation approach, termed GCC-Eval, modifies and extends G-Eval to better address the specific challenges of evaluating chess commentary. The core components of GCC-Eval are: (i) Multidimensional evaluation by an LLM. (ii) Expert model evaluation for chess knowledge. (iii) Auto-CoT for score-only output. (iv) weighted summation for non-integer scores. Note that our contributions are on the first and second aspects to ensure accurate chess commentary evaluation, focusing on informativeness and linguistic quality.\nEvaluation dimensions The evaluation covers four dimensions: relevance, completeness, clarity, and fluency. While clarity and fluency are general linguistic measures, relevance and completeness require a deep understanding of chess. To address this, we employ an expert model to augment the LLM's capabilities when scoring relevance and"}, {"title": "Experiments", "content": "We evaluate our model using Chess Commentary dataset introduced by . This dataset contains full chess games accompanied by user-generated commentary on specific moves, collected from an online chess forum. Following the train/valid/test split introduced by , we use only the test set for our experiments. Since the absence of pre-processing code, we manually align the raw data with pre-processed data to ensure fair comparison with GAC. Additionally, we exclude comments that covering multiple moves for simplicity in analysis.\nBaselines We compare the experimental results within several methods:\nreference: These are reference texts from the GameKnot dataset.\nGAC: An LSTM model trained on the GameKnot dataset for generating chess commentary.\nGPT-40: The unmodified version of GPT-40, accessed via OpenAI API, with a temperature setting of 0.1 to avoid noisy outputs. For detailed discussion of comparison of LLMs, refer to Section 4.4.\nGPT-40 + expert: This is the same GPT-40 model but augmented with evaluations from a chess expert model. Note that use BART with a chess expert model and GPT-40 + expert is superior because it uses more powerful language model and a sufficient expert model."}, {"title": "Human evaluation", "content": "We conduct a manual human evaluation for evaluating the quality of comments. For the reliability of evaluation, we ensure every participant possesses sufficient chess knowledge to evaluate chess comments. Specifically, we recruit five participants from university community and SNS. Each participant has a chess.com rapid rating above 1500, which is 99.51st percentile among chess players, with an average rating of 1776. The human evaluation is conducted in a within-participant setup. For each move, each participant evaluates five versions of comments generated by five methods (i.e., four baselines and CCC), where the order of methods are randomized. A total of 50 moves are evaluated by the participants (i.e., a total of 250 comments). The evaluation take approximately four hours to complete. Each participant is compensated by an amount equivalent to 73 USD. Our university IRB approves the evaluation plan. Appendix C summarizes details of human evaluation, including the instructions and questions used.\nDuring the evaluation, participants are presented with a chessboard displaying a specific move, marked with a blue arrow. Alongside the moves, the corresponding commentaries are provided. Each participant is asked to rate the commentary across six questions: five evaluation metrics and one question for categorizing the type of incorrectness when applicable. The evaluated metrics are: correctness, relevance, completeness, clarity, and fluency. Relevance and completeness evaluate how the comment is informative and insightful, and clarity and fluency evaluate how the comment is linguistically natural. Relevance, completeness, clarity, and fluency are assessed using a five-point Likert scale, while correctness is evaluated using a three-point Likert scale, as the correctness of a comment is closer to a binary decision rather than a scaled question. For clear presentation, the scores were rescaled to a range of 0 to 1.\nMain results Table 1 presents the results of the human evaluation. Our proposed method, CCC, achieves the highest scores in all metrics except correctness, where it ranks second. Also, CCC outperforms the reference comments in every metric except correctness, and the correctness is also comparable to the reference. The reference com-"}, {"title": "Automatic evaluation", "content": "To perform an automatic evaluation of generated chess commentaries, we employ our proposed metric, GCC-Eval. This metric is designed to assess both linguistic quality and domain-specific relevance in chess commentary. To validate its reliability, we calculate the correlation between GCC-Eval scores and human evaluations using the same dataset from prior human evaluation studies. As shown in Table 4, GCC-Eval consistently shows a higher correlation with human assessments across all evaluation criteria compared to traditional metrics, such as BLEU and ROUGE, which rely on surface-level similarity measures with reference comments. We further apply GCC-Eval to evaluate the performance of different chess commentary generation methods. The results in Table 5 indicate that CCC outperforms the baselines in all GCC-Eval metrics, showcasing the effectiveness of integrating domain-specific expertise and concept-based"}, {"title": "Other experiments", "content": "Chess skills and knowledge of language model While LLMs can generate linguistically sound commentary, they lack the deep, inherent understanding of chess strategies. Integrating expert models like chess engines compensates for this limitation, ensuring that the LLM's output is both fluent and grounded in expert knowledge. To verify the chess skill level of LLMs, we use mate-in-one chess problems and evaluate how the models solve them, in Table 7. GPT-40 solves 57% of problems, while other language models are below 12%, even though ChessGPT is fine-tuned on chess-related documents. When the expert model evaluation result is given in prompt, the LLM solves 95% of the problems, which is not surprising because the expert model evaluation includes the answer. While GPT-40 + expert includes the answer in the prompt, GPT-40 + concept also shows significant improvement of 17.2%p, with only a simple hint that there is a mate. It implies that a proper concept serves as a powerful hint for the precise analysis of"}, {"title": "Reliability of the concept-based explanation", "content": "We assess the reliability of the concept-based explanations. Table 6 shows that the average accuracy of the extracted chess concepts is 0.91, demonstrating that the model effectively identifies and utilizes key domain-specific concepts. This further supports the idea that concept-based explanations serve as reliable source for guiding the LLM in generating chess comments."}, {"title": "Interactive commentary generation", "content": "We also explore the potential of CCC for generating interactive and context-aware chess commentary. By augmenting the LLM with the decision-making capabilities of an expert model, it responds to flexible user questions, providing deeper insights beyond simple commentary on a move. The questions can be strategic intentions, long-term plans, and potential threats in a given chess position. An example of these interactive commentary capabilities and corresponding results are found in Appendix D. These experiments demonstrate that CCC is capable"}, {"title": "Discussions", "content": "Language model as an explanation form Our work shows that the CCC framework effectively transfers AI-driven chess knowledge to human users. Beyond concept-based explanation, language models can act as a crucial medium between the expert model's internal reasoning and the end-user. This connection facilitates more intuitive and understandable feedback than traditional explanation methods like saliency-based, which suffer from issues of inconsistency and unreliability. By employing language-based form of explanation, the transparency of the explanation can be improved, making the evaluation of the model's reliability more straightforward."}, {"title": "Fine-tuning with GCC-Eval", "content": "We validate that GCC-Eval is well-correlated with human evaluation. One promising direction to improve the quality of chess commentary is to incorporate GCC-Eval as a training objective, replacing human evaluator. By optimizing models to directly align with this evaluative criterion, we can better ensure that the generated commentary meets the standards of human chess experts. This approach offers a potential pathway toward more robust and human-aligned commentary systems in future applications."}, {"title": "Conclusions", "content": "In this paper, we propose methods for chess commentary generation (CCC) and evaluation (GCC-Eval). CCC integrates expert and language models through concept-based explanations, utilizing techniques such as prioritization, few-shot learning, and Chain-of-Thought prompting to align effectively with expert knowledge. CCC either surpasses or matches the quality of human-generated commentary, demonstrating the capability of LLMs to express expert-level understanding and potentially enhance learning for human users. We also present GCC-Eval, a multi-dimensional evaluation framework that incorporates chess-specific knowledge to assess chess commentary. The strong correlation between human evaluation and GCC-Eval validates the robustness. These findings underscore promising future research directions, including using a language model as an explanation method and using GCC-Eval fine-tuning chess commentary generation models."}, {"title": "Limitations", "content": "Use of proprietary LLMs We plan to release the source code and datasets used in our experiments. However, since we employed proprietary LLMs including GPT-4o, GPT-4o-mini, and GPT-3.5-turbo (from July to October 2024), it can be limited to fully reproduce the results. Nonetheless, the proposed framework remains adaptable and can be further enhanced with the integration of more advanced LLMs. In addition, it is also interesting to further investigate the efficacy of our framework with smaller LLMs.\nEducational purpose / comment for beginners The main audience for commentary is often beginners and or those with less knowledge than the commentator. In the human evaluation in Section 4.2, we assess the commentary in the view of expert chess players. Another human evaluation involving novice players can assess the educational impact of the comments. For the same purpose,  propose counterfactual simulatability, as an automatic evaluation metric of the improvement of students.\nBeyond chess commentary Although we focus on the chess commentary generation, our method can be extended to other tasks, that require comprehensive decision-making abilities and have an expert model. Empirical experiments in other tasks require finding the appropriate tasks and corresponding expert models.\nMore concepts Although we use concepts from Stockfish 8, there are other useful concepts such as fork, pin, double-pawn or open-file. We do not use the concepts because of insufficient concept labels, but they could be valuable, as the concept \"mate-in-one\" improves chess skill in Table 7.\nDifferences between concept evaluation function and extracted concept In our work, we extract the concept vectors from an expert model. Although using oracle concept evaluation functions is relatively more accurate, there are two key reasons for using the extracted concepts. First, recent findings emphasize that expert models often possess super-human knowledge, capturing patterns and strategies not easily interpretable by humans. It implies the extracted concepts can cover the comprehensive knowledege of model, even if the humans do not understand and an oracle concept evaluation function is not present."}]}