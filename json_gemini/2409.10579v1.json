{"title": "Recent advances in deep learning and language models for\nstudying the microbiome", "authors": ["Binhao Yan", "Yunbi Nam", "Lingyao Li", "Rebecca A. Deek", "Hongzhe Li", "Siyuan Ma"], "abstract": "Recent advancements in deep learning, particularly large language models (LLMs), made significant\nimpact on how researchers study microbiome and metagenomics data. Microbial protein and\ngenomic sequences, like natural languages, form a language of life, enabling the adoption of LLMs\nto extract useful insights from complex microbial ecologies. In this paper, we review applications\nof deep learning and language models in analyzing microbiome and metagenomics data. We focus\non problem formulations, necessary datasets, and the integration of language modeling techniques.\nWe provide an extensive overview of protein/genomic language modeling and their contributions\nto microbiome studies. We also discuss applications such as novel viromics language modeling,\nbiosynthetic gene cluster prediction, and knowledge integration for metagenomics studies.\nKeywords: Microbiome, Virome, Artificial Intelligence, Large Language Models, Transformer, Attention", "sections": [{"title": "1 Introduction", "content": "The study of microbiomes and metagenomics has significantly advanced our understanding of microbial\ncommunities and their complex interactions within hosts and environments. The microbiome refers\nto the collective genomes of microorganisms residing in a specific habitat, such as human body sites\n(e.g., gut, skin, airway) and environments (e.g., air, soil, water). Metagenomics research involves the\ndirect profiling and analysis of these microbial communities' genomic sequences, bypassing the need\nfor isolating and culturing individual members. This approach allows for a comprehensive assessment\nof microbial diversity, functions, and dynamics within their natural contexts.\nThe complex dependency encoded in metagenomic sequences represents gene/protein-, organism-, and community-level biological structures and functions. Examples include residue-residue contact patterns for protein 3D structures, functional relationship between genes and their regulatory, non-coding counterparts (e.g., promoters, enhancers), mobility for horizontal gene transfers, and genome-scale organization of functional modules (e.g., operons and biosynthetic gene clusters). Such dependency"}, {"title": "2 Brief review of LLMs and their extension towards modeling\nthe language of life", "content": "LLMs are advanced foundation models specifically designed to understand and generate human\nlanguage. They can perform a wide range of natural language processing (NLP) tasks, such as question-answering, information extraction, and text summarization [Chang et al., 2024, Li et al., 2024]. The\nscalability, versatility, and contextual understanding of LLMs can be attributed to two key factors.\nFirst, LLMs are trained on massive datasets that encompass diverse linguistic patterns, enabling\nthem to learn complexities and nuances in languages as sequences of tokens (i.e., words and phrases).\nSecond, LLMs are built on the transformer architecture, which consists of an encoder and a decoder\nand uses self-attention mechanisms to process input sequences. The attention mechanism efficiently\nencodes dependency structures of sequential tokens, vastly increases the learnable lengths of long-range dependencies, and encodes tokens and sequences accounting for their upstream and downstream\nneighboring \"contexts\". This allows for efficient processing of sequential data, enabling LLMs to\nprovide a meaningful representation of input text and generate coherent and contextually relevant\noutput text based on input prompts [Vaswani et al., 2017].\nInspired by LLMs, language models in microbiome research often employ a similar architectural\ndesign (Figure 1). These language models of genomic sequences [Ligeti et al., 2024, Shang et al., 2023,\nMardikoraem et al., 2023] thus provide an improved representation of sequences with richer context\nand can be scaled up to and trained at impressive complexities (up to billions of model parameters).\nSuch models often include a transformer encoder component that processes input sequences such as\nprotein or DNA sequences and converts them into high-dimensional representations that capture\nessential features of input sequences in their contexts. The attention mechanism in these models\nassigns different weights to various parts of the sequence, characterizing dependency structures shaped\nover evolution and allowing the model to focus on relevant regions. This focus ensures that the model\nprioritizes areas within a sequence that are most significant for biological interpretation. Similar to the\napplication of BERT for various natural language tasks [Devlin, 2018], the encoded representation can\nthen be used for tasks such as contextualizing microbial genes and genomic sequences in their broader\ngenomic neighborhoods [Hwang et al., 2024], predicting the structure and functions of protein given\ntheir sequences [Lin et al., 2023], and segmentation and identification of specific regulatory elements\nacross microbial genomes [Zhou et al., 2025]. In comparison, decoder-style models focus on generating\noutput sequences, given the encoded representation of past sequence tokens. This is more similar to\nGPT-style LLMs [Brown, 2020], whereby the task towards microbiome application often involves the\ngeneration of new, functional, and viable protein sequences [Ferruz et al., 2022, Madani et al., 2023,\nJin et al., 2024]."}, {"title": "3 Language modeling of proteins, contigs, and genomes of the\nmicrobiome", "content": "To facilitate the survey, we categorize existing language models for metagenomic sequences into two\nclasses: (1) models on the protein/gene scale and (2) those on the genome scale. The first, which we\nterm protein language models (Table 1), fits well within the context length for transformers since\nmicrobial proteins are generally under 1,000 AAs (tokens). In contrast, DNA or genomic language\nmodels (Table 2) often require additional techniques to extend their operating ranges due to the large\nscale of microbial contigs or whole genomes. For example, the bacterial genome typically ranges from\n0.5 to 10 million base pairs, a scale that often far exceeds the context window of transformers. In\naddition, the two classes target different applications: protein language models are used for designing\nand predicting individual proteins, while DNA/genomic language models examine genes and proteins\nwithin their broader genomic contexts as well as intergenic regions."}, {"title": "3.1 Protein language models for novel protein generation", "content": "Existing protein language models applied towards microbiome studies are summarized in Table 1.\nWe highlight two specific applications, namely, the generation of novel proteins and the prediction\nof their functions and structures. The dependency structure of amino acids across known microbial\nproteins is learned and utilized to generate artificial, potentially novel protein sequences by protein\nlanguage models such as ProGen [Madani et al., 2023] and ProtGPT2 [Ferruz et al., 2022]. This is\nperformed in an autoregressive fashion, often with decoder-only architecture similar to that of the\nGPT language models, whereby the likely AA at the next position is predicted given the sequence of\npreceding residues. If trained across a sufficiently large variation of raw occurring microbial protein\nspaces (millions or more protein sequences), models with enough flexibility can learn the inherent\nevolutionary patterns that natural protein sequences harbor and thus generate artificial proteins that\nare functionally viable like natural proteins.\nTo this end, ProtGPT2 was based on the GPT-2 architecture and trained on 50 million sequences\nspanning the entire protein space. Proteins generated by the model in return displayed propensities\nof amino acid sequences akin to those of natural proteins, but can still cover under-explored protein\nsequence regions. ProGen and its iteration [Nijkamp et al., 2023] performed similar modeling tasks,\nand additionally (1) allowed the inclusion of \"tags\" to specify protein properties for generating\nproteins in a more controllable fashion, and (2) experimentally verified that model-generated de novo\nprotein sequences were sufficiently distinct from natural proteins but demonstrated functional viability\ncomparable to them. Of note, while these models were typically trained to cover the universal protein\nspace (e.g., UniRef-50), both models highlight good coverage of microbial protein properties. ProGen\nspecifically validated the antibacterial functional property of its generated novel proteins that were\ncomparable to natural lysozyme."}, {"title": "3.2 Protein language models for function and structure prediction", "content": "Related to, but different from the task of generating novel protein sequences, prediction-focused protein\nlanguage models are primarily concerned with predicting proteins' biological properties (e.g., 3D\nstructures, functions) based on their AA residue sequences. Encoder-style language model architectures\nsuch as that of BERT are of particular relevance, as these models aim to learn the best representation\nof each token (i.e., AA) given the broader sequence context and thus can represent entire sequences in\na meaningful, efficient manner. For example, Elnaggar et al. [2022] developed several LMs for protein\nsequences, including two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder\nmodels (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino\nacids. Transformer Uniref90 MT from the Protein BERT project can be downloaded from the project\nGitHub repository (https://github.com/nadavbra/protein_bert) and protein sequences are embedded\nusing the function in the protein Bert python package. Such representations can then be fed as the input\nto downstream predictive models, often also realized with neural networks (NN), for various tasks.\nFor predicting protein structures, with scaling language models from 8 million to 15 billion\nparameters, the ESM-2 model [Lin et al., 2023] effectively internalizes evolutionary patterns directly\nfrom protein sequences. The learned attention patterns provided a low-resolution protein structure,\ncorresponding to residue-residue contact maps. This was further combined with a downstream\npredictive module to form the ESMFold model, which offers direct inference from sequence to protein\n3D structures and achieved comparable performances as SOTA protein structure prediction models\n(e.g., AlphaFold2). Of relevance to the microbiome, the authors applied their model to construct an\natlas of predicted structures of over 600 million metagenomic protein sequences. Another group of\npredictive tasks aims to mine biological functions based on protein sequences. As a representative, Ma"}, {"title": "3.3 DNA language models at the genomic scale", "content": "The full review of DNA/genomic language models is provided in Table 2. As discussed above, microbial\ngenomes have drastically increased scales compared to single genes or proteins. Genomic sequences\nalso possess much sparser biological information than proteins, containing intergenic regions with both\nfunctional and junk DNA elements. The DNA sequence vocabulary also only consists of four different\ntypes of nucleotides, less than the 20 different AAs that typically constitute protein sequences. As\nsuch, language models that operate on the genome scale require additional considerations than protein\nmodels and can be further divided into two categories. The first type, often termed in literature as\nDNA language models, focuses on modeling DNA sequences truly on the full genome scale of organisms,\ne.g., DNABERT [Zhou et al., 2025], Nucleotide Transformer (NT, Dalla-Torre et al. [2023]). As such,\nthey adopt techniques such as specialized tokenization, alternative attention patterns, and hierarchical\nmodeling architecture to drastically extend model contextual lengths. An important advantage of this\napproach is that it allows for the representation and identification of non-coding functional elements\non the DNA (e.g., promoters)."}, {"title": "3.4 Genomic language models contextualize genes and gene clusters.", "content": "Alternatively, another group of metagenome language models examines medium- to long-range contexts\nbetween genes, often operating on the contig scale and excluding intergenic sequences. We term\nthese as genomic models as an intermediate approach between protein and DNA language models.\nThese models often adopt hierarchical scaffolding across genes (genes themselves are embedded by\nprotein language models), to provide a contextualized and richer representation of genes in their\nbroader genomic neighborhood. Gene properties such as their differential functions across microbes\nand genome/community-scale organization (horizontal gene transfer, operon membership) can then\nbe further interrogated, which is not possible in protein language models where they are modeled in\nisolation from each other.\nIn comparison to full-scale DNA language models, genomic language models such as the gLM\n[Hwang et al., 2024] and FGBERT [Duan et al., 2024] instead focus on contig- to genome-scale\norganization of microbial genes (see Table 2). gLM, for example, adopts EMS-2 protein embeddings\nfor each gene and models their genomic dependency structures on the contig (15 to 30 genes in length)\nscale. This enables, first, the enrichment of each gene's embedding in its broader genomic texts. Genes'\n\"higher-order\", genome- and community-level functional properties can be further delineated that are\nindistinguishable from protein-scale language modeling alone, such as differential gene functions in\ndifferent biomes and microbial species, as well as their self-mobility in horizontal gene transfer events\nacross genomes. Secondly, the organization of gene clusters in linkage with each other on the genome\ncan also be represented, whereby subsets of model attention patterns from gLM and FGBERT both\ndemonstrated correspondence with operon memberships. The longer-scale organization of biosynthetic\ngene clusters is also relevant and discussed in a dedicated section as a specialized task. As population-scale studies of the microbiome often focus on gene- or pathway-level sample profiles, such genomic\nlanguage models provide practical intermediate solutions to enrich microbiome studies using recent\nlanguage model advancement with microbial gene elements' broader genomic contexts."}, {"title": "4 Language models for virome annotation and virome-host\ninteractions", "content": "The human virome consists of viruses that infect eukaryotic cells (eukaryotic viruses) and prokaryotic\nviruses, also known as bacteriophages. The gut virome is a vital component of the microbiome in the\nhuman gut, consisting mainly of viruses that infect bacteria (bacteriophages or phages), along with\nother viral species that may infect eukaryotic cells. The virome plays a crucial role in maintaining gut\nhealth by influencing the bacterial population dynamics, shaping immune responses, and potentially\naffecting the overall metabolic environment of the gut.\nMetagenomic sequencing of the gut microbiome provides a wealth of information that aids in\nidentifying viruses, especially bacteriophages, which are key players in viral-bacterial interactions.\nOne important method for studying these interactions is through CRISPR spacers, which serve as a\nmolecular record of past viral infections in bacterial genomes. CRISPR-Cas systems are a bacterial\nimmune defense mechanism that targets invading viruses, primarily bacteriophages [Dion et al., 2021].\nThere has been significant interest in applying recently developed protein or DNA sequence language\nmodels in virome sequence identification and annotation, as well as in building predictive models for\nvirus-bacterium interactions based on sequence data."}, {"title": "4.1 Virome sequence annotation and identification", "content": "Annotation of viral genomes in metagenomic samples is a crucial first step in understanding viral\ndiversity and function. Current annotation approaches primarily rely on sequence homology methods,\nsuch as profile Hidden Markov Model (pHMM)-based approaches. However, these methods are limited\nby the scarcity of characterized viral proteins and the significant divergence among viral sequences. To\naddress these challenges, Flamholz et al. [2024] applied curated virome protein family (VPF) databases\nalongside recently developed protein language models (PLMs). They demonstrated that PLM-based\nrepresentations of viral protein sequences can capture functional homology beyond the reach of\ntraditional sequence homology methods. Their reference annotations were derived from the Prokaryotic\nVirus Remote Homologous Groups (PHROGs) database, a curated library of VPFs designed to detect\nremote sequence homology. PHROGs is manually annotated into high-level functional categories and\ncontains 868,340 protein sequences clustered into 38,880 families, of which 5,088 are assigned to 9\nfunctional classes. Using these data, Flamholz et al. [2024] showed that PLM-based representations of\nviral proteins can effectively predict their functions, even in the absence of close sequence homologs.\nPeng et al. [2024] developed a viral language model (ViraLM) that adapts the genome foundation\nmodel DNABERT-2 [Zhou et al., 2025] for virus detection by fine-tuning the model for a binary\nclassification of novel viral contigs in metagenomic data. DNABERT-2 has been pre-trained on a vast\narray of organisms, acquiring valuable representations of DNA sequences, which is particularly useful\nfor distinguishing viral sequences from those of other species. To adapt the genome foundation model\nfor virus detection, they fine-tuned this model for a binary classification task with two labels: viral\nsequences vs. others, where they constructed a substantial viral dataset comprising 49,929 high-quality\nviral genomes downloaded from the NCBI RefSeq, spanning diverse taxonomic groups as positive\nsamples. The negative data (245,734 non-viral sequences) are complete assemblies of bacteria, archaea,\nfungi, and protozoa, also downloaded from the NCBI RefSeq. The genomes are randomly cut into\nshort contigs ranging from 300 bp to 2k bp to minic variable-length contigs in the metagenomic data.\nThey observed that the model initialized using the pre-trained foundation model converges faster and\nperforms better in virus contig identification."}, {"title": "4.2 Deep learning and LLM methods for virome-host interaction", "content": "One important problem in virome research is to predict which viruses can infect which hosts, a crucial\nstep for understanding how viruses interact with hosts and cause diseases. Virome-host interactions\nalso play a crucial role in understanding and defining phage therapy, which uses bacteriophages\n(viruses that infect bacteria) to treat bacterial infections. Currently, there are no high-throughput\nexperimental methods that can definitively assign a host to the uncultivated viruses.\nA number of computational approaches have been developed to predict unknown virus-host\nassociations. The coevolution of a virus and its host left signals in their genomes, which have been\nexploited for computational prediction of virus-host associations. The alignment-based approaches\nsearch for homology such as prophage [Roux et al., 2015] or CRISPR-cas spacers [Staals and Brouns,\n2013, Horvath and Barrangou, 2010]. Algorithms like BLAST (Basic Local Alignment Search Tool)\nare commonly used to align viral sequences with host genome sequences to detect homology. This can\nreveal conserved regions in viral and host proteins, such as receptor-binding domains that allow viruses\nto enter host cells. In contrast, alignment-free methods use features such as k-mer composition, codon\nusage, or GC content to measure the similarity between viral and host sequences or to other viruses\nwith a known host. By identifying which viral genomes contain sequences matching a bacterium's\nCRISPR spacers, researchers can infer potential virus-host interactions. However, this approach is\nlimited by the set of known CRISPR spacers.\nAs a comparison, predicting virus-host interactions based on k-mer matching and codon usage\nanalysis is another powerful approach for identifying novel viral-bacterial interactions. Codon usage\nrefers to the frequency with which different codons are used to encode amino acids in a genome. When\na virus's codon usage matches that of its host, it suggests that the virus has evolved to efficiently\nexploit the host's translational machinery, enhancing its ability to replicate within that host. This is a\ncritical factor in predicting potential virus-host interactions. By performing joint analysis of codon\nusage and other genomic features, researchers can achieve more accurate predictions regarding which\nhost species are susceptible to particular viruses.\nSince these genomic features are embedded in the viral or bacterial genomes, it is possi-\nble to learn these features automatically using machine learning and AI methods. Liu et al.\n[2023a] developed evoMIL for predicting virus-host association at the species level from viral\nsequence only. They used datasets that were collected from the Virus-Host database VHDB,\n(https://www.genome.jp/virushostdb/), which contains a manually curated set of known species-level\nvirus-host associations collated from a variety of sources, including public databases such as RefSeq,\nGenBank, UniProt, and ViralZone and evidence from the literature surveys [Liu et al., 2023a]. For\neach known interaction, this database provides NCBI taxonomic ID for the virus and host and the\nRefseq IDs for the virus genomes. The final data set includes 17,733 associations between 12,650\nviruses and 3740 hosts that were used to construct binary datasets for both prokaryotic and eukaryotic\nhosts. For each of the hosts, an evoMIL model is built to predict the possible interacting viruses.\nThey then applied the pre-trained ESM-1b model to transform protein sequences into fixed-length\nembedding vectors, which serve as features for downstream binary and multi-class classification.\nAdditionally, they applied multiple instance learning (MIL) [Maron and Lozano-P\u00e9rez, 1997], where\nmultiple instances are grouped together with a single label, and are classified as a whole. They\nemployed attention-based MIL [Ilse et al., 2018] for each host. Specifically, for each host, they collected\nthe same number of positive and negative viruses and then obtained embeddings of protein sequences\nfrom viruses obtained by the pre-trained transformer model ESM-1b. To handle the input length of\nthe PLMs, they split the protein sequences of viruses to sub-sequences for generating embeddings. An\nattention-based MIL was applied to train the model for each host dataset by protein feature matrices\nof viruses. The resulting models can be used to predict whether a new virus interacts with a host for\nwhich a corresponding predictive model has been developed.\nIn addition to species-level virus-bacterium interaction prediction, Gaborieau et al. [2023] introduced\na novel dataset and prediction model that focuses on phage-bacteria interactions at the strain\nlevel, utilizing genomic data of 403 natural, phylogenetically diverse, Escherichia strains and 96\nbacteriophages. Their findings highlight that bacterial surface structures, such as lipopolysaccharides\n(LPS) and capsules, play a critical role in determining these interactions. Specifically, they identified\nbacterial surface polysaccharides as key adsorption factors that significantly enhance the accuracy\nof interaction predictions. This offers a valuable dataset for developing phage cocktails to combat\nemerging bacterial pathogens."}, {"title": "5 Deep learning and language models for prediction of\nbiosynthetic gene clusters", "content": "Microbial secondary metabolites are chemical compounds that exhibit a broad range of functions\nand have great potential in pharmaceutical applications, such as antimicrobial agents and anticancer\ntherapies. These bioactive small molecules are usually encoded by clusters of genes along the bacterial\ngenome known as Biosynthetic Gene Clusters (BGCs). Although accurate, experimental validation of\nBGCs is laborious and costly. High-throughput sequencing techniques, alongside advanced genome\nassembly algorithms, have enabled people to access the vast amount of bacterial genomic data. The\ngenomic sequence data serves as a rich resource for BGCs mining, allowing researchers to better\nunderstand the functional potential of bacteria and discover new secondary metabolites or natural\nproducts.\nMachine learning-based algorithms have been developed for the detection of BGCs in microbial\ngenomes. antiSMASH [Medema et al., 2011] identifies candidate BGCs through multiple sequence\nalignment based on the profile hidden Markov model (pHMM) library constructed from experimentally\ncharacterized signature protein or protein domains, subsequently filtering these candidates using\ncurated rules based on expert knowledge. PRISM [Skinnider et al., 2017] employs a similar approach\nby searching through an HMM library. ClusterFinder [Cimermancic et al., 2014] utilizes a hidden\nMarkov-based probabilistic algorithm to identify known and unknown BGCs. Extending beyond these\nmethods, MetaBGC [Sugimoto et al., 2019] integrates segmented pHMM with clustering strategies,\nmaking it possible to detect BGCs directly from metagenomic reads.\nDespite the success of existing machine learning-based algorithms, traditional machine learning\nmodels cannot handle the long-range dependencies between genome sequences and cannot transfer\nknowledge from other datasets, thereby resulting in a lower power of detecting the new BGCs. Several\nmachine learning frameworks, including those with transformer-type language modeling architecture,\nhave been developed specifically for predicting bacterial BGCs. These models leverage advanced\ncomputational techniques to analyze genomic data and identify regions that encode for biosynthetic\npathways. Many existing methods use sequences of the protein family domains (Pfams) to characterize\nthe BGCs and bacterial genomics. Proteins are generally composed of one or more functional regions,\ncommonly termed domains. Different combinations of domains give rise to the diverse range of proteins\nfound in nature. The identification of domains that occur within proteins can therefore provide insights\ninto their function."}, {"title": "5.1 Deep learning methods for BGC prediction", "content": "DeepBGC is a deep learning-based tool that uses a combination of convolutional neural networks\n(CNNs) and recurrent neural networks (RNNs) to predict and classify BGCs in bacterial genomes. It\nprocesses raw genomic sequences to identify BGCs and provides detailed annotations of their functional\ncomponents [Hannigan et al., 2019]. e-DeepBGC further extended DeepBGC to incorporate functional\ndescription of protein family domains and to utilize the Pfam similarity database in data augmentation\n[Liu et al., 2022]. Pfam also generates higher-level groupings of related entries, known as clans. A clan\nis a collection of Pfam entries which are related by similarity of sequence, structure or profile-HMM.\nRios-Martinez et al. [2023] developed a deep learning model that leverages self-supervised learning\nto detect and classify BGCs in microbial genomes. This approach aims to improve the accuracy and\nefficiency of BGC identification and predict the types of natural products they produce."}, {"title": "5.2 BGC prediction based on language models", "content": "Lai et al. [2023] introduced BGC-Prophet, a neural network model that leverages natural language\nprocessing (NLP) techniques to analyze genomic sequences as linguistic data, identifying patterns\nindicative of biosynthetic gene clusters (BGCs). This innovative approach enables the model to grasp\nthe complex syntax and semantics inherent in genetic sequences. The input to BGC-Prophet consists of\nembeddings represented by 320-dimensional vectors, generated through ESM-2 [Lin et al., 2023]. The\nmodel architecture integrates convolutional neural networks (CNNs) with transformer-based models,\na hybrid design that effectively manages the sequential nature of DNA data, thereby enhancing the"}, {"title": "6 Public knowledge integration in microbiome studies with\nLLMS", "content": "Due largely to the rapid development and growth of metagenomics research in the last two decades, it\nis well established that the human microbiome is associated with overall human-host health. Many of\nthe findings that link the gut microbiome to complex diseases, such as IBD and Crohn's Disease, can\nbe found within individual scientific publications. Manual aggregation of these results, available in the\npublic domain, into an organized and searchable repository would be time-prohibitive and limited to\nonly a small subject of microbes and diseases [Badal et al., 2019]. Such knowledge bases can be used\nfor downstream analysis and discovery. NLP and test mining approaches can be used to automate this\nprocess.\nAutomated extraction of microbiome-disease associations from scientific text requires three steps.\nFirst is to identify the disease and microbe(s) mentioned in the text. This is known as entity extraction,\nwhere the entity is either the disease or microbe. Well-established algorithms such as Named entity\nRecognizers (NERs) and linguistic taggers can be used for this process. The second step is relationship\nextraction which aims to establish the existence of a relationship between a pair of entities (i.e.,\nmicrobe-disease pair). The final step is to refine the categorization of identified relationships into\npositive or negative associations. Several statistical models have been developed for relationship\nextraction. While each step requires the use of NLP algorithms, the integration of deep learning and\nLMMs into steps two and three are of recent particular interest.\nAn early example of using deep learning in relationship extraction comes from Wu et al. [2021].\nIn this work, the authors apply a pretrained BERE model to identify microbe-disease associations.\nBERE is a deep learning model initially developed for extracting drug-related associations [Hong et al.,\n2020]. The model is pretrained using a biomedical corpus. The model converts the text into vector\nrepresentation using word embeddings with sentences represented as 200-dimensional concatenations.\nThen the recurrent neural network encodes short- and long-range dependencies, as well as semantic\nfeatures using gated recurrent units (GRUs). Finally, a classifier performs prediction. The prediction\ntask has four possible labels: positive in which the microbe's presence will increase when disease\noccurs, negative in which the microbe's presence will decrease when the disease occurs, relate when\nthe microbe-disease pair occurs together but the relationship cannot be determined, and NA when\nthere is no relationship description in the text. The model requires a large amount of training data.\nAlthough, the gold standard of manual curation is difficult and costly. The authors implement a\ntransfer learning silver standard corpus, learned with automated tools but potentially with error, first\nand then fine tune with the gold standard manually curated corpus. This transfer learning approach\nresults in a reduction in the error rate.\nDeep learning models like the one just described have been recently refined to use LLMs like\nGPT-3 and BERT [Karkera et al., 2023]. The principal advantage of using LLMs in this setting is\nthat they reduce the requirement for large amounts of training data, given that they are already\npretrained with large amounts of text. the setting where no fine-tuning or training data is used is\nknown as zero-shot learning. Karkera et al. [2023] uses the same positive, negative, relate, and NA\nlabels as Wu et al. [2021] with their LLMs and find that zero- and few-shot learners do not perform\nvery well, particularly with the NA label. Thus indicating that out-of-the-box implementation of LLMs\nfor identifying microbe-disease associations is limited. The performance of generative (e.g., GPT-3)\nand discriminative (e.g., BERT) models improve with fine-tuning. The amount of improvement is\nstrongly dependent on the quality of training data."}, {"title": "7 Discussion", "content": "The recent development of deep learning methods, and large language models in particular, has led to\nmany novel applications that address significant challenges in microbiome and metagenomic research.\nIn this paper, we have reviewed the latest applications of these methods in microbial function analysis,\nincluding the identification of biosynthetic gene clusters in bacterial genomes, annotation of virome\ngenomes, and prediction of virus-bacteria interactions. We have also explored the use of generic LLMS,\nsuch as ChatGPT, for extracting microbe-disease associations from public knowledge.\nWhile significant strides have been made in analyzing microbiomes using metagenomic data,\nintegrating multi-omics datasets (e.g., transcriptomics, proteomics, metabolomics) remains a crucial\narea for future research. Deep learning models and large language models capable of seamlessly\nintegrating these diverse data types could provide a more holistic understanding of microbial functions\nand interactions, leading to more accurate predictions and novel discoveries in microbial ecology.\nTo further advance this promising research area, it is essential to focus on both the collection\nand annotation of datasets from multiple sources and the development of new deep-learning archi-\ntectures specifically tailored for microbiome and metagenomic analysis. The integration of diverse\ndatasets ranging from genomic sequences to environmental metadata will provide a more com-\nprehensive understanding of microbial communities and their interactions. However, this requires\nmeticulous data curation, standardization, and the creation of large, well-annotated datasets that can\nserve as benchmarks for training and evaluating deep learning models.\nOn the architectural front, there is a need to design models that can handle the unique challenges\nposed by microbiome and metagenomic data, such as high dimensionality, sparsity, and complex\nrelationships between microbial species. Innovations in model architectures, such as graph neural\nnetworks, attention mechanisms, and hierarchical models, could play a crucial role in capturing the\nintricate dependencies within the data. Moreover, these models should be adaptable to the evolving\nnature of the datasets, allowing for continuous learning and refinement as new data becomes available."}]}