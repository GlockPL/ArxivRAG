{"title": "Patterns Over Principles: The Fragility of Inductive Reasoning in LLMS\nunder Noisy Observations", "authors": ["Chunyang Li", "Weiqi Wang", "Tianshi Zheng", "Yangqiu Song"], "abstract": "Inductive reasoning, a cornerstone of human\ncognition, enables generalization from lim-\nited data but hasn't yet been fully achieved\nby large language models (LLMs). While\nmodern LLMs excel at reasoning tasks, their\nability to maintain stable and consistent rule\nabstraction under imperfect observations re-\nmains underexplored. To fill this gap, in\nthis work, we introduce Robust Rule Induc-\ntion, a task that evaluates LLMs' capability\nin inferring rules from data that are fused\nwith noisy examples. To address this task,\nwe further propose Sample-steered Rule Re-\nfinement (SRR), a method enhancing reason-\ning stability via observation diversification\nand execution-guided feedback. Experiments\nacross arithmetic, cryptography, and list func-\ntions reveal: (1) SRR outperforms other meth-\nods with minimal performance degradation un-\nder noise; (2) Despite slight accuracy vari-\nation, LLMs exhibit instability under noise\n(e.g., 0% accuracy change with only 70% con-\nsistent score); (3) Counterfactual task gaps\nhighlight LLMs' reliance on memorized pat-\nterns over genuine abstraction. Our findings\nchallenge LLMs' reasoning robustness, re-\nvealing susceptibility to hypothesis drift and\npattern overfitting, while providing empiri-\ncal evidence critical for developing human-\nlike inductive systems. Code and data are\navailable at https://github.com/lcy2723/Robust-\nRule-Induction.", "sections": [{"title": "1 Introduction", "content": "Inductive reasoning-the cognitive capacity to gen-\neralize from specific instances to universal princi-\nples, is fundamental to human intelligence (Lake\net al., 2017). This ability enables humans to\nabstract latent patterns from sparse data while\nmaintaining robustness against conflicting evi-\ndence (Feldman, 1997), as exemplified by children\nmastering linguistic rules despite exposure to oc-\ncasional grammatical errors. The robustness of in-"}, {"title": "2 Preliminary", "content": ""}, {"title": "2.1 Problem Definitions", "content": "We formulate robust rule induction as the problem\nof identifying latent mapping functions from input-\noutput pairs while maintaining consistent reasoning\nperformance under noisy conditions. While prior\nworks (Qiu et al., 2024; Wang et al., 2024) focus\non basic rule discovery capabilities, we specifically\ninvestigate the robustness of inductive reasoning\nwhen the observations contain conflicting patterns.\nThe obser-\nvations may contain both normal and noisy exam-\nples, and the model is required to infer the correct\nrule regardless of the noise."}, {"title": "2.2 Evaluation Pipeline", "content": "As illustrated in figure 2, to evaluate the robustness\nof inductive reasoning in large language models,\nwe propose a novel evaluation pipeline consisting\nof three main stages: data synthesis, model infer-\nence, and performance evaluation. In the data syn-\nthesis stage, due to the characteristics of our data,\nnormal examples, noisy examples, and test exam-\nples for each instance in the dataset can be auto-\nmatically generated by programs given the rule and\nnoise definitions. The first two are mixed to form\nseen examples, while the latter is used to evaluate\nthe model's reasoning ability. In the model infer-\nence stage, we prompt the language model with\nseen examples in standard input-output (IO) for-\nmat, and the induced rules are restricted to Python\nfunction form for automatic evaluation. Finally,\nwe evaluate the rule by executing it on the test ex-\namples. The instance is considered solved if all\nthe test examples are correctly solved. More de-\ntails about the evaluation pipeline, like the data\nsynthesis process, can be found in appendix A.\nWe use task accuracy, which is the proportion of\nsolved instances over the total number of instances,\nand its change under different conditions as the\nevaluation metrics. While task accuracy is a com-\nprehensive metric to evaluate the model's reasoning\nability, it may not fully capture the robustness of\nthe model at the instance level. To address this, we\nintroduce the consistency score, which is defined\nas follows:\nConsistency Score :=$\\frac{1}{N} \\sum_{i=1}^{N} I[Sol_i^{N} = Sol_i^{N'}] $"}, {"title": "3 Sample-steered Rule Refinement", "content": "To address noisy rule induction, we propose\nSample-steered Rule Refinement (SRR), a novel\nframework that combines hypothesis generation,\ncontrastive evidence sampling, and iterative self-\ncorrection. This method conducts inductive rea-\nsoning through three-phase optimization: (1) con-\ntrastive hypothesis generation to bootstrap diverse\nrule candidates, (2) diagnostic feedback construc-\ntion through evidence-aware sampling, and (3) it-\nerative rule refinement using failure-driven correc-\ntions. The full algorithm is detailed in Algorithm 1.\nDiversity-aware Hypothesis Generation We\nfirst generate K hypotheses from random subsets\nof the seen examples plus one hypothesis using\nthe full observations. This step ensures coverage\nof both local patterns and global consistency. The\nnumber of examples in the subset is less than the\nfull seen set, thus the noise in the subset has a\ngreater impact. The sampled subset may also con-\ntain completely noise-free examples, which can\nbetter distinguish the rules."}, {"title": "4 Experiments", "content": "In this section, we evaluate the robustness of in-\nductive reasoning in language models on different\ntasks. We also compare the performance of the\nmodels under different noise levels and analyze the\neffectiveness of the different methods in enhancing\nthe robustness of inductive reasoning."}, {"title": "4.1 Experimental Setup", "content": "Consistent with previous studies (Wang et al., 2024;\nQiu et al., 2024), we adopt few-shot prompting to\nassess the models' inductive capabilities. Each\ninstance contains 10 normal examples, 5 noisy ex-\namples, and 10 test examples. The normal and\nnoisy examples are mixed to form 10 seen exam-\nples, which serve as prompts. This approach pre-\nserves task semantics while introducing controlled\nperturbations, thereby simulating real-world scenar-\nios where observational data often contains inher-\nent imperfections. LLMs are explicitly informed\nthat examples may contain some noise. We formu-\nlate the output of LLMs as Python functions and\nexecute them on the test examples to automatically\nevaluate the inferred rules. More details about the\nexperiments can be found in the appendix \u0412."}, {"title": "4.2 Datasets", "content": "We use three datasets with their subsets: Arith-\nmetic, Cryptography, and List Functions. These\ndatasets offer different rule induction challenges,\nincluding mathematical calculations, symbol rep-\nresentation, and list operations. Comprehensive\nstatistics are shown in Table 1.\nArithmetic The arithmetic task, proposed by\n(Wu et al., 2024), is a counterfactual two-digit ad-\ndition task. Instead of the common base-10 system,\nthe task uses base-8, 9, and 11 as the counterfactual\nsetup to control the difficulty and avoid the impact\nof memorization over reasoning. In this task, we\nfocus on the base-7, 8, 9 systems, and the noisy\nexample is the common 10-based equations. The\ninput of each example is two two-digit numbers in"}, {"title": "4.3 Robustness Under Different Noise Levels", "content": "We first evaluate the robustness of inductive rea-\nsoning in language models under different noise\ninjection ratios. Noise levels are defined as the\nproportion of noisy examples in the seen exam-\nples. We test three representative models: GPT-40-\nmini (OpenAI, 2024a), GPT-40 (OpenAI, 2024b),"}, {"title": "4.4 Method-wise Effectiveness Comparison", "content": "Direct Output may limit the model's reasoning\nability, as the model must output the rule directly\nwithout any intermediate steps. To systemati-\ncally assess the robustness of different reasoning\nparadigms, we compare our Sample-steered Rule\nRefinement (SRR) method with three reasoning\nmethods: (1) Chain of Thought (CoT; Wei et al.\n2022), which decomposes reasoning into step-by-\nstep rationales; (2) Self-Consistency (SC; Wang\net al. 2023), which aggregates multi CoT trajecto-\nries through majority voting; and (3) Self-Refine\n(SR; Madaan et al. 2023), which iteratively im-\nproves hypotheses using self-generated feedback.\nTable 3 presents the task accuracy under 10% noise\nand the deviation from clean data for GPT-40 and\nDeepSeek-V3.\nSuperior Performance of SRR As shown in\nTable 3, our SRR framework achieves state-of-\nthe-art performance across 13/14 task-model com-\nbinations while exhibiting minimal performance\ndegradation (2.1% average drop vs. 3.6%-8.5%\nfor baselines). This dual advantage stems from two\nmechanisms: (1) Diversity-aware hypothesis gen-\neration explores broader solution spaces through\nsubset sampling, outperforming SC's majority vot-"}, {"title": "4.5 Extended Explorations", "content": "To further investigate the robustness of LLMs' in-\nductive reasoning, we conduct a comparative eval-\nuation against DeepSeek-R1 (DeepSeek-AI et al.,\n2025), one of the state-of-the-art reasoning models,"}, {"title": "5 Discussion", "content": "Our experimental results reveal limitations in\nLLMs' inductive reasoning. We try to analyze the\nresults in this section.\nProcess Analysis Inductive reasoning can be con-\nceptualized through a Bayesian paradigm (Tenen-\nbaum et al., 2011), where models update poste-\nrior distributions over a hypothesis space based\non observations. The introduction of noise dis-\nrupts the reasoning through dual mechanisms:\nNoise-Induced Hypothesis Drift, where the ini-\ntial hypothesis space becomes misaligned with true\nrules when noise introduces conflicting patterns.\nThis drift particularly impacts Direct Output (DO)\nmethod, which lacks intermediate reasoning steps\nand relies heavily on the initial hypothesis space.\nThis is more evident for tasks with similar internal\npatterns, the model's performance fluctuates signif-"}, {"title": "6 Related Work", "content": "Inductive Reasoning The study of inductive rea-\nsoning has been a long-standing focus across multi-\nple disciplines. Early work (Heit, 2000) established\nfoundational properties of inductive reasoning. In\ncognitive science, induction is considered a process\nof probabilistic belief updating within the Bayesian\nparadigm (Tenenbaum et al., 2011). Human cog-\nnition and learning are explained through the inte-\ngration of prior knowledge with observed data to\ncompute posterior distributions. Comparative stud-\nies (Lake et al., 2015, 2017) have highlighted the\ncontrast between human learners and machine in-\ntelligence. With the advent of pre-trained language\nmodels (Brown et al., 2020), research on inductive\nreasoning has shifted from domain-specific and\nneural formulation (Tian et al., 2020; Odena et al.,\n2021; Sabl\u00e9-Meyer et al., 2022) to natural language.\nInitial approaches (Alet et al., 2021; Gendron et al.,\n2024; Mirchandani et al., 2023; Yang et al., 2024)\npredominantly relied on input-output (IO) prompt-\ning, which evaluates model performance on un-\nseen examples without explicit rule articulation."}, {"title": "7 Conclusion", "content": "In this paper, we explore the robustness of inductive\nreasoning in large language models under imper-\nfect observations. Through introducing the Robust\nRule Induction task with metrics of both holistic\nand individual levels, we systematically evaluate\nthe ability of LLMs to maintain stable and con-\nsistent rule abstraction. The Sample-steered Rule\nRefinement outperforms other reasoning paradigms\nby effectively leveraging diversity-aware hypothe-\nsis generation and execution-guided feedback. Our\nfindings reveal that while LLMs can demonstrate\nimpressive reasoning capabilities, they are inher-\nently sensitive to noise and prone to hypothesis\ndrifting and pattern overfitting."}, {"title": "Limitations", "content": "We discuss the limitations of our work here: Our\nevaluation focuses on highly formalized and sym-\nbolic tasks. Real-world inductive reasoning often\ninvolves ambiguous rules like social norms from\ntext or visual patterns, which are not captured by\nour formalism. The tasks in Arithmetic and Cryp-\ntography may be relatively simple and lack diver-\nsity. Expanding task diversity could reveal deeper\nlimitations in LLMs' capabilities. A large-scale\nassessment of human performance across all tasks\nunder varying levels of noise is beyond the scope of\nthis study, which poses certain limitations in com-\nparing human reasoning patterns. The purpose of\nour study is to explore the robustness of LLMs' in-\nductive reasoning capabilities, so we do not specif-\nically tune the hyperparameters and prompt tem-\nplates."}, {"title": "Ethics Statement", "content": "The datasets we used are all publicly available,\nand our research does not involve any personal\ninformation. All data is generated by programs.\nTherefore, we anticipate that this paper does not\nraise any ethical concerns. We use ChatGPT to\nparaphrase some sentences."}, {"title": "Appendices", "content": null}, {"title": "A Details on Evaluation Pipeline", "content": "In this section, we provide detailed information on\nthe evaluation pipeline, including the data construc-\ntion and the performance assessment. For Arith-\nmetic, we generate the base-7, base-8, and base-9\ntasks by randomly sampling two two-digit num-\nbers in the corresponding base, and then we check\nwhether there is a carry-over in the addition process.\nIf there is no carry-over, we regenerate the numbers.\nThe noisy examples are generated in base-10. For\nCryptography, we randomly select words of appro-\npriate lengths from the NLTK Word Lists corpus\u00b9,\nand then we encrypt the words using the Caesar,\nAtbash, and Keyboard ciphers. The noisy examples\nare generated by randomly replacing the letters in\nthe output with other letters. For List Functions, we\nfirst write the corresponding rule functions for each\ntask, and then we automatically generate the input\ndata with appropriate lengths and ranges. The in-\nputs are generated by randomly sampling numbers\nfrom a specific range with some constraints. . Dur-\ning the data synthesis process, we attach manual\nsupervision to ensure that the generated data can\ncorrectly induce the rules. During the evaluation\nprocess, we use the inputs in the test set as the input\nfor rule execution. We evaluate the model's perfor-\nmance using exact match. If the model's output is\ncorrect on all the test set examples, we consider the\nmodel to have successfully induced the rule. If the\nmodel fails to output a valid programmatic rule or\nthe program contains an infinite loop or errors, we\nconsider it a failure."}, {"title": "B Experimental Details", "content": null}, {"title": "B.1 Experimental Settings", "content": "For robustness under different noise levels, we run\neach experiment three times and report the mean\nand standard deviation of the results to avoid ran-\ndomness. Except for the self-consistency (SC) and\nsample-steered rule refinement (SRR) that require\ndiverse generations, we set the temperature to 0.0\nfor all models to ensure reproducibility. For SC\nand SRR, the temperature is set to 0.7, consistent\nwith the original work of SC (Wang et al., 2023).\nThe positions of noise in seen examples are ran-\ndom to avoid positional bias (Lu et al., 2022). In\nthe implementation, we choose 2 subsets for SRR\nby splitting the seen examples into two parts. The"}, {"title": "B.2 Prompts and Failure Cases", "content": "For the Direct Output setting, we restrict the model\nto output the rule directly without any additional\noutput, as shown in Table 7. For the chain-of-\nthought and self-consistency setting, we use the\ninstruction in Kojima et al. (2022). For the self-\nrefine and sample-steered rule refinement setting,\nwe use the chain-of-thought prompt as the initial\nprompt; the iterative prompts are shown in Table 9\nand Table 10, respectively.\nWe provide some representative failure cases in\nthe evaluation. For the Arithmetic task, the model\nfails to solve the base-7 and base-9 tasks and mis-\ninterprets the rule as the base-8 addition or the\ndecimal sum with a constant. For the Cryptography\ntask, the model fails to solve the Atbash and tries\nto explain it as shifts. The responses of the models\nare shown in Table 11."}]}