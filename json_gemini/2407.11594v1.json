{"title": "DiNO-Diffusion: Scaling Medical Diffusion via Self-Supervised Pre-Training", "authors": ["Guillermo Jimenez-Perez", "Pedro Osorio", "Josef Cersovsky", "Javier Montalt-Tordera", "Jens Hooge", "Steffen Vogler", "Sadegh Mohammadi"], "abstract": "Diffusion models (DMs) have emerged as powerful foundation models for a variety of tasks, with a large focus in synthetic image generation. However, their requirement of large annotated datasets for training limits their applicability in medical imaging, where datasets are typically smaller and sparsely annotated. We introduce DiNO-Diffusion, a self-supervised method for training latent diffusion models (LDMs) that conditions the generation process on image embeddings extracted from DiNO. By eliminating the reliance on annotations, our training leverages over 868k unlabelled images from public chest X-Ray (CXR) datasets. Despite being self-supervised, DiNO-Diffusion shows comprehensive manifold coverage, with FID scores as low as 4.7, and emerging properties when evaluated in downstream tasks. It can be used to generate semantically-diverse synthetic datasets even from small data pools, demonstrating up to 20% AUC increase in classification performance when used for data augmentation. Images were generated with different sampling strategies over the DiNO embedding manifold and using real images as a starting point. Results suggest, DiNO-Diffusion could facilitate the creation of large datasets for flexible training of downstream AI models from limited amount of real data, while also holding potential for privacy preservation. Additionally, DiNO-Diffusion demonstrates zero-shot segmentation performance of up to 84.4% Dice score when evaluating lung lobe segmentation. This evidences good CXR image-anatomy alignment, akin to segmenting using textual descriptors on vanilla DMs. Finally, DiNO-Diffusion can be easily adapted to other medical imaging modalities or state-of-the-art diffusion models, opening the door for large-scale, multi-domain image generation pipelines for medical imaging.", "sections": [{"title": "1 INTRODUCTION", "content": "Diffusion models (DMs) have recently emerged as robust and proficient foundational models in medical imaging, exhibiting substantial capabilities in image generation, image enhancement, reconstruction, and segmentation [1]. The field of synthetic image generation in particular has greatly shifted to text-to-image DMs, generating images that are nearly indistinguishable from real ones [2\u20136] and facilitating remarkable zero-shot performance in segmentation and classification tasks [7, 8]. However, DMs depend on the availability of large datasets containing images paired with corresponding descriptors (usually text) to guide the generation process, a requirement that presents a considerable obstacle in the medical domain [9]. Medical imaging datasets are typically small, contain free-form and inconsistent annotations including captions, binary labels or segmentations, and are generally prohibitively costly to compile and curate [9].\nTo address these challenges, some works have proposed pseudo-labeling with automatic captioners and other GPT-like vision-language models (VLMs) [10] or have trained lean mapping networks over frozen pretrained backbones to reduce the number of required annotated samples [11, 12]. However, despite their promise, pseudo-labelling approaches find limited applicability in the medical field given a lack of high-quality medical imaging captioners [9]. In addition, while some authors have successfully trained mapping networks to bridge the gap between unimodal foundation models, they still require relatively large annotated datasets to be trained [9].\nThese limitations are the main roadblocks for medical DMs. While the natural imaging literature focuses on saturating generation quality by improving the base architecture, optimization process or condition alignment [10, 13, 14], the medical imaging community navigates these hurdles by leveraging smaller or custom-annotated datasets [2\u20136]. Moreover, although mapping networks have found their footing in the diffusion literature with approaches such as ControlNet [12], these would still rely on large-scale medical DMs trained with prohibitively extensive amounts of annotated images. In this context, applying a self-supervised approach to DM training would be highly beneficial for medical image synthesis. Self-supervision enables models to learn from unlabelled data, providing exceptional results in multiple downstream tasks when used as image embedders [15\u201319].\nWith that in mind, we introduce DiNO-Diffusion, a novel self-supervised methodology for training medical DMs at scale which conditions the image generation process on image-derived tokens extracted from a frozen DiNO model [15, 16], as opposed to textual descriptors. DiNO-Diffusion allows independence from existing annotations, circumventing the limitations imposed by the scarcity and inconsistency of medical image labels. Moreover, it is agnostic to the choice of DM architecture, medical imaging modality or optimization strategy. To test this, a model was trained on a large corpus of open-source CXR data found in the literature [20\u201343], which do not contain any common descriptor to train a regular DM (e.g., text captions). The trained models achieved low FID scores and high image quality."}, {"title": "2 METHODS", "content": "This Section explains the methodology employed for studying the self-supervised DM. In Section 2.1, the datasets used for training and evaluation are described. In Section 2.2, the model\u2019s architecture and theoretical background is outlined. In Section 2.3, the designed mechanisms for self-supervised conditioning are detailed. In Section 2.4, the evaluation protocol employed to benchmark model performance is defined. Finally, in Section 2.5, the specific parameters used for model training and evaluation are enumerated. Figure 1 visually describes the training and evaluation pipeline.\nTo explore DiNO-Diffusion\u2019s self-supervision capability, a large-scale dataset comprised of every openly accessible CXR dataset found in the literature [20\u201343]\u00b9 was collected, reaching over 1.2M total images from 21 distinct data providers. Three different subsets were taken from this compound dataset for different purposes. Firstly, a subset comprising every dataset\n\u00b9Thanks, among others, to the National Library of Medicine, National Institutes of Health, Bethesda, MD, USA.\nLatent Diffusion Models (LDMs) approach image generation as an iterative denoising process, transforming pure noise xT"}, {"title": "2.1 Data", "content": "To explore DiNO-Diffusion's self-supervision capability, a large-scale dataset comprised of every openly accessible CXR dataset found in the literature [20\u201343]\u00b9 was collected, reaching over 1.2M total images from 21 distinct data providers. Three different subsets were taken from this compound dataset for different purposes. Firstly, a subset comprising every dataset\n\u00b9Thanks, among others, to the National Library of Medicine, National Institutes of Health, Bethesda, MD, USA.\nLatent Diffusion Models (LDMs) approach image generation as an iterative denoising process, transforming pure noise xT"}, {"title": "2.2 Generative Architecture - Stable Diffusion", "content": "Latent Diffusion Models (LDMs) approach image generation as an iterative denoising process, transforming pure noise xT into a defined image xo over T steps with a parameterized DM $\\epsilon_{\\theta}(z_t, t, c)$, where c represents an optional condition. LDMs address the prohibitive computational demands of traditional DMs by reducing the dimensionality of the input. LDMs currently find active development with ongoing research in different parameterised models, optimization strategies and dimensionality reduction pipelines.\nThis study adopts the Stable Diffusion (SD) framework (version 1) [45] as its baseline. Despite being outperformed by more recent models and its output size limitation of 512x512 pixels, SD\u2019s lightweight architecture, open-source nature, and community adoption makes it ideal for our proof of concept. SD comprises a frozen variational autoencoder (VAE) and a trainable conditional denoising UNet.\nThe VAE consists of an encoder (E) and a decoder (D). The encoder compresses fixed-size images $x \\in \\mathbb{R}^{H \\times W \\times 3}$ into a latent $z = E(x) \\in \\mathbb{R}^{(H/d) \\times (W/d) \\times k}$, where k = 4 is number of channels extracted by the VAE and d = 8 is the downsampling factor. The decoder maps latents back to the original image space $x = D(z)$. Stable Diffusion\u2019s VAE has been shown to generalize to medical data [3, 46]. The UNet serves as the diffusion component and uses a ResNet architecture as its convolutional backbone, where the condition c is incorporated through attention mechanisms (see Section 2.3).\nWith this model, training with conditional information involves two phases: the forward and reverse diffusion processes. During the forward diffusion, an image xo (or its latent representation zo) and condition c are chosen. A timestep t is randomly selected ($t \\sim U(1, ..., T)$) so a noisy latent $z_t$ is generated by mixing zo with noise $\\epsilon \\sim N(0, 1)$, resulting in a partially noised latent. The reverse process uses the UNet to estimate the original noise \\epsilon from $z_t$, t and c.\nThe network is optimized using the Mean Squared Error (MSE) loss between the predicted and actual noise to adjust the weights of the UNet:\n$\\mathbb{L}_{LDM} = \\mathbb{E}_{x \\sim \\epsilon(x), c, \\epsilon \\sim N(0, 1), t} [||\\epsilon - \\epsilon_{\\theta}(z_t, t, c)||_2]$                                               (1)\nAfter training, image synthesis begins with sampling a noisy latent $z_T \\sim N(0, 1)$, progressively denoising it with condition c to obtain zo so that $z_0 = \\epsilon(z_{T:0}, c)$, and by using the VAE\u2019s decoder, so that $x = D(z_0) = D(E(z_{T:0}, c))$."}, {"title": "2.3 Self-Supervised Conditioning", "content": "LDMs condition image generation using a semantic tensor c to guide the diffusion process. This tensor is usually obtained from a frozen transformer model $f_{\\theta}$ that maps the label information into a tensor $c = f_{\\theta}(x) \\in \\mathbb{R}^{S \\times N}$, where S is the token length (of variable size), N is the embedding dimension and x represents whichever input the embedder model requires (text, image, etc.). Although the current diffusion literature has mainly focused on using textual descriptors as their main conditioning strategy, other conditioning mechanisms have been employed [5, 6, 12].\nIn this work we explore conditioning using image-derived semantic descriptors. Specifically, a vision transformer trained with the DiNO method [15, 47] was used to produce a semantic description of the image to be generated. Vision transformers split an image into small patches (usually P = 14px\u00b2 or"}, {"title": "2.3.1 Reconstruction-based image generation", "content": "the \u201creconstruction\u201d strategy consists in synthesizing images $x = D(\\epsilon(z_{T:0}, c))$ from the global information of an existing real example (x, y), where y is the image\u2019s label, $\\hat{y} = y$ and $f_{\\theta}(x)$ is the conditioning embedding as produced by DiNO. This reconstruction leverages DiNO-Diffusion\u2019s large-scale pretraining to produce semantic variations over the source image x. Exact replicas of x are prevented by design due to conditioning with the compressed information from DiNO\u2019s global embedding, causing a bottleneck. Figure 1 (b-i) depicts the reconstruction process."}, {"title": "2.3.2 Interpolation-based image generation", "content": "the \u201cinterpolation\u201d strategy uses the same image generation mechanism from above. The difference lies in the sampling method of the conditioning embedding c, which is interpolated from two images $(x_1, y_1), (x_2, y_2)$ so that $\\hat{c} = lerp(f_{\\theta}(x_1), f_{\\theta}(x_2), r)$, where $r \\in [0, 1]$ is the interpolation fraction. This strategy attempts to generate synthetic images from less sampled regions of the real data manifold, located between existing samples, following approaches such as MixUp [49]. See Figure 1 (b-ii) for a visual depiction of this strategy."}, {"title": "2.4 Evaluation", "content": "This section details three different evaluation protocols used for benchmarking DiNO-Diffusion. For every evaluation protocol, two variants are evaluated and compared: DiNOv1-Diffusion [15] and DiNOv2-Diffusion [16], depending on the choice of image encoder."}, {"title": "2.4.1 Image Quality & Checkpoint Selection", "content": "Fr\u00e9chet Inception Distance (FID) [50] was used to quantify generation quality at multiple checkpoints for both variants of DiNO-Diffusion. The FID scores computed for the data generated via the \u201creconstruction\u201d strategy (see Section 2.3) were used as a proxy for overall model performance. Similarly to [3], FID scores were computed over a 5k subset of MIMIC-CXR\u2019s"}, {"title": "2.4.2 Data Augmentation", "content": "this experiment explored DiNO-Diffusion\u2019s ability to enhance the sample size of a dataset by training a classification model on real and synthetic data using five-fold cross-validation and testing on a held-out test set (MIMIC\u2019s p19). For this purpose, MIMIC\u2019s training dataset (p10-p18) was subset into different data regimes with decreasing sample size $X_n$, with $n \\in$ {10k, 5k, 1k, 500, 100, 50} samples in the subset. Given that MIMIC has multi-label annotations, label balancing was performed by randomly selecting n/card(L) elements of each label in the labelset L from X without replacement, ensuring sufficient representativity of all labels within the training set. Smaller subsets were also enforced to be contained into bigger ones, so that $X_{N_{i+1}} \\in X_{N_i}$. With $X_n$ defined, synthetic data was created to increase sample size by generating partially-synthetic datasets $X_n$, with real-to-synthetic ratios of 1:1, 1:5, 1:10 and 1:50 for the reconstruction- and interpolation-based synthesis (see Section 2.3).\nFor the reconstruction experiments, ratios larger than 1:1 represent several semantic variations of a single source image (x, y), with the intent to introduce realistic variance into the synthetic data while retaining the label-specific image features. The interpolation experiment addressed whether intermediate embeddings could still be decoded into an image that retains label-specific features from both elements in the pair. For this pur-"}, {"title": "2.4.3 Full Synthetic Training", "content": "this experiment explores whether test-set AUC drops when training a classifier solely on synthetic data, to address whether DINO-Diffusion can serve as a privacy-preserving synthetic replacement for real data. The generation strategies, data regimes, real-to-synthetic ratios and 5-fold cross-validation settings from Section 2.4.2 were followed as evaluation strategy."}, {"title": "2.4.4 Zero-Shot Segmentation", "content": "this experiment investigates the model\u2019s ability to learn semantic coherence by generating segmentation masks from the internal representations generated during the DiNO-Diffusion\u2019s UNet forward pass. For this purpose, the zero-shot segmentation approach from DiffSeg [7] was followed, consisting of leveraging the self-attention weights from each transformer block of the UNet and iteratively merging them based on their Kullback-Leibler divergence. This methodology was applied both to DiNO-Diffusion and a vanilla SD model to generate lung lobe segmentation masks without further training. Using a combined dataset of 1,048 cases with ground truth annotations (See Section 2.1), candidate masks were evaluated by their Dice score and selected via non-maximum suppression. The respective hyperparameters (merging threshold, timestep, number of anchor points) as well as the best performing checkpoint were selected per model using grid-search. Refer to Figure 1 (b-iii) for a visual depiction of the segmentation pipeline."}, {"title": "2.5 Experimental Setup", "content": "The models were trained by adapting HuggingFace Diffusers\u2019 script for training DMs [52]. The DMs were trained for 100 epochs (~140000 steps) using 4 H100 GPUs per model, an aggregated batch size of 512 (bs = 64, gradient accumulation of 2 steps), 8-bit Adam optimizer with constant lr = 10\u22124 and 1000-step warmup and xformers\u2019 memory-efficient attention [53].\nThe specific versions of the DiNOv1 and DiNOv2 image encoder architectures used were \u201cfacebook/dino-vitb16\u201d [15]"}, {"title": "3 RESULTS", "content": "The FID scores were calculated every 2500 steps over a subset of MIMIC\u2019s p19 dataset following [3, 43]. Both the DiNOv1 and DiNOv2 models converged relatively late, reaching scores of 4.7 and 6.4 at 80k and 120k steps, respectively. The full FID scores for every checkpoint can be observed in Figure 3. DiNOv1-Diffusion leads to lower FID scores when compared to DiNOv2-Diffusion. This is also evident by a slightly less saturated synthetic images generated with DiNOv2-Diffusion when compared to the source real images (see Figure 2)."}, {"title": "3.1 Image Quality & Checkpoint Selection", "content": "The FID scores were calculated every 2500 steps over a subset of MIMIC\u2019s p19 dataset following [3, 43]. Both the DiNOv1 and DiNOv2 models converged relatively late, reaching scores of 4.7 and 6.4 at 80k and 120k steps, respectively. The full FID scores for every checkpoint can be observed in Figure 3. DiNOv1-Diffusion leads to lower FID scores when compared to DiNOv2-Diffusion. This is also evident by a slightly less saturated synthetic images generated with DiNOv2-Diffusion when compared to the source real images (see Figure 2)."}, {"title": "3.2 Data Augmentation", "content": "In this experiment, real and synthetic data were used in different proportions to train DenseNet-121 classification models. Table 1-a and Figure 4-a provide the results of the cross-validation trainings. The 'reconstruction' workstream (see Section 2.3.1) depicts consistent improvements when used for data augmentation in all data regimes, with AUC increases up to approximately 20% in small-data regimes. In some larger-data regimes (N \u2208 [1000, 5000]), the addition of large amounts of synthetic data slightly degraded performance, although never by a significant margin (p > 0.05). The 'interpolation' workstream (see Section 2.3.2) also depicts improvements in smaller data regimes as compared to not using synthetic data, although it leads to a significant performance degradation in large-data regimes (p < 0.05). Also, DiNO-Diffusion using DiNOv1 yields larger performance improvements compared to when using DiNOv2. This is always true for both image synthesis strategies, except for the interpolation results on data regime"}, {"title": "3.3 Full Synthetic Training", "content": "The test set results of the full synthetic trainings are shown in Table 1-b and Figure 4-b. The data synthesised via the"}, {"title": "3.4 Zero-Shot Segmentation", "content": "The performance of the zero-shot experiments are shown in Table 2. Both DiNOv1- and DiNOv2-Diffusion showed improvements of up to 10% Dice score when compared to a vanilla SD v1.5 model while also presenting lower variance. When addressing individual results, DiNOv1-Diffusion generated the best average Dice scores. Performance varied between datasets, with the Montgomery [25] producing the lowest Dice scores for both vanilla Stable Diffusion and DiNOv1-Diffusion, but to the highest scores for the DiNOv2-based approach when comparing the overall best model. It should be noted that the best model checkpoint for segmentation was significantly earlier than the one found in Section 3.1. Moreover, the optimal pa-"}, {"title": "4 DISCUSSION", "content": "DMs are a cornerstone in modern foundation models, revolutionizing many tasks in Computer Vision. Their ability to generate high-quality images has caused a large scientific, economic and societal disruption, whose long-term repercussions are difficult to foresee [14]. However, despite their scientific and industrial utility, applying this technology in medical imaging is severely limited by key challenges such as a lack of large-scale labeled datasets including high-quality textual or non-textual descriptions [1]. Although this limitation might be temporary due to current trends in AI data acquisition and improved dataset interoperability [55], it is not clear whether the prevalent text-to-image generative recipe [45] is optimal for medical applications.\nSome approaches employing DMs in medical data exist. Chambon et al. [3] trained an SD architecture on the MIMIC-CXR dataset [43] with good synthesis fidelity, reporting low FID scores and high accuracy scores on several downstream tasks including classification, report generation and image retrieval. However, their approach is severely limited on the size of the development dataset (300k images) and the low quality of accompanying captions. In histopathology, multiple authors have proposed applying DMs for image generation [2, 4, 5, 58]. For instance, Aversa et al. relied on a custom-annotated dataset of large histopathology slides with segmentation masks representing different tissue subtypes within the slide and employed timestep unravelling to generate images larger than the typical 512px\u00b2. However, their approach heavily relied on a closed-source, custom-annotated dataset, and timestep unravelling might be impractical in other medical imaging modalities. In contrast, Xu et al. [58] take a similar approach as the one proposed here, and train a DM conditioned only on an image encoder\u2019s CCLS for histopathology image synthesis. However, their method was partially supervised, as it relied on training additional label-specific DMs for CCLS generation. Besides being compute intensive, their method fails to leverage the emerging data augmentation and segmentation capabilities that a self-supervision DM training conveys. Finally, Pinaya et al. [6] trained an LDM on a large dataset of 31740 3D Brain MRI images from UK BioBank. However, despite the scale of this dataset, the fragmentation of clinical labels forced the authors"}, {"title": "5 CONCLUSIONS", "content": "Diffusion models have significantly impacted the Computer Vision community, offering unprecedented capabilities in high-quality image generation with broad scientific, economic, and societal implications. However, their application to medical imaging is constrained by data and annotation scarcity. Our DiNO-Diffusion approach addresses this problem by conditioning the image generation on the images themselves, eliminating the need for extensive annotations. The approach shows promising results in manifold coverage, data augmentation, privacy preservation and zero-shot segmentation. The image-conditioned nature of the model changes the inference workflow. This is likely beneficial in some use cases, and promising paths exist to address limitations in others. This work highlights the efficacy of DiNO-Diffusion in medical imaging and underscores the need for innovative domain-specific solutions to fully leverage the potential of diffusion models in medical imaging."}]}