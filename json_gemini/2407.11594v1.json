{"title": "DiNO-DIFFUSION: SCALING MEDICAL DIFFUSION VIA SELF-SUPERVISED PRE-TRAINING", "authors": ["Guillermo Jimenez-Perez", "Pedro Osorio", "Josef Cersovsky", "Javier Montalt-Tordera", "Jens Hooge", "Steffen Vogler", "Sadegh Mohammadi"], "abstract": "Diffusion models (DMs) have emerged as powerful foundation models for a variety of tasks, with a large focus in synthetic image generation. However, their requirement of large annotated datasets for training limits their applicability in medical imaging, where datasets are typically smaller and sparsely annotated. We introduce DiNO-Diffusion, a self-supervised method for training latent diffusion models (LDMs) that conditions the generation process on image embeddings extracted from DiNO. By eliminating the reliance on annotations, our training leverages over 868k unlabelled images from public chest X-Ray (CXR) datasets. Despite being self-supervised, DiNO-Diffusion shows comprehensive manifold coverage, with FID scores as low as 4.7, and emerging properties when evaluated in downstream tasks. It can be used to generate semantically-diverse synthetic datasets even from small data pools, demonstrating up to 20% AUC increase in classification performance when used for data augmentation. Images were generated with different sampling strategies over the DiNO embedding manifold and using real images as a starting point. Results suggest, DINO-Diffusion could facilitate the creation of large datasets for flexible training of downstream AI models from limited amount of real data, while also holding potential for privacy preservation. Additionally, DiNO-Diffusion demonstrates zero-shot segmentation performance of up to 84.4% Dice score when evaluating lung lobe segmentation. This evidences good CXR image-anatomy alignment, akin to segmenting using textual descriptors on vanilla DMs. Finally, DiNO-Diffusion can be easily adapted to other medical imaging modalities or state-of-the-art diffusion models, opening the door for large-scale, multi-domain image generation pipelines for medical imaging.", "sections": [{"title": "INTRODUCTION", "content": "Diffusion models (DMs) have recently emerged as robust and proficient foundational models in medical imaging, exhibiting substantial capabilities in image generation, image enhance-ment, reconstruction, and segmentation [1]. The field of synthetic image generation in particular has greatly shifted to text-to-image DMs, generating images that are nearly indistinguishable from real ones [2\u20136] and facilitating remarkable zero-shot performance in segmentation and classification tasks [7, 8].\nHowever, DMs depend on the availability of large datasets containing images paired with corresponding descriptors (usually text) to guide the generation process, a requirement that presents a considerable obstacle in the medical domain [9]. Medical imaging datasets are typically small, contain free-form and inconsistent annotations including captions, binary labels or segmentations, and are generally prohibitively costly to compile and curate [9].\nTo address these challenges, some works have proposed pseudo-labeling with automatic captioners and other GPT-like vision-language models (VLMs) [10] or have trained lean mapping networks over frozen pretrained backbones to reduce the number of required annotated samples [11, 12]. However, despite their promise, pseudo-labelling approaches find limited applicability in the medical field given a lack of high-quality medical imaging captioners [9]. In addition, while some authors have successfully trained mapping networks to bridge the gap between unimodal foundation models, they still require relatively large annotated datasets to be trained [9].\nThese limitations are the main roadblocks for medical DMs. While the natural imaging literature focuses on saturating generation quality by improving the base architecture, optimization process or condition alignment [10, 13, 14], the medical imaging community navigates these hurdles by leveraging smaller or custom-annotated datasets [2\u20136]. Moreover, although mapping networks have found their footing in the diffusion literature with approaches such as ControlNet [12], these would still rely on large-scale medical DMs trained with prohibitively extensive amounts of annotated images. In this context, applying a self-supervised approach to DM training would be highly beneficial for medical image synthesis. Self-supervision enables models to learn from unlabelled data, providing exceptional results in multiple downstream tasks when used as image embedders [15\u201319].\nWith that in mind, we introduce DiNO-Diffusion, a novel self-supervised methodology for training medical DMs at scale which conditions the image generation process on image-derived tokens extracted from a frozen DiNO model [15, 16], as opposed to textual descriptors. DiNO-Diffusion allows independence from existing annotations, circumventing the limitations imposed by the scarcity and inconsistency of medical image labels. Moreover, it is agnostic to the choice of DM architecture, medical imaging modality or optimization strategy. To test this, a model was trained on a large corpus of open-source CXR data found in the literature [20\u201343], which do not contain any common descriptor to train a regular DM (e.g., text captions). The trained models achieved low FID scores and high image quality."}, {"title": "2 METHODS", "content": "This Section explains the methodology employed for study-ing the self-supervised DM. In Section 2.1, the datasets used for training and evaluation are described. In Section 2.2, the model's architecture and theoretical background is outlined. In Section 2.3, the designed mechanisms for self-supervised conditioning are detailed. In Section 2.4, the evaluation protocol employed to benchmark model performance is defined. Finally, in Section 2.5, the specific parameters used for model training and evaluation are enumerated. Figure 1 visually describes the training and evaluation pipeline."}, {"title": "2.1 Data", "content": "To explore DiNO-Diffusion's self-supervision capability, a large-scale dataset comprised of every openly accessible CXR dataset found in the literature [20\u201343]\u00b9 was collected, reaching over 1.2M total images from 21 distinct data providers. Three different subsets were taken from this compound dataset for different purposes. Firstly, a subset comprising every dataset"}, {"title": "2.2 Generative Architecture - Stable Diffusion", "content": "Latent Diffusion Models (LDMs) approach image generation as an iterative denoising process, transforming pure noise xT into a defined image xo over T steps with a parameterized DM $\\epsilon_{\\theta}(z_t, t, c)$, where c represents an optional condition. LDMs address the prohibitive computational demands of traditional DMs by reducing the dimensionality of the input. LDMs currently find active development with ongoing research in different parameterised models, optimization strategies and dimensionality reduction pipelines.\nThis study adopts the Stable Diffusion (SD) framework (version 1) [45] as its baseline. Despite being outperformed by more recent models and its output size limitation of 512x512 pixels, SD's lightweight architecture, open-source nature, and community adoption makes it ideal for our proof of concept. SD comprises a frozen variational autoencoder (VAE) and a trainable conditional denoising UNet.\nThe VAE consists of an encoder (E) and a decoder (D). The encoder compresses fixed-size images $x \\in \\mathbb{R}^{H\\times W\\times 3}$ into a latent $z = E(x) \\in \\mathbb{R}^{(H/d)\\times(W/d)\\times k}$, where k = 4 is number of channels extracted by the VAE and d = 8 is the downsampling factor. The decoder maps latents back to the original image space $x = D(z)$. Stable Diffusion's VAE has been shown to generalize to medical data [3, 46]. The UNet serves as the diffusion component and uses a ResNet architecture as its convolutional backbone, where the condition c is incorporated through attention mechanisms (see Section 2.3).\nWith this model, training with conditional information involves two phases: the forward and reverse diffusion processes. During the forward diffusion, an image xo (or its latent representation zo) and condition c are chosen. A timestep t is randomly selected ($t \\sim U(1, ..., T)$) so a noisy latent $z_t$ is generated by mixing zo with noise $\\epsilon \\sim \\mathcal{N}(0, 1)$, resulting in a partially noised latent. The reverse process uses the UNet to estimate the original noise $\\epsilon$ from $z_t$, t and c.\nThe network is optimized using the Mean Squared Error (MSE) loss between the predicted and actual noise to adjust the weights of the UNet:\n$\\mathcal{L}_{LDM} = \\mathbb{E}_{z \\sim \\mathcal{E}(x), c, \\epsilon \\sim \\mathcal{N}(0,1),t} [||\\epsilon - \\epsilon_{\\theta}(z_t, t, c)||^2]$\nAfter training, image synthesis begins with sampling a noisy latent $z_T \\sim \\mathcal{N}(0, 1)$, progressively denoising it with condition c to obtain zo so that $z_0 = \\epsilon(z_{T:0}, c)$, and by using the VAE's decoder, so that $x = D(z_0) = D(\\epsilon(z_{T:0}, C))$."}, {"title": "2.3 Self-Supervised Conditioning", "content": "LDMs condition image generation using a semantic tensor c to guide the diffusion process. This tensor is usually obtained from a frozen transformer model f\u03c6 that maps the label information into a tensor $c = f_{\\phi}(x) \\in \\mathbb{R}^{S\\times N}$, where S is the token length (of variable size), N is the embedding dimension and x represents whichever input the embedder model requires (text, image, etc.). Although the current diffusion literature has mainly focused on using textual descriptors as their main conditioning strategy, other conditioning mechanisms have been employed [5, 6, 12].\nIn this work we explore conditioning using image-derived semantic descriptors. Specifically, a vision transformer trained with the DiNO method [15, 47] was used to produce a semantic description of the image to be generated. Vision transformers split an image into small patches (usually P = 14px\u00b2 or P = 16px\u00b2) representing \u201cvisual words\u201d and operate over them using a standard transformer architecture. The model outputs a tensor of tokens $c = f_{\\phi}(x) \\in \\mathbb{R}^{S\\times N}$ comprising a class token $c_{CLS} \\in \\mathbb{R}^N$, sometimes a pooler token $c_{PLR} \\in \\mathbb{R}^N$, sometimes a predefined amount R of register tokens $c_{REG} \\in \\mathbb{R}^{R\\times N}$ [48], and finally a series of L patch tokens $c_{LCL} \\in \\mathbb{R}^{L\\times N}$, where $L = H/P_y * W/P_x$. Finally, the conditioning tensor outputted by the embedder was reduced to the available global information $c_{GLB} = [c_{CLS}, c_{PLR}, c_{REG}]$ before feeding it to the UNet, as upon initial exploration the patch tokens contained too much local information of the original image x and led to trivial models that learnt to reconstruct images from redundant information. Figure 1-(a) visually describes the training pipeline.\nConditioning image generation on image embeddings offers flexibility on generation as long as a conditioning embedding exists. In this work, two simple generation strategies were explored, to evaluate the model's in-distribution and out-of-distribution performance, although more advanced approaches could be devised."}, {"title": "2.3.1 Reconstruction-based image generation", "content": "the \u201creconstruction\u201d strategy consists in synthesizing images $x = D(\\epsilon(z_{T:0}, c))$ from the global information of an existing real example (x, y), where y is the image's label, \u0177 = y and f\u03c6(x) is the conditioning embedding as produced by DiNO. This reconstruction leverages DiNO-Diffusion's large-scale pretraining to produce semantic variations over the source image x. Exact replicas of x are prevented by design due to conditioning with the compressed information from DiNO's global embedding, causing a bottleneck. Figure 1 (b-i) depicts the reconstruction process."}, {"title": "2.3.2 Interpolation-based image generation", "content": "the \u201cinterpolation\u201d strategy uses the same image generation mechanism from above. The difference lies in the sampling method of the conditioning embedding c, which is interpolated from two images (x1, y1), (x2, y2) so that $c = lerp(f_{\\phi}(x_1), f_{\\phi}(x_2), r)$, where r \u2208 [0, 1] is the interpolation fraction. This strategy attempts to generate synthetic images from less sampled regions of the real data manifold, located between existing samples, following approaches such as MixUp [49]. See Figure 1 (b-ii) for a visual depiction of this strategy."}, {"title": "2.4 Evaluation", "content": "This section details three different evaluation protocols used for benchmarking DiNO-Diffusion. For every evaluation protocol, two variants are evaluated and compared: DiNOv1-Diffusion [15] and DiNOv2-Diffusion [16], depending on the choice of image encoder."}, {"title": "2.4.1 Image Quality & Checkpoint Selection", "content": "Fr\u00e9chet Inception Distance (FID) [50] was used to quantify generation quality at multiple checkpoints for both variants of DiNO-Diffusion. The FID scores computed for the data generated via the \u201creconstruction\u201d strategy (see Section 2.3) were used as a proxy for overall model performance. Similarly to [3], FID scores were computed over a 5k subset of MIMIC-CXR's"}, {"title": "2.4.2 Data Augmentation", "content": "this experiment explored DiNO-Diffusion's ability to enhance the sample size of a dataset by training a classification model on real and synthetic data using five-fold cross-validation and testing on a held-out test set (MIMIC's p19). For this purpose, MIMIC's training dataset (p10-p18) was subset into different data regimes with decreasing sample size Xn, with $n \\in$ {10k, 5k, 1k, 500, 100, 50} samples in the subset. Given that MIMIC has multi-label annotations, label balancing was performed by randomly selecting n/card(L) elements of each label in the labelset L from X without replacement, ensuring sufficient representativity of all labels within the training set. Smaller subsets were also enforced to be contained into bigger ones, so that $X_{N_{i+1}} \\subseteq X_{N_i}$. With $X_n$ defined, synthetic data was created to increase sample size by generating partially-synthetic datasets $\\mathfrak{E}_n$ with real-to-synthetic ratios of 1:1, 1:5, 1:10 and 1:50 for the reconstruction- and interpolation-based synthesis (see Section 2.3).\nFor the reconstruction experiments, ratios larger than 1:1 represent several semantic variations of a single source image (x, y), with the intent to introduce realistic variance into the synthetic data while retaining the label-specific image features. The interpolation experiment addressed whether intermediate embeddings could still be decoded into an image that retains label-specific features from both elements in the pair. For this purpose, the sample pairs were enforced to have at least one label in common (see Section 2.3.2) without repetition. When not all the labels are in common between the pair, the labels of the interpolated example are set to the ones of the sample it is closest to, as defined by the interpolation fraction r. Finally, in the case of not having enough unique pairs for a given split, some pairings were repeated with different r."}, {"title": "2.4.3 Full Synthetic Training", "content": "this experiment explores whether test-set AUC drops when training a classifier solely on synthetic data, to address whether DINO-Diffusion can serve as a privacy-preserving synthetic replacement for real data. The generation strategies, data regimes, real-to-synthetic ratios and 5-fold cross-validation settings from Section 2.4.2 were followed as evaluation strategy."}, {"title": "2.4.4 Zero-Shot Segmentation", "content": "this experiment investigates the model's ability to learn semantic coherence by generating segmentation masks from the internal representations generated during the DiNO-Diffusion's UNet forward pass. For this purpose, the zero-shot segmentation approach from DiffSeg [7] was followed, consisting of leveraging the self-attention weights from each transformer block of the UNet and iteratively merging them based on their Kullback-Leibler divergence. This methodology was applied both to DiNO-Diffusion and a vanilla SD model to generate lung lobe segmentation masks without further training. Using a combined dataset of 1,048 cases with ground truth annotations (See Section 2.1), candidate masks were evaluated by their Dice score and selected via non-maximum suppression. The respective hyperparameters (merging threshold, timestep, number of anchor points) as well as the best performing checkpoint were selected per model using grid-search. Refer to Figure 1 (b-iii) for a visual depiction of the segmentation pipeline."}, {"title": "2.5 Experimental Setup", "content": "The models were trained by adapting HuggingFace Diffusers' script for training DMs [52]. The DMs were trained for 100 epochs (~140000 steps) using 4 H100 GPUs per model, an aggregated batch size of 512 (bs = 64, gradient accumulation of 2 steps), 8-bit Adam optimizer with constant lr = 10-4 and 1000-step warmup and xformers' memory-efficient attention [53].\nThe specific versions of the DiNOv1 and DiNOv2 image encoder architectures used were \"facebook/dino-vitb16\" [15] and \"timm/vit-base-patch14-reg4-dinov2\" [16, 48], respectively. The webdataset [54] library was used for storing data and to stream it directly from the bucket during all model trainings. The classification experiments were based on training HuggingFace's implementation of a \u201cdensenet121\u201d for 150 max epochs using T4 GPUs with batch size 64, AdamW optimizer with lr = 10-4 and weight decay of 10-5, a LR reduction-on-plateau scheduler with patience 10 and early stopping after 25 epochs with no validation AUC improvement. For the checkpoint evaluation, the specific version of the feature extractor was \u201cdensenet121-res224-all\u201d, a pretrained DenseNet-121 on CXR data from [51].\nAll images followed the same minimal preprocessing strategy before training or evaluation, similar to other works in the literature [3, 51]. Dynamic intensity values (uint8, uint16) were rescaled to uint8. Images were center-cropped with a 1:1 aspect ratio, resized to 512x512 pixels and padded areas were removed. Minimal data augmentations were applied during all model trainings, including random sharpening and affine transformations (5% shearing, 5% translation, 90%-140% scaling)."}, {"title": "3 RESULTS", "content": "3.1 Image Quality & Checkpoint Selection\nThe FID scores were calculated every 2500 steps over a subset of MIMIC's p19 dataset following [3, 43]. Both the DiNOv1 and DiNOv2 models converged relatively late, reaching scores of 4.7 and 6.4 at 80k and 120k steps, respectively. The full FID scores for every checkpoint can be observed in Figure 3. DiNOv1-Diffusion leads to lower FID scores when compared to DiNOv2-Diffusion. This is also evident by a slightly less saturated synthetic images generated with DiNOv2-Diffusion when compared to the source real images (see Figure 2)."}, {"title": "3.2 Data Augmentation", "content": "In this experiment, real and synthetic data were used in different proportions to train DenseNet-121 classification models. Table 1-a and Figure 4-a provide the results of the cross-validation trainings. The 'reconstruction' workstream (see Section 2.3.1) depicts consistent improvements when used for data augmentation in all data regimes, with AUC increases up to approximately 20% in small-data regimes. In some larger-data regimes (N \u2208 [1000, 5000]), the addition of large amounts of synthetic data slightly degraded performance, although never by a significant margin (p > 0.05). The 'interpolation' workstream (see Section 2.3.2) also depicts improvements in smaller data regimes as compared to not using synthetic data, although it leads to a significant performance degradation in large-data regimes (p < 0.05). Also, DiNO-Diffusion using DiNOv1 yields larger performance improvements compared to when using DiNOv2. This is always true for both image synthesis strategies, except for the interpolation results on data regime Nreal = 100, where the best test AUC is achieved with DiNOv2 for 1:50 rs ratio."}, {"title": "3.3 Full Synthetic Training", "content": "The test set results of the full synthetic trainings are shown in Table 1-b and Figure 4-b. The data synthesised via the \u201creconstruction\u201d strategy (see Section 2.3.1) using DiNOv1-Diffusion provided good performance in almost all settings, where statistically significant performance decreases only existed for the lowest rs ratio in the largest three data regimes. For both \"reconstruction\" DiNO-Diffusion variants, training with sufficiently large rs ratios in small-data regimes (Nreal \u2208 [50, 100, 500]) led to significant performance improvements of up to 20%, mirroring the data augmentation results (see Section 2.4.2). However, for the \"interpolation\" based synthesis (see Section 2.3.2), this was only the case in the 1:50 ratio. Generally, the data synthesised via the \"interpolation\" strategy did not reliably train the classifier in splits larger than Nreal = 1k for DiNOv1-Diffusion and Nreal = 500 for DiNOv2-Diffusion. Finally, DiNOv1-Diffusion yielded larger performance improvements and statistical significance when compared to DiNOv2-Diffusion."}, {"title": "3.4 Zero-Shot Segmentation", "content": "The performance of the zero-shot experiments are shown in Table 2. Both DiNOv1- and DiNOv2-Diffusion showed improvements of up to 10% Dice score when compared to a vanilla SD v1.5 model while also presenting lower variance. When addressing individual results, DiNOv1-Diffusion generated the best average Dice scores. Performance varied between datasets, with the Montgomery [25] producing the lowest Dice scores for both vanilla Stable Diffusion and DiNOv1-Diffusion, but to the highest scores for the DiNOv2-based approach when comparing the overall best model. It should be noted that the best model checkpoint for segmentation was significantly earlier than the one found in Section 3.1. Moreover, the optimal parameters for DiffSeg were very similar for both self-supervised DMs, while the optimal merging threshold was 10x larger for the base SD model. Finally, non-optimal combinations of parameters produced significant artifacts in the generated masks as shown in Figure 5 (b)."}, {"title": "4 DISCUSSION", "content": "DMs are a cornerstone in modern foundation models, revolutionizing many tasks in Computer Vision. Their ability to generate high-quality images has caused a large scientific, economic and societal disruption, whose long-term repercussions are difficult to foresee [14]. However, despite their scientific and industrial utility, applying this technology in medical imaging is severely limited by key challenges such as a lack of large-scale labeled datasets including high-quality textual or non-textual descriptions [1]. Although this limitation might be temporary due to current trends in AI data acquisition and improved dataset interoperability [55], it is not clear whether the prevalent text-to-image generative recipe [45] is optimal for medical applications.\nSome approaches employing DMs in medical data exist. Chambon et al. [3] trained an SD architecture on the MIMIC-CXR dataset [43] with good synthesis fidelity, reporting low FID scores and high accuracy scores on several downstream tasks including classification, report generation and image retrieval. However, their approach is severely limited on the size of the development dataset (300k images) and the low quality of accompanying captions. In histopathology, multiple authors have proposed applying DMs for image generation [2, 4, 5, 58]. For instance, Aversa et al. relied on a custom-annotated dataset of large histopathology slides with segmentation masks representing different tissue subtypes within the slide and employed timestep unravelling to generate images larger than the typical 512px\u00b2. However, their approach heavily relied on a closed-source, custom-annotated dataset, and timestep unravelling might be impractical in other medical imaging modalities. In contrast, Xu et al. [58] take a similar approach as the one proposed here, and train a DM conditioned only on an image encoder's cCLS for histopathology image synthesis. However, their method was partially supervised, as it relied on training additional label-specific DMs for cCLS generation. Besides being compute intensive, their method fails to leverage the emerging data augmentation and segmentation capabilities that a self-supervision DM training conveys. Finally, Pinaya et al. [6] trained an LDM on a large dataset of 31740 3D Brain MRI images from UK BioBank. However, despite the scale of this dataset, the fragmentation of clinical labels forced the authors to condition the DM with simplified clinical variables such as age, sex, ventricular volume, and brain volume.\nDiNO-Diffusion addresses the data limitations in medical imaging by conditioning the image generation process on the images themselves. This allows training DMs on unlabelled data, which is more abundant in the medical field. The resulting DINO-Diffusion models demonstrated good manifold coverage, as indicated by low FID scores, and exhibited notable properties in several downstream tasks. Firstly, adding synthetic data using the \"reconstruction\" strategy improved performance across most configurations. However, performance gains diminished as more real data became available, which is to be expected. Secondly, the \u201cinterpolation\u201d strategy degraded performance in higher data regimes. We hypothesize that, although the generated images qualitatively resemble plausible images (see Figure 2-b), na\u00efvely interpolating embeddings did not ensure that the interpolated labels corresponded to the decoded image's features, thereby hurting classification performance. We leave to future work the exploration of more sophisticated interpolation strategies. Thirdly, full synthetic training demonstrated that synthetic data can replace real data while preserving privacy, and even improve performance in small-data regimes, when used in abundance. Finally, DiNO-Diffusion's zero-shot segmentation outperformed a vanilla SD architecture. This is remarkable given that the dataset used to train the vanilla SD model was several orders of magnitude larger.\nDespite DiNO-Diffusion's performance, conditioning the synthesis process on image embeddings has theoretical advantages and disadvantages. This type of conditioning relaxes the need for annotations, enabling the collection of larger datasets for model training, and has proven effective across various tasks. However, usage of an image-conditioned model is fundamentally different from text-based approaches, as image generation requires conditioning on an image. Still, this circular dependency between input and output could be advantageous in some use cases, such as data augmentation or privacy-preserving data sharing.\nThese advantages and disadvantages evidence room for improvement. Firstly, DiNOv1-Diffusion outperformed DiNOv2-Diffusion both quantitatively and qualitatively, despite the larger data pool used to train the DiNOv2 image encoder [16]. This suggests that using domain-specific encoders [17, 19, 51], or even a combination of different image encoders [13, 14] could further improve these results. Secondly, DiNO-Diffusion would benefit from more recent diffusion architectures found in the literature [10, 13, 14]. Thirdly, generation based on other descriptors, such as text, could be enabled by bridging the gap to the image embedding space, e.g., via lean mapping networks [11, 12] or by training other conditional generative models [58]. Finally, the failure cases found in the zero-shot segmentation workstream require adapting the DiffSeg methodology to datasets with different characteristics. These adaptations could include image-level hyperparameter optimization, more sophisticated attention-merging strategies, or using DiNO's attention maps to better locate anatomic structures."}, {"title": "5 CONCLUSIONS", "content": "Diffusion models have significantly impacted the Computer Vision community, offering unprecedented capabilities in high-quality image generation with broad scientific, economic, and societal implications. However, their application to medical imaging is constrained by data and annotation scarcity. Our DiNO-Diffusion approach addresses this problem by conditioning the image generation on the images themselves, eliminating the need for extensive annotations. The approach shows promising results in manifold coverage, data augmentation, privacy preservation and zero-shot segmentation. The image-conditioned nature of the model changes the inference workflow. This is likely beneficial in some use cases, and promising paths exist to address limitations in others. This work highlights the efficacy of DiNO-Diffusion in medical imaging and underscores the need for innovative domain-specific solutions to fully leverage the potential of diffusion models in medical imaging."}]}