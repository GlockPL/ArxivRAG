{"title": "From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models", "authors": ["Mehar Bhatia", "Sahithya Ravi", "Aditya Chinchure", "Eunjeong Hwang", "Vered Shwartz"], "abstract": "Despite recent advancements in vision-language models, their performance remains suboptimal on images from non-western cultures due to underrepresentation in training datasets. Various benchmarks have been proposed to test models' cultural inclusivity, but they have limited coverage of cultures and do not adequately assess cultural diversity across universal as well as culture-specific local concepts. To address these limitations, we introduce the GLOBALRG benchmark, comprising two challenging tasks: retrieval across universals and cultural visual grounding. The former task entails retrieving culturally diverse images for universal concepts from 50 countries, while the latter aims at grounding culture-specific concepts within images from 15 countries. Our evaluation across a wide range of models reveals that the performance varies significantly across cultures \u2013 underscoring the necessity for enhancing multicultural understanding in vision-language models.", "sections": [{"title": "1 Introduction", "content": "Vision-Language Models (VLMs) have shown emergent capabilities through large-scale training that have made them gain popularity in recent years. VLMs show promising results across various vision and language tasks, from image captioning to visual question answering and cross-modal retrieval and grounding. A key component contributing to their strong performance across the board is the scale of their pre-training datasets. However, these large-scale datasets tend to predominantly contain images from Western cultures (Shankar et al., 2017). The underrepresentation of certain cultures in the data translates into performance disparities across cultures. (De Vries et al., 2019; Gustafson et al., 2023).\nSeveral benchmarks and datasets have been proposed to test the cultural inclusivity of VLMs. These include testing the models' performance on questions pertaining to images from certain cultures (Liu et al., 2021a; Yin et al., 2021), on their ability to adapt images from one culture to another (Khanuja et al., 2024), or on stereotypical depiction of various cultures (Jha et al., 2024). Nonetheless, existing benchmarks address a limited set of cultures (5-7), leaving a substantial representational gap. Moreover, current benchmarks leave out a crucial aspect: assessing the cultural diversity in the representation of universal concepts.\nTo address this gap, we present the GLOBALRG benchmark, which consists of two tasks (Figure 1). The first task, retrieval across universals, covers images from 50 countries across 10 regions. It assesses the ability of VLMs to retrieve culturally-diverse images pertaining to textual prompts of universal concepts such as \u201cbreakfast\u201d and \u201cwedding\u201d."}, {"title": "2 Related Work", "content": "The Geo-Diversity Problem. Existing large-scale vision and language datasets are imbalanced in their representation of different regions, over-representing the West (Shankar et al., 2017). As a result, models trained on these datasets may exhibit discrepancies in performance when introduced with inputs concerning various demographic and geographic factors (e.g. Gustafson et al., 2023; De Vries et al., 2019). For instance, image generation models\u2014when asked to generate images of universal concepts such as \"house\", tend to depict the concept as it appears in the US or India, cultures that are more prominently featured in the training data (Basu et al., 2023).\nTo serve users from diverse cultures fairly, it is imperative to collect large-scale datasets from diverse data sources (Kim et al., 2021; Goyal et al., 2022). Two recent geo-diverse image datasets that are popular for training geo-diverse VLMs, Dollar Street (Rojas et al., 2022) and GeoDE (Ramaswamy et al., 2024), focus on common household items, lacking coverage of more abstract and culture-specific concepts. Finally, to make cross-cultural data collection more feasible, researchers proposed to apply domain adaptation (Kalluri et al., 2023) and active learning (Ignat et al., 2024) based on visual similarity.\nGeo-Diverse Benchmarks. With the understanding that language has a social function, there has been growing interest in the NLP community in making models more culturally inclusive (e.g., Hershcovich et al., 2022; Nguyen et al., 2023; Bhatia and Shwartz, 2023). Several benchmarks have been developed to test language models' cultural awareness with respect to values and social norms (Durmus et al., 2024), culinary norms (Palta and Rudinger, 2023), figurative language (Kabra et al., 2023), and more.\nIn the multimodal domain, benchmarks have been developed to test VLMs on visual question answering and reasoning (Liu et al., 2021a; Yin et al., 2021; Zhou et al., 2022), image-text retrieval and visual grounding (Zhou et al., 2022), image captioning (Ye et al., 2023), and cultural adaptation (Khanuja et al., 2024).\nDespite these efforts, current benchmarks typically cover an incredibly small number of cultures (5-7). To bridge this gap, we introduce a benchmark with two tasks covering 50 and 15 cultures respectively. Moreover, our benchmark tests models both on their familiarity with culture-specific concepts and on the diversity of their representation of universal concepts."}, {"title": "3 Task 1: Retrieval across Universals", "content": "Image-text retrieval is a fundamental task for evaluating VLMs, where the objective is to retrieve relevant images based on textual queries. Existing retrieval benchmarks such as COCO (Lin et al., 2014), Flicker30K (Plummer et al., 2015), Image-CoDe (Krojer et al., 2022), and CIRR (Liu et al., 2021b) contain images predominantly from North America and Europe. To develop globally effective retrieval systems, it is crucial to evaluate models on culturally heterogeneous datasets. In this work, we present a dataset containing images from 50 cultures (Table 1). We introduce the novel task of Retrieval across Universals, aimed at retrieving culturally diverse images for universal concepts such as \"wedding\". We describe the dataset collection in Sec 3.1.\nImage-text retrieval is typically evaluated using precision. Beyond measuring the correctness of the retrieved images, this metric overlooks a significant aspect of retrieval systems: cultural diversity. We thus propose an additional evaluation metric to measure the cultural diversity of the retrieved images (Sec 3.2). We evaluate an extensive number of VLMs on the retrieval task (Sec 3.3) and report the results in Sec 3.4."}, {"title": "3.1 Dataset Collection", "content": "Textual Queries. The queries in our dataset are human universals-concepts common across cultures worldwide, such as \"clothing\u201d and \u201cdance\u201d. Table 2 presents the list of 20 human universals used as textual queries in our dataset. The list was adapted from an extensive list of 369 human universals by Brown (2004) and Pinker (2004). We manually selected human universals that can be depicted in images. For example, universals like \"clothing\" are associated with tangible objects, and \"dance\" is a ritual that can be visually depicted. In both cases, these universal concepts are expected to be visually represented differently across diverse cultures.\nImages. To obtain culturally diverse images corresponding to the textual queries, we first used CANDLE (Nguyen et al., 2023), a comprehensive corpus of cultural knowledge, to extract 3 sentences corresponding to each universal concept and each culture. For example, for \"wedding\u201d and \u201cIndia\u201d, CANDLE contains the sentence \"The mehendi ceremony holds significance in Indian tradition\u201d. These sentences provide context and cultural specificity for each universal. We use these sentences to scrape images from Google Images. To ensure the quality of the images, one of the authors manually verified each image in the dataset, filtering out low-resolution images, images with text, and images depicting multiple scenes (i.e., grid images). The final dataset includes a total of 3,000 visually-diverse images (50 cultures \u00d7 20 universals \u00d7 3 images)."}, {"title": "3.2 Task Definition and Evaluation Setup", "content": "We introduce the novel task of Retrieval across Universals, aimed at retrieving culturally diverse images for a given universal concept. Formally, let $Q = \\{q_1, q_2,...,q_n\\}$ be a set of textual queries representing universal concepts, and $I = \\{I_1, I_2,..., I_m\\}$ the set of images from different cultures. Given a query $q \\in Q$, the goal is to retrieve a ranked list of images $R(q,I) = \\{I_{r_1}, I_{r_2},..., I_{r_k} \\} C I$ that maximizes both relevance and cultural diversity.\n\u2022 Relevance: $Rel(q, I)$ refers to how well the image I matches the query q.\n\u2022 Diversity: $Div(R(q,I))$ measures the cultural diversity of the retrieved images.\nSpecifically, relevance is captured by the standard precision@k, the ratio of the top k retrieved images that correctly answer the query. For diversity, we propose the diversity@k metric, which uses entropy to measure the cultural diversity among the top k retrieved images:\ndiversity @k = $\\frac{1}{log(m)} \\sum_{i=1}^{m} p_i log(p_i)$ (1)\nwhere $p_i$ is the proportion of images from the i-th culture in the top k retrieved images R(q), and m is the total number of cultures in the top k. A high normalized entropy value (~ 1) indicates high diversity, meaning the retrieved images are well-distributed across different cultures. Conversely, a low entropy value (~ 0) indicates low diversity, suggesting that the retrieved images are biased towards specific cultures. We report diversity with respect to both the country and the region."}, {"title": "3.3 Models", "content": "We evaluate the performance of several state-of-the-art VLMs on the retrieval task. The models are categorized based on their architectural design and training methodologies in Table 3. We cover a diverse set of models, including dual encoder and encoder-decoder, as well as dual encoders with multimodal fusion encoder. These models facilitate cross-modal alignment via a multitude of pre-training objectives, including contrastive loss on uni-modal encoders, image-text matching, masked language modelling, and more."}, {"title": "3.4 Results and Analysis", "content": "RQ1: Are VLMs able to retrieve relevant and culturally diverse images for universal concept words? Table 3 presents the relevance and diversity scores for each model (see Appendix A.1.1 for a complete breakdown by universal). With respect to relevance, models achieve moderate to high precision scores, with CoCA leading by 5 points.\nWe note that country-level diversity scores are high for all models, indicating that VLMs can retrieve images from a variety of geographical contexts. Among them, CoCA performs exceptionally well, likely attributed to its extensive training on 3 billion images from Google's proprietary JFT dataset (Zhai et al., 2022).\nSimilarly, in dual-encoder models, OpenCLIP demonstrates superior cultural diversity, benefiting from its large training dataset of 2 billion images. CLIP, which uses the same dual-encoder architecture and contrastive loss objectives as OpenCLIP but is trained on a dataset five times smaller, exhibits lower performance across all metrics. Naturally, pre-training on a larger-scale dataset increases the chances that the model was exposed to more culturally diverse images. In contrast, regional diversity scores are notably lower across the board. At the same time, for country diversity@5, BLIP-2 stands out as having the highest cultural diversity, leveraging frozen pre-trained encoders (ViT-G (Fang et al., 2023) as the vision encoder and instruction-tuned FlanT5 (Chung et al., 2024) as the language model) and a QFormer architecture. A particularly surprising finding is the robust performance of TCL with respect to both relevance and diversity \u2013 despite being trained on a the smallest dataset among all models (4M images). TCL incorporates a unique uni-modal objective to make the model invariant to data modifications, which likely benefits the cross-modal alignment and joint multi-modal embedding learning. This may suggest that well-designed training objectives can sometimes compensate for smaller datasets, highlighting the significance of pre-training objectives alongside data scale.\nRQ2: Do VLMs exhibit biases towards images from specific cultures? From the full results in Appendix A.1.2 and A.1.3 we can observe that there are no countries or regions that are consistently retrieved by models. A closer look reveals that the bias towards specific countries or regions is universal-specific. To demonstrate this point, we plot the top 5 retrieved images for 4 universal concepts, \u201cbreakfast\u201d, \u201cfuneral\u201d, \u201cfarming\u201d, and"}, {"title": "4 Task 2: Cultural Visual Grounding", "content": "Visual grounding is essential for human-AI interactions, enabling users to reference regions using spatial cues and models to respond with precise visual answers, such as bounding boxes. Existing grounding datasets such as RefCOCO and its variants (Kazemzadeh et al., 2014; Yu et al., 2016), Flickr Entities (Plummer et al., 2015), Visual Genome (Krishna et al., 2017), and GRIT (Gupta et al., 2022) tend to focus on generic concepts and their images lack cultural contexts.\nTo address this limitation, we propose the task of Cultural Visual Grounding, to evaluate the ability of VLMs to identify culture-specific concepts. We describe our dataset collection (Sec 4.1), the task and evaluation metric (Sec 4.2). We evaluate various models on our task (Sec 4.3), and report the performance in Sec 4.4."}, {"title": "4.1 Dataset Collection", "content": "Cultural Keywords. In this task, we focus on 15 countries across 8 regions, detailed in Table 4. We extract from CANDLE 50 cultural keywords for each culture, covering topics such as food, rituals, clothing, etc. The list of keywords is detailed in Appendix A.2.\nImages. To obtain images corresponding to the keywords, we recruit annotators from the respective cultures through the CloudConnect Platform by Cloud Research. We instructed annotators to find an image depicting the target cultural concept using Google Images. We emphasized that the images should be of high quality and do not solely depict the target concept but also include other visuals, to make sure the grounding task is not trivial. For instance, an image for the Korean sauce \"gochujang\" may contain gochujang along with other dishes.\nBounding Boxes. After selecting the images, annotators used a bounding box tool to draw a single bounding box (bbox) around the target concept. Each annotator was compensated $50 USD for retrieving and annotating images for 50 concepts in their culture.\nVerification. We perform an additional analysis step to verify that the cultural concept is not the main focus of the image. We do so by ensuring that the bbox-to-image ratio is less than 0.3. We also used an off-the-shelf object detection model, YOLOv5, to assess the number of objects in the image, filtering out images with fewer than 3 objects. Additionally, annotators were asked whether the concept was prevalent in their culture, and 1.3% of the concepts were marked as not prevalent. This process resulted in the collection of 591 images. More detailed statistics of the collected data are provided in Table 4.\nFinally, we conduct a human evaluation to ensure quality by recruiting annotators from CloudConnect. Each annotator was asked to draw bounding boxes for the given cultural concept word. Annotator agreement was measured by calculating the Intersection over Union (IoU) score between the bounding boxes drawn by two different annotators. The IoU is calculated as: IoU = $\\frac{|R_{\\text{anno1}} \\cap R_{\\text{anno2}}|}{|R_{\\text{anno1}} \\cup R_{\\text{anno2}}|}$. Each annotator was compensated $0.1 USD of each annotation. More detailed statistics of the collected data and human agreement scores (IoU) are provided in Table 4."}, {"title": "4.2 Task Definition and Evaluation Setup", "content": "Given an image I and a query q describing a cultural keyword, the goal is to predict a bounding box R around the region in I that corresponds to q. We evaluate models based on the overlap between the gold standard and predicted regions of interest, using Intersection over Union (IoU) as the metric: IoU = $\\frac{R \\cap R_{\\text{gold}}}{R \\cup R_{\\text{gold}}}$. We consider a predicted bounding box correct if its IoU with the ground-truth bounding box is greater than 0.5, and report overall accuracy. It is crucial that models perform consistently well across different cultures."}, {"title": "4.3 Models", "content": "We benchmark a series of models on our grounding task, considering both specialist models, designed explicitly for visual grounding tasks, and generalist models, which can handle a wide range of vision-language tasks, such as captioning, question answering, and grounding. These models are listed in Table 5, along with their training data, vision and language backbones, and training methodology.\nThe specialist model we include is Grounding DINO (Liu et al., 2023), a zero-shot object detection model that combines a Transformer-based detector (DINO; Zhang et al., 2022) with phrase grounding pre-training (GLIP; Li et al., 2022).\nThe generalist models are multimodal large language models (MLLMs). MLLMs encode visual patches as tokens that a language model can understand. They perform visual grounding by generating bounding boxes in textual format, typically in the format of (Xleft\u3009\u3008Ytop)\u3009\u3008Xright)\u3008Ybottom), denoting the coordinates of the top-left and bottom-right corners of the generated bounding box."}, {"title": "4.4 Results and Analysis", "content": "RQ1: Are VLMs able to identify culture-specific concepts? Figure 3 presents the country-level accuracy of each model on the cultural visual grounding task. The overall performance across models is rather poor. Among all models, the specialist model Grounding DINO shows a relatively higher average performance (47.99%) compared to the generalist models.\nAnalyzing country-specific performance, we observe that KOSMOS-2 and QwenVL-7B exhibit strong accuracy in grounding elements for Canada and Mexico. Grounding DINO, on the other hand, performs well for Poland and the Philippines. All generalist models perform poorly on images from Vietnam, highlighting limited representation in training datasets.\nRQ2: Do VLMs exhibit biases towards images from certain cultures? To investigate whether VLMs show biases towards specific cultures, we plot the region-level performance for each model in Figure 4. We observe that almost all models achieve the highest performance on images from North America, with an average accuracy of 64.61%, followed by a considerable drop in performance for images from Latin America (46.99%) and Europe (44.49%). This significant performance disparity may suggest that the VLMs were predominantly trained on images from North America.\nDifferent models vary in their performances in the other regions. The generalist models show the most difficulty with images from South East Asia (accuracy between 18.75-27.5%) and East Asia (31.11-35.08%) while Grounding DINO performs worst on Middle Eastern images (25%).\nRQ3: What challenges do VLMs face in grounding culture-specific concepts? Figure 5 presents some failure cases of the VLMs in the grounding task. We can categorize the errors into two primary types. In the first type, models draw a bounding box around an unrelated object. For example, in the image depicting a \u201cbayong\", a type of bag from the Philippines, the models frequently misidentify people as the \"bayong\u201d. This suggests the model is unfamiliar with the term \"bayong\" and its visual representation. The other error type occurs when models draw the bounding box around another object with a shape similar to the target object. For instance, for \"ogene\", a double-bell instrument from Nigeria, some models incorrectly identified a person's arm as the \u201cogene\u201d, which may be due to shape similarity. This may suggest limited familiarity with the concept and its visual form."}, {"title": "5 Conclusion", "content": "In this work, we introduced a challenging benchmark, GLOBALRG, designed to evaluate the multicultural understanding of VLMs. GLOBALRG encompasses two tasks: retrieval of culturally diverse images depicting universal concepts and visual grounding of culture-specific concepts. Our findings from extensive experiments across a wide array of VLMs reveal significant performance variations across cultures, highlighting the existence of biases in current VLMs. Moving forward, future research should focus on collecting large-scale culturally diverse training datasets and devising training objectives that enhance models' representations of images from diverse cultures, ultimately paving the way for developing more inclusive and fair downstream applications."}, {"title": "Limitations", "content": "While our benchmark, GLOBALRG, provides a comprehensive evaluation of the multicultural understanding of VLMs, it is essential to acknowledge certain limitations as follows,\nCultural Coverage. Although our retrieval task encompasses 50 diverse cultures, the grounding task is restricted to only 15 cultures. This constraint arises from the availability of annotators on the crowdsourcing platform we used, Cloud Research. In future work, we aim to expand the grounding task to include a broader range of cultures.\nRestricted cultural concepts. Our study focuses on a selected set of cultural concepts or keywords from the CANDLE dataset. There might be more prominent cultural concepts that we could not cover. This limitation might restrict the comprehensiveness of our evaluation and overlook culturally significant aspects not captured by the selected keywords.\nMetric for diversity. We currently employ a diversity metric based on entropy to evaluate the cultural diversity of retrieved images. While this metric provides insights into the distribution of images across different cultures, it may not fully capture the nuanced variations in cultural representation. Our approach to regional diversity assessment may lack granularity, potentially overlooking finer distinctions in cultural diversity within regions.\nEthical Consideration\nMapping from countries to regions. For the purpose of our tasks, we mapped countries to broad regional categories as specified in Table 1. We acknowledge that cultures do not follow geographic boundaries and that this variation occurs at an individual level, shaped by one's own life experiences. Despite this, we used our mapping as a practical starting point. This approach is a preliminary step, with the ultimate goal of developing systems that can learn from individual user interactions and adapt to diverse and evolving cultures.\nAnnotator selection and compensation Annotators hired from Cloud Research were predominately based in USA, Canada, Australia, New Zealand, United Kingdom and Ireland. Participation was strictly limited to those who met specific criteria to maintain the relevance of the annotation process. Annotators were required to belong to a chosen ethnicity and to have lived in the designated countries for at least 5 of the past 15 years. This criterion ensured that participants had sufficient cultural context and lived experience relevant to the annotation tasks. We employed a second round of annotators for the human evaluation phase, ensuring none were repeated from the first round.\nInadvertent stereotypes in collect images. We recognize that some images used to capture cultural concepts might inadvertently perpetuate stereotypes. While our goal was to gather authentic cultural representations, we are aware of the ethical implications of including such content. We approached this task with the intention of collecting meaningful cultural data while being mindful of the potential for reinforcing harmful stereotypes."}]}