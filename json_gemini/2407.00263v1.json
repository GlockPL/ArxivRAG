{"title": "From Local Concepts to Universals:\nEvaluating the Multicultural Understanding of Vision-Language Models", "authors": ["Mehar Bhatia", "Sahithya Ravi", "Aditya Chinchure", "Eunjeong Hwang", "Vered Shwartz"], "abstract": "Despite recent advancements in vision-\nlanguage models, their performance remains\nsuboptimal on images from non-western\ncultures due to underrepresentation in training\ndatasets. Various benchmarks have been\nproposed to test models' cultural inclusivity,\nbut they have limited coverage of cultures and\ndo not adequately assess cultural diversity\nacross universal as well as culture-specific\nlocal concepts. To address these limitations, we\nintroduce the GLOBALRG benchmark, com-\nprising two challenging tasks: retrieval across\niversals and cultural visual grounding.\nThe former task entails retrieving culturally\ndiverse images for universal concepts from 50\ncountries, while the latter aims at grounding\nculture-specific concepts within images from\n15 countries. Our evaluation across a wide\nrange of models reveals that the performance\nvaries significantly across cultures \u2013 underscor-\ning the necessity for enhancing multicultural\nunderstanding in vision-language models.", "sections": [{"title": "1 Introduction", "content": "Vision-Language Models (VLMs) have shown\nemergent capabilities through large-scale training\nthat have made them gain popularity in recent years.\nVLMs show promising results across various vi-\nsion and language tasks, from image captioning\nto visual question answering and cross-modal re-\ntrieval and grounding. A key component contribut-\ning to their strong performance across the board is\nthe scale of their pre-training datasets. However,\nthese large-scale datasets tend to predominantly\ncontain images from Western cultures (Shankar\net al., 2017). The underrepresentation of certain\ncultures in the data translates into performance dis-\nparities across cultures. (De Vries et al., 2019;\nGustafson et al., 2023).\nSeveral benchmarks and datasets have been pro-\nposed to test the cultural inclusivity of VLMs.\nThese include testing the models' performance on\nquestions pertaining to images from certain cul-\ntures (Liu et al., 2021a; Yin et al., 2021), on their\nability to adapt images from one culture to another\n(Khanuja et al., 2024), or on stereotypical depiction\nof various cultures (Jha et al., 2024). Nonetheless,\nexisting benchmarks address a limited set of cul-\ntures (5-7), leaving a substantial representational\ngap. Moreover, current benchmarks leave out a\ncrucial aspect: assessing the cultural diversity in\nthe representation of universal concepts.\nTo address this gap, we present the GLOBALRG\nbenchmark, which consists of two tasks (Figure 1).\nThe first task, retrieval across universals, covers\nimages from 50 countries across 10 regions. It\nassesses the ability of VLMs to retrieve culturally-\ndiverse images pertaining to textual prompts of\nuniversal concepts such as \u201cbreakfast\u201d and \u201cwed-"}, {"title": "2 Related Work", "content": "The Geo-Diversity Problem. Existing large-\nscale vision and language datasets are imbalanced\nin their representation of different regions, over-\nrepresenting the West (Shankar et al., 2017). As\na result, models trained on these datasets may\nexhibit discrepancies in performance when intro-\nduced with inputs concerning various demographic\nand geographic factors (e.g. Gustafson et al., 2023;\nDe Vries et al., 2019). For instance, image gen-\neration models\u2014when asked to generate images\nof universal concepts such as \"house\", tend to de-\npict the concept as it appears in the US or India,\ncultures that are more prominently featured in the\ntraining data (Basu et al., 2023).\nTo serve users from diverse cultures fairly, it is\nimperative to collect large-scale datasets from di-"}, {"title": "3 Task 1: Retrieval across Universals", "content": "Image-text retrieval is a fundamental task for eval-\nuating VLMs, where the objective is to retrieve\nrelevant images based on textual queries. Existing\nretrieval benchmarks such as COCO (Lin et al.,\n2014), Flicker30K (Plummer et al., 2015), Image-\nCoDe (Krojer et al., 2022), and CIRR (Liu et al.,\n2021b) contain images predominantly from North\nAmerica and Europe. To develop globally effective\nretrieval systems, it is crucial to evaluate models\non culturally heterogeneous datasets. In this work,\nwe present a dataset containing images from 50\ncultures (Table 1). We introduce the novel task of"}, {"title": "3.2 Task Definition and Evaluation Setup", "content": "We introduce the novel task of Retrieval across\nUniversals, aimed at retrieving culturally diverse\nimages for a given universal concept. Formally,\nlet $Q = {q_1, q_2,...,q_n}$ be a set of textual\nqueries representing universal concepts, and $I = {I_1, I_2,..., I_m}$ the set of images from differ-\nent cultures. Given a query $q \\in Q$, the goal\nis to retrieve a ranked list of images $R(q,I) = {I_{r_1}, I_{r_2},..., I_{r_k} } \\subset I$ that maximizes both rele-\nvance and cultural diversity.\nSpecifically, relevance is captured by the stan-\ndard precision@k, the ratio of the top k retrieved\nimages that correctly answer the query. For diver-\nsity, we propose the diversity@k metric, which\nuses entropy to measure the cultural diversity\namong the top k retrieved images:\n$\\text{diversity @k} = \\frac{-1}{\\log(m)} \\sum_{i=1}^{m} p_i \\log(p_i)$ (1)\nwhere $p_i$ is the proportion of images from the $i$-\nth culture in the top k retrieved images $R(q)$, and\n$m$ is the total number of cultures in the top k. A\nhigh normalized entropy value (~ 1) indicates high\ndiversity, meaning the retrieved images are well-\ndistributed across different cultures. Conversely,\na low entropy value (~ 0) indicates low diversity,\nsuggesting that the retrieved images are biased to-\nwards specific cultures. We report diversity with\nrespect to both the country and the region."}, {"title": "3.3 Models", "content": "We evaluate the performance of several state-of-\nthe-art VLMs on the retrieval task. The models\nare categorized based on their architectural design\nand training methodologies in Table 3. We cover\na diverse set of models, including dual encoder\nand encoder-decoder, as well as dual encoders with\nmultimodal fusion encoder. These models facili-\ntate cross-modal alignment via a multitude of pre-\ntraining objectives, including contrastive loss on\nuni-modal encoders, image-text matching, masked\nlanguage modelling, and more."}, {"title": "3.4 Results and Analysis", "content": "RQ1: Are VLMs able to retrieve relevant and\nculturally diverse images for universal concept\nwords? Table 3 presents the relevance and diver-\nsity scores for each model (see Appendix A.1.1 for\na complete breakdown by universal). With respect\nto relevance, models achieve moderate to high pre-\ncision scores, with CoCA leading by 5 points.\nWe note that country-level diversity scores are\nhigh for all models, indicating that VLMs can re-\ntrieve images from a variety of geographical con-\ntexts. Among them, CoCA performs exceptionally\nwell, likely attributed to its extensive training on\n3 billion images from Google's proprietary JFT\ndataset (Zhai et al., 2022).\nSimilarly, in dual-encoder models, OpenCLIP\ndemonstrates superior cultural diversity, benefiting\nfrom its large training dataset of 2 billion images.\nCLIP, which uses the same dual-encoder architec-\nture and contrastive loss objectives as OpenCLIP\nbut is trained on a dataset five times smaller, ex-\nhibits lower performance across all metrics. Nat-\nurally, pre-training on a larger-scale dataset in-\ncreases the chances that the model was exposed\nto more culturally diverse images. In contrast, re-\ngional diversity scores are notably lower across the\nboard. At the same time, for country diversity@5,\nBLIP-2 stands out as having the highest cultural\ndiversity, leveraging frozen pre-trained encoders\n(ViT-G (Fang et al., 2023) as the vision encoder\nand instruction-tuned FlanT5 (Chung et al., 2024)\nas the language model) and a QFormer architecture.\nA particularly surprising finding is the robust\nperformance of TCL with respect to both rele-\nvance and diversity \u2013 despite being trained on a the\nsmallest dataset among all models (4M images).\nTCL incorporates a unique uni-modal objective to\nmake the model invariant to data modifications,\nwhich likely benefits the cross-modal alignment\nand joint multi-modal embedding learning. This\nmay suggest that well-designed training objectives\ncan sometimes compensate for smaller datasets,\nhighlighting the significance of pre-training objec-\ntives alongside data scale.\nRQ2: Do VLMs exhibit biases towards images\nfrom specific cultures? From the full results in\nAppendix A.1.2 and A.1.3 we can observe that\nthere are no countries or regions that are consis-\ntently retrieved by models. A closer look reveals\nthat the bias towards specific countries or regions\nis universal-specific. To demonstrate this point,\nwe plot the top 5 retrieved images for 4 universal\nconcepts, \u201cbreakfast\u201d, \u201cfuneral\u201d, \u201cfarming\u201d, and"}, {"title": "4 Task 2: Cultural Visual Grounding", "content": "Visual grounding is essential for human-AI interac-\ntions, enabling users to reference regions using spa-\ntial cues and models to respond with precise visual\nanswers, such as bounding boxes. Existing ground-\ning datasets such as RefCOCO and its variants\n(Kazemzadeh et al., 2014; Yu et al., 2016), Flickr\nEntities (Plummer et al., 2015), Visual Genome\n(Krishna et al., 2017), and GRIT (Gupta et al.,\n2022) tend to focus on generic concepts and their\nimages lack cultural contexts.\nTo address this limitation, we propose the task of\nCultural Visual Grounding, to evaluate the abil-\nity of VLMs to identify culture-specific concepts.\nWe describe our dataset collection (Sec 4.1), the\ntask and evaluation metric (Sec 4.2). We evaluate\nvarious models on our task (Sec 4.3), and report\nthe performance in Sec 4.4."}, {"title": "4.1 Dataset Collection", "content": "Cultural Keywords. In this task, we focus on 15\ncountries across 8 regions, detailed in Table 4. We\nextract from CANDLE 50 cultural keywords for\neach culture, covering topics such as food, rituals,\nclothing, etc. The list of keywords is detailed in\nAppendix A.2.\nImages. To obtain images corresponding to the\nkeywords, we recruit annotators from the respec-\ntive cultures through the CloudConnect Platform by\nCloud Research. We instructed annotators to find\nan image depicting the target cultural concept using\nGoogle Images. We emphasized that the images\nshould be of high quality and do not solely depict\nthe target concept but also include other visuals, to\nmake sure the grounding task is not trivial. For in-\nstance, an image for the Korean sauce \"gochujang\"\nmay contain gochujang along with other dishes.\nBounding Boxes. After selecting the images, an-\nnotators used a bounding box tool to draw a single\nbounding box (bbox) around the target concept.\nEach annotator was compensated $50 USD for re-\ntrieving and annotating images for 50 concepts in\ntheir culture.\nVerification. We perform an additional analysis\nstep to verify that the cultural concept is not the\nmain focus of the image. We do so by ensuring\nthat the bbox-to-image ratio is less than 0.3. We\nalso used an off-the-shelf object detection model,\nYOLOv5, to assess the number of objects in the im-\nage, filtering out images with fewer than 3 objects.\nAdditionally, annotators were asked whether the\nconcept was prevalent in their culture, and 1.3% of\nthe concepts were marked as not prevalent. This\nprocess resulted in the collection of 591 images.\nMore detailed statistics of the collected data are\nprovided in Table 4."}, {"title": "4.2 Task Definition and Evaluation Setup", "content": "Given an image $I$ and a query $q$ describing a cul-\ntural keyword, the goal is to predict a bounding\nbox $R$ around the region in $I$ that corresponds to\n$q$. We evaluate models based on the overlap be-\ntween the gold standard and predicted regions of\ninterest, using Intersection over Union (IoU) as the\nmetric: $IoU = \\frac{R \\cap R_{gold}}{R \\cup R_{gold}}$. We consider a predicted\nbounding box correct if its IoU with the ground-\ntruth bounding box is greater than 0.5, and report\noverall accuracy. It is crucial that models perform\nconsistently well across different cultures."}, {"title": "4.3 Models", "content": "We benchmark a series of models on our grounding\ntask, considering both specialist models, designed\nexplicitly for visual grounding tasks, and gener-\nalist models, which can handle a wide range of\nvision-language tasks, such as captioning, question\nanswering, and grounding. These models are listed\nin Table 5, along with their training data, vision and\nlanguage backbones, and training methodology.\nThe specialist model we include is Grounding\nDINO (Liu et al., 2023), a zero-shot object de-\ntection model that combines a Transformer-based\ndetector (DINO; Zhang et al., 2022) with phrase\ngrounding pre-training (GLIP; Li et al., 2022).\nThe generalist models are multimodal large lan-\nguage models (MLLMs). MLLMs encode visual\npatches as tokens that a language model can under-\nstand. They perform visual grounding by generat-\ning bounding boxes in textual format, typically in\nthe format of $\u27e8X_{left}\u27e9\u27e8Y_{top}\u27e9\u27e8X_{right}\u27e9\u27e8Y_{bottom}\u27e9$, denot-\ning the coordinates of the top-left and bottom-right\ncorners of the generated bounding box."}, {"title": "4.4 Results and Analysis", "content": "RQ1: Are VLMs able to identify culture-specific\nconcepts? Figure 3 presents the country-level ac-\ncuracy of each model on the cultural visual ground-\ning task. The overall performance across models\nis rather poor. Among all models, the specialist\nmodel Grounding DINO shows a relatively higher\naverage performance (47.99%) compared to the\ngeneralist models.\nAnalyzing country-specific performance, we ob-\nserve that KOSMOS-2 and QwenVL-7B exhibit\nstrong accuracy in grounding elements for Canada\nand Mexico. Grounding DINO, on the other hand,\nperforms well for Poland and the Philippines. All\ngeneralist models perform poorly on images from"}, {"title": "5 Conclusion", "content": "In this work, we introduced a challenging bench-\nmark, GLOBALRG, designed to evaluate the mul-\nticultural understanding of VLMs. GLOBALRG\nencompasses two tasks: retrieval of culturally di-\nverse images depicting universal concepts and vi-\nsual grounding of culture-specific concepts. Our\nfindings from extensive experiments across a wide\narray of VLMs reveal significant performance vari-\nations across cultures, highlighting the existence\nof biases in current VLMs. Moving forward, fu-\nture research should focus on collecting large-scale\nculturally diverse training datasets and devising\ntraining objectives that enhance models' represen-\ntations of images from diverse cultures, ultimately\npaving the way for developing more inclusive and\nfair downstream applications."}, {"title": "Limitations", "content": "While our benchmark, GLOBALRG, provides a\ncomprehensive evaluation of the multicultural un-"}, {"title": "Metric for diversity.", "content": "We currently employ a di-\nversity metric based on entropy to evaluate the cul-\ntural diversity of retrieved images. While this met-\ntric provides insights into the distribution of images\nacross different cultures, it may not fully capture\nthe nuanced variations in cultural representation.\nOur approach to regional diversity assessment may\nlack granularity, potentially overlooking finer dis-\ntinctions in cultural diversity within regions."}, {"title": "Ethical Consideration", "content": "Mapping from countries to regions. For the pur-\npose of our tasks, we mapped countries to broad\nregional categories as specified in Table 1. We ac-\nknowledge that cultures do not follow geographic\nboundaries and that this variation occurs at an in-\ndividual level, shaped by one's own life experi-\nences. Despite this, we used our mapping as a prac-\ntical starting point. This approach is a preliminary\nstep, with the ultimate goal of developing systems\nthat can learn from individual user interactions and\nadapt to diverse and evolving cultures.\nAnnotator selection and compensation Anno-\ntators hired from Cloud Research were predom-\ninately based in USA, Canada, Australia, New\nZealand, United Kingdom and Ireland. Participa-\ntion was strictly limited to those who met specific\ncriteria to maintain the relevance of the annotation\nprocess. Annotators were required to belong to a\nchosen ethnicity and to have lived in the designated\ncountries for at least 5 of the past 15 years. This\ncriterion ensured that participants had sufficient\ncultural context and lived experience relevant to\nthe annotation tasks. We employed a second round\nof annotators for the human evaluation phase, en-\nsuring none were repeated from the first round.\nInadvertent stereotypes in collect images. We\nrecognize that some images used to capture cultural\nconcepts might inadvertently perpetuate stereo-\ntypes. While our goal was to gather authentic cul-\ntural representations, we are aware of the ethical\nimplications of including such content. We ap-\nproached this task with the intention of collecting\nmeaningful cultural data while being mindful of\nthe potential for reinforcing harmful stereotypes."}]}