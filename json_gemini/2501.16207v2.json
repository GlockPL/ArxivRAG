{"title": "From Informal to Formal \u2013 Incorporating and Evaluating LLMs on Natural Language Requirements to Verifiable Formal Proofs", "authors": ["Jialun Cao", "Yaojie Lu", "Meiziniu Li", "Haoyang Ma", "Haokun Li", "Mengda He", "Cheng Wen", "Le Sun", "Hongyu Zhang", "Shengchao Qin", "Shing-Chi Cheung", "Cong Tian"], "abstract": "The research in AI-based formal mathematical reasoning has shown an unstoppable growth trend. These studies have excelled in mathematical competitions like IMO and have made significant progress. However, these studies intertwined multiple skills simultaneously\u2014problem-solving, reasoning, and writing formal specifications-making it hard to precisely identify the LLMs' strengths and weaknesses in each task. This paper focuses on formal verification, an immediate application scenario of formal reasoning, and breaks it down into sub-tasks. We constructed 18k high-quality instruction-response pairs across five mainstream formal specification languages (Coq, Lean4, Dafny, ACSL, and TLA+) in six tasks by distilling gpt-4o and evaluated against ten open-sourced LLMs, including recent popular DeepSeek-R1. We found that LLMs are good at writing proof segments when given either the code, or the detailed description of proof steps. Also, the fine-tuning brought about a nearly threefold improvement at most. Interestingly, we observed that fine-tuning with formal data also enhances mathematics, reasoning, and coding capabilities. Fine-tuned models are released to facilitate subsequent studies at https://huggingface.co/fm-universe.", "sections": [{"title": "Introduction", "content": "\"The more we formalize, the more of our implicit knowledge becomes explicit. \"\nTerence Tao (Tao, 2024)\nAs AI-based formal mathematical reasoning reached an inflection point (Yang et al., 2024b), significant attention and progress in this field have been observed. AlphaProof (AlphaProof and Teams, 2024) achieved silver medal level in the International Mathematical Olympiad (IMO), Alpha- Geometry (Trinh et al., 2024) specialized in proving Euclidean geometry theorems. As reported, the number of publications in this field nearly doubled in 2023, indicating an unstoppable growth trend (Li et al., 2024). As Fields Medalist Terence Tao imagined, \"In the future, instead of typing up our proofs, we would explain them to some GPT\u201d (Tao, 2024).\nHowever, most current benchmarks cannot precisely reflect the capability to convert informal proofs or requirements in natural language into formal proofs. Most of these benchmarks take mathematical problems (AlphaProof and Teams, 2024; Trinh et al., 2024; Welleck et al., 2021) or theorems to be solved (Yang et al., 2023; Welleck et al., 2022; Yang and Deng, 2019) as input, and informal or formal proofs (or parts of proofs) as output. However, these end-to-end benchmarks assess multiple capabilities (e.g., problem-solving, mathematical reasoning, formal specification writing) in an intertwined manner, making it difficult to isolate and observe LLMs' true capabilities in writing formal proofs or models for verification.\nTherefore, we break down the process from informal requirements to formal verifiable proof, as shown in Figure 1. Inspired by the code generation (shown in blue) which translates a description of implementation into executable code (Chen et al., 2021; Austin et al., 2021a), the formal reasoning process (shown in green) can be seen as translating an informal requirement into a verifiable formal proof or checkable formal model 1. Particularly, we decompose this process and formulate six tasks (Figure 2). By doing so, the intertwined capabilities can be separated and individually assessed, providing a clearer understanding of LLMs' strengths and weaknesses in each task.\nScope and Targets \u2013 We focus on formal verification (Appel, 2011; Klein et al., 2009; Leroy et al., 2016; Hawblitzel et al., 2014) because it is an immediate application scenario of formal mathematical reasoning and the correctness of the output can be verified mechanically. In this paper, we mainly explore four research questions (RQs):\nRQ1. How well do LLMs perform in various formal verification tasks? After decomposing the formal verification task into subtasks, we explore LLMs' initial performance in these tasks with zero- shot and few-shot, investigating the strengths and weaknesses that vary between LLMs and tasks.\nRQ2. Do LLMs show variability in their capability across different formal specification languages? When mathematicians and proof engi- neers consider using LLMs to assist in formal ver- ification, they often face uncertainty about which formal specification language is best supported by LLMs. This RQ is designed to provide hints on it.\nRQ3. Can fine-tuning improve LLMs' performance in formal verification? Although recent efforts have been made to fine-tune models (Wang et al., 2024; Yang et al., 2023), these LLMs are typ- ically fine-tuned with single formal languages in- stead of multi-lingual (e.g., combining Coq, Lean, etc.) (Yang et al., 2024b). Therefore, we instruction fine-tuned (Wei et al., 2021; Sanh et al., 2021) three base LLMs to see whether our constructed fine- tuning dataset FM-ALPACA could improve their capability in formal verification tasks.\nRQ4. Can fine-tuning with formal verification data benefit other related tasks (mathematics, rea- soning, code)? As recent works have shown LLMs' potential transferability of skills (Tihanyi et al., 2023) we thus extend our study to see if models fine-tuned on formal data could show enhanced capabilities in mathematics, reasoning, and coding.\nTo facilitate the study, we constructed 18k high-quality instruction-response pairs across five formal specification languages (i.e., Coq, Lean4, Dafny, ACSL, and TLA+) in six formal- verification-related tasks by distilling gpt-4o inspired by prior work (Wang et al., 2024; Ding et al., 2023; Wang et al., 2022), then split them into 14k instruction fine-tuning data (FM-ALPACA) and 4k benchmarking data (FM-BENCH). In particular, we provide executable contexts for all these formal specifications and automated validation scripts to validate the correctness of the generated formal proofs inspired from the prior work's artifact preparation (Jimenez et al., 2023). Finally, we release the fine-tuned LLMs based on three base models at https://huggingface.co/fm-universe.\nInterestingly, there has been recent discussion on the topic of domain transfer (Yang et al., 2024b), particularly the transfer of knowledge from other domains such as coding and reasoning to formal do- mains in order to increase LLMs' reliability (Spiess et al., 2024), and the anticipated potential of AI in enhancing formal verification processes to support mathematical proofs (Tao, 2024; Tao et al., 2023). Our experimental results could potentially provide empirical support for these hypotheses or offer di- rections for further experimental inquiries.\nThe contribution of this paper includes:\n\u2713 Problem Formulation: We decompose the formal verification process into six essential tasks. By doing so, the intertwined capabilities can be separated and individually assessed, providing a clearer understanding of LLMs' strengths and weaknesses in each task.\n\u2714 Dataset and Benchmark: We constructed 18k high-quality instruction-response pairs across five mainstream formal specification languages (i.e., Coq, Lean4, Dafny, ACSL, and TLA+) in six formal-verification-related tasks by distilling gpt- 4o. They are split into a 14k+ fine-tuning dataset FM-ALPACA and a 4k benchmark FM-BENCH.\n\u2713 Executable context and automated validation mechanism: We provide a Docker container equipped with necessary scripts to facilitate the evaluation of FM-BENCH, significantly lowering the entry barrier for this scenario and making sub- sequent contributions easier.\n\u2713 Insight and Vision: We fine-tuned several models on FM-ALPACA and observed promising benefits to not only the formal verification tasks, but also mathematics, reasoning, and coding. Our experimental results provide empirical support for the potential of LLMs' capability transfer and hope to shed some light on future research."}, {"title": "Task Formulation", "content": "Figure 2 illustrates the six sub-tasks. We elaborate on them in detail as follows.\nTask 1. Requirement Analysis (abbrev. ReqAna). Requirement analysis (Davis, 1990; Anton, 1996; Grady, 2010; Jin, 2017) is a critical and long- standing research area in software engineering. It facilitates collecting, identifying, categorizing and modeling the users' needs and expectations using various techniques (Taggart Jr and Tharp, 1977; Deeptimahanti and Babar, 2009; Javed and Lin, 2021; Wang et al., 2021; Jin et al., 2024; Zhou et al., 2022). In this paper, the requirements are the descriptions in natural language (English) (Jin et al., 2024) that details the requirements of the ver- ification/modeling goal and an overall description of the proofs/models. The task is to analyze and break down the final goal into detailed steps de- scribed in natural language. The natural language used in this paper is English.\nTask 2. Full Proof Generation (abbrev. Proof- Gen). This task formalizes a requirement in natural language into verifiable proofs or models written in formal specification languages, similar task for- mulation to existing works (Fatwanto, 2012; Zhou et al., 2022; Davril et al., 2013).\nTask 3. Proof Segment Generation (abbrev. SegGen). Unlike ProofGen, which requires gen- erating complete proofs/models, SegGen provides more detailed descriptions in natural language and requires LLMs to write less. Given a text description articulating how to implement the proofs/mod- eling, the task outputs a segment written in the formal specification that serves as a component in the complete proof/model. This task formulation is similar to prior work (Wang et al., 2024; Wu et al., 2022; Jiang et al., 2022) and similar to the formulation of code generation (Chaudhary, 2023; Sun et al., 2024; Welleck et al., 2022, 2021).\nTask 4. Proof Completion (abbrev. ProofCom- plete). Similar to code completion (Raychev et al., 2014; Husein et al., 2024; Svyatkovskiy et al., 2019; Dakhel et al., 2023), ProofComplete sug- gests the suffix of the given prefix, similar to prior work (Song et al., 2024). Note that in order to prevent LLMs from deviating from the original ver- ification goal, we also provide the requirement in our evaluation, although it is not compulsory for this task formulation.\nTask 5. Proof InFilling (abbrev. ProofInfill). Given a proof/model with a mask in the middle, the task requires LLMs to fill proper formal speci- fications so that the completed proofs/models can pass the verifier. This formulation is the same as code infilling (Fried et al., 2022). Also, similar to ProofComplete, we provide the requirement in our evaluation during the infilling to prevent LLMs from deviating from the original verification goal.\nTask 6. Proof Generation from Code (abbrev. Code2Proof). In addition to generating formal spec- ifications from natural languages, formal specifi- cations can also be generated from code if the ver- ification goal is the property of a given program. In this paper, we focus mainly on specifications in form of code annotations (Baudin et al., 2021; Hatcliff et al., 2012), expressing specifications (e.g., pre-/post-condition, loop invariants) that help one to verify that (part of) a program satisfies certain properties. The task takes the code with properties to be verified as input and outputs the code with generated annotated formal specifications. Similar task formulation can be found in recent works (Wen et al., 2024; Ma et al., 2024)."}, {"title": "Data Construction", "content": "In this study, we consider five formal specifica- tion languages that can be used for formal verifi- cation, including Coq (Huet, 1986), Dafny (Leino, 2010), Lean4 (Moura and Ullrich, 2021), ACSL (ANSI/ISO C Specification) (Baudin et al., 2021)\nand TLA+ (Yu et al., 1999; Lamport, 2002). We selected them in order to cover various verification paradigms (i.e., theorem proving and model check- ing) and verification scenarios (e.g., mathematical reasoning and program verification).\nFirst, for interactive theorem provers which are suitable for developing rigorous mathematical proofs, we consider Coq (Huet, 1986) and Lean4 (Moura and Ullrich, 2021) because Coq has been extensively used in academia and research for proving mathematical theorems and in formal verification of software for a long history, while Lean4 garnered considerable attention from the mathematical community (Wang et al., 2024; Tao et al., 2023; Avigad et al., 2020) recently. Second, for programming languages with built-in specification, we consider Dafny (Leino, 2010) and ACSL (Baudin et al., 2021; Cuoq et al., 2012) be- cause they seamlessly integrate specifications (e.g., pre-/post-conditions, loop invariants) within the code, ensuring the correctness through embedded assertions and conditions. Lastly, for model checking (Jhala and Majumdar, 2009; Clarke, 1997), we consider TLA+ (Yu et al., 1999) since it is a repre- sentative math-based formal language for modeling algorithms and programs such as concurrent and distributed systems.\nThe workflow of data preparation for FM-ALPACA and FM-BENCH is illustrated in Figure 3. The work- flow begins with the data collection, where formal proofs in the desired formal specification languages and related configurations and dependencies are gathered from open-source repositories in Github. Then, formal proofs are extracted from the collected repositories. Next, the proofs go through the data quality assurance check by execution, the proofs that cannot be verified successfully are fil- tered out. The remaining ones are split into seg- ments (e.g., definition of functions or conditions).\nGiven the impracticality of manually writing descriptions for all the collected formal proofs, we leveraged distilled GPT4 (gpt, 2023) to gen- erate high-quality informal proof descriptions via meticulous prompting. This alternative is well- established and frequently employed in prior liter- ature (Wang et al., 2022, 2024; Ding et al., 2023; Wang et al., 2023). Specifically, for each formal specification language, we designated the model as an expert in that particular language, equipping it with comprehensive domain knowledge about the language's specifications, essential grammati- cal cues, and three-shot examples featuring proof segments in the formal language as inputs and natu- ral language descriptions as outputs. This approach ensures that the collected descriptions are of high quality and well-organized. It's important to note that we did not generate descriptions for proof seg-"}, {"title": "Experiments", "content": "We selected ten LLMs as base- lines without fine-tuning, including llama3.1- instruct-8B/70B (Meta, 2024), qwen2.5-instruct- 7B/72B (Yang et al., 2024a), qwen2.5-coder- instruct-7B/-32B (Hui et al., 2024), starcoder- instruct-15B (Lozhkov et al., 2024), deepseek- coder-instruct-7B-v1.5, deepseek-coder-instruct- 33B (Guo et al., 2024), and deepseek-R1 (Guo et al., 2025). Note that we avoid evaluating the GPT-series LLMs by OpenAI because the descrip- tions in FM-BENCH were generated by GPT-40, making the evaluation fairer.\nFine-tuning. Instruction fine-tuning (Wei et al., 2021; Sanh et al., 2021; Ding et al., 2023; Ivison et al., 2023) aims to improve a model's ability to effectively respond to human instructions and has shown strong experimental potential in model en- hancement. We select llama3.1-8B (Meta, 2024), qwen2.5-7B (Yang et al., 2024a), and deepseek- coder-7B-v1.5 (Guo et al., 2024) as base models for fine-tuning. We selected these three models because they have shown promising capability in tasks such as coding, mathematics, and reasoning, and fine-tuning models in their scale is relatively affordable compared with fine-tuning larger scale models. We fine-tuned the three aforementioned base models over three epochs using a learning rate 2e-5, a warm-up ratio of 0.04, a batch size of 512, and a cosine learning rate scheduler.\nBaseline Fine-tuning Datasets: To distinguish whether the capability improvement is simply be- cause more instruction tuning is applied, we also include two commonly used fine-tuning datasets for comparison. We select UltraChat (Ding et al., 2023) and Tulu-V3 (Lambert et al., 2024) as base- line fine-tuning datasets for their popularity. In particular, UltraChat is a large-scale dataset of in- structional conversations that contains 1.5 million high-quality multi-turn dialogues and covers a wide range of topics and instructions. Tulu-v3 (Lam- bert et al., 2024) embraces new data that is either carefully manually curated for quality or generated from GPT models. It is an enhancement of its previous versions (Ivison et al., 2023; Wang et al., 2023), focusing more on core skills of knowledge recall, reasoning, mathematics, coding, instruction following, general chat, and safety.\nBenchmarks for Related Capabilities (RQ4). To comprehensively evaluate the model's capabili- ties, we tested the fine-tuned models on a series of benchmarks: Math (Hendrycks et al., 2021) and GSM-8K (Cobbe et al., 2021) for mathematical reasoning, BBH (Suzgun et al., 2022) for gen- eral reasoning, HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021b) for coding.\nInference Strategies: We adopt different set- tings for different RQs. In particular, We use (1) the greedy sampling strategy to generate one single greedy sample with a temperature of 0.0 and calculate Pass @1, and (2) nucleus sampling (Holtzman et al., 2020), where five solution samples were ran- domly generated with a temperature of 0.2 for RQ1 and RQ2. We also consider different in-context learning strategies, including zero-shot and few- shot (we used 3-shot in the experiment). For RQ3, we use a zero-shot greedy search with a temper- ature of 0.0 and a few-shot nuclear search with a temperature of 0.2 for a fair comparison.\nExperiment Environment. The fine-tuning experiment was conducted on 32 Nvidia A100-40G GPUs, while inference was on a single Nvidia A100-80G GPU with vLLM (Kwon et al., 2023)."}, {"title": "RQ1. Basic Performance across Formal Specification Tasks", "content": "To understand the current LLMs' performance in six tasks, we evaluate 8 LLMs against FM-BENCH with model size ranges from 7B to 72B. The upper part of Table 3 and the upper part of Table 4 show LLMs' basic performance without fine-tuning.\nTask-wise: LLMs perform the best in generat- ing proof from code (Code2Proof), with an aver- age of 43.57% Pass@1, followed by ProofCom- plete (18.44%) and ProofInfill (17.38%). In con- trast, LLMs fall short in generating both the en- tire formal proof (8.65%) and the proof segments (10.61%). We analyzed the failures and found that syntax errors account for a large proportion, with 12.15% failures caused by syntax errors (Ap- pendix B). The observation echoes the motivation of prior work (Wang et al., 2024) and is reasonable due to the grammar difference between most formal specification languages and other programming lan- guages like Python. Regarding requirement anal- ysis, as shown in the upper part of Table 4, the Bleu scores between the ground-truth description and LLM-generated ones range from 0.24 to 0.55.\nLLM-wise: Without fine-tuning, DeepSeek-R1 achieved the best average (27.11%), followed by qwen2.5-coder-instruct-32B (21.19%).\nModel Size \u2013 Larger LLMs generally perform better than smaller LLMs. For example, llama3.1- 8B only achieved 1.43% in generating TLA+ seg- ments, while llama3.1-70B boosts to 22.86% in the same task. However, there are several excep- tions worth noticing, especially for ProofInfill and Code2Proof. For example, llama-3.1-8B achieved 50% in ProofInfill (ACSL), yet the performance drops to 21.43% using the 70B model. Similar ob- servations can be found in qwen2.5-instruct. The decrease in performance is inherently due to the fine-tuning strategy of these instruction models: they are trained to excel in generating rather than filling in the blanks (Fried et al., 2022). Also, we conducted a more detailed examination of gener- ated segments and observed that larger LLMs tend to fill in the proof segments that not only look more plausibly correct and well-organized but also in- clude extra content. The additional content, yet, is either redundant, as it repeats information that appears in the subsequent proof, or is incomplete. Promisingly, recent model developers have noticed such conundrums and refined their fine-tuning strat- egy for fill-in-the-middle tasks (Guo et al., 2024)."}, {"title": "RQ2. Formal Specification Languages-wise Capability", "content": "Table 5 shows the LLMs' performance across for- mal specification languages in the task of generat- ing proof segments (SegGen). This task accounts for the most instructions and serves as the basic capability for other proof generation tasks. We can see that LLMs perform the best in ACSL (aver- age: 34.92%), followed by Dafny (15.97%) while performing unsatisfactorily in other formal speci- fication languages. The observation is reasonable because the syntax of ACSL is basically an anno- tation of C language, while Dafny shares similar grammar as C# and Java. Thus, generating proof segments in ACSL and Dafny is generally easier than generating other specification languages.\nIn addition, we explore whether increasing the attempts (1 \u2192 5) with a higher temperature (0.0 \u2192 0.2) and in-context learning could bring about improvement. The improvement ratios are shown in red in Table 5. The results of Pass@5 are bet- ter than those of Pass@1, with an average score increase from 10.82% (Dafny) to 63.64% (ACSL) in different languages. Moreover, when using 3- shot, the performance increases dramatically, with 51.33% (Dafny) to over five times (ACSL) improve- ment compared with zero-shot Pass@5. The results indicate the potential of in-context learning in gen- erating correct specification languages."}, {"title": "RQ3. Improvement by Fine-tuning", "content": "We further investigate whether FM-ALPACA could bring about improvement. The lower part of Ta- ble 3 and Table 4 shows the results. From Table 3, dramatic improvements can be observed in gener- ating full and segmental proofs after fine-tuning. Note that the model size of fine-tuned models is 7B ~ 8B, while the performance largely outperforms the 70B+ models without fine-tuning. Furthermore, after fine-tuning with formal data, the 7 ~ 8B fine- tuned models can achieve comparable or slightly better performance than Deepseek-R1-671B, with 27.31% achieved by qwen2.5-coder-7B fine-tuned with FM-ALPACA (R1-671B: 27.11%). It may sug- gest the possibility of distilling domain-specific small models for handier usage.\nTask-wise: Improvements in generation tasks (i.e., ProofGen, SegGen, and ProofComplete) are substantial. ProofGen doubles the performance, and SegGen more than triples. The dramatic in- creases happen in all models fine-tuned with FM- ALPACA in SegGen Task, from nearly all zeros to 29.98% ~90.48%. An increase of 41% can also be observed in Table 4. The experimental improve- ments make evident the effectiveness of fine-tuning in formal verification tasks.\nYet, drops can be observed in fill-in-the-middle tasks (i.e., ProofInfill and Code2Proof). The results echo the observation made in RQ1 (Section 4.2), where the large LLMs perform worse than small LLMs in fill-in-the-middle tasks. The results also indicate the necessity of adopting different fine- tuning strategies other than instruction tuning only.\nFine-tuning Datasets: Take a closer look at the LLMs fine-tuned with general-purpose datasets (i.e., llama3.1-ultrachat and llama3.1-tulu) in Ta- ble 3, with them only, no or opposite effects can be observed. The results indicate the complemen- tarity of FM-ALPACA and existing general-purpose fine-tuning datasets. Additionally, by combining with general-purpose datasets, the performance can be further improved (e.g., llama3.1-tulu-fma).\nComparison with Few-shot: Compared with the best results in Table 5 achieved by 3-shot, the results after fine-tuning (Table 3) still generally out- perform the 3-shot results. The results indicate that although in-context learning can improve LLMs' performance, the enhancement is limited. Further significant improvements still require fine-tuning with formal data. This may also suggest that in-context learning alone cannot adequately address capability deficits in formal verification tasks but rather stem from a lack of knowledge."}, {"title": "RQ4. Capability Migration from Formal Verification to Related Tasks", "content": "Finally, we explore whether fine-tuning with FM- ALPACA could benefit related capabilities. Table 6 shows the results. The base model is llama3.1-8B, fine-tuned under two base fine-tuned datasets with and without FM-ALPACA. On average, with FM- ALPACA, an increase of 1.37% to 5.15% can be ob- served. Interestingly, a dramatic increase (62.53%) can be observed in HumanEval compared with the performance of the model that is only fine-tuned with UltraChat. The experiment may indicate that feeding more formal data may improve LLMs' cod- ing, reasoning, and math capabilities."}, {"title": "Conclusion", "content": "This paper contributes a comprehensive assessment and formulation to understand LLMs' capability in formal verification. We constructed 18k high- quality instruction-response pairs across five for- mal specification languages in six tasks. The fine- tuned models, fine-tuning data, and the benchmark are released to facilitate subsequent studies."}, {"title": "Limitations", "content": "This paper has two primary limitations that offer avenues for future research. First, the primary limi- tation of our work is that our benchmark relies on model-generated data. While this approach effec- tively reduces manual efforts; it may introduce bi- ases and data leakage issues in the dataset towards the models that generated the data. To address this limitation, we use gpt-40 to generate the natural language descriptions, while during the evaluation, we use other LLMs for evaluation. Second, another limitation of our work lies in the validation design. When creating ProofInfill and ProofComplete data, it is possible that the properties to be verified or theorems to be proven are masked. If LLMs hap- pened not to generate these properties/theorems, the generated \u201cproofs/models\" could escape the verifier/checker, mistakenly labeling the output as correct. To avoid this scenario, we include the requirement descriptions as part of the input, guid- ing LLMs to generate the necessary properties or theorems without omission."}, {"title": "Related Work", "content": "The formal specification datasets or benchmarks offer a standard, well-defined set of problems, pro- viding a shared challenge that helps build a commu- nity of practice among researchers. According to different verification techniques, the existing bench- marks mainly fall into two categories; we discuss them separately."}, {"title": "Theorem Proving Datasets", "content": "Formal theorem proving represents theorems and proofs in a machine-verifiable format (Cook, 2023), ensuring their correctness using rigorous logical rules. A recent survey (Li et al., 2024) summarized the existing datasets for theorem proving. In par- ticular, the informal benchmarks craft the proofs from various sources such as ProofWiki, textbooks, and public corpus. NL-PS (Ferreira and Freitas, 2020) first builds a natural language premise se- lection dataset source from ProofWiki. Similarly, NaturalProofs (Welleck et al., 2021) further incor- porates data from Stacks and textbooks, resulting in a dataset with roughly 25k examples. Adapted from it, NaturalProofs-Gen (Welleck et al., 2022) con- tains around 14.5k theorems for informal proof gen- eration. Moreover, MATCH (Li et al., 2023) con- structs over 180k statement-proof pairs for match- ing using the MREC corpus 2.\nFor formal datasets, a line of efforts focuses on extracting and cleaning theorems and proofs writ- ten in various specification languages (e.g., Coq, Is- abelle, Lean) from established formal libraries and verification projects. For example, LeanDojo (Yang et al., 2023) extracts over 98k theorems and proofs with 130k premises from Lean mathlib (mathlib Community, 2020). Besides extracting data from existing projects, several works manually annotate or formalize the problems in natural language. For example, MiniF2F (Zheng et al., 2021) manually formalizes 488 Olympiad-level problems across 4 proof systems and equally splits them into a valida- tion set and a test set. FIMO (Liu et al., 2023) and ProofNet (Azerbayev et al., 2023) formalize the theorem statements of the International Mathemat- ical Olympiad and undergraduate-level problems in Lean. In addition, datasets for Dafny also at- tract research contributions because industries like Amazon adopted Dafny to verify cryptographic libraries, authorization protocols, a random num- ber generator, and the Ethereum virtual machine."}, {"title": "Model checking datasets", "content": "Model checking is an automated technique used in computer science and formal methods to verify the correctness of systems, particularly those with finite state spaces. It systematically checks whether a system's model satisfies a given specification, usually expressed in formal specification languages. The basic idea is to explore all possible system states to ensure the desired properties hold in every conceivable scenario.\nModel checking benchmarks are less than that for theorem proving. Currently, there are few model-checking benchmarks for proving, while several model-checking subjects are going with specific model-checking languages such as CMur- phi (Della Penna et al., 2013) and TLA+ (Yu et al., 1999). In particular, CMurphi is a software tool used to verify concurrent and distributed systems through explicit state enumeration. It implements the Murphi verification language, which allows users to describe finite-state systems in a procedu- ral style. The core principle behind CMurphi is to explore the state space of a system exhaustively to check for violations of specified invariants or prop- erties. Another example is TLA+ (Temporal Logic of Actions), a high-level language for modeling programs and systems suitable for concurrent and distributed systems."}, {"title": "Proportion of Failures Caused by Syntax Error", "content": "We listed the proportions of failures caused by syn- tax errors for each LLM and each task in Table 8. We used a set of pre-defined keywords (summa- rized in Table 7 to identify if a verification failure is caused by syntax errors. Specifically, we con- sider a failure caused by syntax error if its error message contains at least one keyword in Table 7."}, {"title": "Example Specifications in FM-ALPACA and FM-BENCH", "content": "The examples of the five formal specification lan- guages are shown in Figure 4."}, {"title": "Collected Repositories", "content": "We listed the repositories that were collected for data construction in the following. Note that one can easily add more repositories into FM-ALPACA and FM-BENCH.\nFor ACSL:\n\u2022 https://github.com/manavpatnaik/ frama-c-problems\n\u2022 https://github.com/fraunhoferfokus/ acsl-by-example\nFor TLA+:\n\u2022 https://github.com/tlaplus/Examples\nFor Lean4:\n\u2022 https://github.com/leanprover/lean4\nFor Coq:\n\u2022 https://github.com/coq/coq\nFor Dafny:\n\u2022 https://github.com/vladstejeroiu/ Dafny-programs"}, {"title": "Complete Evaluation Result", "content": "The Pass@1 and Pass@5 are shown in Table 9. It is a completed version of Table 3."}, {"title": "Prompt Design", "content": "We listed the prompts that are used for data prepa- ration and inference in the following. For data preparation", "parts": 1}]}