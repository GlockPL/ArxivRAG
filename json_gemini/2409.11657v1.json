{"title": "Few-Shot Class-Incremental Learning with Non-IID Decentralized Data", "authors": ["Cuiwei Liu", "Siang Xu", "Huaijun Qiu", "Jing Zhang", "Zhi Liu", "Liang Zhao"], "abstract": "Few-shot class-incremental learning is crucial for developing scalable and adaptive intelligent systems, as it enables models to acquire new classes with minimal annotated data while safeguarding the previously accumulated knowledge. Nonetheless, existing methods deal with continuous data streams in a centralized manner, limiting their applicability in scenarios that prioritize data privacy and security. To this end, this paper introduces federated few-shot class-incremental learning, a decentralized machine learning paradigm tailored to progressively learn new classes from scarce data distributed across multiple clients. In this learning paradigm, clients locally update their models with new classes while preserving data privacy, and then transmit the model updates to a central server where they are aggregated globally. However, this paradigm faces several issues, such as difficulties in few-shot learning, catastrophic forgetting, and data heterogeneity. To address these challenges, we present a synthetic data-driven framework that leverages replay buffer data to maintain existing knowledge and facilitate the acquisition of new knowledge. Within this framework, a noise-aware generative replay module is developed to fine-tune local models with a balance of new and replay data, while generating synthetic data of new classes to further expand the replay buffer for future tasks. Furthermore, a class-specific weighted aggregation strategy is designed to tackle data heterogeneity by adaptively aggregating class-specific parameters based on local models performance on synthetic data. This enables effective global model optimization without direct access to client data. Comprehensive experiments across three widely-used datasets underscore the effectiveness and preeminence of the introduced framework.", "sections": [{"title": "I. INTRODUCTION", "content": "EEP models have achieved remarkable success in various fields, attaining performance levels that are close to human capabilities. However, we humans have the ability to quickly acquire new information from an ongoing series of tasks while retaining previously gained knowledge, which poses significant challenges to deep models. In recent years, continual learning [1], also referred to as lifelong or incremental learning, has garnered increasing attention. Class-incremental learning (CIL) [2]-[4] as one of the most challenging scenarios, requires models to continuously learn new classes without the presence of task identification.\nThis work explores a decentralized machine learning paradigm referred to as Federated Few-Shot Class-Incremental Learning (F2SCIL). As depicted in Fig. 1, clients (data owners) continually update their local models with scarce data to adapt to novel classes. Subsequently, the updated parameters from local models are transmitted to a server for aggregation, producing a unified model that leverages distributed data across multiple clients while preserving data locality. However, the F2SCIL paradigm faces several challenges. Due to the scarcity of data accessible to each client (i.e., few-shot learning), the model encounters difficulties in learning new classes. Secondly, catastrophic forgetting is a significant issue as the model continually updates to incorporate new classes, potentially degrading its performance on previously learned classes. Thirdly, variations in data categories and sample sizes across clients, often described as non-Independent and Identically Distributed (Non-IID), result in data scarcity and heterogeneity. These factors exacerbate catastrophic forgetting and pose substantial difficulties in designing an effective aggregation strategy for refining the global model.\nThis paper handles these challenges by presenting a novel Synthetic Data-Driven (SDD) framework. In the common CIL setting, data typically arrives in a stream, with each task or session introducing a new set of classes. Within the proposed SDD framework, each client updates its local model with newly acquired data to integrate new classes and utilizes synthetic data from the replay buffer to retain earlier knowledge. Although some methods [9]-[11] leverage the outputs of old models on replay data to guide the model learning, these knowledge distillation-based methods exhibit instability in FSCIL. Drawing inspiration from [12], we resort to re-labeling synthetic replay data with one-hot labels, subsequently incorporating these pseudo-labeled data into the current session's dataset. However, a general consensus [13] in generating synthetic data is to include hard samples to enhance sample diversity and represent more class characteristics. This also implies that relabeling synthetic data may introduce noisy labels, adversely affecting local model learning. To address this, we develop a Noise-Aware Generative Replay (NAGR) module which fine-tunes local models using both new data and synthetic replay data to balance learning across old and new classes. Subsequently, a conditional generator is constructed based on the ensemble of local models with the aim of producing synthetic data for the new classes in this session. These synthetic data are further added to the replay buffer to assist the local model learning in future sessions.\nFurthermore, a novel Class-Specific Weighted Aggregation (CSWA) strategy is devised to deal with the degradation of global model performance caused by data heterogeneity among clients. This strategy calculates the weights of class-specific parameters in model aggregation by evaluating the performance of local models on each new class. Notably, the original data cannot leave its owner in the F2SCIL paradigm. Therefore, we innovatively use the generated synthetic data in this session to simulate new class distributions, which facilitates the performance evaluation of local models and supports adaptive aggregation of the global model.\nThis paper makes the following key contributions:\n\u2022 We initiate the exploration of the F2SCIL paradigm, specifically targeting scenarios where a global model continuously learns new classes from limited data distributed across multiple clients.\n\u2022 We propose a new framework called SDD, which utilizes synthetic data to memorize previously learned knowledge in client-side incremental learning and improve server-side model integration.\n\u2022 We highlight the potential issue of noisy labels in synthetic data and propose a Noise-Aware Generative Replay (NAGR) module that alleviates catastrophic forgetting in a data-free manner.\n\u2022 We point out that the data scarcity in F2SCIL makes the non-IID nature of data from different clients particularly challenging. To this end, we devise a Class-Specific Weighted Aggregation (CSWA) strategy aimed at achieving efficient fusion of client models."}, {"title": "II. RELATED WORK", "content": "A. Few-shot class-incremental learning\nCIL allows models to acquire knowledge from an ongoing stream of training data while minimizing the loss of previous information. Mainstream approaches fall into four categories. First, replay-based methods [9], [14], [15] incorporate data from earlier tasks while learning new ones to help maintain the model's memory. Second, regularization-based methods [16], [17] modify the loss function by including regularization terms to steer the optimization direction. Third, model-based methods [18]-[22] employ distinct model architectures or parameters for each task to prevent potential forgetting. Lastly, algorithm-based methods [11], [23] focus on designing algorithms that preserve the knowledge from earlier tasks.\nFSCIL is more challenging since it requires the model to acquire new classes from limited annotated samples. The TOPIC framework [5] is the first to introduce the concept of FSCIL, employing Neural Gas networks to map the spatial relationships within the feature space across various classes for knowledge encoding. [12] presents an entropy-regularized data-free replay method that learns a generator to produce synthetic data. S3C [24] addresses the FSCIL task through a stochastic classifier and a self-supervision approach. CEC [6] integrates the topological structure of graph models with incremental models. FACT [7] tackles this task from the perspective of forward compatibility, allocating multiple virtual prototypes in the feature space as a preservation space. ALICE [25] integrates an angular penalty loss to enhance feature clustering, training the backbone network with both base class data and synthetic data. This work creates additional space for accommodating new classes and utilizes cosine similarity for classification. NC-FSCIL [8] proposes a neural collapse-based framework to reduce the divergence between features of old classes and well-trained classifiers.\nAlthough existing FSCIL methods have shown promising results, they generally perform well under the assumption of centralized training data storage. As a result, these methods may struggle to address challenges in distributed training scenarios, where the data from each incremental session comes from different clients.\nB. Federated continual learning\nFederated Learning [26] is a decentralized approach that allows multiple data sources to collaboratively develop a unified model. Recently, it has achieved considerable success in various research and industry areas. Federated Continual Learning (FCL) is a newly emerging topic that integrates the concepts of federated learning with continual (or incremental) learning. In addition to the catastrophic forgetting issue on the client side, the FCL task also introduces new challenges, notably managing interference between clients and improving communication performance. FedWeIT [27] tackles these issues by breaking down parameters into global, local, and task-adaptive components. Each client selectively assimilates information from others through a weighted aggregation of task-adaptive parameters. However, due to communication overhead, FedWeIT still requires a buffer for data rehearsal, which is deemed infeasible in our F2SCIL paradigm where samples are limited. CFeD [28] combines FCL with knowledge distillation, where the model obtained in the previous task acts as a teacher. Additionally, a client-side distillation strategy is introduced to address the data heterogeneity problem. [29] utilizes prototype networks, leveraging a server model that has been pre-trained in the initial task. Nevertheless, unlike the proposed FCL setting, this work argues that new classes can overlap with old classes, which is not consistent with standard class-incremental learning. The TARGET framework [30] demonstrates that non-IID data exacerbate catastrophic forgetting in federated learning and utilizes previously learned global model to train a generator, transferring knowledge from earlier tasks. DCID [31] designs a decentralized incremental learning paradigm that requires to share some samples of clients for knowledge distillation. MFCL [32] develops a data-free generative model learned at the server side to mitigate catastrophic forgetting and ensure data privacy.\nHowever, these FCL methods rely on collecting and integrating abundant data from multiple sources and struggle to gain sufficient knowledge from limited data in our F2SCIL paradigm. This data scarcity not only complicates the task of distinguishing new classes from existing ones but also exacerbates the model discrepancies caused by the heterogeneity of data across clients.\nC. Data-free replay method\nThe data-free replay methods in CIL generally require training a generator model to produce replay data, which are used to retain previously learned knowledge. DeepInversion [33] introduces a model inversion technique that creates synthetic data for specific classes using random perturbations. Some methods [34], [35] employ generative adversarial networks to synthesize data, utilizing the previously learned model as a discriminator while optimizing the generator with a teacher-student architecture. Recently, another group of methods [36], [37] integrate the idea of generative replay with deep inversion to address this problem. [12] also attemptes to add entropy loss to the training of the generator to produce more uncertain replay samples in the FSCIL scenario.\nIn federated learning, FedGen [38] develops a compact generator to enhance client-side training and consolidate the knowledge from local models without relying on previous data. FedFTG [13] investigates the input space of local models using a server-side generator to produce synthetic data and carries out real-time fine-tuning of the global model through data-free knowledge distillation. DENSE [39] is a data-free one-shot federated learning framework that applies data-free approaches to federated learning for model aggregation.\nUnlike the previous methods, we train a conditional generator with an ensemble of local models to continuously produce synthetic data for new classes in the F2SCIL paradigm. The synthetic data provide replay samples to mitigate catastrophic forgetting in client-side FSCIL and also serve as a validation set for assessing local models, facilitating more effective model aggregation."}, {"title": "III. METHOD", "content": "A. Problem formulation\nFirst, we introduce the F2SCIL paradigm including a base session to learn data-abundant classes and T incremental sessions for novel and data-scarce classes. Let D = {D(t)}_{t=0:T} represents the collection of training datasets corresponding to sessions indexed from 0 to T. Each D(t) is defined as D(t) = {(x,y) | x \u2208 X(t), y \u2208 Y(t)}, including examples from an image set X(t) annotated with labels taken from a fixed group of categories Y(t). Note that the training data for different sessions are mutually exclusive, i.e., X (p)\u2229X(q) = \u00d8 and Y(p) \u2229Y(q) = \u00d8 for p \u2260 q. Furthermore, D(0) is a centralized dataset used in the base session to train an initial model \u03b8(0), while the data in each D(1), D(2), . . ., D(T) is distributed across multiple clients for the subsequent incremental sessions. The goal of the t-th incremental session is to evolve the model \u03b8(t-1) into \u03b8(t), utilizing the corresponding data D(t).\nSpecifically, during the t-th incremental session, the limited data in D(t) is organized in the N-way K-shot format, where the label space Y(t) comprises N distinct classes, each with a total of K training images. Notably, D(t) is distributed heterogeneously (i.e., non-IID) among M clients in the F2SCIL paradigm. In simpler terms, D(t) comprises data sourced from M clients, where the data belonging to the m-th client is denoted as D_m^{(t)}.\nB. The overview framework\nTo address the challenges in F2SCIL as previously discussed, we propose a data-free framework called SDD. The overview of SDD is depicted in Fig. 2. For the base session, we utilize the dataset D(0) to learn an initial model \u03b8(0) in a centralized manner. Following the traditional supervised learning framework, \u03b8(0) is optimized by the cross-entropy loss function to ensure strong classification capabilities for base classes. Next, the initial model is employed to provide supervisory signals for training a conditional generator, which can effectively capture the key feature distributions of the base classes and generate highly representative pseudo samples. These generated samples are then added to a pre-established replay buffer, providing knowledge of previously learned classes for subsequent incremental sessions.\nFor incremental session t, the model \u03b8^{(t-1)} is updated to \u03b8^{(t)} through distributed learning on limited data of the new classes D(t). First, the m-th client performs FSCIL to optimize its local model \u03b8_m^{(t)}. This process involves learning and adapting to the new classes using local data D_m^{(t)}. To avoid catastrophic forgetting, a set of pseudo samples are drawn from the replay buffer to review and consolidate knowledge of the previously learned classes. Next, a conditional generator is constructed using the ensemble of local models, aiming to produce pseudo samples that represent the new classes in the current incremental session. These newly generated pseudo samples are then employed as validation data to assess how well the local models perform on the new classes, resulting in a class-specific weight matrix to guide the aggregation of the global model \u03b8^{(t)}. At last, pseudo samples of new classes are further incorporated into the replay buffer, providing knowledge of these classes for subsequent incremental sessions.\nC. Few-shot class-incremental learning\nOn each client, our goal is to train a local model from the global model learned in the previous session. In incremental session t, the training of model \u03b8_m at client m can be defined as a local FSCIL problem, with the primary challenges of catastrophic forgetting and model overfitting. We present an NAGR module that leverages synthetic replay data to retain previously learned knowledge and addresses the potential issue of noisy labels in synthetic data using a noise-robust loss.\nSpecifically, knowledge preservation for the old classes is achieved by using a set of synthetic data D_{syn}^{(t)} drawn from the replay buffer. As confirmed by [9], [23], knowledge distillation is a typical and effective method in class-incremental learning, utilized to transfer knowledge from previous tasks through replay samples. However, the recent work [12] points out that it is non-trivial to balance the classification loss for new samples with the distillation loss for replay samples in FSCIL scenarios. So it suggests re-labeling the synthetic replay samples using one-hot encoding based on the old model and applying classification loss to these pseudo-labeled data. This approach allows the synthetic dataset to be represented as D_{syn}^{t} = {(x,y^*)} , where x and y^* indicate a synthetic sample and its pseudo-label.\nHowever, we argue that there exist noisy labels in D_{syn}^{(t)} due to the inclusion of hard samples in synthetic data generation to enhance sample diversity. These noisy pseudo-labels can serve as misleading signals and negatively impact the local model learning. Therefore, it is imperative to develop a mechanism that makes the learning process more robust to such noise, ensuring that the model can accurately capture the previously learned knowledge present in the synthetic replay data.\nTo this end, we introduce a noise-robust loss function [40], which optimizes the local model with the synthetic replay data of old classes by\nL_{old}(x, y^*; \\theta_m) = \\alpha CE(\\theta_m(x), y^*) + \\beta RCE(\\theta_m(x), y^*)\n= -\\alpha \\Sigma_{c \\in C_B} y^*log(\\theta_m(x)) + \\beta\\Sigma_{c \\in C_B} \\theta_m(x)log(y^*),\nwhere \u03b8_m(x) indicates the probability distribution output from model \u03b8_m given an input x. CB represents the number of previous classes, \u03b1 and \u03b2 are hyper-parameters to balance the two items. The combination of CE and RCE exhibits a symmetry. The CE loss ensures the accurate prediction of true labels, thereby minimizing classification errors on clean data.\nMeanwhile, RCE mitigates the influence of noisy labels by relying on the model's predicted distribution \u03b8_m(x). This approach encourages the model to focus on replay samples with high confidence, enhancing its robustness to noisy pseudo-labels.\nFor few samples of new classes, the model is trained using the standard cross-entropy loss. This process is formulated by\nL_{new}(x, y; \\theta_m) = CE(\\theta_m(x), y),\nwhere (x, y) is a sample data pair from the local dataset D_m^{(t)} in the current incremental session.\nFinally, the total objective function L_{client} for optimizing the local model \u03b8_m can be formulated as a combination of L_{new} and L_{old}.\nL_{client} = L_{new} + k \\cdot L_{old},\nwhere k indicates the weight for the noise-robust loss applied to the synthetic data of old classes.\nIn addition, to address the issue of the model overfitting to the limited new data, we employ a fine-tuning strategy to update the local model. Concretely, we fine-tune the parameters of the backbone and the old classifiers with a low learning rate, while training the parameters of the newly added classifiers with a higher learning rate. This approach helps prevent excessive updates to the model parameters that could lead to overfitting the new classes.\nD. Data generation\nGiven a well-trained model, our goal is to build a conditional generator O_G for synthesizing pseudo-samples that mimic the original data distribution. The synthetic data are expected to satisfy the following key characteristics: fidelity, diversity, stability, and transferability. To achieve this, we adopt a teacher-student architecture where the condition generator and the student model are jointly optimized through an adversarial learning scheme, as depicted in Fig. 3. In the base session, the well-trained initial model functions as the teacher, while the ensemble of local models serves as the teacher in the subsequent incremental sessions. Note that the teacher model only provides supervisory signals and does not update its parameters. Following [30], we initialize a student model \u03b8_s which transfers knowledge by emulating predictions of the teacher on synthetic samples, thereby learning similar decision boundaries. The generator seeks to make knowledge transfer more challenging for the student model by generating synthetic samples near the decision boundaries. Through this adversarial learning mechanism, the generator is able to produce diverse and challenging samples. The training process consists of two iterative stages:\n\u2022 Optimization of the generator: The generator is continuously optimized to synthesize diverse and hard pseudo-samples that conform to the distribution of the teacher.\n\u2022 Optimization of the student model: The student model is trained on synthetic data to acquire knowledge from the teacher and assess the effectiveness of the synthetic data.\n1) Optimization of the generator: Briefly, the generator \u03b8_G seeks to produce a synthetic sample x = O_G(z, \\tilde{y}) that aligns with the data distribution of the teacher model, where z denotes a perturbation extracted from a standard Gaussian distribution and the label \\tilde{y} is randomly chosen according to the predefined distribution Y(t).\nWe first obtain the logits O(x), namely the outputs of the last fully connected layer in the teacher network. This process can be formulated as\nO(x) = \\begin{cases}f_t(x; \\theta^{(t)}), & \\text{if } t = 0, \\\\ \\frac{1}{M} \\Sigma_{m=1}^{M} f_t(x; \\theta_m^{(t)}), & \\text{if } t > 1.\\end{cases}\nwhere \u03b8^{(t)} denotes the global model and \u03b8_m^{(t)} indicates local model at the m-th client in session t. M is the total number of clients. The function f_t(\\cdot;\\cdot) extracts outputs of the last fully connected layer, corresponding to the new classes in session t. Particularly, the initial model \u03b8^{(0)} learned in a centralized manner is employed as the teacher in the base session. In the subsequent incremental sessions, we use the ensemble of local models rather than the global model as the teacher, because the generator is trained before aggregating the local models into the global model, as shown in Fig. 2.\nFidelity. We employ the standard cross-entropy loss to guarantee that synthetic replay samples are correctly categorized into a specific class \\tilde{y} with high confidence.\nL_{CE}(x, \\tilde{y}; \\theta_G) = CE(Softmax(O(x)), \\tilde{y}).\nBy minimizing L_{CE}, the synthetic data can fit the distribution of class \\tilde{y}. In fact, during the training of the generator, relying only on CE loss often generates synthetic data that remain distant from the decision boundary defined by the teacher model, leading to the issue of overfitting.\nDiversity. Another objective of the generator is to ensure the diversity of the synthetic data. To promote the diversity of generated synthetic data and incorporate more challenging examples, we increase the information entropy of the teacher model's predictions on these data. Specifically, the information entropy of the probability distribution p = [p_1,..., p_C] is computed as\nH_{info}(p) = -\\frac{1}{C} \\Sigma_{i=1}^{C} p_i log(p_i).\nThe entropy loss of a synthetic sample is formulated as\nL_{Ent}(x; \\theta_G) = -H_{info}(Softmax(O(x))).\nStability. Early studies [33], [39] have demonstrated that the Batch Normalization (BN) statistics is beneficial for the convergence of generator. BN layers play a crucial role in reducing internal covariate shifts, a phenomenon where the distribution of network activations changes as training data forward propagate through the layers of a neural network. By ensuring that the activations of synthetic data align with these precomputed statistics of teacher models, the generator produces samples that closely resemble the distribution of the original dataset. The BN statistic constraint is given by\nL_{BN}(x; \\theta_G) = \\frac{1}{M} \\Sigma_{m=1}^{M} \\Sigma_l (||\\mu_l(x) - \\mu_{m,l}||_2 + ||\\sigma_l(x) - \\sigma_{m,l}||_2),\nwhere \u03bcl(x) and \u03bcm,l represent the mean of activations at the l-th batch normalization layer in the generator and the m-th teacher model, respectively. Similarly, \u03c3f(x) and \u03c3m,l denote the variance of activations at the same layers in the generator and teacher model, respectively.\nTransferability. The core motivation for employing a teacher-student architecture is to drive the generator to produce hard samples near the decision boundaries. The student mimics the teacher's outputs on synthetic samples to gain valuable knowledge, while the generator aims to produce samples that lie on different sides of the decision boundaries of the student and teacher, as illustrated in Fig. 4. To achieve this, we encourage synthetic samples to induce prediction discrepancies between the teacher and student models by maximizing the Kullback-Leibler divergence between their output logits.\nL_{KL}(x; \\theta_G) = -wKL (O(x),O_s(x; \\theta_s)),\nwhere O(x) and O_s(x; \u03b8_s) denote the logits produced by the teacher and the student, respectively. Particularly, w = 1 if the student and the teacher predict different classes for the sample x; otherwise, w = 0.\nThe objective function of the generator is composed of the above loss functions and can be formulated as\nL_G(x, \\tilde{y}; \\theta_G) = \\lambda_1 \\cdot L_{CE}(x, \\tilde{y}; \\theta_G) + \\lambda_2 \\cdot L_{Ent} (x; \\theta_G)\n+ \\lambda_3 L_{BN} (x; \\theta_G) + \\lambda_4 \\cdot L_{KL}(x; \\theta_G),\nwhere the pre-defined parameters \u03bb1, \u03bb2, \u03bb3, and \u03bb4 are utilized to weight the four items.\n2) Optimization of the student model: We propose leveraging a student model to assist in training the generator. Given a synthetic sample, the student model tries to mimic outputs of the teacher model through optimization of the Kullback-Leibler divergence\nL_s(x; \\theta_s) = KL (O(x),O_s(x; \\theta_s)),\nso that it can learn similar decision boundaries. In contrast, the generator strives to generate samples close to the category decision boundary, making the knowledge transfer of the student model difficult. Adversarial learning against the generator facilitates the training of a high-performing student model and helps to collect a diverse and challenging set of synthetic samples.\nE. Model aggregation\nMost existing model aggregation algorithms in federated learning adopt model-wise weighted averaging to derive the global model from local ones. However, they do not consider the differences between the internal parameters of various local models. In the proposed SDD framework, we develop a CSWA strategy to achieve efficient fusion of local models by leveraging synthetic data generated in the current session. This strategy accounts for the differences in local model parameters caused by the scarce and non-IID data in FSCIL and implements parameter-wise weighted aggregation for new classes based on the class-specific weight matrix.\nIn incremental session t, parameters of the local model \\theta_m are divided into old parameters W_{old}^{t,m} inherited from the previous session and new parameters W_{new}^{t,m} added for classifying new classes in the current session. We only fine-tune the parameters W_{old}^{t,m} to better preserve knowledge of old classes. Therefore, desired results can be achieved by averaging these old parameters W_{old}^{t,m} on the server side. This process is formulated by\nW_{old}^t = \\Sigma_{m=1}^M \\frac{N_m^{(t)}}{N^{(t)}} W_{old}^{t,m},\nwhere W_{old}^t represents the consolidated outcomes of old parameters in the global model. N_m^{(t)} signifies the count of training samples at client m, whereas N^{(t)} denotes the aggregate number of training samples across all clients in the current session t.\nParameters W_{new}^{t,m} \\in R^{L \\times c} introduced in the current session for classifying new classes are formulated as\nW_{new}^{t,m} = \\begin{bmatrix}w_{1,1}^m & w_{1,2}^m & \\dots & w_{1,c}^m \\\\ w_{2,1}^m & w_{2,2}^m & \\dots & w_{2,c}^m \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ w_{L,1}^m & w_{L,2}^m & \\dots & w_{L,c}^m\\end{bmatrix},\nwhere c and L signify the count of new classes and the feature dimension, respectively. We are randomly initialized at the beginning of the incremental session and subsequently updated with data of new classes in the FSCIL process. During model updates on the client side, these newly added parameters are likely to overfit the limited training data. Therefore, we employ the synthetic data X^t derived from the generator in the current session t to evaluate the performance of multiple local models on new classes. Specifically, we send the synthetic data X^t to the local model on client m and statistically estimate its accuracy on new classes, expressed as\nA_{new}^{t,m} = [a_{1}^m,..., a_{i}^m,..., a_{c}^m],\nwhere a_{i}^m indicates the accuracy of local model on client m for class i. Then the class-specific weight matrix A_{new}^{t} \\in R^{M \\times c} of all local models can be expressed as\nA_{new}^{t} = [A_{new}^{t,1}; A_{new}^{t,2}; ...; A_{new}^{t,M}].\nThen, the accuracy vector {A_{new}^{t,m}}_{m=1:M} are taken as the weights of local models in the aggregation of new parameters. A high accuracy a_{i}^m indicates that the local model on client m provides reliable classification for class i in this session, so parameters pertaining to class i in this model should be assigned high confidence in model aggregation. Specifically, we perform element-wise multiplication between the new parameters W_{new}^{t,m} of a local model and the corresponding accuracy vector A_{new}^{t,m}, producing a weighted parameter matrix \\widehat{W}_{new}^{t,m} by\n\\widehat{W}_{new}^{t,m} = A_{new}^{t,m}W_{new}^{t,m} = \\begin{bmatrix}a_{1}^mw_{1,1}^m & a_{2}^mw_{1,2}^m & \\dots & a_{c}^mw_{1,c}^m \\\\ a_{1}^mw_{2,1}^m & a_{2}^mw_{2,2}^m & \\dots & a_{c}^mw_{2,c}^m \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{1}^mw_{L,1}^m & a_{2}^mw_{L,2}^m & \\dots & a_{c}^mw_{L,c}^m\\end{bmatrix}.\nNext, we sum the parameters of the newly updated classifiers from each client and obtain the aggregation result for this portion of the parameters by\nW_{new}^t = \\Sigma_{m=1:M} \\widehat{W}_{new}^{t,m}.\nFinally, the aggregated new parameters W_{new}^t are combined with the aggregated old parameters W_{old}^t to form the global model \u03b8^{(t)} of incremental session t:\n\\theta^{(t)} = (W_{old}^t, W_{new}^t)."}, {"title": "IV. EXPERIMENTS", "content": "A. Datasets\nComprehensive experiments are performed across three datasets to verify the feasibility of the proposed SDD.\n1) CIFAR100: This is a popular benchmark in incremental learning and federated learning", "miniImageNet": "This dataset contains 100 classes", "41": ".", "TinyImageNet": "This dataset is also a reduced version of the ImageNet dataset [41", "42": "serves as the backbone for the CIFAR100 dataset"}, {"42": "is used for both the miniImageNet and TinyImageNet datasets. In the centralized base session", "methods": "data replay-based iCaRL [9", "16": "and distillation-based LwF [23", "12": "is implemented for a fair comparison in the few-shot learning setting. The above methods are adapted to fit the experimental environment", "+FL\" indicates the introduction of federated learning. To be more specific, the training data in each incremental session are distributed across multiple clients for local training, and the global model is derived by calculating the average of parameters of local models. Additionally, the comparison also includes two data-free federated continual learning methods": "Target [30", "32": "."}, {"32": "making slight adjustments to the classification layer to adapt to its training strategy.\nAccording to the experimental results depicted in Tables I, II, III, we make the following observations.\n\u2022 Results on the three datasets indicate that fine-tuning with limited new data leads to a significant performance drop due to catastrophic forgetting. The data scarcity of new classes causes CIL methods [9", "16": [23], "30": [32]}]}