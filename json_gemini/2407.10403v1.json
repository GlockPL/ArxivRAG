{"title": "Cooperative Reward Shaping for Multi-Agent Pathfinding", "authors": ["Zhenyu Song", "Ronghao Zheng", "Senlin Zhang", "Meiqin Liu"], "abstract": "The primary objective of Multi-Agent Pathfinding (MAPF) is to plan efficient and conflict-free paths for all agents. Traditional multi-agent path planning algorithms struggle to achieve efficient distributed path planning for multiple agents. In contrast, Multi-Agent Reinforcement Learning (MARL) has been demonstrated as an effective approach to achieve this objective. By modeling the MAPF problem as a MARL problem, agents can achieve efficient path planning and collision avoidance through distributed strategies under partial observation. However, MARL strategies often lack cooperation among agents due to the absence of global information, which subsequently leads to reduced MAPF efficiency. To address this challenge, this letter introduces a unique reward shaping technique based on Independent Q-Learning (IQL). The aim of this method is to evaluate the influence of one agent on its neighbors and integrate such an interaction into the reward function, leading to active cooperation among agents. This reward shaping method facilitates cooperation among agents while operating in a distributed manner. The proposed approach has been evaluated through experiments across various scenarios with different scales and agent counts. The results are compared with those from other state-of-the-art (SOTA) planners. The evidence suggests that the approach proposed in this letter parallels other planners in numerous aspects, and outperforms them in scenarios featuring a large number of agents.", "sections": [{"title": "I. INTRODUCTION", "content": "MAPF is a fundamental research area in the field of multi-agent systems, aiming to find conflict-free routes for each agent. This has notable implications in various environments such as ports, airports [1], [2], and warehouses [3], [4], [5]. In these scenarios, there are typically a large number of mobile agents. These scenarios can generally be abstracted into grid maps, as illustrated in Fig. 1. The MAPF methodology is divided mainly into two classes: centralized and decentralized algorithms. Centralized algorithms [6], [7], [8] offer efficient paths by leveraging global information but fall short in scalability when handling a significant number of agents because of increased computational needs and extended planning time. In contrast, decentralized algorithms [9], [10] show better scalability in large-scale environments but struggle to ensure sufficient cooperation among agents, thereby affecting the success rate in pathfinding and overall efficiency.\nMARL techniques, particularly those utilizing distributed execution, provide effective solutions to MAPF problems. By modeling MAPF as a partially observable Markov decision process (POMDP), MARL algorithms can develop policies that make decisions based on agents' local observations and inter-agent communication. Given that MARL-trained policy networks do not rely on global observations, these methods exhibit excellent scalability and flexibility in dynamic environments. MARL enhances the success rate and robustness of path planning, making it particularly suitable for large-scale multi-agent scenarios. Algorithms such as [11], [12], [13] utilize a centralized training distributed execution (CTDE) framework and foster cooperation between agents using global information during training. However, they struggle to scale for larger numbers of agents due to increasing training costs. In contrast, algorithms based on distributed training distributed execution (DTDE) frameworks [14], [15] perform well in large-scale systems. However, due to the lack of global information, individual agents tend to solely focus on maximizing their own rewards, resulting in limited cooperation among the whole system. To address this issue, recent work [16], [17], [18] emerges that improves the performance of RL algorithms in distributed training frameworks through reward shaping. However, some of these reward shaping methods are too computationally complex, while others lack stability. This instability arises because an agent's rewards are influenced by the actions of other agents, which are typically unknown to this agent.\nIn this letter, a reward shaping method named Cooperative Reward Shaping (CoRS) is devised to enhance MAPF efficiency within a DTDE framework. The approach is straightforward and tailored for a limited action space. The cooperative trend of action $a^i$ is represented by the maximum rewards that neighboring agents can achieve after agent $A^i$ performs $a^i$. Specifically, when agent $A^i$ takes action $a^i$, its neighbor $A^j$ traverses its action space, determining the maximum reward that $A^j$ can achieve given the condition of $A^i$ taking action $a^i$. The shaped reward is then generated by weighting this index with the reward earned by $A^i$ itself.\nThe principal contributions of our work are as follows:"}, {"title": "II. RELATED WORKS", "content": "RL-based planners such as [19], [20], [21], [22], typically cast MAPF as a MARL problem to learn distributed policies for agents from partial observations. This method is particularly effective in environments populated by a large number of agents. Techniques like Imitation Learning (IL) often enhance policy learning during this process. Notably, the PRIMAL algorithm [20] utilizes the Asynchronous Advantage Actor Critic (A3C) algorithm and applies behavior cloning for supervised RL training using experiences from the centralized planner ODrM* [23]. However, the use of a centralized planner limits its efficiency, as solving the MAPF problem can be time-intensive, particularly in complex environments with a large number of agents.\nIn contrast, Distributed Heuristic Coordination (DHC) [24] and Decision Causal Communication (DCC) [25] algorithms do not require a centralized planner. Although guided by an individual agent's shortest path, DHC innovatively incorporates all potential shortest path choices into the model's heuristic input rather than obligating an agent to a specific path. Additionally, DHC collects data from neighboring agents to inform its decisions and employs multi-head attention as a convolution kernel to calculate interactions among agents. DCC is an efficient algorithm that enhances the performance of agents by enabling selective communication with neighbors during both training and execution. Specifically, a neighboring agent is deemed significant only if its presence instigates a change in the decision of the central agent. The central agent only needs to communicate with its significant neighbors."}, {"title": "B. Reward Shaping", "content": "The reward function significantly impacts the performance of RL algorithms. Researchers persistently focus on designing reward functions to optimize algorithm learning efficiency and agent performance. A previous study [26] analyzes the effect of modifying the reward function in Markov Decision Processes on optimal strategies, indicating that the addition of a transition reward function can boost the learning efficiency. In multi-agent systems, the aim is to encourage cooperation among agents through appropriate reward shaping methods, thereby improving overall system performance. [27] probes the enhancement of cooperative agent behavior within the context of a two-player Stag Hunt game, achieved through the design of reward functions. Introducing a prosocial coefficient, the study validates through experimentation that prosocial reward shaping methods elevate performance in multi-agent systems with static network structures. Moreover, [28] promotes cooperation among agents through the reward of an agent whose actions causally influence the behavior of other agents. The evaluation of causal impacts is achieved through counterfactual reasoning, with each agent simulating alternative actions at each time step and calculating their effect on other agents' behaviors. Actions that lead to significant changes in the behavior of other agents are deemed influential and are rewarded accordingly.\nSeveral studies, including [29], [17], [30], utilize the Shapley value decomposition method to calculate or redistribute each agent's cooperative benefits. [30] confirms that if a transferable utility game is a convex game, the MARL reward redistribution, based on Shapley values, falls within the core, thereby securing stable and effective cooperation. Consequently, agents should maintain their partnerships or collaborative groups. This concept is the basis for a proposed cooperative strategy learning algorithm rooted in Shapley value reward redistribution. The effectiveness of this reward shaping method in promoting cooperation among agents, specifically within a basic autonomous driving scenario, is demonstrated in the paper. However, the process of calculating the Shapley value can be intricate and laborious. Ref. [29] aims to alleviate these computational challenges by introducing approximation of marginal contributions and employing Monte Carlo sampling to estimate Shapley values. Coordinated Policy Optimization (CoPO)[18] puts forth the concept of \u201ccooperation coefficient\u201d, which shapes the reward by taking a weighted average of an agent's individual rewards and the average rewards of its neighboring agents, based on the cooperation coefficient. This approach proves that rewards shaped in this manner fulfill the Individual Global Max (IGM) condition. Findings from traffic simulation experiments further suggest that this method of reward shaping can significantly enhance the overall performance and safety of the system. However, this approach ties an agent's rewards not merely to its personal actions but also to those of its neighbors. Such dependencies might compromise the stability during the training process and the efficiency of the converged strategy."}, {"title": "III. PRELIMINARY", "content": "Consider a Markov process involving $n$ agents $\\{A^1,..., A^n\\} := A$, represented by the tuple $(S, A, O, R, P, \\gamma)$. $A^i$ represents agent $i$. At each time step $t$, $A^i$ chooses an action $a^i_t$ from its action space $A^i$ based on its state $s^i_t \\in S^i$ and observation $o^i_t \\in O^i$ according to its policy $\\pi^i$. All $a^i_t$ form a joint action $\\bar{a}_t = \\{a^1_t,...,a^n_t\\} \\in (A^1 \\times ... \\times A^n) := A$, and all $s^i_t$ form a joint state $S_t = \\{s^1_t,...,s^n_t\\} \\in (S^1 \\times ... \\times S^n) := S$. For convenience in further discussions, agent's local observations $o^i_t$ are treated as a part of the agent's state $s^i_t$. Whenever a joint action $\\bar{a}_t$ is taken, the agents acquire a reward $r = \\{r^1_t, r^2_t, ..., r^n_t\\} \\in R^n$, which is determined by the local reward function $r^i(S_t, \\bar{a}_t) : S \\times A \\rightarrow R$ with respect to the joint state and action. The state transition function $P(S_{t+1} | S_t, \\bar{a}_t): S \\times S \\times A \\rightarrow [0,1]$ characterizes the probability of transition from the current state $S_t$ to $S_{t+1}$ under action $\\bar{a}_t$. The policy $\\pi^i(a^i | s^i)$ provides the probability of $A^i$ taking action $a^i$ in the state $s^i$. $\\bar{\\pi}$ represents the joint policy for all agents. The action-value function is given by $Q^i(s^i_t, a^i_t) = E_{\\tau} [\\sum_{t'=t}^T \\gamma^{t'-t}r^i_{t'}]$, where the trajectory $\\tau=[s^i_{t+1}, a^i_{t+1}, s^i_{t+2},...]$ represents the path taken by the agent $A^i$. The state-value function is given by $V^i(s^i) = \\sum_{a \\in A^i} \\pi^i(a^i|s^i)Q^i(s^i, a^i)$, and the discounted cumulative reward is $J^i = E_{s_0 \\sim p_0} V^i(s_0)$, where $p_0$ represents the initial state distribution. For cooperative MARL tasks, the objective is to maximize the total cumulative reward $J_{tot} = \\sum_{i=1}^n J^i$ for all agents."}, {"title": "B. Multi-agent Pathfinding Environment Setup", "content": "This letter adopts the same definition of the multi-agent path-finding problem as presented in [24], [25]. Consider $n$ agents in an $w \\times h$ undirected grid graph $G(V, E)$ with $m$ obstacles $\\{B_1,..., B_m\\}$, where $V = \\{v(i, j) | 1 \\le i \\le w, 1 < j \\le h\\}$ is the set of vertices in the graph, and all agents and obstacles located within $V$. All vertices $v(i, j) \\in V$ follow the 4-neighborhood rule, that is, $[v(i, j), v(p,q)] \\in E$ for all $v(p,q) \\in \\{v(i, j \\pm 1), v(i \\pm 1, j)\\} \\cap V$. Each agent $A^i$ has its unique starting vertex $s^i$ and goal vertex $g^i$, and its position at time $t$ is $x^i_t \\in V$. The position of the obstacle $B_k$ is represented as $b_k \\in V$. At each time step, each agent can execute an action chosen from its action space $A = \\{\\text{``Up''}, \\text{``Down''}, \\text{``Left''}, \\text{``Right''}, \\text{``Stop''}\\}$. During the execution process, two types of conflict can arise: vertex conflict $(x^i_t = x^j_t \\text{ or } x^i_t = b_k)$ and edge conflict $([x^i_{t-1}, x^i_t] = [x^j_{t}, x^j_{t-1}])$. If two agents conflict with each other, their positions remain unchanged. The subscript $t$ for all the aforementioned variables can be omitted as long as it does not cause ambiguity. The goal of the MAPF problem is to find a set of non-conflicting paths $P = \\{P_1, P_2, ..., P_n\\}$ for all agents, where the agent's path $P^i = [s^i, x^i_1, ..., x^i_t, ...]$ is an ordered list of $A^i$'s position. Incorporating the setup of multi-agent reinforcement learning, we design a reward function for the MAPF task. The design of the reward function basically follows [24], [25], with slight adjustments to increase the reward for the agent moving towards the target to better align with our reward shaping method."}, {"title": "IV. COOPERATIVE REWARD SHAPING", "content": "Many algorithms employing MARL techniques to address the MAPF problem utilize IQL or other decentralized training and execution frameworks to ensure good scalability. For example, [24], [25] are developed based on IQL. Although IQL can be applied to scenarios with a large number of agents, it often performs poorly in tasks that require a high degree of cooperation among agents, such as the MAPF task. This poor performance arises because, within the IQL framework, each agent greedily maximizes its own cumulative reward, leading agents to behave in an egocentric and aggressive manner, thus reducing the overall efficiency of the system. To counteract this, this letter introduces a reward shaping method named Cooperative Reward Shaping (CoRS), and combines CoRS with the DHC algorithm. The framework combining CoRS with DHC is shown in Fig. 2. The aim of CoRS is to enhance performance within MAPF problem scenarios. The employment of reward shaping intends to stimulate collaboration among agents, effectively curtailing the performance decline in the multi-agent system caused by selfish behaviors within a distributed framework."}, {"title": "A. Design of the Reward Shaping Method", "content": "In the MAPF task, the policies trained using IQL often result in scenarios where one agent blocks the path of other agents or collides with them. To enhance cooperation among agents within the IQL framework, a feasible approach is reward shaping. Reward shaping involves meticulously designing the agents' reward functions to influence their behavior. For example, when a certain type of behavior needs to be encouraged, a higher reward function is typically assigned to that behavior. Thus, to foster cooperation among agents in the MAPF problem, it is necessary to design a metric that accurately evaluates the collaboration of agents' behavior and incorporate this metric into the agents' rewards. Consequently, as each agent maximizes its own reward, it will consider the impact of its action on other agents, thereby promoting cooperation among agents and improving overall system efficiency.\nLet $I_\\delta(s_t, \\bar{a}_t)$ be a metric that measures the cooperativeness of the action $a^i_t$ of agent $A^i$. To better regulate the behavior of the agent, [27] introduces a cooperation coefficient $\\alpha$ and shapes the agent's reward function in the following form:\n$r' = (1 - \\alpha)r + \\alpha I(S_t,\\bar{a}_t), \\tag{1}\nwhere $\\alpha$ describes the cooperativeness of the agent. When $\\alpha = 0$, the agent completely disregards the impact of its actions on other agents, acting entirely selfishly and when $\\alpha = 1.0$, the agent behaves with complete altruism. For agent $A^i$, an intuitive approach to measure the cooperativeness of A's behavior is to use the average reward of all agents except A:\n$I(S_t,\\bar{a}_t) = \\frac{1}{|A^{-i}|} \\sum_{j \\in A^{-i}} r^j(S_t, \\bar{a}_t), \\tag{2}\nwhere $A^{-i}$ denotes the set of all agents except $A^i$, and $|A^{-i}|$ represents the number of agents in $A^{-i}$. This reward shaping method is equivalent to the neighborhood reward proposed in [18] when $d_n$, the neighborhood radius of the agent, approaches infinity. The physical significance of Eq. (2) is as follows: If the average reward of the agents other than A is relatively high, it indicates that A's actions have not harmed the interests of other agents. Hence, A's behavior can be considered as cooperative. Conversely, if the average reward of the other agents is low, it suggests that A's behavior exhibits poor cooperation.\nHowever, $I(S_t, \\bar{a}_t)$ in Eq. (2) is unstable, which is not only related to $a^i_t$ but is also strongly correlated with the actions $\\bar{a}^{-i} = \\{a^j_t|A^j \\in A, j \\neq i\\}$ of other agents. Appendix B-A provides specific examples to illustrate this instability. Within the IQL framework, this instability in the reward function makes learning of Q-values challenging and can even prevent convergence. To address this issue, this letter proposes a new metric to assess the cooperativeness of agent behavior. The specific form of this metric is as follows:\n$I(S_t,\\bar{a}_t) = \\frac{\\max_{\\bar{a}^{-i}} \\sum_{j \\in A^{-i}} r^j(S_t, \\{a^i_t,\\bar{a}^{-i}\\})}{|A^{-i}|}. \\tag{3}$\nHere $\\bar{a}^{-i} = \\{\\bar{a}^j \\neq i\\} \\cup \\{a^i_t, \\bar{a}^{-i}\\}$. The use of the $\\max_{\\bar{a}^{-i}}$ operator in Eq. (3) eliminates the influence of $\\bar{a}^{-i}$ on $I$ while reflecting the impact of $a^i$ on other agents. The term $\\max_{\\bar{a}^{-i}} \\sum_{j \\in A^{-i}} r^j(S_t, \\{a^i_t,\\bar{a}^{-i}\\})$ represents the maximum reward that all the agents except $A^i$ can achieve under the condition of $S_t$ and $a^i_t$, whereas the actual value of $\\sum_{j \\in A^{-i}} r^j(S_t, \\{a^i_t,\\bar{a}^{-i}\\})$ is determined by $\\bar{a}^{-i}$ when $a^i$ is given. Accordingly, it holds true under any circumstances that $\\sum_{j \\in A^{-i}} r^j(S_t, \\{a^i_t,\\bar{a}^{-i}\\}) <\\max_{\\bar{a}^{-i}} \\sum_{j \\in A^{-i}} r^j(S_t, \\{a^i_t,\\bar{a}^{-i}\\})$.\n$\\\\newline$\n$\\\\\\$"}, {"title": "B. Analysis of Reward Shaping", "content": "This section provides a detailed analysis of how reward shaping method Eq. (4) influences agent behavior. For ease of discussion, the subsequent analysis will be conducted from the perspective of $A^i$.\nIn the MAPF problem, the actions of agents are tightly coupled. The action of $A^i$ may impact $A^j$, and subsequently, the action of $A^j$ may also affect $A^k$. Thus, the action of $A^i$ indirectly affect $A^k$. This coupling makes it challenging to analyze the interactions among multiple agents. To mitigate this coupling, we consider all agents in $A^{-i}$ as a single virtual agent $\\tilde{A}^{-i}$. The action of $\\tilde{A}^{-i}$ is $\\bar{a}^{-i}$ and its reward $\\tilde{r}^{-i}$ is their average reward $\\frac{1}{|A^{-i}|} \\sum_{j \\in A^{-i}}r^i$. The use of average here ensures that $A^i$ and $\\tilde{A}^{-i}$ are placed on an equal footing. The virtual agent $\\tilde{A}^{-i}$ must satisfy the condition that no collisions occur between the agents constituting $A^{-i}$. It is important to note that the interaction between $A^i$ and $\\tilde{A}^{-i}$ is not entirely equivalent to the interaction between $A^i$ and agents in $A^{-i}$. This is because when considering the agents in $A^{-i}$ as $\\tilde{A}^{-i}$, all robots within $A^{-i}$ fully cooperate. In contrast, treating these agents as independent individuals makes it difficult to ensure full cooperation. Nonetheless, $\\tilde{A}^{-i}$ can still represent the ideal behavior of agents in $A^{-i}$. Therefore, analyzing the interaction between $A^i$ and $\\tilde{A}^{-i}$ can still illustrate the impact of reward shaping on agent interactions. Consider the interaction between $A^i$ and $A^{-i}$ in the time period $[t_s,t_e]$. Let\n$Q^i(s, a^i) = E_{\\tau^{i}_{t-t_s}} \\sum_{t=t_s}^{t_e} \\gamma^{t}(r^i(s_t, \\bar{a}_t)) | s_{t_s} = s, a^i_{t_s} = a^i$ and\n$Q^{-i}(s, \\bar{a}^{-i}) = E_{\\tau^{i}_{t-t_s}} \\sum_{t=t_s}^{t_e} \\gamma^{t}(\\tilde{r}^{-i}(s_t, \\bar{a}_t)) | s_{t_s} = s, \\bar{a}^{-i}_{t_s} = \\bar{a}^{-i}$ are the cumulative reward of $A^i$ and $\\tilde{A}^{-i}$. $Q^{tot}(s, \\bar{a}) = E_{\\tau^{i}_{t-t_s}} \\sum_{t=t_s}^{t_e} \\gamma^{t}(\\tilde{r}^i(s_t, \\bar{a}_t) + \\tilde{r}^{-i}) | s_{t_s} = s, \\bar{a}_{t_s} = \\bar{a}$ is the cumulative reward for both $A^i$ and $A^{-i}$. The optimal policies $\\pi^{i^*} = \\arg\\max_{a^i} Q^i(s, a^i), \\pi^{-i^*} = \\arg\\max_{\\bar{a}^{-i}} Q^{-i}(s, \\bar{a}^{-i})$, and $\\bar{\\pi}^* = \\arg\\max_{\\bar{a}} Q^{tot}(s, \\bar{a})$. $A^i$ and $A^{-i}$ will select their actions according to $a^i = \\arg\\max_{a^i} Q(s_t, a^i)$ and $\\bar{a}^{-i} = \\arg\\max_{\\bar{a}^{-i}} Q(s_t, \\bar{a}^{-i})$. We hope that $a^i$ and $\\bar{a}^{-i}$ will collectively maximize $Q^{tot}(s, \\bar{a}) = Q^{tot}(s, \\{a^i, \\bar{a}^{-i}\\})$. Specifically,\n$\\arg\\max_{\\bar{a}} Q^{tot}(s, \\bar{a}) = \\{\\begin{array}{ll} \\arg\\max_{a^i} Q^{\\pi^{i^*}}(s_t, a^i), & \\\\ \\arg\\max_{\\bar{a}^{-i}} Q^{\\pi^{-i^*}}(s_t, \\bar{a}^{-i}), \\end{array}\\}$\nThat is, $Q^i$, $Q^{-i}$, and $Q^{tot}(s, \\bar{a})$ satisfy the Individual-Global-Max (IGM) condition. The definition of the IGM condition can be found in [31]. We introduce the following two assumptions to facilitate the analysis."}, {"title": "C. Approximation", "content": "Theorem 1 illustrates that the reward shaping method Eq. (4) can promote cooperation among agents in MAPF tasks. However, calculating Eq. (3) requires traversing the joint action space of $A^{-i}$. For the MAPF problem where the action space size for each agent is 5, the joint action space for n agents contains up to $5^n$ states, significantly reducing the computational efficiency of the reward shaping method. Therefore, we must simplify the calculation of Eq. (3). Before approximating Eq. (3), we first define the neighbors of an agent in the grid map as follows: $A^j$ is considered a neighbor of $A^i$ if the Manhattan distance between them is no more than $d_n$, where $d_n$ is the neighborhood radius of the agent. If the Manhattan distance between two agents is no more than 2, a conflict may arise between them in a single step. Therefore, we choose $d_n = 2$, which maximally simplifies interactions between agents while adequately considering potential collisions. Let $N_i$ denote the set of neighboring agents of $A^i$, and $|N_i|$ represent the number of neighbors of $A^i$. \n$\\newline$\nConsidering that in MAPF tasks, the direct interactions among agents are constrained by the distances between them, the interactions between any given agent and all other agents can be simplified to the interactions between the agent and its neighbors. That is:\n$I(S_t,\\bar{a}_t) \\approx \\frac{1}{\\mid N_i \\mid} \\sum_{j \\in N_i} \\max_{a^{j}} r^i (s_t, \\{a^i, \\bar{a}^j\\}),$ \n$\\newline$"}, {"title": "D. Adjustment of the cooperation coefficient", "content": "For Eq. (5), it is necessary to adjust \u03b1 so that the agent can balance its own interests with the interests of other agents. To find an appropriate cooperation coefficient \u03b1, it is advisable to examine how the policies trained under different \u03b1 perform in practice and then employ a gradient descent algorithm based on the performance of the policy to optimize \u03b1. The cumulative reward J of the agent often serves as a performance metric in RL. However, in environments with discrete action and state spaces, the cumulative reward J is non-differentiable with respect to the \u03b1, presenting a significant challenge for adjusting \u03b1. Some work employs zero-order optimization of stochastic gradient estimation to handle non-differentiable optimization problems, that is, estimating the gradient by finite differences of function values. We also use differences in the cumulative reward of the agent to estimate the gradient of the cooperation coefficient \u03b1:\n$\\frac{\\partial J(\\alpha)}{\\partial \\alpha} = \\frac{\\hat{J}(\\alpha + \\mu) - \\hat{J}(\\alpha)}{\\mu}, \\mu \\in [-\\epsilon, \\epsilon],$\nwhere $\\epsilon$ represents the maximum step size of the differential. If the update step length is too small, the updates are halted. To expedite the training process, a method of fine-tuning the network is employed. That is, after each adjustment of \u03b1, the network is not trained from the initial state. Instead, fine-tuning training is performed based on the optimal policy network obtained previously. This training method improves the speed of the training process."}, {"title": "V. EXPERIMENTS", "content": "We conducted our experiments in the standard MAPF environment, where each agent has a 9 \u00d7 9 FOV and can communicate with up to two nearest neighbors. Following the curriculum learning method [32] used by DHC, we gradually introduced more challenging tasks to the agents. Training began with a simple task that involved a single agent in a 10 \u00d7 10 environment. Upon achieving a success rate above 0.9, we either added an agent or increased the environment size by 5 to establish two more complex tasks. The model was ultimately trained to handle 10 agents in a 40 \u00d7 40 environment. The maximum number of steps per episode was set to 256. Training was carried out with a batch size of 192, a sequence length of 20, and a dynamic learning rate starting at $10^{-4}$, which was halved at 100,000 and 300,000 steps, with a maximum of 500,000 training steps. During fine-tuning, the learning rate was maintained at $10^{-5}$. Distributed training was used to improve efficiency, with 16 independent environments running in parallel to generate agent experiences, which were uploaded to a global buffer. The learner then retrieved these data from the buffer and trained the agent's strategy on a GPU. CORS-DHC adopted the same network structure as DHC. All training and testing were performed on an Intel\u00ae i5-13600KF and Nvidia\u00ae RTX2060 6G."}, {"title": "A. Impact of Reward Shaping", "content": "Following several rounds of network fine-tuning and updates to the cooperation coefficient \u03b1, a value of $\\alpha = 0.1675$ and strategy are obtained. Upon obtaining the final \u03b1 and $\\pi$, the CoRS-DHC-trained policy is compared with the original DHC algorithm-trained policy to assess the effect of the reward shaping method on performance improvement. For a fair comparison, the DHC and CoRS-DHC algorithms are tested on maps of different scales (40 \u00d7 40 and 80 \u00d7 80) with varying agent counts $\\{4, 8, 16, 32, 64\\}$. Recognizing the larger environmental spatial capacity of the 80 \u00d7 80 map, a scenario with 128 agents is also introduced for additional insights. Each experimental scenario includes 200 individual test cases, maintaining a consistent obstacle density of 0.3."}, {"title": "B. Success Rate and Average Step", "content": "Additionally, the policy trained through CoRS-DHC is compared with other advanced MAPF algorithms. The current SOTA algorithm, DCC[25], which is also based on reinforcement learning, is selected as the main comparison object, with the centralized MAPF algorithm ODrM*[6] and PRIMAL[20] (based on RL and IL) serving as references. DCC is an efficient model designed to enhance agent performance by training agents to selectively communicate with their neighbors during both training and execution stages. It introduces a complex decision causal unit to each agent, which determines the appropriate neighbors for communication during these stages. Conversely, the PRIMAL algorithm achieves distributed MAPF by imitating ODrM* and incorporating reinforcement learning algorithms. ODrM* is a centralized algorithm designed to generate optimal paths for multiple agents. It is one of the best centralized MAPF algorithms currently available. We use it as a comparative baseline to show the differences between distributed and centralized algorithms.\nThe experimental results indicate that our CoRS-DHC algorithm consistently exceeds the success rate of the DCC algorithm in the majority of scenarios. Additionally, aside from the 40 \u00d7 40 grid with 64 agents, the makespan of the policies trained by the CoRS-DHC algorithm is comparable to or even shorter than that of the DCC algorithm across other scenarios. These results clearly demonstrate that our CoRS-DHC algorithm achieves a performance comparable to that of DCC. However, it should be noted that DCC employs a significantly more complex communication mechanism during both training and execution, while our CoRS algorithm only utilizes simple reward shaping during the training phase. Compared to PRIMAL and DHC, CoRS-DHC exhibits a remarkably superior performance."}, {"title": "VI. CONCLUSION", "content": "This letter proposes a reward shaping method termed CoRS, applicable to the standard MAPF tasks. By promoting cooperative behavior among multiple agents, CoRS significantly improves the efficiency of MAPF. The experimental results indicate that CoRS significantly enhances the performance of the MARL algorithms in solving the MAPF problem. CoRS also has implications for other multi-agent reinforcement learning tasks. We plan to further extend the application of this reward-shaping strategy to a wider range of MARL environments in future exploration."}, {"title": "APPENDIX A", "content": "In this section", "statement": "for $a^i = \\arg\\max_{a^i} Q^i(s_t, a^i)$, there exists $\\bar{a}^{-i} = \\arg\\max_{\\bar{a}^{-i}} Q^{-i}(s_t, \\bar{a}^{-i})$, such that $Q^{tot}(s_t, \\{a^i,\\bar{a}^{-i}\\}) = \\arg\\max_{\\bar{a}} Q^{tot}(s_t,\\bar{a})$.\n$\\newline$\nFor the sake of simplicity in the discussion, we denote $r^i(\\bar{s}_t,\\bar{a}_t)$ and $\\tilde{r}^{-i}(\\bar{s}_t,\\bar{a}_t)$ as $r^i$ and $\\tilde{r}^{-i}$, and $E_{\\tau^i} [\\sum_{t=t_s}^{t_e} \\gamma^{t} r^i_t"}]}