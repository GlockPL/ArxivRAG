{"title": "PDZSeg: Adapting the Foundation Model for Dissection Zone Segmentation with Visual Prompts in Robot-assisted Endoscopic Submucosal Dissection", "authors": ["Mengya Xu", "Wenjin Mo", "Guankun Wang", "Huxin Gao", "An Wang", "Zhen Li", "Xiaoxiao Yang", "Hongliang Ren"], "abstract": "Purpose: The intricate nature of endoscopic surgical environments poses significant challenges for the task of dissection zone segmentation. Specifically, the boundaries between different tissue types lack clarity which can result in significant segmentation errors, as the models may misidentify or overlook object edges altogether. Thus, the goal of this work is to achieve the precise dissection zone suggestion under these challenges during endoscopic submucosal dissection (ESD) procedures and enhance the overall safety of ESD.\nMethods: We introduce a Prompted-based Dissection Zone Segmentation (PDZSeg) model, aimed at segmenting dissection zones and specifically designed to incorporate different visual prompts, such as scribbles and bounding boxes. Our approach overlays these visual cues directly onto the images, utilizing fine-tuning of the foundational model on a specialized dataset created to handle diverse visual prompt instructions. This shift towards more flexible input methods is intended to significantly improve both the performance of dissection zone segmentation and the overall user experience.\nResults: We evaluate our approaches using the three experimental setups: in-domain evaluation, evaluation under variability in visual prompts availability, and robustness assessment. By validating our approaches on the ESD-DZSeg dataset, specifically focused on the dissection zone segmentation task of ESD, our experimental results show that our solution outperforms state-of-the-art segmentation methods for this task. To the best of our knowledge, this is the first study to incorporate visual prompt design in dissection zone segmentation.\nConclusion: We introduce the Prompted-based Dissection Zone Segmentation (PDZSeg) model, which is specifically designed for dissection zone segmentation and can effectively utilize various visual prompts, including scribbles and bounding boxes. This model improves segmentation performance and enhances user experience by integrating a specialized dataset with a novel visual referral method that optimizes the architecture and boosts the effectiveness of dissection zone suggestions. Furthermore, we present the ESD-DZSeg dataset for robot-assisted endoscopic submucosal dissection (ESD), which serves as a benchmark for assessing dissection zone suggestions and visual prompt interpretation, thus laying the groundwork for future research in this field. Our code is available at https://github.com/FrankMOWJ/PDZSeg.", "sections": [{"title": "1 Introduction", "content": "Endoscopic Submucosal Dissection (ESD) is a surgical procedure employed in the treatment of early-stage gastrointestinal cancers [1, 2]. This procedure entails a complex series of dissection maneuvers that require significant skill to determine the dissection zone. In traditional ESD, a transparent cap is employed to retract lesions, which can often obscure the view of the submucosal layer and lead to an incomplete dissection zone. Conversely, our robot-assisted ESD [3] offers better visualization of the submucosal layer, resulting in a more completed dissection zone by utilizing robotic instruments that enable independent control over retraction and dissection. Achieving successful submucosal dissection requires the careful excision of the lesion or mucosal layer along with the complete submucosal layer while ensuring that both the underlying muscular layer and the mucosal surface remain unharmed. If the electric knife inadvertently contacts tissue outside the designated dissection area, it can lead to damage to the muscle layer, increasing the risk of perforations. Such complications not only elevate the surgical risks but can also complicate the patient's recovery. Therefore, it is imperative to maintain a precise dissection zone during endoscopic procedures. Effective guidance can help ensure that surgeons are adept at identifying and adhering to appropriate dissection boundaries and enhance the overall safety of endoscopic submucosal dissection (ESD)."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Data collection and annotation", "content": "In this study, 21 robotic ESD procedures were collected from ex-vivo porcine models using our custom dual-arm robotic system [3] in Qilu Hospital. Video recordings were captured at 30 FPS with 1920 \u00d7 1080 resolution using a flexible dual-channel endoscope. After processing, the endoscopic images were cropped to achieve a final resolution of 1310 \u00d7 1010. This research specifically targets the submucosal dissection task, which is characterized by extensive interactions with soft tissue and procedural intricacies. Video sequences pertinent to this task were selectively extracted and downsampled to 1 FPS to create the dissection dataset. We curated a total of 1, 849 images requiring dissection zone recommendations, sourced from 21 videos. These images were divided into two distinct subsets. Specifically, video 050744, video 030204, video 000032, and video 103115 comprised the test set, which included 369 images. The remaining videos were allocated to the training set, totaling 1,480 images. Expert"}, {"title": "3.2 Our dissection zone segmentation model", "content": ""}, {"title": "3.2.1 Visual prompts design", "content": "Our research aims to enhance the interaction with models, making it more natural and intuitive. Designing visual cues of varying intensities tailored to the needs of less experienced surgeons can significantly improve the confidence and safety of less experienced surgeons during surgery. Thus we provide flexible visual prompt options. Users can provide inputs in various formats, such as: points: Drawing a single point to indicate a region of interest. scribbles (long/short): Drawing freehand lines to specify the location of regions of interest (ROIs). bounding boxes: Encapsulating objects or areas within rectangular frames for easier identification. These natural visual prompts serve as cues for our dissection zone segmentation model, directing its attention to the specified ROIs, thus enhancing the efficiency and accuracy of the segmentation process. These visual cues can provided by the more experienced surgeon (see Fig. 1)."}, {"title": "3.2.2 Dissection zone segmentation model", "content": "Essentially, our segmentation model suggests the dissection zone for these inexperienced doctors, emphasizing the detailed delineation of critical areas that may be challenging for experienced professionals to guide in real time. The proposed Prompted-based Dissection Zone Segmentation (PDZSeg) model comprises three primary components: an image encoder, a LoRA efficient training module, and a segmentation decoder. The module processes the RGB raw images attached with the visual prompt and generates the dissection zone masks.\nImage encoder. DINOv2 [4] is a state-of-the-art self-supervised vision foundation model, designed to learn powerful image representations from large-scale unlabelled datasets. We adopt the pre-trained DINOv2 as our image encoder, which can generate multi-level, rich feature representations that enhance the subsequent segmentation task. To be specific, an input image $x \\in R^{H \\times W \\times C}$ first be embedded into non-overlapping patches $z_i^p \\in R^{N \\times D}$, $1 \\le k \\le N$, where $N = \\frac{HW}{p^2}$, p is the size of patch and D is the dimension of each patch, attached with an additional class token $z_0^p \\in R^{1 \\times D}$, following the practical in Vision Transformer(ViT) [17]. The combined tokens $z = [z_0^p, z_1^p, ..., z_N^p]$ are then passed through M transformer blocks to extract intermediate feature representations, denoted as $z_m$ for $1 < m < M$. At each selected stage, the intermediate feature representations, consisting of both the class token and image tokens, are concatenated along the channel dimension and upsampled by a factor of 4. The features from all the selected stages are then combined to form the final image representation for the decoder.\nLoRA efficient training module. For efficient training, we only add the LORA layer to the $W_q$ and $W_v$ matrices in each of the attention blocks. Specifically, let x represent a token embedding at the tth transformer block, and $W_q$, $W_k$, $W_v$ and $W_o$ be the key, query, and value projection matrices of the transformer block, respectively. Let $A_q$, $B_q$, $A_v$, and $B_v$ denote the LoRA low-rank matrices corresponding to $W_q$ and $W_v$. The attention scores can then be calculated as: $Att(Q, K,V) = Softmax(\\frac{Q K^T}{\\sqrt{D}})$, $Q = W_q x + B_q A_q x$, $K = W_k x$, $V = W_v x + B_v A_v x$, and D is the numbers of output tokens.\nSegmentation decoder. A lightweight All-MLP network is utilized as the segmentation decoder. Each multi-level feature representation is first standardized to the same number of channels by an MLP layer. Then, they are resized back to the resolution of the input image. Afterward, the multi-level feature representations are concatenated and fused by another MLP layer. Finally, an MLP-based segmentation head maps the fused features to a c-channel output, where c is the number of classes to be classified."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Implementation details", "content": "We implement our models using the PyTorch framework and conduct all experiments on a single NVIDIA RTX 4090 GPU. The images and their corresponding segmentation masks are resized to 532 \u00d7 532. Vit-Base with 12 transformer blocks and feature dimension of 784 is adopted as the backbone of DINOv2 [4]. Intermediate outputs from m = 3, 6, 9, 12 transformer blocks are extracted as image representations. A pre-trained checkpoint is loaded for the DINOv2 encoder, while the segmentation decoder is randomly initialized. The encoder is fine-tuned using LoRA [5] with rank of 4, and the decoder is trained from scratch with a learning rate of 0.001. The batch size is set to 8. We train the models for 100 epochs using the Adam [18] optimizer with cross-entropy (CE) loss and the Cosine Annealing scheduler."}, {"title": "4.2 Results and Analysis", "content": "Model evaluation with and without visual prompts To evaluate the performance of our proposed method, several State-of-the-art (SOTA) models,"}, {"title": "5 Conclusion", "content": "We introduce the Prompted-based Dissection Zone Segmentation (PDZSeg) model, which is specifically designed for dissection zone segmentation and accommodates various visual prompts such as scribbles and bounding boxes. By overlaying these visual prompts onto images and fine-tuning the foundation model on a specialized dataset, we aim to significantly enhance both segmentation performance and user experience. Our novel visual referral method further streamlines the model architecture while improving effectiveness in dissection zone suggestion tasks. Additionally, we have developed the ESD-DZSeg dataset for robot-assisted endoscopic submucosal dissection (ESD) using ex-vivo porcine models with our custom dual-arm robotic system. This dataset serves as a benchmark for evaluating dissection zone suggestions and visual prompt interpretation, establishing a foundational framework for future research in this domain."}]}