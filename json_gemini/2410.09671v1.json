{"title": "OpenR: An Open Source Framework for Advanced Reasoning with Large Language Models", "authors": ["Jun Wang", "Meng Fang", "Ziyu Wan", "Muning Wen", "Jiachen Zhu", "Anjie Liu", "Ziqin Gong", "Yan Song", "Lei Chen", "Lionel M. Ni", "Linyi Yang", "Ying Wen", "Weinan Zhang"], "abstract": "In this technical report, we introduce OpenR, an open-source framework designed\nto integrate key components for enhancing the reasoning capabilities of large\nlanguage models (LLMs). OpenR unifies data acquisition, reinforcement learning\ntraining (both online and offline), and non-autoregressive decoding into a cohesive\nsoftware platform. Our goal is to establish an open-source platform and community\nto accelerate the development of LLM reasoning. Inspired by the success of\nOpenAI's o1 model, which demonstrated improved reasoning abilities through step-\nby-step reasoning and reinforcement learning, OpenR integrates test-time compute,\nreinforcement learning, and process supervision to improve reasoning in LLMs.\nOur work is the first to provide an open-source framework that explores the core\ntechniques of OpenAI's o1 model with reinforcement learning, achieving advanced\nreasoning capabilities beyond traditional autoregressive methods. We demonstrate\nthe efficacy of OpenR by evaluating it on the MATH dataset, utilising publicly\navailable data and search methods. Our initial experiments confirm substantial\ngains, with relative improvements in reasoning and performance driven by test-\ntime computation and reinforcement learning through process reward models.\nThe OpenR framework, including code, models, and datasets, is accessible at\nhttps://openreasoner.github.io.", "sections": [{"title": "1 Introduction", "content": "OpenAI has recently unveiled o1 [OpenAI, 2024], a groundbreaking large language model (LLM)\nthat represents a giant leap forward in strong AI. The model is reported to be five times more\nproficient in math and coding compared to the previous GPT-40, specifically displaying exceptional\nperformance across various domains: it ranks in the 89th percentile for competitive programming,\nplaces among the top 500 students in a prestigious US math olympiad qualifier, and surpasses human\nPhD-level accuracy in physics, biology, and chemistry benchmarks. Trained using reinforcement\nlearning techniques, o1 excels in complex reasoning tasks by explicitly embedding a native \u201cChain-of-\nThought\" (NCoT) process in LLMs, which allows it to \u201cdeep think\u201d through step-by-step reasoning\nbefore generating responses. A key innovation of o1 is that it allows spending more time reasoning\nduring the inference process, marking a shift from fast, direct responses to slow, deliberate, multi-step\ninference-time computation, as illustrated in Figure 1.\nInterestingly, in human cognition, two correlated yet distinct modes of cognitive processing are\npresented to guide human decision-making and behaviours [Kahneman, 2011], each of which has the\npartial distinction between brain circuits and neural pathways. System 1 thinking is fast, automatic,\nand intuitive, operating effortlessly and often unconsciously. It relies on neural pathways that enable\nrapid processing, especially in situations needing quick reactions or when cognitive resources are\nconstrained. System 2 thinking is deliberate, effortful, and conscious, involving focused attention and\nanalytical reasoning. It processes information more slowly and is used for complex problem-solving,\nlogical reasoning, and decision-making tasks.\no1 model is an exciting development for AI, as LLMs can now not only generate rapid responses using\nlearned patterns but, more significantly, simulate complex reasoning processes through mechanisms\nlike chain-of-thought or other forms of search, similar to how humans engage in deeper, step-by-\nstep thinking. ol's improved reasoning skills induce implications for multiple fields, including\nscience, coding, and mathematics. In coding competitions, a specialised version of o1 achieved\nimpressive results, scoring in the 49th percentile in the 2024 International Olympiad in Informatics\nand outperforming 93% of human competitors in simulated Codeforces contests. Beyond its technical\ncapabilities, o1 also represents progress in AI safety and alignment. The model's chain of thought\nreasoning provides new opportunities for integrating human values and principles, resulting in\nimproved performance on safety evaluations and jailbreak tests.\nThe idea of chain-of-thought reasoning [Wei et al., 2022] and step-by-step thinking in large language\nmodels (LLMs) is not new. Previous research has shown that simply adding instructions like \u201cdescribe\nyour reasoning in steps\u201d or \u201cexplain your answer step by step\" to the input questions or providing\na few shot examples can trigger LLMs to generate intermediate reasoning steps (as illustrated in\nFigure 1) and subsequently improve problem-solving, especially in tasks like math and coding [Wei\net al., 2022, Nye et al., 2021]. However, these approaches build on existing LLMs without truly\nembedding the chain of thought ability within the models themselves. As a result, LLMs cannot\ninherently learn this reasoning capability, leading to active research on how to integrate it directly into\nmodel training. Proposed methods range from collecting specialised training data to building reward\nmodels [Ouyang et al., 2022, Li et al., 2022, Luo et al., 2024] and increasing the computational\ncomplexity of decoding [Snell et al., 2024, Wu et al., 2024], but none have yet achieved significant\nperformance breakthroughs at scale.\nIt remains unclear whether ol's innovation is rooted in the model itself, rather than relying on external\nprompting systems. If it indeed involves explicitly embedding step-by-step reasoning natively within\nthe architecture, this would represent a significant breakthrough. Building on substantial performance\ngains, 01 has shown that the scaling principles traditionally applied during training [Kaplan et al., 2020,\nSnell et al., 2024] are now relevant to the inference phase. We should reallocate our computational\nfocus, balancing pre-training efforts with efficient use of inference-time computation. Allowing LLMs\nto enhance their outputs with increased test-time computing is an essential step towards creating\ngenerally self-improving agents capable of managing open-ended strong reasoning and decision-\nmaking tasks. This direction, which we refer to as LLM-Native Chain-of-Thought (NativeCoT),\nshould be able to inherently mirror the deliberate, analytical process possessed by human's System 2\nthinking [Kahneman, 2011].\nIn this report, we present OpenR, an open-source framework built on the principles behind OpenAI's\no1 model, designed to replicate and extend its reasoning capabilities. Our approach focuses on\nimproving LLM reasoning by integrating process supervision, reinforcement learning (RL), and\""}, {"title": "2 Related Work", "content": "Key references in the field of improving reasoning capabilities in large language models (LLMs)\nhighlight several innovative approaches, including inference-time computing, process reward models,\nand data acquisition methods.\nInference-time Computing. To discuss the role of inference-time computation in large language\nmodels (LLMs), recent studies have focused on optimizing the efficiency and effectiveness of\nreasoning during the inference process rather than merely relying on the scaling law of training-time\ncomputing. A pivotal study, Feng et al. [2024] demonstrate the benefits of using MCTS as a decoding\nmechanism, which enhances inference computation by actively planning and selecting higher-quality\nresponses. This approach aligns with the reasoning-as-planning approach proposed in Hao et al.\n[2023], where reasoning is viewed as a process similar to planning in decision-making processes,\nfurther underscoring the centrality of step-wise reasoning at inference time. In recent, the work [Snell\net al., 2024] reinforces that optimizing inference strategies can yield superior performance gains\ncompared to simply increasing model size, underscoring the critical role of test-time computation.\nFinally, this is complemented by the findings of work [Goyal et al., 2023], which introduces an\nimplicit reasoning model by incorporating pause tokens to encourage deliberate reasoning during\ngeneration. Collectively, these recent advances suggest the growing recognition of inference-time\noptimisation - whether through planning-based reasoning models or computational optimisation\n- as a critical factor in improving LLM capabilities, advocating for strategies that enhance reasoning,\nplanning, and compute efficiency beyond mere training-time scaling.\nFrom Outcome Supervision to Process Supervision. The shift from Outcome Supervision to\nProcess Supervision in language model training has gained prominence in recent research, particularly\nwith respect to enhancing reasoning capabilities. The foundational work by Cobbe et al. [2021a]\nintroduces Outcome-supervised Reward Models (ORM) and the widely used math reasoning dataset,\nGSM8K, where verifiers are trained to assess the final correctness of generated solutions. While\nORM plays a crucial role in the early stage, it primarily focuses on evaluating the end result rather\nthan the reasoning steps leading to the final output.\nBuilding on this, the concept of process reward models (PRM) is introduced as a more granular\nand transparent approach. With both ORM and PRM, DeepMind proposes the idea of supervising\nintermediate reasoning steps alongside the final outcome, allowing for more detailed feedback during\nthe reasoning process [Uesato et al., 2022]. This research laid the groundwork for subsequent\ndevelopments in process-based verification. On the other hand, OpenAI's work [Lightman et al.,\n2023] continues this trend by refining PRM through a follow-up study that emphasizes verifying each\nintermediate step in reasoning tasks by providing a high-quality human-labelled process-supervision\ndataset, namely PRM800K, which has been enriched in our work.\nData Acquisition. The problem of Data Acquisition for PRM has evolved significantly, focusing on\nautomating the extraction of step-by-step reasoning data, which is crucial for training models capable\nof complex reasoning tasks. The STaR technique [Zelikman et al., 2022] presents a novel self-taught\nreasoning approach where models generate and bootstrap their own reasoning processes for further\ntraining, thus improving reasoning capabilities without extensive labelled datasets. Building upon\nthe foundation laid by STaR, Zelikman et al. [2024] demonstrate how these techniques could be\ngeneralized beyond specific domains like mathematical problem-solving. By extending the reasoning\nprocess to arbitrary tasks and incorporating the methodology into pre-training, Quiet-STaR highlights\nthe versatility of automated process supervision across various tasks, marking a significant step\nin scaling data acquisition for reasoning tasks. In addition, Luo et al. [2024] represent the latest\nadvancement in the field, specifically focusing on mathematical reasoning. This work refines the\nmethods for automated data acquisition, making the process more robust and applicable to increasingly\ncomplex problem-solving scenarios. Moreover, Wang et al. [2024a] take the concept of automatic\nprocess supervision a step further by proposing a practical solution for training models without\nrelying on human-labelled data. Finally, the empirical results in Wang et al. [2024b] extend these\napproaches by testing their applicability on coding tasks, demonstrating that process supervision\ncan be effectively induced by the model itself. These works underscore the increasing reliance on\nautomated data acquisition methods, where models are equipped to extract and verify their self-\nreasoning processes. To facilitate the research in this direction, we make the generated dataset and\ncode publicly available.\nIn summary, advanced reasoning in models such as OpenAI's o1 relies heavily on careful data\nselection, sophisticated PRM training, and enhanced decoding methods. Approaches such as tree-\nbased search, reinforcement learning, and step-aware verifiers enable these models to tackle more\ncomplex tasks. As research progresses, LLMs are expected to further enhance their autonomous\nreasoning, planning, and problem-solving capabilities. Our project aims to serve as a starting point\nfor transparently investigating and evaluating the potential of inference-time computation."}, {"title": "3 The OpenR LLM Reasoning Framework", "content": "To model the process of reasoning in tasks such as question-answering or problem-solving, we\nstructure the reasoning task using the $Q \\rightarrow {R} \\rightarrow A$ sequence, where:\n\u2022 Q represents the question or prompt that initiates the reasoning process;\n\u2022 R represents the sequence of intermediate reasoning steps the model generates to build\ntoward the solution;\n\u2022 A represents the final answer or solution produced after the reasoning steps.\nThis structure allows the LLM to generate a sequence of reasoning steps that logically connect\nthe question Q to the final answer A. We can define the reasoning process as a Markov Decision\nProcess (MDP) [Bellman, 1958]. An MDP representation offers a flexible framework for modelling\nreasoning. It allows the model to generate sequential reasoning steps toward the final answer step\nby step while also enabling a tree structure by sampling multiple paths at each step for alternative\nreasoning trajectories. By combining both approaches - sequential and branching reasoning - the\nmodel can explore diverse solutions, creating a versatile and comprehensive reasoning process.\nIn an MDP (as illustrated in Figure 2), the LLM policy functions by generating tokens that combine\nto form higher-level reasoning constructs. States represent the sequence of reasoning steps taken up\nto the current point, while actions involve selecting the next reasoning step or the final answer. The\nLLM policy generates these action choices, and the process reward model (PRM) [Lightman et al.,\n2023, Uesato et al., 2022] offers feedback on the quality of both the reasoning steps and the final\nanswer. The PRM guides the LLM toward producing accurate and meaningful reasoning processes\nby optimising the policy to maximise the reward."}, {"title": "3.1 System Design", "content": "The process reward model (PRM) plays a crucial role in enhancing the LLM's policy in two key ways.\nFirst, during training, the PRM improves the LLM policy through policy optimisation techniques\n(Policy Iteration as shown in Figure 3). Second, during the decoding phase, the PRM guides the\nLLM's search process, steering the reasoning toward more effective outcomes (as shown in Figure 3).\nAs we will show next, the LLM policy also helps identify missing intermediate reasoning steps,\nwhich in return enables further training and refinement of the PRM. As shown in Figure 3, this\niterative interaction allows the LLM and PRM to unlock each other's potential for improved reasoning\ncontinuously."}, {"title": "3.2 Data Augmentation", "content": "For a solution or chain-of-thought provided by large language models (LLMs), we use more precise\nand fine-grained feedback instead of relying solely on the final answers. We collect data for process\nsupervision, which provides step-wise feedback for a given solution. Formally, a PRM computes\n$P_t = PRM([q, X_{1:t-1}], x_t)$, where $x_{1:t} = [x_1,\\dots, x_t]$ represents the first t steps of the solution.\nThis method provides more precise and fine-grained feedback compared to outcome reward models\n(ORMS), as it identifies the exact location of errors within the problem-solving process [Lightman\net al., 2023].\nMATH-APS. We augment the data by automatically generating synthetic samples. In addition to\nthe PRM800k dataset [Lightman et al., 2023], which relies on costly human annotation and is difficult\nto scale, we introduce a new dataset called MATH-APS, based on MATH [Hendrycks et al., 2021],\nusing automated methods such as OmegaPRM [Luo et al., 2024]. This approach reduces the reliance\non expensive human annotations, enabling more scalable data collection. Automatic methods such as\nOmegaPRM, Math-Shepherd [Wang et al., 2024a] and MiPS [Wang et al., 2024b] efficiently collect\nhigh-quality process supervision data. While Math-Shepherd and MiPS provide automatic annotation\nfor process supervision, they require lots of policy calls, making them computationally expensive.\nOmegaPRM improves this process by iteratively dividing the solution, performing rollouts, and\nidentifying the first incorrect step in a model's solution.\nWe follow OmegaPRM [Luo et al., 2024] and collect PRM training examples by constructing a\nstate-action tree using LLMs. For each question, a tree is built where each node contains the question\nq, the solution prefix s, and all previous rollouts ${ (s,r_i) }_{i=1}^I$ (with $r_i$ indicating the i-th rollout).\nEach edge represents a single step or a sequence of steps from the node. For each node, we calculate\nthe Monte Carlo estimation MC(s) and the value function Q(s, r) to guide the selection of rollouts\nduring tree traversal. The value function is defined as: $Q(s,r) = \\alpha\\cdot \\frac{1}{1 + e^{-MC(s)}} - \\beta\\cdot len(r)$, where\n$\\alpha$, $\\beta$, and L are constants, and len(r) is the length of the rollout. We also compute the exploration\nterm: $U(s) = C_{puct}\\sqrt{ \\frac{L}{N(s)} } \\frac{1+L}{1+N(s_i)}$, where N(s) is the visit count and $C_{puct}$ is a constant encouraging\nexploration. During the selection phase, a rollout is chosen using a variant of the PUCT algorithm:\n$(s,r) = arg\\ max_{(s,r)}[Q(s, r) + U(s)]$. This heuristic selects the most valuable rollouts. A binary\nsearch is then used to identify the first error in the selected rollouts, and rollouts with 0 < MC(s)\nare added to the candidate pool. All positions before the first error become new states for further\nexploration."}, {"title": "3.3 Supervised Training for PRMS", "content": "In PRMs, the goal is to determine whether the sequence of the solution process is currently on the\nright track, so it should output a binary indicator of correctness. Specifically, we assign a score $Y_t$\nbetween 0 and 1 given a problem q and a sequence of solution steps $X_1 \\rightarrow X_{xt}$. This score represents\nthe correctness of the current problem-solving process. As a result, the problem is reframed as\n$Y_t = PRM(q, X_1,X_2,\\cdots, x_t)$, which can be treated as a binary classification task. The PRM is\ntrained through supervised fine-tuning on a LLM, with the correct/incorrect distinction serving as the\nclassification label. We then use the LLM to predict the next token of the step token.\nMath-psa. The PRM is trained through supervised fine-tuning on an LLM, with the correct/incorrect\ndistinction serving as the classification label. We train a PRM named Math-psa using datasets such\nas PRM800K [Lightman et al., 2023], Math-Shepherd [Wang et al., 2024a], and our MATH-APS\ndataset (see Section 3.2). These datasets are structured into three components: question, process,\nand label. The input consists of a concatenation of the question and the process. In the process, the\nsolution is divided into multiple steps, each separated by a special step token (\u201c\\n\\n\\n\\n\\n\u201d), marking\nthe end of each step where the PRM can make predictions. The label is a classification of the entire\nprocess, with each step labelled as either '+' or '-' based on the correctness of the solution.\nDuring training, the data is fed to the LLM as a next-token prediction task. The model is trained\nto predict a positive or negative token immediately following each step token. As described in the\ndata section, the input consists of the question concatenated with the process, and the step tokens\nseparate the steps in the process. The labels are assigned such that at the positions of the step token,\nthe label is either a positive or negative token, while all other positions are ignored during the loss"}, {"title": "3.4 Policy Learning for LLMs", "content": "We transform math problems into a language-augmented Markov Decision Process (MDP) $M =$\n$(V, S, A, T, R, \\gamma)$ [Van Otterlo and Wiering, 2012, Carta et al., 2023]. Given V the vocabulary and\nw \u2208 V the tokens, $A \\subset V^N, S \\subset V^N$ are action and state space, respectively, i.e., actions and\nstates are sequences of tokens. $T : S \u00d7 A \\rarr S$ is the state transition function. $R : S \u00d7 A \\rarr \\mathbb{R}$ is\nthe reward function that responds to each action, and $\u03b3$ is the discounted factor that typically less\nthan 1. An initial state $s_0$ represents a given problem in mathematical problem-solving scenarios.\nA language model receives this input and generates an intermediate reasoning step, denoted as an\naction $a_0$. This action $a_0$ is then concatenated with the initial problem $s_0$ to form the subsequent state\n$s_1$, which is used to infer the next action $a_1$, This iterative process continues, with each state-action\npair successively informing the following state, i.e., $T : S_{t+1} = {s_t, a_t}$ at time step t, until the\nmodel arrives at the final answer. After inferring each action $a_t$, the model receives a reward signal\n$r_{PRM} = R(s_t,a_t)$ from a well-trained PRM. Following this process with trajectories of a maximum\ntimestep T, the agents earn a discounted cumulative return of $R = \\sum_{t=0}^T \\gamma^t r_{PRM}$, which is aimed\nto be maximised by RL algorithms. We correspondingly implement this MDP as a reinforcement\nlearning environment like OpenAI's Gym. In these environments, math problems are presented as\ntasks where the model takes sequential actions to solve the problem, receiving rewards for correct\nactions and penalties for incorrect ones, which enables the model to iteratively learn and refine its\nproblem-solving strategies through trial and error, ultimately enhancing its mathematical reasoning\nskills.\nRL Training. Training LLMs with reinforcement learning often involves Proximal Policy Opti-\nmisation (PPO) [Schulman et al., 2017] to align generated language outputs with desired actions.\nThis approach bridges the gap between language understanding and actionable outputs by reinforcing\nthe generation of responses that are both contextually accurate and aligned with predefined goals,\neffectively linking language comprehension with strategic planning. We provide both traditional PPO\nand an efficient variant of PPO, i.e., Group Relative Policy Optimisation (GRPO) [Shao et al., 2024].\nThe primary distinction between these two lies in their approaches to advantage value estimation.\nSpecifically, PPO utilises a network to approximate the state value function, leveraging the Gener-\nalized Advantage Estimation (GAE) technique [Schulman et al., 2015] to derive the advantage. In\ncontrast, GRPO simplifies this process by directly employing a normalized reward signal to estimate\nan action's advantage, i.e., $A(s_t, a_t) = 1 - \\frac{r_{PRM} - mean(r_{PRM})}{std(r_{PRM})}$. Compared with PPO, GRPO bypasses\nthe need for an extra critic network and reduces the resources consumed during training, however, it\nemphasizes the stability of PRMs more."}, {"title": "3.5 Decoding: Inference-Time Guided Search and Planning", "content": "Following Snell et al. [2024], we use PRMs to assess the accuracy of each solution step. Once a\nhigh-quality process reward model is trained, we integrate it into the decoding process alongside the\nlanguage model, enabling guided search and scoring or voting across multiple generations.\nTo use PRMs as verifiers, we define a method for evaluating the correctness of LLM-generated\nsolutions. Specifically, we map the scores of individual steps ${r_{PRM}}_{t=0}^T$ to a final score. Following\nthe strategies outlined by Lightman et al. [2023] and Snell et al. [2024], we employ two approaches:\n\u2022 PRM-Min: choose the minimum value among all scores, i.e., $v = \\min{r_{PRM}}_{t=0}^T$.\n\u2022 PRM-Last: choose the last step's score as the final score, i.e., $v = r_{PRM}^T$. This strategy has\nbeen shown to be as good as PRM-Min in Snell et al. [2024].\nOnce multiple answers are generated by scaling test-time computations, we need strategies to select\nthe best answer based on their scores. We adopt three strategies from Feng et al. [2024]:\n\u2022 Majority-Vote: Aggregate answers using majority vote: $f^* = arg\\ max_f \\sum_{y_i} \\mathbb{1}_{final\\_ans(y_i)=f}$,\nwhere $\\mathbb{1}$ is the indicator function."}, {"title": "4 Experiments", "content": "To demonstrate the capabilities of our OpenR framework, we present quantitative results on large\nlanguage model inference and training. We evaluate our open framework using the MATH dataset\n[Hendrycks et al., 2021], which includes a wide range of high-school competition-level math problems.\nThis makes it an ideal benchmark for testing reasoning skills. To ensure fair comparisons with previous\nwork and reduce overfitting, we follow Lightman et al. [2023] and use a subset of 500 problems for\nevaluation, known as MATH500, in which the problems are sampled randomly."}, {"title": "4.1 Scaling LLM Test-Time Compute", "content": "Setting. Our PRM model, Math-psa, is fine-tuned from the Qwen2.5-Math-7B-Instruct [Yang et al.,\n2024] model using multiple datasets, including PRM500K [Lightman et al., 2023], Math-Shepherd\n[Wang et al., 2024a], and our MATH-APS dataset (originally over 500k state-value pairs collected\nfrom Qwen2.5 Math models, reduced to approximately 150k pairs after cleaning and preprocessing).\nIn the meantime, we also experiment with Math-Shepherd PRM for comparison purposes. Following\nSnell et al. [2024], we employ best-of-N and beam search algorithms for test-time computation. We\ncompare multiple test-time computation schemes across pre-defined budgets of token generation.\nAmong different aggregation strategies, we select PRM-Last as a representative. The LLM inference\nserver is implemented using FastChat [Zheng et al., 2023]."}, {"title": "4.2 Online Policy Learning for LLM", "content": "Setting. In the policy learning experiment, we use the Qwen2.5-1.5B-Math-Instruct model as the\npolicy model for training, with the Math-Shepherd model [Wang et al., 2024a] serving as the PRM\nto provide feedback during RL. In addition to the MATH500 dataset, we test the performance of\nthe model on a specific math problem: \u201cHow many positive whole-number divisors does 196 have?\u201d\nwith the final answer being \"9.\""}, {"title": "4.3 Case Study", "content": "Comparison of PRMs. Figures 6 and 7 compare the responses of the Math-psa PRM and the\nMath-Shepherd PRM to a given reasoning step. The PRM scores for each step are represented as\n\"[Math-psa (ours) Score, Math-Shepherd PRM Score]\u201d. Figure 6 shows Math-psa (ours) is able to\nassign more reasonable scores to reasoning steps. Particularly at Step 4 and Step 5, Math-psa shows\nComparison between LLMs Before and After Policy Learning. Figures 8 and 9 present cases\nfrom models before and after policy learning, respectively. Figure 8 shows the results prior to policy\nlearning, collected from the vanilla policy of Qwen2.5-Math-1.5B-Instruct. In this instance, the\nmodel makes an error by incorrectly using the perimeter of a triangle as the side length, resulting in\nan incorrect answer. Figure 9 uses the same question as Figure 8 and demonstrates the improvements\nafter applying the Math-psa PRM. The Math-psa PRM is able to correct the error, showing that the\nRL training process with a well-designed PRM can improve reasoning accuracy.\nComparison of Different Search Methods. Figures 10, 11, and 12 present examples that demon-\nstrate how different test-time compute methods impact the reasoning outputs. The three question-\nand-answer sessions show that both Best-of-N and Beam Search reasoning accurately addressed\nthe questions, thoroughly understanding them and providing correct calculations. In contrast, CoT\nreasoning misinterprets the question, leading to calculation errors starting from Step 2. This suggests\nthat Best-of-N and Beam Search reasoning benefit from a larger search space, enabling them to\nexplore more reasoning paths and converge on the correct solution."}, {"title": "5 Conclusion", "content": "In this work, we have introduced OpenR, an open-source framework designed to advance reasoning\ncapabilities in large language models (LLMs) through the integration of test-time computation,\nreinforcement learning, and process supervision. Our framework provides an open and accessible\nplatform for experimenting with reasoning tasks in LLMs, showcasing how test-time compute, search\nalgorithms, and process reward models (PRMs) can be combined to improve reasoning performance.\nWe believe OpenR will serve as a valuable resource for the research community, offering a compre-\nhensive platform for further exploration of reasoning in LLMs. By making our models, data, and\ncode publicly available, we aim to accelerate advancements in AI reasoning, fostering collaboration\nand innovation in the field. In future work, we plan to extend the framework to support a wider range\nof reasoning tasks and optimise inference-time computation for even more efficient and scalable\nreasoning models."}, {"title": "6 Limitations", "content": "Limited Scale of Experiments: Due to restricted access to large-scale computing infrastructure, our\nevaluations were conducted on relatively smaller models and datasets.\nModel Size: We primarily utilised mid-sized models as our base LLMs. While these models\ndemonstrated substantial gains, larger models could further enhance reasoning capabilities.\nLimited Process Supervision Data: While we leveraged datasets like PRM800K, Math-Shepherd, and\nour generated MATH-APS dataset for training, the scale and diversity of process supervision data\nremain limited.\nFuture work could focus on scaling up experiments, expanding the training datasets, and testing across\na broader range of models and domains to unlock further improvements in reasoning performance."}]}