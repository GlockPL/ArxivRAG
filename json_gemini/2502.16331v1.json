{"title": "A Gap Between the Gaussian RKHS and Neural Networks: An Infinite-Center Asymptotic Analysis", "authors": ["Akash Kumar", "Rahul Parhi", "Mikhail Belkin"], "abstract": "Recent works have characterized the function-space inductive bias of infinite-width bounded-norm single-hidden-layer neural networks as a kind of bounded-variation-type space. This novel neural network Banach space encompasses many classical multivariate function spaces including certain Sobolev spaces and the spectral Barron spaces. Notably, this Banach space also includes functions that exhibit less classical regularity such as those that only vary in a few directions. On bounded domains, it is well-established that the Gaussian reproducing kernel Hilbert space (RKHS) strictly embeds into this Banach space, demonstrating a clear gap between the Gaussian RKHS and the neural network Banach space. It turns out that when investigating these spaces on unbounded domains, e.g., all of $R^d$, the story is fundamentally different. We establish the following fundamental result: Certain functions that lie in the Gaussian RKHS have infinite norm in the neural network Banach space. This provides a nontrivial gap between kernel methods and neural networks by the exhibition of functions in which kernel methods can do strictly better than neural networks.", "sections": [{"title": "1. Introduction", "content": "In supervised learning, we observe samples with corresponding labels, which may represent classes or continuous values. Our primary objective is to construct a function $f : \\mathbb{R}^d \\to \\mathbb{R}$ based on these observations that can accurately predict labels for new, unseen data points. Traditionally, reproducing kernel Hilbert spaces (RKHS) have provided a principled framework for this task, offering both theoretical guarantees and practical algorithms. Their power stems from the representer theorem, which ensures that optimal solutions can be expressed as combinations of kernel functions centered at the training points.\nHowever, the landscape of machine learning has evolved significantly with the emergence of neural networks, which have demonstrated remarkable success across diverse applications over kernel methods. The simplest neural architecture\u2014the single-hidden layer network-builds upon the concept of ridge functions, which map $\\mathbb{R}^d \\to \\mathbb{R}$ via the form $x \\to \\sigma(w^\\top x)$, where $\\sigma : \\mathbb{R} \\to \\mathbb{R}$ is a univariate function and $w \\in \\mathbb{R}^d \\setminus \\{0\\}$. In practice, these networks combine multiple ridge functions:\n$$x \\mapsto \\sum_{k=1}^K v_k \\sigma(w_k^\\top x - b_k),$$"}, {"title": "2. Related Work", "content": "Approximability with Kernel Methods Bach (2017) studied various classes of single-/multi-index models with low intrinsic dimension and bounded RBV2(Rd)-norm. In contrast, Ghorbani et al. (2019) showed that if the covariates have the same dimension as the low intrinsic dimension of the target function, kernel and neural network approximations can be competitive. Empirically, some works show that the curse of dimensionality with kernel methods can be handled with an appropriate choice of dataset-specific kernels (Arora et al., 2019; Novak et al., 2018; Shankar et al., 2020) or mirroring neural network training dynamics closely to kernel methods (Mei et al., 2018; Sirignano and Spiliopoulos, 2020; Rotskoff and Vanden-Eijnden, 2022; Chizat and Bach, 2018). But a wide body of work has also shown a gap in approximation with neural networks capturing richer and more nuanced class of functions compared to kernel methods (see (Allen-Zhu and Li, 2019; Mei et al., 2018; Yehudai and Shamir, 2019; Ghorbani et al., 2019)). In our work, we show that while Gaussian RKHS is embedded within neural networks in bounded domains, in the unbounded regime there exists a non-trivial gap between HGauss (Rd) and RBV2 (Rd).\nThe Function Spaces RBV2 (Rd) The function space RBV2(\u03a9) (bounded variation in the Radon domain) naturally characterizes the function approximation capabilities of shallow ReLU neural networks. Parhi and Nowak (2021) established a representer theorem, showing that solutions to variational problems over RBV\u00b2(\u03a9) correspond to single-hidden layer ReLU networks with weight decay regularization. Unlike RKHS, which suffers from the curse of dimensionality, RBV\u00b2(\u03a9) enables efficient function representation by capturing low-dimensional structure. Moreover, there are function classes for which neural networks achieve near-minimax optimal approximation rates as shown in Parhi and Nowak (2023), while kernel methods cannot. This suggests that, while RKHS embeddings may appear restrictive, RBV2(\u03a9) provides a more expressive framework, positioning shallow networks as solutions to infinite-dimensional variational problems with superior generalization properties. For further details see (Parhi and Nowak, 2021, 2022; Parhi and Unser, 2025)\nEmbeddings of RKHSs and RBV2(\u03a9) For a Lipshitz open domain \u03a9 \u2286 Rd, it is well-known that the Sobolev space H\u00ba(\u03a9) is (equivalent to) an RKHS if and only if s > d/2, e.g. the Laplace and Mat\u00e9rn kernels are associated with Sobolev RKHSs (see, e.g., Kanagawa et al. (2018), Example 2.6). In contrast, the Gasussian RKHS HGauss(\u03a9) is contained in every Sobolev space, i.e., HGauss (\u03a9) \u2282 \u0397 (\u03a9) for all s \u2265 0 (cf., Steinwart and Christmann (2008), Corollary 4.36). Recent work has further demonstrated that the RKHSs of typical neural tangent kernel (NTK) and neural network Gaussian process (NNGP) kernels for the ReLU activation function are equivalent to the Sobolev spaces H(d+1)/2(Sd) and H(d+3)/2 (Sd), respectively (Bietti and Bach, 2021; Chen and Xu, 2021). Steinwart et al. (2009) has shown that an optimal learning rates in Sobolev RKHSs can be achieved by cross-validating the regularization parameter. On another front, embedding properties relating Sobolev spaces and the Radon bounded variation (RBV) spaces have been explored; for example, Ongie et al. (2020) showed that Wd+1(L1(Rd)) embeds in RBV2(Rd). More recently, Mao et al. (2024) established a sharp bound by proving that W\u00b3(Lp(\u03a9)) with s \u2265 2 + (d + 1)/2 for p > 2 embeds in RBV\u00b2(\u03a9) for bounded domains \u03a9\u2282 Rd."}, {"title": "3. Problem Setup and Preliminaries", "content": "3.1. Notation and Basic Definitions\nLet X C Rd denote the centers (aka sample) space, where d \u2208 N represents the odd dimension of the input space. Throughout this work, we focus on the case where X = Rd, considering the full Euclidean space.\n3.2. Gaussian Reproducing Kernel Hilbert Space\nWe begin by defining a reproducing kernel Hilbert space (RKHS) associated with a Gaussian kernel on an infinite domain. For a given positive definite Mahalanobis matrix $M\\in \\text{Sym} \\left(\\mathbb{R}^{d\\times d}\\right)$, we define the Gaussian kernel $k_M : \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}$ as:\n$$k_M(x, y) = \\exp \\left(\\frac{-\\Vert x - y\\Vert_M^2}{2 \\sigma^2} \\right),$$$$||x - y||_M = \\sqrt{(x - y)^\\top M(x - y)}.$$\nThe corresponding RKHS H is defined as the closure of the linear span of kernel functions:\n$$H := \\text{cl} \\left\\{f : X \\to \\mathbb{R} \\mid \\mathbb{R}^n \\in \\mathbb{N}, f(.) = \\sum_{i=1}^n \\alpha_i \\cdot k_M(x_i, . ), x_i \\in X \\right\\}$$\nwhere the (squared) RKHS norm $\\| \\cdot \\|_\\mathcal{H}^2$ of a kernel machine $f \\in H$ is defined as $\\|f\\|_\\mathcal{H}^2 = \\sum_{i,j} \\alpha_i \\alpha_j k_M(x_i, x_j)$. Alternately, we can write $\\|f\\|_\\mathcal{H}^2 = \\alpha^\\top K \\alpha$ where $K = (k_M(x_i, x_j))_{i,j}$ is an $n \\times n$ matrix.\n3.3. Separated Sets and Function Spaces\nFor our analysis, we introduce two key definitions of separated sets that play a crucial role in our theoretical development.\nDefinition 1 (($\\beta, \\delta$)-separated set) For any given scalar $\\delta > 0$ and a vector $\\beta \\in \\mathbb{R}^d$, a ($\\beta, \\delta$)-separated subset of size $n \\in \\mathbb{N}$ is defined as\n$$C_n(\\beta, \\delta) := \\{\\{x_1, ..., x_n\\} \\mid \\forall x_i, x_j, |\\beta^\\top x_i - \\beta^\\top x_j| \\ge \\delta \\}$$\nThis could be further generalized to the notion\nDefinition 2 (($\\beta, \\delta, \\eta$)-separated set) For any given scalars $\\delta, \\eta > 0$ and a vector $\\beta \\in \\mathbb{R}^d$ a ($\\beta, \\delta, \\eta$)-separated subset of size $n \\in \\mathbb{N}$ is defined as\n$$C_n(\\beta, \\delta, \\eta) := \\{\\{x_1, ..., x_n\\} \\mid \\forall x_i, x_j, \\beta' \\in \\mathbb{R}^d, \\text{ s.t. } \\frac{\\beta^\\top \\beta'}{\\|\\beta\\| \\|\\beta'\\|} \\ge \\eta, |\\beta'^\\top x_i - \\beta'^\\top x_j| \\ge \\delta \\}$$"}, {"title": "4. RTV\u00b2 of a kernel machine", "content": "In this section, we study the RTV\u00b2 of kernel machines in the RKHS H. We show that one can write an explicit computable form for the case when the dimension is d is odd.\nConsider the underlying matrix M > 0 for the Gaussian kernel $k_M$ has the following Cholesky decomposition\n$$M = LL^\\top.$$\nSince M is full rank and is in Sym $(\\mathbb{R}^{p \\times p})$ this decomposition is unique. With this we state the following result on RTV2(f) of a kernel machine $f \\in H(\\mathbb{R}^d)$ with the proof in Appendix A.\nTheorem 4 Assume the dimension of the center space is d is odd. For a kernel machine $f \\in H(\\mathbb{R}^d)$ in the space of Gaussian RKHS. If f has the following representation\n$$f(.) = \\sum_{i=1}^k \\alpha_i k_M(x_i, . ),$$\nthen the RTV2 of f has the following form:\n$$RTV^2(f) = \\frac{1}{\\vert det L\\vert \\sqrt{2\\pi}} \\int_{S^{d-1}} \\frac{1}{\\|L^{-T}\\beta\\|} \\int_{\\mathbb{R}} \\Big\\vert \\sum_{i=1}^k \\alpha_i \\frac{\\partial^{d+1}}{\\partial t^{d+1}} exp \\left( \\frac{-(t - x_i^\\top \\beta)^2}{2 \\|L^{-T}\\beta\\|^2} \\right) \\Big\\vert dt d\\beta,$$\nwhere we have used the decomposition $M = L^\\top L$. Furthermore, this can be extended to the case when f has a representation with infinite kernel functions.\nProof outline The proof proceeds in three main steps: First, we leverage the metric factorization $M = L^\\top L$ to express the Gaussian kernel for a single center $x_0$ as\n$$g(x) = \\frac{1}{(2 \\pi)^{d/2} \\sigma^d} exp\\left( \\frac{-\\|L(x - x_0)\\|^2}{2} \\right).$$"}, {"title": "5. A sequence of kernel machines with diverging RTV2", "content": "In this section, we show a construction of a family of kernel machines $\\{f_n \\in H(\\mathbb{R}^d)\\}$ such that the corresponding R-norm, i.e. $\\{RTV^2(f_n)\\}$, diverges.\nFirst, we state some useful assumptions on probabilist's Hermite polynomial which are easy to verify to hold in general (but surely in odd dimension d).\nFirst assumption asserts that Gaussian weighted Hermite polynomial $H_{d+1}(y)e^{\\frac{-y^2}{2}}$ has a peak within a finite interval around the origin.\nAssumption 1 ($\\delta$-peak) Fix a dimension d. For a given Hermite polynomial $H_{d+1}$, we call an interval $[-\\delta, \\delta]$ a region of $\\delta$-peak if:"}]}