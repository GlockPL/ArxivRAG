{"title": "LLMCO2: ADVANCING ACCURATE CARBON FOOTPRINT PREDICTION FOR LLM INFERENCES", "authors": ["Zhenxiao Fu", "Fan Chen", "Shan Zhou", "Haitong Li", "Lei Jiang"], "abstract": "Throughout its lifecycle, a large language model (LLM) generates a substantially larger carbon footprint during inference than training. LLM inference requests vary in batch size, prompt length, and token generation number, while cloud providers employ different GPU types and quantities to meet diverse service-level objectives for accuracy and latency. It is crucial for both users and cloud providers to have a tool that quickly and accurately estimates the carbon impact of LLM inferences based on a combination of inference request and hardware configurations before execution. Estimating the carbon footprint of LLM inferences is more complex than training due to lower and highly variable model FLOPS utilization, rendering previous equation-based models inaccurate. Additionally, existing machine learning (ML) prediction methods either lack accuracy or demand extensive training data, as they inadequately handle the distinct prefill and decode phases, overlook hardware-specific features, and inefficiently sample uncommon inference configurations. We introduce LLMCO2, a graph neural network (GNN)-based model that greatly improves the accuracy of LLM inference carbon footprint predictions compared to previous methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) (BigScience, 2023; Meta, 2024; Mistral, 2024) have demonstrated high efficacy across various generative Natural Language Processing (NLP) tasks, such as code completion (Nam et al., 2024), question-answering (Shao et al., 2023), and text summarization (Pilault et al., 2020). Their integration into daily activities (e.g., web browsing (Campello de Souza et al., 2023)) highlights their increasing prevalence. However, this widespread adoption has led to significant carbon dioxide equivalent (CO2eq) emissions (Luccioni et al., 2024). For instance, training the Google T5 LLM generates 40% more carbon emissions than a round-trip flight between San Francisco and New York (Faiz et al., 2024).\nInferences for LLMs can produce an even larger carbon footprint than their initial training. Conservative estimates suggest that OpenAI handles over 270 million daily inference requests (Chien et al., 2023), with an average prompt length of 1.2K tokens per request (Patel et al., 2024). Training GPT-4 requires approximately 13 trillion tokens (OpenAI, 2024), with a single epoch requiring three times the FLOPs of an inference (Faiz et al., 2024). Consequently, the carbon emissions from 121 days of serving GPT-4 inferences equate to those of its training. As the volume of daily inference requests rises, and with increased adoption of LLMs across various applications (OpenAI, 2024), the period required for inference emissions to match training emissions is rapidly decreasing.\nGiven the significant environmental impact, it is essential for both end users and cloud providers to understand the carbon emission costs of different service-level objectives (SLOs) (Patel et al., 2024) for LLM inference accuracy and latency. An accurate carbon footprint prediction tool is crucial before initiating inference requests, enabling users to assess trade-offs between accuracy, latency, and carbon emissions. This tool would also help cloud providers justify billing policies transparently, promoting low-carbon practices among users.\nHowever, there is a lack of modeling tools for accurately estimating the carbon footprint of LLM inferences. Users submit LLM inference requests with varying configurations (e.g., batch size, prompt length, and token generation number) to cloud services, while cloud providers employ different GPU"}, {"title": "2 BACKGROUND", "content": "Autoregressive LLM Inferences. As shown in Figure 1, during autoregressive inference (Pope et al., 2023) of a decoder-only LLM, all input tokens are processed in parallel during the first iteration, generating the first token-this is the prefill phase (Patel et al., 2024). The context from the LLM's attention layers is stored in the key-value (KV) cache for future iterations. Subsequent tokens are then generated using the latest token and the KV cache as inputs, forming the decode phase (Patel et al., 2024).\nDistinct Characteristics of Two Phases. In autoregressive LLM inference, the prefill phase computes and stores context in the KV cache, while the decode phase primarily accesses this cache. The prefill phase is compute-bound, relying on GPU cores, whereas the decode phase is memory-bound, relying on GPU memory. Consequently, the phases differ in latency, energy consumption, and carbon footprint. As shown in Figure 2, during a Bloom-3b inference (BigScience, 2023) with a batch size of 1 on an Nvidia L4 GPU, the prefill phase's energy use is negligible for requests with many generated tokens but dominates when fewer tokens are generated. Treating both phases as a single task without sufficient training data significantly reduces energy prediction accuracy.\nKernels in a transformer Layer. As illustrated in Figure 4, a transformer layer (Vaswani et al., 2017) comprises a masked multi-head attention (MHA) layer and a feed-forward (FF) layer framed by normalization layers. In the MHA layer, the attention mechanism is executed via Qproj, Kproj,"}, {"title": "All-reduce", "content": "Given the huge memory demands of LLMs and the limited capacity of individual GPUs, multiple GPUs connected via PCIe or NVLink (Patel et al., 2024) are crucial for LLM inferences. Tensor parallelism (Aminabadi et al., 2022) splits tensors across GPUs and replicates all layers, providing a significant speedup over other parallelism strategies. To support tensor parallelism, two all-reduce kernels are incorporated into each transformer layer. An all-reduce kernel (Hidayetoglu et al., 2024) consists of a reduce-scatter operation followed by an all-gather operation, as shown in Figure 3. For instance, a 4 x 4 matrix, evenly distributed across four GPUs (each holding a column), undergoes reduce-scatter, where each row is assembled and summed on one GPU, followed by all-gather, where the summed values are shared across all GPUs."}, {"title": "Roofline model", "content": "The Roofline model (Cardwell & Song, 2019) is a performance analysis tool that estimates a kernel's performance on a GPU, measured in operations per second (OPs/s). It considers factors like peak GPU throughput, peak memory and network bandwidth, and the kernel's memory- and network-specific arithmetic intensities. The memory and network Roofline models for an Nvidia V100 GPU (Nvidia, 2017) are shown in Figures 5(a) and 5(b), respectively. The X-axes represent arithmetic intensity, calculated as total kernel operations divided by total memory or network bytes transferred. A ridge point, or 'balance' point, indicates where compute and data movement performance meet. The Nvidia V100 supports FP32, FP16, and INT8 operations with HBM or GDDR memory, resulting in six ridge points in Figure 5(a). Two network interfaces (NVLink and PCIe) yield two ridge points in Figure 5(b). Kernels with arithmetic intensities below the ridge point are memory- or network-bound, while those above are compute-bound. For instance, in Figure 5(a), the INT8 kernel 0 (Ko) is compute-bound with HBM but memory-bound with GDDR."}, {"title": "Request characterization", "content": "In real-world LLM serving clouds like Microsoft Azure, the distribution of inference configurations such as prompt length and generated token count is not uniform."}, {"title": "3 RELATED WORK", "content": "We compare LLMCO2 with prior work in Table 1. LLMCarbon (Faiz et al., 2024) employs an equation-based approach using FLOPs to estimate LLM inference carbon footprints, resulting in inaccuracies. Other models like nn-Meter (Zhang et al., 2021), DeepEn (Tu et al., 2023), and NNLQP (Liu et al., 2023) use random forests or neural networks to predict latency or energy for CNNs and transformers but treat predictions as a single task, overlooking the autoregressive nature of LLM inferences and failing to distinguish between prefill and decode phases. These methods focus solely on architecture-specific features like input sizes and network structures, relying on brute-force sampling across hardware platforms, yet they omit hardware-specific features such as GPU peak throughput, memory bandwidth, and network bandwidth, resulting in reduced accuracy for unseen hardware configurations. Furthermore, they do not address tensor parallelism across multiple GPUs and uniformly sample inference configurations, neglecting common settings like medium prompt lengths and small batch sizes. In contrast, LLMCO2 separates prefill and decode phases, incorporates hardware-specific features, supports multi-GPU tensor parallelism, and prioritizes frequently occurring inference configurations, achieving more precise carbon footprint predictions."}, {"title": "4 LLMCO2", "content": "We introduce LLMCO2, an accurate carbon footprint regression model for LLM inferences, as outlined in Figure 7. Firstly, we propose a novel graph embedding method that encodes a transformer's layer, running on one or more GPUs during inference, as a graph. In the graph, nodes correspond to the kernels within the transformer layer, while edges depict data dependencies between kernels. Each node is annotated with architectural features such as operation count, memory access size, and network transfer size, along with a hardware-specific feature, i.e., its Roofline performance. Node features are divided into two sets, one for the prefill phase and the other for the decode phase. We then employ graph convolution layers to process this graph. Global LLM architectural features, including total operation count, memory access, and network transfer size, are integrated with the processed graph features, and two linear layers use these to predict the LLM inference's operational energy. To convert operational energy consumption to carbon footprint, we apply the equation from Faiz et al. (2024):\n$CO2eqoper = energyoper \\cdot PUE \\cdot carb\\_inten$,\nwhere $CO2eqoper$ represents the operational carbon footprint, $PUE$ denotes the power usage effectiveness, and $carb\\_inten$ indicates the carbon intensity of the data center. The embedded carbon footprint is calculated using the equations provided in Faiz et al. (2024). Secondly, we introduce"}, {"title": "4.1 GRAPH EMBEDDING", "content": "We introduce a graph embedding technique for LLMCO2 to represent a transformer layer as a directed acyclic graph (DAG) of kernels: $G = (V, E)$, where $V$ is the set of nodes (kernels), and $E$ denotes the edges (data dependencies between kernels). Each edge's source kernel supplies input data to the destination kernel. The key advantages of this technique are: (1) each node has two distinct feature sets-one for the prefill phase and the other for the decode phase, and (2) each node includes a hardware-specific feature, i.e., Roofline performance. Figure 8 provides an example of this graph embedding for a transformer layer.\nNode features. A node $v \\in V$ corresponds to a kernel in the transformer layer. To accurately estimate the carbon footprint of the two distinct phases in an LLM inference, each node is assigned two sets of features: one for the prefill phase and the other for the decode phase. Each feature set concatenates the following elements:\n* T: Defines the type of the kernel. For instance, the value projection kernel $Vproj$ multiplies the input with its weight matrix $Wv$. More kernel types are detailed in Tables 6 and 7 of Appendix A. A one-hot vector is used to encode T.\n* S: Represents the dimensions of the kernel's input, weight, and output.\n* O: Refers to the number of operations of the kernel. While tools like CALFLOPS (Ye, 2023) can compute operation counts for kernels based on LLM architecture, they do not differentiate between the prefill and decode phases, or support the all-reduce kernel. Therefore, we provide equations for calculating O for the kernels used (excluding the all-reduce kernel) in Appendix A. For the same kernel, the operation count in the decode phase $(O^{dec})$ is proportional to the number of generated tokens $(Ngt)$, while in the prefill phase $(O^{pre})$, it is proportional to the input sequence length $(Lseq)$. For the all-reduce kernel operating on an $n \\times m$ matrix distributed across $l$ GPUs (each holding an $n/l \\times m$ portion), the operation count during the decode phase is given by Equation 2, and during the prefill phase by Equation 3. These operations occur during the reduce-scatter step.\n$O_{allr}^{der} = n(\\frac{m}{l} - 1)(Ngt - 1)$  \n$O_{allr}^{pre} = n \\cdot \\frac{m}{l} Lseq$   \n* M: Denotes the total memory footprint accessed by the kernel. We also provide equations for computing M (excluding the all-reduce kernel) in Appendix A. As with O, the memory footprint during the decode phase $(M^{dec})$ is proportional to $Ngt$, while in the prefill phase $(M^{pre})$, it is proportional to $Lseq$. For the all-reduce kernel, the total memory footprint during the decode phase is determined by Equation 4, where Da represents the data type of activations (e.g., FP16), and in the prefill phase by Equation 5.\n$M_{allr}^{dec} = 2\\frac{nm}{l}DA_{l}$  \n$M_{allr}^{pre} = 2\\frac{nm}{l}DA_{l}$  \n* I: Indicates the data size transferred over the GPU network interface for an all-reduce kernel. When the all-reduce kernel operates on an $n \\times m$ matrix distributed across $l$ GPUs, the data size transferred during the decode phase is calculated using Equation 6, and during the prefill phase"}, {"title": "4.2 FOCUSED ENERGY DATA SAMPLING", "content": "To train a carbon footprint regression model, it is crucial to construct a training dataset by sampling a variety of LLM architectural, inference, and hardware configurations. However, random sampling proves ineffective, often resulting in suboptimal predictive models due to the exclusion of vital data related to LLM architecture and inference configurations. Creating a new LLM demands considerable resources, including substantial training data and computational power. Consequently, most LLMs in academia and industry are based on a few foundational models (Meta, 2023), sharing similar architectural attributes, such as head count, layer number, hidden size, and intermediate size. Neglecting these established architectural parameters and relying on random sampling leads to poor regression accuracy. Additionally, as depicted in Figure 6, the distributions of input prompt lengths and generated token counts are non-uniform in major cloud-based LLM services. Treating all inference configurations uniformly and sampling evenly across all possible inference configurations yields an ineffective training dataset, failing to capture the prevalent configuration ranges of inference requests, thereby compromising the accuracy of carbon footprint predictions."}, {"title": "5 EVALUATION", "content": "5.1 EXPERIMENTAL METHODOLOGY\nDataset construction. We developed an energy dataset to evaluate the performance of various energy prediction methods, selecting six LLM series: Bloom (Bloom-560m, Bloom-1b1, Bloom-1b7, Bloom-3b, Bloom-7b1) (BigScience, 2023), Gemma (Gemma-2b, Gemma-7b) (Team et al., 2024a), Gemma2 (Gemma2-2b, Gemma2-9b, Gemma2-27b) (Team et al., 2024b), Qwen2 (Qwen2-0.5b, Qwen2-1.5b, Qwen2-7b, Qwen2-72b) (Yang et al., 2024), Llama3.1 (Llama3.1-8b, Llama3.1-70b) (Meta, 2024), and Mixtral (Mixtral-8\u00d77b) (Jiang et al., 2024). The LLM architectural design space was explored by varying parameters such as quantization bitwidth, hidden size, intermediate size, head count, and layer count. Public Azure LLM serving traces (Microsoft, 2024) were used to generate inference requests, encompassing two distinct traces: one for chat (19,336 entries) and the other for code completion (8,199 entries), for LLMCO2. We sampled inference configurations by adjusting input prompt lengths and generated token numbers, ensuring batch sizes remained under two. We adopted random sampling to generate inference requests with different input prompt lengths and generated token numbers for our baseline schemes. LLM inferences were executed using the GPU configurations detailed in Table 2, with the number of GPUs per inference ranging from the minimum required to a maximum of four, regardless of each hardware configuration's total GPU capacity. For the training dataset, we considered L4, A100, and H100 GPUs, while all four GPU configurations were included in the test dataset.\nMeasurement and implementation. We used the Nvidia Management Library (NVML) (NVIDIA, 2024) to measure the energy consumption of LLM inferences on the target GPUs. Each inference was executed 5 times, and the average energy consumption was recorded as the ground truth. LLMCO2 consists of two graph convolution layers and two linear layers. LLMCO2 was developed using the PyG package (Team, 2024) and trained on a Tesla L4 GPU. The model training used the Adam optimizer, with a learning rate of 0.001 and a batch size of 512.\nEvaluation metrics. We evaluated prediction accuracy using the Mean Absolute Percentage Error (MAPE) and Error Bound Accuracy (EBA(\u03b4)). MAPE quantifies the average absolute percentage"}, {"title": "5.2 OPERATIONAL ENERGY RESULTS", "content": "MAPE. Table 3 presents the comparison of Mean Absolute Percentage Error (MAPE) between LLMCO2 and various baseline schemes. On average, LLMCO2 achieves the lowest MAPE values across different LLMs. LLMCarbon estimates operational energy consumption based solely on FLOP counts, neglecting memory accesses within transformer layers and critical hardware-specific features, such as peak GPU memory and network bandwidth, resulting in the highest MAPE values for all LLMs. The two ML-based predictors, DeepEn and NNLQP, exhibit comparable average MAPE values. Due to a smaller training energy dataset size for Mixtral and Llama3.1, DeepEn, leveraging its random forest model, performs slightly better than NNLQP on these LLMs. Overall, LLMCO2 outperforms DeepEn and NNLQP, reducing the average MAPE by 51.4% and 45.6%, respectively, by treating the prefill and decode phases separately, incorporating kernel-specific Roofline performance, and training with energy data derived from public Azure LLM serving traces."}, {"title": "5.3 ABLATION STUDIES", "content": "We conducted ablation studies on EBA(10%) to evaluate the contribution of each component of LLMCO2, as summarized in Table 5. By using distinct node features for the prefill and decode phases, LLMCO2 improves EBA(10%) by 67% compared to NNLQP. In real-world LLM serving clouds, most inference requests involve medium-length prompts and fewer generated tokens, leading to significant errors when combining the two phases for carbon overhead prediction. Incorporating Roofline performance as a node feature further boosts LLMCO2's EBA(10%) by 13.1%, as this feature facilitates knowledge transfer from L4 to T4 GPUs in the test dataset. Finally, the focused energy data sampling technique elevates LLMCO2's EBA(10%) improvement to 123% of NNLQP, since training with data distributions that mirror real-world LLM inference configurations enhances its performance on test datasets with similar prompt lengths and generated token distributions."}, {"title": "6 USER CASE STUDIES", "content": "Carbon comparison between training and inference. LLM training requires numerous GPUs operating at high throughput over extended periods. For example, Mixtral-8\u00d77b training uses 512 A100 GPUs at about 50% peak throughput for three months (Jiang et al., 2024). In contrast, INT8 Mixtral-8\u00d77b inference utilizes four A100 GPUs at 10%-40% peak throughput for around 4 seconds. Figure 10 shows the normalized carbon footprint per GPU per second for Mixtral-8\u00d77b training and inference, relative to the training baseline. The embodied carbon per GPU per second is consistent across all configurations since A100 GPUs are used. The prefill phase of an inference with a batch size of 4 and a 6K prompt length has a similar operational carbon footprint per GPU per second to training, while the decode phase with a batch size of 1 and a 1K prompt length has a much lower footprint. Unlike training, the embodied carbon overhead dominates the decode phase of Mixtral-8\u00d77b inferences.\nInference on multiple GPUs. Additional GPUs can accelerate LLM inference (Pope et al., 2023), but using more GPUs than necessary is often unsustainable, particularly for real-world cloud inference configurations with small batch sizes and short prompts. Figure 10 shows the operational carbon footprint of Bloom-7b1 inferences across different GPU counts with varying prompt lengths and batch sizes. For example, although an inference with a batch size of 4 and a 1K-token prompt benefits from 2 or 4 GPUs, lowering the per-GPU carbon footprint, an inference with a batch size of 1 and a 64-token prompt incurs increased latency and higher per-GPU operational carbon due to the communication overhead of all-reduce kernels when using more GPUs. As shown in Figure 11, the total carbon overhead rises considerably with more GPUs for inferences with smaller batch sizes and prompts."}, {"title": "7 CONCLUSION", "content": "LLM inference produces a larger carbon footprint than training, necessitating accurate estimation tools for both users and cloud providers. Existing models fall short due to their inability to capture"}, {"title": "Node features", "content": "A node $v \\in V$ corresponds to a kernel in the transformer layer. To accurately estimate the carbon footprint of the two distinct phases in an LLM inference, each node is assigned two sets of features: one for the prefill phase and the other for the decode phase. Each feature set concatenates the following elements:\n* T: Defines the type of the kernel. For instance, the value projection kernel $Vproj$ multiplies the input with its weight matrix $Wv$. More kernel types are detailed in Tables 6 and 7 of Appendix A. A one-hot vector is used to encode T.\n* S: Represents the dimensions of the kernel's input, weight, and output.\n* O: Refers to the number of operations of the kernel. While tools like CALFLOPS (Ye, 2023) can compute operation counts for kernels based on LLM architecture, they do not differentiate between the prefill and decode phases, or support the all-reduce kernel. Therefore, we provide equations for calculating O for the kernels used (excluding the all-reduce kernel) in Appendix A. For the same kernel, the operation count in the decode phase $(O^{dec})$ is proportional to the number of generated tokens $(Ngt)$, while in the prefill phase $(O^{pre})$, it is proportional to the input sequence length $(Lseq)$. For the all-reduce kernel operating on an $n \\times m$ matrix distributed across $l$ GPUs (each holding an $n/l \\times m$ portion), the operation count during the decode phase is given by Equation 2, and during the prefill phase by Equation 3. These operations occur during the reduce-scatter step.\n$O_{allr}^{der} = n(\\frac{m}{l} - 1)(Ngt - 1)$  \n$O_{allr}^{pre} = n \\cdot \\frac{m}{l} Lseq$   \n* M: Denotes the total memory footprint accessed by the kernel. We also provide equations for computing M (excluding the all-reduce kernel) in Appendix A. As with O, the memory footprint during the decode phase $(M^{dec})$ is proportional to $Ngt$, while in the prefill phase $(M^{pre})$, it is proportional to $Lseq$. For the all-reduce kernel, the total memory footprint during the decode phase is determined by Equation 4, where Da represents the data type of activations (e.g., FP16), and in the prefill phase by Equation 5.\n$M_{allr}^{dec} = 2\\frac{nm}{l}DA_{l}$  \n$M_{allr}^{pre} = 2\\frac{nm}{l}DA_{l}$  \n* I: Indicates the data size transferred over the GPU network interface for an all-reduce kernel. When the all-reduce kernel operates on an $n \\times m$ matrix distributed across $l$ GPUs, the data size transferred during the decode phase is calculated using Equation 6, and during the prefill phase"}, {"title": "4.2 FOCUSED ENERGY DATA SAMPLING", "content": "To train a carbon footprint regression model, it is crucial to construct a training dataset by sampling a variety of LLM architectural, inference, and hardware configurations. However, random sampling proves ineffective, often resulting in suboptimal predictive models due to the exclusion of vital data related to LLM architecture and inference configurations. Creating a new LLM demands considerable resources, including substantial training data and computational power. Consequently, most LLMs in academia and industry are based on a few foundational models (Meta, 2023), sharing similar architectural attributes, such as head count, layer number, hidden size, and intermediate size. Neglecting these established architectural parameters and relying on random sampling leads to poor regression accuracy. Additionally, as depicted in Figure 6, the distributions of input prompt lengths and generated token counts are non-uniform in major cloud-based LLM services. Treating all inference configurations uniformly and sampling evenly across all possible inference configurations yields an ineffective training dataset, failing to capture the prevalent configuration ranges of inference requests, thereby compromising the accuracy of carbon footprint predictions."}, {"title": "5 EVALUATION", "content": "5.1 EXPERIMENTAL METHODOLOGY\nDataset construction. We developed an energy dataset to evaluate the performance of various energy prediction methods, selecting six LLM series: Bloom (Bloom-560m, Bloom-1b1, Bloom-1b7, Bloom-3b, Bloom-7b1) (BigScience, 2023), Gemma (Gemma-2b, Gemma-7b) (Team et al., 2024a), Gemma2 (Gemma2-2b, Gemma2-9b, Gemma2-27b) (Team et al., 2024b), Qwen2 (Qwen2-0.5b, Qwen2-1.5b, Qwen2-7b, Qwen2-72b) (Yang et al., 2024), Llama3.1 (Llama3.1-8b, Llama3.1-70b) (Meta, 2024), and Mixtral (Mixtral-8\u00d77b) (Jiang et al., 2024). The LLM architectural design space was explored by varying parameters such as quantization bitwidth, hidden size, intermediate size, head count, and layer count. Public Azure LLM serving traces (Microsoft, 2024) were used to generate inference requests, encompassing two distinct traces: one for chat (19,336 entries) and the other for code completion (8,199 entries), for LLMCO2. We sampled inference configurations by adjusting input prompt lengths and generated token numbers, ensuring batch sizes remained under two. We adopted random sampling to generate inference requests with different input prompt lengths and generated token numbers for our baseline schemes. LLM inferences were executed using the GPU configurations detailed in Table 2, with the number of GPUs per inference ranging from the minimum required to a maximum of four, regardless of each hardware configuration's total GPU capacity. For the training dataset, we considered L4, A100, and H100 GPUs, while all four GPU configurations were included in the test dataset.\nMeasurement and implementation. We used the Nvidia Management Library (NVML) (NVIDIA, 2024) to measure the energy consumption of LLM inferences on the target GPUs. Each inference was executed 5 times, and the average energy consumption was recorded as the ground truth. LLMCO2 consists of two graph convolution layers and two linear layers. LLMCO2 was developed using the PyG package (Team, 2024) and trained on a Tesla L4 GPU. The model training used the Adam optimizer, with a learning rate of 0.001 and a batch size of 512.\nEvaluation metrics. We evaluated prediction accuracy using the Mean Absolute Percentage Error (MAPE) and Error Bound Accuracy (EBA(\u03b4)). MAPE quantifies the average absolute percentage"}, {"title": "5.2 OPERATIONAL ENERGY RESULTS", "content": "MAPE. Table 3 presents the comparison of Mean Absolute Percentage Error (MAPE) between LLMCO2 and various baseline schemes. On average, LLMCO2 achieves the lowest MAPE values across different LLMs. LLMCarbon estimates operational energy consumption based solely on FLOP counts, neglecting memory accesses within transformer layers and critical hardware-specific features, such as peak GPU memory and network bandwidth, resulting in the highest MAPE values for all LLMs. The two ML-based predictors, DeepEn and NNLQP, exhibit comparable average MAPE values. Due to a smaller training energy dataset size for Mixtral and Llama3.1, DeepEn, leveraging its random forest model, performs slightly better than NNLQP on these LLMs. Overall, LLMCO2 outperforms DeepEn and NNLQP, reducing the average MAPE by 51.4% and 45.6%, respectively, by treating the prefill and decode phases separately, incorporating kernel-specific Roofline performance, and training with energy data derived from public Azure LLM serving traces."}, {"title": "5.3 ABLATION STUDIES", "content": "We conducted ablation studies on EBA(10%) to evaluate the contribution of each component of LLMCO2, as summarized in Table 5. By using distinct node features for the prefill and decode phases, LLMCO2 improves EBA(10%) by 67% compared to NNLQP. In real-world LLM serving clouds, most inference requests involve medium-length prompts and fewer generated tokens, leading to significant errors when combining the two phases for carbon overhead prediction. Incorporating Roofline performance as a node feature further boosts LLMCO2's EBA(10%) by 13.1%, as this feature facilitates knowledge transfer from L4 to T4 GPUs in the test dataset. Finally, the focused energy data sampling technique elevates LLMCO2's EBA(10%) improvement to 123% of NNLQP, since training with data distributions that mirror real-world LLM inference configurations enhances its performance on test datasets with similar prompt lengths and generated token distributions."}, {"title": "6 USER CASE STUDIES", "content": "Carbon comparison between training and inference. LLM training requires numerous GPUs operating at high throughput over extended periods. For example, Mixtral-8\u00d77b training uses 512 A100 GPUs at about 50% peak throughput for three months (Jiang et al., 2024). In contrast, INT8 Mixtral-8\u00d77b inference utilizes four A100 GPUs at 10%-40% peak throughput for around 4 seconds. Figure 10 shows the normalized carbon footprint per GPU per second for Mixtral-8\u00d77b training and inference, relative to the training baseline. The embodied carbon per GPU per second is consistent across all configurations since A100 GPUs are used. The prefill phase of an inference with a batch size of 4 and a 6K prompt length has a similar operational carbon footprint per GPU per second to training, while the decode phase with a batch size of 1 and a 1K prompt length has a much lower footprint. Unlike training, the embodied carbon overhead dominates the decode phase of Mixtral-8\u00d77b inferences.\nInference on multiple GPUs. Additional GPUs can accelerate LLM inference (Pope et al., 2023), but using more GPUs than necessary is often unsustainable, particularly for real-world cloud inference configurations with small batch sizes and short prompts. Figure 10 shows the operational carbon footprint of Bloom-7b1 inferences across different GPU counts with varying prompt lengths and batch sizes. For example, although an inference with a batch size of 4 and a 1K-token prompt benefits from 2 or 4 GPUs, lowering the per-GPU carbon footprint, an inference with a batch size of 1 and a 64-token prompt incurs increased latency and higher per-GPU operational carbon due to the communication overhead of all-reduce kernels when using more GPUs. As shown in Figure 11, the total carbon overhead rises considerably with more GPUs for inferences with smaller batch sizes and prompts."}, {"title": "7 CONCLUSION", "content": "LLM inference produces a larger carbon footprint than training, necessitating accurate estimation tools for both users and cloud providers. Existing models fall short due to their inability to capture"}, {"title": "4.1 GRAPH EMBEDDING", "content": "We introduce a graph embedding technique for LLMCO2 to represent a transformer layer as a directed acyclic graph (DAG) of kernels: $G = (V", "are": 1, "features": "one for the prefill phase and the other for the decode phase. Each feature set concatenates the following elements:\n* T: Defines the type of the kernel. For instance", "S": "Represents the dimensions of the kernel's input", "O": "Refers to the number of operations of the kernel. While tools like CALFLOPS (Ye, 2023) can compute operation counts for kernels based on LLM architecture, they do not differentiate between the prefill and decode phases, or support the all-reduce kernel. Therefore, we provide equations for calculating O for the kernels used (excluding the all-reduce kernel) in Appendix A. For the same kernel, the operation count in the decode phase $(O^{dec})$ is proportional to the number of generated tokens $(Ngt)$, while in the prefill phase $(O^{pre})$, it is proportional to the input sequence length $(Lseq)$. For the all-reduce kernel operating on an $n \u00d7 m$ matrix distributed across $l$ GPUs (each holding an $n/l \u00d7 m$ portion), the operation count during the decode phase is given by Equation 2, and during the prefill phase by Equation 3. These operations occur during the reduce-scatter step.\n$O_{allr}^{der} = n(\\frac{m}{l} - 1)(Ngt - 1)$ \n$O_{allr}^{pre"}]}