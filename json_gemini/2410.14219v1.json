{"title": "Formal Explanations for Neuro-Symbolic AI", "authors": ["Sushmita Paul", "Jinqiang Yu", "Jip J. Dekker", "Alexey Ignatiev", "Peter J. Stuckey"], "abstract": "Despite the practical success of Artificial Intelligence (AI), current neural AI algorithms face two significant issues. First, the decisions made by neural architectures are often prone to bias and brittleness. Second, when a chain of reasoning is required, neural systems often perform poorly. Neuro-symbolic artificial intelligence is a promising approach that tackles these (and other) weaknesses by combining the power of neural perception and symbolic reasoning. Meanwhile, the success of AI has made it critical to understand its behaviour, leading to the development of explainable artificial intelligence (XAI). While neuro-symbolic AI systems have important advantages over purely neural AI, we still need to explain their actions, which are obscured by the interactions of the neural and symbolic components. To address the issue, this paper proposes a formal approach to explaining the decisions of neuro-symbolic systems. The approach hinges on the use of formal abductive explanations and on solving the neuro-symbolic explainability problem hierarchically. Namely, it first computes a formal explanation for the symbolic component of the system, which serves to identify a subset of the individual parts of neural information that needs to be explained. This is followed by explaining only those individual neural inputs, independently of each other, which facilitates succinctness of hierarchical formal explanations and helps to increase the overall performance of the approach. Experimental results for a few complex reasoning tasks demonstrate practical efficiency of the proposed approach, in comparison to purely neural systems, from the perspective of explanation size, explanation time, training time, model sizes, and the quality of explanations reported.", "sections": [{"title": "1 Introduction", "content": "Neural Artificial Intelligence (AI) models are widely used by the decision-making procedures of many real-world applications. Their success guarantees AI will prevail as a generic computing paradigm for the foreseeable future, including in safety- and privacy-critical domains. Unfortunately, neural AI models may occasionally fail [14, 16, 60], their decisions may be prone to bias [52] or be confusing due to brittleness [19, 59], where very similar cases are treated completely differently. As a result, there is a need to understand the behaviour of AI models, analyse their potential failures, debug them, and possibly repair them. This has given rise to AI model verification [4, 33, 49, 57] and explainable \u0391\u0399 (\u03a7\u0391\u0399) [38, 44, 47].\nMany XAI approaches exist, including the computation of interpretable AI models [7, 34, 46] and post-hoc explanation extraction for black-box AI models [39, 53, 54]. An important approach to XAI-formal explainable AI (FXAI) \u2013 builds on the use of formal reasoning about the AI models of interest and on computing explanations that capture the semantics of the target models to answer \"why\" or \"why not\" they make certain decisions [27, 43, 58].\nNeuro-symbolic AI is a promising paradigm developed to add symbolic reasoning to neural AI systems [3, 6, 35, 40, 45] by combining neural components (e.g., for sensory understanding) with"}, {"title": "2 Preliminaries", "content": "Datalog, Satisfiability and Unsatisfiability. Datalog is a declarative language defined as a subset of the logic programming language Prolog, with a focus on data manipulation and querying [1]. In particular, its limited features enable the use of bottom-up evaluation, rather than the top-down evaluation common in Prolog. This makes it well-suited as a query language for deductive databases, where it can infer new facts based on existing data and rules. Answer Set Programming (ASP) languages [31] extend Datalog to include constraints and use propositional satisfiability (SAT) solving methods.\nWe assume standard definitions for SAT and maximum satisfiability (MaxSAT) solving [11]. A propositional formula is said to be in conjunctive normal form (CNF) if it is a conjunction of clauses, where a clause is a disjunction of literals, and a literal is either a Boolean variable or its negation. Whenever convenient, clauses are treated as sets of literals while CNF formulas are treated as sets of clauses. A truth assignment maps each variable of a formula to a value from {0, 1}. Given a truth assignment, a clause is said to be satisfied if at least one of its literals is assigned value 1; otherwise, it is falsified by the assignment. A formula $\\phi$ is satisfied if all its clauses are satisfied; otherwise, it is falsified. If there exists no assignment that satisfies $\\phi$, then it is unsatisfiable, written $\\phi \\models 1$.\nIn the context of unsatisfiable formulas, the problems of maximum satisfiability and minimum unsatisfiability are of particular interest. Hereinafter, we consider partial unsatisfiable CNF formulas $\\phi$ represented as a conjunction of hard clauses H, which must be satisfied, and soft clauses S, which are preferred to be satisfied, i.e., $\\phi = H \\wedge S$ (or $\\phi = H \\cup S$ in the set theory notation). In the analysis of unsatisfiability of formula $\\phi$, one is interested in identifying minimal unsatisfiable subsets (MUSes) of $\\phi$, which can be defined as follows. Let $\\phi = H \\cup S$ be unsatisfiable, i.e., $\\phi \\models 1$. A subset of clauses $\\mu \\subseteq S$ is a Minimal Unsatisfiable Subset (MUS) iff $H \\cup \\mu \\models 1$ and $\\forall \\mu' \\subset \\mu$ it holds that $H \\cup \\mu' \\not \\models 1$. Informally, an MUS can be seen as a minimal explanation of unsatisfiability for an unsatisfiable formula $\\phi$. It provides the minimal information that needs to be added to the hard clauses H to obtain unsatisfiability. MUS extraction is applied in our work as the underlying technology for computing formal explanations for symbolic and neural AI systems.\nClassification Problems classify data instances into classes K where |K| = k > 2. Given a set of m features F, where the value of feature i \u2208 F comes from a domain Di which may be Boolean, (bounded) integer or (bounded) real, the complete feature space is defined by $F = \\prod_{i=1}^{m} D_{i}$. A data point in feature space is denoted $v = (v_{1}, ..., v_{m})$ where $v_{i} \\in D_{i}, 1 \\leq i \\leq m$. An instance"}, {"title": "Formal Explainability and Adversarial Robustness.", "content": "Given data point v and classifier K, which classifies it as class \u03ba(v), a post-hoc explanation tries to explain the behaviour of kon v. An abductive explanation (AXp) is a minimal set of features X such that any data point sharing the same feature values with v on these features is guaranteed to be assigned the same class c = \u043a(v) [27, 58]. Formally, X is a subset-minimal set of features such that:\n$\\forall (x \\in F) .[\\bigwedge_{i \\in X}(x_{i}=v_{i})] \\rightarrow (\\kappa(x) = c)$\n(1)\nIt is known [30, 43] that formal AXps for ML predictions are related with the concept of MUSes (defined earlier) of an unsatisfiable formula encoding the ML classification process \u03ba(v) = c, namely if one represents [K(x) \u2260 c] as hard clauses and [$\\kappa_{i}(x_{i} = v_{i})$] as soft clauses.\nBy examining (1), one can observe that AXps are designed to hold globally, i.e., in the entire feature space F. Recent work proposed to use the tools developed for checking adversarial robustness of neural networks in the context of computing distance-restricted abductive explanations [23, 63]."}, {"title": "3 Hierarchical Explanations", "content": "Unlike purely neural AI systems, neuro-symbolic systems are meant to delegate some responsibilities to their symbolic components. This can negatively affect the model's interpretability. First, while purely neural models are deemed an opaque black-box whose decisions are hard for a human-decision maker to comprehend, the problem is aggravated by the (complex) interactions between the neural and symbolic components of a neuro-symbolic system. Second, such interactions arguably represent a challenge for post-hoc explanation approaches. This applies to heuristic explainers, as they are known to be susceptible to unsoundness issues [22, 42], and to formal explanation approaches, as their need to deal with many (typically NP-)hard computational problems often makes them become exponentially harder with the increase of problem complexity. The general setup for explaining a neuro-symbolic system can be seen in Figure 2.\nInterestingly, the formal explainability of such hybrid neuro-symbolic systems should in fact significantly benefit from this separation of responsibilities. First, explainability of the symbolic components is naturally achievable by means of the advanced apparatus used by automated reasoning and discrete optimization for dealing with over-constrained systems [10, 29, 36, 37, 51]. Namely, the symbolic rules can be seen as a set of constraints and, given the inputs to the constraints passed by the neural component, one can use the unsatisfiability of the constraints to identify an irreducible subset of the inputs responsible for the symbolic decision. As a result, one can solve this problem by applying mechanisms similar to computing a (smallest) MUS of the set of rules, which are well understood in discrete optimization [11, Ch. 21].\nSecond, the functionality separation in neuro-symbolic AI leads to the simplification of the neural component as it has to deal with smaller or more focused sub-tasks, without a loss of performance of the entire neuro-symbolic system. For instance, in the Pacman setting of Figure 1, the neural component serves to provide sensor information classifying all the grid cells individually. This in turn may lead to structurally simpler neural models and positively impacts the performance of formal reasoning engines on such models. (Recall that scalability of formal explainability methods is often seen as one of their limitations [43].)"}, {"title": "Definition 1 (Hierarchical Abductive Explanation)", "content": "Let $c = \\kappa_{\\sigma} (\\kappa_{\\eta}(x_{1}),..., \\kappa_{\\eta}(x_{n})), x_{j} \\in F, j \\in [n]$, be a decision made by a neuro-symbolic system involving a neural component $\\kappa_{\\eta}: F \\rightarrow Y$ and a symbolic component $\\kappa_{\\sigma}: \\prod_{j\\in[n]} Y \\rightarrow K$. Then a hierarchical abductive explanation for decision c is a set $X = \\cup_{j\\in y} X_{j}$ such that $y \\subseteq {1,..., n}$ is an abductive explanation for the symbolic decision c = $\\kappa_{\\sigma}(y_{1},..., y_{n})$ and each $X_{j}, j \\in V$, is an abductive explanation for the neural decision $y_{j} = \\kappa_{\\eta}(x_{j})$."}, {"title": "4 Experimental Results", "content": "We use three neuro-symbolic benchmarks to evaluate the scalability and quality of the explanations produced by our hierarchical explanation framework in comparison to those of pure neural approaches with similar quality prediction results. All experiments were conducted on a MacBook Pro with an Apple M3 Pro processor, with 12 CPU cores, 14 GPU cores, and 18 GB RAM. (Source code and experimental data will be made publicly available with the final version of the paper.)"}, {"title": "4.1 Evaluated Explanation Methods", "content": "As our hierarchical explanation framework can flexibly combine different explanation methods for neural components, we evaluate four configurations. First, we consider the use of formal explanations for the neural components using Marabou [33] (dev. version 2024-03-11, BSD-3-Clause), a framework for verifying neural networks using SMT, and PySAT [26] (v1.8.12, MIT). Scallop is the language used to design the neuro-symbolic models (dev. version 2024-02-24, MIT). Using this method, different values of & can be used requiring the explanation to hold for all compatible points in the \u025b-vicinity of the instance. We configure Marabou with \u025b = 1, requiring the explanation to hold for all points, in configuration HX-Marabou-e=1. The configuration HX-Marabou-e<1 uses Marabou with \u025b = 0.3 (except for Pacman-SP where \u025b = 0.2). For the third configuration HX-SHAP, we use our hierarchical framework in combination with Shapley Additive Explanations (SHAP) [39] (v0.44.1, MIT) applied for explaining the neural component. SHAP calculates the contribution of each feature to the model's prediction for a given input, identifying the most important features. The next configuration Scallop-SHAP, directly builds (kernel) SHAP explanations for the entire neuro-symbolic model treated as a black box. The last configuration NN-SHAP uses baseline SHAP to obtain explanations for purely neural solutions for each of the given benchmarks. To achieve a similar accuracy for the purely neural applications, the size of the networks is too large for Marabou to create formal explanations within a reasonable timeframe."}, {"title": "4.2 Benchmarks and Neural Architectures", "content": "All the benchmarks are exemplified in Figures 4 to 6. In Figures 4 to 5, (i) represents the input data, (ii) the symbolic reduction, (iii) the HX-Marabou-e=1 explanation, (iv) the HX-Marabou-e<1 explanation, (v) the HX-SHAP explanation, (vi) the Scallop-SHAP explanation, and (vii) the NN-SHAP explanation. Figure 6 shows the explanations (iv)\u2013(vii) labelled. (See Figure 1 for the other parts.)\nLex1-n. Here we aim to learn a model that can predict the lexicographic order of two handwritten binary numbers, as discussed in Example 4 and shown in Figure 4. All tasks take as input n \u2208 {6,8} images, randomly selected from the MNIST dataset [15] filtered to only contain the digits zero and one. The combination of n images in each task is unique. The model is trained to predict whether the first number, represented by the first images, is lexicographically greater than the second number, represented by the remaining images. In the neuro-symbolic system, a neural network predicts whether an image is zero or one, and a Scallop program then determines the lexicographic order based on the predicted values. The neural model consists of 2 fully connected hidden layers of 10 nodes. Note that the neural component is only trained indirectly through Scallop. For our pure neural application at n = 6, we employ a network that uses 2 convolutional layers, followed by 4 fully connected hidden layers with 7500, 5000, 2500, and 64 nodes sequentially. For n = 8, we use 3 convolutional layers, but use only 2 fully connected hidden layers with 1024 and 256 nodes.\nRegExp-a-n. Our regular expression benchmark, an example of which is shown in Figure 5, aims to predict whether a string of 0-1 digits is in the language of regular expression Ra. Each task takes an input of n \u2208 {6, 8, 10} images, randomly selected zero-one MNIST dataset similar to the Lex1-n benchmark. The model is then trained to recognize whether the string represented by the images is in the language of the regular expression $R_{1} = /1.*11.*0/$, i.e., a string starting with a one, containing two consecutive ones, and ending with a zero, or $R_{2} = /(0.*11.*)|(.*00.*1)/$, i.e., a string that either starts with a zero and contains two consecutive ones or ends with a one and contains two consecutive zeroes. We use R\u2081 together with the shorter string lengths n \u2208 {6,8} and R2 with n = 10. Similar to Lex1-n above, the neural component predicts the digit in the images, and a Scallop program then determines whether the string is in the language of the regular expression based on the predicted values. The pure neural application for RegExp-1-6 and RegExp-2-10 have layers identical to that of the Lex1-6 benchmark, with 7500, 5000, 1024, 10 nodes in the fully connected hidden layers sequentially for both, and that of RegExp-1-8 is the same as for Lex1-8, with 1024 and 10 nodes.\nPacman-SP. As discussed in Examples 2 and 3 and shown in Figure 6, the aim of this benchmark is to find the length of a shortest grid path from the actor's start position to a target flag position without stepping onto squares containing ghosts. Using the same images and obfuscation technique"}, {"title": "4.3 Evaluation", "content": "achieve high accuracy. This is also evident as size of the architectures of the CNN models (detailed in the previous subsections) are much larger to perform the same tasks, as established in [35].\nOn Explanation Sizes. The sizes of the explanations and the average time to compute them is detailed in Table 2. The data demonstrates that the explanation size from HX-Marabou-e=1 is larger than that from HX-Marabou-e<1 (which is to be expected) and comparable to that from HX-SHAP. However, HX-Marabou-e=1 explanations are global abductive explanations and guaranteed to be correct in the entire feature space, i.e., if these input pixels are left unchanged the result will always be the same regardless of other inputs.\nThe average explanation sizes of HX-Marabou-e=1, HX-Marabou-e<1, HX-SHAP, Scallop-SHAP and NN-SHAP are around 16%, 9%, 18%, 7% and 60%, respectively, of the total number of features in the instance. This not only indicates that it is challenging to explain (convolutional) neural networks as large as ours but also proves the efficacy of neuro-symbolic models. Consider Figure 5a, the NN-SHAP explanations focus more on the first and last digits for the prediction, whereas common sense dictates the first digit to be irrelevant to the prediction (given the regular expression). The Scallop-SHAP explanations are also scattered around all the digits in the image, not producing any effective explanation for the prediction. Similar observations can be made w.r.t. the other examples shown, demonstrating that Scallop-SHAP and NN-SHAP essentially get lost while trying to explain the decision. Also, one can rely on explanations generated by HX-SHAP if time is a constraint, but at the cost of formal correctness.\nOn Explanation Quality. Table 3 gives insights on the percentage of features contributing to explanations. For each benchmark, the first row presents the percentage of the minimum, average and maximum number of neural inputs considered in the explanations generated through the configurations HX-Marabou-e=1, HX-Marabou-e<1, HX-SHAP, Scallop-SHAP, and NN-SHAP. The second row of"}, {"title": "Sorting Heuristics for HX-Marabou.", "content": "The quality of abductive explanations generated by HX-Marabou depends on the feature traversal procedure undertaken. Consider an image of size 28 \u00d7 28 as an input, which is to be explained. If the traversal begins from the top left of the image towards the bottom right, the features on the top of the image will be eliminated first and thus, a majority of the features in an explanation will lie towards the bottom of the image. Similarly, if the traversal is initiated from the top right corner towards the bottom left, majority of the features in an explanation will lie towards the left of the image. This creates a need to apply heuristics such that our model eliminates the features that surely do not contribute to the prediction and then focusses on the important features.\nIn our setup, we iterate over the pixels/features based on distance from the centre of the image (of the individual neural inputs), and saturation and lightness of the pixels. The distance measure ensures that we first eliminate the features that reside close to the boundaries and the corners of the image. The saturation and lightness of pixels are considered to eliminate the darker ones first. For grayscale images of benchmarks Lex1-n and RegExp-a-n, the saturation and lightness measure is equivalent to the brightness of the pixel (measured by the value of the pixel: the higher, the brighter). The saturation and lightness is a reasonable measure since our benchmarks of digits and pacman grid cells have bright and/or coloured pixels that are responsible for the prediction. On applying these heuristics, the abductive explanations generated are of improved quality and give better visual representation."}, {"title": "5 Related Work", "content": "There are a large variety of approaches of XAI [38, 44, 47], including interpretable model synthesis [7, 34, 46] and post-hoc explainability of black-box ML models [39, 44, 53, 54]. The approach to explainability closest to ours is based on formal reasoning about the model of interest. It seeks to compute so-called formal abductive and contrastive explanations using a series of oracle calls to a reasoning engine and proving certain properties of the model of interest [27, 43, 58]. The area of FXAI is tightly related [28] to adversarial robustness checking and neural model verification [4, 33, 57]; hence, the reasoning engines developed in the latter community can be applied for computing formal explanations [23, 63].\nNeuro-symbolic AI is often seen as a response to the weaknesses of purely neural ML architectures and often deemed crucial for constructing rich computational cognitive models where reasoning is required [6, 18, 45, 61]. A modern generation of neuro-symbolic systems implements ways to integrate conventional neural model training with the power of existing symbolic reasoning methods to solve complex problems. These systems include DeepProbLog [40], Pylon [3], and Scallop [35], which utilize Prolog, constraint modelling, and Datalog engines respectively. Due to the use of symbolic reasoning, neuro-symbolic AI is often deemed more trustable, interpretable than neural AI and also self-explanatory [2, 8, 32, 50]. We are not aware of other approaches to (formal) post-hoc explainability of neuro-symbolic AI."}, {"title": "6 Conclusions", "content": "This paper proposes a formal approach to computing abductive explanations for neuro-symbolic AI systems. The approach applies in the case of neuro-symbolic systems whose inputs are independent of each other and, as a result, implements the extraction of abductive explanations hierarchically. Importantly, given subset-minimal abductive explanations for the individual decisions made by the neural and symbolic components of the system, the approach is guaranteed to report a subset-minimal abductive explanation for the entire system. Experimental results obtained on a few families of benchmarks demonstrate the high quality of hierarchical explanations. A few lines of future work can be envisioned. First, it is interesting to investigate how the relaxation of the input independence requirement impacts the quality of neuro-symbolic explanations and the difficulty of their extraction. Second, while input independence facilitates our approach to the computation of subset-minimal AXps, it does not directly simplify the process of computing (i) contrastive explanations (CXps) and (ii) cardinality-minimal AXps, which we are planning to address next."}]}