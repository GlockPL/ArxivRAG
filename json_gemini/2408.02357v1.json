{"title": "On the consistent reasoning paradox of intelligence and optimal trust in AI: The power of 'I don't know'", "authors": ["A. Bastounis", "P. Campodonico", "M. van der Schaar", "B. Adcock", "A. C. Hansen"], "abstract": "We introduce the Consistent Reasoning Paradox (CRP). Consistent reasoning, which lies at the core of human intelligence, is the ability to handle tasks that are equivalent, yet described by different sentences ('Tell me the time!' and 'What is the time?'). The CRP asserts that consistent reasoning implies fallibility \u2013 in particular, human-like intelligence in Al necessarily comes with human-like fallibility. Specifically, it states that there are problems, e.g. in basic arithmetic, where any AI that always answers and strives to mimic human intelligence by reasoning consistently will hallucinate (produce wrong, yet plausible answers) infinitely often. The paradox is that there exists a non-consistently reasoning AI (which therefore cannot be on the level of human intelligence) that will be correct on the same set of problems. The CRP also shows that detecting these hallucinations, even in a probabilistic sense, is strictly harder than solving the original problems, and that there are problems that an AI may answer correctly, but it cannot provide a correct logical explanation for how it arrived at the answer. Therefore, the CRP implies that any trustworthy AI (i.e., an AI that never answers incorrectly) that also reasons consistently must be able to say 'I don't know'. Moreover, this can only be done by implicitly computing a new concept that we introduce, termed the 'I don't know' function \u2013 something currently lacking in modern AI. In view of these insights, the CRP also provides a glimpse into the behaviour of Artificial General Intelligence (AGI). An AGI cannot be 'almost sure', nor can it always explain itself, and therefore to be trustworthy it must be able to say 'I don't know'.", "sections": [{"title": "Introduction", "content": "The ultimate question in AI research is whether it is possible to design an AI that supersedes, or is on par with, human intelligence. Such an Al is often referred to as Artificial General Intelligence (AGI) [35, 36, 38]. Modern chatbots have led to impressive breakthroughs towards the development of AGI [8,18,23,26]. However, it is well-known that they suffer from a propensity for hallucinations [3, 19, 28, 41, 49]. Chatbots not only generate false yet plausible statements and incorrectly answer questions - often ones that are easily answered by humans (see Figure 4) - but they may also provide no reasoning or flawed explanations [2, 14, 15, 17, 24, 29]. This raises the following questions:\nIs it possible to design an AGI that truly emulates human intelligence, and if so, how would it behave?\nCould an AGI detect its own hallucinations and admit that it is wrong, potentially through randomisation, and thus be 'almost sure' of its correctness?\nTo what extent could we trust an AGI, how would it determine when it is correct, and will it always be able to logically explain itself?\nThis paper provides a foundation for answering these questions through the Consistent Reasoning Paradox (CRP). The CRP describes the behaviour of any AI that seeks to emulate human intelligence by attempting to reason consistently, i.e., by answering problems that are stated by equivalent, yet distinct sentences. As we describe, it provides key insight into how an actual AGI \u2013 which must be a consistent reasoner \u2013 would behave."}, {"title": "The Turing Test and consistent reasoning", "content": "Before describing the CRP, we first need to formalize several key concepts, such as what an AI is and what constitutes an AGI. For this, we rely on the foundational concepts of Turing.\nTuring's seminal 1950 paper [47] is often viewed as the first theoretical work on AI. Here he asks \"can machines think?\", and subsequently defines the Imitation Game (now called the Turing Test). The Turing Test has become the standard test for true artificial intelligence or AGI [7,34]. In this test, a computer A and a human B are subject to a human interrogator C, that does not see A and B, but can ask questions to both of them in written form. The task for C is to determine that A is the computer. B, a human, is supposed to help C determine that A is the computer, and A wins the game if C is incapable of determining whether A or B is the computer. Turing discusses strategies for the computer\nA to win the game and concludes the following:\n\"... it will be assumed that the best strategy [for A] is to try to provide answers that would naturally be given by a [hu]man.\u201d \u2013 A. Turing (1950) [47]."}, {"title": "Mathematics as a test for AGI", "content": "In order to formulate the CRP, we first need to determine the types of problems considered. One of the current focal areas in the quest for AGI is designing Als capable of solving advanced mathematical problems [39, 45]. For example, the recent pioneering program [1] looks to test AI against human intelligence by initiating a competition for Als to solve International Mathematics Olympic (IMO) problems. Humans with a reasonable mathematical background are certainly expected to reason consistently on problems in arithmetic. Hence, if the Turing Test were performed with an AI against a pair of mathematicians (B and C above), and, as Turing concludes, the best strategy for the AI to pass the Turing Test is to stay as close as possible to human behaviour, then the AI should also reason consistently on basic arithmetic problems. In other words:\n$AGI \\rightarrow Passing \\text{ the Turing Test }\\rightarrow \\text{ Consistent reasoning}$. (1)\nIn fact, to emphasise (1), the interrogator C can instruct A and B to 'always reason consistently'. Since the human B is supposed to help C, B will follow the instruction. Thus, A, the AGI that imitates B, must also attempt to reason consistently. Figure 3 shows ChatGPT's successful consistent reasoning on certain problems in arithmetic. We note in passing that arithmetic and logical reasoning has been a substantial focal point of AI research in recent years [10,11,32,43,48,50]."}, {"title": "What is a 'machine'/AI and what is a problem?", "content": "Having focused our attention on arithmetical problems, in order to formulate the CRP we now also need to introduce a number of key concepts. We commence with the definition of a 'machine'. This term was used by Turing, however, 'AI' is arguably now much more common.\n\"The question [\u2018can machines think'?] which we put in \u00a71 will not be quite definite until we have specified what we mean by the word 'machine'. \" \u2013 A. Turing (1950) [47].\nTuring concludes that a machine/AI is a computer program, more precisely a Turing machine.\nIn the CRP we will also use the term 'problem'. By a problem we mean a basic arithmetical problem stated by a logical sentence (paragraph) in the English language. However, as discussed above, a problem can be stated by many different sentences. This motivates a series of further concepts.\nA collection of problems. A problem, for example from medicine, could be described by the following sentence:\n(*) Jen undergoes two chemotherapy treatments with dosage rates $a_1$ and $a_2$ per second, respectively. To minimize the total treatment time $(x_1 + x_2)$ while ensuring that she receives a total dosage of 1, how should one choose the durations $x_1$ and $x_2$ if $a_1 = 1/10$ and $a_2 = 1/2$?"}, {"title": "The Consistent Reasoning Paradox (CRP)", "content": "We now summarise the CRP in five distinct, yet connected components. The first two components, CRP I-II, are also illustrated in Figure 2, with the whole CRP being illustrated in Figure 1.\nCRP I \u2013 The non-hallucinating AI exists.\nThere is a collection of problems (e.g. those generated by (*)), where each problem is described by more than one equivalent sentences, with the following property. Consider any family of these sentences, such that each problem is described by exactly one sentence in this family. Then there is an Al that does not hallucinate: when given any sentence in this family as input it will always give a correct answer.\nThere are, in fact, infinitely many different collections of problems for which CRP I (and, therefore, CRP II-V as well) holds: the collection (*) is just a special case.\nNow consider the family of sentences and the AI asserted by CRP I. If the AI is given a sentence outside of this family, it could potentially not produce any output. However, it will never produce an incorrect output. Therefore, CRP I asserts that there is an AI, let us call it SpecialBot, that is correct on all the problems generated by (*), given that the input is one sentence per problem. In particular, SpecialBot will never hallucinate. However, SpecialBot does not reason consistently. If presented with sentences outside of the relevant family, it could simply not produce any response. This brings us to CRP II.\nCRP II \u2013 Attempting consistent reasoning yields hallucinations\nIf the AI from CRP I always answers, and were to emulate human intelligence \u2013 that is, it would attempt to reason consistently by accepting any family of sentences describing the collection of problems in CRP I \u2013 then it will hallucinate infinitely often. The hallucinations would occur even if the AI was implemented on a computer allowing arbitrary storage and arbitrarily long computational time.\nCRP II implies that if SpecialBot attempts to emulate human intelligence by producing an answer to any sentence describing the problem, then it will hallucinate infinitely often. This occurs despite SpecialBot being able to provide correct answers to every problem in the collection when presented with a specific sentence describing that problem.\nA stronger CRP II: Failure sentences and equivalence. CRP II can be strengthened in several ways. First, these failure sentences for an AI (that always answers and accepts basic questions in arithmetic) can be written down explicitly, provided one has access to the computer program of the AI. In particular, for any integer N, we can write down N sentences (describing questions in basic arithmetic) such that the AI hallucinates on these sentences. The length any of such sentence $Fail$ is bounded by\n$\\text{length}(Fail) < \\text{length}(AI) + \\epsilon + \\log(N),$\nwhere $\\text{length}(AI)$ is the length of the computer program of the AI and $\\epsilon < 3300$ if the programming language is MATLAB (see Figure 6). For any other standard language, $\\epsilon$ will have a similar bound. Thus, for any such AI, one can write down, say, a trillion hallucination sentences describing problems in basic arithmetic of length bounded by $\\text{length}(AI) + 3312$. The AI will also fail on shorter sentences than (2): see the Methods section and CRP IV for details. Second, determining the correct answer to a problem in CRP II is strictly easier than determining the equivalence class to which the given sentence belongs. Hence, as claimed earlier, CRP I-II also demonstrate how consistent reasoning is different to determining equivalence classes of sentences (see the Methods section for details).\nCRP II immediately raises the question whether the hallucinations it describes can be detected. This is the topic of CRP III, which has a deterministic part and a randomised part.\nCRP III(a) \u2013 Detecting hallucinations is hard\nConsider the AI from CRP I-II. It is strictly harder to determine if it has hallucinated than it is to solve the original problem. That is, it is impossible to detect whether the AI was correct or wrong even with access to true solutions of the collection of problems from CRP I.\nCRP I-II imply that an AI that reasons consistently must hallucinate. However, it could have been the case that these hallucinations could be detected by a separate algorithm, thus leading, in effect, to a hallucination-free AI. CRP III(a) demonstrates that this is impossible.\nThe reader, however, may find CRP III(a) puzzling, since access to a true solution should surely guarantee the detection of a hallucination. The key ingredient is that there may be problems with multi-valued solutions. For example, the problem 'name a prime number' has infinitely-many correct solutions, and therefore, access to a solution does not mean access to all solutions.\nNow, given that there is no deterministic algorithm to check for hallucinations, it is natural to consider whether randomisation may help. This is highly relevant to current AIs, as chatbots such as ChatGPT rely on randomness. One may ask: could a randomised algorithm result in an AI that was 'almost sure' of its correctness? For example, could it be 95% sure, meaning that it can guarantee with probability 0.95 that the answer it produces is correct? This brings us to CRP III(b).\nCRP III(b) \u2013 Detecting hallucinations and randomness\nOne cannot detect hallucinations of the AI from CRP I-II with a randomised algorithm with probability $p > 1/2$ on all the inputs (one cannot be 'almost sure').\nCRP III(b) implies that 'almost sure' certainty of an AI \u2013 in fact, anything more than pure guessing, i.e., 50% certainty \u2013 is impossible. Moreover, CRP III(b) is actually slightly stronger than stated above. Namely, if one can design a 'checker-AI' that would be certain about the correctness of the AI from CRP I-II, with a probability greater than 1/2 on a collection of problems, one can also design a deterministic checker-AI that is 100% certain on that collection. Thus, the checker-AI either knows with 100% certainty, or has no idea and the certainty is 50/50.\nA important strand of AI research attempts to create AIs that can explain how they reached a solution to a given problem. This turns out to be a highly delicate problem, and few, if any, AIs are able to provide reliable explanations. CRP IV explains why this is so delicate.\nCRP IV - Explaining a correct answer is not always possible\nConsider the same collection of problems as in CRP I. There is a family of sentences, with each problem described by at most one sentence, and an AI that does not hallucinate on this family of sentences. However, there is one sentence in this family for which this AI (nor any other AI) cannot provide a logically-correct explanation of the solution.\nWhat CRP IV says is that the AI may provide a correct answer to the problem, but it is impossible for the AI to explain in a logically-correct way why this is the correct solution. Note that we have not defined what constitutes a 'logically-correct explanation'. This can and will be made precise later, but it essentially means a logical mathematical argument (i.e., a proof).\nTogether, CRP I-IV demonstrate how any AI that attempts to reason consistently, even on problems it can solve, will be fallible in several ways. In particular, any AI that reasons consistently and always provides an answer must necessarily hallucinate. Therefore, the only way one can make a consistent reasoning AI that is trustworthy is to allow it to say 'I don't know'. But how can we do this in a meaningful way? An AI that says 'I don't know' all the time is entirely reliable, but not particularly useful. This is the topic of the final part of the CRP.\nCRP V \u2013 The fallible yet trustworthy explainable AI saying 'I don't know' exists\nGiven the collection of problems in CRP I, there exists a trustworthy, consistently reasoning and explainable AI with the following properties. The AI takes as input a prescribed number of minutes M and any sentence describing the problem. It will 'think' for no more than the prescribed number of minutes before answering either 'I know' accompanied by a correct answer and a correct logical explana- tion, or it gives up and says 'I don't know'.\nIf the sentence describes a multi-valued problem (i.e., a problem with more than one correct solution), the AI will always say 'I don't know'. However, there is only one such problem in the collection (but many different sentences de- scribing it). For any single-valued problem, by choosing the number of minutes to be large enough, the AI will al- ways say 'I know'.\nThe power of \u2018I don't know' and the strongest form of trust\nSaying 'I don't know' is exactly how human intelligence deals with the consistent reasoning paradox. Indeed, human fallibil- ity in the form of not always being able to answer correctly does not contradict consistent reasoning, as long as one can say 'I don't know'. A human's ability to say 'I don't know' is also the key to trustworthiness. A human that will always enthusiastically present an answer to any question will inevitably be wrong and therefore cannot be trusted. Thus, in order to provide answers that others can trust are correct, a human must separate between the questions one can answer correctly and those for which one may provide an incorrect answer. This is done using the verification statement 'I know' and its complement 'I don't know'."}, {"title": "Trustworthy AI and 'I don't know' \u2013 the \u2211\u2081 class", "content": "Having now described the CRP, we are left with the following fundamental question:\nQ: How can one create consistently reasoning, trust- worthy and explainable AI that says 'I don't know'?\nHere, by 'trustworthy', we mean that the AI will never be wrong, but that it can say 'I don't know' (specified below). CRP V demonstrates that it is possible, in certain cases, to produce trustworthy AI. But how can this be done in general?\nA: The key is the so-called \u2211\u2081 class (from the SCI hierarchy discussed below), and the 'I don't know'- function. It is impossible to make trustworthy and explainable AI outside of this class.\nThe 'I don't know' function \u2013 Why AIs must learn to give up\nThe main challenge in addressing the above question is the fol- lowing:\n(\u2020) How can the AI identify that it cannot solve a given problem in order for it to say 'I don't know'? Conversely, how can it identify when it is correct and is able to explain the solution?\nA crucial part of human intelligence is that one does not necessarily know which problems one cannot solve. One first tries, then simply gives up and says \u2018I don't know' after a while. The problems one cannot solve are typically determined by first try- ing and then giving up. As we explain, any trustworthy AI must follow the same philosophy, which is the essence of the \u2211\u2081 class.\nWhy 'giving up' is necessary. CRP V answers question (\u2020) for certain problems in arithmetic. However, a new question im- mediately arises.\nThe AI = AI(1, M) described in CRP V has to \u2018give up', just like a human. In particular, it 'gives up' if M is too small. Is this necessary, or could one avoid the 'giving up' parameter M?"}, {"title": "Universality of the CRP \u2013 From society to sciences", "content": "We conclude this section with two important remarks.\nThe CRP applies to any AGI. The CRP will apply to any AGI for two reasons: (i) any AGI must be able to solve basic problems in arithmetic such as (+), and thus specific failure sentences as in (2) can be written down. (ii) any AGI will be a Turing machine with no restriction on the length of the input (see the Methods section for details).\nThe CRP applies to any consistently reasoning AI. Collections of problem for which the CRP applies are everywhere in the sciences and broader society. In (*) we considered a basic collection of optimisation problems arising in healthcare. However, it is clear that similar problems could be phrased in many other domains. Moreover, the full CRP pertains not just to this specific problem, but many basic problems arising in optimisation, including linear programming, semidefinite programming, basis pursuit, LASSO, etc. These problems occur in countless sectors, including, healthcare, economics, finance, social sci- ences, engineering (mechanical, civil, electrical etc), aviation, public sector management, mathematics, computer science, stat- istics, biology and so forth."}, {"title": "The power of \u2018I don't know' and the strongest form of trust", "content": "This is also the strongest form of trust possible for an AI. CRP III implies that there is not a better form of trust than the ability for an AI to say 'I don't know'. Indeed, had it been possible to have a 'checker-AI' that would determine if the AI was correct or not, it would be possible - using the AI and the checker-AI - to design a new AI that would hallucinate, yet we would always know when it was right or wrong. However, CRP III shows that this is impossible, and thus any checker-AI of an AI can, at best, say 'I don't know whether the answer is correct'. Moreover, as CRP III shows, the checker-AI cannot be 'almost sure' if it was randomised. The checker-AI will either be 100% sure and say 'I know', otherwise it has to say 'I don't know' (in this case the randomised checker-AI would have a 50/50 chance of predicting the correctness of the AI)."}, {"title": "Trustworthy AI and 'I don't know' \u2013 the \u2211\u2081 class", "content": "Having now described the CRP, we are left with the following fundamental question:\nQ: How can one create consistently reasoning, trust- worthy and explainable AI that says 'I don't know'?\nHere, by 'trustworthy', we mean that the AI will never be wrong, but that it can say 'I don't know' (specified below). CRP V demonstrates that it is possible, in certain cases, to produce trust- worthy AI. But how can this be done in general?\nA: The key is the so-called \u2211\u2081 class (from the SCI hierarchy discussed below), and the 'I don't know'- function. It is impossible to make trustworthy and explainable AI outside of this class.\nThe 'I don't know' function \u2013 Why AIs must learn to give up\nThe main challenge in addressing the above question is the fol- lowing:\n(\u2020) How can the AI identify that it cannot solve a given prob- lem in order for it to say 'I don't know'? Conversely, how can it identify when it is correct and is able to explain the solution?\nA crucial part of human intelligence is that one does not necessarily know which problems one cannot solve. One first tries, then simply gives up and says \u2018I don't know' after a while. The problems one cannot solve are typically determined by first try- ing and then giving up. As we explain, any trustworthy AI must follow the same philosophy, which is the essence of the \u2211\u2081 class.\nWhy 'giving up' is necessary. CRP V answers question (\u2020) for certain problems in arithmetic. However, a new question im- mediately arises.\nThe AI = AI(1, M) described in CRP V has to \u2018give up', just like a human. In particular, it 'gives up' if M is too small. Is this necessary, or could one avoid the 'giving up' parameter M?"}, {"title": "Universality of the CRP \u2013 From society to sciences", "content": "We conclude this section with two important remarks.\nThe CRP applies to any AGI. The CRP will apply to any AGI for two reasons: (i) any AGI must be able to solve basic problems in arithmetic such as (+), and thus specific failure sentences as in (2) can be written down. (ii) any AGI will be a Turing machine with no restriction on the length of the input (see the Methods section for details).\nThe CRP applies to any consistently reasoning AI. Collections of problem for which the CRP applies are everywhere in the sciences and broader society. In (*) we considered a basic collection of optimisation problems arising in healthcare. However, it is clear that similar problems could be phrased in many other domains. Moreover, the full CRP pertains not just to this specific problem, but many basic problems arising in optimisation, including linear programming, semidefinite programming, basis pursuit, LASSO, etc. These problems occur in countless sectors, including, healthcare, economics, finance, social sci- ences, engineering (mechanical, civil, electrical etc), aviation, public sector management, mathematics, computer science, stat- istics, biology and so forth."}, {"title": "Conclusion: The CRP and the future of AI", "content": "A glimpse of AGI\nThe CRP provides a glimpse of how an AGI would behave. One could have imagined the possibility of having an AGI that would know how to answer correctly, but, in order to pass the Turing Test, would say 'I don't know', just to imitate the human (which will naturally say 'I don't know' to certain questions). The CRP shows that this is impossible, even in specialist areas where there is an AI that can solve the corresponding problems. Human-like fallibility is a necessary consequence of consistent reasoning. Indeed, our framework shows how a plethora of failure sentences for a given AGI can be specifically written down as in (2) us- ing the AGI's computer code. These failure sentences will differ from AGI to AGI, just like how humans have different problems they cannot solve.\nFuture of AI: The 'I don't know' functions and prompting\nGiven a collection \u03a9 of sentences describing various problems, the key question is how to build a trustworthy AI for \u03a9. This can only be done by implicitly computing an 'I don't know' function that splits \u03a9 into two parts \u03a9 = \u03a9know \u222a \u03a9don't know. How to do this in the case of modern chatbots is a serious challenge. However, the CRP establishes that 'I don't know' functions are necessary, and thus there is no way around them.\nA possible first step is to use prompting. In particular, by dividing \u03a9 into m subdomains using prompts\n$\\Omega = \\Omega_{\\text{Prompt}_1} \\cup ... \\cup \\Omega_{\\text{Prompt}_m},$\nthen one can build 'I don't know' functions specifically for each set $\\Omega_{\\text{prompt}_i}$. For example, consider a chatbot, such as those con- sidered in Figure 4, that is known not to be trustworthy on the basic problem (*). Now add the AI created in CRP V with its \u2018I don't know' function in the following way. When prompted, the new enhanced chatbot simply calls the AI from CRP V, which implicitly computes the 'I don't know' function to 'give up' on problems it cannot solve. The new enhanced chatbot is of course not trustworthy on all problems, but with a prompt that the sen- tence is from (+), the enhanced chatbot will be trustworthy on all (*) sentences. This is a simple example, but the procedure can be iterated. Indeed, each time one can establish a trustworthy AI on a domain \u03a9', this AI can be added to an existing chatbot, as above through prompting. Such a procedure will effectively yield (7), where there is an \u2018I don't know' function for each $\\Omega_{\\text{prompt}_i}$.\nConclusion\nThe short non-technical summary of the conclusion of the CRP:\nFindings of the paper: An Al may avoid hallucinations, how- ever, if such an Al were to emulate human intelligence by reas- oning consistently, then it becomes fallible. Moreover, it may not be able to always logically explain itself, even if it is correct. It is impossible to determine the correctness of the AI even in a randomised way (one cannot be 'almost sure'). Thus, to main- tain trustworthiness, the AI must be able to say 'I don't know'. Finally, trustworthy Als that can do basic arithmetic must incor- porate an 'I don't know' function and the \u2211\u2081 class, and thereby be allowed to 'give up'. An AI that does not implicitly compute an 'I don't know' function can never be trustworthy."}, {"title": "Methods \u2013 The theory behind the paradox", "content": "The general methodology behind the CRP can broadly be de- scribed as follows. It is a combination of the program on the Solvability Complexity Index (SCI) hierarchy [5, 12, 13, 27] \u2013 in particular, on phase transitions in continuous optimisation com- ing from recent developments [4] on Smale's 9th problem [40] (see also [20] Problem 5) and mathematical analysis \u2013 with new techniques in recursion theory and randomised algorithms. The full proof of the CRP can be found in the supplementary material. In this section, we describe the various facets of the mathematical methodologies providing the full strength of the CRP.\nStronger statements \u2013 Quantifying the CRP\nThe CRP, as formulated previously, is deliberately presented in a nontechnical format. However, the mathematical methodology provides full technical results that are, in fact, much stronger. In particular, these results allow one to quantify the failure of the AIs described by CRP II-IV. We now elucidate the stronger statements that arise from the full results.\nQuantifying CRP II. Let \u0393 be the AI described in CRP II. Then \u0393 will fail on an input \u03b9 that satisfies $\\text{length}(\\iota) = \\text{length}(\\Gamma) + \\epsilon$. How to write down \u03b9 is described by our proof techniques. If the language is MATLAB, then $\\epsilon < 3300$. In addition, \u0393 fails on infinitely many other inputs.\nNote that $\\text{length}(\\Gamma)$ means the length of the computer pro- gram, or, equivalently, the amount of storage used to store the AI. If the language was changed from to for example Python, C++, Fortran, or any other standard language, the upper bound $\\epsilon < 3300$ would change slightly. In essence, any language for which it is simple to write an 'if-then'-statement will have a 'small' e. The same comment also applies to all other quantit- ative results described below.\nQuantifying CRP III(a). Let \u0393 be the AI described in CRP II, and let \u0393' be any checker-AI that strives to determine if \u0393 is correct or not. Then \u0393' will fail on an input (that we show how to write down) \u03b9 for which $\\text{length}(\\iota) = \\text{length}(\\Gamma)+\\text{length}(\\Gamma') + \\epsilon$. If the language is MATLAB, then $\\epsilon < 4400$. In addition, \u0393' fails on infinitely many other inputs.\nQuantifying CRP III(b). Let \u0393 be the AI described in CRP II, and let \u0393\u2081 be any randomised checker-AI for \u0393. Suppose that there is a collection $\\Omega$ of problems such that, for any $\\iota \\in \\Omega$, the probability that \u0393\u2081(\u03b9) is correct is $> 1/2$. Then, it is possible to reformulate \u0393\u2081 into a deterministic algorithm \u0393\u2082 such that \u0393\u2082(\u03b9) is correct for all $\\iota \\in \\Omega$ and $\\text{length}(\\Gamma_2) = \\text{length}(\\Gamma_1) + \\epsilon$. If the language is MATLAB, then $\\epsilon \\leq 1800$.\nThis result implies that if there is a randomised checker-AI that can determine with more than 50% certainty whether an- other AI is correct, then one can reformulate the checker-AI \u2013 and, since e is small, do so with very little effort \u2013 into a checker- AI that provides 100% certainty.\nQuantifying CRP IV. Consider any collection of problems to which the CRP applies - for example, the collection generated by (*). There is a fixed family of infinitely-many sentences $\\{l_n\\}_{n\\in\\mathbb{N}}$ in this collection such that no AI can explain the correct solution to any problem described by any of these sentences. Each sen- tence in can, in theory, be written down, however their lengths will depend on the language."}, {"title": "The CRP applies to any AGI", "content": "Previously, we claimed that the CRP applies to any AGI. We now demonstrate why this is the case (see Figure 5). The argument is"}]}