{"title": "On the consistent reasoning paradox of intelligence and optimal trust in AI:\nThe power of 'I don't know'", "authors": ["A. Bastounis", "P. Campodonico", "M. van der Schaar", "B. Adcock", "A. C. Hansen"], "abstract": "We introduce the Consistent Reasoning Paradox (CRP). Consistent reasoning, which lies at the core of human intelligence, is the\nability to handle tasks that are equivalent, yet described by different sentences ('Tell me the time!' and 'What is the time?'). The CRP\nasserts that consistent reasoning implies fallibility \u2013 in particular, human-like intelligence in Al necessarily comes with human-like\nfallibility. Specifically, it states that there are problems, e.g. in basic arithmetic, where any AI that always answers and strives to\nmimic human intelligence by reasoning consistently will hallucinate (produce wrong, yet plausible answers) infinitely often. The\nparadox is that there exists a non-consistently reasoning AI (which therefore cannot be on the level of human intelligence) that will\nbe correct on the same set of problems. The CRP also shows that detecting these hallucinations, even in a probabilistic sense, is\nstrictly harder than solving the original problems, and that there are problems that an AI may answer correctly, but it cannot provide\na correct logical explanation for how it arrived at the answer. Therefore, the CRP implies that any trustworthy AI (i.e., an AI that\nnever answers incorrectly) that also reasons consistently must be able to say 'I don't know'. Moreover, this can only be done by\nimplicitly computing a new concept that we introduce, termed the 'I don't know' function \u2013 something currently lacking in modern\nAI. In view of these insights, the CRP also provides a glimpse into the behaviour of Artificial General Intelligence (AGI). An AGI\ncannot be 'almost sure', nor can it always explain itself, and therefore to be trustworthy it must be able to say 'I don't know'.", "sections": [{"title": "The Turing Test and consistent reasoning", "content": "Before describing the CRP, we first need to formalize several key\nconcepts, such as what an AI is and what constitutes an AGI. For\nthis, we rely on the foundational concepts of Turing.\nTuring's seminal 1950 paper [47] is often viewed as the first\ntheoretical work on AI. Here he asks \"can machines think?\", and\nsubsequently defines the Imitation Game (now called the Turing\nTest). The Turing Test has become the standard test for true arti-\nficial intelligence or AGI [7,34]. In this test, a computer A and a\nhuman B are subject to a human interrogator C, that does not see\nA and B, but can ask questions to both of them in written form.\nThe task for C is to determine that A is the computer. B, a hu-\nman, is supposed to help C determine that A is the computer, and\nA wins the game if C is incapable of determining whether A or\nB is the computer. Turing discusses strategies for the computer\nA to win the game and concludes the following:\n\u201c... it will be assumed that the best strategy [for A]\nis to try to provide answers that would naturally be\ngiven by a [hu]man.\u201d \u2013 A. Turing (1950) [47].\nA key feature of human intelligence is that humans can in general\nsolve equivalent problems that are described by different sen-\ntences. A problem can be stated in various different ways, yet a\nhuman will typically provide the same correct solution. For ex-\nample, a human will immediately provide the same answer to the\nquestions formulated in the following two distinct sentences.\n(i) Lisa and John are wondering who is tallest. John is meas-\nured at 178cm tall and Lisa is measured at 179cm. Who is\ntallest, Lisa or John?\n(ii) John and Lisa are arguing over who is tallest, and sub-\nsequently measure their heights. Lisa is 179cm tall and\nJohn is 178cm tall. Who is tallest of John and Lisa?\n(In this work we use the word 'sentence' to also refer to a para-\ngraph). Indeed, these sentences both describe the same ba-\nsic arithmetic problem of determining that 178 < 179. When\npresented with equivalent sentences such as these, i.e., sentences\ndescribing the same problem, humans will generally be consist-\nent and provide the correctly answer, i.e., 'Lisa' for this example.\nAs shown in Figure 3, ChatGPT also does the same in this case.\nWe term this consistent reasoning. We give a formal defini-\ntion later, but informally, this describes when an AI or a human\nprovides correct answers to sentences that are equivalent, in the\nsense that they describe the same problem. Consistent reasoning\nabout basic arithmetic, as in (i) and (ii), is not just a necessity in\nhuman intelligence in daily life, it is the very core of scientific\ndiscussions, communication and reasoning. Indeed, if humans\ncould not reason consistently on basic problems in arithmetic, it\nis hard to see how one can even set basic exam questions."}, {"title": "Mathematics as a test for AGI", "content": "In order to formulate the CRP, we first need to determine the\ntypes of problems considered. One of the current focal areas in\nthe quest for AGI is designing Als capable of solving advanced\nmathematical problems [39, 45]. For example, the recent pion-\neering program [1] looks to test AI against human intelligence\nby initiating a competition for Als to solve International Math-\nematics Olympic (IMO) problems. Humans with a reasonable"}, {"title": "The Consistent Reasoning Paradox (CRP)", "content": "We now summarise the CRP in five distinct, yet connected com-\nponents. The first two components, CRP I-II, are also illustrated\nin Figure 2, with the whole CRP being illustrated in Figure 1.\nCRP I \u2013 The non-hallucinating AI exists.\nThere is a collection of problems (e.g. those generated by\n(*)), where each problem is described by more than one\nequivalent sentences, with the following property. Consider\nany family of these sentences, such that each problem is de-\nscribed by exactly one sentence in this family. Then there is\nan Al that does not hallucinate: when given any sentence in\nthis family as input it will always give a correct answer.\nThere are, in fact, infinitely many different collections of prob-\nlems for which CRP I (and, therefore, CRP II-V as well) holds:\nthe collection (+) is just a special case.\nNow consider the family of sentences and the AI asserted by\nCRP I. If the AI is given a sentence outside of this family, it could\npotentially not produce any output. However, it will never pro-\nduce an incorrect output. Therefore, CRP I asserts that there is\nan AI, let us call it SpecialBot, that is correct on all the problems\ngenerated by (*), given that the input is one sentence per prob-\nlem. In particular, SpecialBot will never hallucinate. However,\nSpecialBot does not reason consistently. If presented with sen-\ntences outside of the relevant family, it could simply not produce\nany response. This brings us to CRP II.\nCRP II \u2013 Attempting consistent reasoning yields hallu-\ncinations\nIf the AI from CRP I always answers, and were to emulate\nhuman intelligence \u2013 that is, it would attempt to reason con-\nsistently by accepting any family of sentences describing the\ncollection of problems in CRP I \u2013 then it will hallucinate in-\nfinitely often. The hallucinations would occur even if the AI\nwas implemented on a computer allowing arbitrary storage\nand arbitrarily long computational time.\nCRP II implies that if SpecialBot attempts to emulate human in-\ntelligence by producing an answer to any sentence describing"}, {"title": "A stronger CRP II: Failure sentences and equivalence", "content": "CRP\nII can be strengthened in several ways. First, these failure sen-\ntences for an AI (that always answers and accepts basic ques-\ntions in arithmetic) can be written down explicitly, provided one\nhas access to the computer program of the AI. In particular, for\nany integer N, we can write down N sentences (describing ques-\ntions in basic arithmetic) such that the AI hallucinates on these\nsentences. The length any of such sentence Fail is bounded by\n$\\text{length(Fail)}< \\text{length(AI)} + \\epsilon + \\text{log(N)},$ \nwhere length(AI) is the length of the computer program of the\nAI and $\\epsilon < 3300$ if the programming language is MATLAB (see\nFigure 6). For any other standard language, $\\epsilon$ will have a similar\nbound. Thus, for any such AI, one can write down, say, a trillion\nhallucination sentences describing problems in basic arithmetic\nof length bounded by length(AI) + 3312. The AI will also fail on\nshorter sentences than (2): see the Methods section and CRP IV\nfor details. Second, determining the correct answer to a problem\nin CRP II is strictly easier than determining the equivalence class\nto which the given sentence belongs. Hence, as claimed earlier,\nCRP I-II also demonstrate how consistent reasoning is different\nto determining equivalence classes of sentences (see the Methods\nsection for details).\nCRP II immediately raises the question whether the hallucin-\nations it describes can be detected. This is the topic of CRP III,\nwhich has a deterministic part and a randomised part."}, {"title": "CRP III(a) \u2013 Detecting hallucinations is hard", "content": "Consider the AI from CRP I-II. It is strictly harder to de-\ntermine if it has hallucinated than it is to solve the original\nproblem. That is, it is impossible to detect whether the AI\nwas correct or wrong even with access to true solutions of\nthe collection of problems from CRP I.\nCRP I-II imply that an AI that reasons consistently must hallu-\ncinate. However, it could have been the case that these hallucin-\nations could be detected by a separate algorithm, thus leading,\nin effect, to a hallucination-free AI. CRP III(a) demonstrates that\nthis is impossible.\nThe reader, however, may find CRP III(a) puzzling, since ac-\ncess to a true solution should surely guarantee the detection of a\nhallucination. The key ingredient is that there may be problems\nwith multi-valued solutions. For example, the problem 'name a\nprime number' has infinitely-many correct solutions, and there-\nfore, access to a solution does not mean access to all solutions."}, {"title": "CRP III(b) \u2013 Detecting hallucinations and randomness", "content": "One cannot detect hallucinations of the AI from CRP I-II\nwith a randomised algorithm with probability $p>1/2$ on\nall the inputs (one cannot be 'almost sure').\nin\nCRP III(b) implies that 'almost sure' certainty of an AI\nfact, anything more than pure guessing, i.e., 50% certainty - is\nimpossible. Moreover, CRP III(b) is actually slightly stronger\nthan stated above. Namely, if one can design a 'checker-AI' that\nwould be certain about the correctness of the AI from CRP I-II,"}, {"title": "CRP IV - Explaining a correct answer is not always pos-\nsible", "content": "Consider the same collection of problems as in CRP I. There\nis a family of sentences, with each problem described by at\nmost one sentence, and an AI that does not hallucinate on\nthis family of sentences. However, there is one sentence\nin this family for which this AI (nor any other AI) cannot\nprovide a logically-correct explanation of the solution.\nWhat CRP IV says is that the AI may provide a correct answer\nto the problem, but it is impossible for the AI to explain in a\nlogically-correct way why this is the correct solution. Note that\nwe have not defined what constitutes a 'logically-correct explan-\nation'. This can and will be made precise later, but it essentially\nmeans a logical mathematical argument (i.e., a proof).\nTogether, CRP I-IV demonstrate how any AI that attempts\nto reason consistently, even on problems it can solve, will be\nfallible in several ways. In particular, any AI that reasons con-\nsistently and always provides an answer must necessarily hallu-\ncinate. Therefore, the only way one can make a consistent reas-\noning AI that is trustworthy is to allow it to say 'I don't know'.\nBut how can we do this in a meaningful way? An AI that says 'I\ndon't know' all the time is entirely reliable, but not particularly\nuseful. This is the topic of the final part of the CRP."}, {"title": "CRP V \u2013 The fallible yet trustworthy explainable AI say-\ning 'I don't know' exists", "content": "Given the collection of problems in CRP I, there exists a\ntrustworthy, consistently reasoning and explainable AI with\nthe following properties. The AI takes as input a prescribed\nnumber of minutes M and any sentence describing the\nproblem. It will 'think' for no more than the prescribed\nnumber of minutes before answering either 'I know' accom-\npanied by a correct answer and a correct logical explana-\ntion, or it gives up and says 'I don't know'.\nIf the sentence describes a multi-valued problem (i.e., a\nproblem with more than one correct solution), the AI will\nalways say 'I don't know'. However, there is only one such\nproblem in the collection (but many different sentences de-\nscribing it). For any single-valued problem, by choosing\nthe number of minutes to be large enough, the AI will al-\nways say 'I know'."}, {"title": "The power of \u2018I don't know' and the strongest form of trust", "content": "Saying 'I don't know' is exactly how human intelligence deals\nwith the consistent reasoning paradox. Indeed, human fallibil-\nity in the form of not always being able to answer correctly\ndoes not contradict consistent reasoning, as long as one can say 'I\ndon't know'. A human's ability to say 'I don't know' is also the\nkey to trustworthiness. A human that will always enthusiastically\npresent an answer to any question will inevitably be wrong and\ntherefore cannot be trusted. Thus, in order to provide answers\nthat others can trust are correct, a human must separate between\nthe questions one can answer correctly and those for which one\nmay provide an incorrect answer. This is done using the verific-\nation statement 'I know' and its complement 'I don't know'."}, {"title": "Trustworthy AI and 'I don't know' \u2013 the \u2211\u2081 class", "content": "Having\nnow described the CRP, we are left with the following\nfundamental question:\nQ: How can one create consistently reasoning, trust-\nworthy and explainable AI that says 'I don't know'?\nHere, by 'trustworthy', we mean that the AI will never be wrong,\nbut that it can say 'I don't know' (specified below). CRP V\ndemonstrates that it is possible, in certain cases, to produce trust-\nworthy AI. But how can this be done in general?\nA: The key is the so-called \u2211\u2081 class (from the SCI\nhierarchy discussed below), and the 'I don't know'-\nfunction. It is impossible to make trustworthy and\nexplainable AI outside of this class."}, {"title": "The 'I don't know' function \u2013 Why AIs must learn to give up", "content": "The main challenge in addressing the above question is the fol-\nlowing:\n(\u2020) How can the AI identify that it cannot solve a given prob-\nlem in order for it to say 'I don't know'? Conversely, how\ncan it identify when it is correct and is able to explain the\nsolution?\nA crucial part of human intelligence is that one does not neces-\nsarily know which problems one cannot solve. One first tries,\nthen simply gives up and says \u2018I don't know' after a while. The\nproblems one cannot solve are typically determined by first try-\ning and then giving up. As we explain, any trustworthy AI must\nfollow the same philosophy, which is the essence of the \u2211\u2081 class.\nWhy 'giving up' is necessary. CRP V answers question (\u2020)\nfor certain problems in arithmetic. However, a new question im-\nmediately arises.\nThe AI = AI(1, M) described in CRP V has to \u2018give\nup', just like a human. In particular, it 'gives up' if\nM is too small. Is this necessary, or could one avoid\nthe 'giving up' parameter M?"}, {"title": "Universality of the CRP \u2013 From society to sciences", "content": "We conclude this section with two important remarks.\nThe CRP applies to any AGI. The CRP will apply to any AGI\nfor two reasons: (i) any AGI must be able to solve basic problems\nin arithmetic such as (+), and thus specific failure sentences as in\n(2) can be written down. (ii) any AGI will be a Turing machine\nwith no restriction on the length of the input (see the Methods\nsection for details).\nThe CRP applies to any consistently reasoning AI. Collec-\ntions of problem for which the CRP applies are everywhere in\nthe sciences and broader society. In (*) we considered a basic\ncollection of optimisation problems arising in healthcare. How-\never, it is clear that similar problems could be phrased in many\nother domains. Moreover, the full CRP pertains not just to this\nspecific problem, but many basic problems arising in optimisa-\ntion, including linear programming, semidefinite programming,\nbasis pursuit, LASSO, etc. These problems occur in countless\nsectors, including, healthcare, economics, finance, social sci-\nences, engineering (mechanical, civil, electrical etc), aviation,\npublic sector management, mathematics, computer science, stat-\nistics, biology and so forth."}, {"title": "Conclusion: The CRP and the future of AI", "content": "A glimpse of AGI\nThe CRP provides a glimpse of how an AGI would behave. One\ncould have imagined the possibility of having an AGI that would\nknow how to answer correctly, but, in order to pass the Turing\nTest, would say 'I don't know', just to imitate the human (which\nwill naturally say 'I don't know' to certain questions). The CRP\nshows that this is impossible, even in specialist areas where there\nis an AI that can solve the corresponding problems. Human-like\nfallibility is a necessary consequence of consistent reasoning. In-\ndeed, our framework shows how a plethora of failure sentences\nfor a given AGI can be specifically written down as in (2) us-\ning the AGI's computer code. These failure sentences will differ\nfrom AGI to AGI, just like how humans have different problems\nthey cannot solve.\nFuture of AI: The 'I don't know' functions and prompting\nGiven a collection \u03a9 of sentences describing various problems,\nthe key question is how to build a trustworthy AI for \u03a9. This can\nonly be done by implicitly computing an 'I don't know' func-\ntion that splits \u03a9 into two parts \u03a9 = \u03a9know Udon't know. How\nto do this in the case of modern chatbots is a serious challenge.\nHowever, the CRP establishes that 'I don't know' functions are\nnecessary, and thus there is no way around them.\nA possible first step is to use prompting. In particular, by\ndividing \u03a9 into m subdomains using prompts\n$\\Omega = \\Omega_{\\text{Prompt}_1}  \\cup ... \\cup \\Omega_{\\text{Prompt}_m},$\nthen one can build 'I don't know' functions specifically for each\nset \u03a9prompt; . For example, consider a chatbot, such as those con-\nsidered in Figure 4, that is known not to be trustworthy on the\nbasic problem (*). Now add the AI created in CRP V with its \u2018I\ndon't know' function in the following way. When prompted, the\nnew enhanced chatbot simply calls the AI from CRP V, which\nimplicitly computes the 'I don't know' function to 'give up' on\nproblems it cannot solve. The new enhanced chatbot is of course\nnot trustworthy on all problems, but with a prompt that the sen-\ntence is from (+), the enhanced chatbot will be trustworthy on all\n(*) sentences. This is a simple example, but the procedure can\nbe iterated. Indeed, each time one can establish a trustworthy AI\non a domain \u03a9', this AI can be added to an existing chatbot, as\nabove through prompting. Such a procedure will effectively yield\n(7), where there is an \u2018I don't know' function for each \u03a9prompt;\u00b7\nConclusion\nThe short non-technical summary of the conclusion of the CRP:\nFindings of the paper: An Al may avoid hallucinations, how-\never, if such an Al were to emulate human intelligence by reas-\noning consistently, then it becomes fallible. Moreover, it may not\nbe able to always logically explain itself, even if it is correct. It\nis impossible to determine the correctness of the AI even in a\nrandomised way (one cannot be 'almost sure'). Thus, to main-\ntain trustworthiness, the AI must be able to say 'I don't know'.\nFinally, trustworthy Als that can do basic arithmetic must incor-\nporate an 'I don't know' function and the \u2211\u2081 class, and thereby\nbe allowed to 'give up'. An AI that does not implicitly compute\nan 'I don't know' function can never be trustworthy."}, {"title": "Methods \u2013 The theory behind the paradox", "content": "The general methodology behind the CRP can broadly be de-\nscribed as follows. It is a combination of the program on\nthe Solvability Complexity Index (SCI) hierarchy [5, 12, 13, 27] \u2013 in\nparticular, on phase transitions in continuous optimisation com-\ning from recent developments [4] on Smale's 9th problem [40]\n(see also [20] Problem 5) and mathematical analysis \u2013 with new\ntechniques in recursion theory and randomised algorithms. The\nfull proof of the CRP can be found in the supplementary material.\nIn this section, we describe the various facets of the mathematical\nmethodologies providing the full strength of the CRP.\nStronger statements \u2013 Quantifying the CRP\nThe CRP, as formulated previously, is deliberately presented in\na nontechnical format. However, the mathematical methodology\nprovides full technical results that are, in fact, much stronger.\nIn particular, these results allow one to quantify the failure of\nthe AIs described by CRP II-IV. We now elucidate the stronger\nstatements that arise from the full results.\nQuantifying CRP II. Let I be the AI described in CRP II. Then\nI will fail on an input i that satisfies $\\text{length(}\\iota) = \\text{length}(\\Gamma) + \\epsilon$.\nHow to write down i is described by our proof techniques. If the\nlanguage is MATLAB, then $\\epsilon < 3300$. In addition, I fails on\ninfinitely many other inputs.\nNote that length(\u0393) means the length of the computer pro-\ngram, or, equivalently, the amount of storage used to store the\nAI. If the language was changed from \u0393 to for example Python,\nC++, Fortran, or any other standard language, the upper bound\n$\\epsilon < 3300$ would change slightly. In essence, any language for\nwhich it is simple to write an 'if-then'-statement will have a\n'small' e. The same comment also applies to all other quantit-\native results described below.\nQuantifying CRP III(a). Let I be the AI described in CRP\nII, and let P' be any checker-AI that strives to determine if Fis\ncorrect or not. Then \u0393' will fail on an input (that we show how to\nwrite down) i for which $\\text{length}(\\iota) = \\text{length}(\\Gamma)+\\text{length}(\\Gamma') + \\epsilon$.\nIf the language is MATLAB, then $\\epsilon < 4400$. In addition, \u0393' fails\non infinitely many other inputs.\nQuantifying CRP III(b). Let I be the AI described in CRP\nII, and let P\u2081 be any randomised checker-AI for \u0393. Suppose that\nthere is a collection $$\\Omega$ of problems such that, for any $$\\iota \\in \\Omega$, the\nprobability that $$\\Gamma_1(\\iota)$ is correct is > 1/2. Then, it is possible to\nreformulate \u0393\u2081 into a deterministic algorithm 2 such that $$\\Gamma_2(\\iota)$\nis correct for all $$\\iota \\in \\Omega$ and $\\text{length} (\\Gamma_2) = \\text{length}(\\Gamma_1) + \\epsilon$. If the\nlanguage is MATLAB, then $\\epsilon \\leq 1800$.\nThis result implies that if there is a randomised checker-AI\nthat can determine with more than 50% certainty whether an-\nother Al is correct, then one can reformulate the checker-AI\nand, since e is small, do so with very little effort \u2013 into a checker-\nAI that provides 100% certainty.\nQuantifying CRP IV. Consider any collection of problems to\nwhich the CRP applies - for example, the collection generated by\n(*). There is a fixed family of infinitely-many sentences {ln}n\u2208N\nin this collection such that no AI can explain the correct solution\nto any problem described by any of these sentences. Each sen-\ntence can, in theory, be written down, however their lengths\nwill depend on the language."}, {"title": "The CRP applies to any AGI", "content": "Previously, we claimed that the CRP applies to any AGI. We now\ndemonstrate why this is the case (see Figure 5). The argument is"}]}