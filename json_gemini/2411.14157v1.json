{"title": "DrugGen: Advancing Drug Discovery with Large Language Models and Reinforcement Learning Feedback", "authors": ["Mahsa Sheikholeslami", "Navid Mazrouei", "Yousof Gheisari", "Afshin Fasihi", "Matin Irajpour", "Ali Motahharynia"], "abstract": "Traditional drug design faces significant challenges due to inherent chemical and biological complexities, often resulting in high failure rates in clinical trials. Deep learning advancements, particularly generative models, offer potential solutions to these challenges. One promising algorithm is DrugGPT, a transformer-based model, that generates small molecules for input protein sequences. Although promising, it generates both chemically valid and invalid structures and does not incorporate the features of approved drugs, resulting in time-consuming and inefficient drug discovery. To address these issues, we introduce DrugGen, an enhanced model based on the DrugGPT structure. DrugGen is fine-tuned on approved drug-target interactions and optimized with proximal policy optimization. By giving reward feedback from protein-ligand binding affinity prediction using pre-trained transformers (PLAPT) and a customized invalid structure assessor, DrugGen significantly improves performance. Evaluation across multiple targets demonstrated that DrugGen achieves 100% valid structure generation compared to 95.5% with DrugGPT and produced molecules with higher predicted binding affinities (7.22 [6.30-8.07]) compared to DrugGPT (5.81 [4.97-6.63]) while maintaining diversity and novelty. Docking simulations further validate its ability to generate molecules targeting binding sites effectively. For example, in the case of fatty acid-binding protein 5 (FABP5), DrugGen generated molecules with superior docking scores (FABP5/11, -9.537 and FABP5/5, -8.399) compared to the reference molecule (Palmitic acid, -6.177). Beyond lead compound generation, DrugGen also shows potential for drug repositioning and creating novel pharmacophores for existing targets. By producing high-quality small molecules, DrugGen provides a high-performance medium for advancing pharmaceutical research and drug discovery.", "sections": [{"title": "1. Introduction", "content": "Traditional drug design often falls short in handling the vast chemical and biological space features involved in ligand-receptor interactions [1,2]. Usually, a major proportion of suggested drug candidates fail in clinical trials [3], making drug discovery a time-consuming and costly process. Recent advances in deep learning (DL), particularly in generative models, offer promising solutions for these obstacles [4,5]. Deep learning models have been extensively used in molecular design [6,7], pharmacokinetics [8\u201311], pharmacodynamics predictions [12], and toxicity assessments [10]. These models improve the efficiency and accuracy of various tasks in drug development, contributing to different stages of drug discovery and optimization projects [13,14]. However, due to the insufficiency of available datasets, complexity of drug-target interactions, and complication of manipulating complex chemical structures, generative DL models also seem to be insufficient in proposing optimal answers to drug design problems [15]. Nevertheless, with the advancement of transformer-based architecture in large language models (LLMs), new horizons have opened up in various biological contexts. ProGen, a model developed to design new proteins with desired functionality and protein-ligand binding affinity prediction using pre-trained transformers (PLAPT), a model for protein-ligand binding affinity prediction, are successful examples of the application of LLMs in bioinformatics [16, 17]. DrugGPT, an LLM based on the generative pre-trained transformer (GPT) architecture [18] is another example that has shown potential in generating novel drug-like molecules having interactions with biological targets [19].\nDrugGPT leverages the transformer architecture to comprehend structural properties and structure-activity relationships. Receiving the amino acid sequence of a given target protein, this model generates simplified molecular input line entry system (SMILES) [20] strings of interacting small molecules. By learning from large datasets of known drugs and their targets, DrugGPT can propose new compounds with desired properties by employing autoregressive algorithms for a stable and effective training process [21], thus accelerating the lead discovery phase in drug development. However, the effectiveness of generative models in drug discovery relies heavily on the quality and relevance of the training data [5]. Models trained on comprehensive and accurately curated datasets are more likely to produce viable drug candidates [22]. Additionally, fine-tuning these models can enhance their performance for predictive applications [23]."}, {"title": "2. Results", "content": "In this study, we developed \u201cDrugGen\u201d, an LLM based on the DrugGPT architecture, fine-tuned using a curated dataset of approved drug-target pairs; which is further enhanced using a policy optimization method. By utilizing this approach, DrugGen is optimized to generate drug candidates with optimized properties. Furthermore, we evaluated the model's performance using custom metrics validity, diversity, and novelty to comprehensively assess the quality and properties of the generated compounds. Our results indicated that DrugGen generates chemically sound and valid molecules in comparison with DrugGPT while maintaining diversity and validity of generated structures. Notably, DrugGen excels in generating molecules with higher predicted binding affinities, increasing the likelihood of strong interactions with biological targets. Docking simulations further demonstrated the model's capability to accurately target binding sites and suggest new pharmacophores. These findings highlight DrugGen's promising potential to advance pharmaceutical research. Moreover, we proposed evaluation metrics that can serve as objective and practical measures for comparing future models.\nIn order to develop an algorithm to generate drug-like structures, we gathered a curated dataset of approved drug-target pairs. We began by selecting a pre-trained model and then enhanced its performance through a two-step process. First, we employed supervised fine-tuning (SFT) on a dataset of approved sequence-SMILES pairs to fine-tune the model. Next, we utilized a reinforcement learning algorithm-proximal policy optimization (PPO)\u2014along with a customized reward system to further optimize its performance. The final model was named DrugGen. The schematic design of the study is illustrated in Fig. 1."}, {"title": "2.1. DrugGen is effectively fine-tuned on a dataset of approved drug-target", "content": "Supervised fine-tuning using the SFT trainer exhibited a steady decrease in training and validation loss over the epochs, indicating effective learning (Fig. 2A and Supplementary file 1). After three epochs of training, the loss of both the training and validation datasets reached a plateau. Therefore, checkpoint number three was selected for the second phase. In the second phase, the model was further optimized using PPO based on the customized reward system. Over 20 epochs of optimization, the model generated 30 unique small molecules for each target in each epoch, ultimately reaching a plateau in the reward diagram (Fig. 2B and Supplementary file 2)."}, {"title": "2.2. DrugGen generates valid, diverse, and novel small molecules", "content": "Eight proteins were selected for models assessments which include two targets with a high probability of association with diabetic kidney disease (DKD) from the DisGeNet database, angiotensin-converting enzyme (ACE) and peroxisome proliferator-activated receptor gamma (PPARG) and six proteins without known approved drugs, i.e., galactose mutarotase (GALM), putative fatty acid-binding protein 5-like protein 3 (FB5L3), short-wave-sensitive opsin"}, {"title": "2.3. DrugGen generates small molecules with high affinity for their targets", "content": "We used two different measures to assess the binding affinity of the generated molecules to their respective targets: PLAPT, an LLM for predicting binding affinity, and molecular docking.\nPLAPT: The same set of small molecules generated in novelty assessment (100 unique small molecules for each target) were used for assessing the quality of generated structures. Except for FABP5, DrugGen consistently produced small molecules with significantly higher binding affinities compared to DrugGPT ([7.22 [6.30-8.07] vs. 5.81 [4.97-6.63], U = 137934, P <10-85], Fig. 3C, Table 1, and Supplementary file 5). This finding underscores DrugGen's superior capability to generate high-quality structures."}, {"title": "3. Discussion", "content": "In this study, we developed DrugGen, a large language model designed to generate small molecules based on the desired targets as input. DrugGen is based on a previously developed model known as DrugGPT, achieving improvements by supervised fine-tuning on approved drugs and reinforcement learning. These improvements aim to facilitate the generation of novel small molecules with stronger binding affinities and a higher probability of approval in future clinical trials. The results indicate that DrugGen can produce high-affinity molecules with robust docking scores, highlighting its potential to accelerate the drug discovery process.\nDrugGen is primarily based on the DrugGPT, which utilizes a GPT-2 architecture trained"}, {"title": "4. Materials and Methods", "content": null}, {"title": "4.1. Dataset Preparation", "content": "A dataset of small molecules, each approved by at least one regulatory body, was collected to enhance the safety and relevance of the generated molecules. First, 1710 small molecules from the DrugBank database (version: 2023-01-04) were retrieved [31], 117 of which were labeled as withdrawn. After initial assessments of withdrawn drugs by a physician (Ali Motahharynia) and a pharmacist (Mahsa Sheikholeslami), consensus was reached to omit 50 entries due to safety concerns. Consequently, 1660 approved small molecules and their respective targets were selected. From the total of 2116 approved drug targets, retrieved from Drug-Bank database, 27 were not present in the UniProt database [32]. After further assessment, these 27 proteins were replaced manually with equivalently reviewed UniProt IDs, identical protein names, or by basic local alignment search tools (BLAST) [33]. The protein with UniProt ID \u201cQ5JXX5\u201d was deleted from the UniProt database and therefore, omitted from the collected dataset as well. Finally, 1660 small molecules and 2093 related protein targets were selected. Available SMILEs (1634) were retrieved from DrugBank, ChEMBL [34], and ZINC20 databases [35]. Protein sequences were retrieved from the UniProt database."}, {"title": "4.2. Data Preprocessing", "content": "Similar to the structure used by DrugGPT, the small molecules and target sequences were merged into the pair of a string consisting of protein sequence and SMILES in the following format: \u201c<|startoftext|> + <P> + target protein sequence + <L> + SMILES + <|end-"}, {"title": "4.3. DrugGen Development Overview", "content": "Using the supervised fine-tuning (SFT) trainer module from the transformer reinforcement learning (TRL) library (version: 0.9.4) [36], the original DrugGPT model was finetuned on our dataset. Afterward, reinforcement learning was applied to further improve the model. For this purpose, a Tesla V100 GPU with 32 GB of VRAM, 64 GB of RAM, and a 4-core CPU were utilized for both phases, i.e., SFT and reinforcement learning using a PPO trainer."}, {"title": "4.3.1. Supervised Fine-tuning", "content": "The training dataset consisted of 9398 strings. The base model was trained using the SFT trainer class for five epochs with the following configuration: Learning rate: 5e-4, batch size: 8, warmup steps (linear warmup strategy): 100, and eval steps: 50. AdamW optimizer with a learning rate of 5e-4 and epsilon value of 1e-8 was used for optimizing the model parameters. The model's performance on the training and validation sets (ratio of 8:2) was evaluated using the cross-entropy loss function during the training phase."}, {"title": "4.3.2. Proximal Policy Optimization", "content": "Hugging Face's PPO Trainer, which is based on OpenAI's original method for \u201cSummarize from Feedback\" [37] was used in this study. PPO is a reinforcement learning algorithm that improves the policy by taking small steps during optimization, avoiding overly large updates that could lead to instability. The key formula used in PPO is:\n$L^{CLIP}(\\theta) = E_t [min(r_t(\\theta) A_t, clip(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon)A_t)]$\nIn this equation, $L^{CLIP}(\\theta)$ represents the clipped objective function that PPO aims to optimize during training. The expectation $E_t$ denotes the average over time steps $t$, capturing the overall performance of the policy. The term $r_t(\\theta)$ is the probability ratio of taking action $a_t$ under the new policy compared to the old policy, defined as $r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$. The advantage estimate $A_t$ quantifies the relative value of the action taken in relation to the expected value of the policy. The clipping function, $clip(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon)$, restricts the ratio to a defined range, preventing large updates to the policy that could destabilize training. This formulation allows PPO to balance exploration and stability, enabling effective policy updates while minimizing the risk of performance degradation. There are three main phases in training a model with PPO. First, the language model generates a response based on an input query in a phase called the rollout phase. In our study, the queries were protein sequences, and the generated responses were SMILES strings. Then in the evaluation phase, the generated"}, {"title": "4.3.3. Reward Function", "content": "PLAPT: PLAPT, a cutting-edge model designed to predict binding affinities with remarkable accuracy was used as a reward function. PLAPT leverages transfer learning from pre-trained transformers, ProtBERT and ChemBERTa, to process one-dimensional protein and ligand sequences, utilizing a branching neural network architecture for the integration of features and estimation of binding affinities. The superior performance of PLAPT has been validated across multiple datasets, where it achieved state-of-the-art results [16]. The affinities of the generated structures with their respective targets were evaluated using PLAPT\u2019s neg_log10_affinity_M output.\nCustomized invalid structure assessor: We developed a customized algorithm using RDKit library (version: 2023.9.5) [38] to assess invalid structure, where specific checks were performed to identify potential issues such as atom count, valence errors, and parsing errors. Invalid structures, including those with fewer than two atoms, incorrect valence states, or parsing failures were flagged and penalized accordingly. To promote the generation of valid molecules, a reward value of 0 was assigned to any invalid SMILES structures. These reward systems provide a rigorous scoring system for model development.\nTo further shift the model toward generating novel molecules, a multiplicative penalty was applied to the reward score when a generated SMILES string matched a molecule already present in the approved SMILES dataset. Specifically, the reward was multiplied by 0.7 for such occurrences, to retain a balance between generating new structures as well as repurposing approved drugs."}, {"title": "4.3.4. DrugGen Assessment", "content": "To evaluate the performance of DrugGen, several metrics were employed to measure its efficacy in generating viable and high-affinity drug candidates. For this purpose, eight targets consisting of two DKD targets with the highest score in DisGeNet database (version 3.12.1) [39], i.e., \"ACE\u201d and \u201cPPARG\" and six targets without any known approved small molecules for them were selected. The selection of these six targets was according to our recent study \u201cDrugTar Improves Druggability Prediction by Integrating Large Language Models and Gene Ontologies\" [24]. According to this study, 6 out of the 10 most probable proteins for future targets were selected. The selected targets are \u201cGALM\u201d, \u201cFB5L3\u201d, \u201cOPSB\u201d, \u201cNAMPT\u201d, \u201cPGK2\u201d, and \u201cFABP5\u201d. The generative quality of DrugGPT and DrugGen in terms of validity, diversity, novelty, and binding affinity was assessed. Additionally, we performed in silico validation of the molecules generated by DrugGen using a rigorous docking method.\nValidity Assessment The validity of the generated molecules was evaluated using the previously mentioned customized invalid structure assessor. The percentage of valid to total generation was reported as models' capability to construct valid structures.\nDiversity Assessment To assess the diversity of the generated molecules, 500 ligands were generated for each target by DrugGPT and DrugGen. The diversity of the generated molecules was quantitatively assessed using the Tanimoto similarity index [40]. The diversity evaluation process involved the following steps: First, each generated molecule was converted to its corresponding molecular fingerprint using Morgan fingerprints (size = 2048 bits, radius = 2) [41]. For each molecule, pairwise Tanimoto similarities were calculated between all possible pairs of fingerprints, and the average value was calculated. Thus, the diversity of the generated set was determined as the \u201c1 average of Tanimoto similarity\" within a generated batch. The distribution of diversity for each target was plotted. The invalid structures were not involved in diversity assessments. Statistical analyses were performed using Mann-Whitney U test.\nNovelty Assessment For each target, a set of 100 unique molecules was generated by DrugGPT and DrugGen. The novelty of the generated molecules was evaluated by comparing them to a dataset of approved drugs. After converting the molecules into Morgan fingerprints, the similarity of each generated molecule to the approved drugs was calculated using Tanimoto similarity index, retaining only the maximum similarity value. The novelty was reported as the \u201c1 max_Tanimoto similarity\u201d. The invalid structures were not included in the novelty assessments. Statistical analyses were performed using Mann-Whitney U test.\nPLAPT Binding Affinity Assessment The same set of molecules generated during the novelty assessment was used to evaluate the binding affinities of the compounds produced by DrugGPT and DrugGen. The invalid structures were involved in the binding affinity assessments. Statistical analysis was conducted using the Mann-Whitney U test, and corrections for multiple comparisons were applied using the Bonferroni method."}, {"title": "Molecular Docking", "content": "Molecular docking was conducted for selected targets with available protein data bank (PDB) structures, specifically ACE, NAMPT, GALM, and FABP5. A set of 100 newly generated molecules, following duplicate removal, were docked into the crystal structures of ACE (PDB ID: 1086), NAMPT (PDB ID: 2gvj), GALM (PDB ID: 1snz), and FABP5 (PDB ID: 1b56). Overall, blind docking [42] was employed for all 122 generated molecules and their references to thoroughly search the entire protein surface for the most favorable active site (Supplementary file 6 and Supplementary file 7). The reference ligands used were Lisinopril for ACE and Palmitic acid for FABP5, both of which were bound in the active site. For NAMPT, Daporinad, a molecule currently in phase 2 clinical trials, served as the highest available reference. In the case of GALM, no reference ligand was found. The retrieved PDB files were prepared using the protein preparation wizard [43] available in the Schr\u00f6dinger suite, ensuring the addition of missing hydrogens, assignment of appropriate charge states at physiological pH, and reconstruction of incomplete side chains and rings. LigPrep [44] with the OPLS4 force field [45] was employed to generate all possible stereoisomers and ionization states at pH 7.4\u00b10.5. The prepared structures were used for docking.\nDocking simulations were performed using the GLIDE program citeFriesner2004. Ligands were docked using the extra precision (XP) protocol. Ligands were allowed full flexibility during the docking process, while the protein was held rigid. The information of the grid boxes is summarized in Table 3."}]}