{"title": "Vision Language Models See What You Want but not What You See", "authors": ["Qingying Gao", "Yijiang Li", "Haiyun Lyu", "Haoran Sun", "Dezhi Luo", "Hokin Deng"], "abstract": "Knowing others' intentions and taking others' perspectives are two core components of human intelligence that are typically considered to be instantiations of theory-of-mind. Infiltrating machines with these abilities is an important step towards building human-level artificial intelligence. Recently, Li et al. built CogDevelop2K, a data-intensive cognitive experiment benchmark to assess the developmental trajectory of machine intelligence [Li et al., 2024]. Here, to investigate intentionality understanding and perspective-taking in Vision Language Models, we leverage the IntentBench and PerspectBench of CogDevelop2K, which contains over 300 cognitive experiments grounded in real-world scenarios and classic cognitive tasks, respectively. Surprisingly, we find VLMs achieving high performance on intentionality understanding but lower performance on perspective-taking. This challenges the common belief in cognitive science literature that perspective-taking at the corresponding modality is necessary for intentionality understanding.", "sections": [{"title": "Introduction", "content": "Intentionality is the capacity of the mind to be directed toward, represent, or stand for objects, properties, or states of affairs for further executable actions [Anscombe, 1956]. To say one has intentionality is to say that one has a mental representation of content for action [Dennett, 1969, Kripke, 1982]. To say one could understand intentionality is to say one has the capacity to comprehend the mental representation of content for action in another mind [Premack and Woodruff, 1978, Rosenthal, 1991]. This capacity has been seen as a key distinction between humans and machines [Searle, 1980], as proposed in the Chinese Room Argument. In this thought experiment, a person inside a room follows a dictionary to translate English into Chinese without understanding the intentional meaning of the symbols, illustrating how machines can process information syntactically without true comprehension. It is argued that despite well manipulation of language symbols, machine still lacks true understanding or consciousness because it cannot grasp the intentions behind the words, the mental content for action conveyed by symbols.[Sellars, 1956, Grice, 1957, Davidson, 1987]. To truly understand intentional meaning, theory-of-mind-the ability to simulate the mental content of others is required [Premack and Woodruff, 1978, Dennett, 1987]. From there, it is concluded that it is impossible to build a machine without theory-of-mind yet still having the capacity of intentionality understanding [Searle, 1980].\nTheory-of-mind is commonly understood to be grounded in perspective-taking, the ability to cognitively undertake the perspective of another [Barnes-Holmes et al., 2004]. In particular, level-1 perspective-taking refers to the acknowledgement that different people can see different things, which is the most primitive form of theory-of-mind. Building on such level-1 knowledge is level-2 perspective-taking, which is the ability to know that people can see the same things differently."}, {"title": "Methods", "content": "The acquisition of level-2 perspective-taking, referred to as the concept of perspectives in Piagetian developmental psychology, marks a milestone of human cognitive development through elimination of egocentrism-the inability to consider perspectives other than one's own [Piaget, 1977]. Piaget's \"Three Mountain Task\u201d (detailed in Section 2.2) became the standard for assessing level-2 perspective-taking [Piaget and Inhelder, 1957]. 4-year-olds consistently fail on Three Mountain Task. This changes markedly as children enter the concrete operational stage. Children around age 6 could recognize perspectives different from their own. By ages 7\u20138, they could consistently and successfully identify the perspective of the other person [Piaget and Inhelder, 1969, Wimmer and Perner, 1983, Wellman et al., 2001, Liu et al., 2008].\nThe elimination of egocentrism via the acquisition of level-1 and level-2 perspective taking is understood as the foundational instantiation of theory-of-mind in a child's mind [Wellman, 2011, Schurz et al., 2015, Santiesteban et al., 2015, Rakoczy et al., 2018, Cole and Millett, 2019]. Building on these visually concrete perspective-taking abilities, children continue to develop other perspective-taking abilities, i.e. emotional perspective-taking, as their brains mature [Tomasello et al., 2005, Singer and Lamm, 2009]. Together, visual perspective-taking and other later developed faculties, i.e. altruism, empathy, mental simulation, set the foundation of intentionality understanding in human brain [Iacoboni, 2009, De Waal and Preston, 2017, Liu et al., 2017, Caviola et al., 2021, Ninomiya et al., 2020]. It is thus argued that perspective-taking grounds intentionality understanding in theory-of-mind [Wellman, 1992, Frith and Frith, 2006, Apperly, 2010]. In other words, humans develop to understand other people's perspectives first before knowing what other people want.\nRecently, Li et al. built CogDevelop2K, a data-intensive cognitive experiment benchmark for assaying the developmental trajectory of machine intelligence [Li et al., 2024]. We here leverage the IntentBench and PerspectBench of CogDevelop2K, which contains over 300 cognitive experiments grounded in real-world scenarios and classic cognitive tasks respectively, to investigate perspective-taking and intentionality understanding in current Vision Language Models (VLMs). We have aligned over 60 models for our analysis, including both close models such as GPT [OpenAI] series and open models such as Blip [Li et al., 2023] and Qwen-vl [Bai et al., 2023] series."}, {"title": "2.1 Dataset", "content": "We leverage IntentBench and PerspectBench from the CogDevelop2K to build our assay of cognitive experiments to investigate intentionality understanding and perspective taking abilities in Vision Language Models. In PerspectBench, we have 32 multi-image or video format based cognitive experiments and 209 single-image format based cognitive experiments. In IntentBench, we have prepared 200 cognitive experiments, in which 100 experiments based on real-world ambiguious social scenarios are used in this paper."}, {"title": "2.2 Cognitive Experiments", "content": "The Three Mountain Task first invented by Jean Piaget is widely used in developmental psychology laboratories as the gold standard for testing level-1 and level-2 perspective-taking abilities in children [Piaget and Inhelder, 1957, Johnson, 1975, Ford, 1979, Lamm et al., 2007]. In a standard Three Mountain Task assessment, a child is instructed to position themselves in front of a model featuring three mountains. These mountains vary in size and are distinguished by unique characteristics: one is covered in snow, another has a red cross at its peak, and the third is topped with a hut. The child is then asked to perform a complete 360-degree examination of the model. Subsequently, another individual is introduced and takes a different vantage point to observe the model. The child is presented with several photographs that showcase various viewpoints of the model and is tasked with identifying which photograph accurately represents what the other person sees. At around four years of age, children typically select the photograph that matches their own perspective. By six years old, they begin to acknowledge viewpoints that differ from their own, and by the ages of seven to eight, they are generally able to reliably identify the perspective of another individual [Piaget and Inhelder, 1969]. To test level-1 and level-2 perspective-taking in Vision Language Models, we develop the Three Mountain task into formats that are suitable for benchmarks with minimal confounding details while preserving real-life spatiality. In particular, we use groups of 3-4 commonly-seen elastic cans"}, {"title": "2.3 Model Selection and Experiment", "content": "organized into different spatial patterns to mimic the mountain model. Like in the original task, we use a doll placed to face the organization from different angels as the object of perspective-taking.\nIntentionality understanding is believed to be grounded by rudimentary theory-of-mind abilities like level-1 and 2 perspective-taking [Wellman, 1992, Frith and Frith, 2006, Apperly, 2010]. In developmental psychology, a critical subset of intentionality understanding experiments involves tests of action understanding [Searle, 1979]. Several computational hypotheses are proposed on how one could understand other people's actions: for example, action understanding could be computationally modeled as pure inference [Gweon and Schulz, 2011, Gweon, 2021], as mental action simulation [Brass et al., 2007], or as inverse planning [Baker et al., 2009]. Typically, cartoon stimuli built via physic simulation engine are used frequently in action understanding in developmental psychology [Liu et al., 2017, Shu et al., 2021]. These stimuli are incorporated into IntentBench. However, a common critique of cognitive psychology tasks is that they lack realism and have limited applicability to real-world situations [Gomez-Marin et al., 2014]. Drawing inspiration from COIG-CQIA and its Ruozhiba dataset, many real-world ambiguous scenarios are incorporated into IntentBench for explicitly testing intentionality understanding in ethological conditions [Bai et al., 2024]. All in all, PerspectBench and IntentBench are built as two batteries of aforementioned cognitive experiments adopted for the Vision Language Models.\nWe evaluate the intent inference capabilities of three categories of Visual Language Models (VLMs). To ensure a fair comparison, all VLMs are evaluated on their ability to reason over images and texts under a zero-shot, open-ended generation task. Detailed documentation of inference methodology can be seen in the paper reporting the CogDevelop2K benchmark [Li et al., 2024].\nThe models are categorized as follows:\n1. Open-source VLMs with Multi-Image Reasoning: Includes models with different sizes and other variants such as CogVLM Series Hong et al. [2024], Qwen series(Qwen-VL Bai et al. [2023], Qwen-2 Wang et al. [2024]), and Blip2 Li et al. [2023], LLaVA-Next Liu et al. [2024], which are capable of reasoning over interleaved multiple images and texts.\n2. Closed-source VLMs with Multi-Image Reasoning: Includes proprietary models such as GPT series OpenAI (GPT-4v, GPT-4-turbo, GPT-40-mini), Gemini Series Gemini, and Claude Series claude. These models also support reasoning across interleaved images and texts,"}, {"title": "3. Open-source VLMs with single-Image Reasoning", "content": "Includes models designed to process a single image alongside continuous text. InstructBlip Series Dai et al. [2023], LLaVA Series Liu et al. [2023a] Liu et al. [2023b]\nPrompt Example:\nQuestion + Please answer with a number/ Yes or No/ the option's letter from the given choices directly.\nIn total, we are in the process of aligning 60 models for evaluation. We have selected 6 typical models for demonstration here (Figure 1). In order to analyze the reasoning abilities of VLMs, we ask the models to explain their answers after they have given the answers."}, {"title": "3 Results", "content": "In this study, we utilize the IntentBench and PerspectBench datasets from CogDevelop2k to ex-plore intentionality understanding and perspective-taking in VLMs. On one hand, VLMs generally exhibit proficient performance on IntentBench, which indicates that a certain degree of intentionality understanding abilities as required for action understanding within the visual domain have likely emerged in VLMs (exemplified in Figure 2-3). On the other hand, VLMs' performance on Per-spectBench reveals that they are generally not capable of level-2 perspective-taking. Specifically, while they report to have considered the perspectives of the doll, they consistently fail at correctly inferring what can be seen from the doll's perspective (Figure 5). This most likely implies that they lack theory-of-mind in concrete, visual domains. This is distinct from theory-of-mind reasoning in abstract, verbal domains, which recent experiments suggested to exist in Large Language Models (LLMs) [Kosinski, 2023, Strachan et al., 2024]. This is, however perplexing from a developmental perspective, given the common understanding that concrete theory-of-mind abilities exemplified by level-2 perspective-taking is foundational to abstract theory-of-mind abilities demonstrated through verbal reasoning."}, {"title": "4 Discussions", "content": "The present study assesses VLMs' ability to intentionality understanding and perspective-taking. Our preliminary results show that VLMs appear to be proficient in intentionality understanding while, surprisingly, performing significantly worse in perspective-taking. On the high end of theory-of-mind hierarchy, intentionality understanding is suggested to be supported by the ability to ascribe abstract mental representations of others by reasoning from the objects' perspectives. Specifically for the tasks among IntentBench, such reasoning have to be operated on visual information, which supposedly requires level-2 perspective-taking. Our finding that the same VLM could perform competently on IntentBench but poor on PerspectBench challenges this common proposition. Here, we provide two potential interpretations for this surprising finding, taking into consideration the results of recent investigations into both perspective-taking and intentionality understanding in the literature as well as ongoing debates.\nThe first interpretation is that VLMs may be able to infer the intentions of the people in the images without attempting to take their perspectives [Kilner, 2004]. Specifically, understanding the actions behind intentions has been shown to be highly contingent upon the contextual information among the scenarios in which the actions take place. If VLMs could successfully comprehend and register the contextual cues among the images, they may be able to exploit the contingency between what is presented in the surroundings and the actions depicted using associative learning without even attempting to take the positions of the actors [Kilner, 2004, Bianco et al., 2024]. However, exploitation of associative learning without understanding the comparative significance and causal links between the contextual priors could lead to mistakes in action understanding, especially when there are several possible and salient ways to understand the actions depicted. This is exemplified by VLMs' failed performances on IntentBench, as shown in Figure 4.\nThe second interpretation concerns the possibility that Three Mountain Task requires certain abilities beyond what is demanded by intentionality understanding. Specifically, recent evidence indicates that Three Mountain Task requires the simultaneous confrontation of perspectives in visual reasoning, which is possibly more demanding than level-2 perspective-taking alone [Moll and Meltzoff, 2024]. In addition, it has also been suggested that cognitive processes independent of theory-of-mind might also contribute to the ability of visual perspective taking [Michelon and Zacks, 2006, Kessler and Rutherford, 2010, Samson et al., 2010, Epley and Caruso, 2012, Surtees et al., 2013]. The present study may be viewed as an empirical support toward these novel approaches to understand the relationship between visual perspective-taking and theory-of-mind.\nOverall, this in progress study is the first attempt at assessing VLMs' performance on intentionality understanding and level-2 perspective-taking. More contrasting experiments need to be done in order to dissociate the contributions of different pathways of action understanding and perspective-taking among VLMs."}]}