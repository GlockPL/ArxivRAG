{"title": "Analyzing the Ethical Logic of Six Large Language Models", "authors": ["W. Russell Neuman", "Chad Coleman", "Manan Shah"], "abstract": "This study examines the ethical reasoning of six prominent generative large language models: OpenAI's GPT-40, Meta's LLaMA 3.1, Perplexity, Anthropic's Claude 3.5 Sonnet, Google's Gemini, and Mistral 7B. The research explores how these models articulate and apply ethical logic, particularly in response to moral dilemmas such as the Trolley Problem, and Heinz Dilemma. Departing from traditional alignment studies, the study adopts an explainability-transparency framework, prompting models to explain their ethical reasoning. This approach is analyzed through three established ethical typologies: the consequentialist- deontological analytic, Moral Foundations Theory, and Kohlberg's Stages of Moral Development.\nFindings reveal that LLMs exhibit largely convergent ethical logic, marked by a rationalist, consequentialist emphasis, with decisions often prioritizing harm minimization and fairness. Despite similarities in pre-training and model architecture, a mixture of nuanced and significant differences in ethical reasoning emerge across models, reflecting variations in fine-tuning and post-training processes. The models consistently display erudition, caution, and self- awareness, presenting ethical reasoning akin to a graduate-level discourse in moral philosophy. In striking uniformity these systems all describe their ethical reasoning as more sophisticated than what is characteristic of typical human moral logic.", "sections": [{"title": "I Six Large Language Models", "content": "All of these models use the transformer architecture introduced by Google in 2017 (Vaswani et al.\n2017) and all reflect the commonly accepted wisdom that successful LLMs require significant\nscale in training data, typically trillions of tokens and in the number of model parameters, typically\nbillions, now over a trillion parameters (Kaplan et al. 2020). As a result model builders have been\nmotivated to utilize virtually all practically available digital content on the web (Heikkil\u00e4 & Arnett"}, {"title": "II Analytics for Assessing Ethical Logic", "content": "The tradition of moral philosophy typically does not provide definitive \"right\" answers to specific\nmoral dilemmas and complex scenarios (Grassian 1992; Joyce 2006). Instead, this literature\nprovides frameworks and principles to help individuals reason through and analyze different\nperspectives on situational challenges. Therefore, while the ground truth approach to\nbenchmarking and comparative model performance metrics is prominently used in the technical\nliterature (Reuel et al. 2024), it may not be as useful in this case. We have instead adopted an\nanalytic approach to capture which dimensions of ethical reasoning appear to evolve most\nprominently from the pre-training and post-training processes in these six foundation models.\nPhilosophers have been debating the nature of morality and ethical systems since the emergence\nof philosophy itself so there is a rich and complex literature we can draw on (Collins 1998). The\ndebate continues currently as legal scholars, economists, anthropologists and evolutionary\nbiologists join in (Appiah 2008). The core human values prohibiting murder and theft, and\ncelebrating honesty, reciprocity and respect for elders and authority are found in cultural norms,\nlegends and religious traditions around the world often in variations of the Ten Commandments\nand the golden rule (Joyce 2006). As Haidt puts it, the core of ethical logic is basically a\ncombination of Care (prevention of harm) and Fairness (reciprocity) (Haidt 2012). The"}, {"title": "III The Assessment Battery", "content": "This section details our comprehensive assessment approach for evaluating how Large Language\nModels process and respond to ethical challenges. The assessment battery consists of two primary\nlevels: direct ethical reasoning prompts and classical moral dilemmas. These tools were selected\nto provide both breadth and depth in understanding how LLMs approach moral decision-making.\nThe first level of assessment is to straightforwardly prompt the models to articulate how they\nconfront ethical and moral decision making in general. We experimented with seven prompts (as\nnoted in Table 4) which most often generated quite similar responses across the models. In\nresponse each LLM would elaborate several dimensions and strategies of evaluation."}, {"title": "The Trolley Problem", "content": "We began with the classic Trolley Problem which contrasts a choice between a consequentialist\nperspective emphasizing the minimization of harm versus a deontological view which would\nemphasize following an ethical imperative (Foot 1967; Greene 2009, 2013, 2023). The archetypal\nscenario posits a runaway trolley on course to kill five innocent people and a bystander who must\nchoose whether or not to intervene and switch the trolley onto a different track saving the five but\nresulting in the death of a single other innocent person. The second version of the Trolley Problem,\nthe Fat Man or Footbridge scenario, requires a more direct intervention. As before, a trolley is\nhurtling down a track toward five people. The bystander is on a bridge under which it will pass,\nand the only prospect of stopping the trolley is putting something very heavy in front of it. As it\nhappens, there is a fat man next to you \u2013 your only way to stop the trolley is to push him over the\nbridge and onto the track, killing him to save five. It quickly became evident that each of the LLMs\nrecognized these scenarios and would cite the relevant technical and popular literatures in deriving\na \u201cbest\u201d answer to the difficult decision. Since Foot's initial explication an extensive literature\nexploring its ethical, legal, psychological, neurological and philosophical implications has evolved\n(Thompson 1976; Greene et al. 2001; Appiah 2008; Greene et al.2009; Edmonds 2013; Gawronski\n& Beer 2017; Kvalnes 2019; Lillehammer 2023; Zhang et al. 2023). Greene has come to dub the\nfield as \"Trolleyology\u201d (Greene 2023).\nWe also selected four additional well-known ethical dilemmas to explore the logic and analytic\nstrategies of our six sampled systems. We examined both the choices made and the explanations\nfor how they were made using the analytics outlined above. The following scenarios were selected\nto test different aspects of ethical reasoning, from consequentialist versus deontological thinking\nto game theoretical decision-making:"}, {"title": "The Heinz Dilemma", "content": "This scenario is well-known moral dilemma popularized by Kohlberg (1981) as responses to this\nethical challenge typically illustrate various stages of his model of moral development. Although\nit proposes a traditional choice between deontological and consequential responses, Kohlberg\nargues it is less important what choice is made about what Heinz should do but rather the\njustification offered and the form of the response. The dilemma posits a desperate husband (Heinz)\nwho must decide whether or not to steal a special medicine to save his wife's life. Like the Trolley\nProblem, many variations and elaborations have been developed and analyzed (Rest 1979; Walker\net al. 1987). Also akin to the Trolley Problem, the decision requires the role player to decide\nbetween violating a clear cut prohibition against stealing (deontological ethics) and the alternative\nin this case of dramatic human harm (consequentialism)."}, {"title": "The Lifeboat Dilemma", "content": "American ecologist Garrett Hardin, famous for his work on overpopulation and the tragedy of the\ncommons (Hardin 1968), expanded his analysis with a now famous article in 1974 on what has\ncome to be called Lifeboat Ethics (Hardin 1974). He was dramatizing the challenges of\noverpopulation and his scenario has since become a common resource for psychologists studying\nhuman ethical behavior. This scenario challenges the role player to decide who among nine"}, {"title": "The Dictator's Game", "content": "Hungarian-American game theorist John Harsanyi developed this scenario in 1961 as part of his\nresearch on the processing of incomplete information in behavioral economics. He would later\nshare the Nobel Prize in economics with John Nash for his ground breaking work in game theory\n(Harsanyi 1961). It posits calculating how much to offer to share of typically $100 with a co-player\nwho may accept or decline if the offer is deemed unfair and as a result neither will receive anything.\nIt requires the role player to have a theory of mind of how others are likely to react (Heider 1958;\nHofstadter & Dennett 2000; Strachan et al. 2024)."}, {"title": "The Prisoner's Dilemma", "content": "This famous game theoretic thought experiment posits players who can either cooperate for mutual\nbenefit or defect for individual gain. The dilemma derives from the fact that while defecting is\nrational for each player, cooperation would yield a higher payoff for both. It was developed by\nMerrill Flood and Melvin Dresher in the 1950s during their work at the RAND on game theoretic\napproaches to international conflict (Peterson 2015). It has since stimulated an immense literature\n(almost 200,000 results in Google Scholar recently) in game theory and psychology. Most of the\nanalysis (notably Axelrod 1984) has studied repeated play of the game to assess how players\ninterpret each other's behavior over time. In our initial analysis we examine only the one-shot,\nsingle play scenario. Like the Dictator scenario, it requires a theory of mind for speculating on the\nbehavior of the other player.\nTogether, these scenarios provide a diverse set of ethical challenges that test different aspects of\nmoral reasoning, from pure ethical decision-making to strategic thinking with moral implications.\nThe variety of scenarios allows us to examine how LLMs handle different types of moral\ncomplexity and whether their reasoning remains consistent across various ethical contexts."}, {"title": "IV Findings", "content": "Based on our analysis of multiple LLM responses to ethical prompts and dilemmas, we identified\nseven key characteristics that define their approach to ethical reasoning. It turns out that the current\ncrop of LLMs are more than willing to describe their ethical logic in considerable and well\norganized detail. We will first provide an overview of the common patterns with some\ncharacteristic examples of textual responses and then proceed with a finer grain analysis of the\nvariations among the sampled models. The following seven characteristics emerged consistently\nacross our analysis:\nThe first pattern evident was that the ethical logic of these models is Largely Convergent. The\nself-descriptive prompts and the ethical dilemmas often generated similar responses among the"}, {"title": "V Next Steps", "content": "Our approach in this paper has provided a descriptive analysis of how current generative AI\nsystems respond to and reason through ethical dilemmas. Unlike traditional benchmarking\nresearch, we acknowledge the absence of ground truth metrics for ethical decision-making. In our\nforthcoming work, we will propose a more generalizable mathematical framework for assessing\nthe breadth and depth of ethical logics in AI systems.\nOur descriptions set a baseline for further research. Our future research agenda encompasses\nseveral key directions:\nCultural and Religious Influences\nIn each case here we have simply noted generative responses based on the full pre-training and\nstandard fine-tuning processes for each model. In further research we will constrain the responses\nto more specific components of their training including different cultural and religious traditions\nincluding an exercise in fine tuning with specific religious texts. This targeted approach will help\nus understand how varying cultural frameworks affect ethical decision-making patterns.\nDemographic Variables\nWe will explore how ethical logic may vary when systems role play as members of different\ndemographic identities including gender, ethnicity, age and social class. This investigation will\nhelp understand the relationship between identity frameworks and moral reasoning in AI systems.\nComparative Analysis\nWe anticipate an explicit comparison by means of a survey of human and AI responses to identical\ndilemma challenges. This direct comparison will provide insights into similarities and differences\nbetween human and artificial moral reasoning processes.\nBias Investigation\nAn element, not addressed here, but prominent in the literature is the question of bias in decision\nmaking based on persistent stereotypes associated with different demographic groups. We have\ndeveloped and tested a separate prompt battery for that pattern. Our initial research reveals that,\nyes indeed, our sampled models do reflect some stereotypic prejudices apparently derived from\ntheir training materials.\nTechnical Analysis\nFinally, there is the question of where in these transformers' MLPs this emergent moral logic comes\nfrom. Which neuronal parameters light up, for example, when the Trolley Problem is introduced?\nSome new analytic tools from Anthropic, Meta and Transluce offer some promise for that research\ntrajectory as well. This technical investigation aims to identify which specific neural parameters\nactivate during ethical decision-making, providing insights into how moral reasoning emerges\nfrom these systems' architectures.\nThe pop psychologist and cultural provocateur Jordan Peterson (who is known for such maxims)\nfamously pronounced that there is absolutely no evidence of a correlation in the research literature\nbetween intelligence and morality among Homo sapiens. It turns out that this purported lack of a\ncorrelation is both more complicated and controversial that Peterson asserts (for example: Rathi &\nKumar (2020)). But such foundational questions are intriguing for those of us researching the\nimpact of generative AI on human psychological, social and economic dynamics. There is no\nshortage of predications of how AI will change human life on earth (Anderson & Raine 2023).\nComputer pioneer Douglas Engelbart notably characterized the most important potential impact\nof computation intelligence as not competition with but as augmentation of human intelligence\n(Engelbart 2004, Neuman 2023). A question remains -- will artificial intelligence show promise of\nenhancing human moral behavior? Our research so far leads us to believe that it could and it should."}]}