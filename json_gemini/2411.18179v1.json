{"title": "Prediction with Action:\nVisual Policy Learning via Joint Denoising Process", "authors": ["Yanjiang Guo", "Yucheng Hu", "Jianke Zhang", "Yen-Jen Wang", "Xiaoyu Chen", "Chaochao Lu", "Jianyu Chen"], "abstract": "Diffusion models have demonstrated remarkable capabilities in image generation\ntasks, including image editing and video creation, representing a good understand-\ning of the physical world. On the other line, diffusion models have also shown\npromise in robotic control tasks by denoising actions, known as diffusion policy.\nAlthough the diffusion generative model and diffusion policy exhibit distinct capa-\nbilities-image prediction and robotic action, respectively\u2014they technically follow\na similar denoising process. In robotic tasks, the ability to predict future images and\ngenerate actions is highly correlated since they share the same underlying dynamics\nof the physical world. Building on this insight, we introduce PAD, a novel visual\npolicy learning framework that unifies image Prediction and robot Action within a\njoint Denoising process. Specifically, PAD utilizes Diffusion Transformers (DiT)\nto seamlessly integrate images and robot states, enabling the simultaneous predic-\ntion of future images and robot actions. Additionally, PAD supports co-training\non both robotic demonstrations and large-scale video datasets and can be easily\nextended to other robotic modalities, such as depth images. PAD outperforms\nprevious methods, achieving a significant 26.3% relative improvement on the full\nMetaworld benchmark, by utilizing a single text-conditioned visual policy within\na data-efficient imitation learning setting. Furthermore, PAD demonstrates supe-\nrior generalization to unseen tasks in real-world robot manipulation settings with\n28.0% success rate increase compared to the strongest baseline.", "sections": [{"title": "1 Introduction", "content": "Making predictions and taking actions are critical human capabilities, allowing individuals to foresee\nthe change of their surroundings and behave appropriately in response [1, 2]. Despite prediction\nand action seeming like two distinct abilities, they are highly coupled since they share the same"}, {"title": "2 Preliminaries", "content": "Problem Statement. We consider pixel-input language-conditioned robotic control under the\nimitation learning setting. We denote a robotic dataset $D_{robot} = {\\tau_1,\\tau_2,...\\tau_n}$ comprising n\ndemonstrations. The ith demonstration $\\tau_i = (I_i, l_i, T_i)$ contains a natural language instruction $l_i, a"}, {"title": "3 PAD: Prediction with Action via Joint Denoising Process", "content": "3.1 Overview of PAD\n\nMulti-modalities Generation. In this section, we introduce our PAD framework, which concurrently\npredicts future frames and actions within a joint latent denoising process. We primarily focus on the\nRGB image modality $M_I$ and the robot action modality $M_A$. Each robot action can be characterized\nby a robot pose that includes the position and rotation of the end-effector, as well as the gripper status.\nNotably, this framework can easily extend to extra modalities $M_E$. For instance, we additionally\nincorporate the depth image modality in the experiment part, which provides a more accurate measure\nof distances.\n\nConditional Generation. In the proposed PAD framework, predictions and actions are conditioned\non multi-modality current observations, which include RGB images $c_I$, robot pose $c_A$, an additional\ndepth map $c_E$ (in Real-World tasks), and natural language instruction text $l$. The framework simul-\ntaneously outputs the corresponding future predictions $X_I, X_E$ and robot action $X_A$. Rather than\npredicting a single future step, PAD can forecast k future steps $x_I^{1:k}, x_E^{1:k}, x_A^{1:k}$, which can be viewed\nas k step planning of the robot. Only the first predicted action $x_A^1$ is executed by the robot, which then\ntriggers a new prediction cycle. This iterative prediction and execution process allows the robot to\ncontinuously plan and act in a closed-loop manner. The implementation details are discussed further\nin the subsequent section.\n\n3.2 Model Architectures\n\nModel Input Process. Given that the original data may come in various formats with high dimensions,\nwe first map all modalities to a latent space and undertake a latent diffusion process. Following\nthe process in [20], the RGB image $x_I$ is initially processed through a pre-trained, frozen VAE[23]\nencoder $\\varepsilon_I$ to derive the latent representation $\\varepsilon_I(x_I)$. This latent representation is then converted into\na sequence of tokens $t_I$ with embedding size h via tokenizer. Similarly, the robot pose $x_A$ is encoded\nusing a Multi-Layer Perceptron (MLP) [24] into $\\varepsilon_A(x_A)$ and linearly transformed into tokens $t_a$\nwith the same embedding size h. If available, the depth image is downsampled and tokenized into $t_E$.\nThe natural language instruction is processed through a frozen CLIP encoder [25] to produce the text\nembedding $c_T$."}, {"title": "3.3 Training Process", "content": "Initialization. Following the initialization process in [15], we also initialize the PAD weights from\nthe DiT model pre-trained on ImageNet for the image generation task conditioned on class [20].\nHowever, we can not directly load the model since we have missing or incompatible model parameters.\nWe discard the label embedding layers in DiT and zero-initialize new layers for text embedding, we\nreplicate the weight of the image latent tokenizer for k + 1 times to encode the stacked latent, and the\nencoder and decoder for robot state are also zero-initialized."}, {"title": "4 Experiments", "content": "In this section, we conduct a series of experiments on the simulated Metaworld Benchmark [21] and\na real-world table manipulation suite, utilizing our joint prediction framework. We aim to answer the\nfollowing questions:\n\n\u2022 Can PAD enhance visual policy learning through joint prediction and action with limited\nrobotic data?\n\n\u2022 Can PAD benefit from co-training on large-scale internet video datasets and better generalize\nto unseen tasks?\n\n\u2022 Can scaling up computational resources improve PAD's performance?\n\n4.1 Environmental Setups and Baselines\n\nMetaworld. Metaworld [21] serves as a widely used benchmark for robotic manipulation, accommo-\ndating both low-level feature and pixel input modalities. Previous studies that utilized pixel input\ngenerally developed separate task-specific policies for each of the 50 tasks. In contrast, our approach\ndemonstrates a significant advancement by employing a single text-conditioned visual policy to\naddress all 50 tasks, within a data-efficient imitation learning framework. We collected 50 trajectories\nper task, consistently using the \u201ccorner2\u201d camera viewpoint and recording the robot's pose with\n4-dimensional states that include end-effector position and gripper status. For a fair comparison, we\ndo not utilize an additional depth input in Metaworld.\n\nReal-World Panda Manipulation Tasks. Our real-world experiments involve a Panda arm per-\nforming diverse manipulation tasks such as pressing buttons, opening drawers, routing cables, and\npicking and placing with various objects. We follow the same hardware setup\ndescribed in SERL [27] and utilize a wrist-mounted camera for pixel input [28]. The robot's poses\nare represented by 7-dimensional vectors, including 3 end-effector positions, 3 rotation angles, and 1"}, {"title": "4.2 Main Results", "content": "Performance Analysis. In all comparisons, we train a single visual policy to address all tasks within\na domain, conditioned on instructions. Our proposed PAD outperforms all baselines by a significant\nmargin. As shown in, in the Metaworld benchmark, PAD achieved an average success rate\nof 72.5%, which markedly surpasses the strongest baseline at 57.4%. Due to space constraints, we\npresent comparisons on a subset of tasks and report the average success rate across all 50 tasks. A\ncomprehensive comparison of all 50 tasks is available in Appendix A.4. Furthermore,  shows\nthe results in real-world seen-tasks where PAD also attains the highest success rate.\n\nWe notice that PAD predicts more precise future images than the GR-1 method likely\ndue to the superior capabilities of diffusion models in image generation tasks. These precise images\nmay more effectively facilitate policy learning, leading to higher success rates, particularly in tasks\nrequiring precise and accurate operation such as picking small blocks, insertion, basketball, etc., in\nMetaworld."}, {"title": "4.3 Generalization Analysis", "content": "PAD can leverage existing physical knowledge from co-training on large-scale internet video datasets\nto enhance its generalization capabilities across new tasks. We evaluated PAD's generalization ability\nin real-world panda manipulation with unseen tasks. As depicted in , the expert dataset\ncomprises only colored square blocks and plates, while we introduce a variety of previously unseen\nfruit and vegetable toys during testing. We designed tasks of three difficulty levels: easy mode,\nfeaturing 1-4 disturbance objects; a middle level with 5-15 disturbance objects; and difficult tasks that\nrequire picking previously unseen objects with 5-15 disturbances or unseen backgrounds. We excluded\ndepth input to ensure a fair comparison. As illustrated in , PAD demonstrates remarkable\ngeneralization abilities, successfully managing out-of-distribution objects such as strawberries, carrots,\nand eggplants, and even adapting to new backgrounds. The baseline method failed to generalize to\ndifficult unseen tasks."}, {"title": "4.4 Ablation Studies", "content": "Effectiveness of RGB image prediction. We evaluated the effectiveness of our joint prediction\nprocess by modifying the original model to exclude the image prediction component, namely in PAD\nw/o image prediction. This modification leads to significant performance drops compared to PAD,\nas illustrated in Table 1. The absence of image prediction compromises the robot's ability to utilize\nthe physical knowledge encoded in the image modalities, which may be crucial for robotic control."}, {"title": "4.5 Scaling Analysis", "content": "We evaluated models across various sizes and patchify sizes [20], as outlined in Table 3. For example,\nthe XL/2 model denotes the model with an XL size and a 2 \u00d7 2 patchify size. Halving the image\npatch size will quadruple the image token lengths, which leads to higher computational costs. Our\nfindings reveal a strong correlation between computational allocation (measured as transformer\nGflops) and the success rate (SR) of the learned policy, as depicted in Figure 10. All the experiments\nare run in Metaworld benchmarks and detailed success rates for each task are provided in Appendix\nA.5."}, {"title": "5 Related Work", "content": "Pre-training for Embodied Control. Vision-language pre-trained models, encoded with physical\nknowledge, can enhance embodied control from multiple aspects. Primarily, the pre-trained model\ncan directly act as policy by either generating high-level plans [37\u201341] or producing direct low-level"}, {"title": "6 Conclusion and Discussion", "content": "We present PAD, a novel framework to predict future images and generate actions under a joint\ndenoising process. Moreover, PAD can co-train with internet video datasets and extend to other\nrobotic modalities. Both simulated and real-world experiments demonstrated the efficiency of PAD.\n\nA limitation of the current method is that we only tested with three types of modalities. Subsequent\nendeavors could extend this framework to incorporate additional robot-related input data, such as\ntactile information, which we believe are valuable research directions. Another limitation is that\nthe control frequency of PAD is not very high since we need to jointly denoise the images and\nactions. Future work can explore efficient ways to leverage image predictions, such as utilizing the\nintermediate latent space of predicted images rather than the high-dimensional pixel spaces."}, {"title": "A Appendix", "content": "Videos of PAD can be found at https://sites.google.com/view/pad-paper.\n\nA.1 Additional Implementation Details of PAD\n\nA.1.1 Input Encoder and Output Decoders\n\nImage Encoder and Tokenizer. The image encoder is a frozen VAE encoder same as [20]. Take\nPAD-XL/2 model for example, the encoded latent space for 256 \u00d7 256 image is 32 \u00d7 32 \u00d7 4, and\npatchify into (32/2) * (32/2) = 256 patches, which then are tokenized into 256 tokens.\n\nRobot action Encoder and Tokenizer. The robot action is concatenated into a vector and passed into\nMLP layers, we predict k steps of the future each with 7-dimensional poses, which totally consists\n(k + 1) * 7 dimensional vectors ((k + 1) * 4 in Metaworld), and then this vector is tokenized into 1\ntoken.\n\nDepth image Encoder and Tokenizer (If presented). We directly down-sample the depth image to\na size of 32 * 32 * 1, and follow the same patchfy process as the RGB image. The patch size is set to\n8. The patchfy for depth image resulted in (32/8) * (32/8) = 16 patches, which then are tokenized\ninto 16 tokens.\n\nOutput Decoder. The decoder part mainly inverses the encoder part. The decoder process first\nreconstructs the future latent from the token output by DiT, then adopts the corresponding decoder to\nrecover the original samples in each modality.\n\nA.2 Additional Implementation Details of Baselines\n\nThe RT-1 baseline is based on official implementation https://github.com/\ngoogle-research/robotics_transformer.\n\nThe Diffusion Policy baseline is based on https://github.com/real-stanford/diffusion_\npolicy, and we follow https://github.com/real-stanford/scalingup to add language\ncondition.\n\nThe RT-2 baseline is re-implemented by ourselves. We use the InstructBlip-vicuna-7b model as\nbackbone https://huggingface.co/Salesforce/instructblip-vicuna-7b.\n\nThe GR-1 baseline is built on https://github.com/bytedance/GR-1. Since we can not access\nthe processed pretraining dataset in the original paper, we initialize the model with the author's\nopen-source checkpoint. The author provide the inference code and we write the GR-1 training code\nby ourselves based on the inference model.\n\nA.2.1 Additional Model Training Details"}, {"title": "A.3 Real world Experiment Details", "content": "Expert data collection. We collected data on 6 categories of tasks, including button press, cable-\nroute, pick and place, and drawer open/close. For the pick task, we include 4 colors of blocks. For the\nplace task, we include 3 colors of plate. We randomly placed 1-5 objects on the table and asked the\nrobot to pick/place certain objects conditioned on instruction. Some samples of expert demonstrations\nare visualized in Figure 11.\n\nGeneralization Test Task Samples. We test the generalization ability of learned policy under\nnumerous unseen objects. The unseen task is much more complicated than expert tasks, as shown\nin Figure 12. For convenience, videos of PAD can be found at https://sites.google.com/\nview/pad-paper."}, {"title": "A.6 Instructions used in tasks", "content": "Instructions used in tasks"}]}