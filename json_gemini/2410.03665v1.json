{"title": "Estimating Body and Hand Motion in an Ego-sensed World", "authors": ["Brent Yi", "Vickie Ye", "Maya Zheng", "Lea M\u00fcller", "Georgios Pavlakos", "Yi Ma", "Jitendra Malik", "Angjoo Kanazawa"], "abstract": "We present EgoAllo, a system for human motion estimation from a head-mounted device. Using only egocentric SLAM poses and images, EgoAllo guides sampling from a conditional diffusion model to estimate 3D body pose, height, and hand parameters that capture a device wearer's actions in the allocentric coordinate frame of the scene. To achieve this, our key insight is in representation: we propose spatial and temporal invariance criteria for improving model performance, from which we derive a head motion conditioning parameterization that improves estimation by up to 18%. We also show how the bodies estimated by our system can improve the hands: the resulting kinematic and temporal constraints can reduce errors in noisy monocular estimates by 40%. Project page:\nhttps://egoallo.github.io", "sections": [{"title": "1. Introduction", "content": "Head-mounted devices are permeating the mainstream market, with applications in areas like virtual reality, content creation, and assistive technologies. For research in machine perception, these devices also present exciting opportunities in the form of egocentric sensing. Sensors from wearable devices provide abundant observations of 3D environments, while capturing the embodied perspective of human agents as they observe, navigate, and interact with the world around them.\nWhat can we understand from these devices as their adoption widens? As a starting point, parallax-inducing egomotion provides excellent conditions for advances in 3D reconstruction and scene understanding [18, 66, 108]. Limiting perception to only the surrounding world, however, would neglect a crucial piece of the ego-sensory puzzle: the individual whose decisions shape the inputs. Capturing the wearer's actions and motion in addition to the scene promises to unlock applications across augmented and virtual reality, robotics, and general human behavior analysis, while unveiling action-tied semantics in the scene itself.\nWe therefore introduce EgoAllo, a system that uses egocentric inputs to estimate the wearer's actions in the allocentric world. This is a difficult task: while body parts like hands occasionally appear in egocentric image frames, most body parameters are never directly observed by these devices. Accurate estimates in the global frame also require harmony between both pose and height parameters. These must be consistent with both the egomotion and scene scale, aligning the feet to the ground and the head to the sensed height of the camera.\nWe cast estimation as guided sampling from a diffusion-based motion model. We consider two egocentric inputs: a proprioceptive one in the form of head pose, which we use for conditioning, and images, which we use for visual hand observations that are incorporated via guidance. We then estimate body pose, height, and hand parameters, unlike most prior works in egocentric human motion estimation [6, 47] that focus on body pose.\nWe achieve this by training a head motion-conditioned diffusion model as a motion prior, and guiding its sampling to recover a hand-body sequence that aligns with image observations. Our results are enabled by a key insight: the head observation conditioning representation is critical for accurate ego-sensed pose estimation. We study the role of choices for this representation by (1) identifying desirable spatial and temporal invariance properties that are not fulfilled by existing systems, (2) using these properties to derive improved parameterizations for our motion prior, and (3) presenting a systematic evaluation that shows between 4.9% and 17.9% error difference when compared against prior work. Furthermore, we show how the resulting system can improve hand estimation, reducing errors by over 40% compared to noisy monocular estimates."}, {"title": "2. Related Work", "content": "3D human recovery from external visual inputs. A large body of work has addressed estimating the parameters of human body models like SCAPE [2] or SMPL and its variants [51, 61, 77] from third-person visual inputs, where human subjects are observed from the view of outside cameras. The majority of these works focus on extracting 3D representations from single images, for example by lifting 2D keypoint observations to 3D [56], via end-to-end regression [20, 31, 33, 39, 58, 60, 75], via optimization [19, 43, 61], or by exploiting synergies between regression and optimization [41]. When multiple frames are available in the form of a video, temporal context and tracking can also be incorporated [16, 34, 38, 62, 63, 69, 106]. The inputs (images) and outputs (human meshes) of many of these systems are superficially similar to the egocentric setting addressed by EgoAllo, but egocentric devices present unique challenges because the body being estimated is typically behind the outwards-facing cameras used as input.\nPriors for human motion. The primary challenge of ego-sensed human motion estimation is limited observability; a prior is required to resolve ambiguities. For human motion, these priors are typically framed as unconditional distributions over plausible human motions. These distributions can be represented either by modeling the physical constraints of our world [5, 49, 65, 71] or by learning generative models of human motion directly from data. For learning unconditional priors, classical data-driven approaches include fitting mixtures-of-Gaussians to 3D keypoint trajectories [25], while modern approaches include training variational autoencoders [37, 73] to model either autoregressive transitions [15, 50, 72] or full spatiotemporal sequences [22]. After training, these priors can be applied to estimation problems in iterative optimization frameworks [40, 72, 99]. EgoAllo is built on the same intuition as these methods, but follows previous work in ego-sensed motion estimation and uses a task-specific conditional prior.\nDenoising diffusion for human motion. The core of EgoAllo is a denoising diffusion model [23, 67, 82] from which we can sample 4D human body motion. While diffusion models are primarily known for their success in text-conditioned image generation [76, 78], they have also enabled advances in human motion synthesis conditioned on modalities like text [35, 36, 109], music [1, 91], poses [35, 47], and object geometry [42, 46, 48]. EgoAllo adopts a similar conditional diffusion approach, while specifically studying the design of conditioning parameters used for ego-sensed human motion estimation. The iterative nature of denoising diffusion also enables guidance [11, 13, 30, 35, 85, 111], where denoising steps are steered to satisfy a desired objective. We use guidance to incorporate observations like visual hand pose observations during test-time.\nHuman motion from egocentric observations. EgoAllo builds on intuition from several prior works in egocentric sensing for human motion estimation. Many rely on fisheye cameras that place the wearer's body into the field of view [27, 74, 89, 89, 90, 93, 94, 96]. Other approaches rely on body-mounted cameras [81], simulation-based physical plausibility [53, 104, 105], body- and hand-mounted inertial sensors [45, 102, 103], handheld controllers [6, 28, 29], and interaction cues from other humans [57]. Concurrent works have also used the Nymeria [54] dataset for egocentric motion with language description outputs [24], as well for online settings with scene geometry and CLIP [68] feature inputs [21]. Most relevantly, EgoEgo [47] demonstrates how human body poses can be estimated offline without body observability assumptions. The authors accomplish this by carefully integrating several components: a monocular SLAM system [87], a pose-conditioned gravity vector regression network, an optical flow feature-conditioned head orientation and scale regression network, and a head pose-conditioned body diffusion model. For estimates that can be more easily grounded in the allocentric coordinate frame, EgoAllo differs in both inputs-we study conditioning parameters computed from the metric SLAM poses provided by devices like Project Aria [83]-and outputs-we consider body height variation and hand poses.\nConditioning for ego-sensed poses. Prior works vary in how head pose information is parameterized and used as neural network input. AvatarPoser [28] and BoDiffusion [6] parameterize head pose as four components: world-frame orientation, orientation deltas, world-frame position, and world-frame position deltas. These works are focused on"}, {"title": "3. Method", "content": "We study the problem of using sensors from an egocentric device to estimate the actions of a wearer in an allocentric coordinate frame. We assume a flat floor and two egocentric inputs-poses from the device's SLAM system and camera images.\nOur system uses head pose information to condition a diffusion-based prior over body pose and height, and incorporates visual hand observations during sampling. This allows it to benefit from both 3D human motion capture datasets [55], which are used for the motion prior, and from large-scale image datasets [64], which are used for hand estimates."}, {"title": "3.1. Ego-conditioned motion diffusion", "content": "Notation: we use \\(T_{A,B} = (R_{A,B}, p_{A,B})\\) to denote an SE(3) transform to frame A from frame B, composed of rotation (\\(R_{A,B}\\)) and position (\\(p_{A,B}\\)) terms. Temporal steps t are superscripted and diffusion noise steps n are subscripted. \\(Z_t\\) thus refers to the t-th timestep of a clean (n=0) human motion sequence.\nGiven an observation window of T timesteps, EgoAllo's motion prior is a diffusion model that aims to capture the distribution of human motions \\(Z_0 = \\{z_0^1,...,z_0^T\\}\\) conditioned on head pose encodings \\(C = \\{c^1,...,c^T\\}\\). For each timestep t, we represent human motion in the form of SMPL-H [51, 77] model parameters \\(\\{T_{world, root}^t, \\Theta^t, \\beta\\}\\): \\(T_{world,root}^t \\in SE(3)\\), where the person's root frame is located at their pelvis, axis-angle local joint rotations \\(\\Theta^t \\in \\mathbb{R}^{51 \\times 3}\\), and time-invariant shape \\(\\beta \\in \\mathbb{R}^{16}\\).\nDependencies between local joint rotations, body size variation, and global motion make this learning task a challenging one. Our key insight is that this difficulty can be reduced by designing parameterizations with desirable invariance properties. Spatial and temporal invariances allow the model to focus on the essential structure of motion, without being affected by irrelevant shifts in position or time."}, {"title": "3.1.1 Diffusion output representation", "content": "As output, we sample joint rotations, body shapes, and binary contact predictions \\(x = \\{\\Theta^t, \\beta, \\psi_{i=1...21}^t\\}\\), where body shape \\(\\beta\\) is supervised to be equal for all timesteps and \\(\\psi^t\\) is a per-joint contact indicator. Notably, these parameters are all local-we discuss how outputs can be placed into the allocentric coordinate frame in Section 3.2.1."}, {"title": "3.1.2 Invariant conditioning", "content": "The goal of our conditioning representation is to map raw SLAM poses (head motion) to a parameterization that is amenable to learning for the diffusion model.\nRaw inputs. To capture the head motion at each time step, we assume as input poses of a central pupil frame (CPF), which the SLAM systems of devices like Project Aria can provide with millimeter-level accuracy [83]. For time 1...T, we reparameterize these poses for conditioning using a function g:\n\n\\(T_{world, cpf}^t = (R_{world, cpf}^t, p_{world,cpf}^t) \\in SE(3),\\) \\(\\{c^1,...,c^T\\} = g(\\{T_{world,cpf}^1,...,T_{world,cpf}^T\\}).\\)\n\nThe CPF frame differs from prior works that condition on a coordinate frame attached to the SMPL human model's \"head joint\" [6, 28, 29, 47]. The offset between this head joint and the device pose depends on the head shape captured by \\(\\beta^t\\), and is thus difficult to precompute in our setting.\nTo encode absolute height, we assume that the world frame's +z-axis faces upwards, and that the ground is located at z=0. Ground parameters are directly available in the training data [55]; at test time, we can also extract these parameters from sparse SLAM points via RANSAC (Appendix A.1).\nInvariance goals. As discussed in Section 2, prior work varies in how the function g is implemented. To understand how they impact the learning problem, we propose two invariance properties for head motion representations. Each reduces representational redundancy, which eases the learning problem.\nInvariance 1 (Spatial) Global transformations along the floor plane should not affect a person's local motion. Given \\(T_{xy} \\in SE(3)\\) restricted to the XY plane, g should fulfill \\(g(\\{T_{xy}T_{world,cpf}^t\\}) = g(\\{T_{world,cpf}^t\\}) \\forall T_{xy}\\).\nInvariance 2 (Temporal) Head motion representations for a given body motion should be independent of location within a temporal window. This can be expressed as temporal shift equivariance. Let \\(c_t\\) be as defined in Equation 2. If we shift inputs by \\(\\delta\\) such that \\(c_{\\text{shift}} = g(\\{T_{world,cpf}^{t+\\delta}\\})\\), then g should satisfy \\(c_{\\text{shift}} = c_{t+\\delta}\\) for overlapping timesteps."}, {"title": "Invariant conditioning.", "content": "We propose a formulation for g that achieves both invariance properties by locally canonicalizing head motion with respect to the floor at each timestep. We build on the relative motion of the CPF frame at each time t, which respects both Invariance 1 and 2:\n\n\\(\\Delta T_{t-1, t} = (T_{world,cpf}^{t-1})^{-1} T_{world,cpf}^t\\)\n\nImportantly, the translation component of this transformation is in the local frame. This is distinct from world-frame position deltas [6, 28, 29], which still violate Invariance 1.\nRelative transforms alone, however, do not encode information relative to the scene or floor: full trajectories can even be flipped upside down without impacting \\(\\Delta T_{t-1, t}\\). We therefore propose to ground relative motion to the floor plane with a transformation between the CPF frame and a per-timestep canonical frame, which is computed by projecting the CPF frame to the floor. This encodes head height and orientation:\n\n\\(c_t = \\{\\Delta T_{t-1, t}^{cpf}, (T_{world, canonical}^{t-1})^{-1} T_{world, cpf}^t\\}.\\)\nInvariant implementation of g(.)"}, {"title": "3.2. Estimation via sampling", "content": "We use our local body representation and invariant conditioning strategies to train a motion prior in the form a denoising diffusion model [23]. Given diffusion step n = N... 1, we follow [70] and approximate the denoising process as:\n\n\\(p_{\\Theta}(z_{n-1} | z_n, C) = \\mathcal{N}(\\mu_{\\Theta}(z_n, n, C), \\sigma^2 I),\\)\n\nwhere a transformer [92] \\(\\mu_{\\Theta}\\) is trained to predict the posterior mean from \\(z_n\\) and conditioning C. With noise-dependent weight term \\(w_n\\), the loss can be written as:\n\n\\(\\min_{\\Theta} \\mathbb{E}_{z_0 \\sim q_0} \\mathbb{E}_{n \\sim U, z_n \\sim q_n | z_0} [w_n ||\\mu_{\\Theta}(z_n, n, C) - z_0||^2].\\)\n\nAfter training, we estimate human motions by following DDIM [84] for conditional sampling. The final EgoAllo sampling procedure includes several additional components: a global alignment phase, guidance losses for physical constraints and visual hand observations, and a path fusion [3] approach for longer sequence lengths. We describe these below."}, {"title": "3.2.1 Global alignment", "content": "To place sampled bodies into the allocentric coordinate system, we compute the absolute pose of the SMPL-H root as:\n\n\\(T_{world,root}^t = T_{world,cpf}^t T_{cpf,root}^\\Theta(\\Theta, \\beta).\\)\n\nwhere \\(T_{cpf,root}^\\Theta(\\Theta, \\beta)\\) computes the transform between the root of the human and their CPF frame for a given set of local pose and shape parameters. Similar processes are applied in [6, 28, 29]. In contrast to directly outputting absolute body transformations from the diffusion model [47], this guarantees exact alignment between estimates and the input SLAM sequences."}, {"title": "3.2.2 Guidance losses", "content": "Our diffusion model learns a distribution of human motion conditioned on the central pupil frame motion. At test time, we incorporate constraints from physical priors and visual hand observations via guidance [11, 30, 111]. Similar to [35, 46], we accomplish this by applying losses to the joint rotations \\(\\Theta= \\{\\Theta^1,...,\\Theta^T\\}\\) predicted by \\(\\mu_{\\Theta}(z_n, n, C)\\). We treat the body shape \\(\\beta\\) and contacts \\(\\psi^t_{i=1...21}\\) as fixed and optimize over body and finger pose to minimize hand observation, skating, and prior costs with a Levenberg-Marquardt optimizer:\n\n\\(\\epsilon_{\\text{guidance}}^{\\Theta} = \\epsilon_{\\text{hands}}^{\\Theta} + \\epsilon_{\\text{skate}}^{\\Theta} + \\epsilon_{\\text{prior}}^{\\Theta}.\\)\n\nWe begin by running HaMeR on the egocentric image corresponding to each timestep t. When detected, this produces 3D hand estimates in the form of MANO [77] joint parameters and camera-centric 3D hand keypoints \\(t_{\\text{camera}}^j\\) for hand joint set j\\(\\in H\\). With each subcripted \\(\\lambda\\) indicating a scalar weighting term, we have:\n\n\\(\\epsilon_{\\text{hands}}^{\\Theta} = \\lambda_{\\text{hands}}^{3D} \\epsilon_{\\text{hands}}^{3D} + \\lambda_{\\text{reproj}} \\epsilon_{\\text{reproj}}.\\)\n\nThe 3D objective \\(\\epsilon_{\\text{hands}}^{3D}\\) minimizes the distance between the detected MANO hand parameters and the corresponding SMPL-H hand parameters, in terms of wrist pose and local joint rotations. With IK as projection with camera intrinsics K, \\(P_{\\text{world}}^j \\in \\mathbb{R}^3\\) as the world position for joint j at time t, and \\(T_{\\text{camera,cpf}}\\) from the device calibration, the reprojection loss is:\n\n\\(\\epsilon_{\\text{reproj}}^{\\Theta} = \\sum_{t, j \\in H} || \\Pi_K (P_{\\text{camera,j}}^{\\Theta}) - \\Pi_K (P_{\\text{camera,j}})||^2,\\)\n\n\\(P_{\\text{camera, j}}^{\\Theta} = T_{\\text{camera,cpf}} (T_{\\text{world, cpf}}^t)^{-1} P_{\\text{world,j}}^{\\Theta}.\\)\n\nTo reduce foot skating, we use contact predictions to apply a skating loss [72, 99] for each time t and joint j:\n\n\\(\\epsilon_{\\text{skate}} = \\lambda_{\\text{skate}} \\sum_{t,j} \\psi_j^t (||(P_{\\text{world,j}}^{t+1} - P_{\\text{world,j}}^{t-1})||^2).\\)\n\nFinally, we minimize a prior term \\(\\epsilon_{\\text{prior}}^{\\Theta}\\). This term penalizes deviations between each constrained rotation matrix \\(\\Theta^t\\) and the original output rotations \\(\\hat{\\Theta}^t\\) from our denoiser \\(\\mu_{\\Theta}(z_n, n, C)\\), in terms of both joint rotation and rotational velocity."}, {"title": "3.2.3 Sequence length extrapolation", "content": "For longer sequences at test time, we draw on existing methods in compositional generation for both image [3, 110] and human motion [4, 80] diffusion models. We train our motion prior using subsequences of up to length 128; when input observations exceed this length at test time, we split into windows with a 32-timestep overlap between neighbors. We then run our model \\(\\mu_{\\Theta}(z_n, C, n)\\) on windows in parallel. Diffusion paths for overlapping regions are fused following MultiDiffusion [3] after each denoising step."}, {"title": "4. Experiments", "content": "We conduct a series of experiments to evaluate EgoAllo's conditioning parameterization, body estimation accuracy, and hand estimation performance.\nTraining. To train EgoAllo models used in our experiments, we need sequences containing human body and hand pose parameters, body shapes, and device SLAM poses \\(T_{world,cpf}^t\\). Similar to prior work [6, 28, 47], we train EgoAllo using AMASS [55] with synthesized device poses. We annotate train split sequences by anchoring a central pupil frame between vertices corresponding to the left and right pupils in the blend skinned mesh, and at train time sample sequences uniformly between length 32 and 128.\nEvaluation. We evaluate with four datasets. We use AMASS [55], RICH [26], and Aria Digital Twins (ADT) [59] for body estimation evaluation, and EgoExo4D [17] for hand estimation evaluation. AMASS and RICH do not include egocentric data; we annotate these with synthetic device poses using the same procedure we use for training. ADT and EgoExo4D both include egocentric images and SLAM poses captured using Project Aria glasses [83], which we use directly.\nMetrics. To quantify performance, we report four metrics: (1) MPJPE is a world-frame mean per-joint position error (millimeters). (2) PA-MPJPE is the Procrustes-aligned mean per-joint position error in millimeters, where joint positions are aligned on a per-timestep basis before error are computed. (3) GND is a grounding metric, designed in response to a phenomena where ego-sensed humans \"float\" above the ground. Given a human body trajectory, this metric contains a simple binary indicator of whether the feet of the human ever touch the ground plane. (4) \\(T_{head}\\) is the average SMPL head joint position error in millimeters."}, {"title": "4.1. Body estimation", "content": "In our first set of experiments, we evaluate body estimation from only device SLAM poses, without considering images or hands. This setting allows us to isolate the advantages of our body motion prior, while facilitating direct comparison against methods that do not consider hands [47]."}, {"title": "4.1.1 Invariant conditioning evaluation", "content": "We begin by evaluating the importance of the spatial and temporal invariance criteria discussed in Section 3.1.2. We do this by comparing five implementations of the conditioning g: (1) EgoAllo is the final invariant representation that we propose in Equation 4. (2) Absolute+Local Relative appends absolute poses with the relative pose deltas written in Equation 3. (3) Absolute+Global Deltas appends absolute poses with relative orientation and the world-frame position deltas used by [6, 28]. (4) Sequence Canonicalization uses the alignment approach implemented by [47], which violates temporal invariance. (5) Absolute naively conditions on absolute poses, which violate spatial invariance.\nWe train conditional diffusion models with otherwise identical architecture using each parameterization, and then evaluate on the AMASS [55] test set. Metrics, including percent difference compared to EgoAllo, are reported in Table 1.\nOverall, we find that the choice of conditioning parameterization makes a dramatic impact on estimation accuracy. We observe accuracy improve consistently as invariance properties are incorporated into the representation. Compared to EgoAllo, Absolute conditioning would increase MPJPE by over 23% for both shorter (length 32) and longer (length 128) sequences. Compared to EgoAllo, SeqCanonical conditioning would increase MPJPE by nearly 18% for length 32 sequences and 12% for length 128 sequences."}, {"title": "4.1.2 Comparisons against baselines", "content": "To further study EgoAllo's body estimation quality, we compare against three baselines. (1) NoShape. First, NoShape refers to a variation of EgoAllo that turns off shape estimation, and thus cannot estimate the wearer's height. (2) EgoEgo. We also compare against the human motion diffusion model from EgoEgo [47]. This is similar to EgoAllo, but considers only the SMPL \"mean\" body shape and uses canonicalized coordinates for conditioning and as model output. (3) VAE+Opt. Finally, we compare against an approach based on the SLAHMR [99] framework for human motion estimation from exocentric video. A key advantage of SLAHMR is that it uses an unconditional motion prior [72] in an optimization framework. It can therefore be adapted to new settings without re-training-we keep the same body pose and shape variables as the original pipeline, but replace the exocentric keypoint [69] cost with an egocentric CPF pose alignment cost.\nDue to differences in problem formulation, many existing methods for egocentric human motion estimation are difficult to directly compare. This is particularly true when they have different inputs, such as fisheye cameras [89, 93, 94], wrist-mounted sensors [45], or handheld controller poses [6, 28, 29]. Additionally, prior works like EgoEgo [47] do not incorporate vision inputs for hand estimation. For fairness, we restrict all methods in this section to only CPF or head pose as input.\nEgoAllo improves body motion estimates. We report metrics in Table 2 and visualize example outputs in Figures 4 and 5. We find that EgoAllo enables significant estimation improvements across all datasets, including 20~30% accuracy improvements over EgoEgo for both shorter and longer evaluation sequences. We found shape estimation critical for producing metric-scale, grounded estimates of human body motion, with the head aligned to input SLAM poses and the feet planted on the observed ground plane. This is evident in qualitative results, improved grounding metrics, and in the 6~7% MPJPE gap between EgoAllo and the NoShape ablation.\nVAE optimization converges poorly. Optimization-based estimation approaches have been effective for settings with keypoint costs [72, 99], but we found convergence difficult in our less constrained setting. In Table 2, we observe poor generalization: VAE+Opt performs competitively on the AMASS test set, but performance deteriorates dramatically when evaluating on RICH or Aria Digital Twins. VAE+Opt outputs in Figure 4 also look overly smoothed, without the same expressiveness as the conditional predictions of EgoAllo or EgoEgo [47]. This highlights the advantage of using a conditional diffusion model problem for this estimation problem.\nShape estimation evaluation. To better understand the shape estimation characteristics of EgoAllo, we compare against against the \"mean\" shape used by EgoEgo and the NoShape ablation. On the AMASS test set, we find: EgoAllo slightly improves overall shape (19mm\u219218mm mean vertex-to-vertex error) and produces much better height (52mm\u219232mm mean height error), but is not able to generalize in terms of body weight (5kg \u2192 8kg mean weight error). The body shape is inferred from the wearer's head pose, which intuitively provides strong height constraints but is less corelated with weight. Accurate height is key for proper scene placement, as reflected by both the MPJPE and GND metrics."}, {"title": "4.2. Hand estimation", "content": "To evaluate evaluate hands estimated by EgoAllo, we run HaMeR on the segment of the EgoExo4D [17] validation set that is labeled with 3D hand pose keypoints. We quantitatively compare four hand estimation methods in Table 4.2. In (1) HaMeR [64], we use HaMeR out-of-the-box on undistorted egocentric RGB images. We do not assume bounding boxes as input; instead, we follow the HaMeR demo code and compute crops using ViTPose [97]. When multiple hands are detected for a single side, we naively take the first one. (2) EgoAllo-Mono refers our full system, which uses the same monocular HaMeR hand estimates for guidance. In (3) EgoAllo-Wrist3D, we augment our system with wrist pose estimates from Project Aria's Machine Perception Services [83] unlike HaMeR, which assumes monocular input, this uses a pair of SLAM cameras that are unique to Project Aria. Finally, (4) EgoAllo-NoReproj removes the reprojection term (Equation 14) from EgoAllo-Mono. Instead, hand guidance is done directly using the 3D wrist poses predicted by HaMeR. For fairness across settings, we compute metrics only on timesteps where HaMeR estimates are available.\nQuantitative results are provided in Table 3. While HaMeR's local poses (PA-MPJPE) are slightly better, EgoAllo's hand-body estimation significantly improves how well hands are estimated in the world coordinate system. Compared to HaMeR, EgoAllo drops MPJPE from 237.90mm\u2192131.45mm. Incorporating more accurate wrist pose estimates (EgoAllo-Wrist3D) offers a practical solution for further improvements: 131.45mm \u2192 60.08mm. Reprojection-based guidance is also more robust: EgoAllo-NoReproj outputs are the worst in both MPJPE and PA-MPJPE.\nQualitatively, we observed that high hand estimation errors in naive monocular estimation with HaMeR are explained by a combination of detection failures and monocular ambiguities. Even when detections succeed, the scale and distance of monocular HaMeR estimates are often incorrect or flicker in between frames. Incorporating these hands via guidance with our diffusion motion prior encourages final outputs that obey the kinematic and smoothness constraints imposed by plausible body motion-we provide examples of HaMeR estimates rendered jointly with EgoAllo outputs in Figure 6, where we note realistic elbow and shoulder poses."}, {"title": "5. Dicussion", "content": "Limitations and future work. While the core contributions of EgoAllo are general, the current implementation of our system has a few limitations that we hope to explore in future work. First, diffusion model guidance is a test-time optimization process that requires hyperparameters and incurs a runtime cost: our optimization uses a CPU-based sparse linear solver [10], which for length-128 sequences takes several seconds for each guidance step. In the future, it may be possible to bootstrap using outputs from our model to train a feedforward model that avoids this step. Success for hand guidance also still depends on reasonable monocular hand estimates. Estimation can therefore fail as a result of errors like left/right flipping or spurious detections, which we found causes high errors in our Table 3 hand estima-tion metrics. Finally, we also train only on AMASS [55], which includes floor planes but no detailed scene geometry. As a result, our method will fail if we cannot detect an approximate floor plane; this is most likely in settings like hills or staircases. In the future, we hope to extend our insights to data with more detailed scene information, which concurrent work has highlighted the usefulness of in informing human body estimation [21].\nConclusion. We presented EgoAllo, a system for estimating human motion using sensors from head-mounted devices. Our method takes advantage of motion capture data [55] and an off-the-shelf visual hand estimator [64] to jointly estimate human body pose, height, and hand parameters. EgoAllo highlights the importance of spatial and temporal invariance in conditioning for this problem, while demonstrating how estimated bodies can be used to improve hand estimation."}]}