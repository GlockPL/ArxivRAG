{"title": "Hamming Attention Distillation: Binarizing Keys and Queries for Efficient Long-Context Transformers", "authors": ["Mark Horton", "Tergel Molom-Ochir", "Peter Liu", "Bhavna Gopal", "Chiyue Wei", "Cong Guo", "Brady Taylor", "Deliang Fan", "Shan X. Wang", "Hai Li", "Yiran Chen"], "abstract": "Pre-trained transformer models with extended context windows are notoriously expensive to run at scale, often limiting real-world deployment due to their high computational and memory requirements. In this paper, we introduce Hamming Attention Distillation (HAD), a novel framework that binarizes keys and queries in the attention mechanism to achieve significant efficiency gains. By converting keys and queries into -1, +1 vectors and replacing dot-product operations with efficient Hamming distance computations, our method drastically reduces computational overhead. Additionally, we incorporate attention matrix sparsification to prune low-impact activations, which further reduces the cost of processing long-context sequences.\nDespite these aggressive compression strategies, our distilled approach preserves a high degree of representational power, leading to substantially improved accuracy compared to prior transformer binarization methods. We evaluate HAD on a range of tasks and models, including the GLUE benchmark, ImageNet, and QuALITY, demonstrating state-of-the-art performance among binarized Transformers while drastically reducing the computational costs of long-context inference.\nWe implement HAD in custom hardware simulations, demonstrating superior performance characteristics compared to a custom hardware implementation of standard attention. HAD achieves just 1.78% performance losses on GLUE compared to 9.08% in state-of-the-art binarization work, and 2.5% performance losses on ImageNet compared to 12.14%, all while targeting custom hardware with a 79% area reduction and 87% power reduction compared to its standard attention counterpart.", "sections": [{"title": "1 Introduction", "content": "Since the introduction of the transformer [32], various adaptations of the architecture have spread across nearly every domain of deep learning, achieving state-of-the-art performance across modalities such as language, vision, video, and audio [7; 8; 2; 33]. The versatility and success of the transformer is largely attributable to its self-attention module, which learns pairwise relationships and information sharing between vectors in its set of inputs, each of which can represent language tokens, image patches, etc. While this provides a flexible approach to learn relationships in complex data, the pairwise operations also result in O(n\u00b2) runtime, where n is the number of inputs to the self-attention module. This means that when processing a text, we would expect the runtime to scale with the square of the length of the text, while all other operations in the transformer scale linearly to the length of the text as visualized in Figure 1.\nThis poor scaling is a critical bottleneck across many fields where we may want to apply transformers. Chatbots processing very long texts for question answering, LLM-based search tools processing a large number of web pages, vision transformers applied to large and high-resolution images, and video models all demand context lengths past what standard transformers can efficiently accommodate. This has led to a large research literature on alternatives for self-attention with better asymptotic scaling, targeting long-context settings [10; 37; 34]. However, none of these approaches has overtaken standard self-attention in popularity for a number of reasons from accuracy/performance losses to poor hardware utilization and throughput. Additionally, many of these approaches cannot be easily used to adapt an existing pre-trained transformer for long-context inference.\nBinarization, a special case of quantization, is another research area which has sought to improve the runtime and performance characteristics of deep neural networks. However, unlike the long context literature which focuses on asymptotic runtime advantages, the binarization literature focuses on efficiently leveraging hardware. This is done by replacing floating point weights and activations with binary values, therefore replacing expensive floating point operations with far more memory and runtime efficient bitwise operations. These approaches often target custom hardware [20] designed to run neural networks on edge devices at the cost of some performance loss.\nCurrently, hardware development is moving past only targeting edge devices and into the server farm, where deep learning architectures such as transformers have become so massive, slow, and energy intensive as to demand custom hardware solutions. This has led to cutting edge accelerators such as the Google TPU [14], Cerebras CS-2 [21], and Groq TSP [1] among others. This development opens the door to new approaches in algorithm development. Conventionally, binarization techniques would have been focused on lightweight edge devices, making long context applications unrealistic. Additionally, long-context work would be constrained to target conventional GPU hardware, which is optimized for dense, floating point arithmetic. Our work bridges this gap, selectively applying binarization techniques to target custom hardware for efficient long context inference. Our main contributions are as follows:\n1. We present a novel, fast, and lightweight framework for fine-tuning transformers to enable binarization of keys and queries and sparsification of the attention matrix, accelerating all O(n\u00b2) attention operations.\n2. We evaluate the proposed binarized models across various architectures and tasks, showcasing minimal accuracy degradation.\n3. We assess the performance of a binarized model on the QUALITY long-context question answering benchmark [24], demonstrating improved performance with extended context.\n4. We simulate the computational efficiency of our binarized attention mechanism on custom hardware, revealing significant performance gains compared to standard attention."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Long Context Architectures", "content": "A wide range of variations of and replacements for self-attention have been proposed to address the O(n\u00b2) scaling problem. A number of these approaches use fixed, sparse attention maps, with fewer than O(n\u00b2) active elements. For example, longformer [3] combines local dilated and non-dilated sliding-window attention with a fixed number of globally attending tokens, producing an attention matrix with O(n) non-zero elements and fully networked tokens over multiple sequential attention layers. BigBird [37] is similar, but also includes randomly selected non-zero elements in the attention matrix.\nAnother approach, the Reformer [16], uses locality sensitive hashing to bucket keys and queries, and performs attention within these limited buckets. All of these approaches produce non-trivial accuracy losses against otherwise equivalent models. More recently, Mamba [10] has gained popularity as a competitive attention replacement across many tasks, radically departing from attention and using a state space model instead. While promising, this offers a radically different set of tradeoffs from standard attention, and is even more poorly suited as a drop-in replacement to finetune pre-trained transformers for long context tasks.\nAcross the long-context field, almost all works focus on asymptotic runtime improvements rather than improving hardware utilization, and very few consider adapting pre-trained transformers."}, {"title": "2.2 Deep Neural Network Binarization", "content": "Binarized neural networks [13], trained with binary weights and activations, were developed to increase the efficiency of deep neural network inference, and predate the transformer architecture. While early works focused on convolutional neural networks [27], many recent works have adapted binarization techniques to address the transformer architecture. Some of these works use variations of weight binarization while quantizing activations gently or not at all [36; 35]. This achieves very small performance degredation at the cost of increased runtime and memory when compared to full binarization approaches.\nThese approaches are well suited for decoder-only GPT-style language models, where the quality of generated text is highly sensitive to small changes in the loss value, scaling the number of parameters drastically improves performance, and long contexts are not often necessary. However, these approaches do little to nothing to address the computational costs of attention which dominates in long-context settings, as attention is an operation only between activations in the form of keys, queries, and values.\nOther works, generally applied to BERT-style encoder-only models and vision transformers, perform full binarization including activations and attention, maximizing efficiency at the cost of accuracy. Among the state of the art in this category is BiViT [12], which proposes a novel softmax-aware attention matrix binarization function, achieving significantly higher accuracies than previous fully binarized vision transformers. BiT [23] achieved state-of-the-art results on the GLUE benchmark among fully binarized BERT variants, learning scale and threshold terms during binarization and using a multi-step distillation procedure. Much like BiT, our work includes explicit loss terms so that the fine-tuned student model's attention map closely replicates the teacher model's. Effectively all of the above listed works use variations of straight-through-estimators (STEs) to estimate gradients for non-differentiable quantized functions, but HAD also uses transformations of the hyperbolic tangent (tanh) function to smoothly transition from continuous to binarized representations, similar to previous binarization works [17].\nMany highly optimized binary multipliers exist, which are tailored to perform XNOR-based computations that reduce area and power while maintaining performance [28; 19; 9]. These multipliers leverage the inherent simplicity of binary operations to enable faster computation with reduced hardware requirements, therefore accelerating binarized neural networks. We would expect binarized attention activations to be particularly valuable, as processing in memory approaches which have been successful in accelerating weight-activation operations [4] are not as easily applied to activation-activation operations in attention, exacerbating its costs."}, {"title": "2.3 Binary Keys and Queries", "content": "The selective binarization of keys and queries in this work is primarily inspired by associative memory and Hopfield networks. In hopfield networks, a set of binary vectors or \"patterns\" are stored and can be queried according to some update rule. Modern hopfield networks [5] are capable of addressing a number of unique patterns scaling exponentially with the number of bits per pattern. \"Hopfield networks is all you need\" [26] demonstrated that self-attention is effectively a generalization of modern hopfield networks to continuous valued patterns in the forms of keys, addressed by queries. This mapping between attention and binary modern hopfield networks, along with the exponential storage capacity of modern hopfield networks, led us to hypothesize that binarizing specifically our keys and queries would still enable expressive attention lookups to retrieve value vectors.\nAdditionally, studies of scaling laws of transformer-based LLMs find that performance is tightly coupled to parameter count and training data, and very weakly to model shape [15], indicating that the capacity of the attention mechanism does not determine the capacity of the transformer in many cases. Therefore, we do not expect the capacity loss induced by key and query binarization to signficantly reduce model performance."}, {"title": "2.4 Sparse Attention", "content": "Like multiple other works, HAD utilizes a sparse attention matrix. Because the attention matrix is the output of a softmax function, and the relative magnitude of outputs of a softmax is exponential relative to the difference in inputs, large attention matrices contain a large number of near-zero values. Some works such as BigBird [37] and Longformer [3] impose structured sparsity on the attention matrix, with a bias towards nearby elements being able to attend to one another. Other works, exploit the intrinsic sparsity of the attention matrix in order to filter out unwanted features [11] or accelerate transformer inference [22] like HAD.\nWe select the top N highest attention scores corresponding to each query, and performing a sparse accumulation over our values matrix. While binarization allows us to accelerate the O(n\u00b2) KQT operation, sparsity allows us to accelerate the O(n\u00b2) softmax, scaling, and accumulation over V. Many works have proposed GPU and custom hardware solutions, leveraging sparsity to improve efficiency [30; 29]."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Overview", "content": "To improve the efficiency of self-attention in long-context scenarios, we propose a novel distillation approach such that given a pre-trained transformer, we can binarize the query (Q) and key (K) matrices and sparsify the attention matrix (A). Our method retains the top N elements of A per query vector, retaining the most relevant attention interactions. We can describe standard attention with the following equations where K, Q, and V are our key, query, and value matrices respectively, At is our attention logit matrix, A is our attention matrix, and dk is the dimension of our attention head:\n$A_l = \\frac{QK^T}{\\sqrt{d_k}}$ (1)\nA = softmax(A\u03b9)\nOutput = AV"}, {"title": "3.2 Selecting N", "content": "Before we start, we must decide how to set our sparsity parameter N. Figure 3 shows our accuracies distilling a full-precision DeiT tiny model [31] with top N sparsity from its teacher. Starting at N = 100 and gradually decreasing, we see accuracy recovery until about N = 30 at which point it starts decreasing. DeiT has a context length of 197, and we fine that N \u2248 30 also works well for our BERT models (Section 4.1) [7] with a similar context length of 256 as well as our DeiT models (Section 4.2). However, as we scale to longer contexts, we look to Figure 4 for insights. We find that over larger softmax operations, the percentage of the largest outputs needed to add to some constant probability threshold approaches a constant. We use this as a guiding principle, linearly scaling N with context length in Section 4.3"}, {"title": "3.3 Loss Functions", "content": "To maintain accuracy after binarization and sparsification, we employ a teacher-student distillation approach. Let $A_l^{(m,t)}$ and $A_l^{(m,s)}$ represent the attention logit matrices from the teacher and student models, respectively, for attention matrix m. We define the Kullback\u2013Leibler (KL) divergence loss for the attention logits as the unweighted mean over all rows of all attention heads:"}, {"title": "3.4 Standardization Coefficients", "content": "To facilitate the binarization of the query and key matrices, we estimate the standard deviations \u03c3Q and \u03c3K based on the training data.\nOur estimation procedure involves performing inference on a subset of the training data. Specifically, we randomly sample 100 minibatches, each containing 16 samples, from the training dataset. For each layer in the model, the standard deviation is computed independently within each minibatch and averaged over all minibatches.\nLet $Q^{(b)}$ and $K^{(b)}$ represent the query and key matrices for minibatch b, respectively. The standard deviation for each minibatch is computed as follows:"}, {"title": "3.5 Stage 1: Approaching Tanh", "content": "In the first stage of training, we aim to approach a tanh activation for Q and K matrices while preserving essential attention patterns. This is achieved by gradually reducing the scaling parameter c in the binarization approximation function. During stage 1, we apply top N masking to sparsify the attention matrix and employ our combined loss function to guide the training process.\nWe approximate the binarization of the query and key matrices by using a scaled tanh function. Specifically, the query and key matrices are transformed as follows:"}, {"title": "3.6 Stage 2: Approaching Sign", "content": "In the second stage of training, we aim to approach a sign activation for Q and K by further reducing the scaling parameter c in the binarization approximation function. Like in stage 1, we apply top N masking and out combined loss function. Our stage 2 transformations are as follows:"}, {"title": "3.7 Stage 3: Straight-Through-Estimator Training", "content": "In stage 3, we aim to refine our binarized K and Q by training with a STE. Once again, we continue to use top N masking and the combined loss. We use the following STE function:"}, {"title": "3.8 Stage 4: Final Refinement", "content": "In stage 4, we use the same binarization function and STE as stage 3. However, we now lower the learning rate and remove the attention map distillation loss, allowing the model more flexibility to adjust its attention maps in service of the output distillation loss:"}, {"title": "3.9 Training Details", "content": "All models are distilled with a batch size of 16, the Adam optimizer, a learning rate of 10-5 during stages 1-3 and 10-6 during stage 4, gradient clipping at magnitude 0.5, and a c decay coefficient of 0.9998 per minibatch."}, {"title": "4 Results and Discussion", "content": ""}, {"title": "4.1 GLUE", "content": "Table 1 presents results for BERT backbone models distilled and evaluated using the GLUE language understanding benchmark, comparing our approach to BiT [23], which employs full binarization, along with several ablation studies. We observe that by binarizing only the Q and K activations while keeping all other weights and activations in full precision, our method consistently achieves significantly higher accuracies compared to BiT's full binarization approach. All models used a maximum context length of 256 tokens and all HAD variants used top 30 sparsification of the attention vector per query. All methods seem to significantly struggle with RTE and MRPC, but on all other tasks HAD achieves accuracies within 3% of the baseline teacher model. Notably, we did not face noticeable overfitting issues on any of these tasks, despite running hundreds of distillation epochs for some of the smaller benchmark training datasets.\nTo further assess the impact of binarizing K and Q versus the attention matrix A, we incorporated the softmax-aware attention binarization (SAB) function and the Straight-Through Estimator (STE) from BiViT [12], while keeping the rest of the training pipeline unchanged (\"w/ SAB\"). This resulted in substantial accuracy losses. However, this does not necessarily imply that architectures utilizing SAB cannot achieve competitive performance. Rather, it suggests that systems with binarized A may struggle to close the accuracy gap with those using full-precision A and may be highly sensitive to factors such as batch size, training time, and learning rate.\nAdditionally, we conducted ablation studies by excluding our attention map distillation loss (\"w/o AD\u201d) and the tanh training phase (\u201cw/o Tanh\u201d), replacing these with an equivalent number of STE training iterations. We find that both ablations perform comparably to our standard training procedure across many tasks, but yield non-trivial performance losses in some cases. It is worth noting, we observed that attention distillation and tanh binarization contribute significantly when using less optimized training pipelines. We find these techniques add robustness against variations in architectures, tasks, the absence of \u03c3Q and \u03c3K normalization, and hyperparameters such as learning rate, batch size, training time, and gradient clipping."}, {"title": "4.2 ImageNet", "content": "Table 2 presents results for the base and tiny variants of DeiT [31], distilled and evaluated on the ImageNet [6] image classification benchmark. We once again find that the selective binarization of Q and K produces significantly higher accuracies than implementations including binarization of the attention matrix, as we also saw with the GLUE evaluations. We find that all binarization techniques struggle with the tiny variant of DeiT, indicating that very low capacity attention modules may be disproportionately adversely effected by binarization and/or top N sparsification. When evaluating on ImageNet, the ablations were on par with our full fine-tuning procedure, indicating that these models are less sensitive to attention map distillation and gradual binarization using the STE."}, {"title": "4.3 Long Context Evaluation", "content": "To evaluate HAD's performance in long-context settings, we trained and tested models using the QUALITY long-context multiple-choice question-answering dataset [24] at various context lengths. The QUALITY benchmark contains inputs ranging from 2,000 to 6,000 words, which we truncated to fit the context limits of our models. For our baseline, we used Google's T5-Base model [25], which was pre-trained on the RACE dataset [18] and fine-tuned on QuALITY truncated to each context length. These truncated QuALITY trainind data were also used for our distillation dataset.\nWhile our token limits were set to 256 for GLUE and 197 for ImageNet, here we explore context lengths from 128 to 1024 tokens in powers of two. The top N sparsification parameter N was scaled linearly with context length, ranging from 15 at 128 tokens to 120 at 1024 tokens, ensuring a consistent sparsity percentage across experiments.\nWe observed some noise in accuracies across runs and epochs, which accounts for the performance drop at 512 tokens. However, overall, HAD's accuracy followed the baseline model's trend of improvement with increasing context length, achieving within 3% of the baseline model's accuracy. Combined with our earlier analysis of the softmax function in the long-context regime, these findings support the conclusion that HAD effectively scales to long contexts while maintaining strong performance."}, {"title": "4.4 Hardware Optimization through Algorithmic Design", "content": "To evaluate the impact of algorithmic optimizations on hardware, we synthesized and analyzed a CAM-based binary attention architecture and a conventional BF16 attention design. HAD's custom design leverages capacitive Content-Addressable Memory (CAM) to replace traditional matrix multiplication in attention mechanisms with in-memory associative matching. Its architecture integrates 1-bit XNOR operations for binarized query-key vectors and employs top N sparsity to significantly reduce power and area. In contrast, the BF16 digital attention showed significantly higher area and power metrics: 31.795 mm\u00b2 and 25.491 W. Both designs performed attention computation via QK multiplication for (1x1024) \u00d7 (1024x256) and Attention Probabilities \u00d7 V for (1x256) x (256x1024). Power and area were estimated using Verilog for a smaller module, synthesized with Synopsys Design Compiler, and scaled to the full design."}, {"title": "5 Conclusion", "content": "In this paper, we presented Hamming Attention Distillation (HAD), a novel distillation framework applying selective binarization to address long context transformer inference. HAD selectively binarizes the key (K) and query (Q) projections in transformers while sparsifying the attention matrix. By reducing floating point operations, this approach enables highly efficient hardware deployment with a very small performance cost when handling long context inputs. Through distillation, HAD retains strong performance on benchmarks such as GLUE, ImageNet, and QuALITY, while using a fraction of the compute resources of standard attention.\nFuture work may explore extending our method to efficient GPU implementations, increasing tolerated sparsity, and reducing the precision of the V matrix to further accelerate the AV value accumulation operation. Additionally, these methods may be adapted to decoder-only LLMs, which are very sensitive to small loss increases and do not tolerate general activation binarization. Finally, variations of this work could be explored as train-time optimizations, using binarization and sparsity to reduce the overhead of training transformer models on long sequences."}]}