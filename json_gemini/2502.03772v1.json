{"title": "A Retrospective Systematic Study on Hierarchical Sparse Query Transformer-assisted Ultrasound Screening for Early Hepatocellular Carcinoma", "authors": ["Chaoyin She", "Ruifang Lu", "Danni He", "Jiayi Lv", "Yadan Lin", "Meiqing Cheng", "Hui Huang", "Lida Chen", "Wei Wang", "Qinghua Huang"], "abstract": "Hepatocellular carcinoma (HCC) ranks as the third leading cause of cancer-related mortality worldwide, with early detection being crucial for improving patient survival rates. However, early screening for HCC using ultrasound suffers from insufficient sensitivity and is highly dependent on the expertise of radiologists for interpretation. Leveraging the latest advancements in artificial intelligence (AI) in medical imaging, this study proposes an innovative Hierarchical Sparse Query Transformer (HSQformer) model that combines the strengths of Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) to enhance the accuracy of HCC diagnosis in ultrasound screening. The HSQformer leverages sparse latent space representations to capture hierarchical details at various granularities without the need for complex adjustments, and adopts a modular, plug-and-play design philosophy, ensuring the model's versatility and ease of use. The HSQformer's performance was rigorously tested across three distinct clinical scenarios: single-center, multi-center, and high-risk patient testing. In each of these settings, it consistently outperformed existing state-of-the-art models, such as ConvNext and SwinTransformer. Notably, the HSQformer even matched the diagnostic capabilities of senior radiologists and comprehensively surpassed those of junior radiologists. The experimental results from this study strongly demonstrate the effectiveness and clinical potential of AI-assisted tools in HCC screening. The full code is available at https://github.com/Asunatan/HSQformer.", "sections": [{"title": "1. Introduction", "content": "Liver cancer is the sixth most common cancer worldwide and the third leading cause of cancer-related deaths, with hepatocellular carcinoma (HCC) being the most prevalent type of primary liver cancer [1]. Early detection and treatment of HCC can improve patient survival rates and life expectancy. Ultrasound (B-mode) is the preferred imaging modality for screening high-risk populations for HCC. Compared to CT or MRI, ultrasound offers the advantages of portability, radiation-free imaging, and real-time visualization. However, according to meta-analyses, the sensitivity of conventional B-mode ultrasound in detecting HCC ranges from only 59% to 78%, which is highly dependent on the clinical experience of radiologists [2, 3]. Moreover, ultrasound examinations require substantial time from these specialists for image interpretation and secondary confirmation, often making clinical assessments inefficient. In recent years, with the rapid development of AI, deep learning-based computer-aided diagnosis (CAD) systems have shown great potential. These systems can help improve scanning efficiency and reduce the dependency of diagnostic outcomes on the clinical experience of radiologists or sonographers.\nExisting computer-aided diagnosis systems are primarily based on Convolutional Neural Networks (CNNs) or Vision Transformers (ViTs) [4]. CNNs have been pivotal in the field of computer vision, particularly for tasks involving image classification, object detection and semantic segmentation, due to their proficiency in capturing local texture information. However, their performance is often constrained by the size of their receptive fields, which limits their ability to perceive long-range dependencies and integrate global contextual information. To overcome these limitations, researchers have explored various techniques, such as enlarging convolutional kernels [5-7], employing structural re-"}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Convolutional Neural Networks", "content": "The advent of CNNs has been pivotal in the field of computer vision, establishing their architecture as a cornerstone for critical tasks such as image classification, object detection, and semantic segmentation [25, 26]. CNNs' widespread adoption stems from their ability to automatically learn complex features and patterns directly from raw images, thereby eliminating the reliance on manual feature engineering. Inherent inductive biases, notably locality and translational invariance, are crucial for effectively addressing a broad spectrum of visual recognition tasks. These biases enable CNNs to concentrate on local feature patterns within images, allowing the network to capture critical visual information while maintaining robustness against positional or scale variations. However, despite their success, CNNs have certain limitations, particularly in the domain of medical imaging. One significant drawback is their restricted receptive field, which limits their ability to capture global semantic information\u2014a critical aspect in medical image analysis where comprehensive understanding of the entire image context is essential for accurate diagnosis. To enhance the generalizability of CNNs, several pioneering approaches have been introduced, such as dilated convolutions (Atrous convolutions) [27], deformable convolutions [11, 12], and attention mechanisms like SENet [28] and CBAM [29], which recalibrate channel-wise features and enhance both channel and spatial attention, respectively."}, {"title": "2.2. Vision Transformers", "content": "In natural language processing (NLP), Transformers [30] have demonstrated exceptional capabilities in modeling long-range dependencies. This success has inspired researchers to explore the application of transformer architectures to computer vision, where the ViT represents a landmark development. Recent works have explored applying ViT to various vision tasks: image classification, object detection, image segmentation, depth estimation, image generation, video processing, and others. Despite its success, ViT also presents certain limitations. One notable drawback is its reliance on large-scale datasets for effective training, as Transformers lack the inductive biases inherent in CNNs, such as locality and translation invariance [31]. As a result, ViTs may struggle to generalize effectively when trained on smaller datasets [32]. Additionally, ViTs are computationally demanding, particularly due to their self-attention mechanisms, which scale quadratically with image size, leading to high memory and processing costs. To enhance the efficiency of ViTs while maintaining performance, several studies [16-21] have proposed methods that incorporate local self-attention mechanisms or pooling operations. By restricting self-attention to localized regions of an image or employing pooling to condense feature maps, these optimizations not only streamline processing but also ensure that the models remain adept at handling diverse vision tasks, thereby achieving a balance between computational efficiency and model efficacy. Several other studies have primarily concentrated on diversifying the ViT architectures and enhancing their adaptability, such as the Focal Transformer [33], MobileViT [34], and Axial-Attention Transformer [35], each contributing to the field with innovative approaches to attention mechanisms and knowledge distillation techniques."}, {"title": "2.3. Latent Space and Sparse Learning", "content": "Latent Space is central to capturing intrinsic data features in machine learning, with its utility in deep learning models demonstrated through intermediate layer representations that are crucial for various tasks [36]. Learned queries, as explored in SetTransformers [37] and Perceiver [38, 39] networks, project inputs into a lower-dimensional space for efficiency and prediction accuracy. Goyal et al. [36] enhance transformers by using learned queries as a shared workspace, reducing computational complexity. Self-attention mechanisms have also been optimized with learnable tokens replacing queries or keys, as shown in Involution [40], VOLO [41], and QnA [42], to generate dynamic affinity matrices and improve model performance. These innovations underscore the importance of Latent Space in advancing deep learning capabilities.\nSparse learning stands as a pivotal approach within machine learning, particularly for its effectiveness in optimizing computational resources and enhancing model performance. A prime exemplar of this is the Mixture-of-Experts (MOE) framework, which segments models into specialized experts, selectively activating only the relevant subset to process inputs, thereby achieving computational efficiency. The Multi-Mixture-of-Experts (MMOE) [43] model epitomizes MOE's application in multi-task learning, designed to explicitly learn task relationships from data. It enables the sharing of expert sub-models across various tasks while simultaneously training a gating network to optimize performance for each individual task. Within large-scale models, MOE has attracted considerable scholarly attention; studies such as Qwen-MOE [44], DeepSeek [45, 46], and LLaMA-MoE [47, 48] are at the forefront of research, showcasing the potential of MOE in scaling up large language models (LLMs) with a consistent number of activated parameters. In summary, MOE emerges as a potent sparse representation model, and it is recommended that readers consult reference [49] for an in-depth exploration of the subject."}, {"title": "3. Approach", "content": null}, {"title": "3.1. Overall Architecture", "content": "Our objective is to design an architecture that effectively combines the complementary strengths of CNNs and ViTs, devoid of any unnecessary bells-and-whistles. This architectural framework is deployed in real-world clinical ultrasound screening for hepatocellular carcinoma (HCC) to investigate the potential of AI-assisted diagnosis in a clinical context. Previous studies [23] have indicated that combining CNNs and ViTs may lead to feature redundancy. To address this issue, we introduce a paradigm of latent space representation and sparse learning. As illustrated in Figure 2, our architecture consists primarily of a feature extractor, a projector, and a HSQformer. The HSQformer is a novel multi-level sparse Q-former, composed of stacked hierarchical learnable Query Transformers. We selected the state-of-the-art ConvNeXt and SwinTransformer as feature extractors due to their simplicity and efficiency. Similar to ConvNeXt and Swin Transformer, the HSQformer is organized into four stages that aggregate feature maps at various scales. Each stage shares a similar modular design, consisting of multiple Cross-Self-attention Mixed experts (CSM) blocks, which enables scalability and reusability.\nGiven an input image $X \\in \\mathbb{R}^{C\\times H \\times W}$, we first apply two distinct feature extractors to capture multi-scale representations. The resulting feature maps are denoted as $\\{f_{c1}, f_{c2}, f_{c3}, f_{c4}\\}$ for ConvNeXt and $\\{f_{s1}, f_{s2}, f_{s3}, f_{s4}\\}$ for SwinTransformer. Each set of feature maps is generated at different spatial resolutions with strides (S) of 4, 8, 16, and 32 pixels, respectively, relative to the original image pixels.\nThe multi-scale feature maps, along with a level embedding, are simultaneously fed into a projector module comprising four CSM modules. The projector module is designed to project the features uniformly into a D-dimensional latent space. This process reduces redundancy within the feature representations and produces a set of compact and efficient latent space representations, which are essential for effectively capturing the critical characteristics of the input data.\nThe latent space features are hierarchically injected into the HSQformer to extract salient features related to diagnosis. Subsequently, a linear projection layer is applied for the classification. This approach adheres to the standard backbone network design, ensuring the rationality and feasibility of the network architecture."}, {"title": "3.2. HSQformer", "content": "To harness the complementary strengths of ConvNeXt and SwinTransformer, a straightforward approach is to simply sum their features. However, this method suffers from feature redundancy, and our preliminary experiments, as depicted in Figure 1, show that the benefits are not significant. To address these issues, inspired by references [42, 43, 50], we introduce latent space representation and sparse learning to construct a novel hierarchical sparse querying transformer, termed HSQformer. This architecture comprises"}, {"title": "3.2.1 CSM", "content": "As illustrated in Figure 2, the Cross-Self-attention Mixed experts (CSM) consists of cross-attention, self-attention, and a mixture of experts, closely aligning with the standard transformer architecture without incorporating unnecessary bells and whistles. The role of CSM varies depending on its specific location within the model. When situated within the projector, the CSM facilitates the integration of fine-grained local features with global contextual information, and projects the resulting representations into a D-dimensional latent space for subsequent processing. In contrast, when integrated into the HSQformer, the CSM primarily leverages query embeddings to extract features from the latent space that are highly pertinent to diagnostic classification tasks, ensuring that the most relevant information is captured for accurate decision-making.\nSpecifically, Cross-Attention (CA) within the CSM facilitates the interaction between inter-feature representations, enabling the model to capture complex interdependencies. Self-Attention (SA) is subsequently applied to refine intra-feature representations, further enhancing the internal coherence of features post-interaction. The Mixture of Experts (MOE) introduces sparse representations by dynamically routing each token to a subset of specialized experts, effectively increasing the model's capacity while preserving computational efficiency through selective activation.\nNote that the CSM structure is symmetric; for simplicity, only one side is discussed here, with the other half being analogous. Assuming that $X_{scr}$ and $X_{tgt}$ are the feature representations of the source sequence and the target sequence respectively, the basic principle can be roughly described as follows:\n$CSM (X_{scr},X_{tgt}) = MOE (SA(CA(X_{scr},X_{tgt})))$ (1)\n$SA (X_{scr}) = Softmax(\\frac{X_{scr}WQ (X_{scr}WK)^T}{\\sqrt{d}})X_{scr}WV$ (2)\n$CA (X_{scr}, X_{tgt}) = Softmax(\\frac{X_{scr}WQ (X_{tgt}WK)^T}{\\sqrt{d}})X_{tgt}WV$ (3)\nwhere $W_Q$, $W_K$, and $W_V$ represent the weight matrices for Query, Key, and Value, respectively; $d$ denotes the dimensionality of the query and key features, which is used to scale the dot product in order to mitigate the vanishing gradient problem."}, {"title": "3.2.2 Latent Space Representation and MOE", "content": "Latent Space Representation. Considering the feature map $f_{i_{1}} \\in \\mathbb{R}^{N_i\\times D}$ ($i \\in [1, 2, 3, 4]$) in the latent space, where $N_i$, the sequence length of stage i, is calculated as $\\frac{H}{S_i} \\times \\frac{W}{S_i}$, and $D$ represents the embedding dimension. Corresponding query embeddings can be denoted as $f_q \\in \\mathbb{R}^{Q\\times D}$, where Q denotes the number of query tokens. The interaction between query tokens and latent space features can be formulated as:\n$O \\in \\mathbb{R}^{Q\\times D} = F(f_q \\in \\mathbb{R}^{Q\\times D}, f_{i_{;}} \\in \\mathbb{R}^{N_i\\times D})$ (4)\nwhere the function $F$ encapsulates the computational process that is analogous to the expressions embodied in Equations 1, 2 and 3. Notably, the sequence lengths $N_i$ for the first two stages are relatively large (e.g., 3136), indicating that these feature maps contain more detailed and low-level features, which may include redundant information. Conversely, the latter two stages exhibit smaller lengths (e.g., 49), typically reflecting abstract high-level semantics. By judiciously selecting Q, we can effectively reduce feature redundancy and enhance semantic richness, thereby optimizing the model's ability to identify features that are crucial for diagnostic classification. This enhancement is empirically validated through a series of comprehensive ablation studies.\nMOE. The Q tokens are processed through the MOE module, which selectively activates the top-k experts using a routing mechanism. This approach dynamically allocates input tokens to different experts, allowing each to focus on their specific area of expertise, thereby enhancing the model's efficiency and promoting sparsity by engaging only a subset of experts during inference. In this study, each expert network $F_i (i \\in [1, E])$ is implemented as a Multilayer Perceptron (MLP) network, which accepts an input x and produces an output $F_i(x)$. Concurrently, a gating network"}, {"title": "Experiments", "content": null}, {"title": "4.1. Experimental Setup", "content": "Datasets. The dataset utilized in this study was sourced from the multi-center dataset of the Ultrasound Engineering Society of China's Medical Industry Branch (UE-MICAP), which includes 19,464 images collected from January 2014 to December 2022. The data were obtained from the following hospitals: the First Affiliated Hospital of Sun Yat-sen University, the Third Affiliated Hospital of Sun Yat-sen University, the Sixth Affiliated Hospital of Sun Yat-sen University, the Seventh Affiliated Hospital of Sun Yat-sen University, the First Affiliated Hospital of Guangzhou Medical University, the First Affiliated Hospital of Guangxi Medical University, Foshan Sanshui District People's Hospital, and West China Xiamen Hospital of Sichuan University. The inclusion criteria for the study were: (a) patients at risk of HCC, including those with clinical diagnoses or imaging evidence of cirrhosis, or pathological confirmation of cirrhosis; (b) males over the age of 40 and females over the age of 50 with a history of viral hepatitis; (c) patients who underwent ultrasound screening and had lesions confirmed by clinical or pathological diagnosis. The exclusion criteria were: (a) individuals under the age of 18; (b) images with inadequate quality; (c) images containing both benign and malignant lesions, as this could potentially impair the AI's ability to accurately classify the lesions. This retrospective study utilized fully anonymized images, thus eliminating the need for informed consent.\nTable 1 outlines the three testing scenarios established in this study: Single-Center Test, Multi-Center Test, and High-Risk Patient Test. The Single-Center Test refers to an independent external center evaluation using a dataset distinct from the training set. For the Multi-Center Test, data were aggregated from eight different hospitals to simulate a more diverse and generalized clinical environment. The High-Risk Patient Test specifically focuses on patients with a high risk of hepatitis, aiming to assess model performance within a particularly challenging and clinically relevant subgroup. These testing scenarios have been designed to closely mirror real-world clinical applications, thereby providing a comprehensive assessment of the model's effectiveness and robustness across varied conditions.\nImplementation details. The original DICOM images were losslessly converted to PNG format using OpenCV, standardizing the image format for subsequent processing. Subsequently, textual metadata, including patient and device identifiers, institutional information, and date stamps, were removed from the images utilizing the YOLO object detection model [55]. This study leveraged the PyTorch 2.3.1 deep learning framework, with computations accelerated on an NVIDIA A100 GPU for both training and evaluation purposes. To ensure a fair comparison, we followed the fine-tuning protocol as detailed in CSWinTransformer [18]. Specifically, all models underwent pre-training on the ImageNet dataset, with an input resolution set at 224x244 pixels. Following pre-training, fine-tuning was conducted using the AdamW optimizer, employing a weight decay of le-4 over 32,000 iterations, while maintaining the aforementioned input resolution. The default settings for batch size and initial learning rate were 18 and 1e-4, respectively, with the learning rate adjusted according to a cosine annealing schedule. To enhance dataset diversity and mitigate overfitting, data augmentation techniques such as horizontal flipping, ColorJitter, and Mixup were applied during training. A 5-fold patient-level cross-validation strategy was implemented to prevent any data leakage between the training and validation sets, ensuring robustness and generalizability of the results. Model weights were saved after each epoch if there was an improvement in validation performance, measured by the area under the receiver operating characteristic curve (AUC). Notably, early stopping was not employed, to avoid potential underfitting or overfitting of the models to the dataset. The appendix provides additional detailed information."}, {"title": "4.2. Results", "content": null}, {"title": "4.2.1 SOTA Model Comparison", "content": "Single-Center Test. In the Single-Center Test scenario, the HSQformer-B demonstrates superior Recall (92.14\u00b14.71 %) and AUC (83.83\u00b10.96 %) compared to other models, indicating its exceptional ability to correctly identify positive cases within a single institutional dataset. The HSQformer-S also shows competitive performance, with an AUC of 82.07\u00b11.66%, surpassing models such as ResNet101 [25], DenseNet201 [26], and ThyNet [52].\nMulti-Center Test. Transitioning to the Multi-Center Test, which evaluates model generalizability across different institutions, HSQformer-B achieves leading scores in Accuracy (88.29\u00b10.71%), AUC (95.38\u00b10.33%), and Recall (90.38\u00b14.13%). This suggests that HSQformer not only excels in identifying positive cases but also maintains high consistency across diverse datasets. In comparison, ConvNext exhibits strong overall performance, particularly in Accuracy (86.39\u00b11.11%) and AUC (94.45\u00b10.53%), yet falls short when contrasted against HSQformer's results.\nHigh-Risk Patient Test. For the High-Risk Patient Test, aimed at assessing the model's effectiveness on patients deemed high-risk, HSQformer-B again leads with top-tier metrics: Accuracy (85.41\u00b10.58%), AUC (88.32\u00b10.59%), and F1 score (90.69\u00b10.38%). Notably, the Recall of 94.09\u00b12.75% underscores its proficiency in detecting critical cases. ViT, while performing well with an Accuracy of 84.02\u00b11.35% and Recall of 92.4\u00b12.19%, does not match the comprehensive superiority of HSQformer-B."}, {"title": "4.2.2 Radiologists vs. AI", "content": "In the human-machine comparison experiment, we invited six radiologists to interpret images from the high-risk patient test set, comprising three senior radiologists with over ten years of experience and three junior radiologists with 3-5 years of experience. Each radiologist was required to provide a definitive benign or malignant diagnosis for each image. To prevent potential data leakage that could bias the interpretation results, these radiologists were not involved in the data collection or exclusion process. Table 3 and Figure 3 detail the comparative performance of HSQformer and clinical experts in the high-risk patient screening scenario. Several intriguing observations emerged from this analysis:\nJunior radiologists exhibited higher Recall (86.21%) compared to senior radiologists (80.12%), but lower Precision (88.62% vs. 91.76%). This suggests that in high-risk patient screening, junior radiologists tend to err on the side of caution by classifying uncertain cases as malignant, leading to a higher rate of false positives and thus lower Precision.\nHSQformer-B demonstrated significantly higher Recall (94.09%) compared to senior radiologists, with only a slight decrease in Precision (87.62% vs. 91.76%), aligning closely with the Precision of junior radiologists (87.62% vs. 88.62%). This indicates that while HSQformer-B is highly sensitive in detecting malignant cases, it maintains a balance between Recall and Precision comparable to experienced clinicians.\nHSQformer-B consistently outperformed the average performance of junior radiologists across all metrics and matched the performance of senior radiologists. Notably, HSQformer-B achieved significant improvements over the overall average of all radiologists in terms of F1 score (90.69% vs. 85.59%) and AUC (88.32% vs. 74.45%). These results underscore HSQformer's practical effectiveness in HCC clinical screening scenarios.\nThese observations highlight the potential of HSQformer as a powerful tool in aiding clinical decision-making, particularly in high-stakes diagnostic contexts. The model's ability to surpass the performance of less experienced clinicians and match that of seasoned experts suggests its utility in enhancing diagnostic accuracy and consistency. Furthermore, HSQformer's robust performance in critical metrics such as Recall and AUC underscores its value in reducing missed diagnoses and improving patient outcomes."}, {"title": "4.3. Ablations", "content": null}, {"title": "4.3.1 Macro Ablations", "content": "Stage Schemes. In this section, we verify the macro design of HSQformer. As illustrated in Figure 4, we validate the effectiveness of the hierarchical design by incrementally incorporating CSM. Unless otherwise specified, all ablation study results are obtained on the multi-center test set, as this approach better validates the generalizability of the model. We observe a progressive increase in both F1 and AUC scores. Merely adding a CSM in the final stage results in a 1.77% improvement in F1 and a 0.43% enhancement in AUC compared to the baseline. The full-stage approach yields a 4.22% increase in F1 and a 0.76% improvement in AUC over the baseline.\nStage ratios. Expanding on the findings depicted in Figure 4, this research delves into the influence exerted by the proportional parameters at various stages on the model's performance. Employing state-of-the-art (SOTA) models as a reference point within the field [16,25,53,56,57], we conducted a comprehensive assessment of the model's performance under multiple proportional settings, as elucidated in Figure 5. The results indicate that the HSQformer model demonstrates a negligible variation in performance with respect to different stage ratios, particularly with regard to the AUC, which shows a minimal trend of change. Despite this robustness, the experimental data has guided us in selecting the most effective ratio configuration, which is 2:2:6:2, to optimize the model's performance.\nQuery number and dimension. Within the framework of model architecture, the parameters of Query number and dimension are crucial determinants of the model's efficiency and efficacy. The Query number delineates the number of tokens the model queries during the information processing, signifying the model's capability to manage a specific number of tokens in a single pass. The Query dimension, conversely, pertains to the dimension associated with these queries, which fundamentally establishes the capacity of each query to capture information. In the Figure 6, we meticulously examined the impact of query number and dimension on model performance. Specifically, when query quantity was held constant, incrementally increasing query dimension significantly enhanced the model's F1 score and AUC. For instance, with a query number of 50, the F1 score improved from approximately 86.12% to about 87.17%, and the AUC score increased from approximately 94.04% to around 94.74% as the query dimension was augmented from 96 to 768. This suggests that a higher query dimension facilitates the model's ability to capture long-range dependencies and subtle features, thereby enhancing classification performance.\nFurther analysis revealed that, with the query dimension held constant, a moderate increase in the number of queries significantly enhanced model performance. Specifically, at a query dimension of 768, the F1 score improved from approximately 87.13% to approximately 88.53%, and the AUC score increased from approximately 94.74% to approximately 95.09% as the number of queries increased from 50 to 100. However, when the number of queries was further increased to 300 and 400, the model performance, while remaining high, did not exhibit significant"}, {"title": "4.3.2 Micro Ablations", "content": "Parallel and Serial Schemes. This section primarily investigates the efficacy of the CSM at a micro-level design. We commence by examining the sequence of self-attention and cross-attention mechanisms, as depicted in Figure 7. Through our analysis, we ascertained that there is a negligible difference between the serial and parallel approaches. Nonetheless, we opted for the serial approach based on the hypothesis that query tokens initially capture relevant features through cross-attention, subsequently reinforcing these features via self-attention. This sequential methodology aligns with the design philosophy of the Transformer architecture, ensuring the rationality of our design.\nMOE. In the study of Mixture of Experts (MoE) models, the number of experts and the Top-K selection strategy employed by the router play a crucial role in determining model performance. In sparse MoE models, where the aim is to maintain computational efficiency, existing research predominantly adopts Top-1 or Top-2 selection strategies. This approach is taken because selecting more experts could undermine the sparsity advantage and increase computational overhead. Conversely, when all experts are activated irrespective of input, this dense configuration does not prioritize sparsity but instead focuses on leveraging the collective knowledge of all experts for potentially better performance. Figure 8 provides an insightful examination into the specific impacts of varying expert counts and routing strategies on model performance.When the number of experts is limited, MoE does not exhibit performance improvements over traditional MLP, which can be considered as having zero experts. Specifically, with only two experts and a Top-1 routing strategy, MoE achieves AUC (Area Under the Curve) scores that are comparable to those of MLP but does not show any significant improvement in F1 score; in fact, there is a decrease of 0.88% in the F1 score compared to the MLP. Furthermore, no substantial differences are observed between the Top-1 and Top-2 routing strategies under these conditions, indicating that a limited number of experts does not contribute to enhanced model performance. These results collectively suggest that when the number of experts is small(\u2264 2), MoE fails to provide performance benefits over conventional architectures like MLP, highlighting the necessity of increasing the number of experts or exploring alternative strategies to achieve superior performance in MoE. Building on this, as the number of experts in MoE increases to four or eight, significant performance improvements over MLPs are observed with a Top-1 routing strategy. The adoption of a Top-2 routing strategy further amplifies these performance benefits compared to the Top-1 approach. Interestingly, activating all experts in a dense MoE configuration does not lead to additional performance gains; instead, it may cause a slight decline, potentially due to the increased model complexity that could result in overfitting. Notably, under the Top-2 routing strategy, MoE with four and eight experts exhibit similar performance outcomes, indicating that four experts might provide an optimal balance between performance and efficiency. This insight is crucial for the design of efficient and high-performing MoE architectures."}, {"title": "5. Discussion and Limitations", "content": "Our research primarily focuses on evaluating the efficacy of AI technology in HCC screening, comparing its diagnostic performance against that of clinical radiologists. The results indicate that AI surpasses the diagnostic performance of junior radiologists and matches or even exceeds that of senior radiologists under certain conditions. This aligns with recent advancements in medical imaging analysis where AI has demonstrated significant improvements in both the speed and accuracy of diagnoses. By leveraging deep learning algorithms, AI models can analyze vast datasets of medical images to detect subtle patterns and features that may elude human observers, thereby aiding clinical decision-making. The designed AI model exhibited robust generalization capabilities across three distinct scenarios, underscoring its adaptability to variations in HCC imaging due to individual differences, tumor size, location, and growth rate. This consistency is particularly crucial for early-stage HCC screening as timely diagnosis plays a pivotal role in improving patient survival rates. However, while discussing the potential applications of AI in HCC diagnostics, it is imperative to acknowledge the challenges it faces in clinical practice. First, training and validating AI models require substantial amounts of high-quality data, which are time-consuming and costly to collect and annotate. Second, despite our provision of some visual insights into the model predictions, the interpretability of AI models remains an ongoing challenge, particularly when it comes to explaining the decision-making process to patients and healthcare providers.\nWhile this study yields promising outcomes, several limitations must be addressed. The HSQformer, specifically tailored for ultrasonic HCC screening, incorporates hyperparameters such as query number and dimension, and the number of experts and Top-K selection strategy within the Mixture of Experts (MoE) framework. The applicability of these parameters to other domains requires further investigation. Additionally, our study emphasizes the efficacy of the model without delving into its parameter count. Although MoE enhances performance, it also substantially increases model size; however, only a subset of experts is activated during inference. Future work will explore the feasibility of using more compact models without compromising performance to enhance accessibility and broader application."}, {"title": "6. Conclusion", "content": "This study introduces an innovative Hierarchical Sparse Query Transformer model, designed to enhance the diagnostic accuracy of HCC ultrasound screening. By integrating the strengths of CNNs and ViTs, the HSQformer achieves superior performance across various clinical scenarios, including single-center, multi-center, and high-risk patient testing, significantly outperforming existing state-of-the-art models such as ConvNext and SwinTransformer. Furthermore, its diagnostic accuracy is comparable to that of senior radiologists and even surpasses junior radiologists in certain aspects. The HSQformer's success underscores the immense potential of artificial intelligence in medical image analysis, particularly in improving diagnostic efficiency and accuracy. Its modular and extensible design philosophy ensures broad applicability and flexibility in real-world clinical settings. Additionally, the open-source code facilitates future research and development, contributing to the advancement of AI in the field of HCC screening."}, {"title": "Appendix", "content": null}, {"title": "A. Architecture Details", "content": "The HSQformer models (HSQformer-S, HSQformer-B, and HSQformer-L) are designed to scale in complexity and computational demand. The Small model offers a basic configuration, while the Base model enhances feature extraction. The Large model provides the most advanced capabilities, suitable for complex tasks. Table 4 summarizes their configurations."}, {"title": "B. Training Settings", "content": "Table 5 provides a detailed overview of the configurations used in both the pre-training and fine-tuning stages. The pre-training phase adheres to the protocol described in [18], with the primary objective of enabling the model to learn a robust set of features from a large-scale dataset. The fine-tuning stage builds upon the pre-trained model by adjusting various parameters to further enhance its performance, ensuring it is better suited to specific task requirements. This optimization strategy, transitioning from pre-training to fine-tuning, helps improve the model's generalization and task adaptability."}, {"title": "C. Visualization", "content": "In Figure 9, we conducted a systematic comparative analysis of the performance of state-of-the-art models on three different test datasets using a 5-fold cross-validation method. The figure provides an intuitive visualization of the models' performance across various testing scenarios, thereby aiding in a deeper understanding of the strengths and limitations of each model. Figure 10, on the other hand, offers a hotspot analysis in the form of a heatmap. This heatmap visualization highlights the regions of interest in the ultrasound images where the models focus their attention for making predictions. The intensity of the colors in the heatmap corresponds to the importance of the respective regions, with brighter colors indicating higher significance. This interpretability analysis helps us understand the underlying patterns and features that the models rely on for diagnosing HCC, providing insights into their decision-making process."}]}