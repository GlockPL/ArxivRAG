{"title": "FROG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in Large Language Models", "authors": ["Yiyuan Li", "Shichao Sun", "Pengfei Liu"], "abstract": "Fuzzy reasoning is vital due to the frequent use of imprecise information in daily contexts. However, the ability of current large language models (LLMs) to handle such reasoning remains largely uncharted. In this paper, we introduce a new benchmark, FROG, for fuzzy reasoning, featuring real-world mathematical word problems that incorporate generalized quantifiers. Our experimental findings reveal that fuzzy reasoning continues to pose significant challenges for LLMs. Moreover, we find that existing methods designed to enhance reasoning do not consistently improve performance in tasks involving fuzzy logic. Additionally, our results show an inverse scaling effect in the performance of LLMs on FROG. Interestingly, we also demonstrate that strong mathematical reasoning skills are not necessarily indicative of success on our benchmark. We release the resource in https://github.com/ Nativeatom/FRoG.", "sections": [{"title": "1 Introduction", "content": "The capability to reason effectively is a critical dimension in evaluating the advancements of large language models (LLMs) (Brown et al., 2020; Huang and Chang, 2022; Bowen et al., 2024; Wang et al., 2024b; Zhu et al., 2024). Commonly, the progress of advancing reasoning abilities is mainly evaluated on mathematical reasoning benchmarks (Xia et al., 2024; Huang et al., 2024a) like GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), which demands precise answers derived from clear, rule-based questions. However, much of human knowledge and daily decision-making processes are not based on precision but rather involve handling fuzzy, imprecise information (Nov\u00e1k, 2015). Fuzzy reasoning, which deals with uncertainties and perceptual data, differs significantly from the deterministic processes evaluated in existing standard benchmarks. It often relies on natural language constructs that do not capture information with precise granularity (Nov\u00e1k, 2015). For instance, generalized quantifiers (GQs), such as \u201cfew\u201d or \"most\", are frequently used in natural language to introduce a level of vagueness (Mostowski, 1957). An illustrative fuzzy problem employing GQ might be: \u201cThere have been successive increases of 20% and then most in the price of gas from the previous month. By what percentage should a driver reduce gas consumption so that the expenditure does not change?\" Here, the term \u201cmost\" introduces ambiguity concerning the extent of the price increase and necessitates an estimation of its semantics to solve the problem accurately. However, such GQ-based fuzzy reasoning problems are under-explored. Faghihi et al. (2020) introduces probabilistic fuzzy logic (Yager and Zadeh, 1992) to enhance reasoning abilities. Existing works like Saki and Faghihi (2022) investigate employing fuzzy probabilistic theory in association-based problems, like the relation between fuzzy features and labels (e.g. a lot of smoking and cancer). The mapping functions to encode fuzzy features are either simulated (Faghihi et al., 2020) or collected from limited data with heuristics (Wang et al., 2019). Those mapping functions fail to evaluate complex, real-world reasoning scenarios and realistically capture how LLMs utilize fuzzy reasoning in practical contexts. In this paper, we aim to explore reasoning challenges associated with fuzzy events (Zadeh, 1968), which are mathematically ambiguous and articulated through fuzzy expressions of GQs, such as \"most in the price of gas\". Specifically, we focus on mathematical reasoning problems that incorporate GQs. To this end, we have developed FROG, a benchmark for Fuzzy Reasoning benchmark of Generalized quantifiers, which transforms real-world mathematical problems from GSM8K (Cobbe et al., 2021) and MathQA (Amini"}, {"title": "2 Related Work", "content": "Reasoning abilities, involving drawing conclusions from existing knowledge, are a cornerstone of human intelligence and are crucial for intricate tasks like decision-making and solving math word problems (Yu et al., 2023a). Recently, mathematical problem-solving has become a key dimension in assessing the reasoning capabilities of LLMs (Xia et al., 2024; Huang et al., 2024a). Various approaches have been developed to enhance the math reasoning ability of LLMs, like prompt-based designs (Chia et al., 2023; Zheng et al., 2023; Chen et al., 2023a). On the other hand, Taylor et al. (2022); Lewkowycz et al. (2022); Paster et al. (2023); Azerbayev et al. (2024) propose data construction for the pertaining stage. And further supervised fine-tuning, instruction tuning or alignment methods like Direct Preference Optimization (DPO, Rafailov et al. (2023)) are used to enhance the reasoning abilities of LLMs (Yu et al., 2023b; Luo et al., 2023; An et al., 2023; Huang et al., 2024b; Li et al., 2024; DeepSeek-AI et al., 2024). Motivated by the fact that external tools are widely used in NLP tasks, tool integration is introduced to enhance math reasoning (Mishra et al., 2022; Gao et al., 2023; Gou et al., 2023a,b; Yue et al., 2023; Zhou et al., 2023; Zhang et al., 2024a). Chen et al. (2023b) employs programming methods to enhance the reasoning ability of LLMs. In FROG, we build the fuzzy reasoning tasks based on mathematical reasoning problems and explore common approaches designed to improve reasoning capabilities including math-specialized tuning, code-specialized tuning, and general alignment. GQs are widely used to indicate proportions of predicate satisfaction in communication (Joshi et al., 2020) and NLP benchmarks (Suhr et al., 2019; Cui et al., 2022). They also contribute as a major source to the deficiencies of NLP systems such as NLI (Cui et al., 2022). Given their prevalence in the real-world, we employ GQs as a natural approach to introduce fuzzy information in FROG. Existing methods for modeling fuzzy logic in natural language, as developed by Lee (2004) and Kapustin and Kapustin (2019), depend on pre-defined mapping functions to process fuzziness. Those mapping functions are mostly built from rule-based heuristics on limited data, or with simple distribution assumption (e.g. the gaussian distribution) and hard to be directly on real-world complex reasoning problems. In FROG, we rely on LLMs that are pretrained from large-scale real world text corpora to process the fuzziness of GQs and conduct complex math reasoning problems."}, {"title": "3 Benchmark Collection", "content": "Problems in FROG are collected from two math word problem datasets from the real world: GSM8K and MathQA. GSM8K includes grade school math word problems that can be largely solved with basic arithmetic operations. MathQA consists of multiple-choice GRE and GMAT-level math problems. In FROG, we include questions with percentage mentions, and Figure 1 displays an overview of the collection workflow. Specifically, \u2022 Step 1: Identifying Mathematical Questions with Percentage Mentions - We begin by filtering the original questions to include only those that contain at least one percentage figure, of which the value is between 0% and 100%. \u2022 Step 2: Masking the Percentage Mention - We obscure the specific target percentage mention by replacing it with a [MASK] token to construct a Mask question. If the original question contains multiple percentage mentions, each mention is masked out separately. We also employ two other masking strategies Mislead and X% where the target mention is substituted with a misleading quantifier (with the incorrectness pointed out in the FROG template) or X%. \u2022 Step 3: Searching for the Nearest Quantifiers - The golden choice is selected by finding the closest GQ according to its average strength provided in QuRe (Li et al., 2023), a quantifier reasoning dataset with human-annotated quantifier strengths (i.e., few is the closest GQ to 10% in the running example). \u2022 Step 4: Constructing the FRoG Task - In FROG, we provide the question and the original answer to infer which GQ can be filled to represent the information masked out. This framework is driven by the acknowledgment that it is more practical to formulate fuzzy reasoning than directly solve mathematical problems with GQs. To carefully investigate the performance, we design the easy and hard mode of choices depending on the discriminability of misleading choices. The incorrect choices in FROG-Hard are the misleading top GQs in the previous step (e.g. tiny amount, small amount, none in the running example), while incorrect choices in FROG-Easy are randomly sampled from all misleading GQs. The original question, original answer and choices are then assembled through FRoG templates. We refer to Appendix D for details."}, {"title": "4 Experiment", "content": "We evaluated several open-sourced LLMs, including Llama-2 (Touvron et al., 2023), CodeL- lama (Rozi\u00e8re et al., 2024), Llemma (Azerbayev et al., 2024), llama-3 (AI, 2024), Mistral (Jiang et al., 2023), Qwen-1.5 (Bai et al., 2023), Tulu-2 (Ivison* et al., 2023), WizardLM (Xu et al., 2024), WizardMath (Luo et al., 2023) and Yi-Chat (Young et al., 2024) on our FRoG benchmark. Specifically, we would like to investigate the following three research questions: 1. Effectiveness of existing reasoning enhancement methods in FROG? 2. Can the scaling law be observed in FROG? 3. Is strong mathematical reasoning ability transferrable to fuzzy reasoning? We employ the greedy decoding strategy with max tokens being 1,000, and temperature being 0.9 in experiments. The LLMs are instructed with task instructions (see Appendix E) and 5 demonstrations (Brown et al., 2020) with manually created chain-of-thought (Wei et al., 2022b) solutions to assist the reasoning procedure. The experiments are conducted on NVIDIA A100 80GB GPUs, each experiment can be finished within 2 hours. Moreover, we investigate the sensitivity of different masking strategies. Specifically, we compare the performance between the Mask and Mislead or X% task by computing the Pearson and Spearman correlation of their accuracy. The results in Table 1 indicate strong positive correlations between the performance of Mask and Mislead or X%, meaning that LLMs are not sensitive to the masking strategy in FROG. We also do not observe strong correlation between accuracy and length of the generation. We choose the Mask task as the major task thereafter."}, {"title": "4.1 Overall Result", "content": "The result on FROG is displayed in Figure 3. In general, the accuracy of all models is around 0.05 and 0.45 (and mostly between 0.15 and 0.3), indicating that the fuzzy reasoning is a challenging task for the current LLMs. Moreover, models with small model sizes can demonstrate suprisingly strong performance in FROG compared to models much larger, e.g. Tulu-2-DPO-7B outperforms Tulu-2-70B, Llama-2-70B and WizardLM-70B."}, {"title": "4.2 Fine-grained Evaluation and Analysis", "content": null}, {"title": "4.2.1 Q1: Effectiveness of existing reasoning enhancement methods in FROG?", "content": "Instruction-tuning (Wei et al., 2022a; Ouyang et al., 2024) is demonstrated to further boost the abilities of LLMs (Zhang et al., 2023; Hu et al., 2024). It is employed to improve the usability and safety of LLM systems (e.g., the chat model Touvron et al. (2023); Wang et al. (2024a)), open-ended generations without sacrificing task-specific abilities (Ivison* et al., 2023) or mathematical reasoning abilities (Bai et al., 2023; Tang et al., 2024; Zhou and Zhao, 2024). Moreover, Zhang et al. (2024b) demonstrates consistent performance gain of LLMs on reasoning by instruction-tuning on code data, and the mathematical reasoning ability can be enhanced by introducing a continuous pre-training stage on mathematical tokens (Luo et al., 2023; Azerbayev et al., 2024). We explore whether the effectiveness of continuous pretraining on math or code data, as well as general alignment tuning methods can be extended to fuzzy reasoning in FROG. In this regard, we selectively compare the performance of sevel models WizardLM, Qwen-1.5, Tulu-2, Llama-2, and Llama-3, and their instruction-tuning or aligned versions: WizardMath (based on WizardLM), CodeLlama (based on Llama-2), and Llemma (based on CodeLlama)."}, {"title": "Math-specialized Tuning", "content": "The results are demonstrated in Figure 4, where we observe that the accuracy of all LLMs are less than 30% and the mathematical continuous training does not bring universal benefits in FROG since WizardLM outperforms WizardMath in FROG. Besides, the scaling benefit of Llemma on the FRoG-Easy (5.2% gain) does not comparably extend to the FRoG-Hard (1.5%)."}, {"title": "Code-specialized Tuning", "content": "CodeLlama does not outperform Llama-2 in FROG, meaning continuous pretraining on code does not directly benefit fuzzy reasoning tasks. In fact, the domain shift from programming to natural language largely impacts the performance. The largest CodeLlama (70B) evaluated directly generates code snippet most of the time, leading to the poor performance on FROG."}, {"title": "General Alignment", "content": "The results are shown in Figure 6. In general, the performance on FRoG-Easy (dashed lines) is better than FRoG-Hard (real lines) among all the models. Regarding the difference between the base models and their instruction-tuned versions, the benefit of instruction-tuning substantially diminishes from FRoG-Easy to FRoG-Hard in Llama-2, Qwen-1.5 and Llama-3. Lastly, the inverse scaling effect displays on Llama-2, Qwen- 1.5 and Tulu-2. In FRoG-Easy, the perfromance of 4 base models adhere to the scaling law, whereas the instruction-tuned models display inverse scaling effect except Llama-3. In FRoG-Hard, Llama-2 demonstrates inverse scaling effect in both base and chat models. Conversely, Qwen-1.5 and Llama-3 consistently exhibit scaling phenomenon in both base and chat models. The Tulu-2 base model adheres to the scaling law, while the Tulu-2-DPO model display the inverse scaling effect."}, {"title": "4.2.2 Q2: Can the scaling law be observed in FROG?", "content": "Scaling law is introduced in Kaplan et al. (2020) to suggest the phenomenon that LLMs can achieve enhanced task performance by scaling up model sizes. However, the scaling law does not hold universally. For example, the inverse scaling phenomenon can be observed when LLMs are instructed to choose which information can help to answer a question (McKenzie et al., 2023), which is similar to the design of FRoG tasks. Here, we list the performance of all models evaluated on FRoG in Figure 7. Each line highlights an observed inverse scaling effect. It turns out that 8 out of the 15 model families evaluated demonstrate inverse scaling effect in FROG, crossing base models, continuous trained models and instruction-tuned models. We further provide a case study of Qwen-1.5- Chat for its large number of open-resourced check-"}, {"title": "4.2.3 Q3: Is strong mathematical reasoning ability transferrable on FROG?", "content": "Mathematical reasoning has become a key signal for reasoning capabilities of LLMs (Huang et al., 2024a). To proxy the fuzzy reasoning ability under the precise setting, we design a mask_percent baseline that substitutes the misleading choices into their corresponding mean average strengths in QuRe and the correct choice being the exact value of the target percentage mention. Therefore, the original Mask task (denoted as mask_quant) is transformed into figuring out the hidden percentage information in precise reasoning, with the predictions still reflect the preference over the quantifiers. The results are demonstrated in Figure 8, where GPT-4-turbo and Llama-3-Instruct are the best performed commercial or open-resourced model in both mask_percent and mask_quant. CodeLlama- 70B and Tulu-2-7B achieve minimal accuracy in both precise reasoning (nearly 0%) and fuzzy reasoning (lower than 10%) tasks, which attributes to their programming or web content style outputs. The accuracy of mask_percent of a model is significantly higher than the mask_quant alternatives in most of the cases. And the advances of mask_percent by scaling up model parameters are hard to transfer to the mask_quant task in FROG. Take Tulu-2-DPO models for instance, even though the mask_percent performance improves by scaling the model size, the performance of those models drops significantly and the scaling effect shrinks drastically when shifted to the mask_quant task. Specifically, we observe that models with a larger number of parameters are more likely to receive a larger accuracy drop when shifted from mask_percent to mask_quant. The average accuracy drop of models smaller than 10 billion parameters is 6.8% compared to 16.9% of models of larger sizes, indicating that successfully solving a reverse mathematical (precise) reasoning problem does not necessarily reflect equivalent competences in solving FRoG problems with fuzziness introduced by GQs."}, {"title": "4.3 Case Study", "content": "Lastly, we are interested in the working mechanism that LLMs pose in conducting fuzzy reasoning in FROG. We randomly sample 50 results from GPT- 4-turbo in FRoG-Hard, and demonstrate examples in Table 2. The mechanism of LLMs to solve fuzzy reasoning can be summarized as follows: firstly, the LLMs attempt to solve the target percentage mention in three major patterns: (1) Precisely solving the target percentage mention. (2) Reaching a percentage value that is close to the target percentage mention. (3) Incomplete solution (e.g. unsolved equations). After solving the target percentage mention, the LLMs select their preferred quantifiers through a quantifier estimation stage, which can be implicit. Such implicit reasoning procedure is also studied in precise math reasoning (Deng et al., 2023, 2024). For example in Ex1 of Table 2, the model first solves the target percentage mention with satisfying precision (17.98%), and then conduct a quantifier estimation stage before selecting the quantifier. Note that this stage is not always explicit in selecting the correct quantifier (e.g. Ex2). We include"}, {"title": "5 Conclusion", "content": "The fuzzy reasoning ability is an under-explored direction of the reasoning ability of LLMs. To measure the fuzzy reasoning ability of LLMs, we collect a fuzzy reasoning benchmark FRoG that is based on generalized quantifiers. The experimental results show that fuzzy reasoning remains challenging for current LLMs, and an inverse scaling effect is observed on the performance of FROG. Besides, prevailing reasoning enhancement approaches including continuous pretraining, instruction tuning and general alignment may not stay effective on fuzzy reasoning of FROG. Lastly, LLMs can demonstrate diverse behaviors in fuzzy reasoning."}, {"title": "Limitations", "content": "In this work, we collect a fuzzy reasoning dataset FROG to evaluate the fuzzy reasoning abilities of several existing LLMs. We are aware that even though the problems in FROG originate from real- world math word problems, the new question created may not naturally occur, and the designed masking-based reasoning protocol is not identical to the real-world reasoning procedure where the vague information is processed directly. We also note that GQ-based fuzzy reasoning is only a subset of the entire family of natural language fuzzy reasoning, and the scope of GQs is broader than the ones being studied in this work."}]}