{"title": "Vectoring Languages", "authors": ["Joseph Chen"], "abstract": "Recent breakthroughs in large language models (LLM) have stirred up global\nattention, and the research has been accelerating non-stop since then. Philoso-\nphers and psychologists have also been researching the structure of language for\ndecades, but they are having a hard time finding a theory that directly benefits\nfrom the breakthroughs of LLMs. In this article, we propose a novel structure of\nlanguage that reflects well on the mechanisms behind language models and go\non to show that this structure is also better at capturing the diverse nature of\nlanguage compared to previous methods. An analogy of linear algebra is adapted\nto strengthen the basis of this perspective. We further argue about the difference\nbetween this perspective and the design philosophy for current language models.\nLastly, we discuss how this perspective can lead us to research directions that\nmay accelerate the improvements of science fastest.", "sections": [{"title": "1 Introduction", "content": "Imagine that you are playing a word-guessing game with your friend. You chose the\npast tense of \u201cread\u201d (the one that sounds similar to \u201cred\u201d) as your answer for her to\nguess.\n\"Is the answer close to 'said'?\", she asked. You said yes, since the two sound alike.\n\"Is the answer close to 'real'?\", she asked. You said yes since the two have similar\nspellings.\n\"Is the answer close to 'look'?\", she asked. You said yes since the two have similar\nmeanings.\n\"Is the answer close to 'book'?\", she asked. You said yes since the two often share\nappearances.\n\"Is the answer 'read'?\", she asked. You said yes.\nWe use language every day so naturally that we sometimes do not notice how ver-\nsatile we can recognize words. We can easily specify a word by comparing its similarity\nwith other words, even if the comparisons we made are based on different reasons.\nThe outline of this article will go as follows. First, I will start by introducing\nvectoring and its similar concepts. Then I will take a look at related works and the\nconcerns they propose. In the third section, I will construct our vectoring view of\nlanguage by starting with some taxonomy and definitions, then proceed to show how\na vectoring view can be a good approach to understanding language. Lastly, I will\nconclude with some inspirations we gain from vectoring, and present the path to a\nfew potential future works.\nPhilosophers have been long discussing about the meaning and usage of specific\nwords and how they resemble the talker's internal stance. For instance, how the truth\nvalue is affected by the word \"but\" in a sentence, or when can we say \"so-and-so means\nso-and-so\". I want to argue that the natural way humans use language is by considering\nmultiple attributes of language embedded in its high-dimension vector property. Either\nconsciously or subconsciously, we tend to consider how fluent and natural our sentence\nis when speaking or writing, how the words sound when composing lyrics, and for\nuniversity students, how long each word or sentence is when writing semester reports.\nRecently, using vectors as word representation has empirically achieved great results in\nthe form of large language models (LLMs). This outcome hints that the idea backing\nup what AI scientists have been working on may be much closer to the essence of\nlanguage than what we expected. For the sake of simplicity, I will call the idea of\ntreating words as high-dimensional vectors, and a language as a high-dimensional\nvector space, \"vectoring\".\nTo make our metaphor with the vector space more concrete and to strengthen the\nconnection between our philosophical research and the scientific basis of AI science,\nwe will be loosely adapting some definitions from linear algebra. I believe the amount\nof linear algebra introduced in this article will not be enough to overwhelm readers\nwithout the corresponding background, but enough for discovering new aspects of\nlanguage that were previously unnoticed."}, {"title": "2 Related Works", "content": "Due to how our work crosses different fields of interest, prior research on the phi-\nlosophy of language, philosophy of mind, natural language processing (NLP), large\nlanguage models(LLM), linear algebra, and neural science brought us significant\nreference materials. It is also worth mentioning that during our research on related\napproaches, we also came across several distinguished articles attempting to bridge\nthe gap between the language space of AI models and human philosophers."}, {"title": "2.1 Word vector representations as a theory", "content": "This work is greatly influenced by \"Plato's Camera: How the Physical Brain\nCaptures a Landscape of Abstract Universals\" by Paul M. Churchland [1]. In his\nwork, Churchland has distinctly pointed out a possibility of how the neural system\nin the human body \"manage to generate a 'language space,'\". However, Church-\nland's publication stayed more general and connected to neuroscience and neural\nnetwork structures, which leads to problems generated by how differently humans\nand machines perceive the world[2]. I want to focus on specifically the language\nvector space and its connection to AI language models in this article. Churchland\nalso uses the word map to represent a \"high-dimensional structural homomorph of\nthe objective similarity-and-difference structure of some objective, abstract feature-\nspace\", where I instead want to use the terms \"Function\" or \"Target function\" for a\nmore consistent taxonomy, since maps in computer science often refer to an injection\nrelationship rather than geographical landscapes.\nGoodman et al. (2014) [3] originated the connection between the computational-\nbased language and the language structure in the human mind. Brenden M. Lake\nand Gregory L. Murphy (2023) [4] specifically pointed out that psychologists showed\ngreat interest in contemporary NLP systems serving as psychological theories, and\nthat while they are fairly successful models of human word similarity, they fall short\nin many other respects. Kyle Mahowald et al.(2024) [5] also compared the linguistic\nabilities of AI models and human minds and grounded the distinction of the two to\nneural science. These works share a common suffering in that it is difficult to compare\nthe human understanding of language and the implementation of contemporary AI\nlanguage models, due to the lack of a common language structure that gives us an\napples-to-apples comparison.\nWong et al.(2023) [6] proposed a rational meaning computational framework for\nlanguage-informed thinking that combines neural language models with probabilistic\nmodels for rational inference, using the probabilistic language of thought (PLOT)\nframework introduced in [3], which models thinking as probabilistic programs rather\nthan high dimensional model spaces. Brandt (2018)[7] also proposes a novel language\nstructure based on Culioli's \"Theory of Enunciative Operations\", but focuses more on\nthe syntactic relationship of words in sentences."}, {"title": "2.2 Word representations in Machines", "content": "How machines understand natural language is one of the most active fields in AI\nscience, centering around how AI can communicate naturally with humans, under-\nstand humans' thoughts, and responding their output in languages that humans\ncan fully understand [8] [9]. These can include the forms of videos, images, audio,\nand understanding of environments, but mostly we want to focus on text language.\n[10][11][12][13][14][4] We do want to point out that our proposed structure is not lim-\nited to text language, and we expect future works to be augmented with other aspects\nof natural language."}, {"title": "2.3 The danger of LLMs", "content": "A significant number of research papers we came across address the danger of LLMs,\nmost of which originate from the distinction of language understanding between\nhumans and AIs[15][16][17][18]. The work, \u201cWord Meaning in Minds and Machines\"\nby Lake et al. (2023) [4] specifically pointed out some flaws in the word vector repre-\nsentation of AI language models, and we will address these flaws in later chapters in\nthis article."}, {"title": "3 Vectoring", "content": "Using vectors as a representation of words has a long history, dating back to research\nin computer science in the 1980s to the 1990s[19][20]. However, language in philosophy\nstill remains anthropocentric, and benefits little from the breakthroughs of computer\nscience research about language models. In this section, we will propose our approach\nof a vectoring perspective on languages, and we will show how this approach directly\nconnects to recent AI science breakthroughs in later sections.\nThere are many aspects of language like the meaning of a word, the sound that\nhappens when we utter a word, the aesthetic of poetry, and so on. Not only that\nwe cannot consider all aspects of language at once, but we, as mortals, also cannot\nrecognize all aspects of language. We are like blindfolded people trying to figure out\nwhat an elephant looks like just by touching it. Since this symbol is a high dimen-\nsional space with an unknown shape that changes dramatically depending on the\nperspective we look at it, we shall conveniently call it the vector space of a language,\nor VL for short.\nAn important concept of vectoring is the projection. Since we are not able to\nobserve the VL space as a whole, we project VL into a subspace with a potentially\nlower dimension in order to understand it better. We can think of casting a shadow\non the wall in order to better understand the shape of an object. An example of a\nprojection of VL is to focus on the meaning of words. In linear algebra, a projection is\na linear transformation P in which applying the projection maps vectors in VL into a\nsubspace W. Additionally, applying the projection twice leaves its image unchanged\ncompared to applying it once. These harmonies with our analogy of the projection:\ntrying to find the meaning of the meaning subspace itself essentially brings us the\nsame subspace.\nWords, being the basic element in nearly all other research programs about similar\ntopics, are just a set (since we are using a plural form of word) of vectors, and aren't\nmuch different from utterances or phrases, which are also some set of vectors in VL."}, {"title": "4 Practical vectoring", "content": "The development of language models in computer science has come a long way, and by\nthe underlying method behind the contemporary models, we can roughly divide the\ntechnology history into two stages. We will, however, later show that the two methods\nare essentially the same."}, {"title": "4.1 Word2Vec", "content": "Arguably one of the most important breakthroughs in language models is the work\nby Mikolov et al. in 2013 [21]. This work exceeds the data size and training efficiency\nof previous works in machine-learning language models by orders of magnitudes. It\nis shown that words can have multiple degrees of similarity (which origins from the\nauthors' previous work, [22]).\nThe resulting model, named Word2Vec, achieved great success in preserving the\nlinear regularities among words. This can be shown when performing arithmetic\noperations on word embedding vectors, we get results that somehow reflect a good\nunderstanding of the meaning of the words.\nFor example, vector(\u201cKing\u201d) - vector(\"Man\") + vector(\"Woman\") results in a\nvector that is closest to the vector representation of the word Queen. This method\nserved as a paradigm for language models for about three years until transformers\nwere introduced and text-generative AI experienced another big breakthrough."}, {"title": "4.2 Transformers", "content": "The idea behind text generation AI models' architecture is to repeatedly predict the\ntoken that should appear next. A \"token\" mentioned in AI language models is a sub-\nword or component that makes up a word. For example, the word \"unnerving\" can\nbe divided into three tokens: \"un\", \"nerv\", and \"ing\", each serving its own purpose\nin constructing the word. The main structure in these LLMs is called Transformers,\nand Transformers are used to capture the context (often considered by scientists as a\nfew words before where the prediction takes place. However, in reality, words coming\nfrom early in the input sequence can also have an impact in late predictions) for the\nnext prediction until a stop signal is generated. The output of the Transformer is a\nhigh dimensional vector, for our reference, GPT-3[23] by openAI uses a vector length\nof 12,288 to represent each token.\nIn plain sight, auto-regressive models that are trained to take in previous context\nand predict the next token seems different from what Word2Vec did: capturing the\nrelationship and similarities between words. We can see in practice that LLMs do\nseem to encode correlated tokens in close locations, so there has to be a connection"}, {"title": "4.3 But how does a machine learning model learn meanings\nfrom data?", "content": "The process of learning from training data is a common practice among all the AI\napplications invented today. But this does not stop us from wondering why this also\nworks on word representations. How exactly do the models learn from, and why can\nthey learn the meaning from these data? Computer scientists did not manually label\nall possible relationships between all possible words for AI models to learn from.\nInstead, they feed the learning process lots of existing documents. This means that\nwhat a language model actually learns, is the pragmatic appearance distribution of\neach word. Even though we are still not sure how the pragmatic appearance yields\nmeaning, there is a hypothesis called the distributional hypothesis that we can refer\nto (Gastaldi JL, 2021)[26]. According to the distributional hypothesis, linguistic items\nwith similar distributions can yield similar meanings. If we adopt a distributional-\nist's view here, we can believe that there exists a positive correlation between the\nprobabilistic distribution of the coming word and the actual meaning of the word."}, {"title": "5 Differences between vectoring and practical\nvectoring", "content": "It is a nice timing to explain here a popular misunderstanding: the difference between\nvectoring and practical vectoring. While using similar wording and wanting to\nachieve similar goals, there are still large differences that shouldn't be ignored when\ndiscussing vectoring."}, {"title": "6 Taxonomy and Definition", "content": "1.  The vector space of a language, or V\u2081 in this article, is a comprehensive model\nof a language L in our interest. Due to its high dimensional nature, humans are\nnot able to observe the vector space. Instead, we are projecting the vector space\ninto a subspace with a lower dimension, and argue reasonably within the subspace.\n2.  A Projection is a linear transformation P where P : V\u2081 \u2192 W, in which W denotes\na subspace of V\u2081 that has a dimension not larger than VL.\n3.  When we say we are interested in an attribute of VL, for example, the meaning\nof words, we are finding a subspace W of VL where there exists a function that\ntakes in a word as an input and gives a set of vectors as an output. Formally\nspeaking, we are looking at an attribute subspace Wa such that there exists a\nfunction F(w) \u2192 {{y}|y \u2208 Wa}.\n4.  A Word w is a single vector in VL.\nGiven the above definitions, we can see that:\n5.  We can denote the \u201cfinding of all meanings of a word\u201d can be expressed as F(w) \u2192\n{{y}|y \u2208 Wmeaning}, whereas \u201cfinding the most common meaning of a word\" can\nbe expressed as F(w) \u2192 {y|y \u2208 Wmeaning}."}, {"title": "7 Response to Word Meaning in Minds and Machines", "content": "In the work \"Word Meaning in Minds and Machines\" by Lake et al.(2023)[4], the\nauthor proposes five reasons why the word representations generated from large lan-\nguage models do not represent word meanings. I will argue that while they do indeed\nexist in practical vectoring (LLMs), it is not that significant when we take vectoring\nas a perspective. The five reasons are:"}, {"title": "7.1 Word Representations Should Support Describing a\nPerceptually Present Scenario or Understanding Such a\nDescription", "content": "Word representations of practical vectoring were criticized for the lack of its interactive\ninterface which can be used to gather information from the environment, rather than\nonly depending on text input sequences. This problem is also addressed in our dis-\ncussion about the differences between vectoring and practical vectoring, where biased\nempirical data can lead to biased distribution in the vectors of word representation.\nIn vectoring, which we discussed earlier, however, there exist subspaces that address\nscenario understanding."}, {"title": "7.2 Word Representations Should Support Choosing Words\non the Basis of Internal Desires, Goals, or Plans", "content": "Assuming that folk psychology holds, the word choices we make should somehow\nreflect how we think internally. This is not the case in how practical vectoring works,\nas it generates text based on a calculated probability distribution over tokens. There\nis still space for arguments about whether there is space for internal states in the\nvectoring perspective, but we can easily define a function F(s, t) such that F denotes\nhow well a sentence s can express to someone that the speaker wanted the task t to\nbe finished."}, {"title": "7.3 Word Representations Should Support Responding to\nInstructions Appropriately", "content": "Current large language models fail to transform text into real actions, which can be\nseen as a piece of evidence that language models couldn't really learn the meaning\nbehind what they generated. However, it is arguable if actions responding to instruc-\ntions do have direct causal relationships with the validity of a word representation\nmethod. Imagine a great mathematician sitting at his desk solving problems no one\nhas solved. Although he does answer your questions thrown at him, he doesn't respond\nto you asking him to leave his seat so you can clean the empty energy drink cans under\nhis desk. In this case, we still say that his words are meaningful, even if he ignores to\nperform any action you asked him to do."}, {"title": "7.4 Word Representations Should Support Producing and\nUnderstanding Novel Conceptual Combinations", "content": "The authors argue that large language models can not generate new words other than\nthose they are already programmed to (in GPT3's case that is the 12288 possible\ntokens). This is true since the dimension count is defined by humans, and it is not\nin the nature of current AI models to generate novel outcomes not existing in the\ntraining data. This, too, doesn't apply to our vectoring perspective, as we leave it to\nthe nature of the language to define the dimensions itself."}, {"title": "7.5 Word Representations Should Support Changing One's\nBeliefs About the World Based on Linguistic Input", "content": "The last point addresses a difference between vectoring and practical vectoring that\nwe didn't cover in previous sections. A fully trained language model will *not* change\nits structure or weight when interacting with others. Although we can consider past\nconversations by using them as context for the model to consider, the weights (a.k.a.\nthe learning outcome of the model) will not change. The only way around this con-\nstraint is to design an AI agent system that retrains the network after new interactions.\nUnfortunately, this is currently too costly both in time and resources, and it is unlikely\nto be realized. This point also shows that language is not static, and its components\nare forever changing. We are very excited about how AI scientists will take on this\nproblem, and the future research in this field."}, {"title": "8 Conclusion", "content": "The vectoring view of language showed great empirical achievements through the\nsuccess of recent large language models. We can see that regardless of the different\ndefinitions our current theories have for word meanings, large language models can\ngive us the meanings we want. This opens up the possibility that there are some\ndefinitions of attributes of language lying above what we can capture using language\ntheories but can be approximated using practical vectoring.\nThe vectoring view of language also tells us crucial clues on how we can proceed\nwith our research of the language. Theories that focus on similar topics generate"}]}