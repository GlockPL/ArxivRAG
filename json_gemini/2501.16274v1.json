{"title": "What is Formal Verification without Specifications?\nA Survey on mining LTL Specifications", "authors": ["Daniel Neider", "Rajarshi Roy"], "abstract": "Virtually all verification techniques using formal methods rely on the availability of a formal\nspecification, which describes the design requirements precisely. However, formulating specifications\nremains a manual task that is notoriously challenging and error-prone. To address this bottleneck in\nformal verification, recent research has thus focussed on automatically generating specifications for\nformal verification from examples of (desired and undesired) system behavior. In this survey, we list\nand compare recent advances in mining specifications in Linear Temporal Logic (LTL), the de facto\nstandard specification language for reactive systems. Several approaches have been designed for\nlearning LTL formulas, which address different aspects and settings of specification design. Moreover,\nthe approaches rely on a diverse range of techniques such as constraint solving, neural network\ntraining, enumerative search, etc. We survey the current state-of-the-art techniques and compare\nthem for the convenience of the formal methods practitioners.", "sections": [{"title": "Introduction", "content": "Formal methods refer to the discipline of computer science that employs mathematically\nrigorous techniques to ensure the safe behavior of software, hardware, and cyber-physical\nsystems. There have been countless success stories of formal methods, ranging over several\napplication domains such as communication systems [28, 55], railway transportation [5, 6],\naerospace [33, 24], and operating systems [79, 47], to name but a few. We refer the reader to\nthe exceptional textbook by Baier and Katoen [7] for a comprehensive introduction.\nHowever, there is an important catch with verification techniques: they assume the avail-\nability of functional and usable specifications that precisely describe the design requirements.\nThis assumption is often unrealistic as designing specifications, which had been primarily a\nmanual task, proves not only to be tedious but also error-prone. Consequently, the availability\nof formal specifications is widely regarded as a major bottleneck in formal methods [2, 11, 76].\nTo overcome this limitation, recent efforts have focused on developing methods that\ncan automatically generate specifications from examples of desired and undesired system\nbehavior. Notably, a significant body of research has emerged that concentrates on learning\nspecifications in Linear Temporal Logic (LTL). This focus on LTL is due to its dual benefits:\nmathematical precision and interpretability. The latter has recently become of increasing\ninterest as it facilitates the application of LTL beyond formal verification to areas such as\nreinforcement learning, planning, and other AI-related domains [53, 18, 43, 17].\nThis survey provides a comprehensive overview of the diverse body of work focused on\nlearning LTL specifications. In the past decade, researchers have tackled this task from"}, {"title": "Preliminaries", "content": ""}, {"title": "System Executions and Words", "content": "Informal methods, executions or trajectories of systems are typically formalized as sequences\nof symbols from a finite non-empty set \u2211, known as alphabet. We refer to such sequences as\nwords over \u03a3. \u0391 word w = \u03b10\u03b11 where ai \u2208 \u03a3, can be either finite or infinite, depending\non whether the execution it represents is finite or infinite. The set of infinite words over \u2211 is\ndenoted by \u03a3\u03c9, while the set of finite words is denoted by 2*. Given a word w = a\u2081a2...\nin \u03a3* or \u03a3\u03c9, we let w[i] := a\u017c denote symbol of w at position i and w[i :] := aiai+1... the\nsuffix starting from the starting from position i. The length |w| of a word w is the number\nof its symbols. In particular, the empty word, denoted by \u025b, has length zero."}, {"title": "Linear Temporal Logic (LTL)", "content": "The logic LTL [67] is the de facto standard for reasoning about executions, or sequences\nof events, of reactive systems. Typically, specific events in a system are abstracted using a\nset P of propositions, which represent events or properties of interest in the system under\nconsideration. A system execution is then modeled by a word over the alphabet \u2211 = 2P,\ncapturing the propositions that hold true at specific time points along the system's execution.\nGiven a set P of propositions, the syntax of LTL formulas is defined inductively using"}, {"title": "Passive Learning of LTL Formulas.", "content": "With the necessary groundwork established, we now turn our attention to the central task\nof this survey: learning LTL formulas from examples. To this end, we assume that the\nexamples of desired and undesired system executions are bundled in a sample, denoted by S.\nIn the standard setting for passive learning of LTL formulas, this sample takes the form of a"}, {"title": "Constraint-Based Approaches", "content": "As hinted at in the introduction, constraint-based approaches leverage off-the-shelf solvers\nto search for prospective LTL formulas. These solvers employ a wide range of technologies,\nincluding (i) solvers for satisfiability (SAT) [26, 8] and maximum satisfiability (MaxSAT)\n[12] for propositional logic, (ii) Inductive Logic Programming (ILP) [35], (iii) Mixed Integer\nLinear Programming (MILP) [62], and (iv) Syntax-Guided Synthesis (SyGuS) [1].\nAt the heart of constraint-based approaches lies the idea of translating the learning\nproblem into one or several satisfiability problems within a suitable logical framework (e.g.,\nSAT or SMT). Although the solver technologies may differ, the underlying logical encodings"}, {"title": "Learning from Noisy Data.", "content": "To accommodate noisy data, a relaxation of the requirement for the generated formula to be\nconsistent with all examples is necessary. This relaxed consistency criterion is often expressed\nusing metrics of misclassification, such as the loss function\n$l(S, \u03c6) = \\frac{\\Sigma_{u\\in P}[u \\nvDash \u03c6] + \\Sigma_{v\\in N}[v\\vDash \u03c6]}{|P| + |N|}$,\nwhere S = (P, N) and the Iverson bracket [ ] maps true statements to 1 and false to 0. This\nloss quantifies the proportion of examples misclassified by the formula and closely mimic\nstandard loss functions used in statistical machine learning.\nTo learn minimal LTL formulas that minimimize the above loss function, Gaglione et\nal. [32] propose translating the problem into a Maximum Satisfiability (MaxSAT) instance,\nmirroring the techniques employed by Neider and Gavran [60] and Riener [73] for propositional\nlogic. MaxSAT extends the classical satisfiability problem of propositional logic, allowing for\nthe definition of hard constraints (mandatory clauses) and soft constraints (optional clauses).\nThe solution to a MaxSAT problem is a variable assignment that satisfies all hard constraints\nand as many soft constraints as possible. Gaglione et al. capitalize on this technology by\ndesignating all clauses in DAG as hard constraints and selected clauses in con as soft\nconstraints. As a result, they obtain a minimal LTL formula that minimizes the specified loss\nfunction. They rely on the MaxSAT solving capabilities of Z3 [12] for their implementation.\nIn fact, by following a similar method, almost all of the constraint-based approaches can\npotenitally be extended to noisy settings if the solver employed allows such relaxations."}, {"title": "Learning from Positive Examples Only.", "content": "The problem of learning from positive examples only is a special case of the one-class learning\ntask, where only one class of inputs (positive or negative) is available. This problem frequently\narises in AI applications, particularly in the context of explainability, where one seeks to\ninfer the behavior of an autonomous agent from observational data.\nUnlike learning from noisy data, extending constraint-based approaches to learn from only\npositive examples is not straightforward. The primary reason for this is that learning LTL\nformulas from positive examples is an inherently ill-posed problem. Given a set of positive\nexamples P, the smallest LTL formula that is consistent with P is the trivial formula true,\nwhich is satisfied by any word. Clearly, this formula is too general and does not provide any\ninsights into the underlying (temporal) patterns in the examples.\nTo address this challenge, Roy et al. [75] propose strongness\u2014or specificity\u2014as an\nadditional optimization parameter besides the size of the formula. In particular, the authors\nsolve formulate a learning task wherein, given a set P of positive examples and a size bound\nn > 0, the goal is to learn an LTL formula that satisfies the following three conditions: (i)\neach w \u2208 P satisfies \u03c6, (ii) o has size at most n, and (iii) there exists no other formula with\nthe former two properties that implies .\nTo tackle this problem, Roy et al. employ a counterexample-guided inductive synthesis\nloop [1], which leverages negative examples to guide the learning algorithm towards a\nmost specific LTL formula. In each iteration of the loop, the authors utilize one of the\naforementioned SAT-based methods to construct a consistent LTL formula. This formula is\nthen analyzed, and if necessary, used to generate a new negative example that directs the"}, {"title": "Enumeration-Based Approaches", "content": "The constraint-based approaches discussed in Section 4 provide a systematic method for\nlearning arbitrary LTL formulas. However, the performance of these approaches is limited\nby the capabilities of the underlying solvers. The search techniques typically employed in\nthe solvers are not optimized for learning LTL formulas, thus often leading to bottlenecks in\nscalability.\nAs a result, recent works have started exploring alternative search strategies that are\ntailored to navigate through the search space of LTL formulas efficiently. These approaches\nsearch through relevant/interesting LTL formulas in a more targeted manner that results in\nscalability, typically at the expense of the minimality of the learned formulas.\nA prominent example of this approach is Scarlet, a tool developed by Raha et al. [69, 70]\nthat detects and accumulates common temporal patterns in a given sample. For instance,\nanalyzing a sample consisting of a positive word u = {p}{p}{q}{p}{r}{p} and a negative\nword v = {p}{p}{r}{p}{q}, Scarlet extracts the formula F(q^ F(r)), which captures the\norder in which the propositions q and r appear. By employing dynamic programming, the\ntool identifies a large number of such patterns, which are then translated into a simple yet\nexpressive LTL fragment named directed LTL. Scarlet then combines a suitable selection\nof directed LTL formulas to construct a consistent formula using a novel procedure called\nBoolean subset cover. Unlike constraint-based approaches, Scarlet's search strategy integrates\nsyntax and semantics computations in a single, unified process, resulting in a more efficient\nand effective method for learning LTL formulas.\nAnother notable example is the highly parallelized algorithm developed by Valizadeh et\nal. [78], which is designed to leverage the processing power of Graphics Processing Units\n(GPUs). Their approach comprises two pivotal procedures: relaxed unique checks (RUCs)\nand divide and conquer (D&C). The RUCs procedure performs a bottom-up search through\nthe syntax of LTL formulas, eliminating redundant formulas that exhibit the same behavior\non the given sample. Since this procedure is resource-intensive, it cannot be easily extended\nto large samples. To mitigate this, the D&C procedure partitions the sample into smaller,\nmanageable subsets on which RUCs can be applied in parallel. The resulting formulas\ncan then be combined using Boolean combinations to generate one consistent LTL formula.\nInternally, Valizadeh et al.'s approach employs bit-vectors to encode the semantics of LTL\nformulas, which can be highly efficiently implemented on GPUs. By exploiting the parallel\nprocessing capabilities of GPUs, the authors achieve a significant speedup, making their\napproach perhaps the most scalable one of all.\nGhiorzi et al. [36] propose a range of heuristics to expedite the enumeration of LTL\nformulas. Inspired by Riener [73], the authors first employ an enumeration strategy based on\npartial DAGs to navigate the search space quickly. Then, they utilize LTL rewrite rules to\neliminate equivalent and redundant formulas, leveraging rules such as \u03c6\u2227 \u00ac\u03c6 = false and"}, {"title": "Neural Network-Based Approaches", "content": "Recent research also focuses on leveraging the optimized training capabilities of neural\nnetworks to achieve scalability in the LTL learning process. However, due to the inherent\nuncertainty of neural network training, these approaches lack theoretical guarantees regarding\nthe consistency of the learned LTL formulas. Nonetheless, they can produce reasonably good\nLTL formulas from large, typically noisy datasets.\nThe current approaches specifically exploit Graph Neural Networks (GNNs) [42] to learn\nLTL formulas. GNNs are a powerful neural architecture that learns vector representations of\nvertex and edge features, typically called embeddings. More formally, GNNs define a message-\npassing process between vertices in a graph, where each vertex aggregates information from its\nneighbors to update its own representation. This process is repeated several times, allowing\nthe model to learn complex patterns and relationships between vertex and edge features.\nThe critical insight to understanding the connection of LTL and GNNs is to view a word\nw = a1a2... An as a linear graph v1 \u2192 v2 \u2192 vn with n nodes. This representation allows\nassociating a feature vector xi to each node vi that tracks the satisfaction of the different\nsubformulas of a prospective LTL formula when evaluated at the i-position of an example\n(see the definition of the model relation on Page 4). By leveraging message passing, the\nsatisfaction of the entire formula can be computed by aggregating the feature vectors of\nnodes vj with j > i according to the semantics of LTL.\nLuo et al. [56] built upon the insight of representing words as linear graphs to train a\nGNN on a sample S = (P, N). Subsequently, the authors use the learned network weights to\nextract an LTL formula that closely approximates the behavior of the GNN on the given\nsample.\nAlthough Luo et al.'s work pioneered the use of GNNs for learning LTL formulas, it\nsuffers from the limitation that the extracted LTL formula might accurately capture the\nbehavior of the trained GNN. Wan et al. [81] address this shortcoming by introducing an\nenhanced architecture. In particular, the authors devise a faithful encoding of the LTL\nsemantics within the GNN architecture, achieved through parametric constraints on the\nnetwork weights. This innovative encoding ensures that a consistent LTL formula can always\nbe reliably extracted from the trained GNN."}, {"title": "Other Settings", "content": "Our discussion thus far has centered around the classical passive learning problem for LTL as\ndefined in Definition 1. However, several variants of this problem have been explored, each\npresenting unique challenges. In this section, we discuss three such extensions, highlighting\ntheir distinct characteristics and proposed solutions."}, {"title": "Mining LTL based on Templates", "content": "A key property of Definition 1 is that it makes no restrictions on the syntactic structure\nof an LTL formula as long as this formula is consistent with the given sample. In practice,\nhowever, users sometimes want to incorporate domain knowledge into the learning process or\nmust confine the solutions to specific LTL fragments. Unfortunately, the methods discussed"}, {"title": "Mining LTL from Natural Language", "content": "One of the significant barriers to adopting temporal logic in practice is the limited expertise of\npractitioners and engineers in this area [44, 40]. As a result, they often prefer to specify their\nrequirements in natural language, which is more intuitive and accessible to them. Several\nresearch efforts have focused on bridging this gap by automatically extracting LTL formulas\nfrom natural language descriptions. Early approaches [30, 50, 64, 37, 39] achieved this by\nefficiently parsing English sentences to translate them into LTL and other temporal logic\nformulas. The advent of data-driven techniques has led to the development of neural-network-\nbased methods [65, 41, 21] that rely on human-labeled pairs of natural language descriptions\nand corresponding logic formulas. More recently, researchers have begun to leverage the\nimpressive natural language understanding capabilities of Large Language Models to enhance\nthe translation capabilities further [25, 66, 54, 31], offering promise for more effective and\nefficient property specification."}, {"title": "Logics beyond LTL", "content": "The widespread adoption of continuous-time logics, such as Signal Temporal Logic (STL), in\nthe context of cyber-physical systems has spawned a significant body of research focused\non learning specifications in STL. In fact, a comprehensive survey by Bartocci et al. [9] is\ndedicated entirely to this problem. Most of these works concentrate on learning formulas\nwith a specific syntactic structure [13, 14] or identifying time intervals for given STL\nformulas [4, 49, 48]. A handful of works also tackle the more general passive learning problem,\nwhere the goal is to learn STL formulas of arbitrary structure [59, 63].\nIn addition to linear-time properties, there exist several works focusing on learning\nbranching-time properties in Computation Tree Logic (CTL). For instance, Chan [20]\naddresses the problem of completing simple CTL templates, while Wasylkowski and Zeller [82]\ninvestigate inferring operational preconditions for Java methods in CTL. Recent research by\nPommellet et al. [68] and Bordais et al. [16] demonstrate that constraint-based techniques\ncan be used to learn not only CTL but also Alternating-time Temporal Logics (ATL), which\nextends CTL for multi-agent systems."}, {"title": "Conclusion", "content": "This survey provides a comprehensive overview of the diverse research efforts focused on\nlearning specifications in temporal logic, with a particular emphasis on Linear Temporal Logic.\nWe systematically compared and contrasted these works based on their search strategies to\nnavigate the vast space of possible formulas. Some approaches leverage off-the-shelf solvers,\nwhile others propose customized enumeration techniques or exploit advances in deep learning\nto facilitate the learning process. By synthesizing the strengths and limitations of these\napproaches, we aim to provide a roadmap for future research in this exciting and rapidly\nevolving field."}], "equations": ["\u03c6 := p \\mid p \\in P \\mid \u00ac\u03c6 \\mid \u03c6\u2228y \\mid Xy \\mid \u03c6\u03c5\u03c6,", "w, i = p if and only if p \u2208 w[i]\nw, i \\nvDash \u00ac\u03c6 if and only if w, i \\nvDash \u03c6\nw, i \\nvDash 41 V 42 if and only if w, i \\vDash 41 or w, i \\vDash 42\nw, i \\nvDash Xy if and only if w, i + 1 \\vDash \u03c6\nw, i \\nvDash 41 U 42 if and only if w, j \\vDash 42 for some i < j and\nw,k \\vDash 41 for each i \u2264 k < j", "w, i \\nvDash X \u03c6 if and only if i < |w| - 1 and w, i + 1 \\vDash \u03c6\nw, i \\nvDash 41 U 42 if and only if w, j \\vDash 42 for some i \u2264 j \u2264 |w \u2013 1| and\nw, k \\vDash 1 for each i \u2264 k < j", "l(S, \u03c6) = \\frac{\\Sigma_{u\\in P}[u \\nvDash \u03c6] + \\Sigma_{v\\in N}[v\\vDash \u03c6]}{|P| + |N|},\"\n    \"u \u2208 P"]}