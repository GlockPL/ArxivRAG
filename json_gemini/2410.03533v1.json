{"title": "Multiscale fusion enhanced spiking neural network\nfor invasive BCI neural signal decoding", "authors": ["Yu Song", "Liyuan Han", "Bo Xu", "Tielin Zhang"], "abstract": "Brain-computer interfaces (BCIs) are an advanced\nfusion of neuroscience and artificial intelligence,\nrequiring stable and long-term decoding of neural\nsignals. Spiking Neural Networks (SNNs), with\ntheir neuronal dynamics and spike-based signal\nprocessing, are inherently well-suited for this task.\nThis paper presents a novel approach utilizing a\nMultiscale Fusion enhanced Spiking Neural Network\n(MFSNN). The MFSNN emulates the parallel\nprocessing and multiscale feature fusion seen in human\nvisual perception to enable real-time, efficient, and\nenergy-conserving neural signal decoding. Initially, the\nMFSNN employs temporal convolutional networks and\nchannel attention mechanisms to extract spatiotemporal\nfeatures from raw data. It then enhances decoding\nperformance by integrating these features through\nskip connections. Additionally, the MFSNN improves\ngeneralizability and robustness in cross-day signal\ndecoding through mini-batch supervised generalization\nlearning. In two benchmark invasive BCI paradigms,\nincluding the single-hand grasp-and-touch and\ncenter-and-out reach tasks, the MFSNN surpasses\ntraditional artificial neural network methods, such as\nMLP and GRU, in both accuracy and computational\nefficiency. Moreover, the MFSNN's multiscale feature\nfusion framework is well-suited for the implementation\non neuromorphic chips, offering an energy-efficient\nsolution for online decoding of invasive BCI signals.", "sections": [{"title": "Introduction", "content": "Simulating the human brain remains a key objective\nin neuroscience and artificial intelligence. While Large\nLanguage Models (LLMs), such as GPT (Achiam et al.\n2023), aim to replicate the brain's broad functionality on a\ngeneral-purpose scale, and brain-inspired neural networks\n(Schmidgall et al. 2024; Zhang et al. 2023; Zhao et al.\n2023b) focus on capturing its dynamic complexity, there\nis also a crucial need to understand and model the\nbrain's inner workings on a microscopic level. Recent\nadvancements in invasive Brain-Computer Interface (BCI)\ntechnology allow for the direct recording of spike signals\nat this microscopic scale. Deep learning models can map\nthese microscopic spike signals to macroscopic behavioral\noutputs through neural signal decoding. For example,\nSGLNet (Gong et al. 2023) converts EEG signals into\nspike trains, utilizing Spiking Neural Networks (SNNs) to\nextract topological information and spike-based LSTM units\nto decode temporal dependencies. Similarly, hand gesture\ndecoding has been achieved by decomposing high-density\nelectromyography signals into motor unit spike trains (Chen\net al. 2020), which are classified to estimate each gesture.\nA major challenge in long-term invasive BCI recordings\nis the issue of data distribution shifts due to factors\nsuch as electrode drift and inflammation, which can\ndegrade model generalization. Addressing stable cross-day\ndecoding is thus a critical goal in BCI research. For\ninstance, a DRNN (Ran et al. 2019) demonstrated high\naccuracy and robustness in decoding arm velocity during a\nmacaque monkey's reaching task. Attention-based models,\nsuch as the Temporal Attention-aware Timestep Selection\n(TTS) (Yang et al. 2021), have improved RNN-based\nneural decoders by selecting key timesteps to enhance\naccuracy and efficiency. To overcome data limitations,\nthe spatiotemporal Transformer model NDT2 (Ye et al.\n2024) leveraged pre-training across sessions, subjects,\nand tasks, using cross-attention mechanisms and the\nPerceiverIO architecture to adapt quickly to new sessions\nwith mini-batch labeled data, effectively analyzing diverse\nneural recordings.\nDespite achieving high decoding accuracy, traditional\nANN models often suffer from high energy consumption. In\ncontrast, the human brain operates with remarkable energy\nefficiency. This paper is driven by the need to explore\nbrain-inspired mechanisms for more efficient information\nprocessing. As depicted in Fig.1 A, the human brain\nemploys parallel processing pathways, specifically the\ndorsal and ventral streams, to handle visual inputs (Kandel\n2000). These pathways process signals hierarchically\nin lower-level brain regions, extracting and integrating\nmultiscale features. The functional differentiation between\nthe pathways allows simultaneous extraction of diverse\nfeatures, which are integrated in higher-level brain regions to\nform a unified visual perception. This approach contributes\nto the brain's exceptional signal processing efficiency (Roy,\nJaiswal, and Panda 2019). Furthermore, the brain transmits\nneural signals through discrete action potentials, or \u201cspikes\u201d,"}, {"title": "Related Works", "content": "Brain-inspired Computing\nThe human brain is the only biological system that\ndemonstrates advanced general intelligence with ultra-low\npower consumption. Insights from the brain have the\npotential to propel a narrow AI towards a more general\none. Brain-inspired computing (BIC) embraces this concept,\nintroducing a new paradigm of computation and learning,\ninspired by the fundamental structures and information\nprocessing mechanisms of the human brain (Liu et al.\n2024). It has been found that the parallel processing and\nmultiscale feature fusion are prevalent in brain information\nprocessing (Zeki 2016), and various parallel subsystems\nextract different aspects of signal features, with feature\nfusion occurring both within and between subsystems.\nFor example, the human visual system employs complex\nand organized strategies of parallel processing, hierarchical\nfusion, and modularity to transform and interpret visual\ninformation (Nassi and Callaway 2009). In the olfactory\nsystem, olfactory perception is processed through two\nparallel pathways within the olfactory bulb(Vaaga and\nWestbrook 2016). Similarly, in the pain system, two\nparallel pathways, medial and lateral, are responsible for\ntransmitting and processing nociceptive and emotional\ninformation, respectively (Wang, Luo, and Han 2003).\nBeyond integrating different signal features, the parallel\nstructures in the brain also help reduce operational energy\nconsumption.\nThe rapid advancements in BCI decoding involve\nprocessing multi-channel, high-sampling signals, which\ncan be inefficient with conventional single-network\narchitectures. In contrast, parallel network structures,\ninspired by brain mechanisms, can handle these signals more\nefficiently and extract features across various dimensions.\nHowever, whether in human visual perception (Katsuki and\nConstantinidis 2014) or in artificial intelligence's parallel\nnetworks, processing high-throughput multi-channel signals\nrequires channel attention mechanisms (Zhao et al. 2023a)\nto discern the importance of different parallel channels,\nenabling efficient feature extraction. This mechanism is\nessential for optimizing decoding algorithms and enhancing\nthe performance of BCI systems."}, {"title": "BCI Signal Decoding", "content": "BCIs are primarily categorized into non-invasive systems,\nwhich place electrodes on the scalp but suffer from\nlow signal-to-noise ratios due to transmission losses\n(Johnson 2006), and invasive systems, which record directly\nfrom the inner brain and have demonstrated remarkable\neffectiveness in decoding motor signals in animals (Velliste\net al. 2008). The integration of CNNs and RNNs has\nsignificantly improved the accuracy of decoding invasive\nsignals (Xie, Schwartz, and Prasad 2018; \u015aliwowski\net al. 2022). Emerging methods based on Transformers,\nsuch as swin-transformer (Chen et al. 2024) and other\ntransformer alternatives (He 2021), have shown great\npotential in managing complex temporal dynamics in\nlarge datasets. However, increasing model complexity to\nhandle high-sampling and multi-channel invasive BCIs\nusually results in substantial computational demands and\npower consumption, which poses challenges for clinical\ndeployment on chips. SNNs, with their energy-efficient and\nbiologically inspired operations, offer a promising solution.\nThis study presents an SNN-based model for invasive\nBCI decoding that strikes a balance between low power\nconsumption and manageable complexity."}, {"title": "Methods", "content": "In this section, we provide a detailed description of our\nproposed method, including the model structure and its\noperational workflow."}, {"title": "Overall Architecture", "content": "Fig.1 B presents the overall architecture of our MFSNN\nalgorithm. The input, a recorded spike train with $N_c$\nchannels from an invasive BCI, is divided into $N_s$\nsub-path signals for parallel processing. Each sub-path\nis then processed by its corresponding sub-encoder for\nspecialized analysis. Each sub-encoder comprises three key\ncomponents: a linear transformation module for initial signal\nmodification, a channel attention module for enhancing\nsalient features across channels, and a temporal convolution\nnetwork for extracting temporal dynamics. The outputs\nfrom these modules are designated as $LT_{out}$, $CA_{out}$,"}, {"title": "Sub-Encoder", "content": "Linear Transformation (LT) The given input neural\nsignal is denoted as $Input \\in R^{C\\times 1\\times T}$, where $C$ represents\nthe channel dimension, jointly determined by the number of\nraw data channels $N_c$ and the number of sub-encoders $N_s$,\nshown as following equations:\n$C = N_c/N_s$.  (1)\nAt the outset of our processing procedure, a learnable\nmatrix $M_1$ for linear transformation (LT) is implemented.\nThis LT reduces the sequence length from $T$ to $T'$,\ngenerating the output $LT_{out} \\in R^{C\\times 1\\times T'}$.\n$LT_{out} = Input * M_1$. (2)\nThis critical step here is to capture the representative\nfeatures at the raw data level of the sequence, thereby"}, {"title": "Channel Attention (CA)", "content": "In invasive BCI neural signals,\neach channel captures the activity of different neuronal\npopulations, which contribute variably to the same task.\nThe sub-encoder employs a spiking Channel Attention (CA)\nmodule to extract spatial features from the sub-path signal.\nAs depicted in Fig.1 C, the CA module first provides\nadaptive global average pooling $F_{ap}(\\cdot)$ to the $Input \\in$\n$R^{C\\times 1\\times T}$ to capture spatial features $f_s \\in R^{C\\times 1\\times 1}$ across\nchannels.\n$f_s [C, 1, 1] = F_{ap}(Input) = \\frac{1}{T} \\sum_{i=1}^{T} Input[C,1,T]$. (3)\nSubsequently, a bottleneck structure, composed of two\nspiking convolutional layers $SConv2d(.)$ and modified\nwith Leaky Integrate-and-Fire (LIF) neuron-based activation\nfunctions $LIF(\\cdot)$, is incorporated to compress features first\nand then expand to derive the channel weight distribution\n$w_c$.\n$SConv2d() = LIF(Conv2d(\\cdot))$\n$w_c = SConv2d(SConv2d(f_s))$. (4)\nThis mechanism allows the model to focus on some\nspecific channels those are more critical for decoding target\ntasks while effectively suppressing redundancy and noise\nin the signals. Consequently, this approach enables more\nprecise channel selection, thereby enhancing the accuracy\nof predictions."}, {"title": "Temporal Convolution Network(TCN)", "content": "Invasive BCI\nsignals are temporal sequences, making the features along\nthe time dimension particularly important. The sub-encoder\nutilizes a Temporal Convolution Network (TCN) to\neffectively capture long-term dependencies along with the\ntime dimension through causal convolution and dilated\nconvolution. As shown in Fig.1 D, taking a single channel\nfrom the $Input \\in R^{C\\times 1\\times T}$ for example, the temporal\nsequence $X = (x_1, x_2,..., x_T)$ is subjected to a time\nconvolution with a kernel size of $k = 3$, a dilation rate of\n$d = 2$, and padding defined as $(k \u2212 1) \\times d + 1 = 4$. The\noutput dimension of the hidden layer remains unchanged.\nFor any moment $x'$ in the hidden layer output $X' =\n(x'_1, x'_2,...,x'_t)$ and its corresponding convolution kernel\n$F_t = (W_1, W_2, W_3)$, the following equation holds:\n$x'_t = b + \\sum_{i=1}^{3} W_i \\times X_{t-(3-i)\\times d}$\nThen, an average pooling operation with a window size of\np, denoted as $F_{ap}()$, is applied to $X'$ to obtain the module\noutput $Y = (y_0, ..., y_{T'})$ for the temporal sequence of a\nsingle channel.\n$Y(y_0, \u2026\u2026, y_{T'}) = F_{ap}(X'(x'_1, x'_2, ..., x'_T))$\n$\\qquad \\qquad  \\qquad Y_t =  \\frac{1}{p}  \\sum_{i=t-p+1}^{t} x'_i$,  $\\rho =  \\frac{t}{T'}$\n(5)"}, {"title": "Feature fusion", "content": "We integrate the raw data-level features\n$LT_{out} \\in R^{C\\times 1\\times T'}$ generated by the LT module, the spatial\nfeatures $w_c \\in R^{C\\times 1\\times 1}$ produced by the CA module, and\nthe temporal features $TCN_{out} \\in R^{C\\times 1\\times T'}$ derived from\nthe TCN module to obtain the integrated features of ith\nsub-encoder $E_i \\in R^{C\\times 1\\times T'}$.\n$E_i = LT_{out} + W_c * TCN_{out}$. (6)\nSubsequently, we concatenate the outputs of all $N_s$\nsub-encoders to form the output of the entire signal encoder.\nThe resulting output is represented as $E_{out} \\in R^{N_c\\times 1\\times T'}$.\n$E_{out} = Concat(E_i)$   $(i = 1, 2, 3, ..., N_s)$. (7)"}, {"title": "Spiking Classifier", "content": "The spiking classifier is composed of a LIF neuron\nlayer and a fully connected layer, which takes the fused\nfeature $E_{out}$ as the input current, leading to fluctuations\nin membrane potential and the generation of spikes.\nUtilizing a spiking layer to extract sparse spike features, the\nclassifier then decodes to achieve the classification result\n$Net_{out}$. Moreover, the spiking classifier can generalize\nwith fine-tuning on small samples after pre-training.\nIts advantages include reduced computational costs and\nenhanced model adaptability, making it particularly suitable\nfor addressing cross-day decoding issues and for future\ndeployment on neuromorphic chips."}, {"title": "Experiments and Results", "content": "Experimental Setting\nData Details Dataset 1: The experimental paradigm is\ndepicted in Fig.2 A. The macaque monkeys are used\nas subjects, with each trial divided into four phases.\nBaseline the task initiation is marked by the monkey\npulling the joystick for 1 second until the appearance of\na cue. Preparation-following the cue, the target object\nappears between 0.1 to 0.5 seconds, after which the\nmonkey releases the joystick. Reach\u2014the monkey releases\nthe joystick and reaches for the position of the ball\nwithin approximately 0.5 to 1 second. Task action-the\ntask concludes upon completion of the touch or grasp\naction, which lasts over 1 second. Each trial lasts 2 to 4\nseconds in total. The tasks are categorized into four types:\nright-hand touch, right-hand grasp, left-hand touch, and\nleft-hand grasp, with each trial conducted independently.\nConcurrently, 128-channel neural signals from the monkey's\nmotor cortex (M1) are recorded during the trial at a sampling\nrate of 30 kHz. The experimental data were collected over\neight separate days, from 01/26/2022 to 03/09/2022, with an\naverage of approximately 300 trials per day.\nDataset 2: The experimental paradigm is depicted in Fig.3\nA. The dataset from the research conducted by Churchland\net al.(Churchland et al. 2012), pertains to the collection of\nneural signals extracted from two rhesus monkeys, identified\nas J and N, during their performance of the center-and-out\ntask to eight directions. The signals were recorded using a\npair of 96-channel electrode arrays implanted in the M1,\nwith an average of 2,000 trials per day.\nImplementation Details All training and testing were\nconducted on two NVIDIA GeForce RTX 4060 graphics\ncards. Within the MFSNN network, the spiking neuron\nmodel employs the LIF neuron model, with a time window\nset to 20ms. During the training process, we train the\nMFSNN with the Adam optimizer and a batch size of 32.\nThe learning rate was dynamically adjusted using the cosine\nannealing learning rate schedule, starting from 0.01 and\nranging down to 0.0001. In comparative experiments, we\nadopted a similar training strategy for MLP and GRU."}, {"title": "Neural Signal Decoding", "content": "Single-hand Grasp-and-touch Task In the study of\nneural signal decoding for single-hand grasp-and-touch task,\nwe focus our analysis on decoding signals from the \"Task\naction\" phase. Acknowledging the variability of neural\nsignal characteristics over time, we employ two testing\nmethods: single-day and cross-day decoding. The single-day\ndecoding uses training and testing sets from the same day\nwith an 8:2 ratio; the cross-day decoding uses sets from\ndifferent days. The data includes eight days spanning from\nJanuary 26 to March 9, 2022. The single-day decoding\nexperiments were conducted daily, amounting to a total of\neight sets.\nThe cross-day decoding experiment is further divided\ninto two parts: one part trained with data from January 26\nand tested with data from January 30 to February 9. The\nleft data were trained with data from March 3 and tested\nwith data from March 6 to 9. Each part consisted of three\nexperiments, making a total of six cross-day decoding tests.\nDuring the cross-day decoding, a small proportion of the test\nset is fine-tuned to enhance the generalization ability of all\nmodels.\nWe compare the performance of three algorithms(MLP,\nGRU, and MFSNN) under the two testing paradigms.\nAs shown in Fig.2 B, the average accuracy of the three\nalgorithms under single-day decoding is comparable and\nnotably high(>95%), attributed to the stability of neural\nsignal feature distribution within the same day. That is also\nwhy the accuracy rates of single-day decoding are higher\nthan those of cross-day decoding. In cross-day decoding,\ndespite the same fine-tuning apply to MLP and GRU as\nMFSNN, MFSNN still show a higher accuracy rate(>80%),\ndemonstrating its superior decoding capability in the face of\ncross-day changes in neural signal characteristics.\nFurthermore, we conduct gradient testing on the ratio\nof fine-tuning data for MFSNN in cross-day decoding to\nanalyze its impact on performance. As shown in Figure 2.C,\nwhen the fine-tuning data ratio reaches 7.8%, the model\nperformance could be stabilized and good enough(>80%).\nThis slightly higher fine-tuning ratio may be due to the\nminimal differences among the four types of actions in the\ntask, resulting in more similar neural signals, thus requiring\nmore fine-tuning data to achieve good generalization."}, {"title": "Center-and-out Task", "content": "In the center-and-out task\nexperiments conducted on monkeys J and N, we test\nboth single-day and cross-day decoding. The single-day\ndecoding task is similar to the single-hand grasp-and-touch\ntask. For cross-day decoding, the model is trained on the\nfirst day's data and test on the remaining days. For example,\nmonkey J had data from four days. The network will be\ntrained on the first day and tested on the second, third, and\nfourth days.\nThe results, shown in Fig.3 B, indicate that all three\nmodels achieved very high accuracy rates (\u226595%) in\nsingle-day decoding, with no significant differences among\nthem. In cross-day decoding, under the same fine-tuning\nconditions, average accuracy rate of MFSNN is significantly\nhigher than that of MLP and GRU, and the results are more\nconcentrated, indicating stronger generalization and higher\nrobustness.\nWe also conduct a gradient test of the fine-tuning data\nratio for MFSNN on both monkeys to assess its impact on\nperformance. As shown in Fig.3 C, with a fine-tuning data\nratio of only 3.2%, stable and effective generalization is\nachieved in both monkeys.\nIn summary, comparing the results of the two\nexperiments, we find that due to the short time span\nof single-day data, the distribution of neural signal features\ndo not change significantly, resulting in high and similar\naccuracy rates for all models. However, in cross-day\ndecoding, the significant changes in the distribution of\nneural signal features due to the longer time span led to\na decrease in the average accuracy rate of all models.\nNevertheless, MFSNN, with its excellent multi-level feature\nfusion mechanism, can stably capture the distribution of\nneural signals on different days, and with a small sample\n(<8%) of fine-tuning, it can achieve efficient and robust\ndecoding performance."}, {"title": "Energy Consumption", "content": "We estimate the theoretical energy consumption of\nMFSNN and Multiscale Fusion enhanced Artificial Neural\nNetwork(MFANN) by the following two equations(Mark\n2014; Yao et al. 2024):\n$SOP(l) = Rate \\times T \\times FLOPs(l)$\n$E_{MFSNN} = E_{AC} \\times \\sum_{i=1}^{16} (SOP_{LT} + SOP_{CA} + SOP_{TCN})$. (8)\n$SOPs(l)$ means synaptic operations (the number of\nspike-based accumulate(AC) operations) of layer 1, Rate\nis the average firing rate of input spike train to layer 1,\nT is the time window of LIF neurons, and $FLOPs(l)$\nrefers to the floating point operations (the number of\nmultiply-and-accumulate (MAC) operations) of layer l. We\nassume that the MAC and AC operations are implemented\non the 45nm hardware(Mark 2014), with $E_{MAC}$ = 4.6pJ\nand $E_{AC}$ = 0.9pJ.\nUnder the cross-day decoding test paradigm of dataset\n2 with monkey J, the computational energy consumption\non 45nm hardware for a single spike train is simulated\nfor MFANN and MFSNN. The results, as shown in Fig.5,\nindicate that the energy consumption of MFSNN is reduced\nby 90.9% compared to that of the similarly structured\nMFANN."}, {"title": "Ablation Study", "content": "In the ablation study conduct on the cross-day decoding\nexperiment of monkey J from dataset 2, we scrutinize the\nroles of the CA, TCN, and LT modules within the MFSNN.\nAnalysis of Fig.5 reveals that all three modules significantly\ncontribute to the model's performance. Particularly, the\ncontributions of LT and TCN are substantial. Given the\ntemporal nature of neural signals, the feature extraction\nalong the temporal dimension by TCN is of paramount\nimportance. LT, on the other hand, focuses on extracting\ndeep features from the raw data, capturing nuances that may\nbe overlooked by temporal and spatial features. These two\nmodules serve as two critical tiers in the multi-level feature\nfusion strategy of the MFSNN, essential for the model's\noverall performance.\nUpon examining Fig.5, it is observed that the performance\ndistribution of MFSNN without the CA module is\nrepresented by a violin plot that is wider at the bottom\nand narrower at the top, whereas the inclusion of the CA\nmodule reverses this, presenting a plot that is narrower\nat the bottom and wider at the top. This indicates that\nalthough the CA module has a limited effect on enhancing\nthe average accuracy, it effectively elevates a number of\ncross-day decoding outcomes from below to over 80%, a\nchange that holds significant practical implications.\nIn summary, the results of the ablation study demonstrate\nthat the CA, TCN, and LT modules, which target different\naspects of feature extraction, collectively underpin the\nefficacy of the MFSNN."}, {"title": "Conclusion", "content": "Stable and long-term decoding of neural signals is\ncrucial for BCIs, which is an advanced fusion of\nneuroscience and artificial intelligence. In this paper, we\npropose the Multiscale Fusion enhanced Spiking Neural\nNetwork (MFSNN) framework, which emulates the parallel\nprocessing and multiscale feature fusion in human visual\nperception, enabling real-time and energy-efficient neural\nsignal decoding. The proposed SNN-based model decodes\nhigh-throughput invasive brain signals with reduced energy\nconsumption, offering a practical solution for invasive BCI\nsystems. Our experiments demonstrate that it is feasible and\nrobust to MFSNN for cross-day decoding in the BCI system.\nFrom Fig.2 C and Fig.3 C, it is evident that the\ncross-day decoding based on the MFSNN performance is\naffected by the ratio of fine-tuning trials. The decoding\naccuracy of the model initially improves as the fine-tuning\nratio increases and then stabilizes. This indicates that the\nmodel, after generalizing over a certain amount of data,\ncan easily achieve good decoding performance. However,\nthe required proportion of fine-tuning data is influenced\nby the complexity of the dataset and the difficulty of\nthe task paradigm. According to the hypothesis of neural\nmanifold (Gallego et al. 2020; Zhao et al. 2024), there is\nfrequently a latent low-dimensional stable neural manifold\nwithin high-dimensional neural signal data. In our next\nsteps, we will adopt this perspective, aiming to design a\nlong-term stable decoding algorithm that first reduces the\ndimensionality of microscopic spike signals to the neural\nmanifold and then decodes macroscopic behavioral signals\nfrom the neural manifold.\nThe MFSNN's multiscale feature fusion framework is\nalso well-suited for implementing neuromorphic chips,\noffering an energy-efficient solution for the online decoding\nof invasive BCI signals. Regarding the deployment of\nalgorithms on hardware, this paper explores the feasibility\nof energy consumption estimation on neuromorphic chips."}]}