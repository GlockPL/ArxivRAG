{"title": "ElasticAST: An Audio Spectrogram Transformer for All Length and Resolutions", "authors": ["Jiu Feng", "Mehmet Hamza Erol", "Joon Son Chung", "Arda Senocak"], "abstract": "Transformers have rapidly overtaken CNN-based architectures as the new standard in audio classification. Transformer-based models, such as the Audio Spectrogram Transformers (AST), also inherit the fixed-size input paradigm from CNNs. However, this leads to performance degradation for ASTs in the inference when input lengths vary from the training. This paper introduces an approach that enables the use of variable-length audio inputs with AST models during both training and inference. By employing sequence packing, our method ElasticAST, accommodates any audio length during training, thereby offering flexibility across all lengths and resolutions at the inference. This flexibility allows ElasticAST to maintain evaluation capabilities at various lengths or resolutions and achieve similar performance to standard ASTs trained at specific lengths or resolutions. Moreover, experiments demonstrate ElasticAST's better performance when trained and evaluated on native-length audio datasets. Code is available at: https://github.com/JiuFengSC/ElasticAST", "sections": [{"title": "1. Introduction", "content": "Until not long ago, convolution-based neural networks were the prominent approach in both computer vision and audio processing [3]. More recently, transformers have made a significant impact on numerous computer vision and audio processing tasks, and CNN-based architectures are being replaced with these attention-based architectures. Despite this replacement, transformers still adhere to the fixed-size input paradigm as CNNS do. Similarly, Audio Spectrogram Transformers (ASTs) [12] take a fixed-size input where the input spectrograms are divided into fixed-size patches to create tokens as input for the transformer encoder. To obtain a fixed-size input, audio spectrograms are either trimmed or padded to a fixed size. Considering transformers can process any sequence length, using varying input sizes rather than fixed ones, can be a more optimal and natural choice for audio processing tasks.\nThis fixed input size paradigm poses several challenges and flaws: (1) Recent datasets, such as VoxCeleb and Epic-Sounds, consist of audio recordings of various lengths. Considering the recent developments in using in-the-wild data, self-supervised learning, and multimodal learning, having data in various lengths becomes quite natural. (2) Trimming or padding the input data is a suboptimal choice, as it can easily lead to the discarding or contamination of information. (3) AST-based models lack flexibility when evaluated with inputs of different lengths or temporal resolutions, both of which result in varying sequence lengths, compared to those used during training. This necessitates training different AST models for circumstances with varying sequence length requirements, such as computation budget or memory consumption. Overall, these considerations make standard ASTs relatively limited. Therefore, it is appealing to explore the flexibility of sequence lengths in Audio Spectrogram Transformers. Specifically, this involves designing a single AST model capable of handling variable input sequence lengths during both training and inference.\nIn this work, we present ElasticAST, an Audio Spectrogram Transformer model that turns standard AST into a model capable of processing audio of any length or temporal resolution during both training and inference without trimming or padding. The resulting model is functionally superior to standard ASTS as it maintains its performance across various lengths or resolutions of audio during the inference stage and also delivers better performance on datasets containing audio of various lengths.\nThere are previous approaches that explore different sequence lengths or temporal resolutions in AST-based architectures.  focus on providing patch-size flexibility to ASTs and ViTs. Different patch sizes lead to different sequence lengths. Patch sizes are randomly selected during training, and a resizing algorithm is applied to convert the patch embedding weights accordingly for the different patch sizes. As a result, models gain flexibility with different patch sizes during the inference stage. Our method follows a similar direction in terms of providing flexibility to sequence length during both training and inference stages. However, instead of patch sizes, our model focuses on variable lengths and temporal resolutions of audio inputs. Another related work is [23], which uses mixed resolutions of audios to train ASTs efficiently. The main idea is to process lower-resolution audio (coarse) early in training,"}, {"title": "2. Approach", "content": "2.1. Preliminaries\nThe Audio Spectrogram Transformer (AST [12]) utilizes a transformer-based architecture to process audio spectrograms.\nIt starts by splitting each spectrogram $x \\in \\mathbb{R}^{f \\times t}$ from a batch $X \\in \\mathbb{R}^{B \\times f \\times t}$ into a sequence of S smaller patches: $x \\rightarrow x_i \\in \\mathbb{R}^{p \\times p}$, where i ranges from 0 to S and $S = (f/p) \\times (t/p)$. These patches are then converted into embeddings via a linear projection layer $e_i = P(x_i) \\in \\mathbb{R}^{D}$. Next, a special [cls] token is prepended to the sequence, increasing the length to $N = S + 1$. The learnable positional embeddings are added to provide the order information of the tokens. The sequences are then transformed by the transformer encoder. The encoder's output for the [cls] token acts as the audio spectrogram's representation for downstream tasks, such as classification.\n2.2. Architectural changes\nElasticAST is a conceptually simple extension of standard ASTs, designed to accommodate the use of various audio spectrogram lengths during both the training and inference stages. This flexibility is achieved with minimal architectural modifications to conventional ASTs.\nSequence Packing. Unlike AST, which allocates N tokens for all the B input spectrograms, our model employs a sequence packing method that accommodates the varying lengths by organizing the spectrograms into token sequences $X \\in \\mathbb{R}^{B' \\times N' \\times D}$ after patchification as illustrated in Figure 3. This method initiates by sequentially filling the first token sequence row with tokens derived from the samples until reaching the preset token limit per row, L' (as a default set to 2048). If the tokens from a sample about to surpass this limit, they are deferred to the subsequent row, and the allocation process continues accordingly for the remaining samples. This packing technique generates B' rows of token sequences, each potentially varying in length but constrained by L'. To facilitate processing by the transformer encoder, as shown in Figure 3, we introduce minimal padding tokens to each row, standardizing their lengths to $N' < L'$, which corresponds to the longest sequence length among the rows. Our approach to packing is deliberately straightforward, prioritizing simplicity by processing samples sequentially in their original order and filling rows on a first-come, first-served basis. Future work may investigate more sophisticated packing algorithms to enhance efficiency.\nMask Self-Attention. In conventional patch-based transformer models , only the tokens from the same sample form a row, and these tokens are aware of each other within the transformer encoder through global attention, formalized as\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{D}}) \\cdot V$ where $Q, K, V \\in$"}, {"title": "3. Experiments", "content": "3.1. Datasets and Evaluation Metrics\nDatasets. In our experiments, we utilize four datasets: AudioSet, VGGSound, VoxCeleb, and Epic-Sounds. AudioSet [26] is a large multi-label dataset with approximately 2 million 10-second clips spanning 527 labels across diverse audio categories. VGGSound [27] includes around 200,000 10-second video clips, labeled with 309 sound classes such as objects and human activities. VoxCeleb is an audio-visual dataset focused on human speech, featuring 1,251 speakers and approximately 145,000 utterances across a range of durations from 4 to 144 seconds. For experimental purposes, we impose a hypothetical upper limit of 30 seconds on this dataset to simplify the experiments and manage the memory usage effectively. Lastly, Epic-Sounds , derived from first-person (egocentric) videos, includes 44 categories and a total of 75.9k audio files of various lengths (see Figure 2). Similar to VoxCeleb, we set a provisional maximum duration of 30 seconds for this dataset, though this limit can be adjusted as needed.\nEvaluation metrics. Given the presence of multi-labels in each AudioSet sample, we use mean average precision (mAP) for evaluation across all categories. For the other datasets, we report the Top-1 classification accuracy (Acc) as our measure of evaluation since each sample has only a single label.\n3.2. Implementation Details\nIn this paper, the configuration for standard ASTs follows the same choices as those in . The batch and the patch size are set to 12 and 16 (B/16) respectively, and all models are initialized with ViT (ImageNet Pretrained) for every dataset, except for VoxCeleb, where SSAST weights are employed. The learning rate is established at 1e-5 for all datasets, apart from the Epic-Sounds dataset, which is set at 1e-4 by following . To accommodate varying audio lengths, we shifted from a 1D to a 2D positional embeddings, encoding frequency and time positions separately. Our ElasticAST uses the identical settings as the standard ASTs in this paper. Additional minor architectural changes are already discussed in Section 2. By default, we use a window of 25 ms with a frame shift of 10 ms to transform waveforms into 128 mel-fbank. The resulting mel-spectrogram shapes for 10-second audio clips are as follows: for AudioSet and VGGSound, 128 x 1024. For variable-length audio clips (in VoxCeleb and Epic-Sounds), the resulting temporal length dimension is different according to the native length of the audios. We adjust the frame shift (Fshift) hyperparameter for each different audio resolutions so that the temporal length dimension changes accordingly. All the results that are used to draw graphs in this paper are available anonymously at https://sites.google.com/view/elasticast-interpseech24.\n3.3. Results of Various Length Training\nWe perform experiments with various natural lengths of audio to assess the potential of ElasticAST. Thus, we use VoxCeleb and Epic-Sounds datasets, which consist of various lengths of audio as shown in Figure 2, for the experiments in this section. For the sake of memory, the maximum length of the audio is limited to 30 seconds (see Section 3.1). ElasticAST is trained at the native length of the audio by packing them into batches of sequences without cutting or padding. Meanwhile, AST models are trained at fixed length by trimming and padding as they can not accommodate varying input lengths during training. We compare the performance of ElasticAST to various AST models in two perspectives. First, We evaluate the performance of ElasticAST by using the native length of the audios in the inference stage without any alteration, while standard ASTs are evaluated with the audio lengths at which they are trained. All of these results are depicted with scattered dots in Figure 4. Second, we evaluate every model at a series of audio lengths $x_i$ from 256 to 3072, such that standard ASTs always apply cutting or padding whenever the native length of the audio is not $x_i$. However, our model only cuts the audio if the native length is longer than $x_i$; otherwise, no padding is applied, and the native audio is processed as is.\nSummary. (1) ElasticAST performs well with various data lengths. In contrast, when AST models are evaluated at a different time length than the ones used during training, their performance collapses. (2) ElasticAST on native length audio can surpass the performance of all standard ASTs by leveraging the full semantic content of audio without the need for cutting or padding (indicated by scattered dots). (3) During inference, with the increasing audio length, more semantics are provided, and ElasticAST can adapt to this change by utilizing a larger semantic volume to boost its performance towards saturation. The more data it can process, the better it performs. However, AST models do not necessarily improve their performance. We hypothesize that the semantic volume that AST can handle is fixed during training. (4) During the training of ElasticAST, no information in the dataset is discarded by trimming, and meanwhile, fewer padded tokens are used, as shown in Table 1. However, the AST model has to trim longer audio or add non-informative padding tokens to fit them into its length requirement. Given the same amount of calculated tokens, ElasticAST consumes a larger meaningful semantic volume. This highlights the resource consumption efficiency of our architecture.\n3.4. Results of Various Temporal Resolutions Training\nThis section presents the results of ElasticAST using various temporal resolutions. We parameterize the temporal resolution through the Fshift (frame shift) value when generating spectrograms . This value determines the length and, consequently, the detail/resolution of the spectrogram. We temporally compress the mel-spectrograms by a factor of $C_i$, where the frame-shift value is multiplied by randomly sampled $C_i$ from $C = \\{1.0, 1.2, 1, 4, ...3.8, 4.0\\}$ during training (mixed-resolution training). Conventional ASTs are trained with a fixed resolution since they cannot accommodate varying input lengths during training. We compare our ElasticAST to standard ASTs during inference across multiple compression rates on VGGSound and AudioSet, as illustrated in Figure 5. The rationale behind choosing these two datasets is that they consist of fixed-length audios, and we convert them into various lengths through different compression rates (Fshift values).\nSummary. (1) Our results show that ElasticAST possesses significant flexibility in handling a wide range of resolutions across both datasets. In comparison, the performance of standard ASTs declines when tested at resolutions different from those used in training. (2) Beyond flexibility, our model generally matches or surpasses the performance of standard ASTs, even on the resolutions for which standard ASTs were specifically trained. (3) Our model allows for the training of a single model adaptable to various resolutions, unlike standard ASTs, which must be trained individually for each resolution. This means that, within a given inference budget, ElasticAST can seamlessly adjust to any computational budget, whereas standard ASTs would require training a new model for each specific computational scenario.\n3.5. Ablation Study\nThe main focus of ElasticAST is to provide flexibility to AST models for using variable time length audio input in both the training and inference stages. In Section 3.4, we present the results of using various temporal resolutions. The time length is changed through the Fshift operation. Additionally, there can be other methods for reducing temporal length . In this ablation study, we investigate average pooling strategy in ElasticAST for using various temporal lengths. By following , a mel-spectrogram is processed through an extra layer of average-pooling. This layer uses a kernel and stride both sized 1 \u00d7 C, effectively reducing the mel-spectrogram's temporal dimension by a factor of C. The training and experimental settings are identical to those described in Section 3.4, except for the new reduction method, average pooling, which requires integer numbers. Therefore, only the values $C = \\{1,2,3,4\\}$ are used as compression rates. To save computational time and resource, we conduct this experiment on VGGSound. The results are shown in the right side of Table 1. Similar to the Fshift approach, ElasticAST also demonstrates flexibility in handling various range of temporal lengths with this method.\n3.6. Analysis on Token Usage\nIn this section we analyze the token usage efficiency of ElasticAST and standard ASTs when various length audios are given during training. Due to fixed-length processing constraints, StandardASTs either cut or pad the inputs. In contrast, ElasticAST flexibly handles audios of varying lengths without resorting to cutting or padding operations. However, as described in Section 2, when packing audios of different lengths into a sequence, we use padding to standardize the length of the token sequence rows, L'. Using the VoxCeleb dataset, our findings are presented in Table 1. The results reveal that StandardAST introduces non-informative padding tokens, which occupy 31.1% of the total training tokens, and cut 15.2% of the tokens in training set. In contrast, ElasticAST minimizes padding and applies no cutting. Moreover, as batch size increases, the amount of padding tokens used for packing decreases. This analysis highlights ElasticAST's training efficiency in processing informative content."}, {"title": "4. Conclusion", "content": "In conclusion, this paper focuses on enhancing Audio Spectrogram Transformers (ASTs) to support training and inference with audios of various lengths. By introducing a strategy that employs mixed-length training and requires only minimal architectural adjustments to the AST framework, we develop ElasticAST. This model is capable of being trained with audios of varying lengths and demonstrates flexibility without performance loss across different lengths, offering one single model for all audio lengths and resolutions. ElasticAST's significance lies in its efficiency and adaptability to different computational budgets without the need for re-training. The ability to handle audio of various lengths is becoming increasingly important, considering the recent developments in self-supervised multimodal learning that utilizes in-the-wild data. ElasticAST's flexibility is particularly valuable for tasks involving alignment, retrieval, and generation in multimodal contexts."}]}