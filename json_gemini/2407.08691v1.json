{"title": "ElasticAST: An Audio Spectrogram Transformer for All Length and Resolutions", "authors": ["Jiu Feng", "Mehmet Hamza Erol", "Joon Son Chung", "Arda Senocak"], "abstract": "Transformers have rapidly overtaken CNN-based architectures as the new standard in audio classification. Transformer-based models, such as the Audio Spectrogram Transformers (AST), also inherit the fixed-size input paradigm from CNNs. However, this leads to performance degradation for ASTs in the inference when input lengths vary from the training. This paper introduces an approach that enables the use of variable-length audio inputs with AST models during both training and inference. By employing sequence packing, our method ElasticAST, accommodates any audio length during training, thereby offering flexibility across all lengths and resolutions at the inference. This flexibility allows ElasticAST to maintain evaluation capabilities at various lengths or resolutions and achieve similar performance to standard ASTs trained at specific lengths or resolutions. Moreover, experiments demonstrate ElasticAST's better performance when trained and evaluated on native-length audio datasets.", "sections": [{"title": "1. Introduction", "content": "Until not long ago, convolution-based neural networks were the prominent approach in both computer vision [1, 2] and audio processing [3]. More recently, transformers [4] have made a significant impact on numerous computer vision [5, 6, 7, 8, 9, 10, 11] and audio processing tasks [12, 13, 14, 15, 16, 17, 18, 19, 20], and CNN-based architectures are being replaced with these attention-based architectures. Despite this replacement, transformers still adhere to the fixed-size input paradigm as CNNS do. Similarly, Audio Spectrogram Transformers (ASTs) [12] take a fixed-size input where the input spectrograms are divided into fixed-size patches to create tokens as input for the transformer encoder. To obtain a fixed-size input, audio spectrograms are either trimmed or padded to a fixed size. Considering transformers can process any sequence length, using varying input sizes rather than fixed ones, can be a more optimal and natural choice for audio processing tasks.\nThis fixed input size paradigm poses several challenges and flaws: (1) Recent datasets, such as VoxCeleb and Epic-Sounds, consist of audio recordings of various lengths (see Figure 2). Considering the recent developments in using in-the-wild data, self-supervised learning, and multimodal learning, having data in various lengths becomes quite natural. (2) Trimming or"}, {"title": "2. Approach", "content": "2.1. Preliminaries\nThe Audio Spectrogram Transformer (AST [12]) utilizes a transformer-based architecture to process audio spectrograms.\n2.2. Architectural changes\nElasticAST is a conceptually simple extension of standard ASTs, designed to accommodate the use of various audio spectrogram lengths during both the training and inference stages. This flexibility is achieved with minimal architectural modifications to conventional ASTs.\nSequence Packing. Unlike AST, which allocates N tokens for all the B input spectrograms, our model employs a sequence packing method that accommodates the varying lengths by organizing the spectrograms into token sequences \\(X \\in \\mathbb{R}^{B' \\times N' \\times D}\\) after patchification as illustrated in Figure 3. This method initiates by sequentially filling the first token sequence row with tokens derived from the samples until reaching the preset token limit per row, L' (as a default set to 2048). If the tokens from a sample about to surpass this limit, they are deferred to the subsequent row, and the allocation process continues accordingly for the remaining samples. This packing technique generates B' rows of token sequences, each potentially varying in length but constrained by L'. To facilitate processing by the transformer encoder, as shown in Figure 3, we introduce minimal padding tokens to each row, standardizing their lengths to N' < L', which corresponds to the longest sequence length among the rows. Our approach to packing is deliberately straightforward, prioritizing simplicity by processing samples sequentially in their original order and filling rows on a first-come, first-served basis. Future work may investigate more sophisticated packing algorithms to enhance efficiency.\nMask Self-Attention. In conventional patch-based transformer models [12, 5], only the tokens from the same sample form a row, and these tokens are aware of each other within the transformer encoder through global attention, formalized as Attention(Q, K, V) = \\(softmax\\left(\\frac{Q K^{T}}{\\sqrt{d}}\\right) \\cdot V\\) where Q, K, V \\(\\in\\) "}, {"title": "3. Experiments", "content": "3.1. Datasets and Evaluation Metrics\nDatasets. In our experiments, we utilize four datasets: AudioSet, VGGSound, VoxCeleb, and Epic-Sounds. AudioSet [26] is a large multi-label dataset with approximately 2 million 10-second clips spanning 527 labels across diverse audio categories. VGGSound [27] includes around 200,000 10-second video clips, labeled with 309 sound classes such as objects and human activities. VoxCeleb [28] is an audio-visual dataset focused on human speech, featuring 1,251 speakers and approximately 145,000 utterances across a range of durations from 4 to 144 seconds. For experimental purposes, we impose a hypothetical upper limit of 30 seconds on this dataset to simplify the experiments and manage the memory usage effectively. Lastly, Epic-Sounds [29], derived from first-person (egocentric) videos, includes 44 categories and a total of 75.9k audio files of various lengths (see Figure 2). Similar to VoxCeleb, we set a provisional maximum duration of 30 seconds for this dataset, though this limit can be adjusted as needed.\nEvaluation metrics. Given the presence of multi-labels in each AudioSet sample, we use mean average precision (mAP) for evaluation across all categories. For the other datasets, we re-"}, {"title": "4. Conclusion", "content": "In conclusion, this paper focuses on enhancing Audio Spectrogram Transformers (ASTs) to support training and inference with audios of various lengths. By introducing a strategy that employs mixed-length training and requires only minimal architectural adjustments to the AST framework, we develop ElasticAST. This model is capable of being trained with audios of varying lengths and demonstrates flexibility without performance loss across different lengths, offering one single model for all audio lengths and resolutions. ElasticAST's significance lies in its efficiency and adaptability to different computational budgets without the need for re-training. The ability to handle audio of various lengths is becoming increasingly important, considering the recent developments in self-supervised multimodal learning that utilizes in-the-wild data. ElasticAST's flexibility is particularly valuable for tasks involving alignment, retrieval, and generation in multimodal contexts."}]}