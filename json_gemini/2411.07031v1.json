{"title": "Evaluating the Accuracy of Chatbots in Financial Literature", "authors": ["Orhan Erdema", "Kristi Hassett", "Feyzullah Egriboyun"], "abstract": "We evaluate the reliability of two chatbots, ChatGPT (4o and o1-preview versions), and Gemini Advanced, in providing references on financial literature and employing novel methodologies. Alongside the conventional binary approach commonly used in the literature, we developed a nonbinary approach and a recency measure to assess how hallucination rates vary with how recent a topic is. After analyzing 150 citations, ChatGPT-40 had a hallucination rate of 20.0% (95% CI, 13.6%-26.4%), while the 01-preview had a hallucination rate of 21.3% (95% CI, 14.8%-27.9%). In contrast, Gemini Advanced exhibited higher hallucination rates: 76.7% (95% CI, 69.9%-83.4%). While hallucination rates increased for more recent topics, this trend was not statistically significant for Gemini Advanced. These findings emphasize the importance of verifying chatbot-provided references, particularly in rapidly evolving fields.", "sections": [{"title": "1. Introduction", "content": "Artificial intelligence (AI) was developed to mimic human cognitive processes, which was different from traditional computer software designs. This type of computer programming involves a new set of challenges, including the phenomenon of \u201challucinations\u201d created by AI. These hallucinations refer to outputs from large language models (LLMs), a specialized subset of AI, that are not based on factual data. Although (Maleki et al., 2024) reviewed fourteen databases and found no consensus on the precise definition or characterization of AI hallucination, researchers have documented various instances of AI chatbots generating nonfactual data.\n(Chen & Chen, 2023) studied hallucinations in healthcare and classified the journal as fake if any of the components, title, authors, publication year, volume, issues, or pages were not accurate. They found that GPT 3.5 had a 98.1% hallucination rate and that GPT-4 had a 20.6% hallucination rate. They showed that broader topics had fewer hallucinations than narrow topics. Other papers took a different approach to differentiating between real vs hallucination. (Chelli et al., 2024) used databases that contained randomized trials for rotator cuff pathology published between 2020 and 2021. They found hallucination rates of 39.6% for GPT-3.5, 28.6% for GPT-4 and 91.4% for Gemini\u00b9. While they researched medical papers in two specific years, we are researching finance topics without a date restriction, our results are similar. (Magesh et al., 2024) labeled responses as \u201ccorrectness\u201d or \u201cgroundedness.\u201d Correctness was determined if the response was both real and relevant. Groundedness was determined by the quality of the output according to legal case law. The authors included a degree of hallucination within these types to include partial hallucinations. The hallucination rates varied with both Lexis+ AI and Ask Practical Law AI at 17%, Westlaw at 33% and ChatGPT at the highest hallucination rate of 43%.\nLLMs are often able to create output faster than humans. (Nazzal et al., 2024) selected two medical topics about the skeleton and had three groups of authors writing on each topic for a total of eight papers. The author groups included human, AI, and AI with citations from human papers. They found that while AI reduces writing time, it often lacks accuracy and may lead to plagiarism. The findings underscore that AI can expedite the initial drafting process but requires substantial human editing to ensure accuracy and originality. Unlike in our study, (Gao et al., 2023) had ChatGPT create fake scientific abstracts, and human reviewers categorize the abstracts as either real or fake. They identified the abstracts created by AI 68% of the time. They also incorrectly labeled the AI-generated abstracts as real 14% of the time. (MIT Sloan, 2023) succinctly identified three reasons why hallucinations could occur, including the need for additional training data, the idea that AI was designed to predict and find patterns, and the fact that it was not designed to distinguish between real and fake information. These reasons for hallucinations are not straightforward solutions for reducing hallucination rates.\nA possible solution was proposed by (Xu et al., 2023) to reduce hallucinations in language translation. They studied automated translation from one language to the next in a \u201cneutral\u201d way.\nThey proposed a solution to reducing hallucinations by intentionally prompting with a small inconsistency in the text. They found that identifying a hallucination was more effective than comparing the text to the generated output. This method of tricking AI into correcting the prompt might not work for all industries but is worthy of investigating. (Nature Medicine Editorial, 2023) stated that the use of LLMs can assist in the medical field, but caution should be taken when relying on AI. Like in our study in the finance literature, they suggest relying on LLMs to perform diagnoses without the use of human expertise and logic. There have been many studies on hallucinations, each using a different definition, technique and approach. Our study evaluated the pieces of each response to determine the degree of hallucination as well as other anomalies we found, such as the year of the article.\nIn a study that closely resembles our study, (Walters & Wilder, 2023) prompted ChatGPT-3.5 and ChatGPT-4 to create papers with 2000 words and five citations per paper. The prompts used 42 topics with a broad range of subjects. Their purpose was to identify \u201cfactually incorrect responses (hallucinations)\u201d and errors in the APA citations. Their results showed that GPT-3.5 had a higher rate of 55% hallucination, while GPT-4 had 18%. Among the non-hallucinated responses GPT-3.5 had a higher error rate in the citation of 43% and GPT-4 had 24%. While their prompts could be considered more detailed by asking the chatbot to \u201cact as\u201d a researcher, write a short academic paper and provide citations, their hallucination rate for GPT-4, 18%, is similar to ours, 20%. Potential differences in hallucination rates could be related to age and type of topic. In this study, we mainly focus on the financial literature (with a wide range of dates) rather than a broader array of topics. We also explored the effect of the recency of the topic on the hallucination rate.\nVery few papers in the economics and finance literature have discussed hallucinations. In one of these papers, (Buchanan et al., 2024) created a similar study to this paper by using ChatGPT-3.5 and GPT-4 to query economic topics from the Journal of Economic Literature. They used three prompts, each with different levels of detail. The authors determined that the hallucination rate for ChatGPT-3.5 exceeded 30%, while ChatGPT-4 generated incorrect citations at a rate higher than 20%. Their results correspond to our scale mentioned below. Our topics are focused on financial literature than on general economics (for example, \u201ceconomic theory\u201d as a general theme versus \u201cincentives\"). The authors added that hallucinations grew in both GPT-3.5 and GPT-4 when the prompts were more specific. This result contrasts with the results of Chen et al., where broader topics had fewer hallucinations than narrow topics.\nOur paper distinguishes itself from the aforementioned studies in several keyways. First, we provide a comparative analysis of the outputs from three of the most recent chatbot models: ChatGPT-40, 01-preview, and Gemini Advanced (Gemini A., hereafter). ChatGPT-40 was released on May 13, 2024, Gemini A. was released on May 14, 2024, and ol-preview on September 14, 2024. Consequently, this study is among the first, if not the earliest, to document the performance of the o1-preview model. According to Open AI 01-preview, which performs better than ChatGPT-4o in many tasks, \u201cOn one of our hardest jailbreaking tests, GPT-4o scored 22 (on a scale of 0-100), while our ol-preview model scored 84.\u201d Moreover, while prior research predominantly investigates medical literature, our focus lies on finance literature. Additionally, we introduced two novel metrics to measure hallucination. The first extends beyond a simple binary scale by incorporating degrees of hallucination, whereas the second, a 'recency measure', quantifies the variance in hallucination depending on the \u201cage\u201d of the topic.\""}, {"title": "2. Data", "content": "In this study, we tasked three of the most recent chatbots, ChatGPT-40, 01-preview, and Gemini A., by providing ten unique references for fifteen topics in finance.\nAfter reviewing the financial literature classification via JEL codes, we determined that these fifteen topics span the bulk of finance literature. Thus, the chatbots each listed a total of 150 references. The following is a prompt we gave for the first topic (see Table 1):\n\u201cPlease provide a literature review of 10 unique references from journal articles that investigate the effects of financial incentives on decision-making. \u201c\nFor the remaining fourteen topics, we modified the underlined segment of the prompt accordingly. We then utilized multiple databases and other web searches to verify the authenticity of the listed papers. The data were collected from August 1, 2024, to September 14, 2024."}, {"title": "3. Methodology", "content": "3.1 Hallucination metrics\nWe verified each cited journal article by confirming its existence in the referenced journal and by searching its title using Google Scholar, and publisher websites. We checked the article's title, journal name, authors, and publication year. We labeled an entry \u201challucination\" if any of these four features did not match reality. To quantify the degree of hallucination, we used two measures:\na. Scale 1: Any article that failed the verification process as described above was classified as hallucination, assigning a score of 1 (hallucination) or 0 (real) to each reference.\n$s^{(1)} = \\begin{cases}1 & \\text{if the citation hallucinated} \\\\ 0 & \\text{if the citation is real}\\end{cases}$\nb. Scale 2: Articles were also categorized into four categories based on the degree of hallucination: 25%, 50%, 75%, and 100%, depending on how many of the four key elements (article title, journal name, authors, publication year) were determined to be inaccurate.\n$s^{(2)} = \\frac{m}{4} \\text{ if m elements are hallucinated (m=0,..,4)}$\nwhere m=0 corresponds to the real citation.\nWe employed a decision tree, as depicted in Figure 1, to evaluate the accuracy of each of the m elements within a citation: [Title, Author(s), Journal, Date]. The initial step involved checking the title. If the title is verified as existent, it becomes the reference point, or \"stem,\" for subsequent checks against the Author(s), Journal, and Date. In this scenario, any configuration beginning with [0,...] is possible, where '0' indicates the presence of a real title and subsequent zeros or ones represent the accuracy of the other elements in varying combinations. If the title is nonexistent, the tree advances to check the author(s). Should the Author(s) be verified, Author(s) then serves as the stem, and the tree proceeds to assess the Journal and Date. Any positive result here starts with a '1' (for the hallucinated title) followed by a '0' (for real Author(s)), and so forth. If neither the title nor the author(s) are confirmed, the citation is considered invalid, resulting in a configuration of [1,1,1,1] (all hallucinated), signifying the absence of all core components (Node 5 in Figure 1). This decision tree ensures a systematic and sequential assessment of the citation's validity.\nTo ensure a reliable error rate, we selected 15 topics and instructed each chatbot to generate 10 references per topic. This process resulted in a total of 150 references for each chatbot. We assessed the hallucination rates by averaging these rates across the 150 citations for each of the three measurement scales. Consequently, we calculated two hallucination rates for each chatbot as follows:\nFor Scale 1: $H^{(1)} = \\sum_{i=1}^{15} \\sum_{j=1}^{10} s_{ij}^{(1)}/150$\nFor Scale 2: $H^{(2)} = \\sum_{i=1}^{15} \\sum_{j=1}^{10} s_{ij}^{(2)}/150$\nwhere i=1,...,15 denotes the topics, and j=1,...10 represents the ten references provided for each topic. By construction $H^{(2)} \\leq H^{(1)}$, for all chatbots."}, {"title": "3.2 Assessing the Effect of Topic Recency on Hallucination", "content": "To test the relationship between topic recency and hallucination rates, we created a recency metric, Recency\u012f, and categorized the topics into \u201cold\u201d and \u201cnew\u201d based on this metric. We then investigated the hallucination rates across these categories.\nFirst, we compute the average age of any topic i as follows:\nAge (chatbot k)\u2081 = $\\frac{\\sum_{j=1}^{n} Age(chatbot k)_{i,j}}{n}$\nwhere k=1, 2, and 3 refer to each of the three chatbots (ChatGPT-40, 01-preview, and Gemini A., respectively). Age(chatbot k)i,j represents the age of the real paper j cited in topic i, and n is the total number of real references for topic i. For instance, if ChatGPT-40 cites three real papers from 2018, 2019, and 2020 for topic 2, then considering the current year is 2024, the age of that specific topic is Age(ChatGPT)Topic 2 = $\\frac{6+5+4}{3}$ = 5. If all 10 citations for a particular topic i are hallucinated (which is the case with Gemini A. citations for topics 3, 6, 7, 8, 9, 11, and 15), then Age(Gemini); for those references is undefined.\nThus, to calculate the recency of a topic, we rely solely on real citations. The average age for topic i, incorporating all chatbots, is given as follows:\nAverage agei = $\\frac{\\sum_{k=1}^{m} Age(chatbot k)_i}{m}$ \u00b7 for all well defined m chatbots with m \u2264 3\nFor example, for topic 3, all of the citations provided by Gemini A. are hallucinated; thus, Age(Gemini)3 is undefined. Therefore, we have m=2 well-defined chatbots. Thus, Average age 3 $\\frac{\\sum_{k=1}^{m} Age(chatbot k)_3}{2}$\nIn our experimental setting, Age(ChatGPT)\u012f has been well defined since there is at least one real (non-hallucinated) citation for all i. As previously mentioned, this is not the case for Gemini A. Therefore, when Age(Gemini); is undefined for topic i, Average age\u00a1 is calculated solely using the well-defined ages from other chatbots.\nFinally, the recency of any topic is assessed based on whether Average age is less than 10 years. Topics with an average age of less than 10 years are considered \u2018New', while those with an average age of 10 years or more are defined as \u2018Old':\nRecencyi = $\\begin{cases} New & \\text{if Average age\u00a1 < 10} \\\\ Old & \\text{Otherwise} \\end{cases}$\nThis recency measure helps us classify whether our 15 topics are new."}, {"title": "4. Results", "content": "Hallucination Rates\nWe first start by presenting the average hallucination rates of the three chatbots\u2014ChatGPT-40, ChatGPT-01-preview, and Gemini A.\u2014across 15 instances and two different scales, as shown in Figure 2.\nBy analyzing the response rates of the chatbots, we found that ChatGPT-4o had a response rate of 20.0% on Scale 1 (95% CI, 13.6%-26.4%). The hallucination rate for the ol-preview method was slightly higher at 21.3% (95% CI, 14.8%-27.9%). In contrast, Gemini A. exhibited a significantly greater percentage (76.7%) on the same scale (95% CI, 69.9%-83.4%).\nAdditionally, on Scale 2, ChatGPT-4o exhibited a hallucination rate of 10.8% (95% CI, 4.6%-13.9%), while ol-preview displays 11.0% (95% CI, 6.0%-16.0%), and Gemini A. had a hallucination rate of 52.7% (95% CI, 44.7%-60.7%). The lower rates under Scale 2, H(2), are due to the scale's allowance for partial hallucinations, in contrast to the rates under Scale 1, H(1), which categorizes any hallucination as 100%.\nThese results, together with the confidence intervals, reveal two main results. First, we cannot reject the hypothesis that ChatGPT-4o and o1-preview have the same hallucination rate. Second, Gemini A. has a higher hallucination rate than both versions of the ChatGPT.\nRecency\nWe further wanted to test whether the incidence of hallucination varied with the recency of the field. The recency of topics is given in the \u201cRecency\" column of Table 2. For instance, in reference to \"Topic 8: Cybersecurity in Financial Services,\u201d which is classified as a \u201cNew\u201d topic, the highest hallucination rates for chatbots were 80%, 80%, and 100% on Scale 1 for ChatGPT-40, 01-preview and Gemini A., respectively. In contrast, \"Topic 1: Financial incentives,\u201d identified as an \u201cOld\u201d topic, had the lowest hallucination rates, with no hallucinations reported on either scale for all chatbots. To explore this further, we formulate a set of hypotheses, with the corresponding test results given in Table 1.\nH.1.a: H(1)(ChatGPT \u2013 40, Recency = New) = H(1)(ChatGPT \u2013 40, Recency = Old)\nH.1.b: H(2) (ChatGPT \u2013 40, Recency = New) = H(2)(ChatGPT \u2013 40, Recency = Old)\nThe second set of hypotheses belongs to the o1-preview:\nH.2.a: H(1) (01 \u2013 preview, Recency = New) = H(1)(01 \u2013 preview, Recency = Old)\nH.2.b: H(2)(01 \u2013 preview, Recency = New) = H(2)(01 \u2013 preview, Recency = Old)\nThe last set belongs to Gemini A:\nH.3.a: H(1)(Gemini A., Recency = New) = H(1)(Gemini A., Recency = Old)\nH.3.b: H(2) (Gemini A., Recency = New) = H(2)(Gemini A., Recency = Old)\nThe first two hypotheses (H.1.a and H.1.b) assert that the recency of the topic (being new or old) has no impact on the hallucination rates of ChatGPT-40, evaluated across two different metrics ($H^{(1)}$ and $H^{(2)}$). Similarly, the third and fourth hypotheses (H.2.a and H.2.b) claim that the recency of the topic has no impact on the hallucination rates of o1-preview-4o, evaluated across two different metrics. Finally, the fourth and fifth hypotheses (H.3.a and H.3.b, respectively) posited that the recency of the topic does not affect the hallucination rates of Gemini A., which were also assessed across two metrics. Even though hallucination rates are higher for newer topics than for older topics across all scales and chatbots, as shown in Table 1, statistical tests reveal nuanced differences. The results of t tests performed using the ChatGPT-40 and 01-preview models differed clearly from those conducted with the Gemini A. model. According to the ChatGPT-40 and o1-preview analyses, the findings are statistically significant at the 10% level in three out of four cases, with p values of 0.14 and 0.08, respectively. However, the significance decreases at the 5% level, with only one case remaining significant.\nSpecifically, while the hallucination rates for ChatGPT-4o are higher in newer topics, the p values are 0.14 and 0.08 for scales 1 and 2, respectively. Consequently, hypothesis H.1.a is not rejected at the 10% significance level, while H.1.b is. Neither hypothesis is rejected at the 5% level, indicating an ambiguous difference in hallucination rates between older and newer topics.\nT tests conducted with ol-preview yield clearer, albeit not strong, results: While hallucination rates for newer topics are significantly greater than those for older topics under scale 1 at the 5% significance level (p value = 0.04), they do not reach statistical significance under scale 2 (p value = 0.07). Thus, both hypotheses are rejected at the 10% level; however, hypothesis H.2.a is not rejected, while H.2.b is rejected at the 5% level.\nThe t-tests conducted using the Gemini A. model produced definitive results. Hypotheses H.3.a and H.3.b cannot be rejected, with p values of 0.62 and 0.11, respectively. Thus, we can safely say that there is no difference in Gemini A.'s hallucination rates between new and old topics. This might be due to the consistently high hallucination rates of Gemini A., which obscures any discernible effects of topic recency."}, {"title": "5. Conclusion and Discussion", "content": "Many researchers have documented that chatbots hallucinate. In this study, we aimed to perform several tasks. First, we compared the hallucination rate of a newly released chatbot, 01-preview, with that of ChatGPT-4o and Gemni Advanced. Second, we propose several new metrics for hallucination studies: one is a nonbinary metric, and the other is a recency index. We test the performance of these metrics with the aforementioned three chatbots. For the first task, we asked ChatGPT-4o, 01-preview, and Gemini Advanced to provide ten unique references for fifteen topics in finance. By analyzing 150 citations given by each chatbot and using two different metrics, our findings yielded several results. First and foremost, we documented the hallucination rates of three chatbots on binary and nonbinary scales. The hallucination rate of ChatGPT-4o under the binary metric (hallucination or no-hallucination) was 20.0% (95% CI, 13.6%-26.4%), and that under the binary metric was 10.8% (95% CI, 4.6%-13.9%). The hallucination rate of the o1-preview under the binary metric was 21.3% (95% CI, 14.8%-27.9%), and that under the binary metric was 11.0% (95% CI, 6.0%-16.0%). Thus, the o1-preview, which was released by the same company, Open AI, for the purpose of improving ChatGPT-4o does not yield better results than ChatGPT-40. In regard to Gemini Advanced, which was developed by Google, the results are even worse. The hallucination rate for Gemini Advanced under the binary metric was 76.7% (95% CI, 69.9%-83.4%). With respect to the nonbinary metric, it decreased to 52.7% (95% CI, 44.7%-60.7%). Taken together, these findings provide our first broader result: Hallucinations occur significantly more frequently with Gemini Advanced than with either the ChatGPT-40 or 01- preview.\nWe further observed that the incidence of hallucination varied with the recency of the field. Even though hallucination rates are higher for newer topics than for older topics across all scales and chatbots, as shown in Table 1, statistical analyses reveal more subtle distinctions. The tests performed with OpenAI's ChatGPT-40 and o1-preview models are statistically significant at the 10% level in three out of four cases, but this significance diminishes at the 5% level, with only one out of four cases remaining significant. However, for Gemini Advanced, although a similar trend\u2014higher hallucination rates in newer topics\u2014is observed, it is statistically insignificant. This insignificance may primarily stem from the consistently high hallucination rates of Gemini Advanced, which may mask any potential effects of topic recency.\nResearchers must exercise caution when using chatbots for citing financial literature, particularly when discussing newer topics. Gemini Advanced had significantly higher and more frequent hallucination rates than OpenAI ChatGPT-40 and 01-preview. This variability in accuracy underscores the importance of thorough verification of chatbot-provided references, especially in rapidly evolving fields."}, {"title": "Declaration of competing interest", "content": "The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper."}, {"title": "Declaration of generative AI and AI-assisted technologies in the writing process", "content": "During the preparation of this work, the author(s) used ChatGPT and Gemini Advanced to assist with drafting the initial content and language refinement. After using this tool/service, the author(s) reviewed and edited the content as needed and take(s) full responsibility for the content of the publication."}]}