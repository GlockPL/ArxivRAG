{"title": "Small Language Model as Data Prospector for Large Language Model", "authors": ["Shiwen Ni", "Haihong Wu", "Di Yang", "Qiang Qu", "Hamid Alinejad-Rokny", "Min Yang"], "abstract": "The quality of instruction data directly affects the performance of fine-tuned Large Language Models (LLMs). Previously, (Li et al., 2023c) proposed NUGGETS, which identifies and selects high-quality quality data from a large dataset by identifying those individual instruction examples that can significantly improve the performance of different tasks after being learnt as one-shot instances. In this work, we propose SuperNUGGETS, an improved variant of NUGGETS optimised for efficiency and performance. Our SuperNUGGETS uses a small language model (SLM) instead of a large language model (LLM) to filter the data for outstanding one-shot instances and refines the predefined set of tests. The experimental results show that the performance of SuperNUGGETS only decreases by 1-2% compared to NUGGETS, but the efficiency can be increased by a factor of 58. Compared to the original NUGGETS, our SuperNUGGETS has a higher utility value due to the significantly lower resource consumption.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have demonstrated excellent performance on a wide range of Natural Language Processing (NLP) tasks by scaling model size and datasets (OpenAI, 2023; Google, 2023; Bai et al., 2023; Cheng et al., 2024) . Fine-tuning LLMs can further enhance the utility of these models by enabling them to better follow human instructions. This process usually involves supervised fine-tuning of input-output pairs, also known as instruction fine-tuning. This kind of fine-tuning not only awakens the knowledge acquired by the model during the pre-training phase, but also allows the model to interact with humans in a more natural conversational form.\nCurrently, much research (Chung et al., 2022; Wang et al., 2022b, 2023) is devoted to optimiz-"}, {"title": "SuperNUGGETS", "content": "Motivation NUGGETS (Li et al., 2023c) utilizes one-shot learning to filter out high-quality instruction data from a large amount of instruction data, achieving excellent data prospecting results. However the original NUGGETS method requires calculating the one-shot score and zero-shot score for each piece of data, and the original size of the predefined task set is 1,000 pieces. To filter Alpaca's 52k pieces of data requires the model to inference a total of 52,002 (zero-shot) + [52,002 \u00d7 1,000] (one-shot) = 52,054,002 times. Using LLM to inference 104 million times is very time costing as well as a big resource drain. In addition, the predefined task set in the original NUGGETS method is obtained by random sampling, which will inevitably contain some low-quality data, which will directly affect the correctness of the subsequent calculation of the gold score. Therefore, to address the above limitations and problems, we propose SuperNUGGETS, an enhanced version of NUGGETS."}, {"title": "Predefined Task Set Refinement", "content": "The number of the alpaca dataset is 52,002, and our first step is to start with the reward model reward-model-deberta-v3-large-v2, to score the overall data. Then the top 10,000 data are filtered based on the score, of which the top 20 are taken separately as a high quality subset, this step is to ensure the high quality of the data. The second step encodes the first 20-10,000 data to obtain semantic vectors, which are clustered using the kcenter_greedy algorithm. Specifically, an initial centroid is selected from the 20-10,000 dataset, usually the data point furthest from the other centroids. The data point furthest from the current set of centroids is then iteratively selected as the new centroid to ensure broader coverage. Finally the point furthest from the current set of centroids is iteratively selected, which ensures that the selected data is as dispersed as possible, covering all aspects of the instruction types, ensuring diversity and coverage of the selected instruction data. This step selects 80 examples from 20-1,000 data, and finally combines the 20 examples from the first step with the 80 examples from the second step to form a refined predefined test set containing 100 examples."}, {"title": "SLM as Instruction Data Prospector", "content": "With an instruction tuning dataset D, we aim to identify a set of examples $D_{gold}$ that are most closely aligned with the golden instructions. Like the original NUGGETS (Li et al., 2023c) method, we first need to calculate the zero-shot score for refined predefined task set. The predefined test set after the previous refinement encompasses a variety of m tasks, where each task is structured as {Task (T), Answer (A)}. Each token in Task or Answer is denoted as $t_k$ or $a_k$. Let SLM denote the instruction data prospector we use. For the j-th task represented by $T_j$, the probability of zero-shot inference by the data prospector can be calculated by continuously predicting the next tokens based on the given task and the preceding words:\n$\\frac{1}{L} \\sum_{i=1}^L \\log p(t_k) | InP_{zero}; SLM),$\n$In P_{zero} = [T_j, t_1, t_2, ..... t_{L}],$\nwhere L is the number of tokens of the ground-truth answer A. The score $s_{zero}$ is used to denote the competence level of the SLM on the jth task. A higher $s_{zero}$ denotes superior model performance on the j-th task, whereas a lower $s_{zero}$ implies inferior performance. Therefore, we can acquire the data prospector's performance across m tasks as:\n$S_{zero} = [s^{zero}_{1}, s^{zero}_{2},..., s^{zero}_{m-1}, s^{zero}_{m}].$\nFor each example $z_k = {IQ_k, IA_k}$, we initially perform one-shot learning on the base model using that specific example. Here, $IQ_k$ denotes the question associated with the k-th example $z_k \u2208 D$, while $IA_k$ signifies its corresponding answer. Subsequently, we employ the model with in-context learning to conduct another round of testing on the tasks within the predefined task set. That is,\n$S_{one}(Z_k) = \\frac{1}{L} \\sum_{i=1}^{L} log p(w | In P_{one}; SLM),$\n$In P_{one} = [T_j, (IQ_k, IA_k), w_1, w_2, ..., w_L],$\nwhere $(IQ_k, IA_k)$ can be considered one-shot prompt. Similarly, we can obtain the performance of the model after implicit fine-tuning across m different tasks:\n$S^{k}_{one} = [S^{one}_{1}(z_k), S^{one}_{2}(z_k),..., S^{one}_{m}(z_k)].$\nWe use Golden Score (GS) to reflect the score of our data prospector SLM for that instruction data. The GS of the example zk is calculated as\n$GS(z_k) = \\frac{1}{m} \\sum_{i=1}^{m}I [S^{one}(Z) > S^{zero}] \u2208 [0, 1],$\nwhere I[\u00b7] is the indicator function. The GS measures the increment of performance improvement of the model after one-shot learning through the given instruction. Finally, we use GS to filter the data to get the top n% of datasets $D^{n%}_{gold}$ with the highest GS as appropriate. Using SLM prospecting to get $D^{n%}_{gold}$ can be used to fine-tune the LLM."}, {"title": "Experiment", "content": "As with (Li et al., 2023c), we chose Alpaca as the instruction dataset to be used for data filtering. This dataset is pivotal within the open-source sphere for the purpose of instruction tuning. It was created using the self-instruct (Wang et al., 2022a) technique, which extracts instruction data from text-davinci-003. The dataset's effectiveness in refining the LLaMA model has catalyzed a wave of research into the realm of instruction fine-tuning (Li et al., 2023a; Ji et al., 2023; Xu et al., 2023).\nSame as the original NUGGETS (Li et al., 2023c), we compare the responses generated by the model with those generated by the davincici -003 model, using the well-established Alpaca-Eval dataset (Li"}, {"title": "Experimental Results", "content": "As shown in Table 1, we use Opt-125m, Opt-350m, and Llama2-7B as data prospectors, respectively, and the predefined test set is the refined 100 data. The results of model performance over using 100% data (52,002) fine-tuning are bolded in the table. From the experimental results, it is evident that our SuperNUGGETS filtered data using only the top 5% exceeds the effect of fine-tuning the model out of 100% of the data. We found that the model trained on top 5% of the data obtained using Opt-350m (20 times smaller than Llama2-7B) as the data prospector also achieves a score of 23.98, which is much higher than the model fine-tuned on 100% of the full amount of data. Even the model trained with TOP 5% of the data obtained using Opt-125m (56 times smaller than Llama2-7B) as the data Prospector achieves a score of 22.11, which is much higher than the model fine-tuned with 100% of the full amount of data. All three models Opt-"}, {"title": "Ablation Study", "content": "While the original NUGGETS used 1,000 random data as a predefined task test set, our SuperNUGGETS uses a refined set of 100 data, which makes the number of computations 10 times smaller. As shown in Table 2, using the refined 100 data as the predefined task test set is far better than randomly selecting 100 data, regardless of which model the data prospector is. We found that the effect of the refined 100 data was even similar to that of the randomly filtered 1,000 data. The above experimental results illustrate the validity letter of our refinement of the predefined task test set."}, {"title": "Conclusion", "content": "Previously, (Li et al., 2023c) proposed NUGGETS, which identifies and selects high-quality data from large datasets through the effect of one-shot learning. In this work, we propose SuperNUGGETS, which is an NUGGETS improved variant optimized for efficiency and performance. Our SuperNUGGETS uses a small language model (SLM) instead of a large language model (LLM) to filter unprocessed single instance data and refines a predefined test set. Experimental results show that SuperNUGGETS is only 1-2% less performant than NUGGETS, but 58 times more efficient. Compared to the original NUGGETS, our SuperNUGGETS has a much higher utility value because of the significantly lower resource consumption."}, {"title": "Limitations", "content": "Due to funding and resource constraints, full-parameter fine-tuning was not carried out for models at scales above 7B. The performance of the filtered high-quality data on larger scale models is unknown."}, {"title": "A Case Study", "content": "To qualitatively evaluate SuperNUGGETS, we also selected some example instructions from the Alpaca dataset for a case study, as shown in Figure 3. We observe that instructions with very short and meaningless outputs give low gold scores for all three different sizes of data prospectors. In contrast, instructions with high gold scores are usually linguistically fluent, logically well thought out, have complete output, and are oriented towards helping humans solve problems."}]}