{"title": "Few-Shot Joint Multimodal Entity-Relation Extraction via Knowledge-Enhanced Cross-modal Prompt Model", "authors": ["Li Yuan", "Yi Cai", "Junsheng Huang"], "abstract": "Joint Multimodal Entity-Relation Extraction (JMERE) is a challenging task that aims to extract entities and their relations from text-image pairs in social media posts. Existing methods for JMERE require large amounts of labeled data. However, gathering and annotating fine-grained multimodal data for JMERE poses significant challenges. Initially, we construct diverse and comprehensive multimodal few-shot datasets fitted to the original data distribution. To address the insufficient information in the few-shot setting, we introduce the Knowledge-Enhanced Cross-modal Prompt Model (KECPM) for JMERE. This method can effectively address the problem of insufficient information in the few-shot setting by guiding a large language model to generate supplementary background knowledge. Our proposed method comprises two stages: (1) a knowledge ingestion stage that dynamically formulates prompts based on semantic similarity guide ChatGPT generating relevant knowledge and employs self-reflection to refine the knowledge; (2) a knowledge-enhanced language model stage that merges the auxiliary knowledge with the original input and utilizes a transformer-based model to align with JMERE's required output format. We extensively evaluate our approach on a few-shot dataset derived from the JMERE dataset, demonstrating its superiority over strong baselines in terms of both micro and macro F1 scores. Additionally, we present qualitative analyses and case studies to elucidate the effectiveness of our model.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent focus has intensified on two pivotal subtasks for building a multimodal knowledge graph: Multimodal Named Entity Recognition (MNER) [17, 37, 42, 46, 53] and Multimodal Relation Extraction (MRE) [49, 50, 52]. These tasks aim to leverage user-generated content from social media, primarily composed of images and text, to generate structured knowledge. However, previous works treated these tasks independently and restricted the exploration of their interconnections [37, 42, 49, 50, 52, 53]. To address this problem, Yuan et al. [45] introduced a Joint Multimodal Entity-Relation Extraction (JMERE) task, aiming to enhance performance by integrating bidirectional interactions between the two subtasks. JMERE involves simultaneously extracting entities and their potential relations. However, previous JMERE work usually extracted features separately using pre-trained models (e.g., BERT for textes and ResNet for images), and employed multi-modal fusion layers for interaction, which typically require substantial training data [43]. Nevertheless, extensive multimodal data collection and annotation demand significant time and labor investments [54]. Furthermore, in real-world applications, access to labeled data is often constrained, and diverse domains necessitate specific datasets, presenting a challenge for data collection efforts. Thus, we first explore the JMERE task in a few-shot setting, aiming to advance the practical applications of multi-modal information extraction. To our knowledge, we are the first to focus on JMERE in a multimodal few-shot scenario (FS-JMERE).\nHowever, in few-shot setting, aligning different modalities and conducting information extraction between modalities pose heightened challenges compared to full datasets setting [43]. For instance, as illustrated in Fig. 1, discerning the relationship between TV3 and MCMC based solely on the concise text \"RT @saladinMY: TV3 with the MCMC.\" proves challenging, despite supplementary images hinting that both entities may belong to an organization such as television stations. An intuitive approach involves utilizing a large number of external Twitter data for pre-training, which can enhance the model's capacity for information extraction and relevant information storage [43, 44]. However, this approach requires extensive pre-train datasets and corresponding training times. Furthermore, some unimodality approaches aim to extract relevant information"}, {"title": "2 RELATED WORK", "content": "2.1 Multimodal Information Extraction\n2.1.1 Multimodal Information Independent Extraction. Previous works [16, 19, 31, 47] used RNNs for text encoding and CNNs (e.g., VGG, Resnet) [8, 24, 48, 51?] for unified image vector representation. For example, Lu et al. [16] proposed a gated network with CRFs regulating image-text interaction, and Zhang et al. [47] used bidirectional LSTM with attention to image-text alignment. However, such a method encoded the entire image as a vector and didn't distinguish different object types. Thus, recent studies use\nJoint Entity-Relation Extraction Task (JMERE) to capture implicit connections between MNRE and MRE. This work also proposed word-pair relation tagging to extract all elements of JMRER in a single step, reducing error propagation.\nIn this paper, we focus on the investigation of joint multimodal entity-relation extraction in the few-shot setting. This choice aims to align with real-world applications that frequently offer only a restricted number of labeled data. We are the first to focus on JMERE in a multimodal few-shot scenario. Besides, We propose a knowledge-enhanced cross-modal prompt model to address the few-shot scenario, utilizing dynamic prompts and knowledge reflection, to obtain background knowledge from LLMs for improved performance FS-JMERE.\n2.2 Few-shot unimodal Information Extraction\nEstablished few-shot learning approaches have been applied in unimodal information extraction (e.g., Named Entity Recognition (NER) [4, 28, 35, 40] and Relation Extraction (RE) [5, 6, 14, 36]). The works in Few-shot Information Extraction (FS-IE) can be divided into two categories: the first category uses limited data for supervised training. Fritzler et al. [4] applied prototype networks [25] to address the few-shot NER (FS-NER) task. Inspired by nearest-neighbor inference in few-shot learning [35], Yang et al. [40] employed supervised feature extractors for FS-NER. Within the few-shot RE (FS-RE) task, Gao et al. [5] introduced attention mechanisms to augment prototype networks [25], and Han et al. [6] incorporated relationship description information as a means of distinguishing between complex and straightforward tasks. Liu et al. [14] incorporate relationship description with support sample representations, resulting in notably superior results. Nevertheless, this approach fails to address the issue of insufficient information in the few-shot setting.\nThe second category focuses on pre-training or integrating external information to address the challenges of the few-shot setting. In the FS-NER task, Huang et al. [9] proposed a specialized"}, {"title": "3 OUR PROPOSED MODEL", "content": "Following the JMERE task setting, the i-th input sample comprises a sentence denoted as X and a corresponding image represented as X. The JMERE task aims to extract entities, including their respective types, while also determining the relationships that exist between these identified entities. The resulting output is structured as a collection of quintuples represented by Y = {(e1, t1, e2, t2, r) c}C=1, where (e1, t1, e2, t2, r)c pertains to the c-th quintuple comprising distinct textual entities e1 and e2. These entities are associated with their respective entity types t\u2081 and t2, as well as the relationship denoted by r between the two entities within the task context.\nFig. 3 presents the overall architecture of the proposed method, which comprises a conceptually straightforward two-stage framework. The Knowledge Ingestion stage is the crucial part of our proposed method. By constructing a cross-modal heuristic prompt, we use this prompt to guide ChatGPT in acquiring initial predictions and background knowledge related to the JMERE sample. In the subsequent step, we aim to integrate additional supplementary knowledge into the language model, such as T5, to enhance the model's capacity to extract entities and relations between entities. We combine the aforementioned knowledge to augment the input text. This augmentation involves integrating type-based prompts and structured image descriptions as inputs to the language model, thereby further enhancing its capabilities. It is worth noting that we did not utilize visual object features (e.g., Mask R-CNN) because we found that incorporating these features did not lead to performance improvement. Detailed experimental setups and descriptions can be found in Section 3.7 Case Studies.\n3.1 Stage-1: Knowledge Ingestion stage\nThis stage aims to generate auxiliary background knowledge from ChatGPT relevant to the given text and image, enhancing the downstream model's capacity to comprehend the context. This stage is divided into three steps: Human Prompts, Prompt Selection, and Knowledge Reflection. Subsequently, we describe each step in detail.\n3.1.1 Human Prompts. To select suitable prompts for a given sample, we initially devised a human prompt set P = {p\u00b9, p2, ... pM} specifically designed for the JMERE task, where M represents the number of predefined human prompt sets. Each human prompt pm includes not only the original text prm and its corresponding image description pm, but also encompasses two distinct external knowledge components, as illustrated in Figure 3 (a): the first component pm delineates the entities and their corresponding relationships within the sentence, customized for the JMERE task. The second component p provides background knowledge related to the text and images, with relevant background knowledge to support entity-relationship predictions. This knowledge and reasoning enhance the credibility of information extraction.\n3.1.2 Prompt Selection. To ensure the relevance of prompts to the given sample, we developed a multimodal prompt-aware module to select appropriate examples from the human prompt set. As an essential component of multimodal knowledge extraction, the JMERE task necessitates the integration of textual and visual information. Therefore, we initially combine the original text pr and its corresponding image description pm as input into a language model Ma (such as T5-large) to encode fused representations for each human prompt,\nzm = Ma([p; pm]) (1)\nwhere zm \u2208 Rdh represents the high-dimensional representation of the m-th human-prompt and dh denotes the output dimension of the language model. Similarly, for the i-th JMERE sample, we obtain its image description X\u00a1 through image X\u00a1 and its high-dimensional representation q\u00b9 \u2208 Rdh using the method in Eq. 1,\nq\u00b2 = Ma([X; X}]) (2)\nIt is crucial to recognize that instances closer in high-dimensional space are likely to share more similar entity types and object information. Therefore, KECPM utilizes cosine similarity to compute the high-dimensional similarity between each JMERE sample and each predefined human prompt. Subsequently, KECPM selects the top-K most similar predefined human prompts as candidate prompts to\n3.1.3 Knowledge Reflection. The composed knowledge prompts Ci send into ChatGPT. We encourage ChatGPT the autonomy to make its own judgments for each input sample by leaving the answer field blank, allowing ChatGPT to generate responses. However, we have observed that responses from ChatGPT are not always definitive, which could potentially lead to inaccuracies in the provided knowledge. Despite ChatGPT subtly hinting at its possession of relevant background knowledge, this may not guarantee correctness. As illustrated in Fig. 2, although ChatGPT did not explicitly identify \"Greece (location)\", it can be implicitly inferred from its reasoning that ChatGPT can recognize \"Greece\" as a city. Therefore, we encourage ChatGPT to reflect more deeply and acquire knowledge that is more pertinent and beneficial [23]. However, in practice, we have observed that while reflection can empower large language models to self-correct, it occasionally leads to meaningless responses, introducing considerable noise into the process. As shown in Fig. 4, for instance, we receive responses like The entities, relations, and reasoning provided above are correct, even inputting a description such as, \"If it is correct, just repeat your answer\".\nTo address these limitations, we propose a knowledge selector to choose meaningful auxiliary knowledge. After N self-iterations in ChatGPT involving human interactions, we collect N responses and process them using the same language model as Eq. (1). Consequently, the resulting representations of the ChatGPT responses (Auxiliary Knowledge) set are denoted as R = {r1, r2,..., rN }. We utilize cosine similarity to compute the high-dimensional similarity between each sample and every item of auxiliary knowledge, as shown in Eq. (3). Subsequently, we designate the most similar auxiliary knowledge as the final auxiliary knowledge, denoted as,\n(qi)Tri\nFi = arg Top1\nje{1,2,...N} 922 (5)\nwhere Fi is the final auxiliary knowledge for i-th input sample.\n3.2 Stage-2:Knowledge-enhance LM\nIn this stage, we integrate auxiliary knowledge denoted as F obtained from ChatGPT with the original input q\u00b9. These combined inputs are fed into a transformer-based language model, such as T5, to generate both entities and their corresponding relationships. To formalize this process, we modify our training objectives to facilitate the generation of entity and relation information. To enhance the readability and interpretability of the generated outputs, we convert each element of the JMERE quintuples, two entities e1 and e2 with their corresponding types t\u2081 and t2 and their relationship c, into a language sketch. For example, as shown in Fig. 3, the ground truth of the given sample is \"The relation between TV3 (organization) and Keadilan (organization) is subsidiary\", where TV3 (organization) and Keadilan (organization) are two entities with their corresponding entity types, and the subsidiary describes their relation. It is important to note that when a sample contains multiple quintuples, we perform template filling for each quintuple, subsequently concatenating these templates using \";\". Furthermore, the final formalized input adheres to the following format,\nI = [Type: TypePrompt; Image :ImageCaption;\nKnowledge:knowledge; Text :Text]\nH = LanguageModel(I) (6)\nwhere each element hi \u2208 Rdh in H represent each token representation of input I, where dh is the dimension of the language model. To optimize the model parameters, KECPM uses the language modeling loss as the loss function for the input sequence with ground-truth labels,\nL= -y\u2211t=1log \u03a3 p(y|H, yo:t-1) (7)\nwhere y is the length of output sequence y."}, {"title": "4 EXPERIMENTS", "content": "4.1 Few-shot Datasets\nWe conduct experiments on few-shot multimodal datasets built according to the distribution of relation categories from JMERE"}, {"title": "4.3 Evaluation Metrics", "content": "To evaluate the performance of triplet extraction, we employ a set of evaluation metrics consistent with the JMERE setting [45]. These metrics include precision (P), recall (R), and micro F\u2081-score (Mic-F1). Additionally, considering the imbalance in relationship labels within the dataset-where the top 3 labels account for 50% of the test set samples while the least 3 labels account for only 1%, we use the macro-F\u2081 score as an additional evaluation metric. This metric enables us to assess the model's effectiveness in learning from few-shot samples. The computation of macro-F1 (Mac-F1) is presented as follows:\nMac-F1=\u2211Ni=1F1 (i)N (8)\nwhere F1 (i) represents the F1 score for the i-th relation type, with N representing the total number of relation types.\n4.4 Implementation Details\nDuring the knowledge ingestion stage, we utilize the OFA model [30], a multimodal pre-trained model used for converting images into corresponding text descriptions. Additionally, we employ T5-large as our feature extractor, facilitating the fusion of image captions and textual data. Furthermore, we utilize ChatGPT with the GPT-3.5-turbo version and set the sampling temperature to 0. In the knowledge-enhanced language model stage, we employ the AdamW optimizer [15] to minimize the loss function. To determine the optimal learning rate, we conduct a grid search within the range of [2\u00d710-4, 2\u00d710\u22126], ultimately setting it to 5\u00d710\u22125. Additionally, we incorporate a warm-up linear scheduler to regulate the learning rate. We set the maximum sentence input length to 500, while the mini-batch size is configured to be 6. Our model undergoes training for 30 epochs, and we select the model with the highest micro F1-score on the development set for evaluation on the test"}, {"title": "4.5 Comparative Results", "content": "Table 2 presents the JMERE results on the few-shot setting, unveiling several crucial observations. Firstly, multimodal models, exemplified by AGBAN+MEGA, UMGF+MEGA, and EEGA, only achieved performance similar to unimodal methods like Tplinker and BiRTE and even fell below the performance of SpERT, which was specifically designed for joint entity-relation extraction in the few-shot setting. This indicates that while the aforementioned multimodal models can obtain some visual features to benefit information extraction, they typically demand a significant number of data to align and fuse different modalities. This requirement contradicts the few-shot setting, making it challenging to extract crucial information from visual features.\nFurthermore, the SpERT model achieved notable performance in terms of Micro F1 but under significant decline in Macro F1 metrics, indicative of its struggle to effectively learn from long-tail data due to data imbalance. Moreover, our proposed model consistently outperforms all baseline models in both Micro F1 and Macro F1 metrics, showcasing its performance with improvements of 2.27% and 1.69% in the respective metrics. Several factors contribute to the outstanding performance of our model: firstly, the utilization of an image caption-based approach enables the effective extraction of image information without the need for additional data training; secondly, the incorporation of refined auxiliary knowledge from ChatGPT equips the model with essential and relevant information, thereby enhancing overall performance and mitigating the impact of long-tail data; lastly, the employment of self-reflection"}, {"title": "4.6 Ablation Experiments", "content": "Ablation studies were conducted to assess the effectiveness of each prompt in the proposed model: the original text, image caption, and three different methods for obtaining knowledge (Directly, Reflection, and Selected). The results of the ablation experiments are presented in Table 3. In comparison to using only the original text (T), the inclusion of Image Captions (T+I) notably improved the results, yielding a 0.5% increase in Micro F\u2081 and a 0.35% increase in macro F1. This demonstrates that image information can offer insights into entity types. Moreover, the auxiliary knowledge of direct responses from ChatGPT (T+I+KDirectly) led to a substantial improvement in Macro F1 (0.51%) and Micro F1 (0.36%). This observation suggests that augmenting with auxiliary knowledge significantly impacts the performance, particularly in smaller sample sizes. Furthermore, through engaging ChatGPT in self-reflective (T+I+KReflection), we acquired higher-quality responses, resulting in an enhancement across both metrics. Nevertheless, when contrasted with our proposed model KECPM(T+I+KSelected), there remains an opportunity for further improvement, particularly in the Macro F1 (0.67%). This highlights the effectiveness of the proposed knowledge selector in filtering hallucinatory knowledge derived from multi-turn reflections.\nFinally, we attempted to incorporate visual object features (OF) from Mask R-CNN into the input of the language model, similar to previous work [39], to include more fine-grained information. However, this integration did not result in performance improvement. We consider that aligning features across different modalities from distinct pre-training datasets (text from T5, objects from Mask R-CNN) demands a significant number of data, which is inappropriate in the few-shot setting."}, {"title": "4.7 Analysis of Prompt Selection and Knowledge Reflection Modules", "content": "This section explores the effectiveness of the Prompt Selection (PS) and knowledge reflection modules. w/o PS Means to remove the Prompt Selection module and replace it with random selection within the context. Additionally, we investigate the performance of ChatGPT in the JMERE task under the in-context learning setting.\nThe results, as shown in Table 4, Firstly, the ChatGPT with few-shot settings has a performance gap with our proposed model. We have observed that ChatGPTgenerates entity and relationship types not present in the FS-JMERE pre-defined, such as \"building.\" This underscores the importance of Stage 2 in our proposed method, as it effectively aligns with the FS-JMERE task while leveraging background knowledge from ChatGPT. Moreover, the PS module can notably enhance the quality of auxiliary knowledge generated by ChatGPT and improve the model performance. Besides, an optimal number of human-prompt instances can further refine the quality of auxiliary knowledge. However, merely incorporating human prompts without consistent improvement in performance may not suffice. Supplying more than 5 prompts (K=10) to ChatGPT may decrease the quality of auxiliary knowledge. In practice, it has been observed that an excessive number of manual examples introduces noise into the generation process of ChatGPT, potentially disrupting its original reasoning process and leading to the replication of manual examples.\nFinally, our proposed model integrates N iterations of ChatGPT responses, enhancing knowledge accuracy through self-reflection. Consequently, we investigate the impact of varying iterations of self-reflection (N) on performance, as depicted in Fig. 5. When N = 0, which indicates the absence of self-reflection, the model exhibits the lowest performance, underscoring the efficacy of self-reflection in rectifying incomplete knowledge for improved performance. However, when N exceeds 2, the model's performance gradually declines. Despite the inclusion of the proposed Knowledge Selector, which filters knowledge for relevance to the input text, multiple rounds of self-reflection also introduce more noise in responses."}, {"title": "4.8 Case Studies", "content": "Through three case studies depicted in Figure 6, we demonstrate how auxiliary refined knowledge enhances the predictive performance of the model. In the first case, only EEGA incorrectly identifies LaLiga as a person, attributed to the presence of only humans in the image, leading to erroneous information fusion. Our model benefits directly from refined auxiliary knowledge, which accurately provides entity recognition and relationships. In the second case, the brevity of the text makes acquiring information about Fulham challenging, resulting in incorrect labeling by both SpERT and EEGA. However, our proposed model, with refined auxiliary knowledge included, effectively identifies the entity's type from such supplementary information.\nThe third case emphasizes the importance of integrating and fine-tuning auxiliary knowledge for the FS-JMERE task. While refined auxiliary knowledge correctly identifies entities Greece and Nafplion and their respective types, it incorrectly identifies the relationship between Greece and Nafplion. In the JMERE task, entities labeled as a person and an organization cannot have a member of relationship. By integrating this knowledge and fine-tuning the model, our method uncovers potential mappings between entity types and relationships, correcting errors and aligning with the FS-JMERE task requirements more effectively."}, {"title": "5 CONCLUSION", "content": "In this paper, we propose a Knowledge-Enhanced Cross-modal Prompt Model (KECPM) for Few-shot Joint Multimodal Entity-Relation Extraction (FS-JMERE), which can effectively handle JMERE in low-data scenarios by guiding a large language model to generate auxiliary background knowledge from text and image. To our knowledge, we are the first to focus on Multimodal Entity-Relation Extraction in the few-shot setting. Our model consists of two stages: firstly, a knowledge ingestion stage that dynamically creates prompts based on semantic similarity and uses self-reflection to refine the knowledge generated by ChatGPT; then, a knowledge-enhanced language model stage that integrates the auxiliary knowledge with the original input and employs a transformer-based model to perform JMERE. We have conducted extensive experiments on a few-shot dataset constructed from the JMERE dataset and demonstrated that our model outperforms strong baselines in terms of micro and macro F1 scores. We have also provided qualitative analysis and case studies to illustrate the effectiveness of our model. For future work, we plan to explore more ways to generate and select high-quality auxiliary knowledge from ChatGPT, such as using reinforcement learning or adversarial learning. We also intend to apply our model to other multimodal information extraction tasks, such as multimodal sentiment analysis."}, {"title": "A EFFECTS OF DIFFERENT LLMS AND LMMS", "content": "We evaluated large language models (Gemini 1.5, GLM4, GPT-4) and large multimodal models (GLM-4v, GPT-40) on the FS-JMERE task, comparing them with the previous best results of ChatGPTk=5. The results are shown in Table 2.\nFirstly, we observed that the performance of LMMs was significantly lower compared to LLMs. The best micro F\u2081 and macro F1 scores for the LLMs were 25.34 and 19.04, respectively, whereas the multimodal models achieved 21.91 and 14.73, respectively. Possible reasons include: 1) LMMs have less textual knowledge compared to GPT-4 and GLM4, and 2) our prompts were designed for LLMs, so on the image side, we only input the test sample images without adding images from multiple manual prompts. As a result, the multimodal models might not have accurately captured the prompt intent.\nAdditionally, both GLM-4 and GPT-4 surpassed the original best ChatGPT performance when the number of prompts K > 5. We incorporated the knowledge from GPT-4 with K = 10 into our proposed method, KECPMGPT-4K=10, and achieved better performance. The scores of micro F\u2081 and macro F\u2081 increased from 39.96 and 21.29 to 42.24 and 24.33, respectively. GPT-4 also exhibited significantly fewer hallucination responses compared to ChatGPT, reducing them from 67 to 8 instances of meaningless responses such as \"There are no named entities or relations mentioned in the given text.\" This improvement is likely due to GPT-4 and GLM-4 supporting up to 128K tokens, allowing for more comprehensive prompts and reducing hallucinations."}, {"title": "B PROMPT TEMPLATE", "content": "We present the template for prompting ChatGPT to generate background knowledge and answers. In Fig. 7, KECPM guides ChatGPT for auxiliary background knowledge generation. In Fig. 7 (a) in-context examples and answers in the template are selected from predefined artificial samples by our proposed Prompt Selection. Fig. 7 (b) is the template that encourages ChatGPT to engage in self-reflection. In Fig. 8, we guide ChatGPT to make direct predictions."}]}