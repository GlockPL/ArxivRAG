{"title": "DeepCloth-ROBOP&P: Towards a Robust Robot Deployment for Quasi-Static Pick-and-Place Cloth-Shaping Neural Controllers", "authors": ["Halid Abdulrahim Kadi", "Jose Alex Chandy", "Luis Figueredo", "Kasim Terzi\u0107", "Praminda Caleb-Solly"], "abstract": "The fidelity gap between simulation-trained vision-based data-driven cloth neural controllers and real-world operation impedes reliable deployment of methods from simulation into physical trials. Real-world grasping errors, such as misgrasping and multilayer grasping, degrade their performance; additionally some fabrics made of synthetic material also tend to stick to the commonly employed Franka Emika Panda's original gripper. Different approaches adopted various strategies to resolve these problems, further complicating real-world comparison between state-of-the-art methods. We propose DeepCloth-ROBsP&P with a simulation-to-reality transfer strategy Towel-Sim2Real and a cloth grasping protocol to consider and mitigate these grasping errors for robustly deploying quasi-static pick-and-place neural controllers in cloth shaping and demonstrate its generalisibility across different deep-learning methods, fabric contexts and robot platforms. Our approach allows us to compare multiple neural controllers in a real environment for the first time, offering valuable insights to the cloth manipulation community.", "sections": [{"title": "I. INTRODUCTION", "content": "Cloth shaping algorithms are hard to compare in real-world experiments. There has been impressive progress in recent years on developing vision-based data-driven controllers for cloth flattening and folding, including both reinforcement learning [1]-[5] and imitation learning [6], [7]. While such approaches have reported successful transfer of simulation-trained policies to the real-world individually [8], we are aware of no comparative study of different algorithms on a real robot, or analysis of how algorithms generalise across different materials, grippers, or robot platforms. Although many studies have compared these approaches in simulated environments [9]-[11], a comprehensive comparison in a real-world setting remains elusive due to the challenges of replicating experiments and accounting for various grasping errors, such as misgrasping, cloth dropping and grasping multiple-layers [1], [7], [10]. In this paper, we analyse the main obstacles to real-world deployment in the cloth-shaping domain and present a first real-world comparison of four recent cloth-shaping algorithms on a variety of flattening and folding tasks, across different platforms and configurations.\nOne of the primary challenges in deploying cloth manip-ulation systems in real-world settings can be attributed to the difference between simulated and physical environments, particularly in the behaviour of the fabric itself and the interaction between the end-effector and the fabric. Two main strategies to address these challenges are: (1) enhancing physics engines to more accurately model cloth behaviour"}, {"title": "II. RELATED WORK", "content": "Numerous methods have been applied to single-gripper quasi-static pick-and-place (QSP&P) towel shaping problems but have not been tested in the same robotic environment. We provide such a comparison in this work.\nSeveral influential meth-ods are based on model predictive control (MPC) including VCD [3] and VSF [1]. PlaNet-ClothPick [5], uses a Re-current State Space Model with variational inference and a domain-specific planning algorithm for fabric flattening. It samples pick locations on the cloth mask using a learned latent dynamic model and optimises policies based on pre-dicted rewards. Among these MPC-based methods, we chose PlaNet-ClothPick as a representative to demonstrate the effectiveness of our deployment approach.\nFoldsformer [10], which is a variant of TimeSformer [13], utilises a space-time attention mechanism to capture the instruction feature of folding demonstrations. Another goal-directed method, FabricFlowNet [7], achieves multi-step fabric folding from canonical states using optical flow; however, it does not support folding from crumpled states and its performance was inferior to Foldsformer [10] which we adopt as a SoTA representative for transformer-based controllers to benchmark folding tasks.\nJA-TN [11] is a towel-shaping controller that extends the TransporterNet architecture [14] with joint-probability action inference. It uses convolutional neural networks to directly predict pick and place action heatmaps from images. It has demonstrated different types of folding from flattened and crumpled states in simulated environments but has not been extensively evaluated in a real-world experiment. We test JA-TN in real world as a representative of popular TransporterNet-based controllers.\nVisual processing of cloth im-ages often relies on cloth segmentation, with colour thresh-olding being a common heuristic. VCD [3] focuses on extracting the point cloud of the cloth using segmentation in the target region. Several methods, including FabricFlowNet [7] and Foldsformer [10], utilise cloth-masked depth images as input to their models to align depth between real and simulated images, they subtract the difference between the average height of the real and the simulated camera."}, {"title": "III. DEEPCLOTH-ROBP&P", "content": "We train the domain specific neural controllers such as PlaNet-ClothPick [5], JA-TN [11], as well as a general visuomotor diffusion policy [12] (Section III-C) with our Towel-Sim2Real protocol (Section III-A). The FoldsFormer model was applied without re-training. Additionally, we employ the proposed grasping protocol (Section III-B) for reliable real-world fabric grasping. We collectively refer to this pipeline as DeepCloth-ROBsP&P (see Fig. 2).\nGrasping multiple layers of an article is a common phe-nomenon in cloth manipulation. It is exacerbated by the fact that most simulation-trained neural controllers do not consider this factor during learning. We propose a new benchmark environment Real2Sim Towels that incorporates real-world behaviours of the cloth-gripper interaction along with a corresponding improved oracle flattening policy to enhance the performance of the transferred deep policies. We enhance the Rainbow Rectangular Fabrics environment [11] by making the fabric share the same colours on both sides, including misgrasping chances and modifying the gripper behaviour to grasp a cluster of particles around the picker instead of precisely picking the closest cloth particle.\nWe extend the oracle smoothing policy used by the original JA-TN implementation [11] to provide expert trajectories for the trained models. The original policy (Oracle Towel Smoothing, or OTS) leads to errors in real deployment: it typically chooses to unfold a fold on the top of the cloth and reveal a corner if it is hidden underneath the fabric; this policy only works well if a gripper can pick precisely the target particle of the cloth without grasping multiple layers which is challenging in reality. Additionally, we modify the oracle so that the entire fabric remains within the camera's field of view after flattening, preventing scenarios where portions of the smoothed cloth become obscured. We call the new policy Real2Sim Smoothing (R2S-S)."}, {"title": "B. Cloth Grasping", "content": "The original Franka Emika Panda gripper struggles with fabric attachment and precise grasping, and lacks precision for tasks involving edges and corners (see Table 3). We introduce a simple tweezers-extended gripper (Figure 2) integrated into the 3D-printed base, significantly improving control and accuracy. The new gripper may still encounter misgrasping issues due to the precision of action-space con-version. To mitigate this, we implement two key strategies: (1) positioning the pick position 2-pixels inside the cloth-mask border and (2) aligning the picking angle parallel to the cut-line of the pick-point when on a cloth edge. Notably, we refrain from incorporating depth estimation methods, allowing the gripper to descend as deeply as possible."}, {"title": "C. Visuomotor Diffusion Policy", "content": "To demonstrate the robustness and versatility of our method, we apply our protocol to an algorithm never before used in the QSP&P domain: a the diffusion policy. This novel application serves to validate the reliability and adaptability of Towel-Sim2Real across different algorithmic approaches while exploring the potential for expanding the toolset avail-able for QSP&P tasks in fabric manipulation.\nChi et al. (2023) [12] adopt Denoising Diffusion Proba-bilistic Models (DDPMs) to approximate the conditional pol-icy distribution \u03c0(\u0391|X), where A = {a_i}_{i=t}^{i=t+Tr} is a sequence of action for prediction horizon Tp from an arbitrary step t; and, X = {x_i}_{i=t}^{i=t+Th} is a sequence of history observation with horizon Th (Th \u2264 Tp). Inference denoises the policies for the prediction horizon Tp and only uses Ta-step actions on the environment. A vision encoder produces the vision features ex by removing the final fully connected layer. The method can also have an internal state vector s that concatenates with the ex to produce conditional global feature z.\nWhile sampling, the sampled noisy actions \u0391\u03ba ~ N(0, I) go through K iterations of diffusion process, where a dif-fusion step k, last diffused policies Ak+1 and the global condition z are fed into noise prediction network e to produce prediction noises for step k that is used to denoise from Ak:\nAk-1 = \\frac{1}{\\sqrt{1-\\bar{\\beta}_k}} (Ak - \\frac{\\beta_k}{\\sqrt{1-\\bar{\\alpha}_k}}\\epsilon_\\theta(A_k, k, z)) + \\sigma_k \\epsilon, \\tag{1}\nwhere \u20ac ~ N(0,I) and \u03c3\u03b5 = \\frac{B_k}{\\sqrt{1-a_k}} = \\sqrt{\\frac{\\bar{\\beta}_{k-1}}{1-\\bar{\\alpha}_{k-1}}}\\frac{\\beta_k}{\\sqrt{1-\\bar{\\alpha}_k}} [19]; square cosine scheduler for \u1e9ek proposed in iDDPM [25] is used for better performance. Reversely, the DDMP noise scheduler uses the following formula to add noise to the actions\nAk ~N(\\sqrt{1-Bk}Ak\u22121, \u03b2\u03ba\u0399). \\tag{2}"}, {"title": "IV. EXPERIMENTS", "content": "We conduct shaping experiments on a dark-grey foamy surface with Realsense D435i cameras UR3e in \"hand-to-eye\" configuration (0.74 m above the surface) and Panda in \"eye-in-hand\" configuration (0.57 m above the surface on taking images). We use the MoveIt! library [27] to execute QSP&P trajectory, as is common in this domain [7], [10]. During physical trials, humans must reset the initial states for towel-shaping tasks, especially folding, and they also need to manually set the goal states for each trial for setup evaluation"}, {"title": "A. Generasibility of Towel-Sim2Real", "content": "To demonstrate generasability of our deployment protocol Towel-Sim2Real, we conducted experiments with 4 different neural controllers and 6 different fabrics on 4 different shaping tasks. Each physical flattening trial concluded within a maximum of 20 steps normalised coverage exceeding 95% defined as success. We also examine the neural controllers on all-corner inward folding (4 steps), corners-edge inward folding (4 steps) and diagonal-cross foldings (2 steps) with extra 2 steps to allow controllers to recover from mistakes. Intersection-over-Union metric between current and goal masks is used to evaluate step-wise performance, with 85% considered a success. For each shaping task, we also verify success through human judgement. As a control, we also provide a human baselines with a robot executing actions entered via a pick-and-place human interface.\nWe train PlaNet-ClothPick, JA-TN and diffusion policies in the newly proposed Real2Sim Towels benchmark envi-ronment\u2014PlaNet-ClothPick with 20 thousands transitional steps (instead of its original 1 million) using its specially"}, {"title": "B. Robustness of Towel-Sim2Real", "content": "Towel-Sim2Real includes three key components: (1) the Real2Sim Towels environment along with its oracle flattening policy R2S-S, (2) a new cloth segmentation heuristics for masking vision inputs and reliable grasping for fabrics, and (3) the image processing for training and deployment proposed in JA-TN [11]. Figure 2 illustrates the reliability of our cloth segmentation method. To assess the significance of the other two components, we compare 4 neural controllers"}, {"title": "C. Grasping with Tweezers-extended Gripper", "content": "We evaluate the tweezers-extended gripper against the default Franka gripper aiming to minimise misgrasping, multilayer grasping, cloth dropping and cloth attachment. We encompass a diverse range of cloth conditions including three cloth types (silk, cotton, and polyester), various cloth states (Flat, 2-folds, 3-folds), and multiple grasping scenarios (4 corners, 4 edges, and 2 random cloth points). Table III indicates that the tweezers perform better compared to the Franka gripper across all pick types, particularly excelling in complex graspings on four edges and corners with over 90% success rate. Misgrasp rates for the tweezers remain consis-tently below 10%, while the Franka Gripper shows higher rates between 16-22%. Both grippers perform similarly for multilayer handling, but the tweezers completely avoid cloth drops and attachment issues, which occur more frequently with the default gripper. Furthermore, the tweezers maintain consistent performance across all cloth types, with particular excellence in handling delicate fabrics like silk."}, {"title": "V. DISCUSSION", "content": "Many neural controllers in towel shaping only work in specific cloth characteristics regarding particular colour, thickness, and initial cloth positions in camera view [1], [4], [7]; their generalisability to other fabric contexts and robot platform is uncertain. With our protocol DeepCloth-ROBP&P, we deployed four neural policies, including a diffusion policy, in at least two robotic platforms, including Franka Emika Panda with an \"eye-in-hand\" setting and UR3e with a \"hand-to-eye\" setting.\nSoft fabrics with high internal friction pose challenges for flattening methods due to the difficulty in separating different parts of the fabric. Conversely, small and stiff fabrics are problematic for folding with a single end-effector as they tend to flip back easily. For all folding controllers, the initial state of the folding tasks should be completely inside the field of vision. Our current robotic sys-tem faces several limitations: (1) it struggles to automatically detect success states accurately; (2) it occasionally produces helical and spiral path trajectories; and (3) it still experiences minor misgrasping issues. Fine-tuning camera extrinsic for precise pixel-to-base conversion also remains tedious and time-consuming. Identifying an appropriate workspace for smooth and stable pick-and-place operations is also tricky, primarily due to the robot's limited reach and the tendency of planned trajectories to produce erratic joint movements when approaching the robot base. Last but not the least, we find that the early checkpoints of the trained networks deploys better for most models, but the surprising diffusion policy does not overfit to the artefact of the simulation under Towel-Sim2Real.\nWe plan to apply the proposed method to more difficult tasks, such as flattening and folding different garment types. We want to further investigate the capacity of diffusion policy through making folding agnostic to the cloth's initial state and achieve folding-from-crumpled in the real world. Integrating depth estimation along with a tactile sensor may help to improve the dexterity of the grasping and substantially reduce the effect of grasping multiple layers of the articles, which may be vital for the successful manipulation of complex garment manipulation."}, {"title": "VI. CONCLUSION", "content": "We introduce a set of robust robotic strategies, collectively known as DeepCloth-ROBsP&P, that enable effective real-world deployment of pick-and-place cloth-shaping neural controllers. We comprehensively evaluate four different neu-ral controllers' real-world performance on shaping towels with varying size, colour, shape, material and thickness, in-cluding a modified visuomotor diffusion policy. Our method also demonstrates the versatility of the proposed protocol across different platforms. Ultimately, this research con-tributes valuable insights and practical solutions to robotic cloth manipulation, paving the way for more advanced and reliable systems in real-world cloth manipulation."}]}