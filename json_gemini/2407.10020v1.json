{"title": "Causality extraction from medical text using Large Language Models (LLMs)", "authors": ["SEETHALAKSHMI GOPALAKRISHNAN", "LUCIANA GARBAYO", "WLODEK ZADROZNY"], "abstract": "This study explores the potential of natural language models, including large language models, to extract causal relations from medical\ntexts, specifically from Clinical Practice Guidelines (CPGs). The outcomes causality extraction from Clinical Practice Guidelines for\ngestational diabetes are presented, marking a first in the field. We report on a set of experiments using variants of BERT (BioBERT,\nDistilBERT, and BERT) and using Large Language Models (LLMs), namely GPT-4 and LLAMA2. Our experiments show that BioBERT\nperformed better than other models, including the Large Language Models, with an average F1-score of 0.72. GPT-4 and LLAMA2\nresults show similar performance but less consistency. We also release the code and an annotated a corpus of causal statements within\nthe Clinical Practice Guidelines for gestational diabetes.", "sections": [{"title": "1 INTRODUCTION", "content": "Clinical Practice Guidelines (CPGs) are a set of expert guidelines developed to guide physicians in navigating the\ncomplexities of the medical decision-making process. Various medical societies provide numerous such guidelines,\nbased on their focus (e.g. cardiology vs. family medicine). This variability can lead to inconsistencies in the comparison\nand application of guidelines, as noted in our previous work [24, 25]. Recognizing these discrepancies is crucial for\neffective communication between patients and physicians.\nPre-trained language models like BERT [12], which dynamically adjusts the weightings between each part of the\noutput and all elements of the input based on their connection (attention), have demonstrated remarkable effectiveness\non numerous natural language processing tasks [12], including causality extraction [18].\nMore recent improvements come from Large Language Models (LLMs) like GPT-4 [42], which are pre-trained on\nextensive data and later enhanced through reinforcement learning feedback from both humans and AI to ensure\nadherence with human principles and policy compliance. Another recent model, also used in this article, is the open\nsource LLAMA2 [52]. It was trained on 2 trillion tokens and it available in three different sizes (7B, 13B, and 70B). It is\nwidely used in numerous tasks, particularly in information extraction [54]. And perhaps more importantly, LLAMA is a\nfocus of research on understanding capabilities and structures in large language models [7], [21]."}, {"title": "2 RELATED WORK", "content": "Causality extraction is the task of automatically extracting the cause/effect relationships from the text. In this section,\nwe briefly discuss studies related to causality extraction."}, {"title": "2.1 Work related to automatic information extraction from Clinical Practice Guidelines (CPG)", "content": "This section summarizes the prior work related to information extraction on the Clinical Practice Guidelines. Extracting\nclinical findings from notes of outpatient progress was early done realized by [14]. Fifteen years later [49] targeted the\nautomated extraction of diagnosis and treatment procedures from clinical guidelines. A similar work [29] introduced a\nmethod for automatically collecting useful information using rules rooted in both syntactic and semantic information.\nA pattern-based approach was further used by [9], which contrasted a manually developed ontology for CPG eligibility\ncriteria with a top-level ontology stemming from a semantic pattern-based approach. A more recent work [15] introduced\nan innovative system that blends together the methodologies of Natural Language Processing (NLP) and Fuzzy Logic. A\nsupervised machine learning methodology was used by another similar work [20] to extract and categorize Conflicts Of\nInterest (COIs) from disclosure statements indexed in PubMed.\nRecently, Large Language Models (LLMs) have (also) been employed for a variety of NLP tasks, including those\ninvolving information extraction. LLMs can be fine-tuned to cater to a specific dataset, or a prompt-based approach\ncan be utilized. An illustrative study [57] measures the efficiency of the few-shot learning performance of GPT-3 in\ntasks related to text classification and information extraction. A recent survey article [31] provides a summary of\nthe methods and solutions employed for information extraction. It also highlights the challenges encountered when"}, {"title": "2.2 Recent work on causality extraction from non-medical text", "content": "The study by [34] directly extracts cause and effect from text without separately extracting candidate pairs and their\nrelations. A work on event extraction [37] focuses on identifying the causal relationship between pairs of event mentions,\nalso known as 'Event Causality Identification' (ECI). Balashankar et al. [3] propose an event extraction (modality) that\nseeks to uncover the hidden relationships between events mentioned in news streams by creating a Predictive Causal\nGraph (PCG). Prompt tuning has been proposed to bridge the gap between pre-training and fine-tuning on many of the\nmainstream NLP tasks like text classification [47, 56], information extraction [8, 10] etc. In [35], Knowledge Enhanced\nPrompt Tuning (KEPT) employs external knowledge sourced from knowledge bases (KBs) to fine-tune pre-trained\nlanguage models through the design of an attention mechanism.\nA recent article [6] describes the use of ChatGPT to extract cause/effect relationships from text on three datasets: (1)\nChoice of Plausible Alternatives (COPA) [19], which is a collection of premises, along with two questions related to\neach premise, that requires causal reasoning in order to solve the inference; (2) e-CARE [13] which is an explainable\ncausal reasoning dataset with cause, effect and two possible explanations; (3) Headline Cause [22] dataset, which aims\nto identify the implicit causal relations between pair of text. On COPA, ChatGPT in-context learning got a 97% accuracy\n(performance); on the eCARE dataset, a 79.6% accuracy was obtained using prompt engineering, and 72.7% accuracy\nwas recorded for on Headline Cause."}, {"title": "2.3 Causality extraction from the medical text", "content": "In 2013 the task of automatic detection conditional statements in medical guidelines was first introduced [53]. The article\nused a rule-based approach, focusing on presence of connectives such as \"if\", and a collection of word-based syntactic\npatterns. Subsequent works on detecting condition action statements from CPGs, [26] and [23], apply supervised\nmachine learning techniques to classify sentences according to whether they express conditions and actions. Another\nstudy [28] used heuristic patterns to identify recommendation statements in Clinical Practice Guidelines (CPG). A\nreview article [17] documents the existing methods and tools for clinical concept extraction. The summarization of\nbiomedical literature is addressed in [55] using pre-trained language models. A more recent study [50] explores the use\nof ChatGPT for clinical text mining, specifically for extracting structured data from unstructured healthcare texts and\nfocusing on biological named entity recognition and relation extraction by identifying and extracting medical entities\nfrom text related to disease and drug, symptoms and treatment, etc."}, {"title": "3 DATA", "content": "We annotated seven documents of gestational diabetes (clinical practice) guidelines from various societies (and medical\nentities) like(such as) the American Diabetes Association (ADA) ([2],[38]), US Preventive Services Task Force (USPSTF)\n([11], [44]), American College of Obstetrics & Gynecology (ACOG) [1], American Academy of Family Physician (AAFP)\n[41], and Endocrine Society [4].\nThe decision to annotate gestational diabetes clinical practice guidelines was based on the opportunity to explore\ncausality inference in the future with situated learning models with prediction (team member Dr. Garbayo worked on\nsafety and quality database development on maternal and child in maternities, resulting in the creation of the largest\nmaternity database in Latin America [32]."}, {"title": "3.1 Inter-annotator agreement for the medical data", "content": "Due to the intricacy of causality extraction, which involves annotators labeling varying text spans as \"cause,\" \"effect,\"\nand so on, computing agreement between two annotators can be challenging as it requires comparing two spans of\ntexts. Traditional methods of inter-annotator agreement, such as the Kappa statistic [16], are inadequate due to their\nneed for classifications to fit into mutually exclusive and discrete categories. Therefore, we decided to assess agreement\nusing both exact match and relaxed match criteria. The F-measure is used for the exact match [27, 39, 51] between the\nlabels. In the case of the relaxed match, the average distance between phrases is computed. Initially, the annotated\nphrases, their corresponding labels, and the full sentence they are derived from are extracted from the entire annotated\ndocument. These annotations, originating from both annotators, are then compared and amalgamated based on the\nsentence. The resulting merged table thus features the sentence, the extracted phrase, and the labels as marked by\nAnnotator 1 and Annotator 2. In total, 514 matching phrases have been identified. An overall agreement computed as a\nJaccard similarity of 0.66 was obtained. Details of the inter-annotator agreement computation are given below.\nFrom the merged data table, the inter-annotator agreement was computed. This is done by computing the match\nbetween the annotations as follows.\n\u2022 Relaxed match - Both annotator's phrases overlap with each other but are not necessarily an exact match.\n\u2022 Exact match - Both annotator's phrases exactly match."}, {"title": "3.2 Data preparation and preprocessing", "content": "Seven documents on gestational diabetes guidelines provided by different societies are downloaded as PDF documents.\nThe PDFs are converted into a document format, and the documents are given to the annotators for annotating them\nmanually. The annotators used tags to annotate the documents.\nAfter annotating them, the NLTK sentence tokenizer is used to extract sentences from all the documents. The\nsentences from all the documents are appended together and converted into a data frame. Regular expressions are used\nto extract the causal sentence. If any of the sentences contain a tag <>, it will be extracted as a causal sentence. Again"}, {"title": "4 METHODOLOGY", "content": ""}, {"title": "4.1 Causality extraction using BERT", "content": "Given the good performance of DistilBERT with organizational data [18], this model was also applied to the medical\ndata. Considering the limited sample size in medical data, we attempted to improve the learning process by increasing\nthe number of epochs. This approach allows for more refined fine-tuning of the model.\nIn order to decide on the correct number of epochs and to avoid overfitting, we tried running the model for 100\nepochs and plotted the validation loss and the training loss. The graph showing the train and validation loss for our\nhighest performing model, BioBERT, is given in Figure 2.\nFrom the graph, we can understand that with the increase in the number of epochs, the training loss is constantly\nincreasing and approaching 0. The validation loss decreases till 18 epochs and then starts to increase. Based on this, we\nfine-tuned DistilBERT for 18 epochs, BERT(BERT-base-uncased) for 20 epochs, and BioBERT for 16 epochs.\nThe data is split into train and test. DistilBERT for token classification is fine-tuned on the training data for 18 epochs.\nOn the test data, the model obtained an average F1-score of 0.57. Similarly, we fine-tuned BioBERT for 16 epochs and\nBERT for 20 epochs. Out of these three models, BioBERT[33] gave us an average higher F1-score. BioBERT gave an\naverage F1 score of 0.61, and BERT gave an average F1 score of 0.60. The detailed results of fine-tuning BioBERT on the\ntest data are given in Table 3; and, for comparison, the summary of the results of using variants of BERT for causality\nextraction task is given in Table 4"}, {"title": "4.2 Observations on using GPT-4 for causality extraction from medical guidelines", "content": "Generative Pre-trained Transformer 4 (GPT-4) [42] outperforms most of the state-of-the-art performing models on\nthe traditional NLP benchmark datasets. In this section, we discuss our results of prompting GPT-4-0314 with a with\ncontext window of 8,192, for the causality extraction task. We explored various prompt sizes (zero, four, six, eight,\nten-shot, and twenty-shot prompting).\nAs an initial step, we tried the sentence with token-level labels for each word in the sentence as prompt examples. For\nthe test data, the model is expected to predict a label for each word in the sentence. However, the model hallucinated by\npredicting a longer number of labels than in the given sentence; that is, a long sequences of non-existing \"non-causal\"\nlabels.\nSince GPT-4 hallucinated for the token-level predictions, we tried extracting the phrases of cause/effect relationships\nin text and tried converting them into token level by assigning labels for each token. We started with a four-shot\nprompting. The annotated data with the tags will be given as an example in the prompt, and the model is expected to\npredict similarly. A sample is given in Example 4.1.\nWe tried converting the predictions with the tags into a token-level format in order to compute the F1 score. However,\nsince the tags are placed in different places in some of the gold annotations and predictions, the number of tokens in\ngold and predictions doesn't match. An example is given below\nIn Example 4.2, the phrases marked indicate the scenario where some extra spaces can be added, leading to the\nindifference in the number of tokens between gold and the predictions. In the gold data, neonatal hypoglycemia\n have a space after the tag, but in the prediction, the tag is predicted after\nthe number, which leads to no space between . In some scenarios, the GPT-4 omits some\nof the words if they do not contain a causal relation (omits the 'O' labels in some places). This mismatch between the\ngold and the predictions impedes the token-level comparison and reporting of the F1 score. An example is given below:\nIn Example 4.3, in the prediction, the keyword \"Hemoglobin\" is missing, which is present in the gold data. In some\nplaces, such inconsistencies lead to token mismatch between the gold and predicted data.\nTo compare the performance of GPT-4 with other models, the predictions are converted into the token level and\nmanually checked to convert both the gold predictions to the same number of tokens for the four-shot prompting.\nIn the predictions, some tokens are missed; those tokens are added to the predictions and marked as label \"O.\"(as O\nindicates tokens that are not cause, effect, condition, action, or signal). After converting the data into a token level, we\ncomputed the F1 score. With GPT-4, we got an average F1 score of 0.39 with four-shot prompting."}, {"title": "5 RESULTS & EXPERIMENTS", "content": "As the predictions of GPT-4 can be unreliable, and missing tokens in a sentence leads to a token mismatch between the\ngold data and the predicted data, therefore Jaccard distance is proposed as an alternative solution to the traditional F1\nscore as the evaluation criteria. The Jaccard similarity was computed using the textdistance Python library. Another\nalternative measure to try is the cosine similarity. The cosine similarity is obtained by computing the vectors of both\nthe gold and the predictions using the Universal Sentence Encoder [5]. The computed values are used to compute the"}, {"title": "5.1 LLAMA2 for causality extraction from medical guidelines", "content": "LLAMA2[52] is a pre-trained and fine-tuned Large Language Model. Three variants of LLAMA2 are available, which\ndiffer in the parameters. 7B, 13B, and 70B parameters are publicly available. LLAMA2 is trained on two trillion tokens\nof data. In our experiments, the LLAMA2 7B parameter is fine-tuned on the medical data. It is fine-tuned using the\nHuggingFace autotrain.\nTo fine-tune LLAMA2, the first step is to prepare the data. At first, when the model was fine-tuned and tested on\nthe token level as BERT, LLAMA2 was predicting a long number of \"O-other\" as GPT-4. So we dealt with this as a\nphrase-level extraction problem. The data is prepared with three parts which are instruction, input, and output. A\nsample training data is given in example 5.1\nThe test data should be similar to the training data except for the output, which should be empty. The gestational\ndiabetes annotated data was split into train and test data. The HuggingFace autotrain for the LLM fine-tuning was used\nto fine-tune the model. The fine-tuned weights are pushed into the HuggingFace dataset for inference. This experiment\nwas done using Google Colab Pro+ with a High-RAM A100 GPU. Similar to the GPT-4, the predictions of LLAMA-2\nwere also at phrase level. So a similar evaluation strategy is followed for LLAMA2. We present the results with three\ntypes of distance.\nThe predictions are split into phrase levels and then compared with gold data. The Jaccard similarity was computed\nusing the textdistance Python library. The cosine similarity is obtained by computing the vectors of both the gold and\nthe predictions using the Universal sentence encoder [5]. The computed values are used to compute the pairwise cosine\nsimilarity between two vectors using Scikit-learn\nInitially, we split the data into train and test using the Scikit learn train_test_split(). We have converted the phrase-\nlevel predictions into token-level. In the test data, there were a total of 59 samples. Out of the 59 samples, only 29\nsamples, LLAMA2 predicted the labels, so the evaluation is only for those sentences. With LLAMA2, we got an average\nF1-score of 0.36, which is lower than that of all the other models.\nSince the test data size is very small, we have also tried a four-fold cross-validation on this data. The results of\nfine-tuning LLAMA2 using four-fold cross-validation with 3,5, and 10 epochs are given in Table 7.\nWith the increase in the number of epochs, both the Jaccard similarity and F1-score increase. Also, the predictions of\nLLAMA2 missed labels in many of the predictions. It extracted the phrases with no label. With three epochs, LLAMA2\nmissed 38% of the labels; with five epochs, 21% of the labels; and with ten epochs, it missed 26% of the labels. We\nomitted the predictions with no labels (108 predictions, 60 predictions, 76 predictions). The results of causality extraction\npresented in Table 7 are after omitting the predictions with no labels."}, {"title": "6 DISCUSSION", "content": "Above we presented results on causality extraction from medical guidelines using recently introduced large language\nmodels such as LLAMA2 and GPT-4, and compared them with the performance of BERT, an older, and smaller LLM. The\nannotated data and the code are all publicly available on GitHub: https://github.com/gseetha04/LLMs-Medicaldata.git.\nWe observed that GPT-4 expresses strong performance for the cause-effect relationships with medical data, and\ngenerally has a good understanding of medical text without fine-tuning. However, in contrast with GPT-3.5 GPT-4\ncannot deal with token classification, which limits the traditional way of finding cause and effect phrases, as discussed\ne.g. in our previous work [18].\nEven though LLAMA2 seems to perform well for causality extraction, the predictions of the LLAMA2 do not predict\nlabels for many cases, which limits its practical application. This perhaps was caused by fine-tuning LLAMA2 on\nour small dataset. Therefore, increasing the size of the dataset before the fine-tuning may improve the performance.\nHowever, large annotated datasets for CPGs are not available, and further experiments would require annotating more\ndata. Since we focused on the accuracy of actual predictions, we omitted 38% of labels with three epochs, 21% of labels\nwith five epochs, and 26% of labels with ten epochs.\nGiven its relatively high performance and ease of use, BERT-based models continue to be a state-of-the-art for\ncausality extraction tasks, even in the age of LLM ."}, {"title": "7 CONCLUSION", "content": "We developed an automated technique for extracting causalities from annotated corpora of medical guidelines. Addi-\ntionally, we exhibited the practicality of employing new Large Language Models for causality extraction tasks. With\nBioBERT, we got an average F1-score of 0.72, whereas with LLAMA2, an average Jaccard distance of 0.40 was obtained.\nWe demonstrated the potential for extracting causalities from medical guidelines using a small annotated corpus. The\nnext logical step could involve expanding the corpus through the annotation of more data and creating a benchmark\ndataset for causality extraction from medical guidelines.\nThe potential of this research opens up novel dimensions for the health domain, as causality extraction from medical\nguidelines can enhance clinical decision-making and patient care. This work explored both machine learning and\nnatural language processing techniques for causality extraction. Despite the abundance of causal sentences within these\nguidelines, automatic extraction is an unexplored field of research. Also, machine learning models often fail in clinical\napplications [48] due to the gap between data (both training and testing). In order to avoid this gap, more realistic tests\nneed to be done so that they can be employed for real-world data."}]}