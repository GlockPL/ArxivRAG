{"title": "Automating Thought of Search: A Journey Towards Soundness and Completeness", "authors": ["Daniel Cao", "Michael Katz", "Harsha Kokel", "Kavitha Srinivas", "Shirin Sohrabi"], "abstract": "Planning remains one of the last standing bastions for large language models (LLMs), which now turn their attention to search. Most of the literature uses the language models as world models to define the search space, forgoing soundness for the sake of flexibility. A recent work, Thought of Search (ToS), proposed defining the search space with code, having the language models produce that code. ToS requires a human in the loop, collaboratively producing a sound successor function and goal test. The result, however, is worth the effort: all the tested datasets were solved with 100% accuracy. At the same time LLMs have demonstrated significant progress in code generation and refinement for complex reasoning tasks.\nIn this work, we automate ToS (AutoToS), completely taking the human out of the loop of solving planning problems. AutoToS guides the language model step by step towards the generation of sound and complete search components, through feedback from both generic and domain specific unit tests. We achieve 100% accuracy, with minimal feedback iterations, using LLMs of various sizes on all evaluated domains.", "sections": [{"title": "Introduction", "content": "Large language models have shown great promise across countless domains and fields, especially as their architectures become more advanced. Spurred by their abilities in natural language tasks, several recent works have studied AI planning in Large Language Models (LLMs) as a subset of code generation and code refinement. The approaches vary from giving a planning problem to an LLM and asking it to output an entire plan in a single call to asking an LLM to generate a planning model to be given to an automated planner. Between these two extremes, lies a body of work on using language models to plan by performing a combinatorial search. Among these, Thought of Search (ToS) stands out; it uses the language models to define the search space for the entire domain at once. It is done simply by soliciting two crucial search components, successor function and goal test. These components are then plugged into a standard search algorithm, such as Breadth-First Search (BFS) or Depth-First Search (DFS).\nToS has an impressive accuracy of 100% on all tested benchmarks and it produces a symbolic model whose soundness and completeness can be verified. However, ToS has a limitation - it requires a human expert in the loop, providing a feedback to the model on the produced code. Our contribution is precisely there. We automate the iterative feedback and exception handling process through the use of unit tests and printed debugging statements for use with few shot and Chain of Thought (CoT) prompting, limiting the human expert involvement with the language model. We test the search components for soundness and completeness and provide feedback to the model when a violation is detected. We use a mixture of domain-independent and domain-specific tests, based on a small number of held out instances.\nWe exemplify our proposed approach on five representative search problems from the recent literature and test with a variety of large language models of different sizes. Through automated feedback, we find that the accuracy of the code generated by language models consistently increases to reach 100% across all tested domains. We show that the total number of calls to the language model is typically small, comparable to the results of ToS with human feedback. In an ablation study, we justify the importance of soundness and completeness feedback for obtaining the highly accurate final code. Finally, we investigate the errors"}, {"title": "Related Works", "content": "Recently, several works have leveraged LLMs for plan generation. analyzed LLMs ability to generate plans for classical planning problems described in natural language. generated task plans and used precondition errors as feedback to revise the generated plan. In the same vein, various works have used external verifiers or validators as feedback for LLMs to generate better plans. investigate training approaches to improve plan generation abilities. All these approaches use LLMs to solve one problem at a time- essentially treating LLM as a policy. Another line of work has tried to extract policies or generalized plans from LLMs. synthesized generalized plans as Python programs from LLMs for planning domains described in a formal language (PDDL). Further, LLMs have also been used to extract planning problems and models in formal language from their natural language description. used LLMs to translate natural language planning problems to PDDL problems, and proposed a benchmark for such evaluating this ability while use LLMs to translate natural language goals to PDDL. Recently, leveraged LLMs to convert natural language domain description to PDDL domains. However, the LLM generated PDDL remains less reliable and difficult to evaluate.\nA burgeoning research field utilizes LLM's to conduct a search via structured prompting and feedback for planning and reasoning problems. used LLMs in the loop for Monte Carlos Tree search by treating LLMs as world models to generate next state as well as treating them as reasoning agents to pick the next state to expand. Similarly, Tree of Thoughts used LLMs to generate a search tree to expand each node in the search tree-and also used LLMs for evaluating the choices and selecting the next best state. Graph of Thoughts modeled LLM generated output as a graph instead of a tree and reduces the number of LLM calls. Similar approaches with integration to search are also proposed for interactive domains. While these approaches have shown some success, their significant reliance on LLMs for generating successors makes them not only extremely inefficient but also very unreliable. Thought of Search (ToS), on the other hand, proposed using LLMs to generate code for the successor and goal functions for problems described with natural language. Once these functions are available, any offline search algorithm can be used to solve any problem in the domain. This approach is significantly more efficient than approaches which use LLMs in the loop during search. However, it requires human expert for the feedback. Our work focuses on alleviating the requirement of human in the loop feedback.\nLLM's abilities are rapidly advancing in program synthesis. Various benchmarks have been established to evaluate correctness of code generated by LLMs and subsequent approaches have demonstrated human level performance on coding benchmarks. use errors from execution as feedback to LLMs so they can refine the code. discussed the use of external verifies to curate feedback for LLMs. introduced unit test results and error messages to LLMs. Recently, LLMs code generation has also shown to help in mathematical reasoning problems. Inspired by successes in these works, we propose to automate the feedback for ToS by using both generic and domain-specific unit tests and validators."}, {"title": "Background", "content": "In this work we follow the notation of , slightly adapting it for our purposes. A deterministic planning problem over a state space is a tuple \\(II = (S, A, s_0, S_G, f)\\), where S is a finite set of states, A is a finite set of action labels, \\(s_0 \\in S\\) is the initial state, \\(S_G \\subseteq S\\) is the set of goal states, and \\(f: S \\times A \\rightarrow S\\) is the transition function, such that f(s, a) is the state which applying action a in state s leads to. A triplet (s, a, f(s, a)) is called a transition. A solution to such a problem is a sequence of states and action labels (also called a trace) \\(\\rho = (s_0, a_1, s_1, a_2,... a_n, s_n)\\), such that \\(f (s_i, a_{i+1}) = s_{i+1}\\) for 0 < i < n and \\(s_n \\in S_G\\). In cases when the action labels are not important, they can be dropped from the definition.\nThe \"black box\" approach encodes the state space with a tuple \\(\\Pi_b = (s_0, succ, isgoal)\\), where \\(s_0\\) is the initial state, \\(succ: S \\rightarrow 2^{A \\times S}\\) is a successor generator, and \\(isgoal : S \\rightarrow \\{T, F\\}\\) is the goal test function.\nA solution to the black-box problem is a sequence of states and action labels (a trace) \\(\\pi = (s_0, a_1, s_1, a_2,...a_n, s_n)\\), such that \\((a_{i+1}, s_{i+1}) \\in succ(s_i)\\) for 0 < i < n and \\(isgoal(s_n) = T\\). Here as well, if action labels are not important, they can be dropped.\nWe now establish the correspondence between the black-box encoding and the planning problem."}, {"title": "Definition 1 (Soundness and completeness)", "content": "We say that isgoal is sound if isgoal(s) = F for all \\(s \\notin S_G\\) and isgoal is complete if isgoal(s) = T for all \\(s \\in S_G\\).\nWe say that succ is sound if \\(succ(s) \\subseteq \\{(a, s') \\mid f(s,a)=s'\\}\\) and succ is complete if \\(succ(s) \\supseteq \\{(a, s') \\mid f(s, a) = s'\\}\\).\nSound and complete successor generator and goal test provide the \"black box\" description of the state space of the planning problem II. In such cases, a solution to \\(\\Pi_b\\) is guaranteed to be a solution to II, and if no solution for \\(\\Pi_b\\) exists, then II also must be unsolvable.\nIf the successor generator and goal test are sound, but not necessarily complete, it is still the case that a solution to \\(\\Pi_b\\)"}, {"title": "Proposed Approach and Methodology", "content": "We build upon the previous work that proposed producing a code implementation of succ and isgoal functions, taking the human out of the feedback loop. Similar to that work, we care about two properties, soundness and completeness. As we deal with planning problems described in a natural language, we do not have the formally defined planning task II. Albeit not stated formally, previous work on generating succ and isgoal with language models assumes the existence of a human expert with the ability to access II (often in their mind). Examples of such access include a feedback on the code of succ and isgoal produced by the LLM or validating a solution obtained from the LLM in cases when succ and isgoal are implemented through LLMs. Here, we make a similar assumption, but request a different access to II. In order to challenge the soundness and completeness of the produced functions, the human expert is asked to produce unit tests, information which can provide evidence of unsoundness or incompleteness. The evidence can then be used to automatically feedback the model with the information needed to fix the code. We deal with three types of information, exemplified on the 24 Game.\n\u2022 Examples of inputs to isgoal for which the correct output is known. For instance, we know that isgoal([24]) should be true and isgoal([24, 1]) should be false.\n\u2022 Examples of inputs to succ for which some of the correct outputs are known. For instance, we know that [24], [2], and [-2] are valid successors of [6,4] and therefore should be in succ([6, 4]).\n\u2022 A partial soundness check for a transition (s, a, t) quickly invalidating (obviously) incorrect transitions. For instance, in 24 Game we know that the successor state t must be of length exactly one less than s.\nThe first two are are usually readily available and often come with the description of the problem. The third one might require some level of understanding of the problem being solved, but it is always possible to use a trivial partial soundness test that always reports that there are no issues. Figure 1 presents an overview of our approach, describing how the provided information is used.\nStep 1 Following , we start with the initial prompts asking for the successor function succ and the goal test isgoal.\nStep 2 Then, we perform the goal unit tests, providing feedback to the model in cases of failure, repeatedly asking for a new isgoal until all goal unit tests have passed or a predefined number of iterations was exhausted.\nStep 3 Once isgoal has passed the unit tests, we perform a soundness check of the current succ and isgoal functions. We do that by plugging these functions in a BFS extended with additional checks and run it on a few example problem instances. If BFS finished, we check whether the goal was indeed reached. If not, that means that isgoal failed to correctly identify a state as a non-goal state and we provide that as feedback to the model, repeating Steps 2 and 3.\nStep 4 (Optional) Once the previous steps were finished, we perform the successor unit test, providing feedback to the language model in case of failure.\nEvery time a goal test fails, we go back to Step 2, every time the successor test fails, we go back to Step 3. After the first step, we always have succ and isgoal that can be plugged into a blind search algorithm. However, if Step 3 fails, we have an indication that we cannot trust the solutions produced by that algorithm.\nExample feedback produced in Steps 2, 3, and 4 can be seen in Listing 1. In what follows, we provide detailed description of each step of AutoToS."}, {"title": "System prompt", "content": "We instruct the model to provide answers in convenient form for integrating as a search component. Thus, the produced code should consist of a single, self-contained function. Following existing work, we devise the following system prompt.\nYou are a Python coding assistant. Help me generate my Python functions based on the task descriptions. Please always generate only a single function and keep all imports in it. If you need to define any additional functions, define them as inner functions. Do not generate examples of how to invoke the function. Please do not add any print statements outside the function. Provide the complete function and do not include any ellipsis notation."}, {"title": "Step 1: Initial prompt", "content": "While the initial prompt is the primary source of information for the language model and therefore very important, we assume that we have very limited control over it. We therefore mostly take the existing initial prompt from previous work, only ensuring that it includes an example input to the requested function in the correct format."}, {"title": "Step 2: Goal function check", "content": "Goal unit tests assume the existence of a few known goal and non-goal states. If the goal function isgoal incorrectly identifies a goal state, then it is incomplete, according to Definition 1. If it incorrectly identifies a non-goal state, then it is not sound. A search with a non-sound goal function can incorrectly report that a solution was found. One illustrative example from the 24 Game is a state [24, 1], which a goal test function may incorrectly identify as a goal state and stop before the actual solution was found - in this case, another arithmetic operation was needed. Whenever an issue with either goal function soundness or completeness was identified, we give feedback to the language model with the description of the failure and the state for which the failure occurred. See Listing 1 (top) for an example feedback. Here and later we use a chain of thought style request, asking the model to discuss why a mistake was made and to come up with a fix."}, {"title": "Step 3: Successor function soundness check", "content": "A soundness check assumes the existence of example problem instances for which we know how to validate that a goal was reached. We extend the BFS/DFS search with additional checks as follows. First, both the successor and goal test functions are wrapped with a timeout of 1 second. These functions should be able to finish in a few milliseconds and therefore 1 second timeout is an indication of an issue with the function. An issue can be as simple as unnecessary computation or multiple successor steps performed instead of a single step or it can even be an infinite loop. Second, successor function is wrapped with a check whether it modifies the input state. Such modifications often happen when successor states are copied from the input state and modified. Shallow copy of the input state was observed in the previous work. Third, for every successor generated at the expansion step of BFS, a partial soundness check is performed, examining the validity of transitioning from the parent state to the successor state. An example of such a partial soundness check in 24 Game is that the successor state size must be one number less than the parent state. If that does not hold, the successor function is not sound according to Definition 1. It is worth emphasizing that this partial soundness check can be trivial, reporting True for every pair of parent and successor states. If any of the checks did not pass, we feedback the language model with the respective error message, providing example input state and the unexpected (or expected and unobserved) output, until all tests are passed or a predefined number of iterations was exhausted. See Listing 1 (middle) for an example feedback."}, {"title": "Step 4: Successor function completeness check", "content": "A successor function completeness check assumes the existence of a few known parent and successor states. These can include all successors for some parent state or a subset thereof. If the successor function does not produce some of the known successors, then it is not complete according to Definition 1. While completeness is not required for producing valid (sound) solutions, incomplete functions may not generate the part of the search space where goal states are located and therefore may not be able to find solutions. Improving completeness is therefore an optional step that may improve the accuracy of the produced code. Here as well, we give feedback to the language model with the respective error message, providing example input state and the missing successors. See Listing 1 (bottom) for an example feedback."}, {"title": "Automation, evaluation and validation", "content": "Since the expensive calls to large language models are not performed during search, there is no need to artificially restrict the algorithms to their incomplete variants and sound and complete algorithms BFS/DFS can be used for solving the search problems. Still, as the human feedback is before the feedback loop and the search components produced are not guaranteed to be sound, the solutions produced must be validated for soundness."}, {"title": "Experiments", "content": "In order to check the feasibility of our approach, Auto-ToS, we conduct experiments with a representative collection of five search/planning problems: Blocks World , PrOntoQA , Mini Crossword and 24 Game , and Sokoban . Four of these domains appeared in ToS , while the Sokoban domain did not. We test the performance of various LLMs from three families, using both the largest and smallest models from the same family. Specifically, we use GPT-40 and GPT-40-Mini , Llama3.1-70b and Llama3.1-405b , as well as DeepSeek-CoderV2 . We additionally tested Llama3-70b , Mistral7x-8b , and DeepSeek-CoderV2-Lite, finding these models to perform poorly and therefore excluded from consideration. We use Greedy decoding with maximum context length for each model. For each domain, we restrict the number of calls to the language model per function to 10 (total maximum of 19 per domain). We repeat each experiment 5 times.\nFollowing ToS, we use a simple implementation of BFS and DFS search algorithms in Python. DFS is used for Mini Crosswords, while BFS is used for the other 4 domains. Each successor function execution is limited to 1 second and each overall search is limited to 600 seconds. For each domain, a few (up to 10) instances are used for creating the unit tests. In one case, these instances are taken out of the available set of instances, in other cases we invent new instances. The rest are used for evaluating the accuracy of the generated code, where accuracy measures the percentage of the instances solved. In the case of BFS search, we also require the solution produced to be optimal. This is relevant to Blocks World and Sokoban where the solution length matters, but irrelevant for PrOntoQA, where solution"}, {"title": "24 Game", "content": "The 24 Game takes 4 integers as an input that can be manipulated through the four most common arithmetic operations: addition, subtraction, multiplication, and division. The goal of the game is to produce a formula that evaluates to 24, if one exists. States are represented as lists of length 4 or less.\nData We use the set of 1362 instances and we take out the first 10 instances for unit tests. Goal unit tests use [24] for goal and [], [3],[24, 1], [1, 6, 4], [1, 1, 4, 6] for non-goal examples. Successor completeness test uses the initial state with all its successors for each of the 10 instances, as well as a single transition along a known solution path for each of these instances. For example, the successors of [6, 6, 6, 6] are [1, 6, 6], [6, 6, 12], [0, 6, 6], and [6, 6, 36]. Also, a successor of [6, 6, 12] along the known solution path is [6, 18] and of [6, 18] is [24].\nPartial soundness test For the partial soundness test we check whether the number of elements in a successor state is one less than for the parent state.\nSolution validation A solution is a sequence of states \\(s_0, s_1, s_2, s_3\\), where \\(s_0\\) is the initial state, \\(s_3 = [24]\\) is the goal state, and \\((s_0, s_1), (s_1, s_2)\\), and \\((s_2, s_3)\\), are valid transitions. We check that all these hold for a given sequence."}, {"title": "Blocks World", "content": "Blocks World is a classic AI planning domain, where the task is to rearrange blocks in towers . There are 4 actions: stack a block on top of another block, unstack a block from another block, put a block down on the table, and pick a block up from the table. States are represented as dictionaries based on 'clear', 'on-table', 'arm-empty', 'holding', and 'on', describing whether a block is clear (no block above it in the tower), the block is on the table, whether the arm is not holding a block and which blocks are on which.\nData The domain has a PDDL representation and a large collection of 502 instances was created by Valmeekam et al. and used in the recent literature . We use the entire collection for evaluation and invent 2 example states (and transitions along 2 plans) per unit test. The examples can be found in the Appendix.\nPartial soundness test For the partial soundness test we notice that in each tower there is a top block (that is clear) and there is a bottom block (that is on the table). Therefore we simply check that the number of blocks in the 'clear' list is the same as in the 'on-table' list.\nSolution validation As the instances are given in PDDL, we simply translate the solution into a PDDL format and use an external validator VAL ."}, {"title": "Mini Crosswords", "content": "The mini crosswords is a 5x5 crosswords dataset where the input describes the 5 horizontal and 5 vertical clues and the output is the full 25 letters board. We provide a list of horizontal and vertical clues which are strings of words. The verifier ensures that the size of each word in the rows or columns does not exceed 5.\nData We use the existing 20 instances , all used for evaluation, with the unit tests constructed based on 3 invented states each, with the successor completeness based on a state in which one horizontal and one vertical clue already filled, which limits the number of possible successors considerably.\nPartial soundness test The partial soundness test verifies that at most 5 new letters are filled in one transition, as well as that the number of unfilled letters does not get larger.\nSolution validation A crossword puzzle is solved if the end result is valid, meaning every vertical and horizontal clue is present in the list of possible clues."}, {"title": "ProntoQA", "content": "Logical reasoning can be viewed as a search problem of finding a sequence of logical rules that when applied to the known facts, derive or disprove the target hypothesis. Previous work applies MCTS with successor function and rewards obtained by calling an LLM, to examples from the PrOntoQA dataset to derive the answer but also the proof, a sequence of reasoning steps. A state is therefore a set of the facts known to be true.\nData We use the existing set of 4000 instances entirely for evaluation, inventing 3 examples per unit test.\nPartial soundness test A partial soundness test simply checks that each transition adds a single known fact to the state, ensuring that the state size increases by exactly 1.\nSolution validation In order to validate the solution, we compare to the known correct answer."}, {"title": "Sokoban", "content": "Sokoban is a planning problem with PSPACE-complete complexity even for non-optimal planning. The problem, despite its simple conceptual rules, is a notoriously hard for generic AI planners and even for specialized solvers. We use a 2-D grid setup, in which, given equal number of boxes and goal squares, the player needs to push all boxes to goal squares without crossing walls or pushing boxes into walls. The player can only move upward, downward, leftward and rightward where many pushes are irreversible. The domain has a known planning model, described in PDDL of varying grid sizes and difficulties. States are represented as dictionaries with entries: 'at-player,' which represents a single pair of coordinates, and 'at-stone', a list of coordinates for the stones.\nData We use the collection of PDDL problem instances from the International Planning Competition (IPC) 2008. Out of these instances, we select a subset that can be solved relatively quickly by using the blind search configuration of the efficient planner Fast Downward and choose the instances that were solved in under 5 seconds. This resulted in 11 instances. We use the entire set for evaluation and invent 3 states per unit test.\nPartial soundness test The test simply checks whether the locations of the player and the stones are all different.\nSolution validation Similar to Blocks World, we translate the solution to PDDL format and use VAL."}, {"title": "Code Errors Discussion", "content": "To be able to improve the performance of the large language models in generating search components, it is important to understand the errors in the code produced by these models. In what follows we first present the error categories and show the partitioning of the errors to these categories and then elaborate on a few interesting cases."}, {"title": "Error categories", "content": "AutoToS distinguishes 10 error categories and gives each a separate feedback.\n1. succ soundness test failed.\n2. Input state changed by succ.\n3. succ completeness failed.\n4. isgoal soundness failed.\n5. succ exception occurred.\n6. isgoal exception occurred.\n7. Search timeout in succ soundness test.\n8. succ execution took too long.\n9. isgoal execution took too long.\n10. Response parsing error.\nInterestingly, we did not observe any errors in the last two categories. Further only 1, 2, and 3 errors in categories 6, 8, and 7, respectively. The partition of the errors to the other 5 categories , shows just how much the models differ in the type of errors produced. Interestingly, the DeepSeek-Coder-V2 model rarely produces code that triggers exception or changes the input state and even typically passes the goal soundness test. Other models, especially the smaller ones, are more diverse in errors produced. Across all models, the majority of the errors account for the failed successor soundness and completeness tests."}, {"title": "Bloopers", "content": "We noticed a few \"bloopers,\" interesting phenomena that occur during AutoToS. We share these observations in a hope of shedding some light onto future understanding of LLM code generation for planning and search problems.\nThe first blooper occurs in the 5x5 Crossword for Llama3.1-70b. The representation of a Crossword instance includes vertical and horizontal clues which are lists of 5 words each. The model handles horizontal clues well by simply checking whether a word in row i is in the ith list in horizontal clues. For vertical clues, however, the model checks whether the word in column i is at position i among the clues for every column. Indeed the initial prompt from obtaining successor function clearly states that:\n[...] horizontal_answers is a list where element i is a list of possible answers to clue in row i, and vertical_answers is a list where element i is a list of possible answers to clue in column i.\nThe second blooper occurs in the GPT-40-mini, Llama3.1-70b, and even in Llama3.1-405b on the Blocks World domain. When generating successors for the unstack block from another block action, the models check if the block is clear, but never actually check whether the arm is empty. The resulting code, in cases when a block is already held, can generate a successor state in which the held block is overwritten with the one that is unstacked, and therefore disappears from the state. On some instances in the evaluation set the situation does not occur. On others, invalid solutions are produced and the accuracy score falls far below 100%. The AutoToS feedback in the next iterations often solves the problem.\nAnother blooper occurs in Sokoban, when Llama3.1-70b generates the initial successor function and the goal test, and no partial soundness check is performed. The model generates a helper function is_clear that only checks whether the location on the grid is 0 or 2 (not a wall), disregarding whether any of the stones are currently at the location. This allows the player to move and push stones to the locations of other stones, resulting in the accuracy score of 0. Since the unit tests pass in this case, no additional iterations were performed. The partial soundness check would catch the error the first time a faulty state is generated (a state where multiple stones are at the same location or a player and a stone are at the same location). The prompt explicitly states what it means to be clear:\nThe maze is defined by a grid of values 0,1, and 2, where 2 means it is a goal location for a stone, 1 means the cell is blocked, and either 0 or 2 means that the cell can be occupied. A cell is clear if it can be occupied, but is not occupied by either the player or any stone.\nYet another blooper happens in 24 Game with GPT-40-mini and DeepSeek-CoderV2 when no partial soundness check is performed. When creating a new state out of the input state, two numbers are chosen to perform an arithmetic operation and in order to obtain the remaining numbers, the code selects the numbers from the state that are different from the two chosen numbers. Thus in cases of duplicate numbers, the state size becomes more than one smaller than of the parent and on some instances the produced solutions would not be valid. The AutoToS completeness feedback eventually solves the problem in these cases."}, {"title": "Conclusions and Future Work", "content": "We automate the process of generating correct and sound code for the search components by leveraging debugging and exception handing with natural language, code feedback, iterative reprompting. We demonstrate the performance of our approach, AutoToS, across various sized models and across search problem domains used by the planning community. With just a few calls to the language model, we demonstrate that we can obtain the search components without any direct human in the loop feedback, ensuring soundness, completeness, accuracy, and nearly 100% accuracy across all models and all domains.\nFor future work it would be interesting to see if the language models could generate the unit tests as well as the partial soundness tests instead of relying on the user writing these for a specific domain. The partial soundness test is related to the notion of invariants in planning . It is worth exploring whether LLMs can help us derive such invariants. Finally, seeing that smaller language models can achieve accuracy on par with the largest ones, begs the question of whether it would be possible to finetune an even smaller model and achieve a similar or better accuracy."}, {"title": "A Additional data for experimental domains", "content": "We provide additional information on the domains included in our experimental evaluation, such as examples used in unit tests, code for the partial successor soundness test, etc."}, {"title": "A.1 24 Game", "content": "Goal Unit Test Goal unit test cases are stored in two jsonl files, one for goal states and one for non-goal states."}, {"title": "Partial Successor Soundness Test", "content": "def validate_transition_complex(s, t):\n    if len(s) - len(t) != 1:\n        feedback = prettyprint(\"Invalid transformation: length mismatch - the length of a successor must be one less than the parent.\")\n        feedback += prettyprint(\"Let's think step by step. First think through in words why the successor function produced a successor that had a length that was not exactly one less than the parent. Then provide the complete Python code for the revised successor function that ensures the length of a successor is exactly one less than the parent.\")\n        feedback += prettyprint(\"Remember how you fixed the previous mistakes, if any. Keep the same function signature.\")\n        return False, feedback\n    return True"}, {"title": "A.2 Blocksworld", "content": "Goal Unit Test Goal unit test cases are stored in two jsonl files, one for goal states and one for non-goal states, depicted in Listings 5 and 6."}, {"title": "A.3 5x5 Crosswords", "content": "Goal Unit Test Goal unit test cases are stored in two jsonl files, one for goal states and one for non-goal states."}]}