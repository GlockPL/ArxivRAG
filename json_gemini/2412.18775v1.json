{"title": "ObitoNet: Multimodal High-Resolution Point Cloud Reconstruction", "authors": ["Vinay Lanka", "Apoorv Thapliyal", "Swathi Baskaran"], "abstract": "We present ObitoNet, a multimodal framework for high-resolution point cloud reconstruction that effectively fuses image features and point cloud data through a Cross-Attention mechanism. Our approach leverages Vision Transformers (ViT) to extract rich semantic features from input images, while a point cloud tokenizer \u2014utilizing Farthest Point Sampling (FPS) and K-Nearest Neighbors (KNN)\u2014captures local geometric details. These multimodal features are combined using a learnable Cross-Attention module, which facilitates effective interaction between the two modalities. A transformer-based decoder is then employed to reconstruct high-fidelity point clouds. The model is trained with Chamfer Distance (L1/L2) as the loss function, ensuring precise alignment between reconstructed outputs and ground truth data. Experimental evaluations on standard benchmark datasets, including ShapeNet, demonstrate that ObitoNet achieves comparable performance to state-of-the-art methods in point cloud reconstruction. The results highlight the robustness and efficacy of our multimodal feature fusion approach in generating high-resolution point clouds, even from sparse or noisy inputs.", "sections": [{"title": "1 Introduction", "content": "Point clouds are collections of discrete data points in three-dimensional space, typically represented as (x, y, z) coordinates. They serve as a critical data structure for representing 3D objects and scenes in fields like computer vision and robotics. However, point clouds are inherently sparse, unordered, and irregular compared to structured data formats like images or grids, posing significant challenges for deep learning-based methods. Consequently, reconstructing high-resolution point clouds from sparse or noisy inputs remains a major research challenge.\nRecent advancements in computer vision have shown the effectiveness of ViTs in capturing semantic information from images using patch-wise self-attention. For point-cloud data, methods like PointNet and PointNet++ provide effective frameworks for processing un-ordered 3D points. Techniques such as Farthest Point Sampling (FPS) and K-Nearest Neighbors (KNN) enable the grouping and sampling of points to preserve local geometric structures. While image and point-cloud-based methods perform well independently, their combination has the potential to enhance high-resolution reconstruction. The core idea is to guide the model in bridging gaps within the point cloud by leveraging complementary features extracted from images. This is achieved through Cross-Attention, which seamlessly integrates image and point-cloud modalities, enabling the generation of detailed and high-resolution point clouds. Our approach consists of three major steps:\nFeature Extraction. For images, we leverage ViTs to extract rich semantic features by treating images as sequences of patches and applying patch-wise self-attention. For point clouds, we utilize a tokenizer that combines FPS to select representative points with KNNs to group local neighborhoods, effectively capturing fine-grained geometric details while maintaining spatial structures."}, {"title": "2 Related Work", "content": "Reconstructing high-resolution 3D point clouds is a core challenge in computer vision and 3D processing. Early methods, like PointNet [Qi et al., 2017] [4], introduced frameworks for unordered point sets but struggled with capturing fine-grained geometric details and handling high-resolution data efficiently. Subsequent approaches, such as PointNet++ [Qi et al., 2017b][3], improved local feature aggregation, while voxel-based methods [Graham et al., 2018][16] enabled efficient processing but faced memory limitations. Building on these foundations, our work tackles the challenge of fine-grained reconstructions by leveraging Cross-Attention to integrate spatial and visual features.\nMultimodal approaches, like multi-view CNNs [Su et al., 2015][15], demonstrate the value of using visual data to complement sparse 3D inputs. Similarly, the success of Transformers [Vaswani et al., 2017][2] in feature fusion has inspired their application in point cloud tasks. For example, PointBERT [Yu et al., 2021][6] and PointMAE [Pang et al., 2022][3] showcase the effectiveness of transformer-based pretraining for point cloud representation and reconstruction.\nBuilding on these advances, our proposed approach, ObitoNet, leverages Cross-Attention modules to integrate high-resolution spatial and visual features for accurate point cloud reconstruction, aligning with recent methods like Neural Radiance Fields (NeRF) [Mildenhall et al., 2020][9], while maintaining computational efficiency."}, {"title": "3 Methodology", "content": "This section outlines the architecture, dataset generation, and training strategies employed in the proposed ObitoNet framework. The model integrates multimodal data from images and point clouds using a modular design, consisting of an image tokenizer, a point cloud tokenizer, and a cross-attention-based decoder. The following subsections detail the model's components, the dataset preparation process, and the training methodology, emphasizing how each step contributes to leveraging the complementary strengths of visual and geometric data. We also describe the loss function, training order, and innovations that ensure robust and accurate reconstruction. The modular nature of the framework enables both end-to-end learning and component-wise optimization, making ObitoNet versatile for a range of applications in 3D processing and computer vision."}, {"title": "3.1 Architecture", "content": "As shown in Figure 2, ObitoNet consists of three key components: an image token extractor, a point cloud token extractor, and a Cross-Attention-based decoder that reconstructs the point cloud.\n1. ObitoNetImg: Employs a standard Vision Transformer (ViT) model to extract rich and meaningful image tokens, which serve as semantic representations for downstream processing.\n2. ObitoNetPC: The point cloud data is segmented and grouped into smaller clusters using FPS to select representative points and KNN to capture local neighborhoods, enabling effective processing of fine-grained geometric structures.\n3. ObitoNetCA: A Cross-Attention mechanism integrates image tokens with point cloud tokens, leveraging visual cues to enhance geometric reconstruction and refine spatial details."}, {"title": "3.2 Dataset Generation", "content": "For evaluating the performance of ObitoNet, we used the Tanks and Temples dataset, a benchmark for multi-view 3D reconstruction tasks. This dataset provides high-quality 3D point clouds and corresponding 2D RGB images of real-world scenes, offering rich geometric and visual data for training and validation.\nTo create paired images and point clouds, we projected the 3D point cloud data onto a 2D plane, generating a reference image representation. This reference was compared against existing images in the Tanks and Temples dataset using similarity metrics. After testing several approaches, we found that CLIP, a vision-language model, was the most effective for identifying matches based on visual and contextual features. Images and point clouds were paired when their similarity score exceeded a predefined threshold, ensuring high-quality matches.\nOne key challenge was the scarcity of point clouds with corresponding point-of-view (POV) images. While datasets like KITTI provide full 360-degree point clouds, their associated images typically capture only limited portions of the scene, leading to incomplete visual coverage. This mismatch makes such datasets unsuitable for high-resolution reconstruction tasks that rely on aligning image cues with geometric structures. ObitoNet specifically depends on aligned POV point clouds and images to learn how to fill in gaps and refine details, a capability not achievable with datasets that lack comprehensive correspondence."}, {"title": "3.2.1 Point Cloud Data", "content": "The Tanks and Temples dataset includes dense and detailed 3D point clouds reconstructed from multi-view stereo (MVS) algorithms. These point clouds represent complex objects and scenes such as buildings, statues, and landscapes. The raw point cloud data is processed as follows:\n\u2022 Normalization: All point clouds are normalized to fit within a unit sphere centered at the origin to ensure scale invariance during training.\n\u2022 Downsampling: To reduce computational overhead, point clouds are downsam-pled using Furthest Point Sampling (FPS), which retains the most geometrically representative points.\n\u2022 Grouping: FPS-sampled centers serve as anchors, and a K-Nearest Neighbors (KNN) algorithm groups neighboring points into clusters of size M. This results in structured input groups $N\\in R^{BXGXM\\times3}$, where B is the batch size, G is the number of tokens, and M is the number of points in each group.\nThe resulting point cloud tokens $P\\in R^{B\\times G\\times C}$ are extracted using the PCTokenizer module and passed to the transformer encoder."}, {"title": "3.2.2 Image Data", "content": "The dataset also provides corresponding multi-view RGB images, ensuring spatial alignment with the 3D point clouds. Each image captures the scene from a unique camera perspective, ensuring full coverage of the target object or scene. The image preprocessing steps are as follows:\n\u2022 Resizing: All images are resized to a fixed resolution $H\\times W$ to ensure compatibility with the Vision Transformer (ViT).\n\u2022 Patch Embedding: Each image is divided into non-overlapping patches of size $P x P$, where P is determined based on the number of point cloud groups G. The patches are flattened and projected into a latent embedding space.\n\u2022 Feature Extraction: The images are passed through a pretrained Vision Trans-former (ViT). The CLS token is discarded, and the remaining patch tokens $T_{img} \\in R^{B\\times G\\times C}$ are retained as input tokens for cross-modal fusion."}, {"title": "3.3 Multimodal Fusion via Cross-Attention", "content": "The Cross-Attention mechanism acts as a bridge between the two modalities-image tokens and point cloud tokens facilitating effective information exchange and alignment. The point cloud tokens encode compact geometric features through grouping and sampling, while the image tokens, extracted by the Vision Transformer (ViT), provide rich semantic rep-resentations by processing the input image as sequences of patches. The encoded point cloud tokens, and the image tokens are seamlessly fused through the Cross-Attention mechanism, enabling the model to leverage complementary features for enhanced reconstruction.\nWithin the Cross-Attention block, the point cloud tokens serve as the primary focus, acting as the queries (Q), while the image tokens provide complementary information as the keys (K) and values (V). This interaction enables the model to establish meaningful relationships between the two modalities. Specifically, the point cloud tokens query the image tokens to extract relevant semantic information that aligns with and enhances the geometric structures represented in the point cloud."}, {"title": "3.4 Transformer-Based Decoder", "content": "The fused features obtained from the Cross-Attention block are further processed by a Transformer-based Decoder, which refines these features to ensure they are coherent, detailed, and ready for reconstruction. The decoder consists of multiple layers that itera-tively process the fused features using self-attention and feedforward networks, enabling the model to capture fine-grained details and improve feature quality."}, {"title": "3.4.1 Chamfer Loss", "content": "Chamfer Loss evaluates the distance between the predicted point cloud and the ground truth point cloud by measuring how close the points in one set are to their nearest neighbors in the other set. This property makes it particularly effective for tasks such as point cloud reconstruction and completion.\nTo evaluate the alignment between the predicted point cloud $P_{recon}$ and the ground truth point cloud $P_{gt}$, we utilize the Chamfer Loss, defined as:\n$L_{Chamfer}(P_{recon}, P_{gt}) = \\frac{1}{|P_{recon}|} \\sum_{x \\in P_{recon}} min_{y \\in P_{gt}} ||x - y||_2 + \\frac{1}{|P_{gt}|} \\sum_{y \\in P_{gt}} min_{x \\in P_{recon}} ||y - x||_2$, (1)\nwhere $|| ||_2$ denotes the Euclidean distance, and $| |$ refers to the number of points in each set."}, {"title": "3.5 Train Order", "content": "The pretrained transformers utilized in our framework have been primarily trained for tasks such as classification and segmentation. Consequently, a carefully defined training sequence was required to ensure that each model could function as an independent unit while con-tributing to the overall reconstruction task. The training process was divided into three distinct stages:\n\u2022 Train Order 1: In this stage, we trained ObitoNetPC and ObitoNetCA in-dependently, replicating the approach of the PointMAE [3] architecture. The objective here was to train the model to fill gaps in the point cloud using only point cloud tokens.\n\u2022 Train Order 2: In the second stage, the ObitoNetPC and ObitoNetCA models were frozen, and the Cross-Attention module within ObitoNetCA was activated. This stage preserved the knowledge learned by ObitoNetPC and ObitoNetCA while allowing ObitoNetImg to generate image tokens specifically optimized for the re-construction task.\n\u2022 Train Order 3: In the final stage, all three models were trained together to perform the reconstruction task. This step allowed the models to collaboratively learn features specific to their tasks while refining their outputs.\nThis sequential training approach ensured that each model contributed effectively to the re-construction pipeline, while also allowing the models to integrate their respective modalities seamlessly."}, {"title": "4 Experiments", "content": "As outlined in the Dataset Generation section, we use the Tanks and Temples Dataset, which includes point clouds and corresponding images, to create point cloud-image pairs for filling gaps in the point cloud. The model was trained with these pairs by providing a masked point cloud and a scene image as inputs. The model was tasked with reconstructing a masked point cloud with three times the input points, upsampling and restoring missing geometry. Training minimized the Chamfer Distance loss between the reconstructed and original point clouds, enabling accurate restoration of fine-grained details while maintaining geometric consistency."}, {"title": "4.1 Experimental Setup", "content": "The experiments were conducted on 4 NVIDIA RTX A5000 GPUs within the Nexus cluster provided by the University of Maryland. The specific parameters and architectures used by each model are detailed in their respective subsections for clarity and reproducibility. The computational setup ensured efficient training and evaluation of the proposed framework."}, {"title": "4.2 ObitoNet/Base Model", "content": "The base ObitoNet model was trained using the specified Train Order sequence, ensuring a structured approach to model development."}, {"title": "4.3 ObitoNet/ViTMAE Model", "content": "The VITMAE model shares the same architecture as the ViT Base model, with the key difference being the use of ViTMAE as the image encoder instead of the standard ViT. The choice to use VITMAE over ViT stems from its suitability for reconstruction tasks in-volving hidden input point clouds. Unlike standard ViT, ViTMAE leverages self-supervised pretraining to reconstruct missing image patches, making it highly effective for tasks that require inferring and filling gaps. This aligns naturally with point cloud reconstruction, where the model needs to restore missing geometric details. By leveraging robust semantic"}, {"title": "4.4 ObitoNet/Large Model", "content": "The ObitoNet/Large model was designed to evaluate the scalability and effectiveness of the model with a larger token set and deeper decoder architecture. In this configuration, the ViT model was used as the image tokenizer, with an increased number of tokens and MAE decoder depth to enhance the capacity for learning complex features. This model was trained end-to-end for 2000 epochs to assess its performance under a prolonged training regime.\nIn this experiment, the model's larger capacity and deeper architecture were tested for their ability to handle high-resolution point cloud reconstruction and multimodal integration. The results highlight the trade-offs between model size, training duration, and performance, offering valuable insights into scaling strategies for multimodal frameworks."}, {"title": "5 Conclusion", "content": "We proposed a Cross-Attention-based pipeline for high-resolution point cloud reconstruc-tion. By integrating Vision Transformer image features with KNN-based point cloud pro-cessing, we achieve results comparable to PointMAE. The modular design of ObitoNet, consisting of three distinct components\u2014the image tokenizer (ObitoNetImg), the point cloud tokenizer (ObitoNetPC), and the Cross-Attention module\u2014opens opportunities for further exploration and downstream tasks. The image tokenizer, built on the ViT or ViT-MAE architectures, extracts semantic features, making it suitable for tasks such as image classification, object detection, and semantic segmentation. Similarly, the point cloud tok-enizer, utilizing FPS and KNN, captures compact geometric features that can be adapted for 3D object classification, scene understanding, and point cloud registration. Beyond these individual components, the Cross-Attention module offers potential for multimodal appli-cations such as augmented reality, robotics, and spatial reasoning tasks, where fusing 2D and 3D data is essential. By fine-tuning these tokenizers for specific use cases, ObitoNet can serve as a versatile framework to address diverse challenges in computer vision and 3D processing, contributing to advancements in multimodal learning."}]}