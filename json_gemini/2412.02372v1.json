{"title": "HERO: Hint-Based Efficient and Reliable Query Optimizer", "authors": ["Zinchenko Sergey", "Iazov Sergey"], "abstract": "We propose a novel model for learned query optimization which provides query hints leading to better execution plans. The model addresses the three key challenges in learned hint-based query optimization: reliable hint recommendation (ensuring non-degradation of query latency), efficient hint exploration, and fast inference. We provide an in-depth analysis of existing NN-based approaches to hint-based optimization and experimentally confirm the named challenges for them. Our alternative solution consists of a new inference schema based on an ensemble of context-aware models and a graph storage for reliable hint suggestion and fast inference, and a budget-controlled training procedure with a local search algorithm that solves the issue of exponential search space exploration. In experiments on standard benchmarks, our model demonstrates optimization capability close to the best achievable with coarse-grained hints. Controlling the degree of parallelism (query dop) in addition to operator-related hints enables our model to achieve 3x latency improvement on JOB benchmark which sets a new standard for optimization. Our model is interpretable and easy to debug, which is particularly important for deployment in production.", "sections": [{"title": "1 INTRODUCTION", "content": "Query optimization is one of the key research areas in data management [7], since the performance of a DBMS directly depends on the quality of query execution plans [6]. As traditional query optimizers often face issues in producing efficient plans [11], recently, quite a number of Machine Learning (ML) solutions have been proposed to address them including learned cost models [21], cardinality estimators [9, 15, 18, 19, 22], and E2E learned optimizers [14, 24]. However, all these approaches face the problem of generalization and eventual query performance degradation [20] which makes them risky for deployment in production. An alternative hint-based optimization model has been first proposed in [12] which can operate on top of an existing optimizer by guiding it with hints potentially leading to better query plans. However as we demonstrate in our paper, the approach of [12] (and its successors) still suffers from the main two issues: the lack of guarantees that suggested hints non-degrade query latency and long training (hint exploration) and inference times. The main reason for eventual query degradation is the use of NN-based models with which, as we show in our paper, it is hard to predict reliable hints, while the reason for the long inference is the need to deal with an exponentially large space of hint combinations.\nIn this paper, we propose a novel hint-based efficient and reliable optimizer (HERO) that addresses the problems of existing solutions. The contribution of our work can be summarized as follows.\n\u2022 We provide a formalization of the problem of reliable hint-based optimization (\u00a73).\n\u2022 We describe the architecture of HERO and ideas behind its components (\u00a74). To overcome long hint exploration and inference times, we propose a parameterized local search algorithm adaptable to varying time budgets. Additionally, we present a more lightweight and reliable alternative to NN models which solves the issue of prediction reliability. Our solution is provided by an ensemble of context-aware models making HERO interpretable, transparent, and easy to debug, which is important for real-world application.\n\u2022 We provide an in-depth study of existing NN-based models for hint-based optimization (\u00a75) and present results of a detailed experimental analysis of NN behavior (\u00a76) highlighting the core difficulties in achieving reliable generalization.\n\u2022 We provide experimental results demonstrating that HERO offers a safer and faster solution (\u00a77) and achieves a higher optimization ratio than the existing NN-based approaches. We also show that HERO components can be combined with NN-models to further boost optimization in scenarios when potential performance gain for some queries is prioritized over eventual degradations.\n\u2022 We provide open-source datasets that we used in our study on the limits of NN-based approaches. These include NN training code,\u00b9 a reusable platform for rapid evaluation of different query acceleration strategies\u00b2, model weights, and extended experimental results. We believe that the platform and datasets can be used as a testbed in the development of novel hint-based optimization tools avoiding the need to run costly query optimization experiments on a DBMS.\nFor experimental evaluation, we used two IMDb-based benchmarks: JOB benchmark [10] consisting of 113 queries and its skewed version SQ (sample_queries from the repository of [13]) with 40 queries. Additionally, we used"}, {"title": "2 RELATED WORK", "content": "One of the first approaches to learned hint-based optimization is Bao [12], which employs the idea of generating query plans with all possible hint combinations and evaluating them independently by using a trained NN. While this allows for potentially achieving optimal performance, in practice, Bao faces challenges with long training and inference times. Additionally, due to the black-box nature of its NN model, it suffers from poor interpretability and unreliable predictions. In [17], some of these problems were solved with a randomized algorithm and heuristics. However, it required a deep integration into the optimizer and still depended on expensive planning. As a result, this solution is only applicable to large-scale workloads, such as Microsoft's SCOPE.\nIn QO-Advisor [25] deployed in Microsoft's production systems, the work of Bao was further extended. To address the risk of performance degradation, the authors incorporated an A/B testing infrastructure into the pipeline for additional validation and exploration, while limiting hint exploration to local changes in only a single hint. Although this solution mitigates the degradation issue, it also limits the potential of hint-based optimization.\nIn AutoSteer [4], the authors proposed to iteratively explore the hint search space by using a greedy algorithm at the training and inference stages. As we have confirmed in experiments, the greedy algorithm tends to skip exploring many potential states, sometimes overlooking highly effective ones, which prevents it from implementing the full potential of hint-based optimization. Furthermore, due to the sequential nature the greedy algorithm can not be parallelised which leads to a big planning overhead at inference and makes the approach impractical.\nIn FastGres [23] instead of using a single NN the authors proposed to used several context-aware models. This approach allowed them to find a balance between the complexity and reliability. However, the authors did not pay attention to the resulting plan, without which, as we show in this paper, it is impossible to make reliable predictions. For this reason, we do not include FastGres into the list of competitors in our experimental comparison."}, {"title": "3 PROBLEM STATEMENT", "content": "We begin with a formal description of the underlying optimization problem and the intuition behind its main ingredients.\n3.1 Search Space, \nHints are typically used to guide an optimizer in the search for better plans [12], but they can also be used to control operation modes such as the degree of parallelism (query dop). For example (Fig. 1), for JOB query 25a, cardinality underestimation leads to INL Join, causing a significant index lookup overhead. Disabling INL Join leads to a Hash Join, but it is still inefficient due to data broadcast overhead amplified by the default use of 64 threads. Only by reducing dop was the overhead mitigated, unlocking the potential of join options. For query 6b, the overhead is not significant and controlling dop results in just an additional speedup.\nObservation (01): Controlling dop may be crucial for unlocking the potential of operation-related hints.\nMotivated by this observation, we extend the operation-related parameter space O$_{Ops}$ by parallelism control O$_{dop}$:\nO$_{Ops}$ = O$_{scans}$ \u00d7 O$_{joins}$ x O$_{dop}$\nThis extension significantly enhances the potential of hint-based optimization, increasing the potential boost from 2x to 3x on JOB benchmark (Fig. 2) 5. Note that improvement depends on a benchmark, with larger gains observed on more challenging workloads as"}, {"title": "3.2 Dependency Under Study", "content": "A query is processed in two stages: a planner PL creates a plan which is then executed. The influence of hints on query execution time is limited to changes in the plan which also depends on statistics Stats. Thus, the dependency between query latency and hints can be represented as\nt(\u03b8) = t$_{pl}$ (q, \u03b8, Stats) + t$_{ex}$ (PL(q$_{i}$, \u03b8, Stats), ...).\nSo, the task of hint-based optimization is to select such parameters \u03b8 that the queries q$_{i}$, characteristic for a workload Q, are executed as quickly as possible:\nt(Q) =  E$_{qi\u2208Q}$t$_{i}$ | \u03b8 \u2192 min.\n\u03b8\u2208\u0398\nObservation (O2): Query latency depends on hints only via their influence on the plan."}, {"title": "3.3 Optimization Problem", "content": "As one can readily see in Fig. 1, there is no single \u03b8 that optimizes all queries. To avoid manual hint selection, we need to build a predictive model M that, given some information info about a query, can predict an optimal (or at least 'good') set of hints \u03b8*. Since we may not know a workload Q in advance, our model must make reliable predictions for any query. The results in Figure 3 clearly show the problem of degradation due to improperly selected hints. So, our goal is to build a predictive model M that minimizes the average workload latency while ensuring reliable and useful predictions:\n E$_{qi\u223cPQ}$ [t$_{i}$ + t$_{i}$ | \u03b8=M(info$_{i}$)] \u2192 min,\nM\nVq$_{i}$ \u2208 supp(P$_{Q}$ ) : t$_{i}$ + t$_{i}$ | \u03b8=M(info$_{i}$) \u2264 t$_{i}$ | \u03b8$_{def}$,\nwhere t$^{M}$ is the time for executing the model and calculating info.\n3.4 Query Representation\nQuery as a text (info = q$_{i}$). If the query is represented solely by its syntactic form, ensuring the required level of reliability would require the model to have an understanding of query plans generated with various \u03b8s and their execution times. Given the inherent complexity of planner and dependency on data statistics, this approach looks problematic. Either the model must have knowledge on planner operation and current statistics, or the training data must be designed in a way to provide this information. The latter option looks highly impractical, considering the exponential number of hint combinations and the dependency of the plan on statistics. Besides, this approach is complicated by the problem of data privacy, because raw query text may disclose vulnerable information contained in query predicates.\nQuery as a plan (info = plan$_{def}$). Intuitively, traditional query optimizers incorporate decades of expertise, which may be used by the model. However, representing a query solely by its default plan is problematic: as shown in Table 3, predictions based only on the default plan are not reliable."}, {"title": "4 HERO'S DESIGN", "content": "In this section, we describe the ideas in the architecture of HERO that overcome the key problems of NN-based solutions (argued further in detail in \u00a76). The design of HERO is driven by the following key features/requirements:\n(R1) degradation risk management\n(R2) near optimal latency acceleration\n(R3) fast model training\n(R4) flexibility to balance between query acceleration and fast training"}, {"title": "4.1 Query Clusterisation: Contextual Predictions", "content": "Example in Fig. 4 shows that in order to guarantee requirement (R1) one has to consider the problem of plan collisions. Interestingly, in order to cope with that problem, we propose to turn plan collisions into advantage by clustering queries with the same default plan and by approximating the following conditional dependency:\n <plan$_{0}$, plan$_{j}$> \u2192 t$_{ex}$ (plan$_{j}$) | plan$_{def}$=plan$_{0}$ \u00b7\nFirstly, this allows us to take into account the actual statistics, since their impact on the planner behavior will be reflected in the plan. Secondly, this allows for sharing information between queries that look similar from the planner's perspective.\n4.2 Removing NN: Ensemble and Increased Reliability\nTo approximate this dependency, we developed an approach conceptually reminiscent to FastGres [23], resulting in an ensemble of context-aware models. Each model in the ensemble handles predictions for queries with the same default plan p$_{0}$. It contains information on (a) custom plans, (b) hints used to generate them, and (c) observed performance gains for queries with the same p$_{0}$. This design, combined with a specialized distance metric, allows us to control the reliability of predictions. The metric is defined as follows: plans are considered to be infinitely distant if their logical"}, {"title": "4.3 Hints Prioritization: Fast Inference.", "content": "The main issue with long inference times in existing hint-based optimizers is the need to generate a multitude of query plans for different hints. HERO solves this problem by using a model ensemble that leverages historical data to select promising hints without requiring multiple planning iterations. We organize the ensemble as a graph storage, where vertices represent observed plans and edges indicate changes due to hint application and the corresponding performance boosts (see Fig. 5). In this representation, data for the context-aware model associated with a plan p corresponds to a subgraph induced by traversing all edges from vertex p. So, a key feature of our data is that we store directly obtained prior plans and use them to avoid unnecessary planning calls. The inference procedure is as follows:\n\u2022 First, we generate the default plan (p$_{0}$) for the given query and identify the closest context-aware model by finding plans similar to it in the graph storage.\n\u2022 Second, we traverse relevant edges to find the most promising hints for planning, notably without the need to call the planner at each step, which greatly speeds up the procedure.\n\u2022 Third, after identifying the best hints, we plan the query with them.\n\u2022 And, finally, we assess the resulting plans (pE\u2192F) to ensure the reliability of the selected hints.\nNote on super-fast inference. Our model also supports super-fast inference, in which we use top-level clustering based on the query template, which can be calculated even without planning. Based on it, we can search for the most promising hints among all default plans of queries with the same template and plan all of them in a first batch along with the default hintset. Until our storage has grown, the number of such promising hints is small enough to put them all into a single batch with parallel planning.\n4.4 Semantics-Informed Search: Rapid Convergence to Optimum in Extended Search Space\nTo satisfy the performance requirement (R2), we propose utilizing both operation-based hints and dop control. While this approach significantly improves query latencies, it also rapidly increases the search space. Our experiments indicate that a simple greedy algorithm (see \u00a75.2 for a description) struggles with search in this space, often selecting suboptimal hint combinations (\u00a77.2). To combat this, we propose to bring knowledge about the semantics of operations inside the algorithm. Firstly, we propose to adjust dop simultaneously with settings for join-hints. Secondly, we use planner-specific combination of hints. For example, to turn off Index Nested Loop Join in openGauss we need to disable BitMap Scan and Index Scan as well as Nested Loop Join."}, {"title": "4.5 Parameterized Local Search Procedure: Balancing Between Performance and Exploration", "content": "To meet requirements (R3) and (R4), we propose a parameterization of the semantics-informed search algorithm that adjusts the changes made at each iteration. This approach seamlessly integrates into a local search framework, where the parameters define the size and structure of local neighborhoods, guiding the exploration process and enabling the algorithm to transition to more promising states when improvements are detected. The design of our parameterization rests on several intuitive assumptions about the search space:\n(A1) Join strategies generally have a larger impact than table scan operators.\n(A2) Simultaneously selecting the join type and degree of parallelism (dop) is highly efficient.\n(A3) Only minor adjustments to the default plan are often sufficient, meaning a few iterations may yield significant improvements.\n(A4) The most common openGauss mistake is related to applying Index Nested Loop Join to large relations.\nFor each of them, we introduced a different parameter that allows us to appropriately influence the shape of the neighborhood during local search. The effectiveness of this parameterization was experimentally confirmed in \u00a77.1.\n4.6 Ranking over Regression: Accelerating Exploration\nTraining HERO essentially involves collecting information for graph storage. Due to easy debugging and the ability to disable context-aware models, as discussed in \u00a74.2, HERO has lower data quality requirements compared to other solutions. This allows us to speed up training, e.g., by prioritizing the preservation of ranking between plans (we discuss its importance in (04)) rather than focusing solely on the accuracy of predicted execution times. To achieve this, we use a parallelized local search where all neighboring plans are explored (executed) at each iteration offline and simultaneously. Parallelization significantly reduces exploration time and in our experiments in a real environment we observed that the plan selected from parallel execution is often either the fastest one or very close to the optimal plan obtained from sequential execution. To further ensure the reliability of the ranking, periodic sequential pairwise comparisons between candidates can be performed during the exploration phase for validation."}, {"title": "5 DESIGN OF EXISTING NN-BASED SOLUTIONS", "content": "In the existing approaches to hint-based query optimization, regression ML models are typically used to predict plan execution time. They are combined with a Steering module responsible for exploring hint combinations and generating corresponding plans (Fig. 6). In this section, we analyze the main steps of this pipeline and in the subsequent section we analyze its limitations.\n5.1 Model\nEncoding. One of the main challenges in using NNs for predicting plan properties lies in the fact that plans are structured objects of varying lengths, whereas conventional machine learning algorithms operate on fixed-size vectors. Moreover, each node within an execution plan contains significant information (e.g., statistics) that needs to be taken into account. A common approach is to use one-hot encoding for operations, along with additional channels representing statistics, such as estimated costs, cardinalities, and computed selectivities. However, if hints in a DBMS are implemented by heavily overestimating costs of alternative plans (this is the case for openGauss [1] and PostgreSQL [3], for instance) then it makes no sense to integrate costs into plan representation. Different options for node encoding are discussed in detail in [26].\nTree Vectorization. To vectorize the tree structure of plans, in [16] the authors proposed a special convolutional layer which is a variant of a graph convolutional network. Since query execution plans generally have at most two child nodes, this layer can be simplified to a 1-D convolution, similar to those used in image processing. This simplification enables the use of highly efficient techniques from image analysis. Combining these layers with non-linear transformations gives convolutional blocks. We refer to a sequence of such blocks as a Tree Convolutional Neural Network (TCNN) [16]. After passing the tree through it, the hierarchical structure of sub-trees is captured, and the tree can be easily vectorized using any aggregation function. The resulting vector can then be fed into a classical Fully Connected Neural Network (FCNN). In addition to having an efficient implementation, this idea also introduces an inductive bias into the model, because we 'assist' the model by embedding our prior knowledge about the data structure directly into the architecture. We note that this concept is actively developed in the field of Geometric Deep Learning [5]. By exploiting the inherent geometry of the data, the model is able to generalize more efficiently and learn more meaningful representations, as we demonstrate in \u00a76.\n5.2 Inference\nThe inference process in the existing NN-based solutions is implemented by a Steering module and looks as follows:\n\u2022 First, it produces execution plans for different hint combinations \u03b8j.\n\u2022 Second, Steering feeds them into NN to predict execution time tj.\n\u2022 Third, based on the obtained predictions, it selects the most promising hint set.\nIn general, the overall process can be iterated, refining hint suggestions and generating new plans for evaluation. We provide an illustration in Figure 6."}, {"title": "5.3 Training and Data Labeling", "content": "Default plans produced by a DB optimizer is not a sufficient source of information for training a good model, as it fails to provide information about the behavior of non-default plans. Thus, an exploration stage is required to analyze query latency under different hint combinations. Exploration can be made offline to avoid runtime degradations or online to account for data/performance drift. Exploration is costly: we have to wait for the query to execute with each of the hint combinations of interest. Moreover, some hints can slow down queries by orders of magnitude, making labeling prohibitively expensive. Efficient exploration is the second principle challenge (C2) in learned hint-based optimization. To solve this problem, we can narrow down the exploration area similarly to the greedy algorithm used at the inference stage. However, as we found out in practice, the greedy algorithm misses good solutions and does not allow for unlocking the full potential of optimization with hints."}, {"title": "6 ON THE RELIABILITY OF NNS", "content": "In this section, we provide results of an experimental study on limitations of NN models for hint-based optimization."}, {"title": "6.1 Experimental Setup", "content": "Data Split. Here and later in the paper we use queries from the IMDb dataset (JOB and SQ). Initially, we grouped all plans for JOB queries under all possible hint combinations, splitting them 4:1 into train and validation subsets randomly. Additionally, we split SQ queries into two groups: those with default logical plans matching JOB queries (Test) and those without (out of distribution, OOD). This allowed us to reflect the similarity of the queries with those from JOB. All learning curves and metrics mentioned in this section are averaged over five runs.\nBaseline. In our experiments, we used NN architectures similar to those in Bao [12] and AutoSteer [4]. As shown in Fig. 7, experiments with varying ratios of TCNN and FCNN blocks demonstrate that adding TCNN greatly improves plan vectorization and simplifies prediction tasks. Additionally, we observed that tree normalization layers play a critical role in building an efficient architecture. Considering the semantics of our data, we implemented and applied a"}, {"title": "6.2 Insights", "content": "Noised Data. To identify which features are most important and what NN relies on in making predictions, we set the following experiment. We considered three types of noised plans: (a) noise instead of information about operators in plan nodes, (b) noise instead of node statistics, and (c) fully noised nodes. The results shown in Figure 8, confirm our intuition that statistics encapsulate optimizer's 'knowledge' into a compact form and are an important feature. For example, by relying on cardinality, NN in fact, becomes a learned cost model. However, due to the different asymptotic behaviors of operators, NN struggles with situations when noise replaces information about them.\nInsight (I1): Statistics implicitly represent the logic of DB optimizer and are an important feature, positioning NN as a cost model with time as the unit.\nInsight (I2): Plan collisions significantly complicate the optimization landscape so that even a uniform noise, playing the role of an artificial labeling, helps to simplify optimization.\nTrain Data. Using the timeout value as a label for degraded queries resulted in a optimistic labels of execution time. As a consequence, some of NN predictions started to look like an overestimation error (Fig. 9a). In a detailed analysis we found that the model was correct: it identified a trend by 'looking' at neighboring plans with significantly higher execution times and produced predictions close to the real execution time. But we note that in practice there may not be enough information about informative neighboring plans.\nInsight (I3): Due to the need to represent knowledge of all plans in the weights of a single NN, any inconsistency (inevitable due to challenge (C2)) may lead to error.\nLearning Dynamics. We also observed a consistent overestimation of plans with low values, even when there were no nearby high-value plans. This likely stems from the weight learning dynamics, particularly due to the use of the softplus function. In regions where predictions are around 200ms, the gradient during backpropagation decreases by roughly a factor of ten. As a result, for later training stages, the small learning rate makes it difficult to adjust weights in these regions.\nValidation Data. The real challenges of generalization become apparent when analyzing errors on plans not included in the training set (confirming our intuition about the complex nature of the approximated surface). We observed regions with steep slopes, where a lack of information led to overestimations, and flatter areas with isolated peaks, resulting in underestimations due to sparse data (see (b) and (d) in Fig. 9). Despite validation errors were noticeably higher than those on the training data (Fig. 10), the model was still able to capture key patterns in predicting execution times. This led us to the following question: are NN predictions at least more reliable than optimizer cost estimates? To answer this, we first measured two types of correlations between neural network prediction / optimizer cost and query runtime. The results are presented in Table 4. Observing that neural network predictions are often more correlated with the actual query execution times, we decided to compare the performance of the best plan selected from top-k candidates ranked by either optimizer costs or NN-predicted values (Fig. 11). As a result, we see that NN can indeed be effectively used for ranking tasks on validation and test data, although no such advantage is observed on the OOD workload.\nInsight (I4): The complex geometry of the studied surface challenges reliable generalization, yet NN is able to outperform DB optimizer in ranking accuracy, provided there is no distribution shift.\nTest Data. Analyzing the behavior on the test dataset (in which plans come from queries not present in the training data), we observed an interesting phenomenon: the errors were significantly lower than those on the validation dataset (Fig. 10). To understand it, we need to recall that we split plans from SQ into OOD and Test based on their logical structure and consider how TCNN processes tree structures. We computed the ratio of subtree structures from the Validation and Test sets that matched substructures present in the Train. We can observe a general trend - the more complex substructures of plans we consider, the greater the advantage of"}, {"title": "7 EXPERIMENTAL EVALUATION OF HERO AND COMPARISON WITH NN-BASED MODELS", "content": "Emulation of NN-based Solutions As the inference stage in NN-based solutions mirrors the exploration phase of HERO, we augmented them with the parameterized local search procedure from our approach. We implemented a NN architecture that aligns with those in Bao and AutoSteer (see Section 6) which allows for emulating existing hint-based optimizers by properly configuring local search. For example, a NN that explores neighborhoods without semantic-informed operations (i.e., without jointly exploring join and dop hints, or using combinations like INL) serves as an emulation of AutoSteer. A simplified version, in which where the search is limited to a single iteration, logically emulates QO-Advisor. We will refer to these configurations as pruned ones. Bao, on the other hand, implemets a specific case of a local algorithm, where the neighborhood encompasses the entire search space, resulting in exhaustive search.\n7.1 Local Search Parameterization\nWhat are the optimization capabilities of HERO under different training time budgets?"}, {"title": "7.2 Static Workload", "content": "In ideal conditions, can NN-based solutions outperform HERO?\nOffline Scenario. The results of comparing HERO and NN-based solutions in conditions, where all data was observable at training and the exploration budget was unlimited, are presented in Table 8. Notably, even in these ideal conditions, NN-based solutions experienced timeouts and led to query latency degradations. The primary advantage of HERO over NN-based approaches lies in its inference which is more than five times faster.\nSummary. Even in ideal conditions, NN-based solutions lead to degradations, while HERO makes reliable hint recommendations, while its main performance advantage is in utilizing query-specific information to reduce the inference time.\nOnline Scenario. We simulated an online learning scenario for NN-based algorithms in which we iterated through all JOB queries, selecting hints based on the current prediction of NN and updating its weights with the accumulated experience. The results presented in Figure 13 highlight three key findings:\n\u2022 As the search space grows, the inference overhead for Bao (Exhaustive) and AutoSteer (Greedy) becomes excessively large, making the approaches impractical, while the workload speedup obtained by QO-Advisor (Pruned Greedy) is minor.\n\u2022 The pruned version of our proposed Local Search algorithm alleviates this issue, significantly reducing inference overhead and improving NN-based methods.\n\u2022 The overall improvement remains well below what was achieved in the offline mode (Tab. 8). We believe that the reason is that in online learning, the model influences the data it later trains on, leading to local optima and the underestimation of higher-performing states.\nSummary. While NN-based models support online learning, the high inference overhead (C1) and the insufficient representativeness of the collected data (C2) make existing approaches impractical. Only the pruned versions show an acceptable performance, but it is well below what is achieved in the offline training mode.\n7.3 Dynamic Workload\nCan HERO generalize to new queries?\nWe also conducted experiments where the model performance was measured on unseen queries. Based on our observations about the properties of data that help or hinder knowledge generalization, we considered several ways of splitting the data: (a) splitting randomly, (b) splitting queries within a group with the same default plan structure, and (c) splitting queries by the execution time of their default plan. Results are given in Table 9. Three important observations can be made from them:\n\u2022 The versions with pruning consistently achieved the highest performance boosts. Not only did this reduce inference time, but it also significantly limited the search space, reducing the likelihood of selection of bad plans.\n\u2022 The presence of queries with similar plan structures in the training set greatly increases overall gains, supporting our insight (15) on the importance of plan structure for generalization ability. NN-based solutions can deliver higher performance boosts but often come with risks of degradations and timeouts. In contrast, HERO offers more moderate, but safer improvements.\n\u2022 Generalization from fast queries to slower ones was significantly more successful than the opposite. This is likely because exploration requires finding plans with shorter"}, {"title": "8 CONCLUSION", "content": "In this paper, we proposed HERO, an efficient and reliable hint-based query optimizer which sets itself apart from existing solutions by (a) replacing NN with a more interpretable, reliable, and controllable ensemble of context-aware models, and by (b) introducing a new parameterized local search procedure that efficiently explores queries and supports adjusting balance between training time and performance gains. We presented detailed experimental results showing that HERO sets a new level for query optimization with coarse-grained hints.\nWe have also provided an in-depth analysis of architecture, benefits, and limitations of the existing NN-based approaches to hint-based optimization and showed that several design decisions from HERO can be used to improve them. We believe that the results of our study will facilitate next steps to unlocking the full potential of hint-based learned query optimization.\nOur work has primarily focused on coarse-grained hints, which affect the entire query plan, but may not achieve optimal query acceleration. Fine-grained hint mechanisms already exist (e.g., pg_hint_plan [2] for PostgreSQL), which allow for setting the range of admissible operators at plan nodes and even taking control over the cardinality model and join order. However, with fine-grained hints, the issue of search space explosion becomes even more pronounced - the dimensionality increases not only with the size of the hint set but also with the complexity of the query itself. Nevertheless, we believe that fine-grained hints have a big potential, and future work could leverage this approach to further improve query optimization."}]}