{"title": "LLM-Net: Democratizing LLMs-as-a-Service through Blockchain-based Expert Networks", "authors": ["Zan-Kai Chong", "Hiroyuki Ohsaki", "Bryan Ng"], "abstract": "The centralization of Large Language Models (LLMs) development has created significant barriers to AI advancement, limiting the de mocratization of these powerful technologies. This centralization, coupled with the scarcity of high-quality training data and mount ing complexity of maintaining comprehensive expertise across rapidly expanding knowledge domains, poses critical challenges to the continued growth of LLMs. While solutions like Retrieval Augmented Generation (RAG) offer potential remedies, maintaining up-to-date expert knowledge across diverse domains remains a sig nificant challenge, particularly given the exponential growth of specialized information. This paper introduces LLMs Networks (LLM-Net), a blockchain-based framework that democratizes LLMs as-a-Service through a decentralized network of specialized LLM providers. By leveraging collective computational resources and dis tributed domain expertise, LLM-Net incorporates fine-tuned expert models for various specific domains, ensuring sustained knowledge growth while maintaining service quality through collaborative prompting mechanisms. The framework\u2019s robust design includes blockchain technology for transparent transaction and performance validation, establishing an immutable record of service delivery. Our simulation, built on top of state-of-the-art LLMs such as Claude 3.5 Sonnet, Llama 3.1, Grok-2, and GPT-4o, validates the effective ness of the reputation-based mechanism in maintaining service quality by selecting high-performing respondents (LLM providers). Thereby it demonstrates the potential of LLM-Net to sustain AI advancement through the integration of decentralized expertise and blockchain-based accountability.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) represent a significant breakthrough in artificial intelligence, functioning as deep learning algorithms capable of understanding, generating, and manipulating human language with remarkable proficiency. The landscape of LLMs un derwent a dramatic transformation with the introduction of GPT-3 in 2020, which demonstrated unprecedented capabilities in natural language processing with its 175 billion parameters. This marked a pivotal moment in AI history, as the model exhibited remarkable zero-shot and few-shot learning abilities, enabling it to perform tasks without explicit training [1].\nGenerally, the success of LLMs can be attributed to several key factors. First, advancements in GPU technology have enabled the training of increasingly large models, providing the necessary com putational power to process massive datasets. Second, the trans former architecture, introduced by Vaswani et al. in 2017, has proven to be highly effective for language modeling tasks, allowing for parallel processing and better capture of long-range depen dencies in text [2]. Lastly, the availability of large-scale datasets has been crucial in training these models to achieve human-like language understanding and generation.\nThe reliance on vast amounts of training data, however, has led to concerns about the future availability of high-quality training material. As models continue to grow in size and capability, re searchers are beginning to face the challenge of exhausting publicly available data suitable for training. Villalobos et al. estimate that if current LLMs development trends continue, models will be trained on datasets roughly equal in size to the available stock of public human text data between 2026 and 2032, or slightly earlier if models are overtrained [3]. This impending data scarcity could lead to a slowdown or stagnation in the pace of LLMs advancement and capabilities, as the models will no longer have new data to learn from.\nIn response to this looming challenge, the AI community has begun exploring alternative approaches to enhance LLMs perfor mance beyond the constraints of public data. One such technique, Retrieval-Augmented Generation (RAG) [4], has emerged as a promis ing solution by incorporating external knowledge sources into the model\u2019s processing. Nonetheless, the challenge of maintaining com prehensive, up-to-date expert knowledge across diverse domains remains significant, especially given the exponential growth of specialized knowledge across various fields. Moreover, attempts to overcome data limitations by sourcing private information from the public raise substantial privacy and ethical concerns, further complicating the path forward for LLMs development.\nThese challenges are further exacerbated by the current land scape of LLMs development, which is predominantly controlled by a few major technology companies. Such oligopolistic dominance may create structural barriers that affect service accessibility for certain regions or organizations due to various factors, including geopolitical restrictions, economic barriers, and regulatory con straints. Given these potential limitations, a promising path toward AI liberation lies in democratizing access to and development of LLMs, which could enable a broader community to participate in and benefit from these technologies.\nTo address these challenges and the limitations of centralized LLMs development, we propose a comprehensive approach towards AI liberation, beginning with the democratization of LLMs-as-a Service. As the name implies, it is a delivery model, where LLMs are provided as cloud-based services, enabling users to access and utilize LLMs capabilities through APIs or interfaces without man aging the underlying infrastructure. Our framework introduces the concept of LLMs Networks (LLM-Net), a blockchain-based in frastructure that enables unrestricted provision of LLM services."}, {"title": "2 RELATED WORK", "content": "In this section, we elaborate on the following important concepts in this paper, i.e., LLMs, structural reasoning with prompting strate gies, blockchain and smart contracts."}, {"title": "2.1 Large Language Models, Expert Models and RAG", "content": "LLMs have revolutionized natural language processing, demon strating remarkable capabilities in tasks such as text generation, translation, and question-answering [1]. While these models have shown impressive performance across various domains, they face significant challenges in specialized fields. As high-quality pub lic training data becomes increasingly scarce, LLMs struggle to maintain relevance and accuracy, particularly in rapidly evolving domains that require access to proprietary or latest information not present in their training data [5].\nIn contrast to the fundamental LLMs, fine-tuned LLMs, also known as expert models, are adapted to specific domains or down stream tasks using smaller, targeted datasets. These models offer improved performance and control in specialized contexts but re quire labeled data and computational resources for fine-tuning. For instance, BioBERT [6] is a fine-tuned version of BERT, a popular language model, specifically trained on biomedical text. By leverag ing domain-specific data, BioBERT outperforms the original BERT model on various biomedical natural language processing tasks,"}, {"title": "2.2 Prompting Strategies for Structural Reasoning", "content": "Prompting strategies have emerged as powerful techniques to en hance the performance and output quality of LLMs. By carefully crafting and optimizing the prompts given to these models, users can guide them to generate more accurate, relevant, and coherent response. For the ease of explanation, we classify the prompting strategies into (a) collaborativeness , and (b) iterativeness as shown in Table 1.\nSingle-shot prompting is a non-iterative, non-collaborative ap proach, where a single well-crafted prompt is used to elicit the desired response from the model. Building upon this, iterative re finement introduces a progressive, yet still non-collaborative strat egy that involves multiple rounds of prompting, with each subse quent prompt building upon the model\u2019s previous responses [8]. Extending this concept further, Chain of Thought (CoT) emerges as a sophisticated prompting technique that encourages the model to break down complex problems into step-by-step reasoning, thereby improving performance on tasks requiring multi-step logical infer ence [9].\nMoving towards collaborative approaches, expert-guided prompt ing involves domain experts or external knowledge sources con tributing to the formulation of a comprehensive prompt. This con cept of leveraging diverse expertise is further developed in the Mixture of Experts (MoE) architectural approach, where multiple specialized sub-models are trained to handle different aspects of a task, with a gating mechanism dynamically routing inputs to the most appropriate expert [10]. Culminating these strategies, multi agent collaboration represents a repetitive, collaborative approach that involves multiple agents, including the language models, hu man experts, and potentially other AI systems, working together through multiple iterations to achieve the desired outcome. This progression from single-shot prompting to multi-agent collabo ration illustrates the evolving sophistication in LLM interaction techniques."}, {"title": "2.3 Blockchain and Smart Contracts", "content": "Blockchain technology is a decentralized and distributed ledger sys tem that enables secure, transparent, and tamper-resistant record keeping across a network of computers [14]. It operates on a peer to-peer network, where each node maintains a copy of the ledger, eliminating the need for a central authority. This decentralized architecture is crucial for maintaining the integrity and security of the data stored on the blockchain.\nMoreover, one of the significant innovations enabled by blockchain technology is smart contracts. In a nutshell, smart contracts are self-executing programs that automatically enforce the terms of an agreement when predetermined conditions are met. They operate on the principle of \"if/when...then...\" statements, encoded on the blockchain [15, 16].\nIn addition to this, smart contracts are used as the backbone of many applications in various industries, including finance, supply chain management, and healthcare nowadays. For instance, in sup ply chain management, when IoT sensors verify that goods have been delivered to a warehouse, the smart contract automatically triggers payment to the supplier and updates inventory records, creating a transparent and tamper-proof record-keeping system across the entire supply chain [17]."}, {"title": "3 LLM-NET", "content": "In this section, we elaborate the network architecture of the pro posed LLM-Net and the reputation-based mechanism that facilitates the network operation."}, {"title": "3.1 Type of Nodes", "content": "LLM-Net is a decentralized system of interconnected LLMs nodes that work together. In general, the coordinators act as intermediaries between the requesters and respondents. Meanwhile, the validators validate all the work from the rest of the nodes.\nThe followings elaborate the types of nodes in detail:\n(1) Requesters initiate the process by submitting queries to the network. These nodes represent users or systems seek ing information or solutions via LLM-Net.\n(2) Coordinators are decent LLMs that act as the controllers that orchestrate the structural reasoning process of the net works. Upon receiving a query from a requester, they select\nthe appropriate respondents in the structural reasoning processes and orchestrate the exchanges of information among the selected respondents.\n(3) Respondents, which include both fundamental and expert LLMs, process the queries routed to them by the Coordi nators in the structural reasoning process. They act as the debaters and generate responses.\n(4) Validators play a crucial role in maintaining the integrity of the network. They create and validate the blocks that record the queries, assess the quality and consistency of responses from respondents."}, {"title": "3.2 Queries Mechanisms", "content": "Figure 2 illustrates an example of the simplified network that con sists of a requester, coordinator and three respondents. The valida tors will be in the network to observe and validate the transactions. Initially, when respondents first join the network, they need to declare their expertise and continuously update everyone about their capabilities. For instance, a requester submits a query to the coordinator as illustrated in Figure 2(a). Upon receiving the query, the coordinator analyzes it to determine the required domain ex pertise. It then selects the relevant respondents to participate in the structural reasoning process, choosing the appropriate respondents based on their capabilities and historical performance (referencing the blockchain records).\nNext, the coordinator deploys a smart contract specific to this query (Figure 2(b)). The smart contract includes the query or task description, the selected respondents to be involved as proposers and debaters, the maximum number of discussion rounds, the ex pected response time, the reward structure for participating nodes, the quality criteria for the final answer, and the conditions for contract fulfillment.\nThe coordinator assigns the query to the selected respondents, designating them as proposers. As each respondent accepts the task, it enters into a subsidiary agreement under the main smart contract, committing to the terms and conditions. Validators verify the transactions and the smart contract deployment.\nNext, in Figure 2(c), the proposer generate initial responses to the query. These responses are then forwarded to the designated debaters for further discussion and refinement. The debaters engage in multiple rounds of discussion, as specified in the smart contract, to improve the quality and accuracy of the responses. Each round of discussion is recorded on the blockchain, and validators verify the transactions and the adherence to the smart contract terms."}, {"title": "3.3 Reputation", "content": "Reputation is a critical concept that facilitates the operation of LLM Net. To leverage this concept, the proposed framework incorporates a reputation mechanism to promote high-quality contributions and ensure fair compensation among participating respondents.\nBasically, the reputation mechanism operates on four fundamen tal assumptions:\n\u2022 Blockchain Immutability: The distributed ledger maintains an unalterable record of all transactions through crypto graphic validation across multiple nodes. This decentralized architecture prevents tampering by individual actors or mi nority groups.\n\u2022 Response Quality Function: The response quality can be expressed as a function of the coordinator and the selected respondents. Getting the poor performance respondents will degrade the response quality.\n\u2022 Reputation Sensitivity: Participating nodes, both coordina tors and respondents, exhibit reputation-sensitive behavior, optimizing their actions for long-term benefits rather than short-term gains.\n\u2022 Quality Discernment: Nodes possess sufficient analytical capabilities to detect and evaluate variations in response quality, enabling effective quality assessment and valida tion.\nWith these assumptions in mind, the peer evaluation process is triggered right before the culmination of the query process, where each participating respondent provides feedback on others\u2019 contri butions in the query. All feedback, transactions, and interactions are recorded in the blocks. The coordinator will refer to these records for subsequent queries in selecting the right respondents.\nGenerally, the numerical reputation scores are convenient, but not employed in this framework. Instead, interaction records and feedback are preserved in text form on the blockchain, eliminat ing the need for converting reputation into numerical scores. This approach enables future coordinator to make informed decisions in selecting the right respondents by directly accessing and in terpreting the detailed historical records of past interactions and performance assessments."}, {"title": "4 SIMULATION AND DISCUSSION", "content": "The simulation evaluates collaborative prompting and reputation based respondent selection in LLM-net, where coordinators interact with respondents of varying levels of domain expertise across dif ferent query types. As detailed in Section 3.3, rather than relying on a single reputation score to represent respondent performance, the simulation will demonstrate that coordinators select subsequent respondents based on feedback from previous collaborations.\nIt is crucial to acknowledge that our simulation heavily relies LLMs to illustrate the reputation-based mechanism. Specifically, we utilize not only Claude 3.5 Sonnet, a prominent LLM from An thropic, but also Llama 3.1, Grok-2, and GPT-4o to simulate both coordinator and respondent behaviors. While this approach enables us to demonstrate the potential of the reputation system, it may not fully encapsulate the complexities and nuances of real-world inter actions between different LLM providers. To address this limitation and create a more realistic simulation environment, we employ a strategy of constraining the models\u2019 capabilities through specific prompting techniques."}, {"title": "5 CONCLUSION", "content": "The development of LLMs faces three critical challenges: the im pending exhaustion of high-quality training data expected in the near future, the oligopolistic dominance by major technology com panies, and the growing complexity of maintaining comprehensive expertise across rapidly expanding knowledge domains. To address these challenges, LLM-Net proposes a novel approach by democ ratizing LLMs-as-a-Service through a decentralized network of specialized LLM providers. By leveraging collective computational resources and distributed domain expertise, this blockchain-based framework enables the unrestricted provision of LLM services while maintaining specialized knowledge across diverse domains through fine-tuned expert models"}]}