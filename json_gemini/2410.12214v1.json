{"title": "Order-Aware Interactive Segmentation", "authors": ["Bin Wang", "Anwesa Choudhuri", "Meng Zheng", "Zhongpai Gao", "Benjamin Planche", "Andong Deng", "Qin Liu", "Terrence Chen", "Ulas Bagci", "Ziyan Wu"], "abstract": "Interactive segmentation aims to accurately segment target objects with minimal user interactions. However, current methods often fail to accurately separate target objects from the background, due to a limited understanding of order, the relative depth between objects in a scene. To address this issue, we propose OIS: order-aware interactive segmentation, where we explicitly encode the relative depth between objects into order maps. We introduce a novel order-aware attention, where the order maps seamlessly guide the user interactions (in the form of clicks) to attend to the image features. We further present an object-aware attention module to incorporate a strong object-level understanding to better differentiate objects with similar order. Our approach allows both dense and sparse integration of user clicks, enhancing both accuracy and efficiency as compared to prior works. Experimental results demonstrate that OIS achieves state-of-the-art performance, improving mIoU after one click by 7.61 on the HQSeg44K dataset and 1.32 on the DAVIS dataset as compared to the previous state-of-the-art SegNext, while also doubling inference speed compared to current leading methods. The project page is https://ukaukaaaa.github.io/OIS.", "sections": [{"title": "1 INTRODUCTION", "content": "Interactive segmentation allows users to segment objects or regions using prompts, which can be in the form of clicks, scribbles, or bounding boxes. This task is indispensable for applications like image editing (Ling et al., 2021; Abdal et al., 2020), video object tracking (Cheng et al., 2024;"}, {"title": "2 RELATED WORK", "content": "Interactive segmentation. Current methods for interactive segmentation can be categorized into two groups: dense fusion and sparse fusion. Dense fusion includes RITM (Sofiiuk et al., 2022), FocalClick (Chen et al., 2022), SimpleClick (Liu et al., 2023), InterFormer (Huang et al., 2023), and SegNext (Liu et al., 2024a). While sparse fusion mainly involves DynaMITe (Rana et al., 2023), SAM (Kirillov et al., 2023), HR-SAM (Huang et al., 2024), and HQ-SAM (Ke et al., 2024). In dense fusion, user prompts are represented as dense maps and aligned with the image using attention mechanisms. However, this approach increases inference time significantly due to the costly attention between spatial feature maps (image and prompt dense map features). In contrast, sparse fusion converts user prompts into sparse embeddings, which lowers computational costs during cross-attention with image features. However, this efficiency comes at the cost of reduced alignment precision between the prompts and the image. We combine the best of both types of fusion, which retains the spatial alignment capabilities of dense fusion while improving the efficiency through sparse fusion. More importantly, all these methods lack relative 3D information, which makes it difficult to accurately distinguish objects in complex scenarios. To address this issue, we introduce order information and order-aware attention to incorporate 3D spatial context, enhancing the prompt's ability to separate targets more effectively. More related works can be found in Appendix A.3.\nDepth in segmentation tasks. Many image and video segmentation tasks have utilized depth as an additional modality to better handle foreground-background separation, such as instance segmentation (Wu et al., 2023), semantic segmentation (Zhang et al., 2023b; Yin et al., 2023), panoptic segmentation (Gao et al., 2022), video object segmentation (Liu et al., 2024b), video panoptic segmentation (Yuan et al., 2022). However, depth has hardly been explored for interactive segmentation. The only related work, MM-SAM (Wang et al., 2022), adopts an extra encoder to integrate depth. However, they achieve poor performance in common interactive segmentation benchmarks due to the insufficient fusion between prompt and depth. In this work, instead of directly inputting depth, we introduce order map that combines both depth and user prompt to further strengthen the model sense of relative depth between objects.\nObject understanding in segmentation tasks. Recent transformer-based approaches like DETR (Carion et al., 2020), MaskFormer (Cheng et al., 2021), Mask2Former (Cheng et al., 2022), ViSTR (Wang et al., 2021), HODOR (Athar et al., 2022), etc, introduce the concept of abstract object queries to represent objects in images and videos and seamlessly perform tasks like object detection and segmentation. Cutie (Cheng et al., 2024) further enhances the object representations for the video object segmentation task by having separate foreground and background regions that the object queries attend to, allowing the queries to distinguish between the foreground and background. However, the aforementioned methods were not directly applicable to the interactive segmentation task. To address this issue, SAM (Kirillov et al., 2023) introduces a sense of objects into the interactive segmentation task by encoding positive and negative prompts into sparse embeddings, all of which together represent the target object. However, SAM fails to clearly distinguish between the foreground and background because both the positive and the negative embeddings attend to the same regions, unlike Cutie. To address this limitation of SAM, we introduce a foreground-background separation into interactive segmentation, referred to as object-aware attention. It enables the sparse embeddings to only attend to their corresponding regions based on their type (positive click embeddings attend to the foreground and negative click embeddings attend to the background)."}, {"title": "3 METHODOLOGY", "content": "3.1 OVERVIEW\nThe framework of our proposed OIS is depicted in Figure 2. Given an image with a target object, the image features are first extracted using an image encoder. Following prior works (Huang et al., 2023; Kirillov et al., 2023), the image is encoded only once to save computational cost. The same extracted image features are reused for subsequent interactive stages when new user clicks are added. Each new user click is encoded in two ways, including dense and sparse embeddings (Sec. 3.4). Dense embeddings are added to the image features to enhance prompt alignment, while sparse embeddings interact with the image features through two types of attention mechanisms: a) our proposed order-aware attention (Sec. 3.2) and b) object-aware attention (Sec. 3.3). This enables the model to contain a sense of relative depth and the notion of objects. Finally, the enhanced features are passed through a decoder to predict the target segmentation mask.\n3.2 ORDER-LEVEL UNDERSTANDING\nOrder map. To make the model have a sense of relative depth, we first introduce the concept of order, which represents the relative depth between objects in a scene. We formulate this information as an order map, defined as the relative distance in camera optical axis direction. It is generated by combining the depth map $R \\in R^{H \\times W}$ of the image with each interactive prompt. Let $\\Omega^p$ indicate the set of positive prompts, and $\\Omega^n$ be the set of negative prompts. For the ith positive prompt $p \\in \\Omega^p$, the order map $M^p$ is defined as:\n$M^p = | R - \\frac{1}{M}\\sum_{p \\in \\Omega^p} R_p |$ (1)\nwhere $R_p$ is the depth value at the coordinate of the positive prompt, and $M$ is the number of positive prompts. Notice that since all the positive prompts refer to the same object, they share a common order map, which means order map $M^p$ is the same for all positive prompts $p \\in \\Omega^p$.\nOrder-aware attention. The order map, proposed above, guides the sparse embeddings to attend to the image features via our novel order-aware attention. In sparse embeddings $S \\in R^{N\\times C}$, the first half of N embeddings are generated from the positive prompts and the second half from the negative prompts. We construct order mask $M \\in (0,1)^{N \\times HW}$ as $[M^p, M^p, ..., M^p, M_1^n, M_2^n ...]$, where the first half contains order maps shared by all positive prompts, and the second half consists of individual order maps for each negative prompt. In order-aware attention, the goal is to make the prompt aware of the order within the image. To achieve this, we update the sparse embeddings by performing masked cross-attention with the image features, where the order mask guides the attention to relevant regions. The order-aware attention is defined as:\n$S' = softmax(QK^T - \\sigma M)V + S$, (3)\nwhere $Q$ is the linear transformation of sparse embedding $S$, and $K, V$ are the linear transformation of image features $F$. Inspired by Athar et al. (2022), $\\sigma M \\in (0,+\\infty)$ is added to control the attention with a learnable scale parameter $\\sigma$. Specifically, if a region is close to the prompt-selected order (i.e., its corresponding order mask value is near 0), it behaves like normal cross attention. However, if the region is far from the prompt-selected order (i.e., its order mask value approaches $+\\infty$ after scaling), the attention weight approaches 0 due to softmax, indicating minimal attention. In summary, regions close to the prompt-selected order receive higher attention, while far regions receive less.\n3.3 OBJECT-LEVEL UNDERSTANDING\nAlthough order-level understanding can guide the prompt in distinguishing objects from different orders, it faces the limitation of differentiating between objects within the same order. To solve this problem, we introduce object-aware attention to impart a sense of objects into our model.\nObject-aware attention. After going through order-aware attention, the sparse embeddings (Sec. 3.2) attend to the image features using a foreground-background separated masked cross-attention following Cutie (Cheng et al., 2024). We call this our object-aware attention module. Specifically, in this module, the sparse embeddings generated from the positive clicks attend only to the foreground regions, while the sparse embeddings generated from the negative clicks focus only on the background regions. The foreground and background regions are obtained from the prediction generated using the previous clicks. This explicit foreground-background separation introduces a strong notion of the target object into the sparse embeddings. Different from Cutie (Cheng et al., 2024), where multiple object queries are randomly initialized, we encode the foreground and background clicks to generate sparse embeddings, which act as attributes of one single target object.\n3.4 PROMPT INTEGRATION VIA DENSE AND SPARSE FUSION\nUnlike previous methods that encode prompts through either dense or sparse fusion, we leverage the strengths of both approaches while addressing their limitations. Dense fusion can preserve more spatial details from the prompt (Liu et al., 2024a). By adding the dense embeddings $D \\in R^{H\\times W \\times C}$ to the image features $F \\in R^{H\\times W \\times C'}$, the prompts achieve better spatial alignment with the image. Typically, this process is followed by a self-attention module (Liu et al., 2024a) to capture spatial relationships. However, direct attention between spatial features is computationally expensive. To address this issue, we remove the self-attention module and introduce the sparse embeddings $S \\in R^{N \\times C}$, where N is the number of prompts. Sparse fusion is more computationally efficient (Cheng et al., 2024), as the attention is applied between sparse embeddings and spatial features. Moreover, sparse fusion offers greater flexibility for performing attention operations. Leveraging this flexibility, we have developed order-aware and object-aware attention mechanisms that enable the model to focus on specific regions based on the sequence of user interactions or the contextual understanding of objects within the image. Thus, our approach improves the alignment between the image and prompt through dense embeddings while maintaining computational efficiency through sparse fusion and simultaneously leverages its flexibility to make the prompt aware of order and object understanding."}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENT SETTINGS\nDatasets and evaluation metrics. We evaluate our method and comparison methods on two widely used benchmarks for interactive segmentation: HQSeg44K (Ke et al., 2024) and DAVIS (Perazzi et al., 2016). More details about these two datasets can be found in Appendix A.2. We report five evaluation metrics: number of clicks (NoC), mean interaction-over-union (mIoU), number of failures (NoF), seconds per click (SPC), and SAT latency. The NoC metrics, including NoC90 and NoC95, represent the number of clicks required to reach mIoU thresholds of 90% and 95%, respectively. The mIoU metrics, such as 1-mIoU and 5-mIoU, denote the average IoU achieved after 1 or 5 consecutive clicks. NoF measures the number of cases that require more than 20 clicks to reach 90% mIoU. SPC evaluates time efficiency by calculating the average time taken per click, while SAT latency refers to the total latency for the Segment Anything Task (with a grid of 16x16 points). For comparison methods, we utilize their released model weights to compute the 1-mIoU and NoF95 scores. For all other metrics, we directly reference the results as reported in their papers.\nImplementation details. In our framework, we use a frozen ViT-Base encoder from DepthAnythingV2 (Ranftl et al., 2021) to encode each image. Note that we re-use the same encoder for depth map generation, allowing us to keep our overall model lightweight (as shown in Table 3). The encoder is kept frozen during training to retain the depth features. We have 2 decoders initialized from DepthAnything V2 (Ranftl et al., 2021): one is kept frozen to generate the depth map and the other one is trained to generate the final segmentation. We formulate the dense embeddings (introduced in Sec. 3.4) by constructing a two-channel dense map (one for positive clicks and the other for negative clicks). Then, we encode the dense map via a single convolution layer. For sparse embeddings, we represent each click as a C-dimensional embedding (C = 128) following SAM (Kirillov et al., 2023). Each of these embeddings is the sum of the positional encoding of its coordinate and a learnable embedding based on its prompt type (positive or negative). We set the maximum number of clicks N as 48 during training, with the first 24 for positive clicks and the remaining for negative. If there are fewer than 48 clicks, we fill the remaining slots with an additional 'non-point' embedding. We use three same blocks of object-level and order-level understanding modules (Sec. 3.2 and Sec. 3.3) to conduct prompt fusion. In these modules, the Feed-Forward Network (FFN) is implemented as a 2-layer Multi-Layer Perceptron (MLP). Each attention module is followed by a LayerNorm, which normalizes the sparse embeddings."}, {"title": "4.2 EVALUATION ON HQSEG44K", "content": "We first evaluate on HQSeg44K dataset (Ke et al., 2024). As shown in Table 1, our method significantly outperforms other methods across all the metrics. In particular, our NoC90 score shows an improvement of more than 1 click, while our NoC95 score improves by approximately 2 clicks. Furthermore, our 1-mIoU achieves a remarkable improvement of around 8 points compared to SegNext. This indicates that with only one click, our model can already provide an accurate and good-quality segmentation mask with mIoU of 89.40% compared to 81.79% of SegNext."}, {"title": "4.3 EVALUATION ON DAVIS", "content": "Table 2 provides the comparison of different methods on the DAVIS (Perazzi et al., 2016) dataset. Our method achieves the state-of-the-art performance in both NoC and mIoU metrics. Specifically, our model requires only 8.59 clicks to reach 95% mIoU, compared to 10.73 clicks for SegNext, which is over 2 clicks improvement. This means that even in complex scenarios, our method can predict high-quality segmentation masks and generalize well. Figure 5 further illustrates this point."}, {"title": "4.4 EFFICIENCY ANALYSIS", "content": "Time efficiency is critical in interactive segmentation models, as users expect minimal latency to ensure smooth and responsive interactions. In Table 3, we compare our methods with leading approaches in terms of efficiency (SPC and SAT Latency) and accuracy (NoC90 and 5-mIoU). The SPC (ms) reflects the average speed per click, while SAT Latency (s) captures the overall time, accounting for both image processing and click processing.\nSimpleClick is the slowest in terms of SPC, primarily because it re-encodes the image after each new user click. This introduces significant time costs that increase its overall SAT latency. In contrast, our method encodes the image only once, regardless of the number of subsequent clicks, similar to HQ-SAM and SegNext. As a result, we exhibit lower SPC and SAT Latency.\nAmong these, although HQ-SAM is slightly faster, its accuracy is significantly lower than our model. SegNext, while being slightly less accurate than ours, performs 2 times slower than our method in terms of SPC and SAT latency. This discrepancy arises because our method computes cross attention between sparse queries and spatial image feature, replacing the heavy self-attention between high resolution spatial features used in SegNext. In summary, our method achieves the optimal balance between efficiency and accuracy, offering low latency while achieving superior segmentation quality."}, {"title": "4.5 ABLATIONS", "content": "To better understand how each proposed module performs, we conduct the ablation experiments here. Table 4 demonstrates the importance of each module in the proposed method by analyzing performance across three different kinds of metrics (NoC90, 5-mIoU, and NoF95) on DAVIS dataset.\nImportance of order-aware attention. The 2nd row in Table 4 shows the effect of removing the order-aware attention module. We observe a significant increase in NoC90 by 1.04 and a decrease in 5-mIoU by 1.15. This indicates the model requires more user interaction without order information. This validates our claim that order-aware attention helps distinguish between objects at different depth levels. This is also highlighted in Figure 6 with two examples where similar objects are overlapping. We visualize the attention weights before and after integrating order-aware attention. We observe that before integrating order-aware attention, the attention weights are distributed across both objects. After incorporating the order information, the attention weights focus solely on the target object. More visualizations of attention weight are in Appendix A.5.\nImportance of object-aware attention. The 3rd row in Table 4 shows the effect of removing the object-aware attention. The result shows a performance decrease, indicating that object-aware attention is indispensable.\nImportance of sparse embeddings. The 4th row of Table 4 shows the performance after completely removing the sparse embedding discussed in Sec. 3.4. This configuration automatically removes both the order-aware and object-aware attention. The prompt integration is solely via the addition of dense embeddings to the image embeddings (Sec. 3.4). We observe a significant performance drop, demonstrating the critical role of the sparse fusion module in effective prompt integration.\nImportance of dense embeddings. The 5th row of Table 4 shows the effect of removing the dense embeddings (Sec. 3.4). The prompt is only represented by the sparse embeddings in this configuration. This results in a large performance decline, as dense fusion improves spatial alignment between the image and prompt, making prompt interactions more effective."}, {"title": "5 CONCLUSION", "content": "In this work, we have introduced Order-Aware Interactive Segmentation (OIS), which incorporates 3D spatial context through the concept of order into 2D interactive segmentation. By leveraging proposed order-aware attention, the model can better distinguish objects based on their relative depths. We introduced object-aware attention to enhance our model's ability to differentiate objects within the same depth level. We integrated user clicks using both sparse and dense representations, improving segmentation accuracy and computational efficiency. Experimental results validated that OIS significantly improves segmentation accuracy and speed as compared to prior methods."}, {"title": "A APPENDIX", "content": "A.1 CLICK SAMPLING STRATEGY.\nTo have fair comparison, we apply the same click sampling strategy from prior works (Sofiiuk et al., 2022; Liu et al., 2023; Huang et al., 2023; Liu et al., 2024a) for evaluation in all the experiments. This strategy generates clicks sequentially, with each new click placed at the center of the largest error region in the model's prediction.\nA.2 MORE DATASETS DETAILS\nWe evaluate our method and comparison methods on two widely used benchmarks for interactive segmentation: HQSeg44K (Ke et al., 2024) and DAVIS (Perazzi et al., 2016). HQSeg44K is a large-scale segmentation dataset containing 44320 images with high-quality mask labels. It contains a diverse range of images spanning over 1,000 semantic classes, covering both simple and complex scenarios, and includes objects with thin shapes as well as more straightforward forms. DAVIS is a high-precision video object segmentation dataset consisting of 50 videos. It contains more complicated scenarios such as occlusion, multi-objects, motion blurs and many other challenges. To be consistent with previous works, we use a subset of 345 frames to conduct the evaluation.\nA.3 ADDITIONAL RELATED WORK ON INTERACTIVE SEGMENTATION\nEarly interactive segmentation methods (Boykov & Jolly, 2001; Grady, 2006; Rother et al., 2004; Gulshan et al., 2010) relied on optimization techniques to solve cost functions defined by image pixels. As deep learning became more popular, approaches began incorporating user interactions directly into neural networks (Xu et al., 2016; Sofiiuk et al., 2020; Maninis et al., 2018; Jang & Kim, 2019; Lin et al., 2020). Further advancements such as RITM (Sofiiuk et al., 2022; Liu et al., 2022) leveraged large-scale data for robustness, FocalClick (Chen et al., 2022) introduced local refinement modules, and SimpleClick (Liu et al., 2023) improved performance using vision transformers. To enhance efficiency, methods like InterFormer (Huang et al., 2023) reduced interaction time by encoding the image only once during interactions, and SegNext (Liu et al., 2024a) applied similar idea with attention mechanism to achieve high quality with low latency. Following the introduction of SAM (Kirillov et al., 2023), numerous SAM-based methods have been proposed to improve interactive segmentation in aspects such as quality (Ke et al., 2024), interaction efficiency (Zhang et al., 2023a), and high-resolution image handling (Huang et al., 2024). Variants of interactive segmentation, like multi-object segmentation (Yue et al., 2023; Rana et al., 2023), medical image segmentation (Ma et al., 2024; Wong et al., 2023), and segmentation with controllable granularity (Zhao et al., 2024), have also been proposed.\nA.4 COMPARISON WITH OTHER 3D INFORMATION INTEGRATION METHOD\nIn this section, we compare our methods to other 3D information integration methods to show that naively incorporating depth is not as effective as our order-aware attention module discussed in Sec. 3.2 and validated in Sec. 4. Following the strategy of MM-SAM (Xiao et al., 2024), which adds an extra encoder for other modalities such as lidar, depth, and thermal, we use the depth map generated by DepthAnything-V2 (Yang et al., 2024) into their additional encoder and test this configuration on the DAVIS dataset. Note that in this configuration, we disable the order-aware attention discussed in 3.2. We call this configuration DepthEncoder in Table 5, which shows the result of this setting. Our method outperforms this configuration and MM-SAM across all metrics. This indicates that formulating the 3D spatial context information into order using our specialized order-aware attention module is a better solution than directly integrating depth maps."}, {"title": "A.5 ADDITIONAL VISUALIZATION OF ATTENTION WEIGHTS IN ORDER-AWARE ATTENTION", "content": "In this section, we provide more visualizations of the attention weights to show the importance of proposed order-aware attention, as shown in Figure 7 and discussed in 4.5."}, {"title": "A.6 QUALITATIVE RESULTS FOR MORE CHALLENGING CASES", "content": "We provide more qualitative results of challenging cases from the HQSeg44K (Ke et al., 2024) dataset as displayed in Figure 8 and Figure 9."}, {"title": "A.7 ANALYSIS OF MULTI-ROUND INTERACTIONS", "content": "Interactive segmentation is a multi-round task. The goal is to minimize the user interactions (i.e., interaction round number) and achieve a good quality segmentation mask. Hence, we evaluate the multi-round interaction performance of our method. Figure 10 illustrates that while both our method and SegNext produce some false positives in the background after the first click, our method eliminates the entire background, including regions inside the bike and adjacent to the human head, with only a single negative click. In contrast, SegNext requires 10 clicks to remove these background false positives and still can not get satisfied segmentation for the entire target. We also provide more multi-round interaction results in Figure 12. For the challenging cases, the multi-round interaction results are displayed in Figure 11 and Figure 13."}, {"title": "A.8 FAILURE CASES", "content": "In this section, we discuss some failure cases of our method. As shown in Figure 14, our method sometimes struggles to accurately predict the segmentation masks if thin objects occlude our target. This can be seen in the first example, where thin branches and leaves occlude the target bus and in the second and third examples, where the grass occludes the target human."}]}