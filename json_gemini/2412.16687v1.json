{"title": "Subgoal Discovery\nUsing a Free Energy Paradigm and State Aggregations", "authors": ["Amirhossein Mesbah", "Reshad Hosseini", "Seyed Pooya Shariatpanahi", "Majid Nili Ahmadabadi"], "abstract": "Reinforcement learning (RL) plays a major role in solving\ncomplex sequential decision-making tasks. Hierarchical and\ngoal-conditioned RL are promising methods for dealing with\ntwo major problems in RL, namely sample inefficiency and\ndifficulties in reward shaping. These methods tackle the men-\ntioned problems by decomposing a task into simpler subtasks\nand temporally abstracting a task in the action space. One of\nthe key components for task decomposition of these meth-\nods is subgoal discovery. We can use the subgoal states to\ndefine hierarchies of actions and also use them in decompos-\ning complex tasks. Under the assumption that subgoal states\nare more unpredictable, we propose a free energy paradigm\nto discover them. This is achieved by using free energy to\nselect between two spaces, the main space and an aggrega-\ntion space. The model changes from neighboring states to\na given state shows the unpredictability of a given state, and\ntherefore it is used in this paper for subgoal discovery. Our\nempirical results on navigation tasks like grid-world environ-\nments show that our proposed method can be applied for sub-\ngoal discovery without prior knowledge of the task. Our pro-\nposed method is also robust to the stochasticity of environ-\nments.", "sections": [{"title": "Introduction", "content": "Reinforcement learning (RL) (Sutton and Barto 2018) is\nwidely used in different aspects of our daily life from chat-\nbots (Christiano et al. 2017) to autonomous driving (Ki-\nran et al. 2021) and chip design (Mirhoseini et al. 2021).\nClassical RL algorithms generally suffer from being time-\nconsuming, sample inefficient, and having difficulties in\ndefining an appropriate reward function. Furthermore, the\nclassical RL algorithms have difficulties in environments\nwith long horizons, delayed rewards, and sparse rewards.\nSuch environments are common in navigation, robotic ma-\nnipulation, and many other tasks that we are dealing with\ndaily.\nStudies like Hierarchical Reinforcement Learning\n(HRL) (Hutsebaut-Buysse, Mets, and Latr\u00e9 2022), Goal-\nConditioned Reinforcement Learning (GCRL) (Liu, Zhu,\nand Zhang 2022), and using sub-spaces (Ghorbani et al.\n2025) are some of the promising efforts that try to solve the\naforementioned problems of classical RL algorithms. These\nworks are trying to use a level of abstraction in the action\nspace and the state space or they try to decompose tasks into\nsimpler tasks and use the agent's experience to generalize\nsolving long-horizon tasks.\nWe can see different examples of action abstraction in our\ndaily lives. For example, instead of thinking about the per-\nformance of thousands of pieces of a car, we abstract the\nsequence of actions taking place into a high-level action like\n\"speed up\". Even more complex tasks like \"turning right\"\ncan be decomposed into the sequence of lower-level actions\n\"slowing down the speed\u201d, \u201cchanging the lane\" and"}, {"title": "Related Work", "content": "The studies on bottleneck or subgoal discovery can be cat-\negorized into two groups, GCRL and option discovery in\nHRL. In GCRL methods, a high-level controller tries to\nidentify subgoals. The detected subgoals are used as the tar-\nget of low-level policies (Chane-Sane, Schmid, and Laptev\n2021; Nachum et al. 2018; Nair and Finn 2020) for learn-\ning decomposed tasks. These methods are implemented on\nthe goal-conditioned Markov Decision Processes (MDPs)\nwhere we have the information about the goal state and our"}, {"title": "Background", "content": "Reinforcement Learning\nRL is a framework for solving sequential decision-making\ntasks in a trial-error manner. Environments in this frame-\nwork are modeled with a MDP defined by a tuple <\nS, A, R, P, \u03b3 > where S is the state space, A is the action\nspace, and R is the expected reward. P specifies the dynam-\nics of the environment and y is the discount factor. Assume\nin a MDP at time step t, the agent in state st \u2208 S commits\naction at \u2208 A on the environment, the next state of the agent\nin the environment st+1 \u2208 S is determined by the transition\nprobability $P_a(s, s') = Pr(s_{t+1} = s'|s_t = s,a_t = a)$,\nand the agent receives the instant reward rt+1, whose its\nexpectation is equal to $R_a(s, s')$. In most of RL problems,\nthe agent does not know the model of the environment, and\nit needs to interact with the environment to learn a policy\n\u03c0: S \u00d7 A \u2192 [0, 1] during its lifetime in the environment\nand it is trying to find an estimate to an optimal policy \u03c0*.\nAn optimal policy in each state is a policy that maximizes\nthe cumulative expected reward given by\n$G_t = \\sum_{k=t}^{\\infty} \\gamma^{t+k+1}.$\nIn this work, we consider model-free algorithms where\nthe agent does not directly learn the model of the environ-\nment, and instead, it calculates an estimate to the action-state"}, {"title": "Free Energy", "content": "The free energy paradigm is related to the laws of thermo-\ndynamics that explain why energy flows in certain direc-\ntions. We can use this paradigm for describing a system or\nmaking predictions. The free energy concept that we use in\ndecision-making and neuroscience was introduced by (Fris-\nton, Kilner, and Harrison 2006) and then developed by (Or-\ntega et al. 2015) for rationally bounded decision-making.\nFrom Friston, Kilner, and Harrison (2006) perspective, the\nhuman brain is trying to minimize a variable called free en-\nergy which is the same as minimizing a surprise function\nor maximizing evidence of the model of the environment.\nTo achieve this goal, we need to have a good model of the\nenvironment.\nIf we model the observations of the environment using a\nrandom variable O, and the hidden state of the environment\nwith the random variable S (we do not have access to the\nreal state of the environment), our brain constructs a gener-\native model defined as P(O, S). We can calculate surprise\nwith - log(P(O)). As the probability of our observation O\nbecomes high (near 1), the surprise of this observation be-\ncomes less. Considering a dummy distribution Q(s) we can\nhave:\n$- log P(O) = log \\sum_{s \\in S}P(O, s) = -log \\sum_{s \\in S} Q(s) \\frac{P(O,s)}{Q(s)}$\nThe considered surprise function is convex, so we can use\nJensens's inequality:\nf(wx + (1 \u2212 w)y) < wf(x) + (1 \u2212 w)f(y),\nto calculate an approximate upper bound for the surprise\nfunction. So we have:\n$- log\\sum_{s \\in S}Q(s)\\frac{P(O, s)}{Q(s)} <= -\\sum_{s \\in S}Q(s) log \\frac{P(O, s)}{Q(s)} = -\\sum_{s \\in S}Q(s)log\\frac{Q(s)}{P(O,s)}$"}, {"title": "Aggregating different perspectives in RL using\nFree Energy", "content": "Aggregating different states, i.e. subspaces, has shown a no-\ntable enhancement in the performance of agents. In addi-\ntion to speeding up learning, especially at the initial steps,\nusing subspaces can lead to enhancing sample efficiency as\nwell as lowering regret (Ghorbani et al. 2025; Hashemzadeh,\nHosseini, and Ahmadabadi 2018, 2020). However, defining\nsubspaces opens up different research questions like \"which\nstates to aggregate as a subspace\", \"How to cope with PA\nproblem in subspaces\" and \"which subspaces to choose for\nlearning at each step\".\nIn the most recent work, Ghorbani et al. (2025) intro-\nduced a free energy based approach for choosing the best\ndefined subspace for learning at each step. They have sup-\nposed a utility function based on the Thompson sampling\npolicy (Russo et al. 2018), as the negative informational sur-\nprise:\n$U(a, s, m) = log \\pi_{TS}(a | s, m)$.\nThis utility gives information about the optimality of action\na at state s and subspace m.\nTo utilize subspaces for learning and avoiding the PA\nproblem, the following constraint is used to ensure the util-\nity of the main space is close to the utility of subspaces for\nany target policy \u03c0:\n$E_{\\pi(a|s,m)} [U(a, s,m)] \u2013 E_{\\pi(a|s,m)} [U(a, s,m_{Main})] < K_1,$\nwhere m is any considered subspace and $m_{Main}$ is the main\nspace. Also, to lower the effect of inaccurate uncertainty es-\ntimation, another constraint is used to limit the policy by an\narbitrary behavioral policy $\u03c0_B$:\n$D_{KL}(\\pi(a|s,m)||\\pi_B(a|s,m)) < K_2$.\nConsidering the defined utility function and mentioned con-\nstraints, the problem of learning the optimal policy utilizing\ndifferent subspaces changes into the following optimization\nproblem which is a free energy minimization, similar to (1):\n$\\pi^*(als, m) = arg min F(s,m, \\pi(a|s,m)),$\n$\\pi(a,s,m)$\nwhere the free energy for any target policy \u03c0(a|s, m) and for"}, {"title": "Subgoal discovery using State Aggregations\nand Free Energy paradigm", "content": "In this paper, we consider the concept of bottleneck to define\noptions in the environment. States like doorways in multi-\nroom environments can be seen as subgoal states, so identi-\nfying such states can help us to decompose the task of navi-\ngation from one room to the other or it can be beneficial for\nautonomous option discovery in the options framework.\nTo detect such states, we assume our agent lives in an en-\nvironment with a defined state space and we call this space\nMain Space:\n$M_{Main}:\\phi_{Main}(s) = s, s \\in S$,\nwhere $\\phi_{Main}$ is an identity function. We update the Q-values\nof the main space with the update rule of our learning algo-\nrithm, SARSA:\n$Q_{Main}(s, a) = Q_{Main}(s,a)$\n$+ \\alpha(r + \\gamma Q_{Main}(s', a') \u2013 Q_{Main}(s, a))$,\nwhere X is the learning rate, and r is the instant reward that\nthe agent gets from the environment. Also, (s', a') indicates\nthe next state-action pair.\nIn addition to the main space, we assume our agent has\naccess to its physical neighbor states and their action-state\nvalues. So we can define an Aggregation Space ($m_{Agg}$):\n$M_{Agg}: \\phi_{Agg}(s) = {s'|s' \\in S & d(s, s') < L}, s \\in S$,\nwhere d is a distance metric like Euclidean or Manhattan dis-\ntance. Also, L is the maximum distance of the neighborhood\nof the current state. We don't have any learning in the aggre-\ngation space and the Q-values of this space are calculated\nusing a weighted average on aggregated states' Q-value in\nthe main space:\n$Q_{Agg}(s, a) = \\frac{1}{\\sum_{s' \\in \\phi_{Agg}(s)}n_{Main}(s', a)} \\times \\sum_{s' \\in \\phi_{Agg}(s)} n_{Main}(s', a) \\times Q_{Main}(s', a).$"}, {"title": "", "content": "In this equation, $n_{Main}(s, a)$ gives the frequency of samples\nof action a in state s in the main space ($m_{Main}$).\nFollowing (Ghorbani et al. 2025), the best space between\nthe main and the aggregation space is the one that minimizes\nthe free energy given by (5), mathematically speaking:\n$m^*(s) = arg min F(s,m, \\pi^*(a|s,m)),$\nm\nwhere \u03c0* can be calculated by (7) for each space.\nThompson sampling policy is equal to\n$\\pi_{TS}(a_i | s,m) = P({Q_m(s, a_i) > Q_m(s,a_j)}),$\nj\u2260i\nwhere $Q_m(s, a)$ is the belief distribution for the value of\nthe action a in state s in space m. Calculating the exact be-\nlief distribution for each state-action value in each space is\ncomputationally complex and hard to achieve. Thus, we can\napply approximation methods by calculating an upper and\nlower bound for these distributions (Audibert, Munos, and\nSzepesv\u00e1ri 2009), by considering a confidence interval of\n1 - \u03bd:\n$P(Q_m(s, a) \u2013 \\mu < Q_m(s,a) < Q_m(s,a) + \\mu) \\geq 1 \u2212 \\nu,$\nwhere u is given by\n$\\mu = std(s,a,m) \\sqrt{\\frac{\\nu}{3} log(\\frac{3}{\\nu})} \\sqrt{\\frac{2}{n_m(s,a) +1}},$\nand std is computed by\n$std(s, a, m) = \\sqrt{\\frac{\\sum_{t}^{n_m(s, a)} Q_{t,m}(s, a)^2 \u2013 (\\sum_{t}^{n_m(s, a)} Q_{t,m}(s, a))^2}{n_m(s, a) \\times (n_m(s, a) \u2013 1)}}$\nIn this equation, Qt,m is the value of action a in state s and\nspace m at the step t of the agent's lifetime.\nWe now have the ingredients to implement our idea of bot-\ntleneck discovery as explained in the introduction (see Figs\n1,2). We defined model changes as a measure of the irregu-\nlarity of a state. If by entering a state from its neighboring\nstates, there is a change between the best spaces, we count\nup the value of the model change of that particular state, that\nis\n$MC(s) = \\begin{cases}\nMC(s) +1 & m^*(s_{t-1}) \u2260 m^*(s_t),\nMC(s) & otherwise.\n\\end{cases}$\nIn (16), $m^*(s_t)$ is the free energy model of the environment\nfor the current state of the agent and $m^*(s_{t\u22121})$ is the free\nenergy model of the previous state.\nFigure 5 shows the flow of our proposed method. In each\nstep of the episode, we first estimate the Thompson sampling\npolicy by approximating the intervals of state-action values\nfor each space, which is computed by (12). After this step,\nwe calculate the free energy of each space to decide which\nspace has minimum free energy, computed by (5). If the free\nenergy model of the current state is different than the pre-\nvious state, we will count for model changes in the current\nstate, as expressed by (16). For the aim of bottleneck detec-\ntion, we apply Otsu's thresholding (Otsu 1979) to determine"}, {"title": "Algorithm 1: Our Proposed Algorithm", "content": "for Each Episode do\nwhile done \u2260 True do\nnext state, reward, done = environment.step(action)\ncalculate free energy model of state, using (5)\nif m*(state) \u2260 m*(previous state) then\nCount for model change in state, using (16)\nend if\nend while\nApply Otsu's thresholding on model changes matri-\nces\nNon maximum suppression on model changes \u00d7\nOtsu's thresholding output\nend for\nreturn bottleneck states"}, {"title": "Algorithm 2: Model Changes in Continuous State Space", "content": "for Each Episode do\nwhile done \u2260 True do\nnext state, reward, done = environment.step(action)\nSample from neighbor states with a distance of L\nCalculate Q-values for aggregation space using re-\nplay buffer\nCalculate Thompson sampling by considering the\nfrequency of actions in the replay buffer\nCalculate free energy model of state, using (5)\nif m*(state) \u2260 m*(previous state) then\nCount for model change in state, using (16)\nend if\nend while\nend for\nreturn Model changes for each state"}, {"title": "Experimental Results", "content": "To test the performance of our algorithm, we designed two\nsets of environments with discrete and continuous state\nspaces. Figure 6 and Figure 7 indicate our gird-world envi-\nronments with discrete and continuous state space, respec-\ntively. The coordinates of the agent in each step are consid-\nered as states. The agent can do four actions navigating up,\ndown, left, and right in both kinds of environments. If the\nagent's action leads to collision with walls, the agent will re-\nmain in the same state. Each action can fail with a probabil-\nity of p, in this case the agent goes to any of the neighboring\nstates with the equal probability. We use the Euclidean dis-\ntance with a maximum distance of L = 2 for the aggregation\nspace in all of the environments.\nThe agent receives a reward of -1 for taking each step in\nthe environment. If the agent reaches the goal state it will\nget +10 as a reward and if it takes an action that leads to\ncollision with walls it will get a reward of -10. In all of the\nenvironments, the agent can start randomly from a state at\nthe corners of the environment, and the episode terminates if\nthe agent reaches the goal state or if it reaches the maximum\nsteps, defined for each environment.\nAs shown in Figure 6, we consider 6 environments with\ndiscrete states, where the environment with a transfer state\nhas two versions. In the first version, we consider an addi-\ntional transfer action which will be fired just in the state (4,\n4) and it moves the agent to the state (8, 8). This action,\nsimilar to other actions, has a probability of p of failing, in\nwhich case the agent transitions to a random neighboring\nstate. In the other version, the state (4, 4) acts like a teleport\nthat transfers the agent to the state (8, 8). The maximum\nstep for 2-rooms, 3-rooms, and rooms with the transfer is\n100 steps, and because of the complexity of tasks in 4-rooms"}, {"title": "", "content": "and 9-rooms environments, the maximum step is considered\n500 in these environments. Also, the 1-room with a hall-\nway environment is a tricky environment, so the number of\nmaximum steps is 150 in this environment. All of the envi-\nronments contain bottleneck states, because of their design\nwhich is room-to-room navigation task they have. It is clear\nthat bottleneck states are the doorway states (in the multi-\nroom environment) and the neighbor states of the transfer\nstate in the environments with the transfer state.\nIn the environment with the continuous state space, each\naction results in a displacement of 0.1 units in the corre-\nsponding coordinates of the agent's current state. For in-\nstance, the right action would move the agent from (0, 0) to\n(0, 0.1). After a maximum of 300 steps, if the agent could not\nreach the goal state, the episode is terminated. The reward\nfunction is the same as that of the discrete environments and\nthere is no transfer state."}, {"title": "Results in Discrete State Spaces", "content": "In our implementations, we consider the agent learns and\ninteracts with the environment using the SARSA algorithm\nthat has an epsilon greedy policy as its behavioral policy.\nThe discount factor is chosen to be equal to 0.9, and the\nlearning rate is equal to 0.99 with a decaying rate of 0.001.\nAlso, the epsilon is 0.3 and it decays exponentially with a\nrate of 0.3. For parameters \u03b1 and \u03b2 in equations (6), (7),\nand (8), we choose a = 4 and \u03b2 = 7, that are the same\nparameters used in the implementation of (Ghorbani et al.\n2025). Similar to (Ghorbani et al. 2025), we also observed\nthat our method is not sensitive to the choice of these two\nparameters. All these parameters are the same for the results\nin all discrete-state environments and all results are an aver-\nage of 10 runs. In addition, the default probability of failing\nan action p is set equal to 33% for all environments. We ini-\ntialized the agent at the top-left corner of the environment\nin all experiments. However, our method's ability to identify\nbottleneck states is not dependent on the initial state, as long\nas the starting and goal states are in different rooms."}, {"title": "", "content": "the first steps. However, our agent can successfully detect\nthe doorway bottleneck after some episodes.\nFigure 4 shows the identified subgoal/bottleneck states in\ndifferent grid-world environments after 50 episodes. We can\nsee that our agent can detect states like doorways and states\naround the transfer states. Also in the tricky 9-rooms envi-\nronment, the agent was able to find some of the doorways\nthat are needed to reach the final goal. In the 1-room with a\nhallway environment, the identified bottleneck states appear\nreasonable from a task decomposition perspective. They can\nbe used to define tasks like \"entering the hallway\" or \"leav-\ning the hallway\". However, careful tuning of the algorithm's\nparameters for this environment can lead to the discovery of\nmore specific subgoals that contribute to reaching the final\ngoal.\nBecause methods based on environment graphs struggle\nin highly stochastic environments, we tested our algorithms'\nability to handle high levels of stochasticity. As shown in\nFigure 8, our algorithm successfully identified bottleneck\nstates in environments with a high probability of action fail-\nure p = 50%."}, {"title": "Comparison with Experience-Based methods", "content": "Since there is no quantitative criterion to compare the per-\nformance of bottleneck discovery algorithms, we show the\nbottlenecks discovered by our method and an experienced-\nbased method. Similar to our method, we apply non-\nmaximum suppression on the resulting state visit counts of\nthe experience-based method. Figure 9 shows the results for\ndifferent environments with the action failure probability of\n33%. Because of the high level of stochasticity in these envi-\nronments, relying on the trajectory information of the agent\ncan be misleading when determining subgoal states. There-\nfore in such settings, the performance of experience-based"}, {"title": "", "content": "methods substantially deteriorates. In contrast, our proposed\nmethod succeeds in identifying the correct subgoal states.\nThis is because our method considers both the behavioral\npolicy and the uncertainty in subspaces which is relatively\nrobust to the stochasticity of the environment."}, {"title": "Model Changes in Continuous State Spaces", "content": "For the environment with continuous state space (Figure 7),\nwe used DQN (Mnih et al. 2015) with a fully connected\narchitecture to learn the task. The replay buffer has the ca-\npacity of 10000 samples. The target network's weights get\nupdated every 5 episodes. The behavioral policy of the agent\nis epsilon-greedy and the parameters a and \u03b2 are the same\nas our experiments in the discrete environments.\nFigure 10 shows the result of Algorithm 2 for comput-\ning model changes in this environment. The results demon-\nstrate that the phenomenon of model changes is not limited\nto the tabular settings and we have model changes when us-\ning function approximations, such as deep neural networks,\nfor learning."}, {"title": "Conclusion", "content": "In this paper, we study the problem of subgoal discovery\nin different grid-room environments. We showed that our\nmethod can detect bottleneck states in different types of\ndoorways and transfer states and it is robust to the noise\nand stochasticity of the environment. Our method does not\nneed to save full information of trajectories in memory or\nto generate a graph from interactions of the agent in the en-\nvironment, which can be misleading when the noise of the\nenvironment is considerably high. Our proposed method de-\ntects the bottleneck states in the environment without any\nsupervision or predefined number.\nThere are several directions to expand this work in the\nfuture. One avenue is using model changes for bottleneck\ndetection in environments with continuous states or actions,\nand environments with sparse rewards. Another direction of\nfuture work is searching for efficient ways to lower the time\ncomplexity of our algorithm. Making use of the discovered\nbottlenecks to learn reasonable and interpretable options in\nHRL or GCRL can be an interesting extension of the pro-\nposed method."}]}