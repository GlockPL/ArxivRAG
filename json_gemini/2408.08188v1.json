{"title": "Scaling Up Natural Language Understanding for Multi-Robots Through the Lens of Hierarchy", "authors": ["Shaojun Xu", "Xusheng Luo", "Yutong Huang", "Letian Leng", "Ruixuan Liu", "Changliu Liu"], "abstract": "Long-horizon planning is hindered by challenges such as uncertainty accumulation, computational complexity, delayed rewards and incomplete information. This work proposes an approach to exploit the task hierarchy from human instructions to facilitate multi-robot planning. Using Large Language Models (LLMs), we propose a two-step approach to translate multi-sentence instructions into a structured language, Hierarchical Linear Temporal Logic (LTL), which serves as a formal representation for planning. Initially, LLMs transform the instructions into a hierarchical representation defined as Hierarchical Task Tree, capturing the logical and temporal relations among tasks. Following this, a domain-specific fine-tuning of LLM translates sub-tasks of each task into flat LTL formulas, aggregating them to form hierarchical LTL specifications. These specifications are then leveraged for planning using off-the-shelf planners. Our framework not only bridges the gap between instructions and algorithmic planning but also showcases the potential of LLMs in harnessing hierarchical reasoning to automate multi-robot task planning. Through evaluations in both simulation and real-world experiments involving human participants, we demonstrate that our method can handle more complex instructions compared to existing methods. The results indicate that our approach achieves higher success rates and lower costs in multi-robot task allocation and plan generation.", "sections": [{"title": "1 Introduction", "content": "The challenge of long-horizon planning arises from factors such as uncertainty accumulation, computational complexity, delayed rewards and incomplete information. A strategy is by leveraging the task hierarchy. Hierarchical models have demonstrated a notable edge over flat models in interpretability and efficiency [1, 2]. However attractive, how to obtain hierarchy still remains an open problem. One pathway is to deduce hierarchy through the observation of task execution [3], which alleviates human effort yet poses a challenge due to a high requirement for abstraction reasoning. Conversely, the other route, which entails acquiring hierarchy directly from human, appears straightforward. Humans excel at hierarchical reasoning and are used to articulate hierarchically through language effortlessly [4]. Nonetheless, the hierarchical insights from humans cannot be readily integrated by algorithms without meticulous engineering. This gap between human preferences and algorithmic formulation impedes the application of hierarchy-based planning algorithms.\nLLMs, being trained on extensive textual corpus, exhibit common sense reasoning abilities, thereby efficiently managing everyday task specifications articulated in human languages. Our key observation is that hierarchy can be progressively obtained from human input with the help of a LLM."}, {"title": "2 Related Work", "content": "Language-Conditioned Robotic Planning Given instructions, there are two primary methods for generating actions [7]. The first uses deep-learning techniques translate instructions into low-level actions, such as joint states, examples of which include Open-X Embodiment [5] and Octo [18]. Such systems exhibit generalization capabilities across multiple modalities [19, 20], but they depend on large volumes of data. Alternatively, another method initially translates instructions into an intermediate representation, then employing off-the-shelf solvers to generate actions. This approach limits the solution space, thereby reducing the need for extensive data. The intermediate representations employed can vary from formal planning formalisms such as Planning Domain Definition Language (PDDL) and temporal logics, to less formal structures like code or predefined skills.\nPDDL is a model-based planning formalism outlining how to achieve a goal state from an initial state. Xie et al. [21] have explored prompting LLMs to extract goal states from instructions. Liu et al. [22] expand the approach to extract domain descriptions using LLMs. More recently, proposals for extensible benchmarks have emerged, aiming to systematically evaluate the planning capabilities of LLMs for tasks defined in PDDL [23]. LLMs also have shown promise in synthesizing code. ProgPrompt [24] leverages LLMs to call APIs that represent action primitives. Similarly, Code as Policies [25] employs LLMs to generate low-level executable code. Similar approaches are used in [26, 27, 28]. Instead of providing all APIs, Voyager [29] continuously writes executable codes and saves them in the skill library as reusable APIs. Saycan [30] uses LLMs to arrange pre-defined skills requiring precise visuomotor control. Inner Monologue [31] enhances this by integrating closed-loop language feedback to address failures. KNOWNO [32] adjusts LLM-based planners to align uncertainty, enabling the systems to seek assistance when necessary. A commonality is their focus on single-robot scenarios; however, extending these approaches remains largely unexplored.\nNatural Language to Temporal Logic Temporal logics are effective in tackling goals that involve temporal constraints and providing performance assurances. Initially, adaptations of natural language into temporal logics adopted grammar-based methods, which are well-suited for structured inputs [33]. More recently, the application of LLMs for such reasoning tasks has become popular [34]. Efforts like prompting GPT-3 to create LTL formulas by relying on established patterns. Cosler et al. [15] use LLMs to facilitate user interactions that help refine ambiguous or incorrect translations. However, these models focus on the translation process and do not tackle the challenges of language grounding in robotics-linking language with physical actions and environments. Pan et al. [35] develop a synthetic dataset of instructions paired with temporal logic formulas, used to fine-tune an LLM. Similarly, [36] translates languages into Signal Temporal Logic (STL) that handles combined task and motion planning. On the other hand, He et al. [37] create neural networks from scratch using synthetic data. Conversely, Patel et al. [38], Wang et al. [39] develop a weakly supervised semantic parser using execution trajectories without explicit LTL annotations. Liu et al. [40] introduce Lang2LTL, a modular system that employs LLMs to convert navigational commands into LTL specifications. Hsu et al. [41] use LLMs to transform natural language queries into First-Order Logic (FOL) programs, which are executed by FOL processors. In a different approach, Wang et al. [42] begin with a predefined LTL specification, with each predicate defined by concise instructions. Our research sets itself apart in several aspects: we address complex instructions using a hierarchical approach and incorporate task allocation among multiple robots."}, {"title": "LLMs to Multi-Robots", "content": "Recently, there has been a notable trend in adapting LLMs for use in multi-robot systems. SMART-LLM [27] uses LLMs to synthesize code that facilitates task decomposition, coalition formation, and task allocation. Roco [43] pairs a LLM with a robot, adopting a dialogue-based approach to coordinate tasks and generate sub-task plans using predefined skills. LLM-MRS [44] fine-tunes LLMs to generate plans in the form of behavior trees. Chen et al. [45] explore the efficacy of various communication frameworks (centralized, decentralized, or hybrid) where LLMs act as task planners. Garg et al. [46] employ LLMs to address deadlock resolution in navigation scenarios. Co-NavGPT [47] integrates LLMs as global planners to assign exploration frontiers to each robot, enhancing the efficiency of target searches. Wang et al. [48] introduce a decentralized LLM-based planner that allows robots to make individual decisions autonomously. However, the works mentioned above primarily focus on finding feasible solutions. In contrast, our research aims to optimize the cost and time required to complete tasks."}, {"title": "3 Preliminary", "content": "Linear Temporal Logic Linear Temporal Logic (LTL) is composed of basic statements, referred to as atomic propositions AP, along with boolean operators such as conjunction (\u2227) and negation (\u00ac), and temporal operators like next (\u25cb) and until (U) [49]. LTL formulas follow the syntax: \u03c6 := \u03a4 | \u03c0 | \u03c61 \u2227 \u03c62 | \u00ac\u03c6 | \u25cb \u03c6 | \u03c61 U \u03c62, where T stands for a true statement, and \u03c0 is a boolean valued atomic proposition. Other temporal operators can be derived from U, such as \u25ca that implies \u03c6 will be true at a future time. We focus on a subset of LTL known as syntactically co-safe formulas (sc-LTL) [50]. Any LTL formula encompassing only the temporal operators \u25c7 and U and written in positive normal form (where negation is exclusively before atomic propositions) is classified under sc-LTL formulas [50], which can be satisfied by finite sequences followed by any infinite repetitions. This makes sc-LTL apt for reasoning about robot tasks with finite durations.\nHierarchical LTL A hierarchical LTL, denoted by \u03a6 = {\u03c6^{k}_{i} | k = 1, ..., K, i = 1,...,|\u03c6_{k}|} where \u03c6^{k}_{i} is the i-th sc-LTL specification at level k, \u03c6_{k} denotes all specifications at level k, and |\u00b7| denotes the cardinality, includes K levels such that each specification at level k, for k = 1, . . ., K \u2013 1, is constructed from specifications at the lower level k + 1.\nWe refer to each specification \u03c6^{k}_{i} or \u03c6_{k} in \u03a6 as the \u201cflat\u201d specification. These flat specifications can be organized in a tree-like specification hierarchy graph, where each node represent a flat sc-LTL specification. Edges between nodes indicate that one specification encompasses another as a composite proposition. This composite proposition is, in essence, another flat sc-LTL formula. Leaf nodes represent leaf specifications at the K-th level that consist only of atomic propositions, while non-leaf nodes represent non-leaf specifications made up of composite propositions."}, {"title": "4 Natural Language to Hierarchical LTL", "content": "LLMs excel in common sense reasoning yet behavior poorly in logical reasoning [16, 51]. Therefore, we propose a two-stage method for translating natural language into hierarchical LTL using an intermediary structure known as the Hierarchical Task Tree. The framework is displayed in Fig. 1."}, {"title": "4.1 Conversion from human instructions to Hierarchical Task Tree", "content": "Definition 4.1 (Hierarchical Task Tree (HTT)) A Hierarchical Task Tree (HTT) is a tree T = (V,E, R), where a) V = {v1, v2, ..., vn} denotes the set of nodes, each representing a task. Each node is associated with a human instruction that describes its respective task. b) E \u2286 V\u00d7V represents the edges, indicating a decomposition relationship between tasks. Specifically, an edge e = (v1, v2) \u2208 E implies that child task v2 is one of sub-tasks of parent task v1. The node set V can be partitioned into multiple disjoint subsets {V1, ..., Vm}, such that all nodes within the same subset Vi share the same parent node. c) R \u2286 V\u00d7V defines the set of temporal relations between sibling tasks, which are decompositions of the same parent task. Specifically, a relation (v1, v2) \u2208 R, where v1, v2 \u2208 Vi for some i \u2208 {1, ..., m}, indicates that task v1 should be completed before task v2.\nThe tree is structured such that it unfolds level by level, where each child task is a decomposition of its parent task. The HTT is a simplified version of the hierarchical task network (HTN) as is specifically designed to align with the structure of hierarchical LTL. The relation R specifically captures the temporal relationships between sibling tasks that share the same parent. The temporal relationship between any two tasks can be inferred by tracing their lineage back to their common ancestor. This is the primary distinction between HTT and HTN, where HTN includes interdependencies between sub-tasks under different parent tasks. Another difference is that each node in the HTT is solely focused on the goal of a sub-task and does not incorporate other properties like preconditions and effects that are found in HTN. When a task instruction is received, we use LLMs to construct the HTT through a two-step process, as outlined in step 1 of Fig. 1.\n1. HTT without temporal relations R. The first step involves generating the nodes V and edges E, excluding the temporal relations R. Leveraging the LLMs' extensive understanding of hierarchical reasoning, the model decomposes the overarching task into a structured hierarchy. The decomposition continues until a task consists solely of sequential operations performed on a single object. Details of the prompt used for this decomposition are provided in Fig. 6 in Appendix A.1.\n2. Add temporal relations R. We achieve this by iterating over each non-leaf node. For each non-leaf node v, we consider V', which represents its child tasks at the level directly beneath it. Using LLMs, we then determine the temporal relations between sibling tasks within V'. The specific prompt used for querying these temporal relations is illustrated in Fig. 7 in Appendix A.1."}, {"title": "4.2 Generation of task-wise flat LTL specifications", "content": "Once the HTT representation is obtained, we advance to generate a single flat LTL specification for each node. This is described in Alg. 1, which employs a breadth-first search.\n1. Logical search For every non-leaf node v, we gather its child tasks V' and the temporal relations among them, defined by R' \u2286 V' \u00d7 V'. We then use LLMs to rephrase these child tasks and their temporal relations into syntactically correct sentences aligned with the semantics of LTL specifications (as illustrated in step 2.1 in Fig. 1). These reformulated sentences are input into a fine-tuned LLM that produces a single LTL formula (as depicted in step 2.2 in Fig. 1). The detail of fine-tuning LLMs is in Appendix A.3. It is important to note that we do not substitute the task node with its corresponding human instruction; instead, we use a \"lifted\" format where specific tasks are replaced with abstract symbols, like \"task 1.1 should be completed before task 1.2\". This abstraction allows the fine-tuned LLMs to operate without needing detailed knowledge of the tasks, as demonstrated in [35, 36]. The prompt is shown in Fig. 8 in Appendix A.2.\n2. Action completion Given an HTT, each leaf node should represent a simple task on certain object, such as \u201ctask 1.1.1 place plates into the lower rack\" in Fig. 1. By viewing such simple task as a sequence of action steps, we prompt the LLM to generate a sequence of pre-defined API calls to expand the simple task. For instance, the symbol \u03c0_{plates} that represents task 1.1.1 can be replaced with LTL specification composed of sequential APIs: \u03c0_{plates} = \u25c7(Pickup(plate) \u2227 \u25c7Move(plate, lower_rack)); see step 2.2 in Fig. 1. Such prompt can be found in Fig. 9 in Appendix A.2. After this step, a complete hierarchical LTL specifications is generated; see example in Fig. 10 in Appendix A.2.\""}, {"title": "5 Experimental Results", "content": "We evaluate the performance both in a simulated environment and through real-world experiments. For simulation, we use the AI2-THOR simulator [52] coupled with the ALFRED dataset [53]. AI2-THOR provides an interactive 3D environment that models various domestic settings. The ALFRED dataset focuses on natural language comprehension and embodied actions. We carried out two real-world experiments: one involving a robotic arm arranging fruits and vegetables on a tabletop, and another where four robotic arms transferred objects through handover. Throughout the evaluation, we employ the LLM ChatGPT-4 and aim to answer three key questions:\n1. Is our approach capable of handling complex human instructions effectively?\n2. Does our method successfully address tasks involving multiple robots while producing a high quality of solutions?\n3. Is our method flexible enough to adjust to the verbal styles of various users?"}, {"title": "5.1 Mobile manipulation tasks in AI2-THOR", "content": "Tasks The ALFRED dataset contains instructions for tasks with a number of strictly sequential steps, which we classify as base tasks. To create more complex tasks, we procedurally combine base tasks from the same floor plan and object configuration to generate derivative tasks, which are detailed in Appendix B.1. Derivative tasks are categorized based on the number of base tasks, which vary from 1 to 4. A list of derivative tasks are shown in Appendix B.2. Within each category,\nComparison We compare our method with SMART-LLM [27], which uses LLMs to generate Python scripts that invoke predefined APIs of actions for the purposes of task decomposition and task allocation. We could not compare our method with PDDL-based methods. First, we are not aware of any studies that translate instructions into PDDL for multi-robot systems, especially using the AI2-THOR simulator. Second, existing research on converting instructions into PDDL does not address temporal constraints. Furthermore, approaches that convert instructions into flat LTL are inadequate for handling tasks here, as they are too complex for flat LTL specifications.\nMetrics The metrics are as follows: 1) Success rate, which measures whether the target goal states of objects are achieved and if the order in which these states occur satisfies the specified temporal requirements. Additionally, to provide a detailed analysis of the success rate, we further break it down into two separate components: a) conversion, b) planning. 2) Travel cost, measured in meters, is defined as the total distance traveled by all robots, excluding any costs related to manipulation. 3) Completion time, quantified as the number of discrete time steps required to complete the tasks."}, {"title": "5.2 Real-world rearrangement experiments involving human participants", "content": "We conduct a real-world tabletop experiment, where a robotic arm places fruits and vegetables onto colored plates. Given the 2D nature of the task, we convert the environment into a discrete grid"}, {"title": "5.3 Multi-robot handover tasks", "content": "We examine the execution of pick-and-place tasks involving multiple objects by four fixed robot arms, which are either aligned in a straight line or arranged in a square configuration; see Fig. 4. Certain tasks might necessitate the transfer of objects between robots, depending on their proximity. The planner our approach uses, inspired from work [54], produces collision-free trajectories by simultaneously considering task and motion planning. The prompt for the baseline that directly uses the LLM as task planner is illustrated in Fig. 4."}, {"title": "6 Conclusions and Limitations", "content": "We proposed a method of transforming unstructured language into a structured formal representation with hierarchical structure. Our simulation and real-world experiment outcomes demonstrated that the framework offers an intuitive and user-friendly approach for deploying robots in daily situations.\nLimitations The proposed framework operates as an open loop without feedback. To transition to a closed-loop one, it is essential to integrate a syntax checker and a semantic checker. These components interact continually with LLMs to enhance the success rate. The syntax checker verifies adherence to the hierarchical LTL structure necessary for HTT representation. Meanwhile, the semantic checker offers feedback on errors when the planner fails to identify a solution. Another limitation is that once created, the HTT representation remains unchanged. Recall that we derive an LTL specification by extracting child tasks from a parent task. As more child tasks are included, the accuracy of translation drops. Therefore, to handle tasks with more base tasks, it is necessary to restructure the HTT to restrict the number of child tasks a single parent task has."}, {"title": "A Construction of hierarchical LTL specifications", "content": ""}, {"title": "A.1 Conversion from human instructions to Hierarchical Task Tree", "content": ""}, {"title": "A.2 Generation of task-wise flat LTL specifications", "content": ""}, {"title": "A.3 Fine-tuning LLMs to translate natural language to LTL specifications", "content": "Firstly, we developed a dataset comprising pairs of natural language descriptions and their corresponding LTL formulas, subsequently fine-tuning the Mistral-7B-Instruct-v0.2 [55] model for translation. Training datasets were synthesized from sources including Efficient-Eng-2-LTL [35], Lang2LTL [40], nl2spec [15], and NL2TL [36]. Given the domain-specific nature of these datasets, we substituted specific tasks with generic symbols such as \u201cp101 should be completed before p103\u201d paired with the LTL \u03c0 = \u25c7(p101 \u2227 \u25c7 p103). Next, we ask LLMs to reinterpret these \"lifted\" LTL specifications, creating a domain-agnostic dataset containing approximately 509 unique LTL formulas and 10,621 natural language descriptions produced by LLMs. Following this, the model was fine-tuned using 8-bit quantization over three epochs, achieving a translation accuracy of 98.5% from formal language descriptions to LTL formulas."}, {"title": "B A12-THOR", "content": ""}, {"title": "B.1 Generation of derivative tasks", "content": "We create larger tasks by merging a specific number of base tasks, introducing various temporal constraints to these combinations, which we refer to as derivative tasks. Specifically, to ensure that the same object is not included in multiple base tasks simultaneously, we first use an LLM to identify the objects involved in each base task. We then combine base tasks that involve distinct objects and randomly establish temporal relationships between them. Subsequently, an LLM reformulates this randomly combined information into derivative tasks that align more naturally with human expression patterns."}, {"title": "B.2 Derivative tasks with 4 base tasks", "content": "A list of these derivative tasks with 4 base tasks is presented below.\n1. Put a microwaved egg in the garbage can. Place a rinsed spoon into the drawer. Heat up a slice of potato and put it against the apple. After these are finished, put the tomato in the microwave.\n2. First Rinse off a tomato slice and cook it in the microwave. Next, heat and place a potato in the sink. Then, put the statue in the sink. Finally, place a heated mug on a counter.\n3. Put a bowl with a credit card on the table at any time. Meanwhile place the watch on the table on a plate in any order. After placing the bowl and the watch, put the box with vase in it beside the laptop on the table. Finally, place newspaper onto the sofa.\n4. Place a credit card inside a box and take the box to a shelf. At same time, take the remote controls and place them on the couch. After completing these tasks, move a pillow from the couch to a chair. Finally, place a plate with key chain on it on the table in the corner of the room.\n5. Move the plunger to the cabinet under the sink at any time. Put the soap bottle on top of the toilet at any time. in any order, put a towel on the toilet at any time. After placing the plunger, soap bottle, and towel, put a bar of soap on the back of the toilet.\n6. Get spray bottle from toilet, place spray bottle on dresser at any time. After that, place a clean bar of soap down on top of the toilet. Once done, store a plunger in a dresser. Lastly, put away candles in the cabinet.\n7. Pick up the key chain near the lamp to turn it off. After turning off the lamp, move the laptop from the table to the chair. At any time, take the box from the table and put it on the couch. Lastly, Pick up a watch from the table, and put them on the TV stand."}, {"title": "C Real-world experiments", "content": ""}]}