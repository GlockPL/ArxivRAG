{"title": "LDP: A Local Diffusion Planner for Efficient Robot Navigation and Collision Avoidance", "authors": ["Wenhao Yu", "Jie Peng", "Huanyu Yang", "Junrui Zhang", "Yifan Duan", "Jianmin Ji", "Yanyong Zhang"], "abstract": "The conditional diffusion model has been demonstrated as an efficient tool for learning robot policies, owing to its advancement to accurately model the conditional distribution of policies. The intricate nature of real-world scenarios, characterized by dynamic obstacles and maze-like structures, underscores the complexity of robot local navigation decision-making as a conditional distribution problem. Nevertheless, leveraging the diffusion model for robot local navigation is not trivial and encounters several under-explored challenges: (1) Data Urgency The complex conditional distribution in local navigation needs training data to include diverse policy in diverse real-world scenarios; (2) Myopic Observation Due to the diversity of the perception scenarios, diffusion decisions based on the local perspective of robots may prove suboptimal for completing the entire task, as they often lack foresight. In certain scenarios requiring detours, the robot may become trapped. To address these issues, our approach begins with an exploration of a diverse data generation mechanism that encompasses multiple agents exhibiting distinct preferences through target selection informed by integrated global-local insights. Then, based on this diverse training data, a diffusion agent is obtained, capable of excellent collision avoidance in diverse scenarios. Subsequently, we augment our Local Diffusion Planner, also known as LDP by incorporating global observations in a lightweight manner. This enhancement broadens the observational scope of LDP, effectively mitigating the risk of becoming ensnared in local optima and promoting more robust navigational decisions. Our experimental results demonstrated that the LDP outperforms other baseline algorithms in navigation performance, exhibiting enhanced robustness across diverse scenarios with different policy preferences and superior generalization capabilities for unseen scenarios. Moreover, we highlighted the competitive advantage of the LDP within real-world settings.", "sections": [{"title": "I. INTRODUCTION", "content": "With the rapid advancement of artificial intelligence and robotics, an increasing number of technologies are being integrated into motion planning for robot collision avoidance [1]. Many learning-based methods model the planning task as a conditional probability generation problem, where the robot's action sequence is a latent variable with its prior distribution [2], [3]. The planning process is accomplished by computing the posterior distribution based on conditions such as robot observations, final rewards, and constraints.\nAs is well known, the real-world environment for robot navigation is complex, encompassing various scenarios. Designing a specific policy for each scenario would require immense effort and lack the necessary flexibility and scalability.\nTherefore, an outstanding navigation policy must effectively handle diverse scenarios. Furthermore, the distribution of near-optimal expert policies often varies across different scenarios, highlighting the need for navigation policies capable of addressing diverse scenarios to exhibit a multimodal distribution. Given these requirements, collecting expert data from diverse scenarios to use as model training data and constructing models that better represent multimodal distributions will become crucial. Additionally, the limited local perspective of robots often fails to provide sufficient information for devising policies to tackle diverse scenarios. Robots frequently fall into suboptimal states during navigation tasks. This situation is commonly known as a local minimum problem [4].\nTherefore, to address the challenges mentioned above, we have made two efforts in this paper: (1) we have collected expert policy data with multiple preferences under diverse scenarios and utilize the diffusion model, which has strong distribution modeling capabilities, to construct the policy model. (2) We incorporate global paths as an additional condition to guide the diffusion model, enhancing policies for better navigation through maze-like scenarios and similar environments. Specifically, we have collected expert policy data in three different types of scenarios, i.e., dense static, dynamic pedestrian, and maze-like. For each scenario, we have gathered expert policy data for two preferences, i.e., the original Soft Actor-Critic (SAC) policy [5] and the SAC policy guided by global paths. Using the Denoising Diffusion Probabilistic Models (DDPM) [6] algorithm, based on robot observations (local costmaps, goals, and global paths), we directly denoise and sample from the posterior trajectory distribution step by step, generating the final robot action sequence.\nIn our experiments, the LDP outperforms other baseline algorithms across various scenarios. It exhibits exceptional learning capabilities when dealing with data from mixed scenarios, displaying remarkable robustness. Moreover, it demonstrates enhanced performance in unseen scenarios, showcasing impressive zero-shot generalization abilities. The addition of the global path as conditional guidance enhances our policy's capacity to comprehend sample distribution, leading to more forward-thinking policy outcomes. Furthermore, our approach leverages the advantages offered by expert policy data from two preferences, resulting in more effective decision-making in diverse navigation scenarios. Ultimately, we deploy our policy algorithm in real-world scenarios and on robotic platforms, thereby showcasing its competitive advantages.\nThe main contributions of our work are summarized as follows:\n\u2022 The paper introduces LDP, a novel local planning algorithm for robotic collision avoidance, leveraging diffusion processes.\n\u2022 We provide a dataset of expert policy based on 2D laser sensing, which spans expert data across three different types of scenarios and two different preferences.\n\u2022 With global paths serving as additional guiding conditions, the diffusion model can better learn the distribution of expert data and make wiser decisions.\n\u2022 We conducted extensive experiments demonstrating that LDP outperforms other baseline algorithms in terms of superior navigation performance, stronger robustness, and more profound generalization capabilities. Furthermore, we validated the effectiveness of the algorithm by deploying it on physical robots, thus highlighting its practical value."}, {"title": "II. RELATED WORK", "content": "A. Traditional Navigation Approaches\nTraditional navigation systems [1], [7] typically adopt a hierarchical paradigm that combines global and local path planning with motion control. These methods can be broadly classified into three categories: search-based planners (e.g., hybrid A* [8], JPS [9]), sampling-based planners (e.g., PRM [10] and RRT [11]), and optimization-based planners (e.g., TEB [12]). Despite their prevalent usage, a significant amount of effort is needed to tune parameters for these methods to adapt to a wide range of scenarios.\nB. Learning-Driven Navigation Approaches\nImitation learning (IL) is a process that involves learning from examples provided by an expert, typically in the form of decision-making data from human operators. IL has a broad and profound influence in areas such as robotic navigation [5], [13], [14], manipulation [3], [15], and autonomous driving [16]\u2013[18]. Depending on the structure of the policy model construction, these methods can be bifurcated into two categories:\n1) Explicit Policy: These methods learn the mapping from observations to actions directly, guiding the policy learning process via a regression loss. [18] employed behavior cloning (BC) to train the end-to-end deep convolutional neural network (CNN) for autonomous driving. The network ingests images from a car camera and yields the steering wheel angle for the vehicle. However, these policies often grapple with the effective modeling of multimodal data distributions. They usually map one policy to one scenario, and learning from data across multiple scenarios can lead to catastrophic forgetting. Furthermore, they often face challenges in generalizing to new, unseen scenarios.\n2) Implicit Policy: These methods employ energy-based models(EBMs) [19] to represent action distributions. Each action is assigned an energy value, and the action prediction problem is transformed into an optimization problem to find the action with the lowest energy. This implicit design approach can more effectively represent the multimodal distribution of expert actions.\nC. Diffusion Model for Robotic Decision Planning\nIn recent years, a growing number of works have leveraged diffusion models to perform intelligent agent decision planning tasks, including imitation learning [3], [20], [21] and reinforcement learning [22]\u2013[24]. Diffuser [22] concatenates state-action sequences of a certain length into a two-dimensional array, employs the original DDPM method for unconditional sampling, and designs a classifier based on rewards, goals, and other information to guide the inference denoising process, thereby ensuring that the generated decision sequences comply with the respective constraints. The diffusion policy [3] represents an alternative modeling method that uses the robot's visual observations as conditions and directly guides the generation of action sequences in a classifier-free manner. MPD [20] integrates diffusion models and optimization-based methods. With the planning of start and end points and various optimization costs as conditions, it guides the generation of global motion planning in static scenarios. However, LDP accomplishes local motion planning in diverse scenarios (static and dynamic environments). NoMaD [21] harnesses the powerful distribution modeling capability of diffusion models and employs a goal mask to achieve a single policy capable of completing both navigation and exploration tasks. In contrast, LDP evaluates the navigation performance of the policy model under a mixed expert trajectory distribution of multiple scenes and preferences and introduces additional global paths as conditions for guidance."}, {"title": "III. METHOD", "content": "In this section, we initially outline our approach to gathering training data focusing on the Data Urgency challenge. Subsequently, we delve into the specifics of developing the diffusion model for local planning using the data collected to address the Myopic Observation problem. The comprehensive workflow of our research is illustrated in Fig. 2.\nA. Expert Policy Data\nIn recent times, the swift advancement of reinforcement learning has unveiled novel solutions for robotic motion planning challenges. We employ the SAC algorithm to learn sophisticated navigation policies. This robot navigation problem is conceptualized as a Markov Decision Process. The expert policy employs the same structure as that in [5], which has been proven to be effective. The policy's state space is bifurcated into (1) egocentric costmaps with dimensions of 84\u00d784, produced by a 3D laser sensor, encompassing a complete 360-degree view; (2) the relative target pose. The action space is bidimensional and continuous, denoting the linear velocity and the angle of the front wheel for Ackermann steering robots. Given that the linear velocity can assume negative values, this expert navigation policy accommodates backward movement. Building upon the meticulously crafted state and action spaces, and enabling the robot to swiftly reach its target without any collisions, the reward function for the reinforcement learning policy has been thoughtfully devised as follows:\n$r_{t} = r_{t}^{goal} + r_{t}^{safe} + r_{t}^{shaping} + r_{t}^{back}$,\nwhere $r_t$ is the sum of four parts, $r_{t}^{goal}$, $r_{t}^{safe}$, $r_{t}^{shaping}$ and $r_{t}^{back}$. Specifically, $r_{t}^{goal}$ represents the reward awarded to the robot upon successfully reaching the designated local target:\n$r_{t}^{goal} =\n\\begin{cases}\nr_{arr},\\\\\n0,\n\\end{cases}\n\\begin{aligned}\n&\\text{if target is reached,}\\\\\n&\\text{otherwise.}\n\\end{aligned}$\n$r_{t}^{safe}$ denotes the penalty applied to the robot in the event of a collision:\n$r_{t}^{safe} =\\begin{cases}\nr_{col},\\\\\n0,\n\\end{cases}\n\\begin{aligned}\n&\\text{if collision,}\\\\\n&\\text{otherwise.}\n\\end{aligned}$\n$r_{t}^{shaping}$ denotes the reward-shaping mechanism that converts sparse rewards into dense rewards, thereby hastening the training process of reinforcement learning algorithms. The underlying design philosophy encompasses two main elements: (1) Actions diverting the robot from its local goal incur specific penalties; (2) To deter the agent from engaging in clever but impractical strategies-like spiraling near the target to gain higher rewards, counter to the necessity for swift navigation task completion-an additional fixed-value penalty is levied on each action.\n$r_{t}^{shaping} = \\epsilon (||p_{t-1} - p_{g} ||_{2} - ||p_{t} - p_{g}||_{2}) - \\gamma_{step}$,\nwhere $p_t$ is the position of the robot at time $t$, $p_g$ is the position of the target point, $\\gamma_{step}$ is a fixed-value penalty, and $\\epsilon$ is a hyper-parameter.\nThe term $r_{t}^{back}$ denotes a fixed-value penalty applied when the robot executes a reverse maneuver. This mechanism is designed to prompt the robot to reverse only when needed, instead of persistently backing up from start to end, thus aligning more closely with practical applications.\n$r_{t}^{back} =\\begin{cases}\nr_{reverse},\\\\\n0,\n\\end{cases}\n\\begin{aligned}\n&\\text{if } v < 0,\\\\\n&\\text{otherwise,}\n\\end{aligned}$\nwhere $r_{reverse}$ is also a fixed-value penalty and $v$ is robot's linear velocity at time $t$. In our experiments, we set $r_{arr} = 500$, $r_{col} = -500$, $\\epsilon = 200$, $r_{step} = 5$ and $r_{reverse} = -10$.\nWithin the scope of our research, for every experimental scenario delineated in the paper, we meticulously trained an expert policy, subjecting each to an extensive training regimen spanning three million steps. Leveraging these expert policies, we collated datasets reflecting two strategic preferences: (1) The original expert policy data. While the reward-shaping mechanism outlined in Eq. (4) offers significant benefits, such as expedited training, diminished complexity, and the facilitation of rapid task completion, it tends to engender policies that overly greedy, lacking in long-term planning, and unsuitable for tasks in maze-like scenarios. (2) Expert policy data guided by global path planning. We utilized the A* algorithm to search for global paths and provided local targets to the expert policies through a sliding window approach. This type of policy overcomes the aforementioned issues but lacks efficiency in task completion, leading to detours in certain scenarios. Our objective is to enhance the policy model's performance by learning from the mixed data of these two types of preferred policies.\nB. Local Diffusion Planner\nThe objective of our research is to develop a local motion planning algorithm for robots, by leveraging multimodal expert strategy data encompassing a variety of environments and preferences. Therefore, we formulate the task as a conditional generation problem via diffusion model:\n$\\max_{\\theta}E_{\\tau \\sim T,\\tau = {O,A}} log p_{\\theta}(A^{0}|O)$,\nwhere is the expert trajectory data used for training, $A^0$ is the final generated sequence of actions, $O$ represents the robot's observations serving as conditions for the diffusion model, and $p_\\theta$ refers to the reverse denoising process within the diffusion model.\nIn this paper, we structure the training data according to a receding-horizon action prediction framework, as outlined in [3]. Here, $\\mathcal{T}_t = {O_t, A_t}$ signifies the chosen robot training trajectory at time $t$, where $O_t$ and $A_t$ respectively denote the corresponding observation sequence ${o_{t-(T_o-1)},\\cdots, o_{t-1}, o_t}$ and action sequence ${a_t, a_{t+1},..., a_{t+(T_a-1)}}$ for that trajectory. $T_o$ and $T_a$ represent the lengths of the observation and action sequences, respectively, with the length of the trajectory $T$, denoted as $T_r$, being equal to $T_o + T_a - 1$. It should be emphasized that, throughout this paper, all superscripts associated with time instances refer to diffusion time steps, and all subscripts associated with time instances pertain to motion time steps.\nAs we have discussed earlier, $O$ consists of three parts: costmaps $C$, goals $G$, and global paths $GP$, i.e., $O = {C, G, GP}$. It is important to note that here, the global paths act merely as conditions for the diffusion process, rather than providing an additional local target for the policy during inference, as is done in our method of collecting expert policy data.\nThe training of the model leverages the DDPM algorithm, incorporating classifier-free guidance [25] for its execution. The ultimate action sequence, $A^0$, is derived by initially sampling from Gaussian noise $A^K$. Through adjacent diffusion time steps, from $A^K$ to $A^{K-1}$, the action sequence is subjected to noise perturbation. This methodical alteration facilitates the denoising and refinement of the sequence itself. The perturbation noise $\\epsilon_\\theta(A^k, O, k)$ is defined as follows [26]:\n$\\epsilon = \\epsilon_{\\theta}(A^{k}, k) + \\omega(\\epsilon_{\\theta}(A^{k}, O, k) - \\epsilon_{\\theta}(A^{k}, k))$,\nwhere $\\omega > 1$ represents the guidance scale, a factor in identifying the expert action sequence that optimally aligns with the robot's current observations from the expert dataset, and $\\epsilon_\\theta$ denotes the noise model. Eq. (6) is inspired by an implicit classifier $p(O|A^k) \\propto p(A^k|O)/p(A^k)$. The gradient of the logarithmic probability of this classifier $\\nabla_{A^{k}} \\log p(O|A^k) \\propto \\nabla_{A^{k}} \\log p(A^k |O) - \\nabla_{A^{k}} \\log p(A^k) \\propto \\epsilon(A^{k}, O, k)-\\epsilon(A^{k}, k)$ is utilized to guide the generation of $\\bar{\\epsilon}$. In the training phase, we aim to optimize the reverse diffusion process $p_\\theta$, which is parameterized by the noise model, pursuing the following objective:\n$\\mathcal{L}(\\theta) := E_{k \\sim U(1,K),\\epsilon \\sim N(0,1)} [||\\epsilon - \\epsilon_{\\theta}(A^{k}, O, k) ||^{2}]$.\nIn the inference phase, the ultimate expert action sequence is derived via a stepwise sampling process, as outlined in the formula $A^{k-1} \\sim N(\\mu_{\\theta}(A^{k-1},O, k - 1), \\beta\\Sigma_{k-1})$.\nThe design of the diffusion model network structure follows the approach of work [3], which has been validated as efficient and outstanding."}, {"title": "IV. EXPERIMENTS", "content": "A. Environments, baselines, and metrics\nIn this study, we meticulously gather expert data reflecting two preferences across three diverse scenarios, subsequently training corresponding navigation policies. Besides, we evaluate the performance of models trained with mixed policy data and assess their zero-shot generalization capabilities in unseen scenarios. Fig. 3 illustrates environments for four different scenarios respectively, where a, c, and d represent enclosed scenarios, while b represents an open scenario.\nWe compare LDP with the following baseline algorithms: LSTM-GMM [27], IBC [28], and DT [29]. We strive to adjust the training details to optimize the performance of the methods as well as possible. Notice that, the input content for these baseline algorithms consists of {C, G, GP}, which aligns with the guiding conditions of LDP.\nWe introduce five distinct metrics to systematically assess the navigation performance of all methods within a range of scenarios.\n\u2022 Success rate (SUCC): the rate of episodes in which the robot reaches the target pose without collision.\n\u2022 Collision rate (Coll): the rate of episodes in which the robot collides.\n\u2022 Stuck rate (Stuck): the rate of episodes in which the robot stucks.\n\u2022 Average time (TIME): the average cost time at each success episode.\n\u2022 Success weighted by Path Length (SPL) [30]: assess navigation performance by examining both the success rate and the length of the robot's trajectory.\nB. Experiments on simulation scenarios\nIn the following, the experimental results for various methods are presented based on the average outcomes from 1,000 randomly generated environments for each scenario. The training data for each scenario consists of 2,000 episodes.\n1) Comparative experiments: Tab. II presents the navigation performance of all algorithms outlined in our paper. Each entry comprises three values: the performance metrics of models trained on single-scenario data (2000 episodes), the performance metrics of models trained on single-scenario data (6000 episodes), and the performance metrics of models trained on mixed data from three scenarios (6000 episodes). The LDP algorithm surpasses other baseline algorithms in success rate, runtime, and SPL. Comparing the first and last two items, we can conclude that increasing the quantity of training data to some extent can enhance the navigation performance across a wide range of algorithms and scenarios. Contrasting the second and third items, we can infer that enriching the diversity of data while maintaining the same training data quantity poses a challenge for baseline algorithms, resulting in a decline in navigation performance. However, thanks to the superior distribution modeling capability of LDP, its navigation performance may remain stable or even improve. An interesting observation is that LSTM-GMM, when trained on data from a single dense static scenario, cannot accurately reach the target point. It tends to wander near the target point, resulting in a high stuck rate. Even increasing the training data volume does not resolve this issue. However, introducing data from diverse scenarios can significantly enhance the performance of LSTM-GMM. Notably, in the zigzag scenario, the performance of LDP underscores its robust zero-shot generalization capability in unseen scenarios.\n2) Ablation study: In the design of the LDP approach, we have additionally introduced global paths GP as conditions for the diffusion model to guide wiser decision-making in complex scenarios. Tab. III illustrates that LDP outperforms LDP without GP, particularly in dense static and maze-like scenarios. In Fig. 4, we showcase a maze-like scenario where LDP effectively navigates around maze walls and reaches the target point, while LDP without GP gets obstructed by the walls, failing to complete the navigation task. The experimental results suggest that additional GP conditions can better assist LDP in modeling data distributions and guiding wiser decision-making.\nIn our paper, we collected expert data reflecting two different preferences. Therefore, in Tab. IV, we explore the impact of training data composition with varying preferences on model performance. All experiments are trained on the same amount of data for the same training steps. The \u201cNo.1 expert\u201d, \u201cNo.2 expert\u201d, and \u201cmixed expert\u201d respectively represent data collected from original SAC (2000 episodes), SAC guided by global paths (2000 episodes), and a mixture of both, with 1000 episodes each. Our experimental results demonstrate that mixed preference data can enhance the policy's performance in dynamic and maze-like scenarios. While in static scenes, the success rate of the mixed data policy slightly lags behind that of the No.2 expert data policy, it outperforms in terms of average execution time and SPL. In the maze-like scenario presented in Fig. 5, LDP with mixed expert data efficiently completes the navigation task. Conversely, LDP with No.2 expert data initially encounters obstacles due to erroneous decisions but eventually succeeds after prolonged exploration, while LDP with No.1 expert data remains trapped and unable to finish the task. Providing data with mixed preferences is meaningful as it allows the policy to leverage the advantages of learning different preference policies, resulting in more efficient and accurate task completion.\nC. Deploy to real-world Ackermann steering robot\nWe've implemented the LDP algorithm on a real Ack-erman robot to evaluate its performance in real-world situations. This experimental robot is built on the Agilex hunter2.0 chassis and features a 32-line 3D RoboSense LiDAR. It's powered by an RTX 3090 GPU and measures 0.95m\u00d70.75m\u00d71.45m in size. For a more detailed overview of the simulation and physical experiment results, please refer to our video."}, {"title": "V. CONCLUSIONS AND FUTURE WORK", "content": "In this paper, we introduce a novel local diffusion planner, LDP, designed for robot collision avoidance, employing diffusion processes. Our approach involves gathering expert data representing two different preferences from three diverse scenarios to train our model. Our series of experiments indicate that LDP exhibits better navigation performance, and stronger robustness in learning from expert data across scenes. Additionally, it can learn wiser and more visionary policies from multi-preference expert data and demonstrate strong generalization ability in unseen scenarios. Our real-world experiments also demonstrate the practical value of LDP.\nIn future work, two aspects could be further explored: (1) Collecting higher quality and more diverse expert data to train superior navigation policies; (2) Improving the real-time performance of LDP. Using flow-based methods [31] or consistency models [32] instead of DDPM to accelerate diffusion model sampling would facilitate the practical deployment of the LDP method."}]}