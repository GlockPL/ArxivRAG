{"title": "How Does the Smoothness Approximation Method Facilitate Generalization for Federated Adversarial Learning?", "authors": ["Wenjun Ding", "Ying An", "Lixing Chen", "Shichao Kan", "Fan Wu", "Zhe Qu"], "abstract": "Federated Adversarial Learning (FAL) is a robust framework\nfor resisting adversarial attacks on federated learning. Al-\nthough some FAL studies have developed efficient algorithms,\nthey primarily focus on convergence performance and over-\nlook generalization. Generalization is crucial for evaluating al-\ngorithm performance on unseen data. However, generalization\nanalysis is more challenging due to non-smooth adversarial\nloss functions. A common approach to addressing this issue\nis to leverage smoothness approximation. In this paper, we\ndevelop algorithm stability measures to evaluate the generaliza-\ntion performance of two popular FAL algorithms: Vanilla FAL\n(VFAL) and Slack FAL (SFAL), using three different smooth\napproximation methods: 1) Surrogate Smoothness Approxi-\nmation (SSA), (2) Randomized Smoothness Approximation\n(RSA), and (3) Over-Parameterized Smoothness Approxima-\ntion (OPSA). Based on our in-depth analysis, we answer the\nquestion of how to properly set the smoothness approximation\nmethod to mitigate generalization error in FAL. Moreover, we\nidentify RSA as the most effective method for reducing gen-\neralization error. In highly data-heterogeneous scenarios, we\nalso recommend employing SFAL to mitigate the deterioration\nof generalization performance caused by heterogeneity. Based\non our theoretical results, we provide insights to help develop\nmore efficient FAL algorithms, such as designing new metrics\nand dynamic aggregation rules to mitigate heterogeneity.", "sections": [{"title": "Introduction", "content": "Federated Learning (FL) (McMahan et al. 2017; Qu et al.\n2022; Li et al. 2023) plays an important role as it allows dif-\nferent clients to train models collaboratively without sharing\ndata samples. Although FL is considered a secure paradigm\nto protect users' private data, it has been vulnerable to adver-\nsarial attacks (Wang et al. 2020; Bagdasaryan et al. 2020). Ad-\nversarial examples (Szegedy et al. 2013; Goodfellow, Shlens,\nand Szegedy 2014) are typically designed to mislead mod-\nels into producing incorrect outputs, significantly degrading\nlearning performance and achieving the attacker's goals.\nTo counter these attacks, much effort has been made to im-\nprove neural networks' resistance to such perturbations using\nAdversarial Learning (AL) (Goodfellow, Shlens, and Szegedy\n2014; Samangouei, Kabkab, and Chellappa 2018; Madry et al.\n2018). Generating adversarial examples during neural net-\nwork training is one of the most effective approaches for\nAL (Carlini and Wagner 2017; Athalye, Carlini, and Wagner\n2018; Croce and Hein 2020). Consequently, recent studies\n(Zizzo et al. 2020; Hong et al. 2021; Shah et al. 2021; Zhou,\nWu, and He 2021; Li et al. 2021) have proposed a new FL\nframework called Federated Adversarial Learning (FAL) to\nmitigate the impact of adversarial attacks. It is important to\nimprove the robustness of FL in real-world applications.\nThere are two popular algorithms for FAL: Vanilla FAL\n(VFAL) (Shah et al. 2021) and Slack FAL (SFAL) (Zhu et al.\n2023). In particular, VFAL combines FedAvg (McMahan\net al. 2017) with AL on the client side to train the global\nmodel. In contrast, SFAL modifies the aggregation process\nby dynamically adjusting the weights of certain client models\nduring aggregation based on their evaluated importance.\nAlthough these two algorithms have shown significant ef-\nfectiveness and robustness, they prioritize convergence at the\nexpense of generalization ability (Li, Song, and Yang 2023;\nZhang et al. 2022). Generalization is a crucial aspect of evalu-\nating an algorithm, as it measures the performance of trained\nmodels on unseen data. One popular approach to studying\ngeneralization is to examine algorithmic stability (Bousquet\nand Elisseeff 2002; Hardt, Recht, and Singer 2016), which\nmeasures sensitivity to perturbations in the training dataset.\nRecently, a series of studies (Xing, Song, and Cheng 2021a;\nXiao et al. 2022a; Cheng, Fu, and Farnia 2024) have investi-\ngated the generalization error in AL.\nUnfortunately, these generalization studies primarily focus\non the centralized setting. In FAL, one of the most important\nproblems is the heterogeneity across clients, which leads to\nlocally generated adversarial examples being highly biased to\neach local distribution (Zhang et al. 2023; Chen et al. 2022).\nExperimental results have demonstrated that this exacerbated\ndata impairs the generalization ability of FAL. Thus, the\ngeneralization bounds of AL are insufficient for generalizing\nto FAL. While (Sun, Niu, and Wei 2023; Lei, Sun, and Liu\n2023) explored the relationship between data heterogeneity\nand generalization in FL, characterizing generalization in\nFAL poses additional challenges.\nUnlike FL, the loss function in FAL (Sadeghi et al. 2020;\nZhou, Wu, and He 2021) is typically non-smooth, violating\nthe stability analysis of algorithms that rely on smooth func-\ntions (Hardt, Recht, and Singer 2016; Sun, Niu, and Wei\n2023; Lei, Sun, and Liu 2023). A practical way to tackle\nthis issue is to employ a method of smoothness approxima-\ntion, e.g., (1) Surrogate Smoothness Approximation (SSA)\n(Cui et al. 2021), (2) Randomized Smoothness Approximation\n(RSA) (Alashqar et al. 2023), and (3) Over-Parameterized\nSmoothness Approximation (OPSA) (Li, Song, and Yang\n2023; Li et al. 2022). Although these methods can mitigate\nthe non-smoothness property, generating uncertain adversar-\nial examples may still lead the global model into sharp valleys\nand reduce the consistency of FAL, hindering its generaliza-\ntion performance. This raises the question: How to properly\nset the smoothness approximation method to reduce the\ngeneralization error for FAL?\nThe answer to the above question is two-fold: 1. Which\napproximation method is most suitable for FAL? 2. How\nshould the parameters of each approximation method be set\nto reduce the generalization error? Therefore, in this paper,\nwe provide detailed analyses for VFAL (Zizzo et al. 2020;\nShah et al. 2021; Li et al. 2022; Zhang et al. 2022) and SFAL\n(Zhu et al. 2023) using the three smoothness approximation\nmethods. To the best of our knowledge, this is the first study\nto investigate the generalization of FAL. Our findings will of-\nfer valuable insights for developing efficient FAL algorithms.\nThe main contributions can be summarized as follows:\n(1) We demonstrate the generalization bound of the three\nsmoothness approximation methods for the VFAL algorithm.\nOur results first indicate that increased heterogeneity signif-\nicantly impairs the ability to generalize. Then, the result of\nRSA shows the best generalization error, scaling with O(T).\nNotably, SSA shows improved performance by adding less\nnoise. For OPSA, properly controlling the width of the neu-\nral network can reduce the generalization error. (2) We also\nprovide the generalization bound of SFAL and compare it\nwith VFAL. Our analysis demonstrates the impact of differ-\nent global aggregation methods on generalization bound. In\nparticular, considering some AL-related metrics, i.e., local\nadversarial loss, as a client contribution evaluation criterion\nto design dynamic global aggregation rules helps to improve\ngeneralization. (3) Based on our results, we have proposed\nsome new metrics related to adversarial loss, e.g., contrastive\nloss and adversarial penalty, that may not have been consid-\nered in previous work. We believe that incorporating these\nmetrics for dynamic global aggregation or into the local train-\ning could help design a more efficient FAL algorithm."}, {"title": "Related Work", "content": "FL is considered an efficient and privacy-preserving method\nfor distributed learning environments. Generally, FL can be\ncategorized into aggregation schemes (McMahan et al. 2017;\nQu et al. 2022) and optimization schemes (Mohri, Sivek, and\nSuresh 2019; Reddi et al. 2020). However, FL remains vul-\nnerable to adversarial attacks. Adversarial examples pose a\nsignificant threat to learning models, as perturbations in input\ndata can mislead classifiers. To combat this, AL has been pro-\nposed to bolster robustness (Goodfellow, Shlens, and Szegedy\n2014; Samangouei, Kabkab, and Chellappa 2018; Madry et al.\n2018). Recent studies have also delved into robust overfitting\n(Rice, Wong, and Kolter 2020). However, directly integrating\nAL into FL poses several challenges, including poor con-\nvergence, data heterogeneity, and communication costs. To\naddress these challenges, (Zizzo et al. 2020; Shah et al. 2021;\nZhang et al. 2022; Li, Song, and Yang 2023; Zhu et al. 2023)\nhave proposed corresponding FAL algorithms.\nGeneralization analysis has been widely used to evaluate al-\ngorithms in both FL and AL. (Mohri, Sivek, and Suresh 2019)\nleverages Rademacher complexity to develop a uniform con-\nvergence bound for FL. Subsequently, various FL studies\nhave introduced generalization bounds based on Rademacher\ncomplexity (Sun and Wei 2022; Qu et al. 2023). (Hu, Li,\nand Liu 2022) considers a faster rate with Bernstein con-\nditions and bounded losses under convex objectives. Addi-\ntionally, (Sun, Niu, and Wei 2023; Lei, Sun, and Liu 2023)\nexplore generalization upper bounds using on-average stabil-\nity (Kearns and Ron 1997; Kuzborskij and Lampert 2018).\nMoreover, (Xing, Song, and Cheng 2021a) explores stability\nby highlighting the non-smooth nature of adversarial loss, and\n(Xiao et al. 2022a) develops stability bounds using smooth-\nness approximation. Building on these insights, (Xiao et al.\n2022b; Xing, Song, and Cheng 2021b) propose smoothed\nversions of SGDmax and robust deep neural networks."}, {"title": "Preliminaries", "content": "Federated Adversarial Learning (FAL)\nTo improve the robustness of FL against adversarial attacks,\nsome studies have developed the FAL framework (Zizzo et al.\n2020; Shah et al. 2021; Zhang et al. 2022; Li, Song, and Yang\n2023; Zhu et al. 2023). Although these frameworks have\ndemonstrated efficiency from the convergence perspective,\ntheir generalization performance may limit their applicability,\nwhich motivates us to analyze the underlying issues.\nTypically, we consider a FAL framework with m clients.\nEach client i obtain the local dataset $(x_i, Y_i) \\in S_i$ coming\nfrom the unknown distribution $P_i (P_i \\neq P_{i'}, i \\neq i')$ with the\nsize $n_i = |S_i|$. Let $l(\\theta, (x, y))$ be the loss function with the\nlearning model $\\theta$. The local objective of each client i is to\nminimize the local population risk, which is defined by:\n$R_i(\\theta) = E_{(x_i,y_i)\\sim P_i}[l (f_\\theta[x_i + A_\\rho(f_\\theta, x_i, Y_i)], Y_i)],$ (1)\nwhere $A_\\rho$ is an attack of strength $\\rho > 0$ and intended to\ndeteriorate the loss in the following way:\n$A_\\rho(f_\\theta, x_i, Y_i) := \\underset{\\delta \\epsilon B_\\rho(0, \\rho)}{argmax} {l(f_\\theta(x_i + \\delta), y_i)},$ (2)\nwhere $B_\\rho(0, \\rho)$ is a $L_p$ ball with radius $\\rho$ and $p = 1,2$ or,\n$\\infty$ for different types of attacks. For simplicity, we rewrite\n$l (f_\\theta[x + A_\\rho(f_\\theta, x, y)], y) = l_\\rho(\\theta, z)$ and $z = \\{x,y\\}$ in this\npaper. Beyond the individual local objectives, all m clients\ncollaboratively minimize the global objective, defined as\n$R(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}R_i(\\theta)$. However, directly minimizing the\nglobal objective $R(\\theta)$ is challenging due to the unknown dis-\ntributions $P_i$. Thus, a common way to approach $R(\\theta)$ is to\nminimize the following global empirical risk:\n$R_S(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}R_{S_i}(\\theta),$ (3)\n$R_{S_i}(\\theta) := \\frac{1}{n_i}\\sum_{j=1}^{n_i}l_\\rho (\\theta; z_{i,j}),$"}, {"title": "Stability and Generalization", "content": "where $R_{S_i}(\\theta)$ is the local empirical risk $R_{S_i}(\\theta) :=\\frac{1}{n_i}\\sum_{j=1}^{n_i} l_\\rho(\\theta; z_{i,j})$ with the local sample dataset $S_i$. To in-\nvestigate the generalization error in FAL, we focus on ana-\nlyzing its algorithmic stability, which evaluates how sensitive\na model is to changes in its training data. In the context of\nFAL, where the training dataset is distributed across various\nclients, it is crucial to assess how perturbations in the data at\neach client level affect the global model's stability.\nThe generalization adversarial risk $\\epsilon_{gen}(\\theta)$ is defined as\nthe difference between population and empirical risk, i.e.,\n$\\epsilon_{gen} := R(\\theta) - R_S(\\theta)$. For a potentially randomized algo-\nrithm A that takes a dataset S as input and outputs a random\nvector $\\theta = A(S)$, we can define its expected generalization\nadversarial risk over the randomness of a training set S and\nstochastic algorithm A as follows:\n$\\epsilon_{gen} (A) = E_{S,A}[R(A(S)) \u2013 R_S(A(S)].$ (4)\nOne of the most popular ways to approach (4) is to\nconsider the on-average stability (Kearns and Ron 1997;\nShalev-Shwartz et al. 2010; Kuzborskij and Lampert\n2018). Moreover, in our focused FAL, the generaliza-\ntion adversarial risk can be decomposed as $\\epsilon_{gen} (A) =E_{S,A} [\\frac{1}{m}\\sum_{i=1}^{m}(R(A(S)) \u2013 R_{S_i}(A(S)))]$, which implies\nthat the generalization ability of the server is related to each\nclient. Therefore, we are interested in the change in algorithm\nperformance when one data sample is perturbed in any client.\nWe introduce the following definitions related to on-average\nstability for the FAL framework, which is similar to those in\n(Sun, Niu, and Wei 2023; Lei, Sun, and Liu 2023).\nDefinition 1 (Neighboring Datasets). Given the entire\ndataset $S = \\cup_{i=1}^{m} S_i$, where $S_i$ is the local dataset of\nthe i-th client with $S_i = \\{z_{i,1},..., z_{i,n_i} \\}, \\forall i \\in [m]$, an-\nother dataset is called as neighboring to S for client $i'$,\ndenoted by $S^{(i')}$, if $S^{(i')} := \\cup_{i\\neq i'}S_i \\cup S_{i'}^{(j)}$, where $S_{i'}^{(j)} =$\n$\\{z_{i',1},..., z_{i',j-1}, z'_{i',j}, z_{i',j+1}, ..., z_{i',n_{i'}} \\}$ with $z'_{i',j} \\sim P_{i'},$\n$\\forall j \\in [n_{i'}]$. And we call $z'_{i',j}$ the perturbed sample in $S^{(i')}$.\nDefinition 2 (On-Average Stability for FAL). A random-\nized algorithm A is $\\epsilon$-on-average stability if given any two\nneighboring datasets S and $S^{(i')}$, then\n$\\underset{j\\epsilon[n_{i'}]}{max} |E_{A,S,z'_{i,j}}[l_\\rho(A(S); z'_{i',j}) - l_\\rho(A(S^{(i')}); z_{i',j})]| \\leq \\epsilon,$\nwhere $z'_{i',j}$ is the perturbed sample in $S^{(i')}, \\forall i \\in [m]$.\nOn-average stability basically means any perturbation of\nsamples across all clients cannot lead to a big change in the\nmodel trained by the algorithm in expectation. Moreover, we\nneed to state a global assumption and a key lemma to indicate\nthe data heterogeneity which is crucial for our later theorems.\nAssumption 1. (Lipschitz, continuity). The loss function\nl satisfies the following Lipschitz smoothness conditions:\n$||l(\\theta_1, z) - l(\\theta_2,z)|| \\leq L||\\theta_1 - \\theta_2||, ||\\nabla l(\\theta_1,z) -\\nabla l(\\theta_2,z)|| \\leq L_\\theta||\\theta_1 \u2013 \\theta_2||$, and $||\\nabla l(\\theta, z_1) \u2013 \\nabla l(\\theta, z_2) || \\leq L_z||z_1-z_2||_p$, where $\\nabla$ is the abbreviation for $\\nabla_\\theta$ used\nthroughout the paper.\nNote that Assumption 1 is widely used in existing studies\n(Xing, Song, and Cheng 2021a; Li, Song, and Yang 2023;\nZhu et al. 2023; Kanai et al. 2023). Intuitively, different\nlocal distributions affect the global population risk and hence\nmay affect the model generalization as well. To effectively\nmeasure the exacerbated heterogeneity of client i in AL, we\naccount for both their original data distribution $P_i$ and the\ndistribution of adversarially generated samples $P'_i$.\nLemma 1. Under Assumption 1 and given $i \\in [m]$, for any\n$\\theta$ we have\n$||\\nabla R_i(\\theta) \u2013 \\nabla R(\\theta)|| \\leq (2\\rho L_z + 6L)D_i,$\nwhere $D_i = max\\{drv(P_i, P_i), drv (P'_i, P), drv (P, P')\\}$.\nRemark 1. The total variation distance $d_{TV}$ is used to\ncompare these distributions against their respective global\ncounterparts, $P$ and $P'$. We define $D_i$ as the maximum total\nvariation observed among these comparisons. This metric\ncaptures the extent of data variation from both regular and\nadversarial perspectives, with higher values indicating greater\nheterogeneity. This lemma reveals that when AL tries to gain\nmore robustness through stronger adversarial generation, the\nheterogeneity will be exacerbated. When $\\rho = 0$, we have\n$drv (P_i, P'_i) = 0, drv(P, P') = 0$, thus $D_i$ is the same as\n(Sun, Niu, and Wei 2023) in FL. In particular, we define\n$D_{max} = \\underset{i \\in [m]}{max} D_i$ to denote the maximum heterogeneity\namong all clients in the FAL framework."}, {"title": "Vanilla FAL Algorithm", "content": "To approach the global objective in (3), (Zizzo et al. 2020;\nShah et al. 2021; Li et al. 2022; Zhang et al. 2022) designed\nthe Vanilla FAL (VFAL) algorithm. The main idea of VFAL\nis to leverage AL in FedAvg (McMahan et al. 2017) locally.\nEach client runs on a local copy of the global model $\\theta_t$ with\nits local data to conduct AL. Then, the server receives updated\nmodel parameters $\\{\\theta_i^K\\}_{i=1}^m$ for all clients and performs the\nfollowing aggregation: $\\theta_{t+1} = \\frac{1}{m}\\sum_{i=1}^m \\theta_i^K$, where K is\nthe epoch of local training. The parameters $\\theta_{t+1}$ for the global\nmodel are then sent back to each client for another epoch\nof training. Next, we use the on-average bound to derive\nthe generalization error in the VFAL algorithm through the\nfollowing theorem.\nTheorem 1. If a VFAL algorithm A is $\\epsilon$-on-averagely stable,\nwe can obtain the generalization error $\\epsilon_{gen} (A)$ as follows:\n$E_{S,A} [\\frac{1}{m}\\sum_{i=1}^m(R(A(S)) \u2013 R_{S_i}(A(S)))] \\leq \\epsilon.$\nMost existing studies analyze the generalization error of\nalgorithm stability under smooth loss functions (Sun, Niu,\nand Wei 2023; Huang et al. 2023; Lei, Sun, and Liu 2023),\nexploring the dependence of FL generalization properties\non heterogeneity. However, (Liu et al. 2020) suggest that\nthe adversarial loss $l_\\rho(\\theta, z)$ remains non-smooth, even if\nwe assume the standard loss $l(\\theta, z)$ is smooth. This non-\nsmoothness violates some basic properties in stability analy-\nsis (Hardt, Recht, and Singer 2016; Lei, Sun, and Liu 2023),\nbringing additional challenges."}, {"title": "Surrogate Smoothness Approximation", "content": "Using the surrogate smoothness helps improve the quality of\nthe gradient of the original function $l_\\rho$, potentially improving\ngeneralization (Xie et al. 2020). In particular, we reconsider\nthe following surrogate loss to substitute $l_\\rho$ in (3):\n$h(\\theta; z_{i,j}) = \\underset{|| z_{i, j}-\\hat{z}_{i j} ||_p \\leq \\rho}{max} {l(\\theta; \\hat{z}_{i,j})},$ (5)\nand each client performs $\\theta_i^{k+1} = \\theta_i^{k} - \\eta_t\\nabla h(\\theta_i^{k}, z_{i,j})$,\nwhere $k \\in [K]$ and $\\eta_t$ is the local stepsize at the global epoch\nt. Based on the surrogate loss h, we have a set of properties\nof SSA, which can be defined as follows:\nDefinition 3. Let $\\beta \\geq 0, \\xi > 0$ and $h(\\theta)$ be a differentiable\nfunction. We say $h(\\theta)$ is $\\xi$-approximately $\\beta$-gradient Lips-\nchitz, if $\\forall \\theta_1$ and $\\theta_2$, we have\n$||\\nabla h (\\theta_1) \u2013 \\nabla h (\\theta_2)|| \\leq \\beta ||\\theta_1 \u2013 \\theta_2|| + \\xi, \\xi = 2\\rho z.$\nFrom Definition 3, we can see that the SSA method dynam-\nically inherits the non-smooth properties by AL. If $\\rho = 0, h$\nis gradient Lipschitz; otherwise, $\\rho > 0, h$ is a general non-\nsmooth function. In particular, the maximization operation\nof the surrogate function h improves the continuity of the\ngradient and helps smooth out potentially sharp gradients.\nTheorem 2. Let the step size be chosen as $\\eta_t \\leq \\frac{1}{\\beta K (t+1)}$.\nUnder Assumption 1, the generalization bound $\\epsilon_{gen}$ with the\nSSA method satisfies:\n$O(\\frac{\\rho T \\log T}{\\eta_{\\min}} + \\frac{T \\sqrt{\\log T} \\Delta}{\\eta_{\\min}} + \\frac{T\\log T (\\rho + 1) D_{max}}{\\eta_{\\min}} ),$\nwhere $\\Delta = E[R(\\theta_0)] \u2013 E[R(\\theta^*)]$ and $\\eta_{\\min} = \\underset{i\\epsilon[m]}{min} N_i$.\nRemark 2. As shown in Theorem 2, the first term,\n$\\frac{\\rho T \\log T}{\\eta_{\\min}}$, represents the approximation error from SSA,\nwhich is affected by $\\rho$ of AL. We can see that a smaller $\\rho$\nhelps reduce the error in this dominant term. The second term\n$\\frac{T \\sqrt{\\log T} \\Delta}{\\eta_{\\min}}$ pertains to the algorithm's convergence perfor-\nmance. However, in practice, the third term, $\\frac{T\\log T (\\rho+1) D_{max}}{\\eta_{\\min}}$,\narises from the heterogeneity, which quantifies the variation\nbetween the original and adversarial data. This term reveals\nthe increased heterogeneity leads to poorer robust general-\nization ability. When $\\rho = 0$, the first term disappears, while\nthe third still reflects the original heterogeneity, similar to FL\n(Sun, Niu, and Wei 2023)."}, {"title": "Randomized Smoothness Approximation", "content": "In RSA, we consider a smoothed alternative version of $l_\\rho$ in\n(3) based on the randomized smoothing technique (Duchi,\nBartlett, and Wainwright 2012; Lin, Zheng, and Jordan\n2022). Specifically, let $\\gamma \\geq 0, l_\\gamma(\\theta, z_{i,j}) = E_{u\\sim P_d}[l_\\rho(\\theta +$\n$\\gamma u_{i,j}, z_{i,j})]$, where $\\gamma$ is a smoothing parameter, $P_d$ is an uni-\nform distribution on a unit ball in d-dimensional space with\nthe $L_2$-norm, and $u_{i,j} \\sim P_d$ is a random variable generated\nfrom $P_d$. By adding multiple instances of randomized noise\nto the parameters, we aim to steer the new objective function,\n$l_\\gamma$, towards a flatter weight loss landscape compared to the\noriginal objective $l_\\rho$, which can moderate the non-smooth\nproperty (Neyshabur et al. 2017; Kanai et al. 2023).\nIn practice, the gradient of $l_\\gamma(\\theta)$ is difficult to compute\ndue to the expectation. Therefore, we can estimate it using\nMarkov chain Monte-Carlo techniques (Vrugt et al. 2009) on\neach client i as follows:\n$\\theta_i^{k+1} = \\theta_i^{k} - \\frac{\\eta_t}{Q}\\sum_{q=1}^{Q}\\nabla l_\\rho(\\theta_i^{k} + \\gamma u_{i,k}^q, z_{i,k}).$ (6)\nNote that in each local epoch $k \\in [K]$, $u_{i,k}^q$ is sampled from\n[Pd locally, and we need to repeat the number of Q times\nsamples to reduce the estimated error between real gradient\nand expected gradient (Duchi, Bartlett, and Wainwright 2012;\nLin, Zheng, and Jordan 2022).\nTheorem 3. Let $c \\geq 0$ be a constant and the step size be\nchosen as $\\eta_t \\leq \\frac{c}{4K\\sqrt{d} c L(t+1)}$.\nUnder Assumption 1, the gen-\neralization bound $\\epsilon_{gen}$ with the RSA method satisfies:\n$O(\\frac{T^{1/4} \\log T}{\\sqrt{Q}} + \\frac{T^{3/4} \\sqrt{\\Delta}}{\\eta_{\\min}} + \\frac{T(( \\rho + 1) D_{max})^{1/3}}{\\eta_{\\min}} ),$\nwhere $\\Delta = E[R(\\theta_0)] \u2013 E[R(\\theta^*)]$ and $\\eta_{\\min} = \\underset{i\\epsilon[m]}{min} N_i$.\nRemark 3. Similar to Theorem 2, the generalization error\nbound of RSA is also composed of three terms. We can see\nthe first term, i.e., $\\frac{T^{1/4} \\log T}{\\sqrt{Q}}$, where $\\frac{1}{\\sqrt{Q}}$ represents the dis-\ncrepancy in the gradient estimate between real gradient and\nexpected gradient, is independent on $\\rho$. Note that this term is\nsmaller as Q increases, but a larger Q imposes an unbearable\ncomputational cost on clients. Hence, the adjustment of Q\nshould be appropriately in a small range. In addition, since\neach client optimizes a new smoothed objective function, $l_\\gamma$,\nthe second term $\\frac{T^{3/4} \\sqrt{\\Delta}}{\\eta_{\\min}}$ and the third term $\\frac{T(( \\rho + 1) D_{max})^{1/3}}{\\eta_{\\min}}$\nare generated based on a flatter weight space compared to the\noriginal. This is why they are independent of Q and the order\nof T is smaller."}, {"title": "Over-parameterized Smoothness Approximation", "content": "In OPSA (Lei, Jin, and Ying 2022), we focus on the following\nshallow neural network of the form in (1):\n$f_W(x) := \\sum_{\\tau=1}^{S}\\mu_\\tau \\varphi(\\langle w_\\tau ,x\\rangle),$ (7)\nwhere we fix $\\mu_\\tau \\in \\{-\\frac{1}{\\sqrt{s}},\\frac{1}{\\sqrt{s}}\\}$, $\\varphi : R \\rightarrow R$ is an activa-\ntion function and $W = (w_1,..., w_s) \\in R^{d\\times s}$ is the weight"}, {"title": "Slack FAL Algorithm", "content": "From the above results", "follows": "n$R_{S}(\\theta) = \\frac{1}{m}\\bigg[(1+\\alpha)\\sum_{i=1}^{m} R_{S_{(i)}}(\\theta)\n+(1-\\alpha)\\sum_{i=m+1}^{m} R_{S_{(i)}}(\\theta)\\bigg"}]}