{"title": "AI-Based Teat Shape and Skin Condition Prediction for Dairy Management", "authors": ["Yuexing Hao", "Tiancheng Yuan", "Yuting Yang", "Aarushi Gupta", "Matthias Wieland", "Ken Birman", "Parminder S. Basran"], "abstract": "Dairy owners spend significant effort to keep their animals healthy. There is good reason to hope that technologies such as computer vision and artificial intelligence (AI) could reduce these costs, yet obstacles arise when adapting advanced tools to farming environments. In this work, we adapt AI tools to dairy cow teat localization, teat shape, and teat skin condition classifications. We also curate a data collection and analysis methodology for a Machine Learning (ML) pipeline. The resulting teat shape prediction model achieves a mean Average Precision (mAP) of 0.783, and the teat skin condition model achieves a mean average precision of 0.828. Our work leverages existing ML vision models to facilitate the individualized identification of teat health and skin conditions, applying AI to the dairy management industry.", "sections": [{"title": "Introduction", "content": "Traditionally, dairy cow teat health assessment requires close examination by a trained professional. Although veterinarians routinely perform this task as part of dairy clinical practice, dairy workers in small farms find the task time-consuming, reducing the accessibility of a valuable predictive tool. On large farms, individualized teat health assessments are impractical: thousands of cows might be managed by a few dozen workers. Yet daily examination of cow teat health could help identify changes that might be early precursors of animal health issues. Our work focuses on dairy cow teat health assessment through the 1) automated and accurate teat shape assessment, and 2) creation and deployment of computer vision.\nThere has been limited research on machine-learning techniques for solving this problem even in rotary milking parlors with excellent lighting, good animal separation, and high-quality animal identification. One widely cited effort studied cow teat condition classification from a veterinary perspective (Mein et al. 2001), but focused on clinical settings and did not consider the use of machine learning models for identifying teat shape. Our project provides a more comprehensive machine learning solution for use in milking parlors. Here we report on data collection, preparation of training data sets labeled with domain-expert knowledge, development of fully-trained ML models, and assessment of its performance using data from commercial farms.\nA well-known concern about ML is that training models can be prohibitively expensive. Unusually, our approach avoids the need to undertake model training from the ground up. We evaluated a variety of preexisting open-source computer-vision models, identifying one model that had good baseline performance. We then performed fine-tuning of its model parameters and conducted additional training with our labeled data, obtaining a refined open-source model that can perform cow teat localization, teat shape, and skin condition classification with high accuracy and yet at low cost.\nAccordingly, this paper focuses on three questions:\n1. Can we obtain high quality still images (keyframes) from fixed video cameras in a rotary milking parlor?\n2. Given a choice of images for one cow, can we select the image that best visualizes the stall-id and the cow's teats?\n3. Can we accurately classify teat shape and skin condition?\nAnswering these questions will contribute to dairy science in several ways. In a practical sense, our work is a step towards routine monitoring of teat shape and teat skin condition in a medium-size dairy farm, enabling us to study the actual value of this sort of information. We hypothesize that deploying our ML models could improve dairy herd management, pinpoint issues that arise, and enable timely intervention to head off mastitis or prevent the spread of potentially contagious pathogens, but follow-up studies of deployed solutions will be needed to validate or refute this belief. Our approach additionally yields data suitable for inclusion into repositories that could be used to develop follow-on machine-intelligent solutions (such as for evaluating animal gait and to sense evidence of discomfort), even as we also use to further refine our models. We also hope to extract a variety of metrics for dairy productivity, which would be valuable when optimizing farm performance."}, {"title": "Scientific Background", "content": "Dairy cow teat condition is widely used as a predictor of animal health and anticipated milk quality (A J Seykora 1985; Wieland et al. 2018; Seykora and McDaniel 1985b). Poor or gradually degrading teat health is recognized as a risk of mastitis: one of the most important dairy diseases due to its harmful consequences for farm productivity (Ruegg 2003). Mastitis prevention strategies typically focus on two approaches: minimizing bacterial presence at the teat end and enhancing the cow's natural resistance to these pathogens (Hogeveen et al. 2011). Studies have shown that teat-end shape is correlated with a cow's resistance to developing mastitis (Lojda and Matouskova. 1976), somatic cell count and milkability (Seykora and McDaniel 1985a; Wieland, Nydam, and Virkler 2017).\nTo create a ground-truth data set for teat condition classification, our team works with veterinarians and veterinary assistants, who supervise certain milking sessions, manually scoring each cow's teats with respect to shape and skin condition. The scoring metrics used for teat shape assessment are based on Seykora and Daniel (A J Seykora 1985) guidelines, wherein teat shape is scored as [1: pointed, 3: flat, 7: round-flat, 8: round-ring]. For skin condition assessment, the veterinary team employed Neijenhuis (Mein et al. 2001) guidelines, scored as: [1: normal skin, 3: teat with open lesion].\nIn a clinical setting, visual teat analysis would be supplemented by tactile assessments. There are other condition scoring dimensions that could be performed, including evidence of hyperkeratosis (Hillerton 2005), presence of hock lesions (Kielland et al. 2009), quality of lower leg hygiene, quality of udder hygiene (Schreiner and Ruegg 2003; Cook and Reinemann 2007), and presence of skin-open lesions. All of these are important in clinical mastitis risk health assessment, and our future work will need to explore, although physical manipulation of the teats would not be practical in our setting, hence we would need to explore other traits that track the evolution of teat condition over time, such as redness/swelling and painful reaction to contact with the milking equipment."}, {"title": "Machine Learning for Dairy Health Management", "content": "Our effort contributes within the broader area of technology development for dairy farm automation and management. The area is active, and includes prior work that studied, evaluated, and deployed machine learning techniques for tasks that include overall farm management (nutrition, hydration, animal activity), herd reproduction management, and animal behavior analysis (Slob, Catal, and Kassahun 2021a; Cockburn 2020; MR, N K, and V 2022a). Many in the field are arguing that the future dairy farm could be reconceived as having a cyber counterpart (sometimes called a digital twin), in which the farm is modelled as a generator of many distinct data streams, each with its own purpose and data formats, and each used to train and then trigger a specialized task-specific model or database functionality (Gupta et al.).\nDairy cows must be identified when entering the rotary milking parlor so the milking data can be obtained from each cow and integrated with the existing dairy information management system. Currently, this is done using numbered ear tags, Radio Frequency Identification (RFID), and (as needed) human visual inspection. Our work does not currently explore options for augmenting these with computer vision tools, but such a step is certainly a possibility for future investigation.\nGiven an identified animal, two data types can be used as inputs to a machine-learning pipeline. One category consists of numerical (tabular) data. Numerical metrics can be captured using sensors, laboratory reports, and milk quantity measurements. The resulting data set can then be used to train models for assessing health metrics, such as heat stress (Gorczyca and Gebremedhin 2020), estrus (Fauvel et al. 2019), mastitis (Fadul-Pacheco, Delgado, and Cabrera 2021)"}, {"title": "Data Collection", "content": "We collected video datasets from an Upstate New York dairy farm on October 9th, 2023. The video streams were captured using dual GoPro cameras positioned at lower and parallel angles relative to the cow teats. The veterinarian (a milk quality and udder health specialist with 17 years of experience as bovine veterinarian, certifications: Dip. ECBHM, PhD, DVM) scored the teat shape and skin condition manually, following the Seykora and Daniel (Seykora and McDaniel 1985a; Mein et al. 2001) guidelines.\nAlthough a GoPro captures video, the video data stream itself consists of a series of still images called keyframes separated by zero or more delta frames. For our work, we limited consideration to the key frames. We disable GoPro data compression and automated image touchup: any image transformation could conceal a teat condition issue much as makeup and digital transformations can conceal skin defects or artificially manipulate an actor's appearance in a movie.\nAs shown in Figure 1, the milking parlor consists of a series of stalls that move slowly in a circle. The cow enters for premilking teat preparation, is milked, then released back into the dairy herd. Our cameras are fixed in place and continuously record video of the cows' teats and udders as the parlor rotates past. This yields multiple images of each animal after milking, but while still in the rotary parlor (Green stalls, Figure 2). Our camera position allowed for an automated response to the analysis, provided the assessment is completed within one to two seconds."}, {"title": "Data Processing", "content": "Traditionally, computer vision training starts with the acquisition and annotation of comprehensive image datasets that often have hundreds of thousands of examples. In contrast, our work adopts a preexisting computer vision model trained on very general data but then additionally trains it for the daily task. Thus, our focus is on aspects specific to dairy teat health assessment. We start by selecting high-quality keyframe images from the data set collected from the farm. This selection process discards images where teats are difficult to distinguish, with blurring or poor lighting and motion effects. For training purposes, our veterinary experts considered only the selected data, annotating a portion that we used to refine the vision model's ability to detect the teats, classify teat end shape, and assess teat skin condition.\nData preparation is carried out using a package called LabelMe. LabelMe output takes the form of JSON files containing annotation details for each image in a dataset (Russell et al. 2008). To conform to the standard COCO (Common Objects in Context) object detection dataset format (Lin et al. 2014), a format favored in many deep learning frameworks, we then implement a custom aggregation process that consolidates these annotation files into cohesive datasets. Data consolidation involves the development of a tailored script to systematically collate annotation data from the individual JSON files generated by LabelMe. The resulting"}, {"title": "Automated Keyframe Selection", "content": "The first step is to create an ML specialized in evaluating image quality within a stream of keyframes. There are two subtasks: (1) identification of images that include the cow's stall ID; (2) selection of 2-3 high-quality teat images. These both occur on the same video segment, which shows an individual cow for approximately 3 seconds each.\nData selection proves to be surprisingly challenging. As an example, consider the identification of the stall ID. Even if an image contains an ID tag, it could be out of focus, the tag may be obstructed, or the frame may capture half of it as the parlor rotates. For example, Figure 3a is a frame in which the stall ID tag is blocked by the milking parlor device. Accordingly, the algorithm uses two criteria for the frame selection (1) high confidence from the Optical Character Recognition (OCR) model; (2) if the location of the tag is not on the left or right edge in the frame, which is likely to truncate out part of the number. Figure 3b shows a frame in which the stall id is easily visible. The OCR model we use to identify the numbers in a frame has an accuracy of 99%. We fine-tune a FasterRCNN model to identify and segment the sub-keyframe. The model achieves an accuracy of 99% on a given frame. An example of a stall ID is seen in 3b. Pseudo-code for the data extraction task appears in Algorithm 1.\nHaving selected an image, we organize data about a given cow using a single file system folder per animal, per milking session. To this end, we write a Python program that automatically extracts keyframes, determines the stall ID, creates a suitable folder, and then stores the associated keyframes in that folder. The program obtains frame-by-frame access using the OpenCV package. To perform teat localization,, we trained an ML model which entails identifying each of the cow's teats and segmenting them using bounding boxes. Similar to the process used for stall ID identification, line 8 uses the function Loc(teat_segments) to check two things: (1) whether all teats were identified with high confidence; (2) whether the teats are centered in the frame. The first one uses the ML model confidence score to check not only if there are teats in the frame, but also that they were properly segmented. The second one uses the 2D coordinate of the ML result, to ensure that all the teats are captured in the frames, avoiding the frames where just a subset of the teats were captured. If a frame satisfies both criteria, it is saved to the folder under its stall ID folder, on line 15."}, {"title": "Experimental Evaluation", "content": "For teat health assessment purposes, we consider a set of candidate object detection models. We select Faster-RCNN"}, {"title": "Model Settings", "content": "(Ren et al. 2015) model as a baseline. The foundational vision models in this project utilize either convolutional layers or multi-head attention blocks, and sometimes both. These models are benchmarked in our dataset with different scales to study the trade-off between better model system metrics (run time, memory consumption) and better model performance metrics (validation accuracy and bounding boxes mean average precision for small objects). We include both two- and single-stage models and will discuss this in the following section. In the experiment described below, we use mean average precision (mAP) as the performance metric, more specifically, mAP for small objects. We defer the detailed discussion of the metric in later sections."}, {"title": "Fine-tuning the Candidate Models", "content": "Our overall approach is as follows. First, we undertake an offline process to fine-tune each of the candidate computer vision models using an inexpensive training process that refines the standard model parameters to optimize performance for data collected in our milking parlor. Next, we expose each tuned model to production data. The human-expert ground truth labels are used to assess the performance of our automated scoring solutions.\nOur work requires models for teat shape identification and teat skin condition classification. We run both tasks on each sub-image (each distinct teat). We consider both two-stage models and single-stage models. Faster-RCNN (Ren et al. 2015) is a two-stage detector, which relies on a Regional Proposal Networks (RPN) to propose many potential regions of interest (RoI) and then applies a classifier backbone. YOLO-F (Chen et al. 2021), a modified version of YOLO, is a single-stage detector. We then consider the State-Of-The-Art (SOTA) models often observed to have end-to-end transformer architecture. DINO (Zhang et al. 2022), a modified version DETR (Carion et al. 2020), uses a transformer architecture.\nOur review of prior research on automated teat condition"}, {"title": "Experimental Results", "content": "All experiments are carried out on an NVIDIA RTX 4090 GPU. We use mAP as our performance metric. For COCO datasets, mAP is calculated for Intersection over Union(IoU) values. The IoU is derived by the area of overlap divided by the area of the union in between the ground truth bounding box and the predicted bounding box. Our dataset consists of only small-scale objects whose areas are often smaller than 32 x 32 pixels. So, during training, we focus on the mAPs.\nFor the teat shape identification task, we adopt the aforementioned scoring system and assign one of four class labels [1,3,7,8] from worst to best teat shape conditions. For the skin condition detection, we consider a total of 2 class labels, [C1, C3], with class C3 indicating the existence of skin lesions (Mein et al. 2001), and C1 indicating the healthy skin condition.\nFor the model configurations, we use a standard ResNet-50 as the classifier backbone for all three models, while the model scales are rather different. For Faster-RCNN, if we use a batch of 100 images with an input shape of 2704 \u00d7 1520 \u00d7 3, the model consists of 41.364 million parameters, and it requires 0.208 TFLOPs (tera floating-point operations per second). For YOLO-F, using the same input shape, the model consists of 42.409 million parameters and requires 98.808 GFLOPs (giga floating point operations per second) to execute. For DINO, we have a model with 47.546 million parameters and requires 0.274 TFLOPs."}, {"title": "Discussion", "content": "The automatic data processing pipeline described in Section transfers the camera-captured video to a keyframe for storage as part of an animal health record and training dataset. One of the reasons for using keyframes instead of raw video is memory efficiency. While raw video contains a lot of information, much of that information is irrelevant to the research, and the video camera continues to run even when there is no cow in the milking parlor. Moreover, there are circumstances such as the one shown in Figure 3c where the image shows crossed teats, or where one teat obscures another, and hence little can be determined about the condition of the hidden teats. From a different angle, that same teat might have been clearly visible.\nA keyframe is much smaller then a full video clip, and the segmented portion of the frame containing the cow teats even more so. From our collected data, the measured average size per image frame that contains full image with four teats is 800KB on disk. The average size for a segmented teat image is 10KB on the hard disk. In comparison, for a clip of 10-minute raw video that takes 4GB on disk, the distilled keyframe folder is only 139.5MB, whereas removable intermediate images occupy 581MB. The intermediate images contain the keyframes for stall ID and teat candidate images, from which we choose the one where the teats are centered and clear as the record to store. The memory required to store the raw video file would be almost 28 times more than is required to store the keyframe."}, {"title": "Efficient Data Storage", "content": "Machine Intelligence for Dairy Farms\nML models can significantly enhance dairy farm health management by operating more efficiently and effectively, capturing nuances that expert veterinarians might miss during long working hours or in an intense farming environment. These roles often involve repetitive teat health scoring tasks. Our duo-camera models can operate 24/7, collecting time series data of teat keyframes. This machine intelligence can provide veterinarians with valuable evidence to support their evaluations and judgments. Furthermore, this technology can be scaled and adapted to other agricultural fields."}, {"title": "Machine Intelligence for Dairy Farms", "content": "We discuss our positive Life Cycle for iteratively improving our model's performance with the improved quality and quantity of data we collected. We consider a multi-phase setup, where the deliverable for each stage would be deployed to help with further improvement that happens during the next stage. In particular, we started with a low-data paradigm, where we have quite a limited amount of data, but with high-quality annotation. We train a model based on this preliminary dataset. With this model deployed, we were able to automate the process of data collection and remove the unnecessary storage overhead of most video files, and only obtain keyframes. Our animal scientist would move on to annotate the high-quality raw keyframes. While we are expanding our dataset, we will be expecting our dataset to incorporate the quantity and quality requirements for developing the ML models. Additionally, we also argue that with the amount of data we are aggregating, we will be able to automatically eliminate the long-tail distribution of classes that currently exists in our dataset."}, {"title": "Limitations & Future Work", "content": "This paper focuses solely on teat shape identification and skin condition score predictions. However, in future studies, we aim to incorporate additional criteria for teat evaluations, such as predicting teat-end hyperkeratosis scores or assessing udder health in multidimensional teat health analysis. By expanding the scope of teat evaluations, we can achieve a more comprehensive analysis of teat health.\nMoreover, there is a need for more balanced datasets in AI-based duo-dimensional teat health analysis, particularly due to the scarcity of labels for rare cases. For instance, in our current skin condition dataset, our ratio between normal C1 labels and abnormal C3 labels is 925:44. The unbalanced dataset limits the model to learn from the abnormal situations and impacts model performance. Through large-scale, long-term data collection efforts, we anticipate that our models will demonstrate improved performance in identifying and analyzing these less common labels. In the future work, we plan to collect data from additional farms to ensure more balanced datasets.\nWe could further investigate additional data augmentation techniques, such as large-scale jittering (LSJ), to enhance image resolutions, camera angles, and lighting conditions, ultimately improving the overall performance of our models. Given that our current datasets were collected under favorable lighting conditions, future large-scale data collection efforts will involve capturing keyframes from diverse environments and implementing methods to enhance image quality.\nOur project relies on ground truth labels derived from veterinary expertise. However, teat condition is subjective, hence any single professional could err when scoring, creating a puzzle: if our model is incorrect, did it learn from incorrect training data, or was it confused by poor lighting, animal skin pigmentation, or some other factor? In situations where ground truth eventually becomes available, techniques such as a confusion matrix (gradient ascent) can offer insights into when and why automation classification errors arise. This suggests that one could eventually create systems that might dynamically improve their performance, effectively learning from experience."}, {"title": "Conclusion", "content": "We explore teat localization and shape classification using ML models using a preliminary dataset of 348 images with 968 objects from 4 distinct classes. For teat skin conditions, we generate 946 labels to train ML models for teat health analysis. In this paper, we explore different object detectors across various architectures and found that DINO performs best overall. Our automated digital-twin approach has been shown to yield accurate classifications. Although our experiments are performed on a size-limited initial dataset, we plan to aggregate a dataset that incorporates both the quantity and quality requirements for developing ML models in the future."}]}