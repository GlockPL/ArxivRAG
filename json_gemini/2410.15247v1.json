{"title": "TENSOR-FUSED MULTI-VIEW GRAPH CONTRASTIVE LEARNING", "authors": ["Yujia Wu", "Junyi Mo", "Elynn Chen", "Yuzhou Chen"], "abstract": "Graph contrastive learning (GCL) has emerged as a promising approach to enhance graph neural networks' (GNNs) ability to learn rich representations from unlabeled graph-structured data. However, current GCL models face challenges with computational demands and limited feature utilization, often relying only on basic graph properties like node degrees and edge attributes. This constrains their capacity to fully capture the complex topological characteristics of real-world phenomena represented by graphs. To address these limitations, we propose Tensor-Fused Multi-View Graph Contrastive Learning (TensorMV-GCL), a novel framework that integrates extended persistent homology (EPH) with GCL representations and facilitates multi-scale feature extraction. Our approach uniquely employs tensor aggregation and compression to fuse information from graph and topological features obtained from multiple augmented views of the same graph. By incorporating tensor concatenation and contraction modules, we reduce computational overhead by separating feature tensor aggregation and transformation. Furthermore, we enhance the quality of learned topological features and model robustness through noise injected EPH. Experiments on molecular, bioinformatic, and social network datasets demonstrate TensorMV-GCL's superiority, outperforming 15 state-of-the-art methods in graph classification tasks across 9 out of 11 benchmarks, while achieving comparable results on the remaining two. The code for this paper is publicly available at https://github.com/CS-SAIL/Tensor-MV-GCL.git.", "sections": [{"title": "1 Introduction", "content": "In recent years, graph neural networks (GNNs) have become indispensable tools for learning from graph-structured data prevalent in various domains such as social networks (Davies and Ajmeri, 2022; Fan et al., 2019), molecular biology (Xia et al., 2023; Sun et al., 2021; Wang et al., 2022), and recommendation systems (Di Rocco et al., 2021; Yang et al., 2023). GNNs extend traditional neural networks by leveraging the relationships between nodes in a graph, using neighborhood aggregation to capture both node features and local graph structures. This method has proven effective in tasks such as node classification (Kipf and Welling, 2016; Veli\u010dkovi\u0107 et al., 2017), link prediction (Wang and Vinel, 2021; Ai et al., 2022; Chen et al., 2022), and graph classification (Wen et al., 2024; Ying et al., 2018). However, GNNs often rely heavily on task-specific labels, which can be scarce or expensive to obtain in many real-world applications. This dependence on labeled data limits the generalization ability of GNNs, especially when applied to noisy or sparse graphs. To mitigate this issue, graph contrastive learning (GCL) has emerged as a promising solution in the field of unsupervised and semi-supervised learning (You et al., 2020a; Kefato and Girdzijauskas, 2021). GCL enables models to learn meaningful representations by contrasting different augmented views of the same graph, thus reducing the need for extensive labeled data. By applying augmentation techniques such as node dropping, edge perturbation, and subgraph sampling, GCL encourages the model to capture invariant structural features, improving robustness and expressiveness in the face of data sparsity or noise.\nDespite the progress made by GCL, existing frameworks continue to face several challenges. A key limitation is their inability to capture higher-order structural relationships and latent topological information embedded in graphs. Most GCL models focus on local node-level or edge-level augmentations, which often overlook the global topological properties necessary for a complete understanding of graph data. This limitation is particularly evident in tasks where capturing complex, multi-scale topological features is crucial, such as molecular interactions or long-range dependencies in network systems. To overcome these issues, we propose the use of extended persistent homology (EPH), an advanced technique built on the foundations of persistent homology (PH) from topological data analysis (TDA). While PH has been studied by many articles (Horn et al., 2021; Wasserman, 2018; Carlsson and Gabrielsson, 2020), it is often limited to single filtrations, reducing its ability to capture multi-dimensional structures in complex graphs. EPH introduces multi-dimensional filtrations, offering a richer, more flexible representation of both local and global topological information, making it more effective for understanding complex topological structures. Several applications of EPH have shown promising results in tasks, including link prediction, node classification, and graph classification (Zhao et al., 2020; Yan et al., 2021; Carri\u00e8re et al., 2020). Among them, TopoGCL (Chen et al., 2024e) is the first one to integrate EPH with GCL to capture important topological and geometric features and enhance representation learning. While TopoGCL shows notable promise in representation learning, it fails to (1) preserve the low-rank structure of representation tensors during learning, leading to inefficiencies and higher computational costs as the graph complexity increases, and (2) mitigate the additional noise introduced by EPH, which arises from its broader and more complex filtration process, potentially capturing irrelevant topological features and amplifying noise.\nIn this paper, we address these issues by introducing a novel framework called Tensor-Fused Multi-View Graph Contrastive Learning (TensorMV-GCL), which innovatively combines tensor learning with graph contrastive learning. Specifically, our framework comprises multiple channels for information aggregation: one for structural information and another for topological features. In the first channel, augmented graph views are processed through a shared-weight graph convolutional network (GCN) to learn structural representations. In the second channel, we aggregate multiple extended persistent images (EPIs) in tensor form to capture multi-modal topological features. These EPI tensors are injected with noise to counteract the noise introduced by the EPH procedure before being passed through a convolutional neural network (CNN) to further extract tensor representations. Both channels incorporate a tensor concatenation layer for information aggregation and a tensor contraction layer for information compression, reducing redundancy and computational complexity. Finally, all channels are integrated using a contrastive loss that fuses the structural and topological information, enabling our model to learn robust, comprehensive graph representations. By employing these techniques, we achieve more expressive and robust graph representations that better capture the underlying structural and topological properties of graphs. Extensive experiments validate the robustness and generalization capabilities of TensorMV-GCL, positioning it as a strong contender among state-of-the-art methods in graph classification tasks.\nIn brief, our key contributions are as follows:\n\u2022 TensorMV-GCL pioneers the integration of tensor learning with graph contrastive learning. Graph features from multiple views are aggregated in tensor forms, distinguishing different modes. Tensor contraction layers adaptively extract core features, reducing information redundancy and computational complexity. The synergy between tensor concatenation and contraction layers effectively extracts comprehensive graph representations from multiple views.\n\u2022 We introduce a simple yet effective method to mitigate the noise introduced by the extended persistent homology's broad filtration process, thereby stabilizing topological representations and enhancing robustness.\n\u2022 Our extensive experimental results demonstrate TensorMV-GCL's superior performance in graph classification tasks. It outperforms 15 state-of-the-art methods on 9 out of 11 datasets and achieves comparable performance on the remaining two."}, {"title": "2 Related Work", "content": "Graph Neural Networks. Graph Neural Networks (GNNs) are widely used for learning from graph-structured data by aggregating information from node neighborhoods, which enables effective representation learning for various tasks such as node classification (Veli\u010dkovi\u0107 et al., 2017; Zhang et al., 2019), link prediction (Kipf and Welling, 2016; Wang and Vinel, 2021), and graph classification (Nguyen et al., 2022; Knyazev et al., 2019). A notable GNN variant is the Graph Convolutional Network (GCN), introduced by Kipf and Welling (2016), which leverages convolution operations on graphs. GCNs propagate node information through graph edges, aggregating features from neighboring nodes to refine the representation of each node. This method effectively generalizes the convolutional operation from traditional grid-based data (such as images) to non-Euclidean graph data. However, GCNs, and GNNs more generally, tend to focus on simple node and edge features, limiting their ability to capture more complex topological structures in graphs. To overcome this limitation, recent approaches have explored integrating richer topological and structural information to enhance the expressive power of GNNs, thereby enabling models to better capture the intricate dependencies found in real-world graph data. Approaches like Graph Attention Networks (Veli\u010dkovi\u0107 et al., 2018) and Graph Isomorphism Networks (Xu et al., 2019)are examples of how GNNs continue to evolve to better capture these complexities, although challenges remain in balancing computational efficiency and capturing both local and global graph structures.\nGraph Contrastive Learning. Graph Contrastive Learning (GCL) is a recent and powerful paradigm for self-supervised learning on graph-structured data. Self-supervised learning allows models to extract informative embeddings from vast amounts of unlabeled data by defining surrogate tasks that generate supervision signals from the data itself. In contrastive learning, the model aims to distinguish between similar and dissimilar data points, typically by maximizing the similarity between different augmented views of the same input (positive pairs) while minimizing the similarity to negative samples. Existing graph contrastive learning frameworks employ various contrasting modes, including global-global, global-local, and local-local contrastive learning. DGI (Velickovic et al., 2019) and InfoGraph (Sun et al., 2019b) both adopt global-local contrastive approaches, where global graph representations are contrasted with local node-level embeddings to maximize mutual information. Similarly, GraphCL (You et al., 2020a) and MolCLR (Wang et al., 2022) use a global-global contrastive approach by contrasting two augmented views of the entire graph to enhance graph representations. Another mode, i.e. local-local approach (Jiao et al., 2020; Zhu et al., 2021) is also applied by many models. For instance, SubG-Con (Jiao et al., 2020) proposes contrasting sampled subgraphs to capture localized patterns, which is effective on node-level tasks. Although GCL has shown remarkable success, existing frameworks often focus primarily on structural graph properties and may overlook the rich topological features that are crucial in many real-world graphs. Addressing this gap, methods like our proposed TensorMV-GCL aim to integrate both structural and topological information, allowing for multi-scale feature extraction and ultimately leading to more robust and comprehensive graph representations.\nTensor-based Neural Networks. Tensors, which represent multidimensional data, are widely used across fields like neuroimaging (Zhou et al., 2013), economics (Chen et al., 2020; Liu and Chen, 2022), international trade (Chen and Chen, 2022), recommendation systems (Bi et al., 2018), and biomedical applications (Chen et al., 2024d,a). Core areas of study include tensor decomposition (Zhang and Xia, 2018; Chen et al., 2024a), tensor regression (Zhou et al., 2013; Li and Zhang, 2017; Xia et al., 2022; Chen et al., 2024b), and tensor clustering (Sun and Li, 2019; Mai et al., 2021; Luo and Zhang, 2022). These methods have been effective in classification tasks, leading to techniques like support tensor machines (Hao et al., 2013; Guo et al., 2016), tensor discriminant analysis (Chen et al., 2024c), tensor logistic regression (Wimalawarne et al., 2016), and tensor neural networks (Kossaifi et al., 2020a; Wen et al., 2024).\nWhile neural networks with tensor inputs efficiently process high-dimensional data, most approaches use tensors for computation rather than statistical analysis. Cohen et al. (2016) showed that deep networks can be understood through hierarchical tensor factorizations. Tensor contraction layers (Kossaifi et al., 2017) and regression layers (Kossaifi et al., 2020b) further regularize models, reducing parameters while maintaining accuracy. Recent advancements like the Graph Tensor Network (Xu et al., 2023) and Tensor-view Topological Graph Neural Network (Wen et al., 2024) offer new frameworks for deep learning on large, multi-dimensional data.\nHowever, there remains a lack of in-depth research on tensor-input neural networks with contrastive learning. To our best knowledge, TensorMV-GCL is the first approach to bridge this knowledge gap.\nExtended Persistence Homology. Extended Persistence Homology (EPH) extends the classical persistence homology framework by considering not only the birth and death of topological features but also their evolution through different filtration levels, offering a more comprehensive characterization of data topology. The pioneering work by Cohen-Steiner et al. (2009) laid the foundation for this approach, demonstrating its utility in capturing essential features that persist across various scales. Subsequent research, such as Dey et al. (2014), further refined these concepts by introducing algorithms for computing extended persistence diagrams, which visualize the birth, death, and extension of features. Applications of EPH have shown promising results in machine learning, where it helps in understanding the intrinsic geometry of high-dimensional data (Zhao et al., 2020; Yan et al., 2021). Despite its advantages, the integration of EPH with other machine learning techniques, such as GNNs and GCL, is still in its early stages, presenting an exciting avenue for future research. Our work aims to address these gaps by developing novel frameworks that leverage EPH to enhance the topological feature extraction and representation learning capabilities in graph-based models."}, {"title": "3 Preliminaries", "content": "Problem Setting. We consider an attributed graph G = (V, E, X), where V is the set of nodes with |V| = N, E is the set of edges, and X \u2208 RN\u00d7F is the node feature matrix where F represents the dimensionality of node features. Let A \u2208 RN\u00d7N be the symmetric adjacency matrix, where each entry aij equals wij if nodes i and j are connected, and is 0 otherwise. Then, wij is the edge weight with Wij = 1 for unweighted graphs. Additionally, D denotes the degree matrix of A, defined as dii = \u2211jaij.\nExtended Persistent Homology. Persistent homology (PH) is a computational method in algebraic topology used to analyze the multi-scale topological features of data by tracking their evolution across different scales. Specifically, PH captures changes in topological properties, such as connected components, loops, and higher-dimensional holes, as a filtration parameter changes, allowing it to detect features that persist over a range of scales, which are indicative of significant structure in the data (Edelsbrunner et al., 2002; Zomorodian and Carlsson, 2004). By using a multi-resolution approach, PH addresses the inherent limitations of classical homology and enables the extraction of latent shape properties of G. The main approach involves selecting appropriate scale parameters y and analyzing how the shape of G evolves as G changes with respect to \u03b3. Instead of studying G as a single object, we study a filtration GY... Gyn = G where Y1 <\u00b7\u00b7 <Yn. We can then record the birth b\u2081 and death di of each topological feature, representing the scale at which each feature first and last appears in the sublevel filtration. However, PH has limitations in retaining information about features that persist indefinitely, which results in the loss of crucial latent topological information for certain applications. To address these limitations, an alternative approach is to supplement the sublevel filtration with its superlevel counterpart, which is called extended persistent homology (EPH) (Chen et al., 2024e; Cohen-Steiner et al., 2009; Dey et al., 2014). Opposite to the sublevel filtration, we record the scale at which each feature first and last appears in the superlevel filtration G1 G2 \u2283\nGyn. Hence, we can simultaneously assess topological features that persist and reappear in both directions, thereby providing a more comprehensive representation of the underlying topological properties of the data. Then, the extended topological information can be summarized as a multiset in R, called the Extended Persistence Diagram (EPD), which can be denoted as EPD = {(bp, dp) \u2208 R2}.\nTensor Concatenation and Contraction. The tensor structures' benefits stem from their reduced dimensionality in each mode \u2013 since vectorizing tensors multiplicatively increases dimensionality \u2013 and their potential statistical and computational advantages when their inherent multi-linear operations are leveraged (Kolda and Bader, 2009).\nIn this paper, tensor concatenation refers to the process of combining multiple tensors along a new mode, while tensor contraction denotes the extraction of core tensor factors from a tensor decomposition. We examine three common tensor decompositions, defined as follows: consider an M-th order tensor X of dimension D\u2081 \u00d7\uff65\uff65\uff65 \u00d7 DM. If X assumes a (canonical) rank-R CP low-rank structure, then it can be expressed as\n$$X = \\sum_{r=1}^{R} c_r u_{1r} \\circ u_{2r} \\circ ... \\circ u_{Mr},$$\nwhere o denotes the outer product, umr \u2208 RDm and ||umr||2 = 1 for all mode m \u2208 [M] and latent dimension r \u2208 [R]. Concatenating all R vectors corresponding to a mode m, we have Um=[Um1,\u2026\u2026, UmR] \u2208 RDm\u00d7R which is referred to as the loading matrix for mode m \u2208 [M].\nIf X assumes a rank-(R\u2081,\u00b7\u00b7\u00b7, R\u043c) Tucker low-rank structure, then it writes\n$$X = C \\times_1 U_1 \\times_2 ... \\times_M U_M = \\sum_{r_1=1}^{R_1} ... \\sum_{r_M=1}^{R_M} C_{r_1...r_M} (u_{1r_1} \\circ ... \\circ u_{Mr_M}),$$\nwhere umrm are all Dm-dimensional vectors, and Cr\u2081rm are elements in the R\u2081 \u00d7 \uff65\uff65\uff65 \u00d7 Rp-dimensional core tensor C.\nTensor Train (TT) low-rank approximates a D\u2081 \u00d7 \uff65\uff65\uff65 \u00d7 DM tensor X with a chain of products of third order core tensors Ci, i \u2208 [M], of dimension Ri\u22121 \u00d7 Di \u00d7 Ri. Specifically, each element of tensor X can be written as\n$$X_{i_1,...,i_M} = C_{1,1,i_1,:} \\times C_{2,:,i_2,:} \\times ... \\times C_{M,:,i_M,:} \\times C_{M+1,:,1,1},$$\nwhere cm,:,im,: is an Rm\u22121 \u00d7 Rm matrix for m \u2208 [M] \u222a {M + 1}. The product of those matrices is a matrix of size R0 \u00d7 RM+1. Letting R0 = 1, the first core tensor C\u2081 is of dimension 1 \u00d7 D1 \u00d7 R1, which is actually a matrix and whose 11-th slice of the middle dimension (i.e., c1,1,11,:) is actually a R\u2081 vector. To deal with the \"boundary condition\" at the end, we augmented the chain with an additional tensor CM+1 with DM+1 = 1 and RM+1 = 1 of dimension RM x 1 x 1. So the last tensor can be treated as a vector of dimension RM."}, {"title": "4 Methodology", "content": "This section introduces our Tensor-Fused Multi-View Graph Contrastive Learning framework, TensorMV-GCL, which is depicted in Figure 1. As shown, our method comprises two main components. The first is Tensor-view Graph Contrastive Learning, represented by the inner two channels. This component aims to align two global representations derived from the same graph but from two different augmented views. The second component is Stabilized Extended Persistent Images Contrastive Learning, represented by the outer two channels. This component aligns two Extended Persistent Images (EPIs) extracted from two augmented views of a single graph. We introduce noise to these EPIs to enhance the stability and robustness of their representations."}, {"title": "4.1 Tensor-view Graph Contrastive Learning", "content": "Our first contrastive learning module is Tensor-view Graph Contrastive Learning, designed to leverage both structural and feature representations of graphs through a contrastive learning framework. Given a set of graphs {G1, G2, ..., GN}, we define each graph G\u2081 by its node set Vi, edge set E\u00bf, and node feature matrix X\u00bf. We have G\u2081 = (Vi, Ei, Xi).\nIn order to capture multiple perspectives of each graph, we apply graph data augmentations, resulting in two augmented versions of each graph: G\u2081 = Ti(G) and G\u2081 = T;(G). The augmentations applied are carefully designed to preserve the core structure and essential properties of the graph, while introducing variations that simulate different real-world conditions or incomplete data. This helps the model learn representations that are invariant to such changes, leading to more robust performance in downstream tasks. After the graph data augmentations, we can construct the corresponding symmetric adjacency matrix and the degree matrix G\u2081 with A\u017c and Di; Gj with Aj and Dj.\nWe used a shared-weight Graph Convolutional Network (GCN) to encode these two augmented graphs. Specifically, the graph convolution operation involves multiplying the input of each layer by the T-th power of the normalized adjacency matrix. This 7-th power encodes information from the 7-th step of a random walk on the graph, allowing nodes to indirectly gather information from more distant nodes within the graph. Combined with an extra MLP layer, the representation learned at the l-th layer is\n$$C_{G_i}^{(l+1)} = f_{MLP} (\\sigma (\\tilde{A}_i C_{G_i}^{(l)} W^{(l)})),$$\nwhere \u00c2\u2081 = DAD, A = A\u2081 + I, and \u010e is the degree matrix of \u00c3\u00bf, C(0) = X\u2081. \u03c3(\u00b7) is a non-linear activation function. W(e) is a trainable weight of l-th layer. fMLP is a MLP layer with batch normalization.\nWe concatenate all layers of L-layer graph convolutions [C), C),..., C)] to form a node embedding tensor XG of dimension N \u00d7 L \u00d7 D \u00d7 D. Then, X, is fed into a Tensor Transformation Layer (TTL) as H) = Xg. The dimension of H) is also N \u00d7 L \u00d7 D \u00d7 D. The output of TTL is X. Let X denote the flattened version of X', where X \u2208 RN\u00d7(L\u00b7D\u00b7D). At the end of graph representation extraction, we have two embeddings corresponding to two different augmented graphs: X\u0123\u2081 and XG\nThe core of the contrastive learning framework is to encourage the representations of two augmented views of the same graph to be similar while pushing apart representations of different graphs. To achieve this, we define the contrastive loss as:\n$$l_{i,G} = -log \\frac{exp (sim(X_{G_i}, X_{G_j})/\\zeta)}{\\sum_{n=1,n\\neq i,j}^{2N}exp(sim(X_{G_i}, X_{G_n})/\\zeta)},$$\nwhere sim(X, X\u2081\u2084) = (XG\u00b8)\u00afXG; /\\X6\u2081 || ||X\u00fd, || represent the cosine similarity between two graph representations, and X is the flattened decomposed node embedding tensor representation of Gn. The temperature parameter ( smooths the distribution of similarities, helping balance between strong and weak similarities."}, {"title": "4.2 Stabilized Extend Persistent Images Contrastive Learning", "content": "Our second contrastive learning module is the Stabilized Extended Persistent Images Contrastive Learning. To capture the underlying topological information of each graph, we use K vertex filtration functions, defined as fi : V\u2194 R for i\u2208 {1, ..., K}. Each filtration function fi reveals a specific topological structure of the graph at different levels of connectivity. For the two augmented graphs Gi and Gj, we construct two sets of M EPIs of resolution P \u00d7 P, with each EPI corresponding to a filtration function, denoted as M\u2081 and Mj. With these two sets of EPIs, we can combine them respectively to construct the topological representations of G\u2081 and Gj, which denote as Zi and Z; with dimension of K \u00d7 MXP \u00d7 P. To enhance the robustness of the model, we add Gaussian noise to Zi and Z; before further processing. Specifically, we generate noise tensors Ni and Nj with the same shape as Zi and Zj, where each element of Ni and N; follows a Gaussian distribution with mean 0 and variance \u03c3\u00b2 = 1. The noisy tensors are then defined as:\n$$Z'_i = Z_i + N_i, Z'_j = Z_j + N_j,$$\nNext, we use a shared-weight Convolutional Neural Network (CNN) and a global pooling layer to encode the hidden representations of Z' and Z' into topological tensor representations:\n$$X_{P_i} = \\begin{cases} f_{CNN} (Z'_i) & \\text{if } |M| = 1, \\\\ P_{POOL} (f_{CNN}(Z'_i)) & \\text{if } |M| > 1, \\end{cases}$$\nwhere fCNN is a CNN, and PPOOL is a pooling layer that preserves the input information in a fixed-size representation. We divide the encoding process into two cases: (1) if the set of EPIs contains only one image, we apply a CNN directly to extract the latent features of Z; (2) if there are multiple EPIs in the set, we employ a global pooling layer to aggregate the latent features into a fixed-size representation, ensuring consistent information retention across different numbers of EPIs. Then, the topological tensor representations are fed into a TTL with H(0) = Xp\u2081. The output of TTL is denoted as Xp\u2081. Let Xp denote the flattened version of X'p. At the end of topological representation extraction, we have two flattened topological tensor representations corresponding to two augmented graphs: Xp, and X. Finally, we define the contrastive loss between these topological tensor representations to be:\n$$l_{i,T} = -log \\frac{exp (sim(X'_{P_i}, X'_{P_j})/\\zeta)}{\\sum_{n=1,n\\neq i,j}^{2N} exp (sim(X'_{P_i}, X'_{P_n})/\\zeta)},$$"}, {"title": "4.3 Tensor Transformation Layer", "content": "The Tensor Transformation Layer (TTL) preserves the tensor structure of the feature matrix X, which has dimensions given by D = \u03a0m_1 Dm, while maintaining hidden representations throughout the network. Let L be a positive integer, and let a = [a(1), . . ., a(L+1)] denote the width of each layer. A deep ReLU Tensor Neural Network can be expressed as a function mapping in the following form:\n$$f(X) = L^{(L+1)} \\circ \\sigma \\circ L^{(L)} \\circ \\sigma \\circ ... \\circ \\sigma \\circ L^{(1)}(X),$$\nwhere \u03c3(\u00b7) represents an element-wise activation function. The affine transformation L(l)(\u00b7), along with the hidden input and output tensors at the l-th layer, denoted by H(l+1) and H(l), are defined as follows:\n$$L^{(l)} (H^{(l)}) := \\langle W^{(l)}, H^{(l)} \\rangle + B^{(l)},$$\nand He+1) := \u03c3 (L(e) (H(e))),\nwhere H(0) = X takes the tensor feature, \u3008\u00b7, \u00b7\u3009 is the tensor inner product, and low-rank weight tensor W(l) and a bias tensor B(e). The tensor structure kicks in when we incorporate tensor low-rank structures such as CP low-rank (1), Tucker low-rank (2), and Tensor Train low-rank (3). We focus on CP low-rank in this paper due to its superior performance (Wen et al., 2024)."}, {"title": "5 Experiments", "content": "5.1 Experiment Settings\nDatasets. In this work, we evaluate the performance of our TensorMV-GCL model on graph classification tasks on chemical compounds, molecule compounds, and social networks datasets. For chemical compounds, the datasets used include NCI1, DHFR, MUTAG, BZR, and COX2 (Sutherland et al., 2003; Kriege and Mutzel, 2012), which are composed of graphs representing chemical compounds with nodes as atoms and edges as chemical bonds. In the case of molecules compounds, the datasets employed are PROTEINS, D&D, PTC_MR, and PTC_FM (Helma et al., 2001; Dobson and Doig, 2003; Borgwardt et al., 2005), where each protein is depicted as a graph with nodes representing amino acids and edges denoting interactions like physical bonds or spatial proximity. We also include a social network dataset, IMDB-B, in which nodes represent actors and actresses, with edges connecting those who have appeared in the same movie. Each dataset has graphs belonging to a certain class and our model aims to classify graphs' classes.\nBaseline. We compare our proposed TensorMV-GCL on 11 real-world datasets with 15 state-of-the-art baselines including (1) Graphlet Kernel (GL) (Nino Shervashidze, 2009), (2) Contrastive Multi-View Representation Learning on Graphs (MVGRL) (Hassani and Ahmadi, 2020), (3) Weisfeiler-Lehman Sub-tree Kernel (WL) (Nino Shervashidze, 2011), (4) Deep Graph Kernels (DGK) (Yanardag and Vishwanathan, 2015), (5) node2vec (Grover and Leskovec, 2016), (6) sub2vec (Adhikari et al., 2017), (7) graph2vec (Narayanan et al., 2017), (8) InfoGraph (Sun et al., 2019a), (9) Graph Contrastive Learning with Augmentations (GraphCL) (You et al., 2020b), (10) Graph Contrastive Learning Automated (JOAO) (You et al., 2021), (11) Adversarial Graph Augmentation to Improve Graph Contrastive Learning (AD-GCL) (Suresh et al., 2021), (12) AutoGCL (Yin et al., 2021), (13) A Review-aware Graph Contrastive Learning Framework for Recommendation (RGCL) (Shuai et al., 2022), (14) GCL-TAGS (Lin et al., 2022), and (15) TopoGCL (Chen et al., 2024e)."}, {"title": "5.2 Classification Performance", "content": "As shown in Table 3, we observe that TensorMV-GCL consistently outperforms the runner-up models across nearly all datasets.\nComparative Performance Analysis. More specifically, we observe that TensorMV-GCL provides significant relative improvements over traditional graph embedding methods like node2Vec and Sub2Vec, with gains exceeding 10% on most datasets. One key reason for this improvement is that, unlike traditional graph embedding methods, TensorMV-GCL incorporates both topological features and graph convolutional networks (GCNs). This allows the model to consider not only local node information but also the global structure of the entire graph. Traditional methods typically focus on node-level proximity and local neighborhoods, missing out on the richer, higher-order topological relationships that are essential for fully understanding complex graphs. By integrating topological features, TensorMV-GCL is able to capture a more comprehensive representation of both node-level and whole-graph information, leading to superior performance in graph classification tasks.\nIn addition, when compare to popular contrastive learning frameworks such as GraphCL, JOAO, and AD-GCL, TensorMV-GCL achieves up to a 9.5% relative improvement. The advantage of our tensor-based contrastive learning approach lies in its ability to capture multi-scale structural and topological information from multiple augmented graph views. While traditional contrastive learning methods typically focus on contrasting graph views based on simple structural transformations, TensorMV-GCL goes further by leveraging topological insights through extended persistent homology (EPH). This allows the model to retain critical information at both local and global levels, improving its ability to distinguish between subtle variations in graph structures.\nScalability of TensorMV-GCL. As expected, our proposed TensorMV-GCL significantly outperforms recent state-of-the-art models such as AutoGCL, InfoGraph, and TopoGCL on most datasets. In particular, TensorMV-GCL demonstrates strong results in small dataset benchmarks such as MUTAG, BZR, COX2, and both PTC_MR and PTC_FM, which are often challenging due to their limited data sizes. The model's ability to effectively capture both structural and topological features enables it to outperform these state-of-the-art approaches, especially in scenarios where extracting meaningful information from small datasets is critical. Additionally, to test TensorMV-GCL's scalability, we conducted experiments on datasets like NCI1, REDDIT-B, and DD, which either consist of a large number of graphs or feature graphs with significantly higher average node and edge counts. The results show that TensorMV-GCL is capable of maintaining high performance, even when applied to large-scale graphs. In summary, our experimental results demonstrate that TensorMV-GCL effectively captures both structural and topological information in graphs, offering a significant performance boost in a wide range of graph classification tasks."}, {"title": "5.3 Ablation Studies", "content": "To better understand the importance of different components in TensorMV-GCL, we have conducted 4 ablation studies during the pre-training stage. Specifically, we evaluated the impact of (1) using Persistent Homology (PH) instead of Extended Persistent Homology (EPH), (2) disabling the Stabilized Extended Persistent Images Contrastive Learning, i.e., set \u1e9e = 0 in equation 9, (3) removing the noise injection from EPH, and (4) omitting the Tensor Transformation Layer (TTL) at the end of all channels. We tested the ablations on the NCI1, PROTEINS, MUTAG, DHFR, BZR, COX2, PTC_MR, PTC_FM, and IMDB-B datasets. Results are shown in Table 4.\nThe Effect of EPH. The first row presents the results using Persistent Homology (PH) alone, which leads to inferior performance on all datasets except NCI1. While switching from Extended Persistent Homology (EPH) to PH results in a performance drop, it is not substantial for most datasets, indicating that PH still captures some essential topological features. However, the comprehensive representation provided by EPH, which considers both sublevel and superlevel filtrations, generally results in higher performance across the majority of datasets.\nAs shown in the second row, the most significant performance impact occurs when we completely disable the Stabilized Extended Persistent Images Contrastive Learning, i.e., the TDA channel, meaning we do not use PH or EPH at all. In this case, the performance drops dramatically across all datasets, demonstrating the critical role of topological data analysis in the overall architecture. This highlights that, while both PH and EPH contribute meaningfully to feature extraction, entirely removing the topological learning channel severely limits the model's ability to capture key structural properties, leading to a substantial degradation in performance.\nThe Effect of Adding Noise. In the third row, we evaluate our model without noise injection applied to the EPH-based features. While most datasets show a drop in performance compared to the configuration with noise injection, NCI1 and DHFR performed better without it. This suggests that although noise injection typically enhances robustness by helping the model learn more stable and generalized topological features, it may not be necessary for certain datasets where the model already generalizes effectively. Adding noise during the training process offers several advantages. It helps prevent overfitting by acting as a form of regularization, ensuring the model doesn't become overly specialized to the training data. Furthermore, noise injection introduces variability, which forces the model to learn more robust features that are less sensitive to minor perturbations in the data. This leads to improved generalization across different datasets. For larger datasets, we observe"}]}