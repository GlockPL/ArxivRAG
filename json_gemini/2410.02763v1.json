{"title": "VINOGROUND: SCRUTINIZING LMMS OVER DENSE\nTEMPORAL REASONING WITH SHORT VIDEOS", "authors": ["Jianrui Zhang", "Mu Cai", "Yong Jae Lee"], "abstract": "There has been growing sentiment recently that modern large multimodal models\n(LMMs) have addressed most of the key challenges related to short video com-\nprehension. As a result, both academia and industry are gradually shifting their\nattention towards the more complex challenges posed by understanding long-form\nvideos. However, is this really the case? Our studies indicate that LMMs still lack\nmany fundamental reasoning capabilities even when dealing with short videos.\nWe introduce Vinoground, a temporal counterfactual LMM evaluation benchmark\nencompassing 1000 short and natural video-caption pairs. We demonstrate that\nexisting LMMs severely struggle to distinguish temporal differences between dif-\nferent actions and object transformations. For example, the best model GPT-\n40 only obtains ~50% on our text and video scores, showing a large gap com-\npared to the human baseline of ~90%. All open-source multimodal models and\nCLIP-based models perform much worse, producing mostly random chance per-\nformance. Through this work, we shed light onto the fact that temporal reasoning\nin short videos is a problem yet to be fully solved. The dataset and evaluation code\nare available at https://vinoground.github.io.", "sections": [{"title": "1 INTRODUCTION", "content": "Large multimodal models (LMMs) have become very competitive in not only image comprehension\nbut also short video comprehension. Proprietary models such as GPT-40 (OpenAI, 2024a) and\nGemini-1.5-Pro (Gemini Team, 2024) as well as open-source models like LLaVA-OneVision (Li\net al., 2024a) and Qwen2-VL (Wang et al., 2024) demonstrate strong performance in summarizing a\nshort video's contents and answering questions regarding its details. This has led many researchers to\nbelieve that short video comprehension has mostly been solved, and consequently, the community's\nfocus has been increasingly trending toward creating models that understand longer-form videos that\nare 10s of seconds or even minutes long. Our study, however, indicates that existing models are far\nfrom being capable of fully understanding short videos that are just a few seconds long, especially\nwhen there is dense temporal information.\nAs demonstrated in Wu (2024) and Mangalam et al. (2023), for many existing video benchmarks like\nEgoSchema (Mangalam et al., 2023), ActivityNet-QA (Yu et al., 2019), MSVD and MSRVTT (Xu\net al., 2017), the performance of most modern LMMs does not vary significantly with number of\nsampled frames. In fact, it is often the case that an LMM only needs to see a single frame to produce\na correct response. This 'single-frame bias' (Lei et al., 2023) reduces the video comprehension\nproblem into the much easier image comprehension problem, essentially discarding the temporal\naspect of a video. Researchers have also proposed harder temporal counterfactual benchmarks (Li\net al., 2024b; Saravanan et al., 2024; Liu et al., 2024b) in order to better evaluate an LMM's temporal\nunderstanding capabilities. Existing counterfactual datasets test a model's ability to distinguish\nslight changes from a video's original (positive) caption to the new (negative) caption by asking the\nmodel to match the video with the correct caption. However, they either do not contain any negative\nvideos corresponding to the negative caption, or simply swap the order of two unrelated videos\nto form the positive and negative videos, making it easy to distinguish the negative pair from the"}, {"title": "2 RELATED WORK", "content": "Counterfactual Reasoning Counterfactual reasoning (Morgan & Winship, 2015) in the context\nof computer vision typically involves curating negative images and captions by manipulating the\noriginal data and observing how the outcome changes (Hendricks et al., 2018; Yeh et al., 2019; Goyal\net al., 2019; Verma et al., 2020; Guo et al., 2023; Zhang et al., 2021; Thrush et al., 2022; Le et al.,\n2023; Zhang et al., 2024a). The idea is that a model should understand cause and effect and be able\nto make predictions in unseen situations. For evaluation, curating meaningful and hard negatives is\nimportant. Winoground (Thrush et al., 2022) is a pioneering benchmark for counterfactual reasoning\nwhere each data point contains two images and two corresponding captions. Given an image, a\nvision-language model is asked to find the matching caption from the provided two options, and vice\nversa. COCO-Counterfactual (Le et al., 2023) explores simple linguistic rules to generate negative\ncaptions and uses an image editing model to produce negative images. In this work, we introduce a\nnovel benchmark with counterfactuals that are temporal, an attribute specific to the video modality.\nSingle-Frame Bias and Temporal Reasoning An important aspect of video data is its temporal-\nity, i.e., how events change as time progresses. Modern LMMs sample frames and treat the video as\na set of images, both during training and evaluation. Benchmarks such as EgoSchema (Mangalam\net al., 2023), MSVD and MSRVTT (Xu et al., 2017) exhibit a 'single-frame bias' (Lei et al., 2023)\nwhere only one video frame is needed for a model to predict correctly, as a model's performance\ndoes not vary significantly as the number of frames sampled increases (Wu, 2024; Mangalam et al.,\n2023). To better evaluate a model's temporal understanding capabilities, researchers have developed\ndatasets such as YouCook2 (Zhou et al., 2018), ActivityNet-QA (Yu et al., 2019) and COIN (Lin\net al., 2022), which mainly involve procedural activities that often have a specific temporal depen-\ndency (e.g., if a video shows a person washing and slicing apples, and then baking an apple pie, a\nmodel would easily predict that \"bake it to make a pie before washing the apple\" is a wrong caption\neven without looking at the video). In contrast, Vinoground also includes actions that are entirely\nunrelated, making it more challenging for models to infer answers based solely on textual cues.\nTemporal Counterfactuals Recent benchmarks combine counterfactuals with temporal reason-\ning. EgoSchema (Mangalam et al., 2023) introduces long-form videos where each video has 1\npositive caption and 4 negative captions to choose from, while VITATECS (Li et al., 2024b) intro-\nduces temporal counterfactual data where a word or phrase is swapped/replaced from the positive\ncaption to form the negative caption. However, neither has any negative videos and thus do not fully\nevaluate an LMM's dense temporal reasoning capabilities like we do. VELOCITI (Saravanan et al.,\n2024) introduces positive/negative videos as a part of their intra-video association benchmark by\nclipping random portions in the same video, and asking the model to distinguish between the events.\nThese videos, however, are not truly counterfactual pairs as different clips within the same movie\nare not guaranteed to have a positive-negative relation. TempCompass (Liu et al., 2024b) includes\nvideos that tests a model's ability to differentiate the order of events, but the videos are either con-\ncatenations of two completely unrelated videos with drastic frame changes in between the events,\nor reversed in time and thus impossible to happen in real life, and do not belong to the true data\ndistribution. As we will illustrate in Section 4.4.2, LMMs tend to do much better when it comes to\nsuch videos when compared to our benchmark's more natural negative videos."}, {"title": "3 VINOGROUND", "content": "In this section, we introduce our data curation and categorization process. In order to curate\nVinoground's video-caption pairs, we first explain how we generate the required captions in Sec-\ntion 3.1, how we find the corresponding videos in Section 3.2, and finally the details of categorizing\nthe videos in Section 3.3. An illustration of the overall process can be found in Appendix A."}, {"title": "3.1 GENERATING COUNTERFACTUAL CAPTIONS", "content": "The first step in curating our data is to find counterfactual caption pairs. We want to ensure that the\ncaptions we curate are of high-quality and temporal in nature. While human annotation is a possible\nsolution, it is costly and hard to scale up. Instead, we leverage a SoTA LLM, specifically the GPT-\n4 (OpenAI, 2024b) model, as it is much cheaper, follows the multiple requirements we impose, and"}, {"title": "3.2 VIDEO CURATION", "content": "After curating counterfactual caption candidates, we next try to find corresponding videos for those\ncaptions. We make use of the VATEX (Wang et al., 2019) dataset, which contains 5 distinct captions\nfor each maximum 10-second long video. We only use the validation and test subsets of VATEX to\nmake sure none of Vinoground is ever used as training data. This results in a pool of 9000 videos\nand 45000 captions.\nWe want to be able to quickly retrieve potential matches in VATEX according to the generated cap-\ntion candidates. We leverage sentence transformers (Song et al., 2020), which are good at summariz-"}, {"title": "3.3 CATEGORIZATION", "content": "Finally, we want to be able to evaluate LMMs in a fine-grained manner on multiple aspects rep-\nresented by our dataset. Hence, we categorize Vinoground according to the unique characteristics\ndiscovered through the data curation process, as shown in Figure 2. We report the number of coun-\nterfactual data pairs assigned under each category in Table 1. We define each category as follows:\n\u2022 Object requires LMMs to detect changes in the status of one specific object, such as \"water\nturning into ice\" vs. \"ice turning into water.\" This category is similar to the \"Reversing\"\ncategory in TempCompass (Liu et al., 2024b) that evaluates a model's ability to detect\nattribute and directional changes. While TempCompass reverses positive videos in time to\ncreate negatives and thus can be unnatural, we curate real, natural videos that correspond\nto the negative captions.\n\u2022 Action, on the other hand, simply asks models to distinguish the order in which two or\nmore different actions happened, e.g. \"the man eats and then watches TV\u201d vs. \u201cthe man\nwatches TV and then eats.\" The two actions need not be correlated at all, and thus less\nlogical comprehension is necessary for a correct prediction.\n\u2022 Viewpoint specifically describes changes in the camera angle, perspective, or focus within\nthe video, such as \u201ca person films the car in front of him before he films himself\u201d vs. \u201ca\nperson films himself before he films the car in front of him.\" The change in viewpoint is\nusually accompanied by a drastic difference in between the frames, whereas other events\nmost likely happen within the same context or background.\nWe also divide Vinoground into 4 minor categories: interaction, cyclical, spatial, and contextual.\nSome pairs belong to a multitude of these minor categories, while some do not belong to any of\nthem.\n\u2022 Interaction involves videos where a human changes their way of interacting with an object\nin the course of the video, e.g. \"the calligrapher writes with his pen before he dips it into\nthe ink\" vs. \"the calligrapher dips his pen into the ink before he writes with it.\"\n\u2022 Cyclical tests a model's ability to identify either procedural temporal activities or two ac-\ntions that are dependent on each other. The calligrapher example earlier is also cyclical as\nthe person repeats the procedure \"write, dip, write, dip...\", and the action \"dip\" happens as\na result of \"write\u201d in the positive, while \u201cwrite\u201d is enabled after \u201cdip\u201d in the negative. In\ncontrast, the general \u201caction\u201d category can involve completely unrelated actions.\n\u2022 Spatial It has been shown that LMMs struggle to distinguish physical locations between\nobjects in image-caption pairs (Zhang et al., 2024a). We want to further evaluate this\""}, {"title": "4 EXPERIMENTS", "content": "In this section, we evaluate state-of-the-art vision-language models on our benchmark. We first\ndescribe the models and evaluation metrics in Section 4.1; then we explain our experimental setup,\nincluding prompting methods and human studies, in Section 4.2; we analyze the performances of\nthe models in Section 4.3, and provide further ablation studies in Section 4.4."}, {"title": "4.1 MODELS AND EVALUATION METRICS", "content": "We evaluate both CLIP-based models (Radford et al., 2021) and large generative models, both pro-\nprietary and open-source. The exact list of models we evaluate can be found in Table 2. CLIP-based\nmodels use contrastive learning between videos and captions, while text-generation LMM models\nuse next-word prediction to generate a response. Due to the different nature of the CLIP-based\nvs. LMM methods, we introduce our metrics in different fashions accordingly.\nWe use C to denote captions and V to denote videos. For each positive and negative set of counter-\nfactual video-caption pairs, $(C_i, V_i)$ and $(C_i', V_i')$, $V_i \\in \\{1, 2, ..., 500\\}$, we ask CLIP-based models\nto compute a similarity score e between not only the correct pairs but also the incorrect pairs $(C_i, V_i')$\nand $(C_i', V_i)$ (identical to Winoground (Thrush et al., 2022)). For generative LMMs, we can only\nprovide inputs (e.g., 2 captions and 1 video) to the model and ask it to output an answer A or B.\nWe first evaluate the text score $s_t$ where the model is presented with both positive and negative\ncaptions but only one of the videos, forming the triplets $(C_i, C_i', V_i)$ and $(C_i', C_i, V_i')$. For each\ntriplet, the model is then asked to choose the caption that describes the contained video. We denote\nthe score function of a model response given any triplet as $s$; for instance,\n$s(C_i, C_i', V_i) = \\begin{cases} 1 & \\text{if LMM chooses } C_i \\text{ or } e(c_i, v_i) > e(c_i', v_i) \\text{ for CLIP-based} \\\\ 0 & \\text{otherwise} \\end{cases}$\n$s(C_i', C_i, V_i') = \\begin{cases} 1 & \\text{if LMM chooses } C_i' \\text{ or } e(c_i', v_i') > e(c_i, v_i) \\text{ for CLIP-based} \\\\ 0 & \\text{otherwise} \\end{cases}$\nThen the text score for the given counterfactual pair $(C_i, V_i)$ and $(C_i', V_i')$ is:\n$s_t(C_i, C_i', V_i, V_i') = s(C_i, C_i', V_i) \\land s(C_i', C_i, V_i')$\nwhere $\\land$ is the logical and operator; i.e., $s_t$ is 1 only if both triplets are correct. This exposes the\nmodels when they guess randomly.\nSimilarly, for video score $s_v$, the model is presented with one caption and both positive and negative\nvideos, forming triplets $(C_i, V_i, V_i')$ and $(C_i, V_i', V_i')$. For each triplet, the model is asked to choose\nthe video that is described by the caption. In this case, the response scoring becomes:\n$s(C_i, V_i, V_i') = \\begin{cases} 1 & \\text{if LMM chooses } V_i \\text{ or } e(c_i, v_i) > e(c_i, v_i') \\text{ for CLIP-based} \\\\ 0 & \\text{otherwise} \\end{cases}$"}, {"title": "4.2 EXPERIMENTAL SETUP", "content": "Since for each pair of counterfactuals, we have 2 text-score questions and 2 video-score questions,\nwe have 2000 questions in total. To evaluate CLIP-based models, we use the evaluation code pro-\nvided by the authors to calculate video-caption embeddings and similarity scores. Evaluating text-\ngenerative models is slightly more complicated. We first introduce the different prompts we used.\nFor text score, we provide the model with the video and the two corresponding captions, and prompt\n\u201c<video> Which caption best describes this video? A. {Caption 1}, B. {Caption 2}\". For video\nscore, however, since some LMMs only support 1 video input, we concatenate the positive and neg-\native videos into a single video with a 2 second black screen in between. When sampling N frames\nfor the model's input, we make sure we sample $(N \u2212 1)/2$ frames from the positive and negative\nvideo fragments and at least 1 frame of black screen in between. For the sake of consistency, we\nprovide all models with the single concatenated video, regardless of how many videos they can\nactually take as input. We then prompt the model with \u201c<video> Which video segment matches\nthis caption? Note: The video contains two segments separated by a 2-second black frame. Cap-\ntion: {Caption}. A. First segment (before black frame), B. Second segment (after black frame)\u201d to\nchoose between the two video segments. We also report the results with respect to the number of\nframes sampled by the model from the video, if supported, to evaluate the effect of temporality in\nSection 4.4.1.\nIn addition, we also use Prolific (https://www.prolific.com) to evaluate human perfor-\nmance, and find that our dataset is fairly easy for an average human to complete with high accuracy.\nProlific is a platform similar to Amazon MTurk which recruits workers to complete tasks such as\ndata annotation. The interface we present to the workers is in Appendix D. To filter out unfaith-\nful workers, we employ a qualification process prior to evaluating on Vinoground. We sample 10\nvideo-question pairs from TempCompass (Liu et al., 2024b) that are of the event order category,\nwhich contains concatenated videos with no correlation, such as \u201ca man lifts weights in a gym, then\na cat plays on the grass\". Such examples are easy enough for an average human to obtain 100%\naccuracy. We ask the workers the 10 beginner-level questions first, and they are qualified only if\nthey answer every question correctly. This process results in 170 qualified workers.\nWe conduct human evaluation under two settings. First, the Prolific workers are provided the full\nvideos with audio. To create another environment where we want the workers see the same input\nas the models, we uniformly sample 32 frames from each video and concatenate them together\ninto a new 10-second video with no audio. The results for the two settings are also compared in\nSection 4.4.1. For each question, we obtain answers from 10 unique workers. For the 10 answers\nfrom a single question, we calculate the \"average\" human response by taking the mode of the 10\nanswers. We then report the mean over all the questions as the final result."}, {"title": "4.3 MAIN RESULTS", "content": "Table 2 presents the results. (Please refer to Appendix F for more detailed results, as we only include\neach model's best performances here.)\nFirst, all CLIP-based models (VideoCLIP, LanguageBind, ImageBind) perform much worse than\nrandom chance, suggesting that contrastive learning does not provide models with enough knowl-\nedge of temporality. Among text-generative models, GPT-40 performs best, achieving 54.0% on the"}, {"title": "4.4 IN-DEPTH ANALYSIS OF PERFORMANCE VARIATIONS", "content": "Vinoground's temporal understanding requirements can be demonstrated by varying the different\nnumber of frames sampled, either from the video entirely, or as measured by frames-per-second\n(fps). If a dataset suffers from 'single-frame bias', a model would not perform very differently when\nonly 1 or more frames are sampled. The results of the strongest proprietary and open-source mod-\nels in Table 3 (and additional results in Appendix F) show that the more frames a model takes, the\nbetter its performance. This indicates that a model does need the entirety of each video to fully\ncomprehend the task at hand. Interestingly, too many sampled frames, however, can hurt a model's\nperformance; for GPT-40, its 64-frame variant performs 5% worse on all three metrics compared to\nits 32-frame variant. We suspect that current models are not good at discarding redundant informa-\ntion and isolating signal from noise when there are too many visual tokens."}, {"title": "4.4.2 CATEGORY", "content": "Figure 3 shows results per category as defined in Section 3.3. Interestingly, many models perform\nsignificantly better on the viewpoint and contextual categories, while being significantly worse on\nother categories. Here, we only report the group score for a selected set of models due to space.\nPlease see Appendix E for the full results.\nBoth viewpoint and contextual bring forth drastic changes in between the video frames whenever the\nevents change, as contextual involves background changes that occupy most of the frame while in\nviewpoint, as the camera angle changes, the entirety of the video frame changes as well. On the other\nhand, interaction and cyclical not only require a model to have strong logical understanding of the\nconnection between events, but also the ability to focus on small temporal changes for the different\nactions involved. Spatial, as previously hypothesized, also poses a difficult challenge for models in\nunderstanding changes in object location. Overall, today's models are much better at understanding\ncoarse-level information over a set of frames in their entirety than understanding fine-grained details\nfrom a part of each video frame. This also demonstrates how fine-grained comprehension is also\ncrucial for dense temporal reasoning."}, {"title": "5 CONCLUSION", "content": "We introduced Vinoground, a novel temporal counterfactual benchmark encompassing 1000 short\nand natural video-caption pairs. We demonstrated that existing video understanding models are quite\nincapable in terms of temporal reasoning, even for short (<10 seconds) videos. While an average\nhuman can easily and accurately complete our benchmark, the best model, GPT-40, performs much\nworse, and most models barely perform better than random chance. Our work demonstrates that\nthere is much more to do still in the area of short video comprehension. We believe our bench-\nmark can serve as an important checkpoint in evaluating a model's true performance for temporal\nunderstanding of different actions, background transitions, and object transformations."}, {"title": "LIMITATIONS", "content": "One cannot fully analyze the behavior of proprietary models included in this paper due to the lack\nof access to these models, which are GPT-40, Gemini-1.5-Pro and Claude 3.5 Sonnet."}]}