{"title": "A tutorial on the use of physics-informed neural networks to compute the spectrum of quantum systems", "authors": ["Lorenzo Brevi", "Antonio Mandarino", "Enrico Prati"], "abstract": "Quantum many-body systems are of great interest for many research areas, including physics, biology and chemistry. However, their simulation is extremely challenging, due to the exponential growth of the Hilbert space with the system size, making it exceedingly difficult to parameterize the wave functions of large systems by using exact methods. Neural networks and machine learning in general are a way to face this challenge. For instance, methods like Tensor networks and Neural Quantum States are being investigated as promising tools to obtain the wave function of a quantum mechanical system. In this tutorial, we focus on a particularly promising class of deep learning algorithms. We explain how to construct a Physics-Informed Neural Network (PINN) able to solve the Schr\u00f6dinger equation for a given potential, by finding its eigenvalues and eigenfunctions. This technique is unsupervised, and utilizes a novel computational method in a manner that is barely explored. PINNs are a deep learning method that exploits Automatic Differentiation to solve Integro-Differential Equations in a mesh-free way. We show how to find both the ground and the excited states. The method discovers the states progressively by starting from the ground state. We explain how to introduce inductive biases in the loss to exploit further knowledge of the physical system. Such additional constraints allow for a faster and more accurate convergence. This technique can then be enhanced by a smart choice of collocation points in order to take advantage of the mesh-free nature of the PINN. The methods are made explicit by applying them to the infinite potential well and the particle in a ring, a challenging problem to be learned by an Al agent due to the presence of complex-valued eigenfunctions and degenerate states", "sections": [{"title": "1. Introduction", "content": "The deep learning (DL) approach is one of the most successful machine learning (ML) paradigms, carrying outstanding results on several complex cognitive tasks [1,2]. Training a Deep Learning model, however, usually requires an enormous amount of data. Even when the dataset is large enough to allow for effective training, such data will be finite and noisy, inevitably leading to issues such as overfitting. However, when addressing the dynamics of a physical system there is often prior knowledge about its behavior and the laws that shape it, usually in the form of a system of Integro-Differential Equations.\nPhysics-informed Neural Networks [3] are a promising tool to discover and address the parametrization of a system governed by Partial Differential Equations (PDEs) or Integro-Differential Equations [4]. This rather new family of models has already shown success in a plethora of different fields [5,6], thanks to their ability to learn the implicit solution of a PDE even when given little to no data [7]. It allows to build models able to more efficiently simulate those systems and even to discover new physics [8]. Furthermore, integrating the physical knowledge of the system in the training process allows to prevent overfitting. They have been widely applied in fluid dynamics [8-10] where, among many other things, they have been used in predictive large-eddy-simulation wall modeling, to obtain a model with improved extrapolation capabilities to flow conditions that are not within the training data. Unknown physics has been learned by improving on empirical models for viscosity in non-Newtonian fluids. There, the PINN model removes an unphysical singularity present in the empirical one. In astrophysics [11], they have been used to compute the first quasinormal"}, {"title": "2. Materials and Methods", "content": "In this Section, we explain how to build a PINN and how to decide design the loss function and to apply heuristics to optimize the network's hyperparameters. First, Section 2.1 contains in a concise form the required background information on artificial neural networks and deep learning. Then in Section 2.3 we give a detailed explanation of what a PINN is and how to build it, and the auxiliary output method used to calculate the normalization is explained. Afterwards, in Section 2.4, we explain how to analyze the problem to understand the best possible way to encode the physics in the loss function of the PINN. After that in Section 2.5 we show how to enforce the physical constraints of the system, such as normalization and boundary conditions. After those Sections the reader should have gained a picture of the main losses needed to find an eigenstate for the given potential. More work is, however, required to find the corresponding eigenvalue. In Section 2.6 we show how to find the energy by solving the equation in a self-consistent way. Next, the problem of inducing the PINN to converge to a specific eigenstate is addressed. In particular, Section 2.7 demonstrates how to build a series of PINNs where the first predicts the ground state of the system while each subsequent network predicts the next state, in order of increasing energy. At this point, the problem has been set up. The next Sections show how to build the best possible computational framework to solve it: in Section 2.8 it is explained how to set the best weights, which are given special attention since those are the main hyperparameters to tune for the PINN. Then in Section 2.9 we introduce a method to choose the best set of training points, and in Section 2.10 how to choose the computational domain. Lastly, Section 2.11 gives some pointers on how to best build the actual network, with the number of neurons, hidden layers, activation function, and loss metrics. This pipeline is summarized in Figure 1"}, {"title": "2.1. Neural Networks and Deep Learning", "content": "This Section gives a broad qualitative overview of the basic tools needed for this tutorial, namely deep neural networks. The information here has been largely taken from Refs. [24] and [25]. (Artificial) neural networks are information processing systems, whose structure and operation principles are inspired by the nervous system and the brain of animals and humans. They consist of a large number of fairly simple units, the so-called neurons, which are working in parallel. These neurons communicate by sending information in the form of activation signals, along directed connections, to each other. Deep Learning algorithms face a series of challenges to train them effectively. For instance, one of the most common issues is the problem of overfitting. With a larger number of hidden neurons, a neural network may adapt not only to the regular dependence between outputs and inputs we try to capture, but also to the accidental specifics (and thus also the errors and deviations) of the training data set. This is usually called overfitting. Overfitting will usually lead to the effect that the error a neural network with many hidden neurons yields on the validation data\u00b9 will be greater than the error on the training data. This is due to the fact that the validation data set is likely distorted in a different fashion, since the errors and deviations are random. Furthermore, while DL methods are known to be extremely effective when immense amounts of data are available, this availability is not a given in all circumstances. For instance, the data might be costly to obtain. Or it might be tied to the observation of some phenomenon that cannot be replicated at will and is not common enough to allow the training of a DL model that relies on big data. PINNs aim to addresso both the problem of data availability and mitigate overfitting by making use of the underlying physics of the system."}, {"title": "2.2. Materials", "content": "All results in this paper are reproducible using a laptop with a NVIDIA GeForce RTX 3060 Laptop GPU."}, {"title": "2.3. Physics-Informed neural networks", "content": "In this Section we revise the ideas behind a generic physics-informed neural network. It is then detailed the particular kind of PINN used here for solving Schr\u00f6dinger's equation, the A-PINN [4]: Physics-informed neural network with Auxiliary output. Physics-informed neural networks [3], or PINNs are a category of neural networks that aim to solve Partial Differential Equations by encoding prior knowledge about the physical system, most importantly the differential equation itself, in their loss function. This loss can be written as:\n$L = Loss_{data} + Loss_{PDE} + Loss_{phys}$ (3)\n$Loss_{data}$ is the standard loss for a neural network: take a set of N training points {$x_i$} with 0 \u2264 i \u2264 N - 1 and the corresponding labels {\u1ef9\u1d62}, where \u1ef9\u1d62 is the value that the objective function assumes in x\u1d62. Given the network outputs {out\u1d62} for the inputs {$x_i$}, $Loss_{data}$ will be:\n$Loss_{data} = Err({\u0177_i}, {out_i})$ (4)\nWhere Err represents a chosen metric. We will now list the relevant ones for this tutorial:\n$MSE({\u0177_i}, {out_i}) = \\frac{1}{N} \\sum_i ||\u0177_i - out_i||^2$\n$SSE({\u0177_i}, {out_i}) = \\sum_i ||\u0177_i - out_i||^2$ (5)\n$MAE({\u0177_i}, {out_i}) = \\frac{1}{N} \\sum_i ||\u0177_i - out_i||$\n$SAE({\u0177_i}, {out_i}) = \\sum_i ||\u0177_i - out_i||$\nNote that in our case this $Loss_{data}$ will not be present since we are working in an unsupervised setting. Those metrics are still used to compute the unsupervised losses. $Loss_{phys}$ corresponds to the losses given by the physical constraints of the system, like boundary conditions and initial conditions. Lastly, $Loss_{PDE}$ is the error for the differential equation. For instance, given a PDE with implicit solution u(t) that depends on u(t) and its derivative $\\partial_t u(t)$ with respect to the independent variable t, the differential equation can be written as:\n$\\partial_t u(t) + N[u; \\lambda] = 0$ (6)\nwhere N is a nonlinear operator and \u03bb is some set of parameters. Given this, we can write $LOSSPDE$ (using MSE as metric) for a network designed to solve this equation as:\n$\\frac{1}{n} \\sum_{i=1}^n (\\partial_t u(t_i) + N[u(t_i); \\lambda])^2$ (7)\nWhere {$t_i$} is the set of n points, called collocation points, where the PDE will be evaluated. This makes it possible to train neural networks even in the presence of noisy or very little data, and even in a fully unsupervised setting, such as in this paper.\nHowever, when working with quantum mechanical systems, we are usually not just dealing with Partial Differential Equations, but often with Integro-Differential Equations. At the very least there is always the need to compute an integral to enforce the wave function's normalization. Normally, to compute an integral numerically finite difference methods are employed. Those methods utilise a mesh, a set of finite elements used to represent the function, to compute the integral. However, being mesh-free is one of the advantages of utilizing a neural method. This advantage can be retained if we are able to employ a mesh-free way to compute the required integrals. To this end we employ so-called auxiliary outputs: beyond the wave function, we add an additional output to the network for each integral we need to compute. For instance, an integral is needed for"}, {"title": "2.4. Potential analysis", "content": "This Section enters into the specifics of how to build a PINN to solve Schr\u00f6dinger's equation. Here we focus in particular on the analysis of the potential to find the optimal representation of the problem. The potential is then used to define the associated Schr\u00f6dinger equation. Beyond that, it is also important to identify the independent variables that will make up the input of the neural network. Likewise, we must identify any integrals that will make up the auxiliary output. For instance, for an infinite potential well, the representation of the Schr\u00f6dinger's equation will just contain the kinetic part, and the potential:\n$V(x) = 0$ for $x \\in [-\\frac{L}{2}, \\frac{L}{2}]$\n$V(x) = \\infty$ otherwise (9)\ncan be represented by having $[-\\frac{L}{2}, \\frac{L}{2}]$ be the computational domain and imposing Dirichlet boundary conditions:\n$\\psi(-\\frac{L}{2}) = \\psi(\\frac{L}{2}) = 0$ (10)\nThe position x will be the input. The outputs will be the main one, \u03c8(x), and an auxiliary one, v(x), as defined in (8)."}, {"title": "2.5. Physical constraints and inductive biases", "content": "After defining the main differential equation loss in the previous Section, in this Section the other losses used to build a neural network capable of solving the equation are explained. In particular, here are defined the losses linked to the physical constraints of the system. We also add a kind of loss not essential to solve the equation but which greatly improves convergence, namely the inductive biases.\nTo define these losses one needs to understand which are the physical constraints of the system and if any symmetries can be exploited to obtain an inductive bias. By inductive bias we mean a set of conditions that leads the network to prioritize a set of solution over other, for instance prioritizing solutions that follow a given symmetry. Biases can be invaluable because they limit the search space, and thus reduce the number of epochs needed to obtain a solution.\nA condition that is always needed is the normalization:\n$\\int_\\Omega |\\psi(x)|^2 dx = 1$ (11)\nwhere \u03a9 is the domain of the wave function, chosen depending on the dimensionality and nature of the system. For instance, for a one-dimensional infinite potential well in $[-\\frac{L}{2}, \\frac{L}{2}]$, \u03a9 can be restricted to \u03a9 = $[-\\frac{L}{2}, \\frac{L}{2}]$. It is important to note that imposing the condition defined in (11) is the only thing that forces the wave function to be nonzero. Otherwise \u03c8(x) = 0 for all x would exactly satisfy all losses. This integral can be solved by utilizing an auxiliary output v(x) with form (8) and by imposing v($\\frac{L}{2}$) = 1; v(-$\\frac{L}{2}$) = 0. However, we must first train the network so that v(x) has the desired form (8). Once again, this can"}, {"title": "2.6. Self-consistent computation of the eigenenergy", "content": "In this Section, we explain how to find the eigenvalue to pair with the eigenfunction obtained thanks to the techniques described in the previous Sections. Currently, the network can only find the eigenfunction of the potential when the energy is known. However, in general when solving the Schr\u00f6dinger equation that will not be the case. Therefore a way to find it in parallel with the eigenfunction is needed. There are two ways to do this: the first is to set the energy E itself as a trainable parameter, but this seems to result in fairly slow training. The second is to build a second neural network to train in parallel with the one that predicts the wave function. It will take as input a constant stream of \"1\"s of the same length as the input vector, and output a guess for the energy E. Since the output is constant, this second network will have no hidden layers and there will be no nonlinear activation function. The network will just perform a linear transformation from 1 to the value of the energy, based on a trainable parameter."}, {"title": "2.7. Finding the desired state", "content": "A crucial step is to build the losses that allow the network to distinguish the desired eigenstate from the infinite spectrum of the Hamiltonian. This is explained in this Section. By training the PINN with the current losses, it will solve the Schr\u00f6dinger equation for a random state anywhere in the spectrum. Usually, however, we are looking for some amount of eigenstate starting from the ground state and going in order of increasing energy. In particular it is especially important to build a network that finds the ground state first. This can be achieved by introducing an energy minimization condition in the loss:\n$e^{\\alpha (E_{PINN} - E_{init})} = 0$ (14)\nwhere $E_{PINN}$ is the predicted energy, \u03b1 is a hyperparameter. $E_{init}$ is either another hyperparameter when training the network to predict the ground state, or the energy of the previous state, for the excited states. The choice to increase $E_{init}$ as the quantum number increases has been made to avoid the loss exploding for higher energy states due to its exponential increase. Otherwise the network might fail to converge at all. We also have to"}, {"title": "2.8. Weights", "content": "In the previous Sections the losses to minimize have been defined. In the following Sections is explained how to build the actual computational framework that ensures the network is able to minimize those losses. In this Section, in particular, some pointers are given on how to tune one of the most important set of hyperparameters: the weights. Usually, when training PINNs it is better to set weights that ensure all the losses have the same scale. That is not the case for this problem: it is very easy for a network that has to predict eigenfunctions to get stuck in local minimas such as \u03c8(x) = 0 \u2200x or an already discovered eigenvector. This means that it is actually better to set the weights in such a way that the network first enforces the physical constraints of the system, such as normalization and orthogonality. Only once these are fulfilled the network should be trained to actually solve the equation. This means setting the losses for those physical constraints higher that the differential equations'. In general, this means than the integral loss should have the highest weight, then the normalization, needed to avoid the constant solution, and the orthogonality, needed to avoid repeating solutions. At this point the network should minimize the boundary conditions loss. The last loss to minimize is the differential equation loss. However, since solving the differential equation is the main objective of the network it is often worth it to gradually increase its loss' weight. This allows the network to learn how to enforce the physical conditions in the first epochs but makes it focus more on the differential equation in the latter epochs. It is important to stress that weights are probably the most impactful hyperparameter to tune: a correct choice of weights will be the difference between quick convergence and a network getting stuck on a suboptimal solution."}, {"title": "2.9. Choice of training points", "content": "In order to train the PINN, the losses must be evaluated on a set of points, known as collocation points. These can be taken anywhere in the domain, and this Section contains the optimal way to choose them. When training a PINN such as this, each batch of training points should span the whole domain. Furthermore, since no dataset is being used, the choice of points is not constrained in any way: they can be taken anywhere in the domain. This can be exploited to basically work with an infinitely large dataset. To fully take advantage of this, first a batch size N is decided. Then a mesh of N points that spans the whole domain is built. Each of this points will be the centre of a normal distribution from which one of the points in the given batch will be sampled. This results in batches that are"}, {"title": "2.10. Domain definition", "content": "This computational framework inevitably requires the collocation points to be chosen within a certain domain. In this Section, the best way to identify this domain is explained. The choice of the computational domain is crucial for an effective convergence of the network. First of all, this method only makes sense if almost all of the nonzero part of the desired wave function exists within the chosen domain. This means that outside it the wave function must be either exactly 0 or vanishingly small and thus negligible. On the other hand, if the computational domain is too wide then the network will be trained on a vast area of just Os that will provide no information to it, resulting in a greatly slowed down training. This means that more points in each batch would be needed to properly sample the \"interesting\" part of the domain. In the case of the well the choice of the optimal domain is an immediate consequence of the structure of the system: as mentioned in Section 2.4 just using $[-\\frac{L}{2}, \\frac{L}{2}]$ is ideal since the wave function will be nonzero only inside the well. For more complex potentials the choice of the domain must be made based on an analysis of the potential under study. One needs to find out at which distance from the origin the wave function becomes negligible. Doing this will probably require some trial and error."}, {"title": "2.11. Network architecture", "content": "In this Section some suggestions are given on how to better build a feed-forward neural network to solve the Schr\u00f6dinger in a physics-informed way. We focus on which metrics to use for the loss and which activation function should be employed in the network. PINNs tend to work better as relatively shallow but wide networks. For instance in [7] the main network was made up of 7 hidden layers of 256 neurons each. As for the activation function, an infinitely differentiable one such as tanh is needed. Otherwise the network would be unable to compute higher order derivative via automatic differentiation. Choosing an activation function of this type instead of something like ReLU might, however, cause vanishing gradient problems. If this happens, it might be worth it to look into adaptive activation functions [26] as a solution. Lastly, it is not necessary to compute all the partial losses using the same metric. It usually makes more sense to utilize something like Sum of Absolute Errors for the losses enforcing the physical conditions of the system, such as the Boundary Conditions loss or the Normalization loss. This is because the rest of the training makes sense only if those losses are approximately 0 in all points. And this is enforced more strictly with SAE rather then something like MSE. On the other hand, Mean Squared Error is usually the best choice for the integral and differential equation losses. Utilizing Sum of Squared Error to compute these losses is also worth considering. This especially true when the wave function is expected to have a complex behaviour in a small section of the domain and a relatively simple one in the rest. This choice of metric will avoid underestimating the loss due to the averaging of the error for the points in the large \"simple\" area and the small \"complex\" area. As for the optimizer, Adam [27] is usually rather effective."}, {"title": "3. How to evaluate training", "content": "In this section some ways to evaluate the PINN's training in order to optimize their architecture and hyperparameters are shown. The focus will be on some typical pathological behaviour and some ways to understand what causes them and how to fix them. The behaviour of the loss actually tends to be a pretty reliable metric when training PINNs in an unsupervised way, and is actually more reliable then for standard neural networks. This is because a PINN follows a given physical law, and the loss quantifies how well its output follow said law. It does not have a set of training points it might memorize instead of learning the underlying law. As such, overfitting on some training set is not a risk and it does not make sense to build a training, validation and test sets from a dataset. In spite of this, due to the complexity of the loss function just looking at the full loss does not tell the"}, {"title": "3.1. Partial losses", "content": "The first thing to look at when the PINN does not converge is to try to understand which loss or losses is failing to converge. At this point, the first thing one can do to try and improve the training can be to try to either increase the weight for the losses that are struggling to converge or reduce the weights for other, competing losses. One very common problem is the orthogonality loss failing to converge while other losses get very close 0. This means that the network is predicting an already discovered eigenfunction 2. For this increasing the weight of the orthogonality loss is usually a pretty safe solution. Another common occurrence is the normalization loss staying high while all other losses become 0. This usually means that the network has converged to the local minima \u03c8(x) = 0 \u2200x. While it could be fixed by increasing the normalization loss' weight 3 it can also be useful to change the initialization of the network, for instance going from Xavier uniform [28] to Kaiming normal [29], in order to start the network further from this local minima. This could also be a sign of the energy minimization loss having excessive weight or growing too quickly. A way to decide this in low dimensions is to display the plot of the best wave function (i.e., the one with the lowest loss) every number of epochs, and look for any pathological behaviour in its evolution. Another example is the case in which the boundary condition loss seems to fail to converge: in this case if the plotted wave function resembles what we expect for the given potential but does not vanish at the border of the domain it might mean that a larger domain is needed. It is important to note that a plateauing loss is not always a sign of a failed convergence. While training it might happen that the PINN's loss remains stuck around a certain value for thousands of epochs but will start decreasing again after some time. On the other hand, wild oscillations of the loss might be a sign of an excessively high learning rate. Note that these are only some empirical heuristics and might not apply in all cases. It is important to look at the loss' behaviour in a case-by-case basis and adjust the weights depending on what is best in the current circumstances."}, {"title": "3.2. Evaluation metrics", "content": "In the previous section only the losses have been utilized to understand how successful the training has been. However, depending on the problem other, more informative, metrics might be available. In this section two metrics that allow to gauge the goodness of PINN's solution for the Schr\u00f6dinger equation are detailed. When a solution is already known due to different numerical methods or because the potential has an analytic solution, the correctness of the PINN's output can be easily evaluated by utilizing fidelity [30-32] and relative error on the energy. To gauge the eigenvalue's correctness the relative error for the energy is used:\n$erre = \\frac{|E_{Ex} - E_{PINN}|}{E_{Ex}}$ (16)\nwhere $E_{Ex}$ is the analytic value of the energy, while $E_{PINN}$ is the one predicted by the network. On the other hand to evaluate the eigenvector the fidelity between the predicted wave function and the exact one is employed:\n$F_{\\psi} = |\\langle \\Psi_{Ex}|\\Psi_{PINN}\\rangle|^2$ (17)"}, {"title": "4. Examples", "content": "Some examples of quantum mechanical systems that can be solved by using PINNs are now shown: the Infinite potential well and the particle in a ring."}, {"title": "4.1. Infinite potential well", "content": "In this section we study the infinite potential well. This system has been chosen due to its extreme simplicity, making it an excellent testbed to build the basis of the method. We are studying, in particular, the infinite potential well $[-\\frac{L}{2}, \\frac{L}{2}]$, with L = 3:\n$V(x) = 0$ for $x \\in [-\\frac{L}{2}, \\frac{L}{2}]$\n$V(x) = \\infty$ otherwise (19)\nAs mentioned in section 2.4 this potential can be represented just by studying the region with V(x) = 0 with Dirichlet boundary conditions. Therefore, based on the previous sections, the neural network takes as input the position x and outputs the wave function \u03c8(x) and auxiliary output v(x). The network's losses will be:\n$\\frac{\\partial v(x')}{\\partial x'}|_{x'=x} = |\\psi(x)|^2$\n$v(-\\frac{L}{2}) = 0$, $v(\\frac{L}{2}) = 1$\n$\\psi(-\\frac{L}{2}) = \\psi(\\frac{L}{2}) = 0$\n$e^{\\alpha (E_{PINN} - E_{init})} = 0$, with \u03b1 = 0.8; $E_{init}$ = 0 when n = 0\n$\\sum \\langle \\psi(x)|\\psi_i(x) \\rangle = 0$\n$\\frac{-1}{2} \\frac{\\partial^2 \\psi(x)}{\\partial x^2} - E \\psi(x) = 0$ (20)\nTo those losses an inductive bias can be added, since the potential is symmetric with respect to 0. Due to this symmetry, The wave functions that solve the equation are alternatively symmetric and anti-symmetric with respect to 0. This condition can be enforced by adding the loss:\n$\\psi(x) - s \\psi(-x) = 0$ (21)\nwhere s is a parameter that assumes the value one if we are looking for a symmetric state (n even), or s = \u22121 if we are looking for an anti-symmetric one (n odd). For the sake of clarity, it is worth mentioning that the value of s starts at one and is switched every time a new model is trained."}, {"title": "4.2. Particle in a ring", "content": "In this Section we show another example of potential: the particle in a ring. In this case, the potential is 0 on a ring of radius L, and \u221e everywhere else. Using polar coordinates, the wave function depends only on the angular coordinate \u03b8. The shape of the potential actually becomes the same as the infinite potential well of Section 4.1, with the difference that instead of having Dirichlet boundary conditions it requires periodic boundary conditions, that is:\n$\\psi(0) = \\psi(2\\pi)$ (23)\nOr, more in general, we require:\n$\\psi(\\theta) = \\psi(\\theta + 2\\pi)$ (24)\nFurthermore, the change of variable makes the gradient assume the following form:\n$\\frac{1}{L^2} \\frac{\\partial^2}{\\partial \\theta^2}$ (25)\nThis makes the analytical form of the solutions:\n$\\psi(\\theta) = \\frac{1}{\\sqrt{2\\pi}} e^{in\\theta}$\n$E_n = \\frac{n^2}{2 L^2}$ where n = 0,\u00b11, \u00b12, \u00b13,... (26)\nThis actually introduces two significant challenges in training the PINN: first of all, \u03c8(\u03b8) will in general be a complex number, which means that the main output of the"}]}