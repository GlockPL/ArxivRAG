{"title": "A tutorial on the use of physics-informed neural networks to\ncompute the spectrum of quantum systems", "authors": ["Lorenzo Brevi", "Antonio Mandarino", "Enrico Prati"], "abstract": "Quantum many-body systems are of great interest for many research areas, including\nphysics, biology and chemistry. However, their simulation is extremely challenging, due to the\nexponential growth of the Hilbert space with the system size, making it exceedingly difficult to\nparameterize the wave functions of large systems by using exact methods. Neural networks and\nmachine learning in general are a way to face this challenge. For instance, methods like Tensor\nnetworks and Neural Quantum States are being investigated as promising tools to obtain the wave\nfunction of a quantum mechanical system. In this tutorial, we focus on a particularly promising\nclass of deep learning algorithms. We explain how to construct a Physics-Informed Neural Network\n(PINN) able to solve the Schr\u00f6dinger equation for a given potential, by finding its eigenvalues\nand eigenfunctions. This technique is unsupervised, and utilizes a novel computational method\nin a manner that is barely explored. PINNs are a deep learning method that exploits Automatic\nDifferentiation to solve Integro-Differential Equations in a mesh-free way. We show how to find\nboth the ground and the excited states. The method discovers the states progressively by starting\nfrom the ground state. We explain how to introduce inductive biases in the loss to exploit further\nknowledge of the physical system. Such additional constraints allow for a faster and more accurate\nconvergence. This technique can then be enhanced by a smart choice of collocation points in order\nto take advantage of the mesh-free nature of the PINN. The methods are made explicit by applying\nthem to the infinite potential well and the particle in a ring, a challenging problem to be learned by\nan Al agent due to the presence of complex-valued eigenfunctions and degenerate states", "sections": [{"title": "1. Introduction", "content": "The deep learning (DL) approach is one of the most successful machine learning (ML)\nparadigms, carrying outstanding results on several complex cognitive tasks [1,2]. Training\na Deep Learning model, however, usually requires an enormous amount of data. Even\nwhen the dataset is large enough to allow for effective training, such data will be finite\nand noisy, inevitably leading to issues such as overfitting. However, when addressing the\ndynamics of a physical system there is often prior knowledge about its behavior and the\nlaws that shape it, usually in the form of a system of Integro-Differential Equations.\nPhysics-informed Neural Networks [3] are a promising tool to discover and address the\nparametrization of a system governed by Partial Differential Equations (PDEs) or Integro-\nDifferential Equations [4]. This rather new family of models has already shown success in\na plethora of different fields [5,6], thanks to their ability to learn the implicit solution of a\nPDE even when given little to no data [7]. It allows to build models able to more efficiently\nsimulate those systems and even to discover new physics [8]. Furthermore, integrating the\nphysical knowledge of the system in the training process allows to prevent overfitting. They\nhave been widely applied in fluid dynamics [8-10] where, among many other things, they\nhave been used in predictive large-eddy-simulation wall modeling, to obtain a model with\nimproved extrapolation capabilities to flow conditions that are not within the training data.\nUnknown physics has been learned by improving on empirical models for viscosity in non-\nNewtonian fluids. There, the PINN model removes an unphysical singularity present in the\nempirical one. In astrophysics [11], they have been used to compute the first quasinormal"}, {"title": "2. Materials and Methods", "content": "In this Section, we explain how to build a PINN and how to decide design the loss\nfunction and to apply heuristics to optimize the network's hyperparameters. First, Section\n2.1 contains in a concise form the required background information on artificial neural\nnetworks and deep learning. Then in Section 2.3 we give a detailed explanation of what\na PINN is and how to build it, and the auxiliary output method used to calculate the\nnormalization is explained. Afterwards, in Section 2.4, we explain how to analyze the\nproblem to understand the best possible way to encode the physics in the loss function of\nthe PINN. After that in Section 2.5 we show how to enforce the physical constraints of the\nsystem, such as normalization and boundary conditions. After those Sections the reader\nshould have gained a picture of the main losses needed to find an eigenstate for the given\npotential. More work is, however, required to find the corresponding eigenvalue. In Section\n2.6 we show how to find the energy by solving the equation in a self-consistent way. Next,\nthe problem of inducing the PINN to converge to a specific eigenstate is addressed. In\nparticular, Section 2.7 demonstrates how to build a series of PINNs where the first predicts\nthe ground state of the system while each subsequent network predicts the next state, in\norder of increasing energy. At this point, the problem has been set up. The next Sections\nshow how to build the best possible computational framework to solve it: in Section 2.8\nit is explained how to set the best weights, which are given special attention since those\nare the main hyperparameters to tune for the PINN. Then in Section 2.9 we introduce a\nmethod to choose the best set of training points, and in Section 2.10 how to choose the\ncomputational domain. Lastly, Section 2.11 gives some pointers on how to best build the\nactual network, with the number of neurons, hidden layers, activation function, and loss\nmetrics. This pipeline is summarized in Figure 1"}, {"title": "2.1. Neural Networks and Deep Learning", "content": "This Section gives a broad qualitative overview of the basic tools needed for this\ntutorial, namely deep neural networks. The information here has been largely taken\nfrom Refs. [24] and [25]. (Artificial) neural networks are information processing systems,\nwhose structure and operation principles are inspired by the nervous system and the\nbrain of animals and humans. They consist of a large number of fairly simple units, the\nso-called neurons, which are working in parallel. These neurons communicate by sending\ninformation in the form of activation signals, along directed connections, to each other.\nDeep Learning algorithms face a series of challenges to train them effectively. For instance,\none of the most common issues is the problem of overfitting. With a larger number of\nhidden neurons, a neural network may adapt not only to the regular dependence between\noutputs and inputs we try to capture, but also to the accidental specifics (and thus also the\nerrors and deviations) of the training data set. This is usually called overfitting. Overfitting\nwill usually lead to the effect that the error a neural network with many hidden neurons\nyields on the validation data\u00b9 will be greater than the error on the training data. This is\ndue to the fact that the validation data set is likely distorted in a different fashion, since\nthe errors and deviations are random. Furthermore, while DL methods are known to be\nextremely effective when immense amounts of data are available, this availability is not a\ngiven in all circumstances. For instance, the data might be costly to obtain. Or it might be\ntied to the observation of some phenomenon that cannot be replicated at will and is not\ncommon enough to allow the training of a DL model that relies on big data. PINNs aim to\naddresso both the problem of data availability and mitigate overfitting by making use of\nthe underlying physics of the system."}, {"title": "2.2. Materials", "content": "All results in this paper are reproducible using a laptop with a NVIDIA GeForce RTX\n3060 Laptop GPU."}, {"title": "2.3. Physics-Informed neural networks", "content": "In this Section we revise the ideas behind a generic physics-informed neural network.\nIt is then detailed the particular kind of PINN used here for solving Schr\u00f6dinger's equation,\nthe A-PINN [4]: Physics-informed neural network with Auxiliary output. Physics-informed\nneural networks [3], or PINNs are a category of neural networks that aim to solve Partial\nDifferential Equations by encoding prior knowledge about the physical system, most\nimportantly the differential equation itself, in their loss function. This loss can be written\nas:\n$\\mathcal{L}$ = $Loss_{data}$ + $Loss_{PDE}$ + $Loss_{phys}$       (3)\n$Loss_{data}$ is the standard loss for a neural network: take a set of N training points {$x_i$} with\n0 \u2264 i \u2264 N - 1 and the corresponding labels {\u1ef9\u2081}, where \u1ef9; is the value that the objective\nfunction assumes in $x_i$. Given the network outputs {$out_i$} for the inputs {$x_i$}, $Loss_{data}$ will\nbe:\n$Loss_{data}$ = Err({$\\hat{y_i}$}, {$out_i$})   (4)\nWhere Err represents a chosen metric. We will now list the relevant ones for this tutorial:\nMean Squared Error: $MSE$({$\\hat{y_i}$}, {$out_i$}) = $\\frac{1}{N}$ $\\sum_i$ $|\\hat{y_i} - out_i|^2$\nSum of Squared Errors: $SSE$({$\\hat{y_i}$}, {$out_i$}) = $\\sum_i$ $|\\hat{y_i} - out_i|^2$            (5)\nMean Absolute Error: $MAE$({$\\hat{y_i}$}, {$out_i$}) = $\\frac{1}{N}$ $\\sum_i$ $|\\hat{y_i} - out_i|$\nSub of Absolute Errors: $SAE$({$\\hat{y_i}$}, {$out_i$}) = $\\sum_i$ $|\\hat{y_i} - out_i|$\nNote that in our case this $Loss_{data}$ will not be present since we are working in an unsuper-\nvised setting. Those metrics are still used to compute the unsupervised losses. $Loss_{phys}$\ncorresponds to the losses given by the physical constraints of the system, like boundary\nconditions and initial conditions. Lastly, $Loss_{PDE}$ is the error for the differential equation.\nFor instance, given a PDE with implicit solution \u03bc(t) that depends on u(t) and its derivative\n\u2202tu(t) with respect to the independent variable t, the differential equation can be written\nas:\n$\\delta_t\\mu(t)$ + N[u; \u03bb] = 0       (6)\nwhere N is a nonlinear operator and A is some set of parameters. Given this, we can write\n$LOSS_{PDE}$ (using MSE as metric) for a network designed to solve this equation as:\n$\\frac{1}{n}$ $\\sum_{i=1}^{n}$($\\delta_t\\mu(t_i)$ + N[$\u03bc(t_i)$; \u03bb])$^2$      (7)\nWhere {$t_i$} is the set of n points, called collocation points, where the PDE will be evaluated.\nThis makes it possible to train neural networks even in the presence of noisy or very little\ndata, and even in a fully unsupervised setting, such as in this paper.\nHowever, when working with quantum mechanical systems, we are usually not just\ndealing with Partial Differential Equations, but often with Integro-Differential Equations.\nAt the very least there is always the need to compute an integral to enforce the wave\nfunction's normalization. Normally, to compute an integral numerically finite difference\nmethods are employed. Those methods utilise a mesh, a set of finite elements used to\nrepresent the function, to compute the integral. However, being mesh-free is one of the\nadvantages of utilizing a neural method. This advantage can be retained if we are able\nto employ a mesh-free way to compute the required integrals. To this end we employ\nso-called auxiliary outputs: beyond the wave function, we add an additional output to\nthe network for each integral we need to compute. For instance, an integral is needed for"}, {"title": "2.4. Potential analysis", "content": "This Section enters into the specifics of how to build a PINN to solve Schr\u00f6dinger's\nequation. Here we focus in particular on the analysis of the potential to find the optimal rep-\nresentation of the problem. The potential is then used to define the associated Schr\u00f6dinger\nequation. Beyond that, it is also important to identify the independent variables that will\nmake up the input of the neural network. Likewise, we must identify any integrals that will\nmake up the auxiliary output. For instance, for an infinite potential well, the representation\nof the Schr\u00f6dinger's equation will just contain the kinetic part, and the potential:\n$\\begin{cases}\nV(x) = 0  \\text{for} x \\in [-\\frac{L}{2}, \\frac{L}{2}]\\\\\nV(x) = \\infty  \\text{otherwise}\n\\end{cases}$       (9)\ncan be represented by having $[-\\frac{L}{2}, \\frac{L}{2}]$ be the computational domain and imposing Dirichlet\nboundary conditions:\n$\\psi(-\\frac{L}{2}) = \\psi(\\frac{L}{2}) = 0$       (10)\nThe position x will be the input. The outputs will be the main one, \u03c8(x), and an auxiliary\none, v(x), as defined in (8)."}, {"title": "2.5. Physical constraints and inductive biases", "content": "After defining the main differential equation loss in the previous Section, in this\nSection the other losses used to build a neural network capable of solving the equation are\nexplained. In particular, here are defined the losses linked to the physical constraints of the\nsystem. We also add a kind of loss not essential to solve the equation but which greatly\nimproves convergence, namely the inductive biases.\nTo define these losses one needs to understand which are the physical constraints of\nthe system and if any symmetries can be exploited to obtain an inductive bias. By inductive\nbias we mean a set of conditions that leads the network to prioritize a set of solution\nover other, for instance prioritizing solutions that follow a given symmetry. Biases can\nbe invaluable because they limit the search space, and thus reduce the number of epochs\nneeded to obtain a solution.\nA condition that is always needed is the normalization:\n$\\int_{\\Omega} |\\psi(x)|^2 dx = 1$     (11)\nwhere O is the domain of the wave function, chosen depending on the dimensionality and\nnature of the system. For instance, for a one-dimensional infinite potential well in $[-\\frac{L}{2}, \\frac{L}{2}]$,\nO can be restricted to Q = $[-\\frac{L}{2}, \\frac{L}{2}]$. It is important to note that imposing the condition\ndefined in (11) is the only thing that forces the wave function to be nonzero. Otherwise\n\u03c8(x) = 0 for all x would exactly satisfy all losses. This integral can be solved by utilizing\nan auxiliary output v(x) with form (8) and by imposing v($\\frac{L}{2}$) = 1; v(-$\\frac{L}{2}$) = 0. However,\nwe must first train the network so that v(x) has the desired form (8). Once again, this can"}, {"title": "2.6. Self-consistent computation of the eigenenergy", "content": "In this Section, we explain how to find the eigenvalue to pair with the eigenfunction\nobtained thanks to the techniques described in the previous Sections. Currently, the network\ncan only find the eigenfunction of the potential when the energy is known. However, in\ngeneral when solving the Schr\u00f6dinger equation that will not be the case. Therefore a way\nto find it in parallel with the eigenfunction is needed. There are two ways to do this: the\nfirst is to set the energy E itself as a trainable parameter, but this seems to result in fairly\nslow training. The second is to build a second neural network to train in parallel with the\none that predicts the wave function. It will take as input a constant stream of \"1\"s of the\nsame length as the input vector, and output a guess for the energy E. Since the output is\nconstant, this second network will have no hidden layers and there will be no nonlinear\nactivation function. The network will just perform a linear transformation from 1 to the\nvalue of the energy, based on a trainable parameter."}, {"title": "2.7. Finding the desired state", "content": "A crucial step is to build the losses that allow the network to distinguish the desired\neigenstate from the infinite spectrum of the Hamiltonian. This is explained in this Section.\nBy training the PINN with the current losses, it will solve the Schr\u00f6dinger equation for\na random state anywhere in the spectrum. Usually, however, we are looking for some\namount of eigenstate starting from the ground state and going in order of increasing energy.\nIn particular it is especially important to build a network that finds the ground state first.\nThis can be achieved by introducing an energy minimization condition in the loss:\n$e^{\\alpha(E_{PINN} - E_{init})}$ = 0       (14)\nwhere $E_{PINN}$ is the predicted energy, a is a hyperparameter. $E_{init}$ is either another hyper-\nparameter when training the network to predict the ground state, or the energy of the\nprevious state, for the excited states. The choice to increase $E_{init}$ as the quantum number\nincreases has been made to avoid the loss exploding for higher energy states due to its\nexponential increase. Otherwise the network might fail to converge at all. We also have to"}, {"title": "2.8. Weights", "content": "In the previous Sections the losses to minimize have been defined. In the following\nSections is explained how to build the actual computational framework that ensures the\nnetwork is able to minimize those losses. In this Section, in particular, some pointers are\ngiven on how to tune one of the most important set of hyperparameters: the weights.\nUsually, when training PINNs it is better to set weights that ensure all the losses have\nthe same scale. That is not the case for this problem: it is very easy for a network that\nhas to predict eigenfunctions to get stuck in local minimas such as \u03c8(x) = 0 \u2200x or an\nalready discovered eigenvector. This means that it is actually better to set the weights in\nsuch a way that the network first enforces the physical constraints of the system, such\nas normalization and orthogonality. Only once these are fulfilled the network should be\ntrained to actually solve the equation. This means setting the losses for those physical\nconstraints higher that the differential equations'. In general, this means than the integral\nloss should have the highest weight, then the normalization, needed to avoid the constant\nsolution, and the orthogonality, needed to avoid repeating solutions. At this point the\nnetwork should minimize the boundary conditions loss. The last loss to minimize is the\ndifferential equation loss. However, since solving the differential equation is the main\nobjective of the network it is often worth it to gradually increase its loss' weight. This\nallows the network to learn how to enforce the physical conditions in the first epochs but\nmakes it focus more on the differential equation in the latter epochs. It is important to stress\nthat weights are probably the most impactful hyperparameter to tune: a correct choice of\nweights will be the difference between quick convergence and a network getting stuck on a\nsuboptimal solution."}, {"title": "2.9. Choice of training points", "content": "In order to train the PINN, the losses must be evaluated on a set of points, known as\ncollocation points. These can be taken anywhere in the domain, and this Section contains\nthe optimal way to choose them. When training a PINN such as this, each batch of training\npoints should span the whole domain. Furthermore, since no dataset is being used, the\nchoice of points is not constrained in any way: they can be taken anywhere in the domain.\nThis can be exploited to basically work with an infinitely large dataset. To fully take\nadvantage of this, first a batch size N is decided. Then a mesh of N points that spans the\nwhole domain is built. Each of this points will be the centre of a normal distribution from\nwhich one of the points in the given batch will be sampled. This results in batches that are"}, {"title": "2.10. Domain definition", "content": "This computational framework inevitably requires the collocation points to be chosen\nwithin a certain domain. In this Section, the best way to identify this domain is explained.\nThe choice of the computational domain is crucial for an effective convergence of the\nnetwork. First of all, this method only makes sense if almost all of the nonzero part of the\ndesired wave function exists within the chosen domain. This means that outside it the\nwave function must be either exactly 0 or vanishingly small and thus negligible. On the\nother hand, if the computational domain is too wide then the network will be trained on a\nvast area of just Os that will provide no information to it, resulting in a greatly slowed down\ntraining. This means that more points in each batch would be needed to properly sample\nthe \"interesting\" part of the domain. In the case of the well the choice of the optimal domain\nis an immediate consequence of the structure of the system: as mentioned in Section 2.4\njust using $[-\\frac{L}{2}, \\frac{L}{2}]$ is ideal since the wave function will be nonzero only inside the well. For\nmore complex potentials the choice of the domain must be made based on an analysis of\nthe potential under study. One needs to find out at which distance from the origin the wave\nfunction becomes negligible. Doing this will probably require some trial and error."}, {"title": "2.11. Network architecture", "content": "In this Section some suggestions are given on how to better build a feed-forward neural\nnetwork to solve the Schr\u00f6dinger in a physics-informed way. We focus on which metrics to\nuse for the loss and which activation function should be employed in the network. PINNs\ntend to work better as relatively shallow but wide networks. For instance in [7] the main\nnetwork was made up of 7 hidden layers of 256 neurons each. As for the activation function,\nan infinitely differentiable one such as tanh is needed. Otherwise the network would be\nunable to compute higher order derivative via automatic differentiation. Choosing an\nactivation function of this type instead of something like ReLU might, however, cause\nvanishing gradient problems. If this happens, it might be worth it to look into adaptive\nactivation functions [26] as a solution. Lastly, it is not necessary to compute all the partial\nlosses using the same metric. It usually makes more sense to utilize something like Sum of\nAbsolute Errors for the losses enforcing the physical conditions of the system, such as the\nBoundary Conditions loss or the Normalization loss. This is because the rest of the training\nmakes sense only if those losses are approximately 0 in all points. And this is enforced more\nstrictly with SAE rather then something like MSE. On the other hand, Mean Squared Error\nis usually the best choice for the integral and differential equation losses. Utilizing Sum of\nSquared Error to compute these losses is also worth considering. This especially true when\nthe wave function is expected to have a complex behaviour in a small section of the domain\nand a relatively simple one in the rest. This choice of metric will avoid underestimating the\nloss due to the averaging of the error for the points in the large \"simple\" area and the small\n\"complex\" area. As for the optimizer, Adam [27] is usually rather effective."}, {"title": "3. How to evaluate training", "content": "In this section some ways to evaluate the PINN's training in order to optimize their\narchitecture and hyperparameters are shown. The focus will be on some typical pathological\nbehaviour and some ways to understand what causes them and how to fix them. The\nbehaviour of the loss actually tends to be a pretty reliable metric when training PINNs\nin an unsupervised way, and is actually more reliable then for standard neural networks.\nThis is because a PINN follows a given physical law, and the loss quantifies how well its\noutput follow said law. It does not have a set of training points it might memorize instead\nof learning the underlying law. As such, overfitting on some training set is not a risk and it\ndoes not make sense to build a training, validation and test sets from a dataset. In spite of\nthis, due to the complexity of the loss function just looking at the full loss does not tell the"}, {"title": "3.1. Partial losses", "content": "The first thing to look at when the PINN does not converge is to try to understand\nwhich loss or losses is failing to converge. At this point, the first thing one can do to try\nand improve the training can be to try to either increase the weight for the losses that\nare struggling to converge or reduce the weights for other, competing losses. One very\ncommon problem is the orthogonality loss failing to converge while other losses get very\nclose 0. This means that the network is predicting an already discovered eigenfunction 2.\nFor this increasing the weight of the orthogonality loss is usually a pretty safe solution.\nAnother common occurrence is the normalization loss staying high while all other losses\nbecome 0. This usually means that the network has converged to the local minima y(x) = 0\n\u2200\u1eef. While it could be fixed by increasing the normalization loss' weight 3 it can also be\nuseful to change the initialization of the network, for instance going from Xavier uniform\n[28] to Kaiming normal [29], in order to start the network further from this local minima.\nThis could also be a sign of the energy minimization loss having excessive weight or\ngrowing too quickly. A way to decide this in low dimensions is to display the plot of the\nbest wave function (i.e., the one with the lowest loss) every number of epochs, and look\nfor any pathological behaviour in its evolution. Another example is the case in which the\nboundary condition loss seems to fail to converge: in this case if the plotted wave function\nresembles what we expect for the given potential but does not vanish at the border of\nthe domain it might mean that a larger domain is needed. It is important to note that a\nplateauing loss is not always a sign of a failed convergence. While training it might happen\nthat the PINN's loss remains stuck around a certain value for thousands of epochs but\nwill start decreasing again after some time. On the other hand, wild oscillations of the\nloss might be a sign of an excessively high learning rate. Note that these are only some\nempirical heuristics and might not apply in all cases. It is important to look at the loss'\nbehaviour in a case-by-case basis and adjust the weights depending on what is best in the\ncurrent circumstances."}, {"title": "3.2. Evaluation metrics", "content": "In the previous section only the losses have been utilized to understand how successful\nthe training has been. However, depending on the problem other, more informative, metrics\nmight be available. In this section two metrics that allow to gauge the goodness of PINN's\nsolution for the Schr\u00f6dinger equation are detailed. When a solution is already known\ndue to different numerical methods or because the potential has an analytic solution, the\ncorrectness of the PINN's output can be easily evaluated by utilizing fidelity [30-32] and\nrelative error on the energy. To gauge the eigenvalue's correctness the relative error for the\nenergy is used:\n$err_e$ = $\\frac{|E_{Ex} - E_{PINN}|}{E_{Ex}}$       (16)\nwhere $E_{Ex}$ is the analytic value of the energy, while $E_{PINN}$ is the one predicted by the\nnetwork. On the other hand to evaluate the eigenvector the fidelity between the predicted\nwave function and the exact one is employed:\n$F_\u03c8$ = |\u3008\u03a8Ex|\u03a8\u03a1\u0399\u039dN)|$2$       (17)"}, {"title": "4. Examples", "content": "Some examples of quantum mechanical systems that can be solved by using PINNs\nare now shown: the Infinite potential well and the particle in a ring."}, {"title": "4.1. Infinite potential well", "content": "In this section we study the infinite potential well. This system has been chosen due to\nits extreme simplicity, making it an excellent testbed to build the basis of the method. We\nare studying, in particular, the infinite potential well $[-\\frac{L}{2}, \\frac{L}{2}]$, with L = 3:\n$\\begin{cases}\nV(x) = 0  \\text{for} x \\in [-\\frac{L}{2}, \\frac{L}{2}]\\\\\nV(x) = \\infty  \\text{otherwise}\n\\end{cases}$       (19)\nAs mentioned in section 2.4 this potential can be represented just by studying the region\nwith V(x) = 0 with Dirichlet boundary conditions. Therefore, based on the previous\nsections, the neural network takes as input the position x and outputs the wave function\n\u03c8(x) and auxiliary output v(x). The network's losses will be:\nIntegral loss: $\\frac{\\delta v(x')}{\\delta x'} |_{x'=x}$ = $|\u03c8(x)|^2$\nNormalization loss: v(-$\\frac{L}{2}$) = 0, v($\\frac{L}{2}$) = 1\nBoundary conditions loss: \u03c8(-$\\frac{L}{2}$) = \u03c8($\\frac{L}{2}$) = 0\nEnergy minimization loss: $e^{\\alpha(E_{PINN} - E_{init})}$ = 0, with \u03b1 = 0.8; $E_{init}$ = 0 when n = 0\nOrthogonality loss: \u03a3\u3008\u03c6(x)|\u03c8i(x)) = 0\nDifferential equation loss: -$\\frac{1}{2} \\frac{d^2\u03c8(x)}{dx^2}$ - E\u03c8(x) = 0      (20)\nTo those losses an inductive bias can be added, since the potential is symmetric\nwith respect to 0. Due to this symmetry, The wave functions that solve the equation\nare alternatively symmetric and anti-symmetric with respect to 0. This condition can be\nenforced by adding the loss:\n\u03c8(x) \u2013 s\u03c8(\u2013x) = 0       (21)\nwhere s is a parameter that assumes the value one if we are looking for a symmetric state\n(n even), or s = \u22121 if we are looking for an anti-symmetric one (n odd). For the sake of\nclarity, it is worth mentioning that the value of s starts at one and is switched every time a\nnew model is trained.\nThe weight for each of these losses are summarized in table 1. Note how they follow the\nheuristics in Section 2.8."}, {"title": "4.2. Particle in a ring", "content": "In this Section we show another example of potential: the particle in a ring. In this case,\nthe potential is 0 on a ring of radius L, and \u221e everywhere else. Using polar coordinates, the\nwave function depends only on the angular coordinate \u03b8. The shape of the potential actually\nbecomes the same as the infinite potential well of Section 4.1, with the difference that instead\nof having Dirichlet boundary conditions it requires periodic boundary conditions, that is:\n\u03c8(0) = \u03c8(2\u03c0)       (23)\nOr, more in general, we require:\n\u03c8(\u03b8) = \u03c8(\u03b8 + 2\u03c0)        (24)\nFurthermore, the change of variable makes the gradient assume the following form:\n$\\frac{1}{L^2} \\frac{d^2}{d\\theta^2}$       (25)\nThis makes the analytical form of the solutions:\n\u03c8(\u03b8) = $\\frac{1}{\\sqrt{2\\pi}}$ $e^{in\u03b8}$\n$E_n$ = $\\frac{n^2}{2L^2}$ where n = 0,\u00b11, \u00b12, \u00b13,...        (26)\nThis actually introduces two significant challenges in training the PINN: first of all,\n\u03c8(\u03b8) will in general be a complex number, which means that the main output of the"}]}