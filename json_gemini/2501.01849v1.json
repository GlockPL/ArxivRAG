{"title": "Multi-Agent Conversational Online Learning for Adaptive LLM Response Identification", "authors": ["Xiangxiang Dai", "Yuejin Xie", "Maoli Liu", "Xuchuang Wang", "Zhuohua Li", "Huanyu Wang", "John C.S. Lui"], "abstract": "The remarkable generative capability of large language models (LLMs) has sparked a growing interest in automatically generating responses for different applications. Given the dynamic nature of user preferences and the uncertainty of LLM response performance, it is crucial to design efficient online learning algorithms to identify optimal LLM responses (i.e., high-quality responses that also meet user preferences). Most existing online algorithms adopt a centralized approach and fail to leverage explicit user preferences for more efficient and personalized LLM response identification. In contrast, this paper introduces MACO (Multi-Agent Conversational Online Learning for Adaptive LLM Response Identification): 1) The online LLM response identification process is accelerated by multiple local agents (such as smartphones), while enhancing data privacy; 2) A novel conversational mechanism is proposed to adaptively conduct conversations for soliciting user preferences (e.g., a preference for a humorous tone over a serious one in generated responses), so to minimize uncertainty in preference estimation. Our theoretical analysis demonstrates that MACO is near-optimal regarding cumulative regret. Additionally, MACO offers reduced communication costs and computational complexity by eliminating the traditional, computing-intensive \u201cG-optimal design\" found in previous works. Extensive experiments with the open LLM Llama, coupled with two different embedding models from Google and OpenAI for text vector representation, demonstrate that MACO significantly outperforms the current state-of-the-art in online LLM response identification.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) have swiftly transformed the technological landscape of our society [1], [2]. A significant line of research is the exploration of prompts to identify optimal responses from LLMs [3]. This approach is compelling since it does not need to alter the internal parameters of an LLM, and can align well with human conversational patterns. Consequently, there is a growing interest in automatically identifying LLM responses, e.g., through prompt engineering methods [4], [5], [6]. These efforts aim to enhance LLMs' capability to produce more accurate and relevant responses, collectively referred to as \"LLM response identification\u201d. Note that these prompt engineering methods are done offline, and only provide a \u201cinitiatory set of relatively good responses\u201d by pre-specified prompt instructions. However, considering the diversity of responses generated by LLMs and the uncertainty in LLM performance, identifying the most suitable LLM response is inherently challenging [7], [8], as suitable responses are usually unknown in advance and context-dependent. Therefore, continuous online response adaptation is necessary [9], especially in scenarios such as medical diagnosis where highly accurate answers are required. Note that the online response identification approach can enhance the initiatory set of offline-generated responses so to match the specific context.\nFurthermore, previous research has often overlooked the need to address diverse user preferences. It is crucial to not only ensure the quality of responses generated by LLMS, but also to tailor them to meet the specific preferences and expectations of different users. For instance, some users may prefer LLM-generated responses to be humorous, while others might prefer a more formal tone. Although [10] considers the optimization of preferences for LLMs, it only addresses the binary case of users' likes and dislikes. LLM response identification must address the growing demand to cater to diverse user preferences. To address such needs, one can utilize cloud servers to continuously learn and refine LLM response identification by collecting feedback on the assessment of LLM responses. This feedback can be derived from users' direct input or measurement of score functions [11], [12]. A response that not only meets quality standards but also aligns with user preferences is termed an \u201coptimal LLM response.\u201d"}, {"title": "A. Multi-Agent Conversational Properties", "content": "In the context of LLM response identification, we observe two significant properties in typical LLM application scenarios. These properties inform and motivate our proposed formulation.\nFirst, in the utilization of LLMs, users commonly access LLM services across multiple devices, such as smartphones, tablets, and desktops, collectively referred to as \u201clocal agents.\u201d For example, the Poe AI chatting platform [13] handles user queries originating from various devices. Leveraging this multi-"}, {"title": "B. Challenges and Our Contributions", "content": "To adaptively identify the appropriate LLM responses, which were generated from an initiatory set of responses generated through offline prompt engineering techniques, we propose to utilize online contextual bandit approaches, where a sequential decision-making cloud server selects LLM responses (i.e., an arms corresponds to a response) for users and receives feedback. Besides the arm-level feedback, the cloud server can occasionally prompt users with questions about key terms [17], [18]. For example, asking about the user's preference on a category: \"Are you interested in news about basketball?\", or asking about the user's preference on an entity: \"Do you like to read news related to LeBron James?\". The feedback from key terms like \u201cbasketball\" and \"LeBron James\" can reflect user preferences, allowing the cloud server to accelerate the learning process. The objective is to develop an online adaptive strategy that maximizes user satisfaction over the long term. However, the current works of conversational contextual bandit algorithms fall short of addressing the unique challenges of online adaptive LLM response identification:\nFirstly, existing bandit models that account for user preferences are predominantly employed in recommendation systems [18], [19], [20]. These models typically utilize Singular Value Decomposition (SVD) to extract feature vectors of comparatively lower dimensions. However, quantifying features from LLM text responses, which contain complex semantic information and lead to much higher dimensional feature spaces, presents significant computational challenges.\nSecondly, previous conversational bandit works primarily follow the framework by [21], which addresses the infinitely arms. However, the number of LLM responses that need online identification from an initiatory set of responses generated via prompt engineering is typically finite. While elimination-"}, {"title": "II. SYSTEM MODEL", "content": "This section formulates the multi-agent conversational bandit for online LLM response identification."}, {"title": "A. Online LLM Response Identification", "content": "We define the set of local agents as M with $|M| = M$, which represent devices such as smartphones, laptops, and tablets. For any local agent $m \\in M$, the finite arm set of LLM responses is denoted as $A_m$, which represents possible responses generated from various prompts. Given the heterogeneity of agents, different local agents may have different arm sets, which is different from the assumption in [26] that all local agents share the same arm set. As mentioned in Section I, traditional offline techniques (e.g., prompt engineering) can help to construct a set of initial responses, but due to the diversity of LLM outputs and user preferences, it is essential to adaptively fine-tune the optimal response online, despite having an offline initiatory set of LLM responses. Our model adopts a time-slotted approach, denoted by discrete-time rounds $T = \\{1, 2, 3, ...,T\\}$, where each local agent selects one arm, i.e., LLM response, at each round $t \\in T$."}, {"title": "B. Multi-Agent User-Personalized Bandits", "content": "We consider a multi-agent conversational bandit setting involving M agents and a cloud server. At each round $t \\in T$, a local agent $m \\in M$ selects an arm $a_{m,t} \\in A_m$, which denotes one possible LLM response, and receives reward feedback $r_{m,t}$ that reflects the corresponding performance. Eliciting user feedback is beyond the scope of this work. Here, the term \"feedback\" broadly encompasses direct user input, data inferred from techniques that measure user behavior, and preference simulators [12]. The user's preference for LLM responses is represented by an \"unknown\" preference feature vector $\\theta^* \\in \\mathbb{R}^d$, which all local agents aim to learn. For a local agent $m\\in M$, considering both the impact of the LLM response (i.e., arm $a_{m,t} \\in A_m$) and the unknown user preference $\\theta^*$, the reward can be expressed as a linear combination with a noise term $\\eta_{m,t}$: $r_{a_{m,t}} = (x_{a_{m,t}}, \\theta^*) + \\eta_{m,t}$, where $x_{a_{m,t}} \\in \\mathbb{R}^d$ is the embedding feature vector the corresponding arm $a_m$, to capture the textual information [1], [3]. We will demonstrate the generalization of our model using two different open embedding approaches in Section V. Our objective is to design a policy that selects arms (i.e., LLM responses) each round to minimize cumulative regret, defined as the difference between the cumulative rewards of our policy and the best unknown policy across all local agents, tailored to personalized user preferences, which is defined as:\n$$R_M(T) = \\sum_{m=1}^{M} \\sum_{t=1}^{T} (x^*_{a_{m}} \\theta^* - x_{a_{m,t}} \\theta^*).$$\nwhere $a^*_{m} \\in arg\\max_{a \\in A_m} x^*_a \\theta^*$ denotes the locally optimal arm with the highest expected reward at local agent $m \\in M$. This regret definition follows prior works [21], [17], [18]."}, {"title": "C. Conversational Contextual Mechanism", "content": "In addition to obtaining feedback by selecting arms on suitable LLM responses, the cloud server can occasionally query users from each local agent for feedback to better estimate user preferences. However, relying solely on directly considering all answers can lead to inefficiencies due to the issue of information dispersion. Specifically, the contextual vectors of different answers may vary significantly, even if they share similarities at an abstract level. For instance, responses about \u201csyntax rules,\u201d \u201cbest practices,\u201d or \u201ccompiler optimizations\u201d may all relate to \"C/C++,\" but their contextual representations can differ greatly. Similarly, responses with a \"humorous tone\" could vary between \"lighthearted,\u201d \u201csarcastic,\" or \"playful\" expressions. To address this issue, we introduce \"key terms\" to represent core topics or features of user interests from [17], [18]. A key term groups multiple related arms under a single concept. For example, the key term \u201cC/C++\u201d can encompass responses about \u201csyntax rules,\u201d \u201cbest practices,\u201d and \u201ccompiler optimizations,\u201d while the key term \u201chumorous tone\u201d might include responses that are \"lighthearted,\u201d \u201csarcastic,\u201d or \"playful.\u201d Feedback on a key term propagates to its related arms, enabling the system to infer preferences across multiple responses with minimal interaction."}, {"title": "III. ALGORITHM DESIGN", "content": "We present the design of multi-agent conversational online learning (MACO) algorithms, implemented by local agents and a cloud server for adaptive identifying LLM response. Then, we compare our design to the traditional phase elimination-based online learning algorithm [23].\nFor any real vector $x$ and a positive semi-definite matrix $M$, let $||x||_M := \\sqrt{x^T M x}$. Denote the cardinality of a set $A$ as $|A|$. We introduce the notation $[z] := \\{1, ...,z\\}$ for $\\forall z \\in \\mathbb{N}^+$. Define $T_{m,a}$ as the set of rounds where local agent $m$ selects arm $a$ in phase $P$, $T_{m,k}$ as the set of rounds when agent $m$ conducts interaction on key term $k$ in the same phase, and $\\mathcal{A}$ (where $\\mathcal{A} \\leq |A|$) as the size of actually pulled arms from the LLM response set at each round."}, {"title": "A. MACO Algorithm on Local Agent", "content": "As outlined in Algorithm 1, which is executed by the local agents and referred to as MACO Agent (MACO-A), the online process of handling and updating information for LLM response identification within the multi-agent system operates as follows. Initially, the local agent $m\\in M$ computes the information matrix $M_m$ from its active arm set $A_m$ (which is later updated in Line 15) during each phase $p$. Specifically, $M_m$ is calculated as $M_m := \\sum_{a \\in A_m} x_a x_a^T$, which refines the model's ability to adapt to LLM responses by analyzing the principal directions in the feature space (Line 2). The eigenvalue $\\lambda_v$ of its eigenvector $v$ represents the variance captured along its direction, with higher values indicating richer information, which is essential for the precise estimation of $\\theta^*$. Following this, the local agent $m$ diagonalizes its information matrix $M_m = \\sum_{j=1}^{d} \\lambda_v v v^T$, examining all principal directions in the feature space (Line 3). If an eigenvalue $\\lambda_j$ falls below the threshold $h_p := \\frac{4(1-2^{-2p})d}{3}$, whose value is determined by Lemma 1 in Section IV, the local agent $m$ uploads the corresponding eigenvector to the cloud server (Line 4). This mechanism helps to address under-explored areas of the feature space, enhancing the accuracy in selecting LLM responses.\nThe cloud server processes the uploaded information and returns a set of key terms $K_m$ along with the required repetition times $\\{n_{m,k}\\}_{k\\in K_m}$ (Line 5). The local agent $m$ then engages in conversations with these key terms while pulling arms the requisite number of times, to ensure robust exploration of LLM"}, {"title": "B. MACO Algorithm on Cloud Server", "content": "Next, we present the part of the MACO algorithm, which is executed on the cloud server, called MACO Server (MACO-S). As mentioned in Section I, a significant challenge arises from the heterogeneity of local agents in the multi-agent conversational bandits model. This diversity can hinder effective data aggregation, potentially leading to suboptimal estimation of the user preference vector $\\theta^*$. To address this issue, the cloud server employs a strategic approach using key terms to probe and enrich the information in underrepresented directions of the feature space, thereby enhancing the overall accuracy of the estimation process.\nAs detailed in Algorithm 2, the cloud server first receives eigenvectors representing directions with insufficient information about the LLM response space from each local agent (Line 7). Utilizing these insights, the cloud server identifies and selects key terms by calculating the closest match in terms of the inner product with the underexplored directions. The chosen key term $k \\in K$, along with the designated repetition times $n_{m,k}$, is then communicated back to the respective local agents (Line 8). This targeted intervention allows for focused exploration and refinement of LLM responses related to these key terms. Finally, the cloud server aggregates the enriched data from all local agents. This aggregated data is used to estimate the unknown preference parameter $\\theta^*$ via linear regression, effectively minimizing uncertainty and enhancing the model's ability to predict and adapt LLM responses tailored to user preferences (Lines 10-11). Moreover, $G$ can also be initialized as an identity matrix to ensure invertibility, especially when the dimension $d$ is large."}, {"title": "C. Comparative Analysis", "content": "Generally, as mentioned in Section I, the number of LLM responses needing online identification from an initial set generated by prompt engineering is typically finite. Therefore, we employ phase elimination-based algorithms for linear bandits, referred to as PE-Lin, instead of the classical con-versational bandit framework proposed by [17]. This choice is motivated by the better performance guarantees of PE-Lin under finite arm sets. Our work builds upon and improves the classical PE-Lin [23]. In PE-Lin, a learning agent always estimates the unknown preference vector $\\theta^*$ using optimal least squares design. Specifically, the algorithm minimizes prediction variance by implementing the computing-intensive G-optimal design, a probability distribution over the arm feature vector $\\mathcal{X} \\subset \\mathbb{R}^d$ (represented by distribution policy $\\pi : \\mathcal{X} \\rightarrow [0,1]$), to ensure minimal variance $g(\\pi)$. The conditions are defined as [29]:\n$$\\sum_{x \\in \\mathcal{X}} \\pi(x) = 1, \\quad M(\\pi) = \\sum_{x \\in \\mathcal{X}} \\pi(x) xx^T,$$$$\ng(\\pi) = \\max_{x \\in \\mathcal{X}} ||x||^2_{M(\\pi)^{-1}} = d.$$\nThen the learning agent plays arms according to the policy $\\pi$ for local agent $m$ at phase $p$, estimates the unknown parameter $\\theta^*$, and eliminates inferior arms accordingly. As noted in [22], there is currently no efficient algorithm for computing the G-optimal design in the multi-agent scenario.\nWe avoid using G-optimal design by leveraging the inherent multi-agent heterogeneity in LLM application, combined with an adaptive conversational mechanism to address this issue. MACO eliminates the need for the resource-intensive G-optimal design, thereby significantly reducing computation time and resources. Additionally, merely executing PE-Lin independently on each local agent with subsequent data aggregation by the server cloud may fail to minimize regret efficiently. This is because different agents may have distinct LLM response sets, resulting in a trivial regret bound of $\\tilde{O}(M\\sqrt{dT})$, which is equivalent to running PE-Lin on each agent without any direct communication. In contrast, our algorithm improves the regret upper bound to $\\tilde{O}(\\sqrt{dMT})$ via efficiently utilizing the conversation to aggregate the information from different local agents, which will be detailed in Section IV."}, {"title": "IV. PERFORMANCE ANALYSIS", "content": "This section presents the theoretical results of MACO, including its cumulative regret, communication costs, and con-versation frequency. In line with common practices in [21], [20],"}, {"title": "A. Main Results", "content": "We first present a \"new technical condition\" that addresses general issues related to feature space coverage.\nCondition 1 (Feature Space Coverage). We say a key term set K as sufficiently rich for covering the feature space if, for any unit vector $v \\in \\mathbb{R}^d$, there exists a key term $k \\in K$ such that its feature vector $x_k$ satisfies $x_k^T v \\geq \\beta$, where $\\beta\\in (0,1]$ is a positive coverage parameter close to 1.\nRemark 1. Condition 1 is crucial for ensuring the compre-hensive distribution of key terms across the feature space, which can facilitate effective uncertainty minimization for each local agent. This condition is easily met if the key term set K includes an orthonormal basis of $\\mathbb{R}^d$. Condition 1 enables us to sidestep the G-optimal design procedure, typically employed in traditional elimination-based algorithms to minimize maximum prediction variance, as described in [23].\nFor sufficiently rich key term sets, based on Condition 1, we provide the following theorems.\nTheorem 1 (Regret Bounds). For the cumulative regret defined in Eq. 1, we have the following upper bound and lower bound:\n1) Upper Bound: With probability at least 1 \u2013 \u03b4, the regret is bounded above by $O(\\sqrt{dMT} \\log \\frac{AM}{\\delta} \\log \\frac{T}{\\delta})$.\n2) Lower Bound: For any policy that selects at most one key term per round, there exists an instance where the policy incurs an expected regret of at least $\\Omega(\\sqrt{dMT})$.\nRemark 2. The regret bounds established in Theorem 1 reveal important insights into the performance of our approach:\n\u2022\nWhen M = 1, the problem simplifies to single-agent con-versational bandits, reducing the regret to $O(\\sqrt{dT})$. This reduction outperforms previous regret upper bound results of $\\tilde{O}(d\\sqrt{T})$ from studies such as [19], [17], by leveraging phase elimination on finite arm sets. This improvement is particularly significant in high-dimensional LLM response feature vectors.\n\u2022\nFor multi-agent systems, our upper bound result aligns with the nearly optimal results described in [22], [24], while eliminating the reliance on computationally intensive G-optimal design, thereby speeding up the online process.\n\u2022 Collectively, the regret upper and lower bound indicate that MACO is minimax optimal up to a logarithmic factor [23], aligning closely with the theoretical regret bounds in multi-agent conversational bandits scenarios.\nTheorem 2 (Communication Cost). The total communication cost scales in $O(d^2 M \\log T)$ for MACO algorithm.\nRemark 3. The communication cost of our algorithm MACO is notably independent of the arm pool size A, which can range into thousands based on the diversity of candidate LLM responses. This contrasts with the approach described in"}, {"title": "B. Technical Analysis", "content": "We now provide an analysis of the upper bound in Theorem 1. Proofs for other theorems can be found in Appendices C to E. Below, we present two critical lemmas related to the design of our multi-agent conversational bandit algorithm. Lemma 1 guarantees that for any local agent $m$, the smallest eigenvalue of the information matrix, adjusted for conversational feedback, remains above $h_p$. This supports the design of line 4 in Algorithm 1. Lemma 2 ensures that the algorithm operates within established error limits, which is essential for reliable LLM response identification.\nLemma 1 (Stability of the Information Matrix). For any local agent $m \\in M$ during phase $p$, we have $\\lambda_{min}(M_m) \\geq h_p$, where $M_m = M_m + \\sum_{k \\in K_m} \\frac{\\beta^2}{1-\\beta^2} x_k x_k^T$.\nProof. Please refer to Appendix A for the proof.\nLemma 2 (Reliability of Estimation Error Bounds). Define the \u201cbad\u201d event $\\mathcal{E}$ where any local agent $m$ at phase $p$ has:\n$$\\mathcal{E} = \\{ \\exists m \\in M, a \\in A, |(x_a - x_a^*, \\theta^p - \\theta^*)| > \\frac{2^{-p}}{\\sqrt{M}} \\}.$$The probability of $\\mathcal{E}$ is bounded by $\\delta$, i.e., $\\Pr[\\mathcal{E}] \\leq \\delta$.\nProof. See Appendix B for details.\nNow, consider the \"good\" event $\\mathcal{E}^c$ for agent $m$ at phase $p$. Lemma 2 confirms that the discrepancy for any arm $a$ in $A_m$: $(x_a - x_a^*, \\theta^p) < \\frac{2^{-p+1}}{\\sqrt{M}}$. This, combined with line 15 in Algorithm 1, supports the following lemma on the arm preservation and performance bound under good event $\\mathcal{E}^c$."}, {"title": "V. PERFORMANCE EVALUATION", "content": "In this section, we conduct extensive experiments to demon-strate the effectiveness of our algorithm. The code is accessible at the following link: Code Repository."}, {"title": "A. Experimental Settings", "content": "Embedding Models. We demonstrate our framework's generalization capabilities using two open embedding models: Google's text-embedding-preview-0409 and OpenAI's Text-embedding-3-large, which generate the embedding feature vector $x_a \\in \\mathbb{R}^d$ for the corresponding arm $a$ (i.e., response) to capture text information.\n1) Text-embedding-preview-0409: Google's advanced em-bedding model, which streamlines synthetic training data creation by generating queries and task descriptions [30].\n2) Text-embedding-3-large: OpenAI's new generation em-bedding model, which surpasses its predecessor, though its technical details remain undisclosed [31].\nResponse Settings. We explore the implementation of two response settings using the aforementioned embedding models, based on a real-world dataset and an open-source LLM.\n1) Following the style classification by [32], we gather a comprehensive set of 13 keywords representing diverse styles such as \u201chumorous\" and \"helpful\", each representing a key term. These keyword styles generate 510 unique combinations, each forming an \u201carm\", where each arm represents a potential style of LLM response. Users have varying priorities for different keyword combinations, and their preference vector \u03b8 has the highest cosine similarity with the feature vector x of their most favored keyword style (which is unknown to the algorithms in advance). To generate these feature vectors x for LLM responses and user"}, {"title": "VI. RELATED WORK", "content": "Bandits tackle the exploitation-exploration tradeoff of online decision-making problems [21]. Based on this, conversational contextual linear bandits, introduced by [17], allow the cloud server to obtain user feedback on key terms to elicit preferences, in addition to arm selection. Later studies introduce clustering to avoid labeling efforts [18], integrate knowledge graphs for term selection [27], and compute the barycentric spanner as an efficient exploration basis [19]. Regarding the multi-agent bandit setting under finite arm sets, [26] assumes homogeneous arm sets, and [22] requires the local agents to upload arm sets, increasing costs and privacy concerns, and [24] utilizes the computationally intensive G-optimal design. Unlike existing works, we are the first to extend conversational bandits to multi-agent settings for online LLM response adaptation, with reduced computation resources, where the theoretical analysis can be an independent component.\nResearch on prompt learning for automatically generating suitable LLM responses has made significant progress [4], [37]. However, offline generating methods face challenges like \u201cdata drift,\" emphasizing the need for online approaches to optimize LLM responses [38], [7]. [39] introduces an online non-stationary bandit method across different LLMs. [8] proposes"}, {"title": "VII. CONCLUSION", "content": "This paper presents MACO, a multi-agent conversational online framework designed to identify optimal responses from LLMs while minimizing cumulative regret and aligning with user preferences. The framework consists of local agents (MACO-A) that adaptively manage conversations and response selection, and a cloud server (MACO-S) that aggregates data to learn user preferences efficiently. We have proved that MACO achieves optimal regret bounds, reduces conversations, and enhances computational efficiency. Our extensive evaluations, utilizing open LLMs like Llama and embedding models from Google and OpenAI, confirm that our approach significantly improves performance over traditional methods. Future work could explore clustering similar user preferences and extend-ing beyond the linear reward model to further enhance the adaptability and effectiveness of the MACO framework."}, {"title": "APPENDIX", "content": "Proof. Using the eigenvectors as an orthonormal basis, for any $j \\in [d]$, any key term's $k$ feature vector can be expressed as $x_k = \\sum_{i=1}^{d} c_i v_i = \\sum_{i=1,i\\neq j}^{d} c_i v_i + c_j v_j$, where $x = \\sum_{a=1,i\\neq j}^{d} c_i v_i$ is orthogonal to $v_j$. According to Line 7 of Al-gorithm 2 and Condition 1, we have $v_j > \\beta$ for the selected key term $k$. Therefore, we have $(\\sum_{i=1}^{d} c_i v_i)^T v_j = c_j \\geq \\beta$, and $x_k x_k^T = (c_j v_j + x)(c_j v_j + x)^T = c_j v_j v_j^T + x x^T$. By spectral decomposition and line 4 in Algorithm 1, we have \n$M_m = \\sum_{i=1}^{d} \\lambda_i v_i v_i^T + \\sum_{j: \\lambda_j < h_p} (v_j v_j^T + \\frac{\\beta^2}{1-\\beta^2} x x^T)$.\nThen, $M_m \\geq \\sum_{j=1}^{d} \\lambda_{min} + \\sum_{\\lambda_j < h_p} (h_p - \\lambda_i) v_i v_i^T = \\sum_{j=1}^{d} \\frac{4(1-2^{-2p})d}{3} v_i v_i^T$. The proof concludes by the Loewner order property, stating if A > B, then $\\lambda_i(A) \\geq \\lambda_i(B)$."}]}