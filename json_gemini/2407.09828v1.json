{"title": "Enhancing Semantic Segmentation with Adaptive\nFocal Loss: A Novel Approach", "authors": ["Md Rakibul Islam", "Riad Hassan", "Abdullah Nazib", "Kien Nguyen", "Clinton\nFookes", "Md Zahidul Islam"], "abstract": "Deep learning has achieved outstanding accuracy in medi-\ncal image segmentation, particularly for objects like organs or tumors\nwith smooth boundaries or large sizes. Whereas, it encounters signifi-\ncant difficulties with objects that have zigzag boundaries or are small in\nsize, leading to a notable decrease in segmentation effectiveness. In this\ncontext, using a loss function that incorporates smoothness and volume\ninformation into a model's predictions offers a promising solution to these\nshortcomings. In this work, we introduce an Adaptive Focal Loss (A-FL)\nfunction designed to mitigate class imbalance by down-weighting the loss\nfor easy examples that results in up-weighting the loss for hard examples\nand giving greater emphasis to challenging examples, such as small and\nirregularly shaped objects. The proposed A-FL involves dynamically ad-\njusting a focusing parameter based on an object's surface smoothness,\nsize information, and adjusting the class balancing parameter based on\nthe ratio of targeted area to total area in an image. We evaluated the\nperformance of the A-FL using ResNet50-encoded U-Net architecture on\nthe Picai 2022 and BraTS 2018 datasets. On the Picai 2022 dataset, the\nA-FL achieved an Intersection over Union (IoU) of 0.696 and a Dice Sim-\nilarity Coefficient (DSC) of 0.769, outperforming the regular Focal Loss\n(FL) by 5.5% and 5.4% respectively. It also surpassed the best base-\nline Dice-Focal by 2.0% and 1.2%. On the BraTS 2018 dataset, A-FL\nachieved an IoU of 0.883 and a DSC of 0.931. The comparative studies\nshow that the proposed A-FL function surpasses conventional methods,\nincluding Dice Loss, Focal Loss, and their hybrid variants, in IoU, DSC,\nSensitivity, and Specificity metrics. This work highlights A-FL's poten-\ntial to improve deep learning models for segmenting clinically significant\nregions in medical images, leading to more precise and reliable diagnostic\ntools.", "sections": [{"title": "1 Introduction", "content": "Precise segmentation of affected regions is essential for optimal outcomes in\nrobotic surgery, computer-aided diagnostics, and targeted radiation therapy [24].\nThe targeted region declination is tedious and time consuming. Recent advance-\nment in deep neural network increases the performance of segmentation and it\nis relatively high when the shape of the targeted segmentation region is large\nand comparatively smooth. But the segmentation is challenging when the ob-\njects are with small size or zigzag boundary. In recent time, many segmentation\nnetworks have been proposed to segment the challenging objects more accu-\nrately [1,12,13,17,18,26]. Although the features are extracted with the segmen-\ntation networks from different view, the performance enhancement is minimal.\nSuch challenging objects are common for segmentation in medical imaging.\nOrgans, tumors' shapes are not only uneven but also poses variability person to\nperson. Segmentation networks commonly rely on loss functions such as Dice [23],\nCross-Entropy [19], and Focal [7], which focus mainly on object overlap and the\nentropy between predicted and ground truth masks. However, these loss func-\ntions often fail to consider critical factors like surface boundary characteristics\nand object volume, which are crucial for accurate segmentation of small or ir-\nregularly shaped objects.\nTo solve these issues, we introduce a novel Adaptive Focal Loss (A-FL) that\nconsiders tumor volume information, surface smoothness information, and dy-\nnamically adjusts class balancing parameter. When the objects are ordinary with\nlarge size and smooth surface, the A-FL posses lower loss. In contrast, during\nthe challenging object segmentation, the A-FL offer higher loss and force the\nnetwork to optimize the loss properly. This dynamic adaptive nature of A-FL\nsuccessfully feedback the loss appropriately to the network which leads the higher\nsegmentation accuracy."}, {"title": "2 Related Work", "content": "Binary Cross Entropy (BCE) loss [19] and its variations [5,19,25], are frequently\nutilized for model training within the domain of semantic segmentation [6, 8].\nThe model's performance may be hampered by the simple pixels' gradients being\noverwhelmed by the hard pixels' due to BCE's equal treatment of all pixels. By\ncorrecting the imbalance between positive and negative samples [10,15], or the\ndisparity between easy and hard samples [5, 7], previous initiatives [14] have\nattempted to remedy this.\nWeighted Binary Cross Entropy (WBCE) [3] introduces a weighting factor\nfor positive samples in order to rectify the imbalance between positive and neg-\native data. In a similar manner, positive and negative samples are given weights\nvia Balanced Cross Entropy (BCE) [25]. These techniques are beneficial when\napplied to skewed data distributions [4], but their impact on model performance\nwhen applied to balanced datasets may be less pronounced. An unique solution\nto this problem was offered by Leng et al. [5], who proposed Poly Loss (PL), a\nlinear mix of polynomial functions."}, {"title": "3 Methodology", "content": "The overall working pipeline of this research implementation is illustrated in\nFig.1. It consists of three main components: (a) dataset pre-processing, (b)\nResNet50-encoded U-Net architecture, and (c) proposed A-FL function. Details\non data pre-processing are provided in Section 4. In this section, we provide a\nsummary of A-FL, detail the implementation of our innovative adaptive focal loss\nthat dynamically integrates tumor volume and surface smoothness information,\nand describe the segmentation network architecture."}, {"title": "3.1 Overview", "content": "The core concept of Adaptive Focal Loss (A-FL) is to dynamically calculate and\nthen incorporate the tumor volume and surface smoothness information into\nregular Focal loss function through a focusing parameter for each patient during\nthe training process. A-FL uses dynamically calculated non-cancerous pixel to\ntotal pixels ratio as class balancing parameter, which helps to address the class\nimbalance between the numerous non-cancerous pixels and the comparatively\nfew cancerous pixels. As shown in Fig.1 step c, we introduce two straightforward\nbut highly effective modifications to the regular focal loss function during the\ntraining process:\n1.  During training, we assess tumor surface smoothness by computing the gra-\ndient along the x, y, and z axes, and we also evaluate tumor volume by\ncalculating the ratio of cancerous pixels to the total pixels in the corre-\nsponding label mask. We use this smoothness and volume information as\nfocusing parameter.\n2.  We calculate the ratio of non-cancerous pixels to the total pixel count and\nutilize this ratio as a class balancing parameter.\nTo compute both focusing parameter and class balancing parameter during the\ntraining process we have introduced a novel specifically designed mathematical\nmodel. This approach is optimized for computational efficiency and integrates\nseamlessly into the training pipeline. By calculating these parameters dynami-\ncally during training, the proposed loss function ensures that the segmentation\nmodel can better adapt to variations in tumor sizes and surface characteristics\nthat results in greater accuracy and robustness of the segmentation outcomes."}, {"title": "3.2 Tumor volume & surface smoothness aware Adaptive Focal\nLoss (A-FL)", "content": "We used the regular focal loss function [7] as our baseline to focus on easy/hard\nexamples by reshaping the Balanced Cross Entropy loss with a modulating factor\nthat is based on manually tune-able focusing parameter. Their research showed\nthat a fixed focusing parameter of 2 and class balancing parameter of 0.25 yielded\nthe best results. This approach fails to address the varying difficulty levels within\nthe dataset, particularly struggling with small and irregularly shaped tumors.\nAs a result, the model often fails to achieve optimal segmentation accuracy for\nthese challenging examples.\nWe address these limitations by dynamically adjusting a focusing parameter\nbased on tumor volume and surface smoothness information, and a class bal-\nancing parameter based on the non-cancerous to total pixel ratio. This adaptive\nfocal loss effectively handles class imbalance, able to give more focus on hard\nexamples like small or irregularly shaped tumors. By ensuring higher training\nloss for challenging examples and lower for easy ones, our approach allows the\nmodel to update its weights more effectively, improving dice accuracy for small\nand more zigzag shaped tumors."}, {"title": "1. Calculating Tumor Volume Based Adaptive Parameters:", "content": "To address\nclass imbalance and give more focus to small tumor cases, we dynamically\ncalculate the class balancing adaptive parameter and tumor volume infor-\nmation adaptive parameters using the cancerous and non-cancerous pixels\nratios to the total pixels for each patient's tumor during training. The equa-\ntion in 1 and 2 are the mathematical formula of Class Balancing Adaptive\nParameter ($\\alpha_{va}$) and volume information adaptive parameter."}, {"title": "Calculating Mean Surface Smoothness Adaptive Parameter:", "content": "To\ncompute the mean smoothness of a patient's mask, we perform the following\nsteps.\nI. Gradients along the x, y, and z Axes: Let I be the image tensor. The\ngradients along the x, y, and z axes are denoted as $\\nabla_xI$, $\\nabla_yI$, and $\\nabla_zI$\nrespectively, and the formula can be expressed as in Equation 3.\nII. Gradient Magnitude: Using the Euclidean norm of the gradients [21]\npresented in Equation 4, we calculate the magnitude of the gradient at\neach point along tumor boundary.\nIII. Mean Smoothness: The mean surface smoothness adaptive parameter\n($\\gamma_{msa}$) is calculated as the average of the gradient magnitudes over the\nentire image tensor. Let N be the total number of elements in the image\ntensor and the formula is as follows:"}, {"title": "3. Calculating the Adaptive Focusing Parameter:", "content": "The adaptive parame-\nter $\\gamma_{adaptive}$ is then calculated as the sum of the volume adaptive parameter\n($\\gamma_{va}$) and the mean smoothness adaptive parameter ($\\gamma_{msa}$) as follows:"}, {"title": "4. Defining the Adaptive Focal Loss (A-FL):", "content": "Our proposed A-FL denoted\nas A - FL($P_t$) expands on conventional Focal loss by utilizing dynamically\nadaptive parameter $\\gamma_{adaptive}$. The Equation 7 shows the mathematical for-\nmula of A-FL.\nWe note two key properties of our adaptive focal loss (A-FL):\n1.  When an example is misclassified and pt is low, the modulating factor stays\nnear 1, keeping the loss unchanged. On the other hand, as pt nears 1, the\nfactor reduces to 0, thus down-weighting the loss for accurately classified\nexamples.\n2.  The value of the modulating parameter (1 - $P_t$)$^{\\gamma_{adaptive}}$ changes in response\nto variations in pt for all patients. We have investigated whether the value\nof pt varies based on tumor volume and tumor surface smoothness in each\npatient. Thus, we incorporate these two factors (volume and smoothness)\ninto our adaptive focusing parameter $\\gamma_{adaptive}$\nIn practice, we have incorporated the class balancing adaptive parameter ava\nas defined in Equation 1, into our proposed loss function. This inclusion results\nin slightly better accuracy compared to the compared to the non-ava-included\nform. The final A-FL formula is presented in Equation 8."}, {"title": "3.3 Our ResNet50 Encoded U-Net Architecture", "content": "For all our experiments, we utilize ResNet50 [2] as the encoder in the U-Net\narchitecture. This backbone is extensively employed in semantic segmentation\n[11], making it an ideal baseline for comparison and future studies. Integrat-\ning ResNet50 into U-Net encoder (displayed in Fig. 2) significantly boosts the\nnetwork's feature extraction capabilities. The residual blocks in ResNet50 effec-\ntively mitigate the vanishing gradient issue, enabling the network to learn more\nrobust and abstract features."}, {"title": "4 Experiment Setup", "content": ""}, {"title": "4.1 Dataset", "content": "All our experiments use two publicly available MRI datasets: 1) the Picai 2022\ndataset [20] and 2) the BraTS 2018 dataset [9]. Both datasets are designed to"}, {"title": "4.2 Data Preparation", "content": "As mentioned earlier, the Picai-2022 dataset includes three modalities: T2w\nimages, ADC, and HBV maps for each patient. To ensure uniformity, ADC and\nHBV maps are resampled using the maximum voxel spacing (43%) observed\nfrom axial T2W images, resulting in a voxel size of 3.0\u00d70.5\u00d70.5 mm. A two-\nstep normalization procedure is applied, consisting of min-max normalization\nfollowed by z-score normalization. After normalization, all images, labels, and\nprostate whole gland masks are resized to 30 x 256 x 256. A bounding box is\ncomputed around the prostate whole gland mask and extended by 30 pixels in\neach direction. The prostate region is then extracted from the T2W, ADC, and\nHBV maps corresponding to the same slice, reducing image and mask size for\nquicker experimentation. These extracted regions are resized to 24 x 160 x 128\nfor segmentation model training. Finally, the N4 bias field correction filter is\napplied to the dataset to reduce bias corruption.\nThe BraTS 2018 dataset consists of 4 modalities (T1-weighted, T1ce, T2-\nweighted, and FLAIR ) for each patient. To ensure pixel dimension unifor-\nmity between these modalities, the 4 modalities are resampled to 1.0\u00d71.0\u00d71.0\nmm/voxel. A two-step normalizing procedure is also implemented here, as Picai\ndataset. After intensity normalization, all image modalities and labels are resized\nto 155 x 224 x 224 along z, x and y directions."}, {"title": "4.3 Experiment Design & Implementation", "content": "We use the Stochastic Gradient Descent (SGD) optimizer [16] for all training\nmethods. The optimizer has a base learning rate of 0.01, a momentum of 0.9, a\nweight decay of 0.0001, and we train for 300 epochs. The training is conducted\nwith a batch size of 1. In order to address the problem of over-fitting, we employ\ndata augmentation methods such as random affine transformations, flipping,\nGaussian noise, and intensity scaling. The experiments are carried out utilizing\nPyTorch 2.3.1 on a high-performance computer configuration, comprising an\nIntel Xeon 2.40 GHz processor, an NVIDIA RTX 3060 GPU, and 32 GB of\nRAM."}, {"title": "4.4 Evaluation Metrics:", "content": "Evaluation metrics are crucial for assessing the performance of segmentation\nmodels. In this study, we utilized four primary metrics: mean IoU, DSC, Sensitiv-\nity, and Specificity which are described in [4]. IoU and Dice Coefficient quantify\nthe overlap between the ground truth and predicted output. Sensitivity mea-\nsures the proportion of True Positives, while Specificity measures the proportion\nof True Negatives. Together, these metrics provide a comprehensive evaluation\nof the model's effectiveness."}, {"title": "5 Result and Discussion", "content": "In this section, we present experiments demonstrating the benefits of integrat-\ning a dynamic focusing parameter and adaptive imbalance weighting into the\nregular Focal Loss (FL) function. We provide quantitative comparisons between\nconventional FL and A-FL across various datasets. Additionally, we conduct\ncomparative analyses of A-FL against FL using different baseline models and\nother loss functions. We also evaluate qualitative examples and perform abla-\ntion studies to assess the impact of our A-FL function."}, {"title": "5.1 Quantitative Results Evaluation", "content": "Tables 1 and 2 present a quantitative comparison between the regular FL and\nour proposed A-FL on the Picai 2022 [20] and BraTS 2018 [9] datasets.\nOn the Picai 2020 dataset, A-FL achieves a 5.5% increase in IoU, a 5.4% rise\nin DSC, and a 1.7% boost in Sensitivity, demonstrating improved handling of\nsmall and irregular tumors. Additionally, a slight increase in Specificity (0.05%)\nindicates balanced performance.\nOn the BraTS 2018 dataset, A-FL shows a 5.2% improvement in IoU and a\n3.8% increase in DSC, reflecting better segmentation accuracy. Despite a minor\ndecrease in Sensitivity (1.2%), the gain in Specificity (1.1%) suggests fewer false\npositives and a more robust architecture.\nthe results show A-FL's superiority in key metrics: IoU, Dice coefficient, and\nSensitivity. Specifically, A-FL achieves an IoU of 0.696 and a Dice coefficient of\n0.769, outperforming other loss functions such as Traversky Loss (IoU: 0.654,\nDice: 0.726), Cross Entropy Loss (IoU: 0.630, Dice: 0.705), IoU Loss (IoU: 0.654,\nDice: 0.727), Dice Loss (IoU: 0.665, Dice: 0.739), Dice Cross Entropy Loss (IoU:\n0.670, Dice: 0.742), and Dice Focal Loss (IoU: 0.685, Dice: 0.757).\nA-FL also achieves the highest Sensitivity at 0.951, indicating its accuracy in\nidentifying true positives. Compared to the best-performing baseline, Dice Focal\nLoss, A-FL improves IoU by 1.61%, Dice coefficient by 1.58%, and Sensitivity\nby 6.14%. Although A-FL's Specificity (0.948) is slightly lower than Dice Loss\n(0.952), it maintains high performance overall."}, {"title": "5.2 Qualitative Results Evaluation", "content": "The qualitative results for the Picai dataset, shown in Fig. 4, illustrate that\nA-FL outperforms baseline FL in prostate cancer (PCa) segmentation. While\nlarge volume and smooth surface cases (i) and (iii) show similar performance\nfor both methods, A-FL excels in segmenting uneven or small volume tumors\n(cases (ii) and (iv)). This is evident in the 2D slice-by-slice visualizations, where\nA-FL demonstrates higher accuracy due to the dynamically adjusted focusing\nparameter.\nIn summary, the visual results in Fig. 4 support our quantitative findings,\nhighlighting A-FL's superior performance in managing various tumor volumes\nand surface complexities."}, {"title": "5.3 The Ablation Studies", "content": "To assess the effectiveness of proposed dynamically tumor volume and smooth-\nness adaptive focal loss (A-FL), twelve different experiments are conducted and"}, {"title": "6 Conclusion", "content": "This paper introduces A-FL, a novel Adaptive Focal Loss (A-FL) function tai-\nlored for semantic segmentation, specifically addressing tumor volume and sur-\nface smoothness considerations. A-FL improves upon traditional focal loss by dy-\nnamically adjusting focusing and balancing parameters at the pixel level during\ntraining. This adaptation allows our models to achieve more balanced and precise\nsegmentation performance by integrating tumor volume and surface smoothness\nas focal parameters, while also considering background volume for class bal-\nlancing. Experimental evaluations conducted on the Picai and BraTS datasets\nusing ResNet50-based U-Net architecture demonstrate the superior performance\nof A-FL compared to conventional focal loss methods."}]}