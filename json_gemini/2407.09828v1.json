{"title": "Enhancing Semantic Segmentation with Adaptive Focal Loss: A Novel Approach", "authors": ["Md Rakibul Islam", "Riad Hassan", "Abdullah Nazib", "Kien Nguyen", "Clinton Fookes", "Md Zahidul Islam"], "abstract": "Deep learning has achieved outstanding accuracy in medical image segmentation, particularly for objects like organs or tumors with smooth boundaries or large sizes. Whereas, it encounters significant difficulties with objects that have zigzag boundaries or are small in size, leading to a notable decrease in segmentation effectiveness. In this context, using a loss function that incorporates smoothness and volume information into a model's predictions offers a promising solution to these shortcomings. In this work, we introduce an Adaptive Focal Loss (A-FL) function designed to mitigate class imbalance by down-weighting the loss for easy examples that results in up-weighting the loss for hard examples and giving greater emphasis to challenging examples, such as small and irregularly shaped objects. The proposed A-FL involves dynamically adjusting a focusing parameter based on an object's surface smoothness, size information, and adjusting the class balancing parameter based on the ratio of targeted area to total area in an image. We evaluated the performance of the A-FL using ResNet50-encoded U-Net architecture on the Picai 2022 and BraTS 2018 datasets. On the Picai 2022 dataset, the A-FL achieved an Intersection over Union (IoU) of 0.696 and a Dice Similarity Coefficient (DSC) of 0.769, outperforming the regular Focal Loss (FL) by 5.5% and 5.4% respectively. It also surpassed the best baseline Dice-Focal by 2.0% and 1.2%. On the BraTS 2018 dataset, A-FL achieved an IoU of 0.883 and a DSC of 0.931. The comparative studies show that the proposed A-FL function surpasses conventional methods, including Dice Loss, Focal Loss, and their hybrid variants, in IoU, DSC, Sensitivity, and Specificity metrics. This work highlights A-FL's potential to improve deep learning models for segmenting clinically significant regions in medical images, leading to more precise and reliable diagnostic tools.", "sections": [{"title": "Introduction", "content": "Precise segmentation of affected regions is essential for optimal outcomes in robotic surgery, computer-aided diagnostics, and targeted radiation therapy [24]. The targeted region declination is tedious and time consuming. Recent advancement in deep neural network increases the performance of segmentation and it is relatively high when the shape of the targeted segmentation region is large and comparatively smooth. But the segmentation is challenging when the objects are with small size or zigzag boundary. In recent time, many segmentation networks have been proposed to segment the challenging objects more accurately [1,12,13,17,18,26]. Although the features are extracted with the segmentation networks from different view, the performance enhancement is minimal. Such challenging objects are common for segmentation in medical imaging. Organs, tumors' shapes are not only uneven but also poses variability person to person. Segmentation networks commonly rely on loss functions such as Dice [23], Cross-Entropy [19], and Focal [7], which focus mainly on object overlap and the entropy between predicted and ground truth masks. However, these loss functions often fail to consider critical factors like surface boundary characteristics and object volume, which are crucial for accurate segmentation of small or irregularly shaped objects.\nTo solve these issues, we introduce a novel Adaptive Focal Loss (A-FL) that considers tumor volume information, surface smoothness information, and dynamically adjusts class balancing parameter. When the objects are ordinary with large size and smooth surface, the A-FL posses lower loss. In contrast, during the challenging object segmentation, the A-FL offer higher loss and force the network to optimize the loss properly. This dynamic adaptive nature of A-FL successfully feedback the loss appropriately to the network which leads the higher segmentation accuracy."}, {"title": "Related Work", "content": "Binary Cross Entropy (BCE) loss [19] and its variations [5,19,25], are frequently utilized for model training within the domain of semantic segmentation [6, 8]. The model's performance may be hampered by the simple pixels' gradients being overwhelmed by the hard pixels' due to BCE's equal treatment of all pixels. By correcting the imbalance between positive and negative samples [10,15], or the disparity between easy and hard samples [5, 7], previous initiatives [14] have attempted to remedy this.\nWeighted Binary Cross Entropy (WBCE) [3] introduces a weighting factor for positive samples in order to rectify the imbalance between positive and negative data. In a similar manner, positive and negative samples are given weights via Balanced Cross Entropy (BCE) [25]. These techniques are beneficial when applied to skewed data distributions [4], but their impact on model performance when applied to balanced datasets may be less pronounced. An unique solution to this problem was offered by Leng et al. [5], who proposed Poly Loss (PL), a linear mix of polynomial functions."}, {"title": "Methodology", "content": "The overall working pipeline of this research implementation is illustrated in Fig.1. It consists of three main components: (a) dataset pre-processing, (b) ResNet50-encoded U-Net architecture, and (c) proposed A-FL function. Details on data pre-processing are provided in Section 4. In this section, we provide a summary of A-FL, detail the implementation of our innovative adaptive focal loss that dynamically integrates tumor volume and surface smoothness information, and describe the segmentation network architecture."}, {"title": "Overview", "content": "The core concept of Adaptive Focal Loss (A-FL) is to dynamically calculate and then incorporate the tumor volume and surface smoothness information into regular Focal loss function through a focusing parameter for each patient during the training process. A-FL uses dynamically calculated non-cancerous pixel to total pixels ratio as class balancing parameter, which helps to address the class imbalance between the numerous non-cancerous pixels and the comparatively few cancerous pixels. As shown in Fig.1 step c, we introduce two straightforward but highly effective modifications to the regular focal loss function during the training process:\n1.  During training, we assess tumor surface smoothness by computing the gradient along the x, y, and z axes, and we also evaluate tumor volume by calculating the ratio of cancerous pixels to the total pixels in the corre-sponding label mask. We use this smoothness and volume information as focusing parameter.\n2.  We calculate the ratio of non-cancerous pixels to the total pixel count and utilize this ratio as a class balancing parameter.\nTo compute both focusing parameter and class balancing parameter during the training process we have introduced a novel specifically designed mathematical model. This approach is optimized for computational efficiency and integrates seamlessly into the training pipeline. By calculating these parameters dynamically during training, the proposed loss function ensures that the segmentation model can better adapt to variations in tumor sizes and surface characteristics that results in greater accuracy and robustness of the segmentation outcomes."}, {"title": "Tumor volume & surface smoothness aware Adaptive Focal Loss (A-FL)", "content": "We used the regular focal loss function [7] as our baseline to focus on easy/hard examples by reshaping the Balanced Cross Entropy loss with a modulating factor that is based on manually tune-able focusing parameter. Their research showed that a fixed focusing parameter of 2 and class balancing parameter of 0.25 yielded the best results. This approach fails to address the varying difficulty levels within the dataset, particularly struggling with small and irregularly shaped tumors. As a result, the model often fails to achieve optimal segmentation accuracy for these challenging examples.\nWe address these limitations by dynamically adjusting a focusing parameter based on tumor volume and surface smoothness information, and a class balancing parameter based on the non-cancerous to total pixel ratio. This adaptive focal loss effectively handles class imbalance, able to give more focus on hard examples like small or irregularly shaped tumors. By ensuring higher training loss for challenging examples and lower for easy ones, our approach allows the model to update its weights more effectively, improving dice accuracy for small and more zigzag shaped tumors."}, {"title": "Calculating Tumor Volume Based Adaptive Parameters", "content": "To address class imbalance and give more focus to small tumor cases, we dynamically calculate the class balancing adaptive parameter and tumor volume information adaptive parameters using the cancerous and non-cancerous pixels ratios to the total pixels for each patient's tumor during training. The equation in 1 and 2 are the mathematical formula of Class Balancing Adaptive Parameter (${\\alpha _{va}}$) and volume information adaptive parameter.\n$\\alpha_{va} = \\frac{P_{bg}}{P_{fg} + P_{bg}}$ (1)\n$\\gamma_{va} = \\frac{P_{fg}}{P_{fg} + P_{bg}}$ (2)\nwhere $P_{fg}$ represents the count of foreground pixels (non-zero elements) and $P_{bg}$ represents the count of background pixels (zero elements)."}, {"title": "Calculating Mean Surface Smoothness Adaptive Parameter", "content": "To compute the mean smoothness of a patient's mask, we perform the following steps.\nI. Gradients along the x, y, and z Axes: Let $I$ be the image tensor. The gradients along the x, y, and z axes are denoted as $\\nabla_{x}I$, $\\nabla_{y}I$, and $\\nabla_{z}I$ respectively, and the formula can be expressed as in Equation 3.\n$\\nabla_{x}I = \\frac{\\partial I}{\\partial x}, \\nabla_{y}I = \\frac{\\partial I}{\\partial y}, \\nabla_{z}I = \\frac{\\partial I}{\\partial z}$ (3)\nII. Gradient Magnitude: Using the Euclidean norm of the gradients [21] presented in Equation 4, we calculate the magnitude of the gradient at each point along tumor boundary.\n$||\\nabla I|| = \\sqrt{(\\nabla_{x}I)^{2} + (\\nabla_{y}I)^{2} + (\\nabla_{z}I)^{2}}$ (4)\nIII. Mean Smoothness: The mean surface smoothness adaptive parameter (${\\gamma_{msa}}$) is calculated as the average of the gradient magnitudes over the entire image tensor. Let $N$ be the total number of elements in the image tensor and the formula is as follows:\n$\\gamma_{msa} = \\frac{1}{N} \\sum_{i=1}^{N}{||\\nabla I_{i}||}$ (5)"}, {"title": "Calculating the Adaptive Focusing Parameter", "content": "The adaptive parameter ${\\gamma_{adaptive}}$ is then calculated as the sum of the volume adaptive parameter (${\\gamma_{va}}$) and the mean smoothness adaptive parameter (${\\gamma_{msa}}$) as follows:\n${\\gamma_{adaptive}} = {\\gamma_{va}} + {\\gamma_{msa}}$ (6)"}, {"title": "Defining the Adaptive Focal Loss (A-FL)", "content": "Our proposed A-FL denoted as A - FL($P_{t}$) expands on conventional Focal loss by utilizing dynamically adaptive parameter ${\\gamma_{adaptive}}$. The Equation 7 shows the mathematical formula of A-FL.\nA-FL($P_{t}$) = $(1 - P_{t})^{\\gamma_{adaptive}}.log(P_{t})$ (7)\nWe note two key properties of our adaptive focal loss (A-FL):\n1.  When an example is misclassified and $p_{t}$ is low, the modulating factor stays near 1, keeping the loss unchanged. On the other hand, as $p_{t}$ nears 1, the factor reduces to 0, thus down-weighting the loss for accurately classified examples.\n2.  The value of the modulating parameter $(1 - P_{t})^{\\gamma_{adaptive}}$ changes in response to variations in $p_{t}$ for all patients. We have investigated whether the value of $p_{t}$ varies based on tumor volume and tumor surface smoothness in each patient. Thus, we incorporate these two factors (volume and smoothness) into our adaptive focusing parameter ${\\gamma_{adaptive}}$\nIn practice, we have incorporated the class balancing adaptive parameter ${\\alpha_{va}}$ as defined in Equation 1, into our proposed loss function. This inclusion results in slightly better accuracy compared to the compared to the non-${\\alpha_{va}}$-included form. The final A-FL formula is presented in Equation 8.\nA-FL($P_{t}$) = ${\\alpha_{va}} \\cdot (1 \u2013 P_{t})^{\\gamma_{adaptive}}.log(P_{t})$ (8)"}, {"title": "Our ResNet50 Encoded U-Net Architecture", "content": "For all our experiments, we utilize ResNet50 [2] as the encoder in the U-Net architecture. This backbone is extensively employed in semantic segmentation [11], making it an ideal baseline for comparison and future studies. Integrating ResNet50 into U-Net encoder (displayed in Fig. 2) significantly boosts the network's feature extraction capabilities. The residual blocks in ResNet50 effectively mitigate the vanishing gradient issue, enabling the network to learn more robust and abstract features."}, {"title": "Experiment Setup", "content": "All our experiments use two publicly available MRI datasets: 1) the Picai 2022 dataset [20] and 2) the BraTS 2018 dataset [9]. Both datasets are designed to"}, {"title": "Dataset", "content": "All our experiments use two publicly available MRI datasets: 1) the Picai 2022 dataset [20] and 2) the BraTS 2018 dataset [9]. Both datasets are designed to improve cancer diagnosis using deep learning (DL) tools.\nPicai 2022 dataset: This dataset includes 1,500 Bi-parametric MRI (bpMRI) cases with three modalities: Apparent Diffusion Coefficient (ADC), High b-value (HBV), and T2-weighted (T2w). It comprises 1,075 benign or indolent prostate cancer (PCa) cases, 205 unlabeled malignant cases, and 220 manually labeled malignant cases. For segmentation model training, 220 patients are utilized, divided into a training set of 180 patients (80%) and a validation set of 40 patients (20%).\nBraTS 2018 dataset: This dataset includes multi-modal MRI scans from 650 patients, with sequences as Fluid-Attenuated Inversion Recovery (FLAIR), T1-weighted, T1-weighted with contrast enhancement (T1ce), and T2-weighted. Of these, 484 cases are manually labeled. For model training and validation, 384 cases (80%) are allocated for training, and 100 cases (20%) for validation."}, {"title": "Data Preparation", "content": "As mentioned earlier, the Picai-2022 dataset includes three modalities: T2w images, ADC, and HBV maps for each patient. To ensure uniformity, ADC and HBV maps are resampled using the maximum voxel spacing (43%) observed from axial T2W images, resulting in a voxel size of 3.0\u00d70.5\u00d70.5 mm. A two-step normalization procedure is applied, consisting of min-max normalization followed by z-score normalization. After normalization, all images, labels, and prostate whole gland masks are resized to 30 x 256 x 256. A bounding box is computed around the prostate whole gland mask and extended by 30 pixels in each direction. The prostate region is then extracted from the T2W, ADC, and HBV maps corresponding to the same slice, reducing image and mask size for quicker experimentation. These extracted regions are resized to 24 x 160 x 128 for segmentation model training. Finally, the N4 bias field correction filter is applied to the dataset to reduce bias corruption.\nThe BraTS 2018 dataset consists of 4 modalities (T1-weighted, T1ce, T2-weighted, and FLAIR ) for each patient. To ensure pixel dimension uniformity between these modalities, the 4 modalities are resampled to 1.0\u00d71.0\u00d71.0 mm/voxel. A two-step normalizing procedure is also implemented here, as Picai dataset. After intensity normalization, all image modalities and labels are resized to 155 x 224 x 224 along z, x and y directions."}, {"title": "Experiment Design & Implementation", "content": "We use the Stochastic Gradient Descent (SGD) optimizer [16] for all training methods. The optimizer has a base learning rate of 0.01, a momentum of 0.9, a weight decay of 0.0001, and we train for 300 epochs. The training is conducted with a batch size of 1. In order to address the problem of over-fitting, we employ data augmentation methods such as random affine transformations, flipping, Gaussian noise, and intensity scaling. The experiments are carried out utilizing PyTorch 2.3.1 on a high-performance computer configuration, comprising an Intel Xeon 2.40 GHz processor, an NVIDIA RTX 3060 GPU, and 32 GB of RAM."}, {"title": "Evaluation Metrics", "content": "Evaluation metrics are crucial for assessing the performance of segmentation models. In this study, we utilized four primary metrics: mean IoU, DSC, Sensitivity, and Specificity which are described in [4]. IoU and Dice Coefficient quantify the overlap between the ground truth and predicted output. Sensitivity measures the proportion of True Positives, while Specificity measures the proportion of True Negatives. Together, these metrics provide a comprehensive evaluation of the model's effectiveness."}, {"title": "Result and Discussion", "content": "In this section, we present experiments demonstrating the benefits of integrating a dynamic focusing parameter and adaptive imbalance weighting into the regular Focal Loss (FL) function. We provide quantitative comparisons between conventional FL and A-FL across various datasets. Additionally, we conduct comparative analyses of A-FL against FL using different baseline models and other loss functions. We also evaluate qualitative examples and perform ablation studies to assess the impact of our A-FL function."}, {"title": "Quantitative Results Evaluation", "content": "Tables 1 and 2 present a quantitative comparison between the regular FL and our proposed A-FL on the Picai 2022 [20] and BraTS 2018 [9] datasets.\nOn the Picai 2020 dataset, A-FL achieves a 5.5% increase in IoU, a 5.4% rise in DSC, and a 1.7% boost in Sensitivity, demonstrating improved handling of small and irregular tumors. Additionally, a slight increase in Specificity (0.05%) indicates balanced performance.\nOn the BraTS 2018 dataset, A-FL shows a 5.2% improvement in IoU and a 3.8% increase in DSC, reflecting better segmentation accuracy. Despite a minor decrease in Sensitivity (1.2%), the gain in Specificity (1.1%) suggests fewer false positives and a more robust architecture.\nperformance of the proposed Adaptive Focal Loss (A-FL) against several baseline loss functions, using a ResNet50-encoded U-Net model on the Picai 2022 dataset. The results show A-FL's superiority in key metrics: IoU, Dice coefficient, and Sensitivity. Specifically, A-FL achieves an IoU of 0.696 and a Dice coefficient of 0.769, outperforming other loss functions such as Traversky Loss (IoU: 0.654, Dice: 0.726), Cross Entropy Loss (IoU: 0.630, Dice: 0.705), IoU Loss (IoU: 0.654, Dice: 0.727), Dice Loss (IoU: 0.665, Dice: 0.739), Dice Cross Entropy Loss (IoU: 0.670, Dice: 0.742), and Dice Focal Loss (IoU: 0.685, Dice: 0.757).\nA-FL also achieves the highest Sensitivity at 0.951, indicating its accuracy in identifying true positives. Compared to the best-performing baseline, Dice Focal Loss, A-FL improves IoU by 1.61%, Dice coefficient by 1.58%, and Sensitivity by 6.14%. Although A-FL's Specificity (0.948) is slightly lower than Dice Loss (0.952), it maintains high performance overall."}, {"title": "Ablation Studies", "content": "To assess the effectiveness of proposed dynamically tumor volume and smoothness adaptive focal loss (A-FL), twelve different experiments are conducted and presented on Table-5. Each variable and combination of variables is tested with ResNet50 encoded U-Net architecture. The accuracy is measured by mean IoU and mean Dice from the respective dataset.\nIn Table 5, the first row is the baseline results using the original Focal Loss (FL) in where \u03b1 and \u03b3 is manually set to 0.25 and 2 respectively [7]. while the 2nd row shows the results after introducing ${\\alpha_{va}}$ alone which leads to a slight improvements due to assigning higher weights to minority foreground class. On the Picai dataset, IoU increased by 1.3% and Dice by 1.8%, while on the BraTS dataset, IoU increased by 0.7% and Dice by 1.3%.\nCombining the background volume adapting weight ${\\alpha_{va}}$ with the foreground volume adapting weight ${\\gamma_{va}}$ improves results by balancing both background and foreground adjustments, leading to more precise segmentation. This increased IoU by 3.6% and Dice by 3.3% on the Picai dataset, and IoU by 1.5% and Dice by 1.9% on the BraTS dataset. Similarly, combining ${\\alpha_{va}}$ with the most influential parameter of our A-FL, tumor surface mean smoothness adapting weight ${\\gamma_{msa}}$ enhances performance by ensuring more focus regarding more zigzag shaped tumor, increasing IoU by 4.6% and Dice by 4.1% on the Picai dataset, and IoU by 2.6% and Dice by 3.6% on the BraTS dataset.\nThe optimal configuration is achieved when all three parameters are enabled (shows in row 4), resulting in the highest metrics. Compared to the baseline, IoU on the Picai dataset increased by 5.5% and Dice by 5.4%, while on the BraTS dataset, IoU increased by 5.2% and Dice by 4.8%. Conversely, disabling ${\\alpha_{va}}$ while enabling both ${\\gamma_{va}}$ and ${\\gamma_{msa}}$ results in lower performance compared to the optimal configuration. This demonstrates the effectiveness of dynamic parameter adjustments in addressing class imbalance and complex tumor structures."}, {"title": "Conclusion", "content": "This paper introduces A-FL, a novel Adaptive Focal Loss (A-FL) function tailored for semantic segmentation, specifically addressing tumor volume and surface smoothness considerations. A-FL improves upon traditional focal loss by dynamically adjusting focusing and balancing parameters at the pixel level during training. This adaptation allows our models to achieve more balanced and precise segmentation performance by integrating tumor volume and surface smoothness as focal parameters, while also considering background volume for class balancing. Experimental evaluations conducted on the Picai and BraTS datasets using ResNet50-based U-Net architecture demonstrate the superior performance of A-FL compared to conventional focal loss methods."}]}