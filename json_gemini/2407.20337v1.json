{"title": "Contrasting Deepfakes Diffusion via Contrastive\nLearning and Global-Local Similarities", "authors": ["Lorenzo Baraldi", "Federico Cocchi", "Marcella Cornia", "Lorenzo Baraldi", "Alessandro Nicolosi", "Rita Cucchiara"], "abstract": "Discerning between authentic content and that generated by\nadvanced AI methods has become increasingly challenging. While pre-\nvious research primarily addresses the detection of fake faces, the iden-\ntification of generated natural images has only recently surfaced. This\nprompted the recent exploration of solutions that employ foundation\nvision-and-language models, like CLIP. However, the CLIP embedding\nspace is optimized for global image-to-text alignment and is not inher-\nently designed for deepfake detection, neglecting the potential benefits\nof tailored training and local image features. In this study, we propose\nCODE (Contrastive Deepfake Embeddings), a novel embedding space\nspecifically designed for deepfake detection. CoDE is trained via con-\ntrastive learning by additionally enforcing global-local similarities. To\nsustain the training of our model, we generate a comprehensive dataset\nthat focuses on images generated by diffusion models and encompasses a\ncollection of 9.2 million images produced by using four different genera-\ntors. Experimental results demonstrate that CoDE achieves state-of-the-\nart accuracy on the newly collected dataset, while also showing excel-\nlent generalization capabilities to unseen image generators. Our source\ncode, trained models, and collected dataset are publicly available at:\nhttps://github.com/aimagelab/CODE.", "sections": [{"title": "1 Introduction", "content": "Thanks to the increasing generation quality of text-to-image models [44], the\ndissemination of generated visual content has raised concerns about their use for\nmalicious purposes. Indeed, the threat of a scenario in which visually indistin-\nguishable fake images can be easily generated from simple textual descriptions is\nevolving. In this context, deepfake detection methods [34] play a significant role\nin safeguarding society from the risky scenario in which real and fake images are\nno longer distinguishable from the naked eye.\nDeepfake detection approaches have been extensively studied for the case of\nAI-manipulated faces [45,54,58], finding common generation imprints [55] among\ndifferent Generative Adversarial Networks (GANs) [22, 28]. However, with the\nadvent of diffusion models [51], the landscape of image generation has under-\ngone a profound transformation in terms of generation quality and detection\ntechniques. Recent deepfake detection proposals [1,34] have considered the adop-\ntion of general-purpose vision-and-language backbones like CLIP [40], and have\nshown their capability of discriminating images and generalize across generators,\nincluding diffusion-based ones.\nHowever, the CLIP embedding space has been trained on real images only,\nand enforcing global text-to-image similarities through contrastive learning.\nClearly, training on real images only and treating the \"fake\" class as an out-\nlier can improve generalization across different generators, but also comes at the\ncost of limiting the recognition accuracy with respect to a training scenario in\nwhich visual features can be learned on fake images as well. Further, generated\nimages can be distinguished prominently by looking at local and fine-grained de-\ntails, rather than exclusively considering the global context of the image, making\nthe CLIP training paradigm sub-optimal in this context. Lastly, employing pre-\ntrained backbones prevents any architectural deviation from already existing\nmodels. As CLIP backbones employ at least 86 million parameters (in the ViT\nBase [17] configuration), this also has a significant impact on their adoption in\nproduction-ready scenarios where deepfake detection should run in real-time.\nIn light of these considerations, in this paper we propose Contrastive\nDeepfake Embeddings (CoDE), a novel contrastive-based embedding space\nspecifically tailored for distinguishing real and generated images. Our model\nis trained by employing both real and fake images, and by stimulating the learn-\ning of local-to-global correspondences between real and fake counterparts and\nbetween same-class images, in addition to enforcing global similarities. Further,\nby training from scratch on the real-fake distribution, we also reduce the scale\nof the model and find that a ViT Tiny architecture can provide state-of-the-art\nresults while being significantly lighter in computational terms. The resulting\nembedding space can better separate between real and fake samples, and also\nbetter distinguish among different generators, as shown in Fig. 1."}, {"title": "2 Related Work", "content": "Image generation models. Images can be generated with different approaches,\nranging from autoregressive models [16,20,42,59] and generative adversarial net-\nworks [5,9,28,29,52] to diffusion models [2,25,51]. The emergence of latent dif-\nfusion models [15, 33, 39, 43, 44, 47] has significantly propelled the adoption of\ndiffusion-based generators. This is attributed to the heightened efficiency ob-\nserved in both training and inference, achieved by transitioning the generation\nprocess from pixel space to latent space. Notably, this transition maintains state-\nof-the-art performance in the quality of generated images. Furthermore, numer-\nous endeavors in literature are directed towards improving both image quality"}, {"title": "3 Proposed Method", "content": "Given an input image, we tackle the task of classifying whether it was actually\nproduced by a camera (real image) or whether it was generated by a generative\nmodel (fake image). In CoDE, we train a contrastive space for deepfake detection\nwith a pair of losses that encourage features coming from real and fake images\nto be separated in the embedding space, while considering both global and local\nvisual cues. An overview of our approach is shown in Fig. 2.\nAn embedding space for deepfake detection. Our contrastive pre-training\nobjective relies on the Info-NCE framework [35]. It utilizes a large collection of\nfake images generated from web-scraped prompts, paired with a collection of real\nimages having the same size. Given a minibatch $B = (\\{R_i\\}_{i=1}^{N}, \\{F_i\\}_{i=1}^{N})$, where\n$\\{R_i\\}_{i=1}^{N}$ and $\\{F_i\\}_{i=1}^{N}$ denote, respectively, randomly sampled sets of real and fake\nimages, the contrastive objective encourages real and fake images to lie separated\nin a shared embedding space, while ensuring invariance to image transformation.\nImages $R_i$ and $F_i$ are processed by a learnable Vision Transformer [17] $F$ to get"}, {"title": null, "content": "their feature embeddings. These are then normalized by their $l_2$ norm to get\n$r_i = \\frac{F(R_i)}{\\Vert F(R_i)\\Vert_2} \\in \\mathbb{R}^d$ and $f_i = \\frac{F(F_i)}{\\Vert F(F_i)\\Vert_2} \\in \\mathbb{R}^d$, where $d$ is the dimensionality of\nthe shared embedding space. The dot product between $r_i$ and $f_i$ computes their\ncosine similarity and accounts for their similarity in the embedding space.\nIn contrast to previous literature which tends to leverage pre-trained back-\nbones, we train $F$ from scratch and exclusively for the task of detecting generated\nimages. This gives us additional flexibility in architectural terms and ultimately\nallows us to employ a smaller and more efficient backbone, which advantages its\napplication in production-ready scenarios.\nImposing robustness to transformations. To ensure robustness to image\ntransformations, we also define a transform operator $T(\\cdot)$ which samples random\ntransformation types from a pre-defined set of operators (e.g. resize, blurring,\nrotation, etc.) and applies them to an input image at training time. In addition\nto sampling transformation types, the operator randomly samples the number\nof transformations in the chain and their strength (e.g. the number of degrees\nof a rotation, or the area ratio of a crop with respect to the input image). As a\nresult, an image to which $T$ is applied simulates a transformation process with\na variable number of transformations, of variable type and strength.\nWith the objective of realistically simulating the manipulations that an image\ncan encounter, we include a comprehensive set of transformations: (i) blurring;\n(ii) alteration of brightness, contrast, saturation, sharpness and opacity; (iii)\npixelization, padding, rotation, horizontal flip, aspect ratio change, resize and\nscale change, skew on the x or y axis; (iv) reduction of the JPEG encoding quality\nlevel; (v) transformation to grayscale; (vi) overlay of striped patterns. Additional\ndetails on the transformation protocol are reported in the supplementary.\nSeparating real and fake samples. On the basis of the InfoNCE paradigm\nand of the transformation protocol outlined above, we define a loss for deepfake\ndetection which aims at imposing a separation between real and fake samples in"}, {"title": null, "content": "the shared embedding space, starting from embeddings of entire images. Given\na batch consisting of real and fake images in equal quantity, we augment each\nof them by employing the transform operator $T$. We respectively call $f'_i$ the\nfeature vector of a transformed fake image $T(F_i)$ and $r'_i$ the feature vector of a\ntransformed real image $T(R_i)$.\nGiven a real sample $r_i$ our loss function aims at maximizing its similarity with\na randomly chosen augmented sample $r'_z$, where $z \\in N - \\{i\\}$ and minimizing its\nsimilarity with respect to all augmented fake samples in the minibatch $\\{f'_j\\}_{j=1}^{N}$.\nThe same objective is applied, symmetrically, when considering fake instances.\nEach fake sample $f_i$ is attracted to a randomly chosen augmented sample $f'_w$\nand repulsed from all augmented real samples in the minibatch $\\{r'_j\\}_{j=1}^{N}$. As this\nobjective is applied to embeddings obtained from entire images, the loss operates\non a global scale of the image. Formally, our loss function is defined as a pair of\ncross-entropy losses as follows:\n\\begin{equation}\n\\mathcal{L}_{global} = -\\frac{1}{2 |B|} \\sum_{i=1}^B \\left(\\log \\frac{e^{r'_i \\cdot r'_z / \\tau}}{\\sum_{j=1}^N e^{r'_i \\cdot r'_j / \\tau} + \\sum_{j=1}^N e^{r'_i \\cdot f'_j / \\tau}} + \\log \\frac{e^{f'_i \\cdot f'_w / \\tau}}{\\sum_{j=1}^N e^{f'_i \\cdot f'_j / \\tau} + \\sum_{l=1}^N e^{f'_i \\cdot r'_l / \\tau}} \\right) \\quad \\text{with } z \\neq i,\n\\tag{1}\n\\end{equation}\nwhere $\\cdot$ indicates the dot product and $\\tau$ is a fixed temperature hyper-parameter\nthat controls the peakness of the probability distribution of the loss function.\nLearning multi-scale contrastive features. The aforementioned contrastive\nloss requires learning embeddings of the entire image which are appropriate for\ndeepfake detection. However, this objective alone does not explicitly focus on\nlearning good local and multi-scale visual features. Detecting a generated image,\nindeed, is not exclusively a matter of extracting features from the image as a\nwhole, but is also a setting where local image features play an important role.\nFollowing this insight, we build a loss component that acts in a multi-scale\nmanner. To this end, we construct different crops of both real and fake images\nwith a multi-crop strategy. Noticeably, multi-crop strategies have been popular-\nized by self-supervised methods like DINO [6,7], which employ a teacher and a\nstudent network and encourage local-to-global correspondences by feeding global\nviews to the teacher and local views to the student. In our case, instead, both\nglobal and local views are passed to the same network which, together with\na contrastive objective, is in charge of learning proper \"local-to-global\" corre-\nspondences shared across samples of the same category (i.e. across different real\nor fake samples) and discriminative \"local-to-global\" differences between differ-\nent categories. Computationally, this also has the advantage of training a single\nbackbone rather than training two backbones at once.\nIn particular, we extract global scale and local scale views of smaller reso-\nlutions. Both crops are passed through the embedding network $F$ to get their\nembeddings. We respectively call $f^{local}_i$ and $f^{global}_i$ the embeddings of the local"}, {"title": null, "content": "scale and global scale crop of a fake image $F_i$, and $r^{local}_i$ and $r^{global}_i$ the embed-\ndings of the local and global crop of a real image $R_i$. For improved robustness,\nalso local and global crops are augmented using the transform operator $T$. The\nloss function is then defined as\n\\begin{equation}\n\\mathcal{L}_{multi-scale} = \\frac{1}{2|B|} \\sum_{i=1}^B \\left(\\log \\frac{e^{r^{local}_i \\cdot r^{global}_i / \\tau}}{e^{r^{local}_i \\cdot r^{global}_i / \\tau} + \\sum_{j=1}^N e^{r^{local}_i \\cdot f^{global}_j / \\tau}} + \\log \\frac{e^{f^{local}_i \\cdot f^{global}_i / \\tau}}{e^{f^{local}_i \\cdot f^{global}_i / \\tau} + \\sum_{l=1}^N e^{f^{local}_i \\cdot r^{global}_l / \\tau}} \\right) \\quad \\text{with } z \\neq i.\n\\tag{2}\n\\end{equation}\nWe follow the standard setting for multi-crop by using global views at resolu-\ntion 2242 covering a large area of the original image, and local views of resolution\n962 covering small areas (less than 50%) of the original image.\nTest protocol. Once the embedding space has been trained, we predict the class\nof an input image (i.e. real or fake) according to three protocols: a nearest neigh-\nbor approach, a linear classification strategy, and a one-class SVM approach. In\nthe nearest neighbor, we use the trained visual encoder to map the entire training\nset to its embedding representation, building a bank of real and fake embeddings.\nAt test time, an input image is firstly projected into the same embedding space,\nand then cosine distance is applied as the metric we find its nearest neighbor in\nthe training set. The prediction (i.e. the output class) is then the same class of\nthe nearest training element found in the bank. In linear classification, instead,\nwe add a single linear layer with sigmoid activation to our embedding space,\nand train only this new classification layer for real-vs-fake classification, with\na binary cross-entropy loss. When using a one-class SVM classifier, we fit only\non real images and treat fake images as outliers residing beyond the boundaries\ndelineated by the classifier with a polynomial kernel."}, {"title": "4 The D\u00b3 Dataset", "content": "As existing datasets are limited in their diversity of generators and quantity\nof images, we opt for creating and releasing a new dataset that can support\nlearning an embedding space from scratch. Our Diffusion-generated Deepfake\nDetection dataset (D\u00b3) contains nearly 2.3M records and 11.5M images. Each\nrecord in the dataset consists of a prompt, a real image, and four images gen-\nerated with as many generators. Prompts and corresponding real images are\ntaken from LAION-400M [49], while fake images are generated, starting from\nthe same prompt, using different text-to-image generators. Some sample records\nof our dataset are shown in Fig. 3.\nDataset collection. We employ four state-of-the-art opensource diffusion mod-\nels, namely Stable Diffusion v1.4 (SD-1.4) [44], Stable Diffusion v2.1 (SD-\n2.1) [44], Stable Diffusion XL (SD-XL) [39], and DeepFloyd IF (DF-IF)."}, {"title": "5 Experiments", "content": "Experimental Setting\nImplementation and training details. We employ the Tiny version of the\nstandard Vision Transformer architecture [53] as our backbone F, and train it\nfrom scratch. Given that our approach relies on a contrastive learning training\nobjective, the use of a smaller model allows us to increase the batch size, which is\nconfigured to 256 for each GPU. We set the learning rate at 2e-3 and employ the\nAdamW optimizer [31]. During training, the number of image transformations\napplied through the T operator is uniformly chosen between 0 and 2 for each im-\nage. After eventually applying random transformations, all images are randomly\ncropped to 2242. When computing the final loss, we apply a constant weight of\n1.0 to both loss components, Lglobal and Lmulti-scale. Overall, our training takes\ntwo days with 4 GPUs. We refer the reader to the supplementary material for\ndetailed training and transformations hyper-parameters.\nClassifiers training details. To train the linear, nearest neighbor, and one-\nclass SVM classifiers, a set of 9,600 records is randomly sampled from the training\nsplit and is then randomly transformed. Following this, we gather features by\napplying the backbone to the selected real and fake images. The nearest neighbor\nclassifier subsequently utilizes these feature banks as a repository for making real-\nfake predictions. For the linear classifier, we employ a maximum of 1,500 training"}, {"title": null, "content": "iterations and a balanced loss function that accounts for the discrepancy in the\nfrequency of real and fake samples. This is crucial, as the prevalence of fake data\nis fourfold compared to real data within the training collection. When fitting\nthe one-class SVM classifier [48], we only utilize the real images contained in the\n9,600 selected records. To create a correct boundary among different sources of\nimages, we consider a polynomial kernel with the v parameter set to 0.1.\nEvaluation. When testing on D\u00b3, we consider both a standard test set contain-\ning images generated using the four diffusion models contained in the training\nsplit. Under this setting, we consider both a case in which test images remain\nunaltered and a case where test images are randomly transformed. Additionally,\nwe report experiments on the extended test set described in Sec. 4. Both test\nsplits are composed of 4,800 different records."}, {"title": "5.2 Experimental Results", "content": "Evaluation on D\u00b3 dataset. To validate our proposal, we first conduct experi-\nments on the D\u00b3 dataset. As previously mentioned, our comparisons encompass\nvarious pre-trained visual backbones followed by a linear, a nearest neighbor\n(NN), or a one-class SVM classifier fitted on the same 9,600 records from D\u00b3\nused in our model. In particular, we use a standard ViT Tiny (ViT-T) [53] pre-\ntrained on ImageNet-21k [14], a CLIP-based model in its ViT Base (ViT-B) and\nViT Large (ViT-L) versions [40], and a backbone pre-trained with self-supervised\nobjectives like DINOv2 [36], using the ViT Base model. To verify the effective-\nness of using contrastive learning, we also train a ViT-Tiny model from scratch\non our dataset with a binary cross-entropy (BCE) loss.\nIn addition to these models, we also compare with existing approaches for\ndeepfake detection specifically tailored to recognize fake images from GAN gen-\nerators. Specifically, we include the models proposed by Wang et al. [55] which\nare based on a ResNet-50 (RN50) [24] model trained with different image trans-\nformations. Other considered approaches involve variations of the ResNet-50\narchitecture specifically trained with images sourced from GANs [23] and from\nlatent diffusion models [12]. Furthermore, we consider the approach proposed by\nWang et al. [56] that takes into account the reconstructed image and its dispar-\nities with the original counterpart and the model presented by Ojha et al. [34]\nwhich leverages a CLIP model based on the ViT Large architecture, followed\nby a linear classifier. Noteworthy, CoDE employs 5M parameters compared to\nRN50 (26M), ViT-B (86M), and ViT-L (307M) resulting in a smaller and more\nefficient model. For all competitors, we use the pre-trained weights downloaded\nfrom the official repositories provided by the authors."}, {"title": "6 Conclusion", "content": "We introduced CoDE, a methodology aimed at cultivating a contrastive-learned\nembedding space designed for the purpose of detecting deepfake content. Our\nmethod strategically incorporates a global contrastive loss, simultaneously em-\nphasizing global-local correspondences via local and global crops. The training\nof our model involved the creation of the D\u00b3 dataset, comprising 9.2 million\nimages generated through the utilization of a large variety of state-of-the-art\ndiffusion models. Experimental results demonstrate the superior performance\nand enhanced generalization capabilities exhibited by our proposed approach."}, {"title": "A Additional Experimental Analyses", "content": "Inference time analysis. As outlined in the main paper, one of the main\nbenefits of training CoDE from scratch is that this allows for more freedom\nin architectural choices. This ultimately results in a smaller and more efficient\nmodel compared to state-of-the-art detectors.\nTo showcase this, in Table 6 we compare CoDE with the models proposed\nby Wang et al. [55], Corvi et al. [12], and Ojha et al. [34] in terms of number\nof parameters, expected input size, and inference throughput (i.e. number of\nprocessed images per second). In the comparison, we also include CLIP DINOv2\nwith ViT-B as backbones. Specifically, we assess the throughput of the different\narchitectures according to two settings, the former involving the evaluation of a\nsingle image at a time (\"single sample\") and the latter considering the batching of\nmultiple images to expedite the evaluation process (\"batched samples\"). Within\nthese two setups, we evaluate performances both with and without the incorpo-\nration of a final nearest neighbor classifier [18]. Experiments are performed on\na single RTX 6000 GPU equipped with 24 GB of VRAM. In the batched case,"}, {"title": "C Additional Details on the D\u00b3 Dataset", "content": "As outlined in Section 4 of the main paper, the standard training and test\nsplits of our D\u00b3 dataset comprise images from four state-of-the-art generators.\nAdditionally, we also collect an extended test set with images generated by 12\ndifferent diffusion-based models, including the four contained in the standard\ntraining and test sets. The specific models and model names employed from the\ndiffusers library are reported in Table 14.\nAspect ratios and sizes. The aspect ratios and sizes of the images generated\nwith the three Stable Diffusion models are subject to probabilistic sampling, with\na 0.5 probability assigned to 5122 and a 0.25 probability assigned to 640 \u00d7 480\nand 640 \u00d7 360. When generating images with the DeepFloyd IF model, instead,\nwe only perform the first two steps of its pipeline, thus generating an image with\na size of 2562. Nevertheless, future works might still apply the upscaling model\u2079\nand obtain images with a size of 10242 starting from samples contained in D3.\nEncoding. Encoding formats for the generated images are chosen by following\nthe distribution of image formats in LAION-400M [49]. As a consequence, we\nencode roughly 91% of the generated images in JPEG format, and the remaining\n9% with other formats (BMP, GIF, TIFF, PNG).\nNegative prompts. To enhance the quality of generated content, we also em-\nploy negative prompts. These aim at decreasing the probability of generating\na specific subject or propriety. Notably, not all diffusion models accommodate\nnegative prompts as input. Consequently, we implement this technique across all\ngenerators except for Stable Diffusion 1.4. A detailed list of the negative prompts\nemployed in the generation process is reported in Table 15. During the gener-\nation of the dataset, we apply with a probability of 0.5 five randomly sampled\nnegative prompts."}, {"title": "D Limitations and Societal Impact", "content": "Limitations. CoDE is an embedding space specifically tailored for deepfake de-\ntection. However, it must be acknowledged that the model does not guarantee an\ninfallible classification of each input image as either real or counterfeit. Indeed,\nas all other deepfake detection approaches, the model can potentially misclas-\nsify images, resulting in both false positives and false negatives. Furthermore,\nalthough CODE has been evaluated across a spectrum of image generators, its\nadaptability to new, unforeseen generators cannot be guaranteed. Future deep-\nfakes may necessitate a fine-tuning of CoDE to maintain its efficacy.\nSocietal impact. Deepfakes pose a direct threat to the integrity of information,\nundermining trust in digital media. Indeed, the ability to create indistinguish-\nable fake content can reduce public confidence in visual evidence. Also, the use\nof deepfakes to generate imagery without the consent of the individuals involved\nconstitutes a grave invasion of privacy and personal autonomy. Such manipula-\ntions not only violate individual rights but also have the potential to catalyze\nsocial discord and manipulate narratives related to public safety, either by prop-\nagating unfounded alarms or by detracting from legitimate warnings. Preserving\nthe authenticity and integrity of digital content underscores the necessity of\nimplementing rapid inference systems, a point further highlighted in Table 6."}]}