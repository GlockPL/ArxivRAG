{"title": "Contrasting Deepfakes Diffusion via Contrastive Learning and Global-Local Similarities", "authors": ["Lorenzo Baraldi", "Federico Cocchi", "Marcella Cornia", "Lorenzo Baraldi", "Alessandro Nicolosi", "Rita Cucchiara"], "abstract": "Discerning between authentic content and that generated by advanced AI methods has become increasingly challenging. While previous research primarily addresses the detection of fake faces, the identification of generated natural images has only recently surfaced. This prompted the recent exploration of solutions that employ foundation vision-and-language models, like CLIP. However, the CLIP embedding space is optimized for global image-to-text alignment and is not inherently designed for deepfake detection, neglecting the potential benefits of tailored training and local image features. In this study, we propose CODE (Contrastive Deepfake Embeddings), a novel embedding space specifically designed for deepfake detection. CoDE is trained via contrastive learning by additionally enforcing global-local similarities. To sustain the training of our model, we generate a comprehensive dataset that focuses on images generated by diffusion models and encompasses a collection of 9.2 million images produced by using four different generators. Experimental results demonstrate that CoDE achieves state-of-the-art accuracy on the newly collected dataset, while also showing excellent generalization capabilities to unseen image generators. Our source code, trained models, and collected dataset are publicly available at: https://github.com/aimagelab/CODE.", "sections": [{"title": "1 Introduction", "content": "Thanks to the increasing generation quality of text-to-image models [44], the dissemination of generated visual content has raised concerns about their use for malicious purposes. Indeed, the threat of a scenario in which visually indistinguishable fake images can be easily generated from simple textual descriptions is evolving. In this context, deepfake detection methods [34] play a significant role in safeguarding society from the risky scenario in which real and fake images are no longer distinguishable from the naked eye.\nDeepfake detection approaches have been extensively studied for the case of AI-manipulated faces [45,54,58], finding common generation imprints [55] among different Generative Adversarial Networks (GANs) [22, 28]. However, with the advent of diffusion models [51], the landscape of image generation has undergone a profound transformation in terms of generation quality and detection techniques. Recent deepfake detection proposals [1,34] have considered the adoption of general-purpose vision-and-language backbones like CLIP [40], and have shown their capability of discriminating images and generalize across generators, including diffusion-based ones.\nHowever, the CLIP embedding space has been trained on real images only, and enforcing global text-to-image similarities through contrastive learning. Clearly, training on real images only and treating the \"fake\" class as an outlier can improve generalization across different generators, but also comes at the cost of limiting the recognition accuracy with respect to a training scenario in which visual features can be learned on fake images as well. Further, generated images can be distinguished prominently by looking at local and fine-grained details, rather than exclusively considering the global context of the image, making the CLIP training paradigm sub-optimal in this context. Lastly, employing pretrained backbones prevents any architectural deviation from already existing models. As CLIP backbones employ at least 86 million parameters (in the ViT Base [17] configuration), this also has a significant impact on their adoption in production-ready scenarios where deepfake detection should run in real-time.\nIn light of these considerations, in this paper we propose Contrastive Deepfake Embeddings (CoDE), a novel contrastive-based embedding space specifically tailored for distinguishing real and generated images. Our model is trained by employing both real and fake images, and by stimulating the learning of local-to-global correspondences between real and fake counterparts and between same-class images, in addition to enforcing global similarities. Further, by training from scratch on the real-fake distribution, we also reduce the scale of the model and find that a ViT Tiny architecture can provide state-of-the-art results while being significantly lighter in computational terms. The resulting embedding space can better separate between real and fake samples, and also better distinguish among different generators, as shown in Fig. 1."}, {"title": "2 Related Work", "content": "Image generation models. Images can be generated with different approaches, ranging from autoregressive models [16,20,42,59] and generative adversarial networks [5,9,28,29,52] to diffusion models [2,25,51]. The emergence of latent diffusion models [15, 33, 39, 43, 44, 47] has significantly propelled the adoption of diffusion-based generators. This is attributed to the heightened efficiency observed in both training and inference, achieved by transitioning the generation process from pixel space to latent space. Notably, this transition maintains stateof-the-art performance in the quality of generated images. Furthermore, numerous endeavors in literature are directed towards improving both image quality"}, {"title": "3 Proposed Method", "content": "Given an input image, we tackle the task of classifying whether it was actually produced by a camera (real image) or whether it was generated by a generative model (fake image). In CoDE, we train a contrastive space for deepfake detection with a pair of losses that encourage features coming from real and fake images to be separated in the embedding space, while considering both global and local visual cues. An overview of our approach is shown in Fig. 2.\nAn embedding space for deepfake detection. Our contrastive pre-training objective relies on the Info-NCE framework [35]. It utilizes a large collection of fake images generated from web-scraped prompts, paired with a collection of real images having the same size. Given a minibatch \\(B = (\\{R_i\\}_{i=1}^B, \\{F_i\\}_{i=1}^B)\\), where \\(\\{R_i\\}_{i=1}^B\\) and \\(\\{F_i\\}_{i=1}^B\\) denote, respectively, randomly sampled sets of real and fake images, the contrastive objective encourages real and fake images to lie separated in a shared embedding space, while ensuring invariance to image transformation. Images \\(R_i\\) and \\(F_i\\) are processed by a learnable Vision Transformer [17] \\(F\\) to get their feature embeddings. These are then normalized by their \\(l_2\\) norm to get \\(r_i = \\frac{F(R_i)}{||F(R_i)||_2} \\in \\mathbb{R}^d\\) and \\(f_i = \\frac{F(F_i)}{||F(F_i)||_2} \\in \\mathbb{R}^d\\), where \\(d\\) is the dimensionality of the shared embedding space. The dot product between \\(r_i\\) and \\(f_i\\) computes their cosine similarity and accounts for their similarity in the embedding space.\nIn contrast to previous literature which tends to leverage pre-trained backbones, we train \\(F\\) from scratch and exclusively for the task of detecting generated images. This gives us additional flexibility in architectural terms and ultimately allows us to employ a smaller and more efficient backbone, which advantages its application in production-ready scenarios.\nImposing robustness to transformations. To ensure robustness to image transformations, we also define a transform operator \\(T(\\cdot)\\) which samples random transformation types from a pre-defined set of operators (e.g. resize, blurring, rotation, etc.) and applies them to an input image at training time. In addition to sampling transformation types, the operator randomly samples the number of transformations in the chain and their strength (e.g. the number of degrees of a rotation, or the area ratio of a crop with respect to the input image). As a result, an image to which \\(T\\) is applied simulates a transformation process with a variable number of transformations, of variable type and strength.\nWith the objective of realistically simulating the manipulations that an image can encounter, we include a comprehensive set of transformations: (i) blurring; (ii) alteration of brightness, contrast, saturation, sharpness and opacity; (iii) pixelization, padding, rotation, horizontal flip, aspect ratio change, resize and scale change, skew on the x or y axis; (iv) reduction of the JPEG encoding quality level; (v) transformation to grayscale; (vi) overlay of striped patterns. Additional details on the transformation protocol are reported in the supplementary.\nSeparating real and fake samples. On the basis of the InfoNCE paradigm and of the transformation protocol outlined above, we define a loss for deepfake detection which aims at imposing a separation between real and fake samples in the shared embedding space, starting from embeddings of entire images. Given a batch consisting of real and fake images in equal quantity, we augment each of them by employing the transform operator \\(T\\). We respectively call \\(f_i^T\\) the feature vector of a transformed fake image \\(T(F_i)\\) and \\(r_i^T\\) the feature vector of a transformed real image \\(T(R_i)\\).\nGiven a real sample \\(r_i\\), our loss function aims at maximizing its similarity with a randomly chosen augmented sample \\(r_z^T\\), where \\(z \\in \\mathbb{N} - \\{i\\}\\) and minimizing its similarity with respect to all augmented fake samples in the minibatch \\(\\{f_j^T\\}_{j=1}^B\\). The same objective is applied, symmetrically, when considering fake instances. Each fake sample \\(f_i\\) is attracted to a randomly chosen augmented sample \\(f_z^T\\), and repulsed from all augmented real samples in the minibatch \\(\\{r_j^T\\}_{j=1}^B\\). As this objective is applied to embeddings obtained from entire images, the loss operates on a global scale of the image. Formally, our loss function is defined as a pair of cross-entropy losses as follows:\n\\[\\begin{aligned} \\mathcal{L}_{\\text{global}} = & \\frac{1}{2|B|} \\left( \\log \\frac{e^{r_i^T \\cdot r_z^T}}{e^{r_i^T \\cdot r_z^T} + \\sum_{j=1}^B e^{r_i^T \\cdot f_j^T}} + \\log \\frac{e^{f_i^T \\cdot f_z^T}}{e^{f_i^T \\cdot f_z^T} + \\sum_{j=1}^B e^{f_i^T \\cdot r_j^T}} \\right), \\text{with } z \\neq i, \\label{eq:1} \\end{aligned}\\]\nwhere \\(\\cdot\\) indicates the dot product and \\(\\tau\\) is a fixed temperature hyper-parameter that controls the peakness of the probability distribution of the loss function.\nLearning multi-scale contrastive features. The aforementioned contrastive loss requires learning embeddings of the entire image which are appropriate for deepfake detection. However, this objective alone does not explicitly focus on learning good local and multi-scale visual features. Detecting a generated image, indeed, is not exclusively a matter of extracting features from the image as a whole, but is also a setting where local image features play an important role. Following this insight, we build a loss component that acts in a multi-scale manner. To this end, we construct different crops of both real and fake images with a multi-crop strategy. Noticeably, multi-crop strategies have been popularized by self-supervised methods like DINO [6,7], which employ a teacher and a student network and encourage local-to-global correspondences by feeding global views to the teacher and local views to the student. In our case, instead, both global and local views are passed to the same network which, together with a contrastive objective, is in charge of learning proper \"local-to-global\" correspondences shared across samples of the same category (i.e. across different real or fake samples) and discriminative \"local-to-global\" differences between different categories. Computationally, this also has the advantage of training a single backbone rather than training two backbones at once.\nIn particular, we extract global scale and local scale views of smaller resolutions. Both crops are passed through the embedding network \\(F\\) to get their embeddings. We respectively call \\(f_i^{\\text{local}}\\) and \\(f_i^{\\text{global}}\\) the embeddings of the local scale and global scale crop of a fake image \\(F_i\\), and \\(r_i^{\\text{local}}\\) and \\(r_i^{\\text{global}}\\) the embeddings of the local and global crop of a real image \\(R_i\\). For improved robustness, also local and global crops are augmented using the transform operator \\(T\\). The loss function is then defined as\n\\[\\begin{aligned} \\mathcal{L}_{\\text{multi-scale}} = & \\frac{1}{2|B|} \\sum_{i=1}^{|B|} \\left( \\log \\frac{e^{r_i^{\\text{local}} \\cdot r_i^{\\text{global}}}}{e^{r_i^{\\text{local}} \\cdot r_i^{\\text{global}}} + \\sum_{j=1}^B e^{r_i^{\\text{local}} \\cdot r_j^{\\text{global}}} + \\log \\frac{e^{f_i^{\\text{local}} \\cdot f_i^{\\text{global}}}}{e^{f_i^{\\text{local}} \\cdot f_i^{\\text{global}}} + \\sum_{j=1}^B e^{f_i^{\\text{local}} \\cdot f_j^{\\text{global}}}} \\right) \\text{with } z \\neq i. \\label{eq:2} \\end{aligned}\\]\nWe follow the standard setting for multi-crop by using global views at resolution 224^2 covering a large area of the original image, and local views of resolution 96^2 covering small areas (less than 50%) of the original image.\nTest protocol. Once the embedding space has been trained, we predict the class of an input image (i.e. real or fake) according to three protocols: a nearest neighbor approach, a linear classification strategy, and a one-class SVM approach. In the nearest neighbor, we use the trained visual encoder to map the entire training set to its embedding representation, building a bank of real and fake embeddings. At test time, an input image is firstly projected into the same embedding space, and then cosine distance is applied as the metric we find its nearest neighbor in the training set. The prediction (i.e. the output class) is then the same class of the nearest training element found in the bank. In linear classification, instead, we add a single linear layer with sigmoid activation to our embedding space, and train only this new classification layer for real-vs-fake classification, with a binary cross-entropy loss. When using a one-class SVM classifier, we fit only on real images and treat fake images as outliers residing beyond the boundaries delineated by the classifier with a polynomial kernel."}, {"title": "4 The D\u00b3 Dataset", "content": "As existing datasets are limited in their diversity of generators and quantity of images, we opt for creating and releasing a new dataset that can support learning an embedding space from scratch. Our Diffusion-generated Deepfake Detection dataset (D\u00b3) contains nearly 2.3M records and 11.5M images. Each record in the dataset consists of a prompt, a real image, and four images generated with as many generators. Prompts and corresponding real images are taken from LAION-400M [49], while fake images are generated, starting from the same prompt, using different text-to-image generators. Some sample records of our dataset are shown in Fig. 3.\nDataset collection. We employ four state-of-the-art opensource diffusion models, namely Stable Diffusion v1.4 (SD-1.4) [44], Stable Diffusion v2.1 (SD2.1) [44], Stable Diffusion XL (SD-XL) [39], and DeepFloyd IF (DF-IF). While"}, {"title": "5 Experiments", "content": "5.1 Experimental Setting\nImplementation and training details. We employ the Tiny version of the standard Vision Transformer architecture [53] as our backbone \\(F\\), and train it from scratch. Given that our approach relies on a contrastive learning training objective, the use of a smaller model allows us to increase the batch size, which is configured to 256 for each GPU. We set the learning rate at 2e-3 and employ the AdamW optimizer [31]. During training, the number of image transformations applied through the \\(T\\) operator is uniformly chosen between 0 and 2 for each image. After eventually applying random transformations, all images are randomly cropped to 224^2. When computing the final loss, we apply a constant weight of 1.0 to both loss components, \\(\\mathcal{L}_{\\text{global}}\\) and \\(\\mathcal{L}_{\\text{multi-scale}}\\). Overall, our training takes two days with 4 GPUs. We refer the reader to the supplementary material for detailed training and transformations hyper-parameters.\nClassifiers training details. To train the linear, nearest neighbor, and oneclass SVM classifiers, a set of 9,600 records is randomly sampled from the training split and is then randomly transformed. Following this, we gather features by applying the backbone to the selected real and fake images. The nearest neighbor classifier subsequently utilizes these feature banks as a repository for making realfake predictions. For the linear classifier, we employ a maximum of 1,500 training"}, {"title": "6 Conclusion", "content": "We introduced CoDE, a methodology aimed at cultivating a contrastive-learned embedding space designed for the purpose of detecting deepfake content. Our method strategically incorporates a global contrastive loss, simultaneously emphasizing global-local correspondences via local and global crops. The training of our model involved the creation of the D\u00b3 dataset, comprising 9.2 million images generated through the utilization of a large variety of state-of-the-art diffusion models. Experimental results demonstrate the superior performance and enhanced generalization capabilities exhibited by our proposed approach."}, {"title": "A Additional Experimental Analyses", "content": "Inference time analysis. As outlined in the main paper, one of the main benefits of training CoDE from scratch is that this allows for more freedom in architectural choices. This ultimately results in a smaller and more efficient model compared to state-of-the-art detectors.\nTo showcase this, in Table 6 we compare CoDE with the models proposed by Wang et al. [55], Corvi et al. [12], and Ojha et al. [34] in terms of number of parameters, expected input size, and inference throughput (i.e. number of processed images per second). In the comparison, we also include CLIP DINOv2 with ViT-B as backbones. Specifically, we assess the throughput of the different architectures according to two settings, the former involving the evaluation of a single image at a time (\"single sample\") and the latter considering the batching of multiple images to expedite the evaluation process (\"batched samples\"). Within these two setups, we evaluate performances both with and without the incorporation of a final nearest neighbor classifier [18]. Experiments are performed on a single RTX 6000 GPU equipped with 24 GB of VRAM. In the batched case,"}, {"title": "B Additional Implementation Details", "content": "Images from commercial generative tools. In Table 4 of the main, we present the performance metrics of CoDE, alongside various other deepfake detection algorithms, when applied to images generated by state-of-the-art commercial generative models such as DALL-E 2 [41]5, DALL-E 3 [3]6, and Midjourney V57. For the evaluation of each generative tool, a dataset comprising 1,000 images has been assembled utilizing publicly available collections.\nBackbone training. During CoDE training, we employ a linear learning rate warmup strategy which starts from a learning rate of 1e-6 and ends with a learning rate of 2e-3 after five epochs. Then, a cosine learning rate schedule is employed. For optimization, we use the AdamW [30] optimizer with \\(\\epsilon\\) set to 1e-8 and \\(\\beta\\) to (0.9, 0.99). When training on D\u00b3, we employ a mini-batch size of 1,024 distributed across 4 GPUs, which amounts to processing 1,770 batches in each epoch. Moreover, we apply early stopping with patience set to 6, resulting in approximately 30 epochs of training. All the ViT backbones considered in this study share the same input size of 224^2 and patch dimensionality of 16^2, with the exception of CLIP ViT-L [40] and DINOv2 ViT-B [36]. The latter two models employ 14^2 patches, and in the case of the DINOv2 ViT-B model, an input size of 518^2.\nClassifier training. To evaluate the pre-trained backbones and CoDE, we employ logistic regression [38], nearest neighbor classifier [27], and one-class"}, {"title": "C Additional Details on the D\u00b3 Dataset", "content": "As outlined in Section 4 of the main paper, the standard training and test splits of our D\u00b3 dataset comprise images from four state-of-the-art generators. Additionally, we also collect an extended test set with images generated by 12 different diffusion-based models, including the four contained in the standard training and test sets. The specific models and model names employed from the diffusers library are reported in Table 14.\nAspect ratios and sizes. The aspect ratios and sizes of the images generated with the three Stable Diffusion models are subject to probabilistic sampling, with a 0.5 probability assigned to 512^2 and a 0.25 probability assigned to 640 \u00d7 480 and 640 \u00d7 360. When generating images with the DeepFloyd IF model, instead, we only perform the first two steps of its pipeline, thus generating an image with a size of 256^2. Nevertheless, future works might still apply the upscaling model\u2079 and obtain images with a size of 1024^2 starting from samples contained in D3.\nEncoding. Encoding formats for the generated images are chosen by following the distribution of image formats in LAION-400M [49]. As a consequence, we encode roughly 91% of the generated images in JPEG format, and the remaining 9% with other formats (BMP, GIF, TIFF, PNG).\nNegative prompts. To enhance the quality of generated content, we also employ negative prompts. These aim at decreasing the probability of generating a specific subject or propriety. Notably, not all diffusion models accommodate negative prompts as input. Consequently, we implement this technique across all generators except for Stable Diffusion 1.4. A detailed list of the negative prompts employed in the generation process is reported in Table 15. During the generation of the dataset, we apply with a probability of 0.5 five randomly sampled negative prompts."}, {"title": "D Limitations and Societal Impact", "content": "Limitations. CoDE is an embedding space specifically tailored for deepfake detection. However, it must be acknowledged that the model does not guarantee an infallible classification of each input image as either real or counterfeit. Indeed, as all other deepfake detection approaches, the model can potentially misclassify images, resulting in both false positives and false negatives. Furthermore, although CoDE has been evaluated across a spectrum of image generators, its adaptability to new, unforeseen generators cannot be guaranteed. Future deepfakes may necessitate a fine-tuning of CoDE to maintain its efficacy.\nSocietal impact. Deepfakes pose a direct threat to the integrity of information, undermining trust in digital media. Indeed, the ability to create indistinguishable fake content can reduce public confidence in visual evidence. Also, the use of deepfakes to generate imagery without the consent of the individuals involved constitutes a grave invasion of privacy and personal autonomy. Such manipulations not only violate individual rights but also have the potential to catalyze social discord and manipulate narratives related to public safety, either by propagating unfounded alarms or by detracting from legitimate warnings. Preserving the authenticity and integrity of digital content underscores the necessity of implementing rapid inference systems, a point further highlighted in Table 6."}]}