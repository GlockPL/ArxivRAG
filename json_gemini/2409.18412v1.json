{"title": "SciDFM: A Large Language Model with Mixture-of-Experts for Science", "authors": ["Liangtai Sun", "Danyu Luo", "Da Ma", "Zihan Zhao", "Baocai Chen", "Zhennan Shen", "Su Zhu", "Lu Chen", "Xin Chen", "Kai Yu"], "abstract": "Recently, there has been a significant upsurge of interest in leveraging large language models (LLMs) to assist scientific discovery. However, most LLMs only focus on general science, while they lack domain-specific knowledge, such as chemical molecules and amino acid sequences. To bridge these gaps, we introduce SciDFM, a mixture-of-experts LLM, which is trained from scratch and is able to conduct college-level scientific reasoning and understand molecules and amino acid sequences. We collect a large-scale training corpus containing numerous scientific papers and books from different disciplines as well as data from domain-specific databases. We further fine-tune the pre-trained model on lots of instruction data to improve performances on downstream benchmarks. From experiment results, we show that SciDFM achieves strong performance on general scientific benchmarks such as SciEval and SciQ, and it reaches a SOTA performance on domain-specific benchmarks among models of similar size. We further analyze the expert layers and show that the results of expert selection vary with data from different disciplines. To benefit the broader research community, we open-source SciDFM at https://huggingface.co/OpenDFM/SciDFM-MoE-A5.6B-v1.0.", "sections": [{"title": "Introduction", "content": "The advent of Large Language Models (LLMs) [1, 2, 3] has ignited a revolution in the realm of artificial intelligence and has pushed the field of AI for Science (AI4S) to an unprecedented new height. LLMs have demonstrated promising performances in assisting and accelerating scientific discovery [4, 5, 6], such as protein design [7], weather forecasting [8], and geoscience [9]. Despite remarkable achievements in science, LLMs primarily focus on general scientific knowledge represented in text form, ignoring domain-specific contents such as molecules in chemistry and proteins in biology, which are fundamental to advances in these fields.\nTo overcome this limitation and fully exploit the potential of LLMs for scientific discovery, we introduce SciDFM, a mixture-of-experts Large Language Model trained from scratch with 18.2 billion parameters in total and 5.6 billion parameters activated. SciDFM integrates a mixture-of-experts (MoE) [10, 11, 12] architecture into a transformer-based [13] framework, aiming at enhancing"}, {"title": "SciDFM", "content": "In this section, we introduce the pretraining and instruction tuning details of SciDFM, including the training data construction, model architecture and infrastructure."}, {"title": "Pretraining", "content": ""}, {"title": "Model Architecture", "content": "SciDFM is based on a transformer architecture [13], and follows modifications of Llama [2], i.e. RMSNorm, ROPE and SwiGLU. SciDFM uses the same hyper-parameters of OpenLLaMa-3B [17], the details are shown in Table 1. And in order to better model knowledge of different disciplines, we replace the feed-forward block with Mixture-of-Expert (MoE) layers [10, 11].\nWe also use the tokenizer of OpenLLaMa-3B, which is trained from scratch using the Bype-Pair Encoding (BPE) method. To better encode the molecules and amino acid sequences and distinguish them from normal characters for better modeling, we treat each chemical atom and amino acid character as a single token and add them into the vocabulary, with special identifiers wrapped [4]. For example, molecules C(C(=O)O)N will be encoded as C,(,C,(,=,O,),O,),N, and amino acid sequences MIRLGAPQTL will be encoded as M,I,R,L,G,A,P,Q,T,L, where the special identifiers are omitted."}, {"title": "Data Construction", "content": "To enhance the understanding and reasoning abilities of SciDFM on science domain, we collect a large-scale training corpus containing a large number of open-access scientific papers and books of different disciplines. And to acquire domain-specific knowledge, we also include data from some databases. Furthermore, in order to maintain the generic capabilities of SciDFM, we use data from some open-source generic corpora. The details of our pretraining corpus are shown in Table 2. Our pretraining corpus contains about 300B science-domain tokens and 270B general-domain tokens, with 570B tokens in total. We train SciDFM for two epochs, and for the second epoch, we re-sample data of C4, CC, and Github subsets of SlimPajama [23, 24]. And for the-stack dataset, we only use data from programming languages that are relevant to scientific computing, such as Matlab and Scilab."}, {"title": "Training Details", "content": "Following Llama [2], SciDFM is trained from scratch using the AdamW optimizer [43] with $\u03b2_1$ = 0.9, $\u03b2_2$ = 0.95. We use a cosine learning rate schedule, such that the final learning rate is equal to 10% of the initial learning rate. We use a weight decay of 0.1 and gradient clipping of 1.0, and we set the macro batch size to 4M tokens and sequence length to 8192 tokens. For MoE layers, we set the auxiliary loss factor to 0.02 and set the expert capacity factor to 1.0. In total, we train SciDFM for two epochs, resulting in about 1.1T tokens fed. We use a learning rate of 3e-4 for the first epoch and 3e-5 for the second epoch, while the other settings remain the same as the above. We train SciDFM on a cluster of 16 nodes, with 8 A800 GPUs on each node for about two months."}, {"title": "Instruction Tuning", "content": "To improve the performance of SciDFM on downstream benchmarks, we collect a number of instruction-tuning data from open-source datasets. The details are shown in Table 3. We fine-tune the pre-trained SciDFM for 5 epochs in total. During fine-tuning, we use a learning rate of 2e-5, and we set the sequence length to 2048 and macro batch size to 32. The other settings are the same as the pretraining stage."}, {"title": "Evaluation", "content": "In this section, we show the performance of SciDFM on some general and domain-specific benchmarks, and analyze the results of expert selection in different domains."}, {"title": "Evaluation Setup", "content": "Evaluation Tasks Summarization of evaluation tasks are shown in Table 5. These evaluation datasets cover a wide range of subjects, including math, chemistry, physics, biology, protein and molecule.\n\u2022 SciEval [14] is a comprehensive and multidisciplinary benchmark designed to assess the scientific research capabilities of Large Language Models.\n\u2022 SciQ [15] comprises high-quality, domain-targeted multiple-choice science exam questions, created through a novel method that combines crowd-sourcing with AI-assisted document and answer option selection.\n\u2022 ARC [44] presents a sophisticated question set, designed to advance AI research in complex question answering within the context of grade-school science, composed of an easy subset and a challenging subset.\n\u2022 GSM8K [45] is composed of diverse and linguistically rich grade school math word problems, designed to benchmark and improve the multi-step mathematical reasoning abilities of LLMs, revealing their limitations in handling complex reasoning tasks.\n\u2022 MATH [18] contains challenging competition mathematics problems, each with detailed solutions, designed to assess and enhance mathematical problem-solving and reasoning capabilities of LLMs, accompanied by an auxiliary pretraining dataset to bolster fundamental math understanding.\n\u2022 MedQA [31] is a pioneering multiple-choice dataset for open-domain question answering in the medical field, encompassing a number of questions sourced from professional medical board exams.\n\u2022 MedMCQA [30] is a large-scale medical multiple-choice question-answering dataset, spanning 2,400 healthcare topics and 21 subjects, designed to challenge models with diverse reasoning skills across various medical domains.\n\u2022 PubMedQA [32] is a biomedical question-answering dataset based on PubMed abstracts, requiring quantitative reasoning over research texts.\n\u2022 Mol-Instructions [16] is a specialized, meticulously curated dataset containing diverse biomolecular instructions, designed to enhance LLMs' understanding and prediction capabilities within the realms of molecular, protein, and broader biomolecular texts.\n\u2022 MoleculeNet [46] is a comprehensive benchmark dataset for molecular machine learning, featuring curated public datasets, standardized evaluation metrics, and open-source implementation of various molecular featurization and learning methods."}, {"title": "Evaluation Methods", "content": "Since SciDFM is an instruction-following model by default, we conduct all experiments using zero-shot settings. And most of the models we select for comparison are able to follow instructions:\n\u2022 Galactica [4] is a large language model specifically designed to store, combine, and reason about vast amounts of scientific knowledge, outperforming existing models on various scientific tasks and aiming to serve as a new, advanced interface for scientific research. We select Galactica-30B and Galactica-6.7B for comparison.\n\u2022 Llama [2] is a series of open-source powerful language models, ranging from 7 billion to 70 billion parameters, trained on massive public datasets, and outperforms many of the available open-source models on common benchmarks. We select Llama3-8B, Llama3-8B-Instruct, Llama2-7B and Llama2-13B for comparison.\n\u2022 ChatGLM [47, 48] is a series of advanced language models, excel in various metrics and tasks, rivaling or surpassing counterparts like GPT-4, thanks to their extensive training on multilingual data, specialized alignment techniques, and the ability to integrate diverse tools dynamically. We select ChatGLM2-6B, ChatGLM3-6B and ChatGLM3-6B-base for comparison.\n\u2022 SciGLM [6] is a suite of scientific language models that enhance college-level scientific reasoning through a self-reflective instruction annotation framework, addressing data scarcity in the science domain, and improving upon ChatGLM in handling complex scientific and mathematical problems without compromising language understanding. We select SciGLM-6B for comparison.\n\u2022 ChemDFM [5] is specifically trained for Chemistry, combining knowledge from chemical literature and general domains to excel in understanding, reasoning, and applying chemical information, outperforming generic LLMs and even GPT-4 on chemical tasks. We select ChemDFM-13B for comparison."}, {"title": "Main Results", "content": "General Scientific Benchmark Table 4 presents the evaluation results on eight general scientific language understanding and reasoning tasks. The results show that SciDFM reaches a better performance on average than Galactica-series models, Llama-series models except Llama3-8B-Instruct and ChatGLM-series models except ChatGLM3-6B-base. In Table 6, we also present the average performance of general science, math and biology tasks on the above eight benchmarks, in which SciEval, SciQ and ARC belong to general science task, GSM8K and MATH belong to math task, MedQA, MedMCQA and PubMedQA belong to biology task. We find that SciDFM outperforms all models except Llama3-8B-Instruct on math and biology domain, while it is weak in general science tasks. In conclusion, SciDFM can reach a similar performance to top-tier models of similar amount of compute, while it is weaker compared to models that are larger and trained using more data.\nDomain-specific Scientific Benchmark Table 7 presents the performance of molecular property prediction tasks on MoleculeNet. From the results shown in AUC-ROC scores, we find that SciDFM"}, {"title": "Expert Choices Analysis", "content": "In this subsection, we conduct analysis on expert choice results on data from different domains. Formally, we denote the output of the ith attention layer as $h_i \u2208 R^{l\u00d7d}$, where l is the sequence length and d is the hidden dimension, and we denote the weight of the ith gate network in the corresponding MoE layer as $W_g \u2208 R^{d\u00d7e}$, where e represents the number of experts. Then, we have $g_i = h_i \u2022 W_g \u2208 R^{l\u00d7e}$, representing the probability of each token being assigned to each expert. Suppose the number of hidden layers is N, for a given text T, we define the expert choice results as:\n$C_i = Softmax(\\sum_{j=1}^l g_i[j, :]) \u2208 R^e$, (1)\n$\u0415_\u0442 = Concat([e_1, e_2, ..., e_N]) \u2208 R^{Ne}$. (2)\nWe randomly select 100 research papers each in the fields of math, chemistry, biology, and physics, and also select 100 chemical molecules and 100 amino acid sequences as analysis data. For each text"}, {"title": "Related Works", "content": "The success of pretraining language models like BERT [50] and GPT [51] makes researchers wonder whether the language model can bring about improved performance in the field of Science.\nDomain-Specific Language Model for Science BioGPT [52] is a domain-specific generative language model pre-trained on large-scale biomedical literature, which outperforms previous models on six biomedical-related tasks. Based on case studies, the researchers further demonstrated the advantages of BioGPT in generating fluent descriptions for biomedical terms in biomedical literature. ProGen2 [53] is a protein language model pre-trained on a corpus of more than one billion protein sequences including genome, metagenome, and immune library databases. ProGen2 shows optimal performance in capturing observed evolutionary sequence distributions, generating new protein sequences, and predicting protein fitness without additional fine-tuning. Med-PaLM [54] is a large language model (LLM) designed to provide high-quality answers to medical questions, which is an instruction prompt-tuned version of Flan-PaLM [55] specialized for the medical domain. They reveal limitations of Flan-PaLM in scientific grounding, harm, and bias through evaluation, while Med-PaLM significantly reduces the gap (or even compares favorably) to clinicians on several of these axes, according to both clinicians and lay users. MTL-BERT [56] proposes to use large-scale pre-training, multi-task learning, and SMILES enumeration to alleviate the data sparsity problem. It mines the rich contextual information in SMILES strings through self-supervised pre-training, and then fine-tunes the pre-trained model simultaneously using multiple downstream tasks. At the same time, it combines SMILES enumeration as a data augmentation strategy to increase data diversity. Experimental results show that MTL-BERT can achieve optimal performance on molecular datasets. ChemDFM [5] is the first LLM towards Chemical General Intelligence (CGI), which is trained on"}, {"title": "General-domain Language Model for Science", "content": "SciBERT [57] is a pre-trained language model based on the BERT model architecture, which aims to address the lack of high-quality, large-scale labeled scientific data. SciBERT uses a large multi-domain scientific publication corpus for pre-training to improve the performance of downstream scientific benchmarks and has achieved state-of-the-art performance on multiple tasks. Galactica [4] is a large language model that can store, combine and reason about scientific knowledge, which is trained on a large scientific corpus of papers, reference material, knowledge bases and many other sources. Galactica outperforms previous models on a range of scientific tasks and sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA. SciGLM [6] is a suite of scientific language models designed to enhance college-level scientific reasoning. It utilizes a self-reflective instruction annotation framework to address data scarcity in the science domain. SciGLM significantly improves upon ChatGLM by effectively handling complex scientific and mathematical problems, all while maintaining strong language understanding capabilities.\nCompared to prior works, SciDFM either can achieve a better performance, or is more generalized. With the utilization of Mixture-of-Experts architecture, SciDFM can better model similarities and differences across different disciplines and modalities and have stronger sophisticated scientific reasoning and understanding capabilities."}, {"title": "Conclusion", "content": "In this paper, we introduce SciDFM, a mixture-of-experts LLM able to conduct college-level scientific reasoning and understand molecules and amino acid sequences. We show the pretraining and instruction-tuning process of SciDFM in detail, including data, architecture and hyper-parameters. We conduct evaluation on eight general scientific language understanding and reasoning tasks and two domain-specific tasks. From the results, we show that SciDFM achieves strong performance on general scientific benchmarks such as SciEval and SciQ, and it reaches a SOTA performance on domain-specific benchmarks among models of similar size. We further analyze the expert choices of MoE layers and show that the results of expert selection vary with data from different disciplines and exhibit clustering phenomena related to their relationships."}]}