{"title": "Beyond Few-shot Object Detection: A Detailed Survey", "authors": ["Vishal Chudasama", "Hiran Sarkar", "Pankaj Wasnik", "Vineeth N Balasubramanian", "Jayateja Kalla"], "abstract": "Object detection is a critical field in computer vision focusing on accurately identifying and locating specific objects in images or videos. Traditional methods for object detection rely on large labeled training datasets for each object category, which can be time-consuming and expensive to collect and annotate. To address this issue, researchers have introduced few-shot object detection (FSOD) approaches that merge few-shot learning and object detection principles. These approaches allow models to quickly adapt to new object categories with only a few annotated samples. While traditional FSOD methods have been studied before, this survey paper comprehensively reviews FSOD research with a specific focus on covering different FSOD settings such as standard FSOD, generalized FSOD, incremental FSOD, open-set FSOD, and domain adaptive FSOD. These approaches play a vital role in reducing the reliance on extensive labeled datasets, particularly as the need for efficient machine learning models continues to rise. This survey paper aims to provide a comprehensive understanding of the above-mentioned few-shot settings and explore the methodologies for each FSOD task. It thoroughly compares state-of-the-art methods across different FSOD settings, analyzing them in detail based on their evaluation protocols. Additionally, it offers insights into their applications, challenges, and potential future directions in the evolving field of object detection with limited data.", "sections": [{"title": "INTRODUCTION", "content": "Object detection has experienced remarkable advancements in recent years due to significant progress in deep learning techniques, as demonstrated by methods like Faster R-CNN [148], YOLO [146], and DETR [9]. The primary goal of object detection is to accurately identify and locate objects within an image while also categorizing these objects into specific predefined classes. However, traditional deep learning approaches for object detection heavily depend on large-scale labeled training datasets [84]. This dependence poses significant challenges in real-world scenarios where collecting large amounts of data is often impractical [156]. Acquiring a sufficient number of images can be infeasible, and annotating these images for object detection is both expensive and time-consuming. Additionally, training complex deep learning models with limited data frequently results in overfitting issues, where the model performs well on the training data but fails to gener- alize to unseen data. In contrast, humans possess an exceptional ability to learn new concepts with minimal data, especially during the early stages of development. For example, children can quickly identify and differentiate new objects after encountering them only a few times. Inspired by this impressive human capacity, a setting called few-shot learning [156, 180] has emerged, where the model is trained to learn from a few number of samples.\nFig. 1 illustrates the standard few-shot learning regime, where a model is initially trained on a large amount of labeled data and adapts to new classes with a significantly smaller number of samples. In the field of few-shot object detection (FSOD), the objective is to detect certain objects using only a limited number of annotated instances, thus eliminating the need for extensive annotated data, which is a primary limitation of state-of-the-art object detection approaches. The first stage in FSOD is to pre-train the model using a large amount of data from known classes, called base classes. In the second stage, this knowledge enables the model to recognize new classes, termed novel classes, with only a few examples.\nFSOD is important in various real-world applications where obtaining a large amount of annotated data is challenging, expensive, or time-consuming. In medical imaging [92], FSOD methods can help identify rare new diseases from a limited number of labeled examples, enabling quicker diagnosis and treatment. In wildlife conservation, FSOD methods can help monitor endangered species with minimal data, supporting conservation efforts. FSOD is also valuable in industrial inspection [166, 228], where defects or anomalies in manufacturing processes can be detected with limited training samples, enhancing quality control. In security and surveillance applications, FSOD can detect suspicious activities or objects with minimal labeled data, improving safety and response times. FSOD methods also have a vital role to play in other domains such as remote sensing or multispectral imaging [68, 188], as well as other settings such as cross-domain generalization [42, 134], further broadening its applicability and impact across diverse fields. Given the increased significance, newer variants of the traditional FSOD setting have emerged in recent years. Exploring this expansion of the FSOD setting in recent literature is the key objective of this survey.\nFig. 2 outlines the evolution of the few-shot object detection task in recent years. While the initial efforts focused on standard FSOD, extensions and variants have emerged as listed below:\n\u2022 Standard FSOD\n\u2022 Generalized FSOD (G-FSOD)\n\u2022 Incremental FSOD (I-FSOD)\n\u2022 Open-set FSOD (O-FSOD)\n\u2022 Domain Adaptation based FSOD (FSDAOD)\nStandard FSOD eliminates the dependency on vast amounts of labeled training data, and its primary focus lies in enhancing performance in novel classes rather than maintaining performance in base classes. Chen et al. provide the first paper in this direction in their work LSTD [11]. Nevertheless, learning new classes while maintaining performance in base classes is often crucial in real-world applications. To address this challenge, two other tasks, generalized few-shot object detection (G-FSOD) and incremental few-shot object detection (I-FSOD), aim to perform strongly on both base and novel classes. G-FSOD tackles the challenge of proper knowledge retention of the base classes while learning the new classes. TFA [179] is the first work in the task of G-FSOD that provides results on both the base and novel classes. Both standard FSOD and G-FSOD rely on the availability of the base classes while learning the new classes. However, it is not feasible in the real-world scenario to avail the old classes while learning the new classes. ONCE [140] is the first to address this issue and introduce the task of I-FSOD. I-FSOD, unlike the previous tasks, does not require the old classes to adapt to new ones.\nOpen-set few-shot object detection (O-FSOD) is another FSOD sub-category that focuses on not only detecting objects of trained classes but also detecting objects of unseen classes. In many real-world scenarios, it is impractical to pre-define or pre-label all possible object categories, making it essential for systems to recognize and handle new or rare objects that weren't part of the initial training set. FOOD [159] is the first work proposed in this direction by Su et al.. Another sub- category of FSOD, called Few-shot domain adaptive object detection (FSDAOD), involves adapting a detector to a new domain. FSDAOD is trained on a source domain with abundant data, which is then adapted to a new domain with only a small amount of data. This is particularly useful in real-world applications where collecting extensive labeled data for every possible domain is impractical. Wang et al. [178] proposed the first work in this direction, which can generalize to a target domain given a few samples. All of these different approaches aim to tackle the difficulties associated with limited data in real-world scenarios, striving to balance the need for recognizing new classes and maintaining the accuracy of known classes.\nThe above-mentioned variants of standard FSOD, G-FSOD, I-FSOD, O-FSOD and FSDAOD tasks differ primarily based on the availability of training data and the classes on which a model is evaluated. This survey seeks to comprehensively study these variants and analyze the developments beyond the standard FSOD setting in recent years. We begin with a comparison of existing FSOD- based survey papers in Section 2, and clarify the need for this survey. We then briefly cover the background of object detection in Section 3. The problem statement with detailed notations and differences between few shot tasks are provided in Section 4.1. Then, we comprehensively review research works related to each few-shot task in Subsections 4.2 to 4.6. This thorough review offers an overview of the recent state-of-the-art research in all these approaches. Benchmark datasets and evaluation protocols are discussed in Section 5. The result analysis of all these methods is discussed in Section 6. Finally, various research directions, applications and challenges in this field are explored in Section 7."}, {"title": "COMPARISON WITH RELATED SURVEY PAPERS", "content": "In the field of FSOD learning, several surveys [1, 21, 64, 69, 72, 83, 118, 151, 192] have been conducted to investigate and analyze various aspects of this domain. a detailed summary of the existing FSOD- based survey papers papers can be found in Table 1, while a comparison between our survey paper with the existing FSOD-based survey are depicted in Table 2.\nNotably, Huang et al. [64] focused on exploring the fusion of self-supervised representations with FSOD, emphasizing the importance of self-supervision pre-training in improving object detection tasks. Additionally, they discussed the challenges associated with integrating self-supervised representations with detection techniques. Another significant contribution was made by Kohler et al. [83], who provided a comprehensive overview of the current state-of-the-art (SOTA) in FSOD methods, categorizing these approaches based on their training schemes and architectural layouts. Jiaxu et al. [72] presented a data-driven taxonomy of the training data and the type of corresponding supervision utilized during the training phase. Huang et al. [69] conducted a study on low-shot object detection, encompassing zero-shot, one-shot, and few-shot object detection. Antonelli et al. [1] dissected FSOD methods into categories such as data augmentation, transfer learning, distance metric learning, and meta-learning-based approaches. Zhang et al. [213] delved into the realm of few-shot class incremental learning and object detection from both anchor-free and anchor-based perspectives. The authors in [83, 118] also examined the extensive field of FSOD, with [118] analyzing existing FSOD algorithms from a new viewpoint based on their contributions, and [83] categorizing approaches based on their training scheme and architectural layout, broadly classifying them into meta-learning and transfer-learning based methods. Sa et al. [151] reviewed standard FSOD models focusing on few-shot object detection in a cross-domain setting. Xin et al. [192] evaluated FSOD from episodic-task and single-task perspectives. However, it is worth noting that these recent surveys did not delve into the intricacies of the training mechanisms that distinguish between standard FSOD, G-FSOD, I-FSOD, O-FSOD, and FSDAOD tasks.\nAnalyzing these distinctions is essential for conducting fair comparisons between works and fostering a deeper understanding of these research fields. In this paper, we take a unique perspective by examining few-shot works through the lens of data availability, providing greater clarity for researchers in this domain. We divide the FSOD task into five different categories: i) standard"}, {"title": "BACKGROUND ON OBJECT DETECTION", "content": "In this section, we provide an overview of generic object detection. Object detection involves the tasks of localizing and recognizing objects within an image. Specifically, an object detector aims to predict bounding boxes around each object while correctly identifying their respective categories. For those new to this field, comprehensive survey papers [114, 205, 227] offer valuable insights into object detection. The selection of model architectures significantly influences the performance of the object detection task. We categorize SOTA object detection model architectures into two primary categories:\n\u2022 Convolution Neural Network (CNN)-based object detectors and\n\u2022 Transformer-based object detectors.\nFig. 3 shows the taxonomy summarizing standard object detection architectures and we also present a summary of these standard object detectors in Table 3. In the following subsections, we will discuss each category in detail.\nCNN shows impressive performance on image object classification tasks [84] due to its capability of complex hierarchical features from the images. The research community has subsequently proposed leveraging these robust feature representations to enhance the performance of object detection tasks. These CNN-based object detectors can be classified into two categories:\n\u2022 Two-Stage detectors\n\u2022 Single-Stage detectors\nFaster R-CNN [148], in conjunction with Feature Pyramid Networks (FPN) [110], is one of the most popular two-stage architectures widely adopted in object detection. This approach is inspired from the Regions with CNN features (R-CNN) [46] and Fast R-CNN [45] methods.\nFig. 4 illustrates the network design of Faster R-CNN. In the initial stage, the object detector extracts features from the input image using a backbone network, resulting in single or multi-scale feature maps. These features are then input into the Region Proposal Network (RPN) [148], which generates object proposals as bounding boxes. These proposals are predicted at predefined locations, scales, and aspect ratios, refined using regression, and scored for objectness. Following this, Non- Maximum Suppression (NMS) [45] is applied to eliminate redundant and low-quality proposals. In the second stage, each object proposal undergoes further processing. A pooled feature map is extracted by resampling the features within its bounding box to a fixed size using techniques like RoIAlign or RoIPool. This pooled feature is then passed through the Box Head or Region-of-Interest (RoI) head, which predicts the object's category and refines the bounding box using regression. NMS is applied once more to remove redundant and low-confidence predictions. The combination of the RPN and the box head is referred to as the object detector in two-stage approaches. It is important to note that two-stage approaches like R-CNN [46], SPPNet [60], Fast R-CNN [45] and Faster R- CNN [148] require significant computational resources due to the NMS and RoI pooling processing steps, which increases the inference time and makes them highly sensitive to hyperparameters.\n Single-stage approaches were developed to address the complexity of two-stage detectors and optimize them for real-time applications. However, single- stage detectors may face challenges when it comes to detect dense and small objects. The pioneering You Only Look Once (YOLO) [145] is the first single-stage detector for object detection. YOLO splits images into a 7 \u00d7 7 grid, and for each grid cell, it predicts bounding boxes and class probabilities, resulting in a fixed number of predictions. This process is illustrated in Fig. 5. The approach of the YOLO model differs inherently from the iterative proposals and classifications of prior methods. Subsequently, various versions of YOLO [4, 99, 146, 147, 172] have been proposed to improve the detection performance further.\nIn order to improve the performance of single-stage detectors for small objects, the Single Shot MultiBox Detector (SSD) [119] introduced techniques such as multi-resolution and multi-reference. This involves dividing the input image into an S \u00d7 S grid, with different S values for different scales. For each grid cell, a set of anchor boxes with different aspect ratios and scales is considered. The network is then trained to predict bounding box offsets and confidences for each anchor box and class. SSD builds upon YOLO by using anchor boxes adjusted to different object shapes. Subsequently, several single-stage object detectors were introduced. In [111], Lin et al. proposed RetinaNet, which addresses the class imbalance issue between background and foreground classes by introducing focal loss. In [89], Law et al. introduced CornerNet, which first identifies critical points and then uses additional embedding information to decouple and re-group these points, effectively forming bounding boxes. ExtremeNet [231], on the other hand, addresses the difficulties of detecting corner points and proposes to detect extreme points, which often lie on an object and have consistent local appearance features that make them easier to detect. While techniques like CornerNet [89] and ExtremeNet [231] have introduced valuable approaches, they often involve costly post-processing steps, such as group-based keypoint assignment. In contrast, Zhou et al. introduced CenterNet [230], which streamlines the detection pipeline and eliminates the need for post-processing techniques like NMS, resulting in an efficient end-to-end detection network. These advancements collectively improve the accuracy and efficiency of single-stage detectors, particularly when addressing the challenges posed by small and densely packed objects.\nRecently, transformer-based architectures have led to significant improvements in solving language and vision problems. Carion et al. proposed a model called DETR [9], which treats object detection as a set prediction problem and proposed an end-to-end detection network with transformers. The architecture of the DETR object detector is shown in Fig. 6. Here, the image is fed to the backbone, and positional encodings are added to the features before being fed into the transformer encoder. The decoder takes object query embeddings as input and cross-attends to the encoded"}, {"title": "SURVEY OF FEW SHOT OBJECT DETECTION METHODS", "content": "Figure 7 intuitively illustrates the network flow for various FSOD settings, including standard FSOD, G-FSOD, I-FSOD, O-FSOD, and FSDAOD. These settings are also detailed in Algorithm 1, which outlines the base training, fine-tuning, and inference steps for each FSOD task. This section begins with a problem definition, including notations and the differences between FSOD settings. Following this, we will discuss the taxonomy of methods related to FSOD tasks in detail.\nLet (x, y) \u2208 D, where D represents the dataset consisting of images x paired with their correspond- ing labels y. The labels in y contain information about the category class label and bounding box coordinates. In FSOD research, the dataset D is typically divided into two subsets: the base dataset Dbase and the novel dataset Dnovel, where Dbase \u2229 Dnovel = \u00d8. Dbase contains a substantial amount of data and includes classes represented by labels ybase \u2208 CB (base classes). On the other hand, Dnovel contains only a few instances from each category, representing classes denoted by labels Ynovel \u2208 CN (novel classes). The standard notation for the few-shot object detection problem is 'K-shot, M-way,' where 'K' signifies the number of labeled instances per category, and 'M' represents the total number of distinct classes. For instance, in a '10-shot, 20-way' setting, the model learns to recognize 20 novel categories, each with 10 instances.\nBelow, we outline various settings for differentiating FSOD tasks.\n\u2022 Standard FSOD: The training process of standard FSOD methods involves two stages. Initially, a base model Mbase is trained on the dataset Dbase, which contains the target classes CB. The prior knowledge acquired in Mbase is subsequently transferred to Dfinetune = Dtrain_base \u222a Dtrain_novel, a dataset comprising both the base classes CB and few-shot classes CN. Finally, the evaluation is carried out on Dtest_novel, which includes only the novel classes CN. For example, detecting rare wildlife species often involves limited data, which can lead to overfitting and poor performance. FSOD enhances detection performance on rare objects by pre-training the model on abundant data from other species.\n\u2022 G-FSOD: The base training and fine-tuning phases are similar to FSOD. However, the evaluation is conducted on Dtest = Dtest_base \u222a Dtest_novel, which contains both the base classes CB and the novel classes CN. For instance, an autonomous vehicle in a dynamic urban environment must recognize rare objects without forgetting previously learned information. A small amount of data from base classes can be combined with new class data to ensure high performance across all categories.\n\u2022 I-FSOD: The base training process is identical to FSOD. However, during the fine-tuning stage, only the novel classes dataset Dtrain_novel, containing C\u0145, is used, without access to the base dataset Dbase. The evaluation process is the same as in G-FSOD and is conducted on Dtest, which includes both the sets of base classes CB and the novel classes CN. The primary goal of I-FSOD is to learn the novel classes CN while avoiding catastrophic forgetting of the base classes CB. For instance, consider an AI system designed to recognize new faces with limited data. Due to privacy regulations, it is not allowed to store images and information of previously recognized faces. In this scenario, I-FSOD ensures that the face recognition system maintains high accuracy in identifying new and previously encountered individuals while adhering to privacy constraints.\n\u2022 O-FSOD: The base training is performed on Dtrain_base which contains abundant base classes CB while the fine-tuning is performed on Dtrain_novel with scarce novel classes CN which together forms the known classes CK = CB \u222a CN. The final model is tested on Dtest = Dtest_base \u222a Dtest_novel \u222a Dtest_unknown which contains the test classes Ctest, where Ctest = CK \u222a CU, Cu is the unknown class, and CK \u2229 Cu = 0. The goal is to employ the unbalanced data to train a detector, which can be used to identify the base classes, the novel classes, and the unknown class. For example, an autonomous vehicle may encounter an unusual type of construction equipment or a new kind of road obstacle, like debris or temporary signs. With O-FSOD, the vehicle's detection system can quickly learn to recognize these new objects using just a few labeled examples, allowing it to navigate safely around them.\n\u2022 FSDAOD: The model is initially trained on a source domain dataset Dsource, which is sufficiently large and contains classes Csource. It is then adapted to a target domain dataset Dtarget, which is data-scarce and contains classes Ctarget. This approach addresses transfer scenarios involving domain discrepancies between the source and target distributions. The label space is identical for both domains, i.e., Csource = Ctarget, but the data distributions differ. During the base training stage, the source dataset serves as the base dataset. In the fine-tuning/adaptation stage, the target training dataset Dtarget is used, and the evaluation is performed on the target testing dataset Dtarget."}, {"title": "Standard Few Shot Object Detection (FSOD)", "content": "We classified the standard FSOD approaches into subcategories based on the proposed techniques: 1) Meta-learning based approaches, 2) Metric learning and classification refinement-based approaches, 3) Data sampling and scale variation-based approaches, 4) Attention mechanism and feature enhancement-based approaches, 5) Class margin and knowledge transfer based approaches, 6) Proposal generation and quality improvement based approaches. In the following subsections, we discuss these different approaches in detail.\nMeta-learning is a widely used approach in few-shot learning that enables models to acquire the ability to learn. By exposing models to various training scenarios with limited data, meta-learning allows quick adaptation and generalization to new tasks. The support set utilized in meta-training includes examples from base tasks, which helps the model grasp general patterns. On the other hand, the query set used during evaluation consists of examples from new tasks to evaluate the model's generalization and prediction capabilities. Mainly meta-learning approaches adopt either feature reweighting [74, 182] or its variants to aggregate query and support features [91, 219] to tackle the FSOD problem.\nIn this category, YOLO-FR [74] addressed the FSOD problem by using a single-stage YOLO-v2 object detection model. This approach utilizes meta-training to extract generalizable meta-features from fully labeled base classes. A reweighting module is used to assign importance to meta-features for novel object detection. By taking support images as input, embedding them into class-specific representations, and using these embeddings to reweigh the meta-features, YOLO-FR produce more crucial features for detecting new target objects.\nWang et al. [182] introduced a unified meta-learning approach known as Meta-Det for few-shot classification and localization tasks. This separates the learning process of category-agnostic and category-specific parameters in CNN-based detectors, specifically Faster-RCNN. Meta-Det is initially trained with a large dataset to acquire category-agnostic parameters and then fine-tuned with samples from few-shot tasks to learn category-specific parameters. Despite the challenges of effectively learning from limited examples, the authors utilize a meta-model trained through a meta-training procedure to estimate category-agnostic transformations and parametrized weights for classification based on these transformations. In [200], Yan et al. extended the meta-learning capabilities to both object detection and segmentation tasks using Faster/Mask R-CNN (referred to as Meta-RCNN). The meta-predictor head of Meta-RCNN predicts bounding boxes and segmentation masks based on RoI features generated by Meta-RCNN using the support set.\nThe above-mentioned meta-learning strategies utilized a single prototype for each category derived from support samples. Recent advancements aim to enhance the utilization of information from each support sample. Lee et al. [91] introduced the concept of Attending to Per-Sample- Prototype (APSP), which treats each support sample as an individual prototype. By employing an attention mechanism, APSP enhances model feature representations by capturing shared informa- tion among these individual prototypes. This versatile module can be seamlessly integrated into existing meta-learning frameworks. In contrast, Support-Query Mutual Guidance (SQMGH) [219] utilizes a support-query mutual guidance approach to obtain more relevant support proposals. This method generates the final aggregated support feature using a query guidance strategy and achieves mutual guidance between support and query features using contrastive loss and focal loss. Han et al. [55] observed that proposals for few-shot classes tend to be less accurate compared to those for many-shot classes, resulting in problems like missing boxes due to misclassification or imprecise spatial locations from noisy RPN proposals. To overcome this limitation, Han et al. introduced the prototype matching network known as Meta Faster R-CNN. This method replaces the traditional linear object classifier in RPN with a \u201cMeta-Classifier", "104": "introduced the Meta RetinaNet method", "211": "introduced an approach utilizing transformer architectures. Their solution", "87": "Lai et al. introduced a novel approach called AHT to address the limitations of previous approaches. ATH incorporates multi-level fine-grained two-branch interactions and dynamically generates class-specific primary network weights using a feed-forward meta-learning approach. The proposed method consists of two modules: the Dynamic Aggregation Module (DAM) adap- tively generates inter-image prominent features into aggregated weights", "29": "addressed the drawback of meta-learning-based methods utilizing a single K-average-pooled prototype in both RPN and detection head for query detection. This straightforward approach impacts FSOD performance in two key ways: 1) the poor quality of the prototype and 2) the equivocal guidance due to the contradictions between the RPN and the detection head. Du et al. prioritize salient representations and de-emphasize trivial variations by accessing both angle distance and magnitude dispersion (\u03c3) across K-support samples to generate high-quality proposals. It robustly deals with intra-class variations", "FSOD": "metric learning and classification refinement. Metric learning focuses on learning a similarity function that can accurately measure the similarity or dissimilarity between different objects. By learning an effective metric", "75": "which uses a unique strategy for FSOD by employing metric learning based on class representatives. Here", "127": "proposed DMNet that improves efficiency with a single-stage FSOD design. DMNet comprises two key components: Decoupled Representation Transformation (DRT) and Image-Level DML (IDML). DRT focuses on three areas: 1) extracting foreground representations to filter backgrounds", "232": "introduced a method called Semantic Relation Reason- ing for Shot-Stable Few-Shot Object Detection (SRR-FSD)", "challenges": "destructive samples and limited inter-class separability [107", "107": "addressed these challenges through two strategies: (i) Confidence-Guided Dataset Pruning (CGDP) to refine the training dataset by eliminating distractions", "125": "addressed the destructive samples and inter-class separability issues with three components: a memory-based classifier (MemCla), a fully connected neural network classifier (FCC), and an adaptive fusion block (AdFus). MemCla stores embedding vectors to preserve previously encountered classes and enhance inter-class separability. FCC predicts categories based on current features. AdFus adjusts the fusion method between MemCla and FCC based on available sam- ples, ensuring adaptability. However, empirical evidence of this approach preventing catastrophic forgetting is lacking, which is crucial for improving inter-class separability.\nZhang et al. [206", "224": "the absence of just one high-IOU training box during RPN training can significantly impact the classifier's ability to capture object appearance variations. To overcome this issue, they introduced CoRPN (Cooperating RPNs), which involves training multiple redundant RPNs. These RPNs work independently but collaborate to ensure that if one misses a high-IOU box, another will likely detect it. This approach enhances the quality of proposals and ultimately aids in classifier training with limited data.\nIn a subsequent study, Zhang et al. [223", "76": "take a different approach by utilizing unlabeled data and a pseudo-labeling technique to enhance proposal quality. They generate high-quality pseudo-annotations for novel categories by leveraging unlabeled images in a few-shot adaptation. This involves building a classifier for novel categories using features from a self-supervised network to verify candidate detections and training a specialized box regressor to refine the bounding boxes of verified candi- dates. Through this two-step verification and refinement process, they can achieve high-precision pseudo-annotations, effectively balancing the training data and boosting the performance of FSOD.\nOn the other hand, Han et al. [57", "130": "highlight the drawbacks of using meta-learning and transfer learning-based approaches, where images from the base set containing unlabeled novel-class objects can easily lead to performance degradation and poor plasticity since those novel objects are served as the background. In contrast, MRSN uses a semi-supervised framework to identify unlabeled novel class instances. It includes a mining model to discover these instances and an absorbed model to learn from them. It designs the Proposal Contrastive Consistency (PCC) module in the absorbed model to exploit class characteristics and avoid bias from noise labels. It utilizes PCC at the proposal level to compare the global and local information of the instance simultaneously.\nIn [12", "56": "studied FSOD using various foundational models for visual feature extraction and few-shot proposal classification. They proposed a method called FM-FSOD, which is evaluated on multiple pre-trained vision models [80, 136, 143"}]}