{"title": "Beyond Few-shot Object Detection: A Detailed Survey", "authors": ["Vishal Chudasama", "Hiran Sarkar", "Pankaj Wasnik", "Vineeth N Balasubramanian", "Jayateja Kalla"], "abstract": "Object detection is a critical field in computer vision focusing on accurately identifying and locating specific objects in images or videos. Traditional methods for object detection rely on large labeled training datasets for each object category, which can be time-consuming and expensive to collect and annotate. To address this issue, researchers have introduced few-shot object detection (FSOD) approaches that merge few-shot learning and object detection principles. These approaches allow models to quickly adapt to new object categories with only a few annotated samples. While traditional FSOD methods have been studied before, this survey paper comprehensively reviews FSOD research with a specific focus on covering different FSOD settings such as standard FSOD, generalized FSOD, incremental FSOD, open-set FSOD, and domain adaptive FSOD. These approaches play a vital role in reducing the reliance on extensive labeled datasets, particularly as the need for efficient machine learning models continues to rise. This survey paper aims to provide a comprehensive understanding of the above-mentioned few-shot settings and explore the methodologies for each FSOD task. It thoroughly compares state-of-the-art methods across different FSOD settings, analyzing them in detail based on their evaluation protocols. Additionally, it offers insights into their applications, challenges, and potential future directions in the evolving field of object detection with limited data.", "sections": [{"title": "1 INTRODUCTION", "content": "Object detection has experienced remarkable advancements in recent years due to significant progress in deep learning techniques, as demonstrated by methods like Faster R-CNN [148], YOLO [146], and DETR [9]. The primary goal of object detection is to accurately identify and locate objects within an image while also categorizing these objects into specific predefined classes. However, traditional deep learning approaches for object detection heavily depend on large-scale labeled training datasets [84]. This dependence poses significant challenges in real-world scenarios where collecting large amounts of data is often impractical [156]. Acquiring a sufficient number of images can be infeasible, and annotating these images for object detection is both expensive and time-consuming. Additionally, training complex deep learning models with limited data frequently results in overfitting issues, where the model performs well on the training data but fails to generalize to unseen data. In contrast, humans possess an exceptional ability to learn new concepts with minimal data, especially during the early stages of development. For example, children can quickly identify and differentiate new objects after encountering them only a few times. Inspired by this impressive human capacity, a setting called few-shot learning [156, 180] has emerged, where the model is trained to learn from a few number of samples.\nFig. 1 illustrates the standard few-shot learning regime, where a model is initially trained on a large amount of labeled data and adapts to new classes with a significantly smaller number of samples. In the field of few-shot object detection (FSOD), the objective is to detect certain objects using only a limited number of annotated instances, thus eliminating the need for extensive annotated data, which is a primary limitation of state-of-the-art object detection approaches. The first stage in FSOD is to pre-train the model using a large amount of data from known classes, called base classes. In the second stage, this knowledge enables the model to recognize new classes, termed novel classes, with only a few examples.\nFSOD is important in various real-world applications where obtaining a large amount of annotated data is challenging, expensive, or time-consuming. In medical imaging [92], FSOD methods can help identify rare new diseases from a limited number of labeled examples, enabling quicker diagnosis and treatment. In wildlife conservation, FSOD methods can help monitor endangered species with minimal data, supporting conservation efforts. FSOD is also valuable in industrial inspection [166, 228], where defects or anomalies in manufacturing processes can be detected with limited training samples, enhancing quality control. In security and surveillance applications, FSOD can detect suspicious activities or objects with minimal labeled data, improving safety and response times. FSOD methods also have a vital role to play in other domains such as remote sensing or multispectral imaging [68, 188], as well as other settings such as cross-domain generalization [42, 134], further broadening its applicability and impact across diverse fields. Given the increased significance, newer variants of the traditional FSOD setting have emerged in recent years. Exploring this expansion of the FSOD setting in recent literature is the key objective of this survey.\nFig. 2 outlines the evolution of the few-shot object detection task in recent years. While the initial efforts focused on standard FSOD, extensions and variants have emerged as listed below:\n\u2022 Standard FSOD\n\u2022 Generalized FSOD (G-FSOD)\n\u2022 Incremental FSOD (I-FSOD)\n\u2022 Open-set FSOD (O-FSOD)\n\u2022 Domain Adaptation based FSOD (FSDAOD)\nStandard FSOD eliminates the dependency on vast amounts of labeled training data, and its primary focus lies in enhancing performance in novel classes rather than maintaining performance in base classes. Chen et al.  provide the first paper in this direction in their work LSTD [11]. Nevertheless, learning new classes while maintaining performance in base classes is often crucial in real-world applications. To address this challenge, two other tasks, generalized few-shot object detection (G-FSOD) and incremental few-shot object detection (I-FSOD), aim to perform strongly on both base and novel classes. G-FSOD tackles the challenge of proper knowledge retention of the base classes while learning the new classes. TFA [179] is the first work in the task of G-FSOD that provides results on both the base and novel classes. Both standard FSOD and G-FSOD rely on the availability of the base classes while learning the new classes. However, it is not feasible in the real-world scenario to avail the old classes while learning the new classes. ONCE [140] is the first to address this issue and introduce the task of I-FSOD. I-FSOD, unlike the previous tasks, does not require the old classes to adapt to new ones.\nOpen-set few-shot object detection (O-FSOD) is another FSOD sub-category that focuses on not only detecting objects of trained classes but also detecting objects of unseen classes. In many real-world scenarios, it is impractical to pre-define or pre-label all possible object categories, making it essential for systems to recognize and handle new or rare objects that weren't part of the initial training set. FOOD [159] is the first work proposed in this direction by Su et al.. Another sub-category of FSOD, called Few-shot domain adaptive object detection (FSDAOD), involves adapting a detector to a new domain. FSDAOD is trained on a source domain with abundant data, which is then adapted to a new domain with only a small amount of data. This is particularly useful in real-world applications where collecting extensive labeled data for every possible domain is impractical. Wang et al. [178] proposed the first work in this direction, which can generalize to a target domain given a few samples. All of these different approaches aim to tackle the difficulties associated with limited data in real-world scenarios, striving to balance the need for recognizing new classes and maintaining the accuracy of known classes.\nThe above-mentioned variants of standard FSOD, G-FSOD, I-FSOD, O-FSOD and FSDAOD tasks differ primarily based on the availability of training data and the classes on which a model is evaluated. This survey seeks to comprehensively study these variants and analyze the developments beyond the standard FSOD setting in recent years. We begin with a comparison of existing FSOD-based survey papers in Section 2, and clarify the need for this survey. We then briefly cover the background of object detection in Section 3. The problem statement with detailed notations and differences between few shot tasks are provided in Section 4.1. Then, we comprehensively review research works related to each few-shot task in Subsections 4.2 to 4.6. This thorough review offers an overview of the recent state-of-the-art research in all these approaches. Benchmark datasets and evaluation protocols are discussed in Section 5. The result analysis of all these methods is discussed in Section 6. Finally, various research directions, applications and challenges in this field are explored in Section 7."}, {"title": "2 COMPARISON WITH RELATED SURVEY PAPERS", "content": "In the field of FSOD learning, several surveys [1, 21, 64, 69, 72, 83, 118, 151, 192] have been conducted to investigate and analyze various aspects of this domain. a detailed summary of the existing FSOD-based survey papers papers can be found in Table 1, while a comparison between our survey paper with the existing FSOD-based survey are depicted in Table 2.\nNotably, Huang et al. [64] focused on exploring the fusion of self-supervised representations with FSOD, emphasizing the importance of self-supervision pre-training in improving object detection tasks. Additionally, they discussed the challenges associated with integrating self-supervised representations with detection techniques. Another significant contribution was made by Kohler et al. [83], who provided a comprehensive overview of the current state-of-the-art (SOTA) in FSOD methods, categorizing these approaches based on their training schemes and architectural layouts. Jiaxu et al. [72] presented a data-driven taxonomy of the training data and the type of corresponding supervision utilized during the training phase. Huang et al. [69] conducted a study on low-shot object detection, encompassing zero-shot, one-shot, and few-shot object detection. Antonelli et al. [1] dissected FSOD methods into categories such as data augmentation, transfer learning, distance metric learning, and meta-learning-based approaches. Zhang et al. [213] delved into the realm of few-shot class incremental learning and object detection from both anchor-free and anchor-based perspectives. The authors in [83, 118] also examined the extensive field of FSOD, with [118] analyzing existing FSOD algorithms from a new viewpoint based on their contributions, and [83] categorizing approaches based on their training scheme and architectural layout, broadly classifying them into meta-learning and transfer-learning based methods. Sa et al. [151] reviewed standard FSOD models focusing on few-shot object detection in a cross-domain setting. Xin et al. [192] evaluated FSOD from episodic-task and single-task perspectives. However, it is worth noting that these recent surveys did not delve into the intricacies of the training mechanisms that distinguish between standard FSOD, G-FSOD, I-FSOD, O-FSOD, and FSDAOD tasks.\nAnalyzing these distinctions is essential for conducting fair comparisons between works and fostering a deeper understanding of these research fields. In this paper, we take a unique perspective by examining few-shot works through the lens of data availability, providing greater clarity for researchers in this domain. We divide the FSOD task into five different categories: i) standard"}, {"title": "3 BACKGROUND ON OBJECT DETECTION", "content": "In this section, we provide an overview of generic object detection. Object detection involves the tasks of localizing and recognizing objects within an image. Specifically, an object detector aims to predict bounding boxes around each object while correctly identifying their respective categories. For those new to this field, comprehensive survey papers [114, 205, 227] offer valuable insights into object detection. The selection of model architectures significantly influences the performance of the object detection task. We categorize SOTA object detection model architectures into two primary categories:\n\u2022 Convolution Neural Network (CNN)-based object detectors and\n\u2022 Transformer-based object detectors.\nFig. 3 shows the taxonomy summarizing standard object detection architectures and we also present a summary of these standard object detectors in Table 3. In the following subsections, we will discuss each category in detail."}, {"title": "3.1 CNN-based object detectors", "content": "CNN shows impressive performance on image object classification tasks [84] due to its capability of complex hierarchical features from the images. The research community has subsequently proposed leveraging these robust feature representations to enhance the performance of object detection tasks. These CNN-based object detectors can be classified into two categories:\n\u2022 Two-Stage detectors\n\u2022 Single-Stage detectors"}, {"title": "3.1.1 Two-Stage CNN-based Object Detectors:", "content": "Faster R-CNN [148], in conjunction with Feature Pyramid Networks (FPN) [110], is one of the most popular two-stage architectures widely adopted in object detection. This approach is inspired from the Regions with CNN features (R-CNN) [46] and Fast R-CNN [45] methods.\nFig. 4 illustrates the network design of Faster R-CNN. In the initial stage, the object detector extracts features from the input image using a backbone network, resulting in single or multi-scale feature maps. These features are then input into the Region Proposal Network (RPN) [148], which generates object proposals as bounding boxes. These proposals are predicted at predefined locations, scales, and aspect ratios, refined using regression, and scored for objectness. Following this, Non-Maximum Suppression (NMS) [45] is applied to eliminate redundant and low-quality proposals. In the second stage, each object proposal undergoes further processing. A pooled feature map is extracted by resampling the features within its bounding box to a fixed size using techniques like RoIAlign or RoIPool. This pooled feature is then passed through the Box Head or Region-of-Interest (RoI) head, which predicts the object's category and refines the bounding box using regression. NMS is applied once more to remove redundant and low-confidence predictions. The combination of the RPN and the box head is referred to as the object detector in two-stage approaches. It is important to note that two-stage approaches like R-CNN [46], SPPNet [60], Fast R-CNN [45] and Faster R-CNN [148] require significant computational resources due to the NMS and RoI pooling processing steps, which increases the inference time and makes them highly sensitive to hyperparameters."}, {"title": "3.1.2 Single-Stage CNN-based Object Detectors:", "content": "Single-stage approaches were developed to address the complexity of two-stage detectors and optimize them for real-time applications. However, single-stage detectors may face challenges when it comes to detect dense and small objects. The pioneering You Only Look Once (YOLO) [145] is the first single-stage detector for object detection. YOLO splits images into a 7 \u00d7 7 grid, and for each grid cell, it predicts bounding boxes and class probabilities, resulting in a fixed number of predictions. This process is illustrated in Fig. 5. The approach of the YOLO model differs inherently from the iterative proposals and classifications of prior methods. Subsequently, various versions of YOLO [4, 99, 146, 147, 172] have been proposed to improve the detection performance further.\nIn order to improve the performance of single-stage detectors for small objects, the Single Shot MultiBox Detector (SSD) [119] introduced techniques such as multi-resolution and multi-reference. This involves dividing the input image into an S \u00d7 S grid, with different S values for different scales. For each grid cell, a set of anchor boxes with different aspect ratios and scales is considered. The network is then trained to predict bounding box offsets and confidences for each anchor box and class. SSD builds upon YOLO by using anchor boxes adjusted to different object shapes. Subsequently, several single-stage object detectors were introduced. In [111], Lin et al. proposed RetinaNet, which addresses the class imbalance issue between background and foreground classes by introducing focal loss. In [89], Law et al. introduced CornerNet, which first identifies critical points and then uses additional embedding information to decouple and re-group these points, effectively forming bounding boxes. ExtremeNet [231], on the other hand, addresses the difficulties of detecting corner points and proposes to detect extreme points, which often lie on an object and have consistent local appearance features that make them easier to detect. While techniques like CornerNet [89] and ExtremeNet [231] have introduced valuable approaches, they often involve costly post-processing steps, such as group-based keypoint assignment. In contrast, Zhou et al. introduced CenterNet [230], which streamlines the detection pipeline and eliminates the need for post-processing techniques like NMS, resulting in an efficient end-to-end detection network. These advancements collectively improve the accuracy and efficiency of single-stage detectors, particularly when addressing the challenges posed by small and densely packed objects."}, {"title": "3.2 Transformer-based object detectors", "content": "Recently, transformer-based architectures have led to significant improvements in solving language and vision problems. Carion et al.  proposed a model called DETR [9], which treats object detection as a set prediction problem and proposed an end-to-end detection network with transformers. The architecture of the DETR object detector is shown in Fig. 6. Here, the image is fed to the backbone, and positional encodings are added to the features before being fed into the transformer encoder. The decoder takes object query embeddings as input and cross-attends to the encoded representation while performing self-attention on the transformed query embeddings. It then outputs a fixed number of object detections, which are finally thresholded, without needing NMS.\nOn top of DETR, various models are been proposed. In [238], Zhu et al. proposed Deformable DETR to address the long convergence issues in DETR, where attention modules only attend to a small set of key sampling points around a reference. Dai et al.  proposed unsupervised pre-training DETR, where they took inspiration from natural language pre-trained transformers and used crop patches from the given image as queries to the decoder. Recently, Liu et al. introduced Swin Transformer [123], a hierarchical Transformer with shifted windows that enhance efficiency by limiting self-attention computation to non-overlapping local windows. Swin Transformer V2 [122] builds upon this architecture, further scaling model capacity and window resolution. DAB-DETR [117] formulates DETR queries as dynamic anchor boxes (DAB), bridging the gap between anchor-based and DETR-like detectors. DN-DETR [100] addresses bipartite matching instability by introducing denoising (DN) techniques. Building on these ideas, DINO (DETR with Improved deNoising anchOr box) [212] proposes contrastive denoising training and mixed query selection to enhance object detection performance."}, {"title": "4 SURVEY OF FEW SHOT OBJECT DETECTION METHODS", "content": "Figure 7 intuitively illustrates the network flow for various FSOD settings, including standard FSOD, G-FSOD, I-FSOD, O-FSOD, and FSDAOD. These settings are also detailed in Algorithm 1, which outlines the base training, fine-tuning, and inference steps for each FSOD task. This section begins with a problem definition, including notations and the differences between FSOD settings. Following this, we will discuss the taxonomy of methods related to FSOD tasks in detail."}, {"title": "4.1 Problem Definition and Difference Between FSOD Settings", "content": "Let (x, y) \u2208 D, where D represents the dataset consisting of images x paired with their corresponding labels y. The labels in y contain information about the category class label and bounding box coordinates. In FSOD research, the dataset D is typically divided into two subsets: the base dataset $D_{base}$ and the novel dataset $D_{novel}$, where $D_{base} \\cap D_{novel} = \\varnothing$. $D_{base}$ contains a substantial amount of data and includes classes represented by labels $y_{base} \\in C_B$ (base classes). On the other hand, $D_{novel}$ contains only a few instances from each category, representing classes denoted by labels $y_{novel} \\in C_N$ (novel classes). The standard notation for the few-shot object detection problem is 'K-shot, M-way,' where 'K' signifies the number of labeled instances per category, and 'M' represents the total number of distinct classes. For instance, in a '10-shot, 20-way' setting, the model learns to recognize 20 novel categories, each with 10 instances.\nBelow, we outline various settings for differentiating FSOD tasks.\n\u2022 Standard FSOD: The training process of standard FSOD methods involves two stages. Initially, a base model $M_{base}$ is trained on the dataset $D_{base}$, which contains the target classes $C_B$. The prior knowledge acquired in $M_{base}$ is subsequently transferred to $D_{finetune} = D^{train}_{base} \\cup D^{train}_{novel}$, a dataset comprising both the base classes $C_B$ and few-shot classes $C_N$. Finally, the evaluation is carried out on $D^{test}_{novel}$, which includes only the novel classes $C_N$. For example, detecting rare wildlife species often involves limited data, which can lead to overfitting and poor performance. FSOD enhances detection performance on rare objects by pre-training the model on abundant data from other species.\n\u2022 G-FSOD: The base training and fine tuning phases are similar to FSOD. However, the evaluation is conducted on $D^{test} = D^{test}_{base} \\cup D^{test}_{novel}$, which contains both the base classes $C_B$ and the novel classes $C_N$. For instance, an autonomous vehicle in a dynamic urban environment must recognize rare objects without forgetting previously learned information. A small amount of data from base classes can be combined with new class data to ensure high performance across all categories.\n\u2022 I-FSOD: The base training process is identical to FSOD. However, during the fine-tuning stage, only the novel classes dataset $D^{train}_{novel}$, containing $C_N$, is used, without access to the base dataset $D_{base}$. The evaluation process is the same as in G-FSOD and is conducted on $D^{test}$, which includes both the sets of base classes $C_B$ and the novel classes $C_N$. The primary goal of I-FSOD is to learn the novel classes $C_N$ while avoiding catastrophic forgetting of the base classes $C_B$. For instance, consider an AI system designed to recognize new faces with limited data. Due to privacy regulations, it is not allowed to store images and information of previously recognized faces. In this scenario, I-FSOD ensures that the face recognition system maintains high accuracy in identifying new and previously encountered individuals while adhering to privacy constraints.\n\u2022 O-FSOD: The base training is performed on $D^{train}_{base}$ which contains abundant base classes $C_B$ while the fine-tuning is performed on $D^{train}_{novel}$ with scarce novel classes $C_N$ which together forms the known classes $C_K = C_B \\cup C_N$. The final model is tested on $D^{test} = D^{test}_{base} \\cup D^{test}_{novel} \\cup D_{unknown}$ which contains the test classes $C_{test}$, where $C_{test} = C_K \\cup C_U$, $C_U$ is the unknown class, and $C_K \\cap C_U = \\varnothing$. The goal is to employ the unbalanced data to train a detector, which can be used to identify the base classes, the novel classes, and the unknown class. For example, an autonomous vehicle may encounter an unusual type of construction equipment or a new kind of road obstacle, like debris or temporary signs. With O-FSOD, the vehicle's detection system can quickly learn to recognize these new objects using just a few labeled examples, allowing it to navigate safely around them."}, {"title": "4.2 Standard Few Shot Object Detection (FSOD)", "content": "We classified the standard FSOD approaches into subcategories based on the proposed techniques: 1) Meta-learning based approaches, 2) Metric learning and classification refinement-based approaches, 3) Data sampling and scale variation-based approaches, 4) Attention mechanism and feature enhancement-based approaches, 5) Class margin and knowledge transfer based approaches, 6) Proposal generation and quality improvement based approaches. In the following subsections, we discuss these different approaches in detail."}, {"title": "4.2.1 Meta-Learning Approaches:", "content": "Meta-learning is a widely used approach in few-shot learning that enables models to acquire the ability to learn. By exposing models to various training scenarios with limited data, meta-learning allows quick adaptation and generalization to new tasks. The support set utilized in meta-training includes examples from base tasks, which helps the model grasp general patterns. On the other hand, the query set used during evaluation consists of examples from new tasks to evaluate the model's generalization and prediction capabilities. Mainly meta-learning approaches adopt either feature reweighting [74, 182] or its variants to aggregate query and support features [91, 219] to tackle the FSOD problem.\nIn this category, YOLO-FR [74] addressed the FSOD problem by using a single-stage YOLO-v2 object detection model. This approach utilizes meta-training to extract generalizable meta-features from fully labeled base classes. A reweighting module is used to assign importance to meta-features for novel object detection. By taking support images as input, embedding them into class-specific representations, and using these embeddings to reweigh the meta-features, YOLO-FR produce more crucial features for detecting new target objects.\nWang et al. [182] introduced a unified meta-learning approach known as Meta-Det for few-shot classification and localization tasks. This separates the learning process of category-agnostic and category-specific parameters in CNN-based detectors, specifically Faster-RCNN. Meta-Det is initially trained with a large dataset to acquire category-agnostic parameters and then fine-tuned with samples from few-shot tasks to learn category-specific parameters. Despite the challenges of effectively learning from limited examples, the authors utilize a meta-model trained through a meta-training procedure to estimate category-agnostic transformations and parametrized weights for classification based on these transformations. In [200], Yan et al. extended the meta-learning capabilities to both object detection and segmentation tasks using Faster/Mask R-CNN (referred to as Meta-RCNN). The meta-predictor head of Meta-RCNN predicts bounding boxes and segmentation masks based on RoI features generated by Meta-RCNN using the support set.\nThe above-mentioned meta-learning strategies utilized a single prototype for each category derived from support samples. Recent advancements aim to enhance the utilization of information from each support sample. Lee et al.  introduced the concept of Attending to Per-Sample-Prototype (APSP), which treats each support sample as an individual prototype. By employing an attention mechanism, APSP enhances model feature representations by capturing shared information among these individual prototypes. This versatile module can be seamlessly integrated into existing meta-learning frameworks. In contrast, Support-Query Mutual Guidance (SQMGH) [219] utilizes a support-query mutual guidance approach to obtain more relevant support proposals. This method generates the final aggregated support feature using a query guidance strategy and achieves mutual guidance between support and query features using contrastive loss and focal loss.\nHan et al. [55] observed that proposals for few-shot classes tend to be less accurate compared to those for many-shot classes, resulting in problems like missing boxes due to misclassification or imprecise spatial locations from noisy RPN proposals. To overcome this limitation, Han et al. introduced the prototype matching network known as Meta Faster R-CNN. This method replaces the traditional linear object classifier in RPN with a \u201cMeta-Classifier\", showcasing enhanced accuracy of produced bounding boxes for few-shot classes. Li et al. [104] introduced the Meta RetinaNet method, incorporating the single-stage RetinaNet architecture. The authors argue that the focal loss function in RetinaNet helps alleviate bias towards base classes, ultimately improving generalization on new classes by enhancing proposals. Additionally, they propose a balanced loss to work alongside the focal loss, boosting performance in the FSOD scenario. Nonetheless, the approach does not address how RetinaNet handles noisy region proposals.\nWithin the region-based detection framework, the accuracy of the final predictions relies heavily on the proposed regions. However, generating high-quality region proposals is challenging in\""}, {"title": "4.2.2 Metric Learning and Classification Refinement:", "content": "This section explores two key techniques in standard FSOD: metric learning and classification refinement. Metric learning focuses on learning a similarity function that can accurately measure the similarity or dissimilarity between different objects. By learning an effective metric, the model can better distinguish between similar and dissimilar objects, even with limited data. Classification refinement, on the other hand, improves a pre-trained classifier's decision-making, often tackling specific data challenges with ease.\nOne notable work is Representative-based metric learning (RepMet) [75], which uses a unique strategy for FSOD by employing metric learning based on class representatives. Here, each class is represented by a multi-modal mixture model, with representative vectors as the centers, capturing intra-class variations and creating a customized embedding space for similarity-based classification. Instead of using a traditional classifier head, a subnet calculates class posteriors for each region of interest (ROI) by comparing its embedding vector to the class representatives. This architecture allows joint training of the embedding space and mixture distributions, enabling few-shot capabilities. Once trained, the distance metric learner classifier can easily accommodate new categories with minimal supervision, transforming into a powerful few-shot detector.\nRepMet has the potential for few-shot detection but is inefficient due to its two-stage approach with ROI pooling in the distance metric learning (DML) module. Lu et al.  proposed DMNet that improves efficiency with a single-stage FSOD design. DMNet comprises two key components: Decoupled Representation Transformation (DRT) and Image-Level DML (IDML). DRT focuses on three areas: 1) extracting foreground representations to filter backgrounds, 2) predicting adaptive anchor shapes to improve over manually crafted ones, and 3) adjusting features for classification and localization tasks based on their receptive field needs. This separation optimizes feature learning for each task. On the other hand, IDML works on the entire feature map, enabling parallel multi-object inference and boosting efficiency and generalization. This integration fits smoothly into single-stage detection pipelines, marking DMNet as a significant advancement in FSOD."}, {"title": "4.2.3 Proposal Generation and Quality Improvement:", "content": "This section explores various methods to improve the quality of proposals in object detection networks to tackle challenges in FSOD. Proposal generation is a popular step in an object detection task, and therefore optimizing this process is crucial for enhancing detection accuracy and efficiency in few-shot object detection scenarios.\nAccording to Zhang et al. [224], the absence of just one high-IOU training box during RPN training can significantly impact the classifier's ability to capture object appearance variations. To overcome this issue, they introduced CoRPN (Cooperating RPNs), which involves training multiple redundant RPNs. These RPNs work independently but collaborate to ensure that if one misses a high-IOU box, another will likely detect it. This approach enhances the quality of proposals and ultimately aids in classifier training with limited data.\nIn a subsequent study, Zhang et al. introduced the Hallucinator Network, which generates additional training examples in the Region of Interest (RoI) feature space. These examples are then integrated into the object detection model to select high-IOU boxes. The authors emphasized the importance of effectively addressing the lack of variation in training data for extremely few-shot detection performance. Training the hallucinator and the detector's classifier using an expectation-maximization (EM)-like approach is crucial in addressing this issue.\nKaul et al.  take a different approach by utilizing unlabeled data and a pseudo-labeling technique to enhance proposal quality. They generate high-quality pseudo-annotations for novel categories by leveraging unlabeled images in a few-shot adaptation. This involves building a classifier for novel categories using features from a self-supervised network to verify candidate detections and training a specialized box regressor to refine the bounding boxes of verified candidates. Through this two-step verification and refinement process, they can achieve high-precision pseudo-annotations, effectively balancing the training data and boosting the performance of FSOD.\nOn the other hand, Han et al.  draw inspiration from the transformer architecture to propose a cross-transformer RoI feature extractor called FCT. This method integrates the transformer architecture into the backbone network to improve proposal quality. By incorporating self-attention mechanisms, FCT can capture long-range dependencies and contextual information within the features. This can potentially enhance proposal generation and object detection in few-shot scenarios.\nMa et al. in their work MRSN [130], highlight the drawbacks of using meta-learning and transfer learning-based approaches, where images from the base set containing unlabeled novel-class objects can easily lead to performance degradation and poor plasticity since those novel objects are served as the background. In contrast, MRSN uses a semi-supervised framework to identify unlabeled novel class instances. It includes a mining model to discover these instances and an absorbed model to learn from them. It designs the Proposal Contrastive Consistency (PCC) module in the absorbed model to exploit class characteristics and avoid bias from noise labels. It utilizes PCC at the proposal level to compare the global and local information of the instance simultaneously.\nIn [12], Chen et al.  identified issues with previous methods, such as increased latency from extensive fine-tuning and subpar performance when adapting to new classes. To address these shortcomings, they introduced a new FSOD model called CRTED. This model utilizes a correlation-aware region proposal network (Correlation-RPN) structure to enhance detectors' object localization and generalization capabilities. The CRTED model focuses on learning object-specific features related to inter-class correlation and intra-class compactness while minimizing object-agnostic background features, even with limited annotated samples. This approach fosters the learning of correlated features across different categories, which in turn facilitates the transfer of knowledge from base to novel categories for object detection.\nHan et al.  studied FSOD using various foundational models for visual feature extraction and few-shot proposal classification. They proposed a method called FM-FSOD, which is evaluated on multiple pre-trained vision models [80, 136, 143]. Their findings showed that DINOv2, pre-trained with both image-level and patch-level self-supervised objectives and equipped with a Transformer-based detection framework, achieved the best performance. For proposal generation, FM-FSOD utilizes the in-context learning capabilities of pre-trained Large Language Models (LLMs) for contextualized few-shot proposal classification in FSOD. The FM-FSOD can automatically exploit various contextual information between proposals and classes through the LLMs, including proposal-proposal, proposal-class, and class-class relations. The extracted context information significantly enhances few-shot proposal classification from the same query image."}, {"title": "4.2.4 Attention Mechanisms and Feature Enhancement:", "content": "This section explores methods that utilize attention mechanisms and feature enhancement for FSOD tasks. Attention mechanisms are often used to direct models towards relevant spatial areas, prioritizing them during parameter updates. However, training an attention model that can generalize well can be difficult with limited training data, as it heavily depends on top-down supervision. To address this, Chen et al.  introduced the Attentive Few-Shot Detection Network (AttFDNet) with two key innovations: (i) Bottom-up Attention: Instead of just top-down attention [17], AttFDNet uses bottom-up attention to leverage visual saliency for identifying interesting objects, even from unseen categories, aiding accurate classification with limited data, and (ii) Enhanced Intra-Class Agreement: AttFDNet uses object and background concentration loss to improve learning from few samples. These losses help objects of the same class to cluster together and push background regions apart, addressing issues with \u201chard negative anchors", "162": "proposed FSCE (Few-Shot Object Detection via Contrastive Proposals Encoding) to enhance feature representations in FSOD by using contrastive loss. FSCE introduces a contrastive branch to the primary RoI head when transferring the base detector to few-shot novel data. This branch measures the similarity between object proposal encodings. The supervised contrastive objective, called Contrastive Proposal Encoding (CPE) loss, reduces the variance of embeddings from the same category and separates different"}]}