{"title": "Resource Governance in Networked Systems via Integrated Variational Autoencoders and Reinforcement Learning", "authors": ["Qiliang Chen", "Babak Heydari"], "abstract": "We introduce a framework that integrates variational autoencoders (VAE) with reinforcement learning (RL) to balance system performance and resource usage in multi-agent systems by dynamically adjusting network structures over time. A key innovation of this method is its capability to handle the vast action space of the network structure. This is achieved by combining Variational Auto-Encoder and Deep Reinforcement Learning to control the latent space encoded from the network structures. The proposed method, evaluated on the modified OpenAI particle environment under various scenarios, not only demonstrates superior performance compared to baselines but also reveals interesting strategies and insights through the learned behaviors.", "sections": [{"title": "1 Introduction", "content": "Modern complex systems increasingly rely on the coordinated actions of multiple autonomous agents, each equipped with the ability to sense, decide, and act independently. These multi-agent systems have transformed numerous domains including robotics [1, 2, 3, 4], supply chain management [5, 6, 7], communication networks [8, 9, 10], transportation systems [11, 12, 13], and most recently, systems involving multiple Large Language Model agents [14, 15, 16]. A fundamental challenge in these systems lies in their decentralized nature - while each agent operates based on its individual objectives and local information, the overall system must achieve broader collective goals. System managers face the complex task of steering these autonomous agents toward system-level objectives through strategic allocation of limited resources, such as communication channels or energy.\nAt the core of these decentralized systems lies the crucial element of inter-agent communication and interaction, which distinguishes multi-agent systems from independent single-agent scenarios. These interactions can be modeled as networks, with resources represented by network links. Research has shown that different network structures significantly impact system-level metrics including performance, resource utilization, and cooperation levels (See the background section). This raises a"}, {"title": "2 Background", "content": "In this section, we review literature relevant to our research across three key areas. First, we explore network structure governance in various domains, which aligns with our paper's primary focus. Second, we examine generative models for network-based tasks, which relates to our approach of transforming discrete action spaces into continuous latent spaces. Finally, we review our previous Two-Tier framework, which serves as the foundation for evaluating our proposed VAE-RL method."}, {"title": "2.1 Network Structure Governance", "content": "Network structure governance - the ability to modify network configurations to influence agent behavior and system performance - has emerged as a critical research area across multiple domains. We examine its applications in three key areas: communication systems, public health policy, and socio-technical systems. In communication systems, researchers have developed various approaches to network governance. Nakamura et al. [30] address telecommunication service cost reduction while maintaining network stability under varying electricity prices. Bijami et al. [31] introduce a Distributed Networked Control Scheme (DNCS) for stabilizing large-scale interconnected systems, specifically addressing communication challenges like random delays and packet dropouts.\nDeep Reinforcement Learning (DRL) applications in this domain include Chu et al.'s [32] scalable, decentralized Multi-Agent Reinforcement Learning (MARL) algorithm for adaptive traffic signal control, and Shibata et al.'s [33] exploration of MARL for multi-agent cooperative transport. Network structures have also been studied in social and economic contexts, examining communication benefits, costs, and decentralized strategic decisions [34, 35]. These studies have identified structures that balance network stability and efficiency [36, 37]. Public health policy has increasingly embraced network governance approaches. Dynamic complex networks have proven effective in modeling micro-social interactions for public health [38], urban policies [39], and platform economies [40]. During the COVID-19 pandemic, Mazzitello et al. [41] developed network-based pool testing strategies for low-resource settings, while Robins et al. [42] analyzed network interventions like social distancing. Siciliano et al. [43] contributed a framework for network control in public administration to promote behavioral change. In socio-technical systems, network governance has facilitated various advances. Ion et al. [44] developed a scalable algorithm for self-organizing control systems using networked multi-agent systems. Ellis et al. [45] studied network impacts on self-management support interventions, while Joseph et al. [46] proposed a complex systems approach to model social media platform interventions. Network structure has proven crucial in promoting prosocial behaviors such as cooperation [47, 48, 49], coordination [50], and fairness [51], particularly in social dilemma situations.\nWhile network structure control has garnered significant attention, most existing approaches rely on traditional methods like hard-coded policies or heuristics, which struggle with complex, evolving, and partially observable environments. Deep Reinforcement Learning offers greater flexibility but often focuses on decentralized approaches, potentially compromising system-level optimization. Our approach addresses these limitations through a centralized method that tackles the curse of dimensionality."}, {"title": "2.2 Generative Models for Networks", "content": "Generative models have revolutionized various fields, including images [52, 53], voice [54, 55, 56], and text [57, 58]. In network modeling, Kipf and Welling [59] introduced the Variational Graph Auto-Encoder (VGAE), enabling unsupervised learning on graph-structured data. Li et al. [60] developed an approach using graph neural networks to express probabilistic dependencies among nodes and edges. Domain-specific applications of network generation include DeepLigBuilder by Li et al. [61] for drug design, Zhao et al.'s [62] physics-aware crystal structure generation, and Singh et al.'s [63] TripletFit technique for network classification. Comprehensive surveys [64, 65, 66] provide extensive coverage of network generation methods. Our work takes a novel approach by integrating generative models with Deep Reinforcement Learning to enable centralized network structure control while addressing the curse of dimensionality."}, {"title": "2.3 Overview of the Two-Tier RL Governance Framework", "content": "Our previous work [27] introduced a Two-Tier framework integrating Reinforcement Learning to optimize resource distribution in Systems of Systems (SoS). The framework operates across two training levels. In Tier I, individual SoS components undergo decentralized-training-decentralized-execution (DTDE) using the Deep Deterministic Policy Gradient (DDPG) algorithm [20]. This tier develops essential skills based on partial observations, after which policies become fixed. Tier II focuses on the centralized resource manager's allocation decisions, including resource type and timing, using Deep Q-learning to maximize operational effectiveness. While our previous work limited the system manager to choosing between empty or fully connected communication networks, our current research expands these capabilities. The manager can now assign any possible communication network configuration at each time step, enabling more adaptive and tailored strategies."}, {"title": "3 Methodology", "content": "In this section, we provide background knowledge and explain the method we are using. First, we briefly introduce the Partially Observable Markov Decision Process (POMDP), the fundamental model underlying Deep Reinforcement Learning (DRL). Next, we discuss the Deep Deterministic Policy Gradient (DDPG), the DRL algorithm we employ for managing continuous latent action spaces. We also explain the Branching Structured-Based DQN, an important baseline method in our study. Finally, we comprehensively introduce our methodology for training Variational Auto-Encoders (VAE) on networks and the integration of VAE with DDPG to optimize our system's performance."}, {"title": "3.1 Partial observable Markov decision process", "content": "The environmental dynamics in our study are aptly modeled using a Partial Observable Markov Decision Process (POMDP), as outlined in [67]. A POMDP is characteristically defined by a tuple < S, s\u00ba, A, O, T, R >: where S is a set of potential states; s\u00ba represents the initial state of the environment, with s\u00ba \u2208 S; A denotes the set of available actions; O comprises the observations derived from the actual state; R is the reward outputted by the environment; T is the transition function, defined as T: O \u00d7 A \u2192 O; and U is the reward function, where U: O \u00d7 A \u2192 R. The primary objective within a POMDP framework is to develop an optimal policy, \u03c0\u03c1: \u039f \u00d7 A \u2192 [0, 1].\nThis policy aims to maximize the expected return, calculated as $\\sum_{t=0} \\gamma^{t}r_t$, where \u03b3 represents the discount factor."}, {"title": "3.2 Deep deterministic policy gradient", "content": "Policy gradient methods, as described in [68], represent a potent subclass of reinforcement learning (RL) algorithms. The fundamental concept involves directly modifying the policy parameters, denoted as \u03b8, to optimize the objective function $J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}} [R]$. This optimization is achieved by progressing in the direction of the gradient $\\nabla_{\\theta}J(\\theta)$. The policy gradient can be expressed as follows: $\\nabla_{\\theta}J(\\theta) = \\mathbb{E}_{o,a}[\\nabla_{\\theta} \\log \\pi_{\\theta}(a|o)Q^{\\pi} (o, a)]$. Here, $Q^{\\pi} (o, a)$, representing the action-value function, can be updated using techniques outlined in the Q learning section. This approach evolves into the actor-critic algorithm. Extending this framework to deterministic policies, denoted as $\\mu_{\\theta} : O \\rightarrow A$, leads to the formulation of the Deep Deterministic Policy Gradient (DDPG) algorithm. In DDPG, the gradient of the objective undergoes a transformation: $\\nabla_{\\theta}J(\\theta) = \\mathbb{E}_{o,a}[\\nabla_{\\theta}\\mu_{\\theta}(a|o)\\nabla_{a}Q(o,a)|_{a = \\mu_{\\theta}(o)}]$. In this context, $\\mu_{\\theta}(a|s)$, which represents the deterministic policy, is modeled using a deep"}, {"title": "3.3 Branching Deep Q Network for Network-based system", "content": "The Branching Deep Q Network (BDQN) [69] presents a solution for implementing Deep Q learning in scenarios characterized by large discrete action spaces. When an action space encompasses several dimensions, denoted as N, with each dimension offering a multitude of options, symbolized as D, the complexity of this space escalates exponentially, represented as O(DN). This exponential growth not only results in impractically large model sizes but also significantly complicates the optimization process. To address this, BDQN leverages the Dueling Q Network architecture [70], which separately learns Q functions for each dimension. It then integrates these functions using a shared state value, facilitating coordination among them. Consequently, this approach effectively reduces the action space complexity to O(N * D), rendering the learning process feasible for tasks with extensive discrete action spaces.\nTo elucidate how BDQN serves as a baseline in our context, our action space of network topology can be converted into actions based on the number of link dimensions, where each dimension has two options \u2013 to have a link or not. Therefore, in our case, N corresponds to the number of links, and D is 2, representing the binary choice for each link (either present or absent). Utilizing the BDQN training scheme, we can effectively learn policies for network structure assignment."}, {"title": "3.4 Variational Auto-encoder", "content": "The Variational Auto-encoder (VAE) [28], a renowned generative model, has garnered considerable acclaim in fields like image and network generation. Its primary objective is to encode a dataset of independently and identically distributed samples into an embedded distribution that represents certain latent variables (encoder) and to subsequently generate new samples from this distribution (decoder). During the training phase, the encoder and decoder are trained jointly, facilitating a cohesive learning process. Upon establishing the embedded distribution, the decoder's parameters are fixed, enabling it to generate samples that mirror the distribution of the original dataset. This capability is the cornerstone of VAE's proficiency in producing images or networks akin to those in the dataset. In our specific context, the dataset comprises network topologies, represented by adjacency matrices. The training regimen adheres closely to the standard VAE protocol, which unfolds as follows:\nIn our dataset, let's denote the variables as x, with each data point following a distribution p(x). The latent variables are characterized by the distribution p(z). The encoder in a VAE is effectively represented by the conditional probability p(z|x), while the decoder is represented by p(x|z). Direct computation of p(z|x) poses practical challenges, primarily because the underlying distribution p(x) is intractable, as evidenced when expanding p(z|x) using Bayes' rule. To circumvent this, VAE introduces an approximate function q(z|x), assuming its tractability. In traditional VAE models, a standard multivariate Gaussian distribution is employed for this approximation. To refine our approximation of p(z|x), we focus on minimizing the Kullback\u2013Leibler divergence between q(z|x) and p(z). This minimization process involves a series of mathematical operations, ultimately leading us to maximize the following objective: $\\mathbb{E}_{q(z|x)} \\log p(x|z) \u2013 KL(q(z|x)||p(z))$.\nThe objective function in VAEs comprises two key components. The first is the log-likelihood of the reconstructed variables, which measures the accuracy of the reconstruction. The second component is a regularization term, which ensures that the distribution of the latent variable z aligns closely with"}, {"title": "3.5 DDPG with VAE for network topology governance", "content": "In controlling multi-agent systems at the system level, a centralized management approach is pivotal, primarily to harmonize high-level performance with the allocation of various types of resources. This approach's efficacy is substantiated in studies such as [27, 25]. Within these complex systems, the dynamics of information sharing and agent interaction are crucial. The topology of the network, in particular, plays a crucial role in influencing outcomes across different scenarios. Consequently, when controlling multi-agent systems, a frequent challenge encountered is the need for centralized control over the network topology. This task is complicated by the sheer complexity and diversity of potential network topologies, resulting in an action space that is vast and often impractical for traditional control algorithms with discrete action spaces, such as Deep Q-learning [71].\nPolicy gradient methods are notably proficient in handling complex and continuous action spaces [20]. In our cases, where the inherent action space comprises network topologies, it will be beneficial to encode this vast, discrete action space into a manageable, continuous latent action space. By applying policy gradient methods to this transformed, latent continuous space, we can adeptly navigate and optimize within it. Subsequently, the optimized latent space can be decoded back into reconstructed network topologies. This approach effectively circumvents the challenges associated with the original, large discrete action space, presenting a more efficient method for network topology construction and optimization.\nInspired by this idea, we introduce the VAE-RL framework as a solution. This approach begins by employing a Variational Auto-encoder (VAE) to transform the extensive, discrete action space associated with network topologies into a continuous action space. Following this transformation, policy gradient algorithms with continuous action spaces are utilized to learn effective control policies. A key advantage of this framework is its ability to leverage the generative capabilities of the VAE, allowing for the reversion of the controlled, continuous action space back to its original form \u2014 the network topology. This process effectively surmounts the challenge posed by the vast discrete action space, thereby enhancing the control mechanisms within multi-agent systems. The training process"}, {"title": "3.5.1 VAE training process", "content": "Our primary objective centers on managing network topologies within multi-agent systems. To achieve this, we concentrate on training a Variational Auto-encoder (VAE) specifically designed to encode adjacency matrices into a latent space, and then decode this latent space back into reconstructed adjacency matrices. For both the encoder and decoder components of the VAE, we utilize fully connected neural networks. However, in scenarios where the system manager possesses comprehensive control with higher authority, extending not only to network topology but also directly to agent properties, Graph Neural Networks (GNNs) [72] emerge as a viable alternative for representing both the encoder and decoder. The training framework for the VAE, as applied to network topology, is depicted in Figure 1.\nInitially, we assemble a dataset, denoted as D, comprising samples of potential network topologies given the number of nodes. It's crucial to acknowledge that encompassing every possible network topology becomes more intractable as the number of nodes increases. Subsequently, this dataset is partitioned into a training set and a validation set, adhering to conventional supervised learning methodologies. Network topologies are represented through a flattened adjacency matrix, symbolized as A.\nThe architecture of our model employs fully connected neural networks to instantiate the encoder and decoder, denoted by $f_{encoder}(x; \\theta_1)$ and $f_{decoder}(x; \\theta_2)$, respectively. During each training iteration, a mini-batch of data a is sampled from the training set. The encoder processes this data to yield the mean and covariance of a multi-variate Gaussian distribution: \u03bc, \u03a3 = $f_{encoder}(a; \\theta_1)$. Subsequently, the latent variable values z are sampled from this distribution, where z \u2208 Rd and d, the latent variable dimension, is a pre-defined hyperparameter.\nThe decoder then takes z as input and produces the reconstructed adjacency matrix a: \u00e2 = $f_{decoder}(z; \\theta_2)$. Loss is calculated using the standard VAE loss we mentioned earlier, and the gradient of the loss is backpropagated to update $\\theta_1$ and $\\theta_2$ in $f_{encoder}(x; \\theta_1)$ and $f_{decoder}(x; \\theta_2)$. Ultimately, the model is evaluated on the validation set, and the model with better performance through validation is saved."}, {"title": "3.5.2 DDPG training process with learned VAE", "content": "In the VAE training phase, we have two key outcomes: a learned encoder that effectively embeds input network topologies into a latent space, and a learned decoder capable of reconstructing network topologies from latent variables. During the DDPG training, the parameters of the pre-trained VAE are fixed, with an exclusive focus on utilizing the decoder. This process is illustrated in Figure 2.\nThe environment dynamics are defined as R, S' = Transition(S, Aadj), where R represents the reward for the current step, S is the current state, and S' is the subsequent state of the environment. The current action, Aadj, corresponds to the network topology. For modeling the actor \u03bc(\u03bf; \u03c6) and the critic Q(o, \u03b1; \u03b8), fully connected networks are employed. In each step of the DDPG training, the manager observes the current state of the environment, obtaining the observation of at time step t. The actor then outputs latent variable values: zt = \u03bc(ot;$). As part of exploration, noise is added to actions, diminishing over time, a strategy recommended by [20]. Subsequently, the decoder transforms these latent variables into a reconstructed adjacency matrix: aadj = $f_{decoder}(zt; \\theta_2)$. The environment takes this reconstructed matrix, updates its state, and generates corresponding"}, {"title": "4 Experiment results and discussion", "content": "In this section, we first introduce the environment we are using and the related settings. Next, we compare the performance of the proposed VAE-RL framework to baseline methods in different scenarios. We then analyze the learned behavior of the system manager using VAE-RL and discuss the trends and insights derived from the results. Finally, we present snapshots of the network evolution over time, visualizing the system's dynamics and justifying our findings."}, {"title": "4.1 Experiment design", "content": "We utilize the modified OpenAI Gym particle environment [29] to evaluate the effectiveness of our VAE-RL framework. In the original 'spread' task within this particle environment, multiple agents are tasked with spreading themselves on landmarks while minimizing collisions. This scenario constitutes a multi-agent system where effective coordination among agents is essential. In our previous work [27], we introduced modifications to this environment, making it partially observable. Furthermore, we introduced different types of resources, such as additional vision and communication capabilities. These resources are dynamically selected by the SoS manager to maintain a balance between system performance and resource utilization for the SoS workers. However, the communication network options were limited (either empty or complete) in the previous study. Additionally, the SoS agents were homogeneous, and the proposed framework fell short in handling heterogeneous situations.\nIn this paper, we focus solely on the allocation of communication resources and adding flexibility to the action space. The SoS manager can select different communication network topologies during tasks, providing opportunities to save communication resources and improve system performance in uncertain environments with heterogeneous agents. Therefore, we retain the essential components in"}, {"title": "4.2 Performance of proposed method", "content": "To assess the generalization and robustness capabilities of our proposed VAE-RL method, we aim to compare its performance against the traditional DRL approach using a discrete action space for network topology, hereafter referred to as Flat-RL, within a 10-agent system. Given that the action space in this system is O(2N*(N-1)), where N = 10, its enormity renders the training for Flat-RL impractical. Therefore, we introduce another baseline, BDQN [69], which is scalable in larger networks. Initially, we apply both VAE-RL and the two baseline methods (Flat-RL and BDQN) to a smaller system comprising 4 agents. After establishing that BDQN demonstrates equivalent or superior performance compared to Flat-RL in this smaller setup, we then extend our evaluation to a larger 10-agent system. Here, BDQN's performance serves as a proxy for the upper bound of Flat-RL's capabilities. This approach allows us to indirectly gauge the performance of Flat-RL in larger systems, thereby ensuring the coherence of our results."}, {"title": "4.2.1 Results on two environments", "content": "We first applied all methods on the small environment with 4 agents and got the results in Figure 3. From the results, our VAE-RL approach consistently outperforms other baseline methods across all tasks including heterogeneous cases, ranging from difficult to easy. This not only underscores the superior performance of our method but also underscores its robustness under different scenarios. Because it is hard to compare the homogeneous scenarios and heterogeneous scenarios directly, for the purpose of analyzing trends, our subsequent findings and discussions in this section will primarily focus on homogeneous cases."}, {"title": "4.3 Evolution of Network Behavior", "content": "After confirming that our VAE-RL method outperforms the baseline in both small and large systems, we also aim to analyze the learned behaviors of SoS managers to enhance the explainability of our"}, {"title": "4.3.1 Scenarios with homogeneous agents", "content": "We have four environments for homogeneous cases, where agents have vision ranges of 0.6, 0.8, 1.0, and 1.2. We categorize the communication networks based on the number of links they contain. Sparse networks are defined as those with fewer than 9 links; mid-dense networks as those with more than 9 but fewer than 18 links; dense networks as those with more than 18 but fewer than 27 links; and very dense networks as those with more than 27 links. We then plot the distribution of networks with varying densities over time in Figure 5.\nFirstly, the graphs show that as tasks become less challenging, there is a notable increase in the frequency of using less dense communication networks, while the frequency of employing costly denser communication networks decreases. In the easiest task, with a vision range of 1.2, the use of sparse communication networks approaches approximately 100% at the end of the game. This observation aligns with our earlier explanation that easier tasks correspond to reduced utilization of communication resources.\nSecondly, the behavior of the SoS manager exhibits two distinct phases: during the initial 10 time steps, the manager leans towards employing more costly communication networks with a higher number of links, then it shifts towards utilizing cheaper communication networks. At the beginning of each task, due to the constraints posed by limited vision ranges, some SoS workers may not have the capacity to observe any entities. Consequently, the manager's preference is to encourage all workers to aggregate their information through dense communication networks. However, denser communication networks entail higher communication resource costs. Therefore, as workers approach landmarks and can perceive landmarks within their own limited vision ranges, the manager is inclined to assign sparser communication networks to save communication resources. For instance, in the initial phase of sub-graph (a), where the vision range is 0.6, resulting in exceptionally challenging tasks, the manager exhibits a frequency of assigning very dense communication networks of almost 60%. Subsequently, there is an immediate decrease in the frequency of employing costly communication networks, while the frequency of employing sparse communication networks exceeds 50% shortly after a few time steps."}, {"title": "4.3.2 Extending the model to heterogeneous agents", "content": "In numerous real-world multi-agent systems, agents often exhibit heterogeneity in their properties or capabilities. Regrettably, many studies in the domain of multi-agent system control or SoS control [27] tend to exclusively focus on homogeneous scenarios, rendering them less adaptable for deployment in heterogeneous environments. To address this limitation, we have designed an experiment with 10 agents where agents possess heterogeneous vision ranges: (2, 1, 1, 1, 0.5, 0.5, 0.5, 0, 0, 0). This experiment serves as a crucial and representative example within the realm of heterogeneous scenarios. Instead of analyzing the communication networks in general, we focus more on the agents' properties within networks. Thus, we examine the average node degrees of the agents and the average node betweenness centrality for those with the same vision range and plot these through time. The calculation of node betweenness centrality includes both the start nodes and end nodes. Our analysis of the learned behaviors of the resource manager and the corresponding results are depicted in Figure 6.\nUpon analysis, we can discern two distinct phases in the behavior patterns of the SoS manager, paralleling our previous findings. During the first phase (0-10 time steps), the node degrees of agents are relatively high, indicating a greater level of connectivity among agents. This is quickly followed by a shift to a state with fewer degrees. In the second phase (10-50 time steps), a notable stabilization occurs, with the agents maintaining generally low node degrees. The rationale behind this pattern mirrors our earlier analysis: At the beginning, agents require a higher degree of information exchange for effective coordination and task completion, leading to an increased number of connections. However, maintaining a high node degree comes with substantial costs. Consequently, once the agents have acquired essential information and start converging toward their objectives, there's a significant reduction in node degrees to minimize costs while maintaining efficiency.\nFor node betweenness centrality, we observe an increase during the 0-10 time steps, a decrease between 10-20 time steps, and stabilization afterward. Since betweenness centrality measures the importance and effectiveness of each node in the information flow, we infer that initially, agents cannot effectively share their information due to their random positions and connections within the network. With guidance from the VAE-RL manager, agents are strategically positioned within more reasonable network structures, enhancing their importance and effectiveness in the network. Finally, as agents approach their targets, the need for a complex communication network diminishes, reducing the overall importance of all agents and resulting in stabilization.\nFurthermore, we sought to identify patterns among agents with differing vision ranges. It was observed that agents with a vision range of 2 consistently exhibited the highest node degree and node betweenness centrality over time. This suggests that agents with a vision range of 2 tend to remain central within the communication network, maintaining more connections and a higher level of importance. Contrary to expectations, a clear trend where agents with greater vision ranges demonstrate larger node degrees and node betweenness centrality was not evident. Specifically, agents with a vision range of 1 generally had lower node degrees and node betweenness centrality compared to agents with a vision range of 0.5.\nWe propose several explanations for our findings. First, agents with moderate abilities serve a dual role: they require assistance from others to solve tasks in certain scenarios, yet they are also capable of providing valuable aid in different contexts. This bidirectional flow of information in a complex environment may contribute to the observed phenomenon. Second, while agents with moderate abilities lack the power to significantly assist others, they are able to solve tasks independently. This suggests that in some cases, they should be isolated from the communication network, resulting in"}, {"title": "4.3.3 Evolution of network structure", "content": "This visualization aims to elucidate the interactions between agents and the flow of information within communication networks, which is shown in Figure 8. For this demonstration, we standardize the positions of landmarks while initializing the agents' positions randomly, each with vision ranges of 0.6, 0.8, 1.0, and 1.2. We capture snapshots of the environment at various time steps-0, 5, 10, 15, 20-to observe the progression of the environment and the behaviors of the agents. It should be noted that the training and testing procedures described in our paper incorporate significant randomness to ensure the robustness of our VAE-RL framework. The statistical analysis provided in the previous"}, {"title": "5 Conclusion", "content": "We introduce VAE-RL, a novel approach for optimizing multi-agent communication networks. Traditional Deep Reinforcement Learning (DRL) struggles with the exponentially large action space of network topologies. VAE-RL addresses this by using a Variational Autoencoder (VAE) to transform the discrete action space into a manageable continuous latent space. DRL algorithms for continuous action spaces (DDPG) then learn policies in this latent space, which are decoded back into network topologies. Tested on a modified OpenAI particle environment [27], VAE-RL outperforms baseline methods like Flat-RL and BDQN in both small (4 agents) and large (10 agents) systems, across homogeneous and heterogeneous scenarios. Our analysis reveals insightful trends in VAE-RL's behavior.\nWhile our environment is primarily modeled after the communication networks of multi-robotic systems in the real world, it can also find parallels in other real-world situations. For instance, consider a corporate management system where a department comprises multiple employees, including at least one manager. These employees collaborate to address their respective tasks. However, variations in talent and work experience among the employees lead to differences in task completion performance. In such scenarios, the manager is tasked with making decisions on how to establish connections among employees to facilitate collective task completion, especially for those employees with comparatively weaker abilities. Optimizing departmental performance by strategically assigning connection networks among employees is a critical responsibility of department managers. This scenario closely mirrors the communication network assignment strategy learned by VAE-RL. Notably, our method operates autonomously and is adaptable to systems featuring heterogeneous workers, making it versatile and applicable across various contexts.\nThe VAE-RL framework holds significant potential for application in numerous real-world multi-agent systems, particularly those reliant on network-based information sharing and interaction processes. However, it has its limitations. A fundamental assumption underpinning VAE-RL is the strong authority of the manager, responsible for assigning network structures to multi-agent systems. This framework presupposes that agents will comply with the manager's commands, a scenario that may not always hold true in real-life situations. Multi-agent systems can be broadly classified into three categories: human-only systems, human-AI systems, and AI-only systems. In the last two categories, the assumption of compliance is more likely to be valid, given the predominance of AI agents programmed to adhere to managerial decisions, except in cases of technical failures. However, in systems with a higher proportion of human agents, this assumption may not always be correct. Even in scenarios where the manager represents high-authority entities such as governments, military, or corporations, human agents may sometimes exhibit rebellious behavior, as evidenced by instances of non-compliance with social distancing and masking policies during the Covid-19 pandemic [73, 74]. Therefore, while our framework is highly effective in AI-dominated systems, its applicability may be"}]}