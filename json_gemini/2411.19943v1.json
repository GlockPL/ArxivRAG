{"title": "Critical Tokens Matter:\nToken-Level Contrastive Estimation Enhence LLM's Reasoning Capability", "authors": ["Zicheng Lin", "Tian Liang", "Jiahao Xu", "Xing Wang", "Ruilin Luo", "Chufan Shi", "Siheng Li", "Yujiu Yang", "Zhaopeng Tu"], "abstract": "Large Language Models (LLMs) have exhibited\nremarkable performance on reasoning tasks. They\nutilize autoregressive token generation to con-\nstruct reasoning trajectories, enabling the devel-\nopment of a coherent chain of thought. In this\nwork, we explore the impact of individual tokens\non the final outcomes of reasoning tasks. We iden-\ntify the existence of \"critical tokens\" that lead to\nincorrect reasoning trajectories in LLMs. Specifi-\ncally, we find that LLMs tend to produce positive\noutcomes when forced to decode other tokens\ninstead of critical tokens. Motivated by this ob-\nservation, we propose a novel approach - cDPO\ndesigned to automatically recognize and con-\nduct token-level rewards for the critical tokens\nduring the alignment process. Specifically, we\ndevelop a contrastive estimation approach to auto-\nmatically identify critical tokens. It is achieved by\ncomparing the generation likelihood of positive\nand negative models. To achieve this, we sepa-\nrately fine-tune the positive and negative models\non various reasoning trajectories, consequently,\nthey are capable of identifying identify critical\ntokens within incorrect trajectories that contribute\nto erroneous outcomes. Moreover, to further align\nthe model with the critical token information dur-\ning the alignment process, we extend the conven-\ntional DPO algorithms to token-level DPO and uti-\nlize the differential likelihood from the aforemen-\ntioned positive and negative model as important\nweight for token-level DPO learning. Experimen-\ntal results on GSM8K and MATH500 benchmarks\nwith two-widely used models Llama-3 (8B and\n70B) and deepseek-math (7B) demonstrate the\neffectiveness of the propsoed approach cDPO.", "sections": [{"title": "1. Introduction", "content": "Aligning large language models (LLMs) with human prefer-\nences represents a crucial challenge in the current research.\nIt is the process of fine-tuning pre-trained LLMs on various\ninstruction data, and thereby, the model would be consis-\ntent with human values, preferences, and instructions. This\nalignment paradigm has made significant progress and is\nwidely applied in all downstream applications of LLMs.\nAmong all the contributions made to alignment algorithms\n(Christiano et al., 2017; Schulman et al., 2017; Ziegler et al.,\n2019; Ouyang et al., 2022; Bai et al., 2022), Direct Prefer-\nence Optimization (DPO) (Rafailov et al., 2024) is one of\nthe most representative algorithms. It utilizes the LLM itself\nas a secret reward model and conducts preference optimiza-\ntion on a preference pair of positive and negative examples.\nSince then, various contributions have been made to further\nadvance the DPO development of LLM alignment (Pal et al.,\n2024; Amini et al., 2024; Azar et al., 2024; Lai et al., 2024;\nPang et al., 2024; Tang et al., 2024; Zeng et al., 2024)."}, {"title": "2. Estimating Critical Tokens", "content": "In this section, we mainly demonstrate the identification\nof critical tokens of incorrect reasoning trajectory. We fur-\nther demonstrate a novel proposed contrastive estimation\nstrategy to automatically identify those tokens.\nCritical Tokens Affect Reasoning Mathematical reason-\ning tasks emphasize logical and sequential deduction to\narrive at solutions. However, within incorrect reasoning\ntrajectories, certain tokens play a pivotal role in driving\nthe trajectory toward an incorrect outcome. These tokens\ndisrupt the logical flow, misrepresent relationships, or intro-\nduce computational errors that significantly impact the final\nresult. Unlike other tokens that may have a negligible effect\non the reasoning process, these \u201ccritical tokens\" serve as\npivotal points of failure. Identifying these tokens is essen-\ntial, as avoiding or correcting them can often lead to correct\noutcomes, even within an incorrect trajectory.\nAs shown in Fig. 2, the token \u201cowed\u201d is responsible for\nmost of the incorrect reasoning trajectories, as it misleads\nthe logical deduction process. Conversely, forcing the model\nto decode alternative tokens, such as \u201cpaid\" significantly\nimproves the likelihood of producing a correct final result.\nTo further validate the existence of critical tokens and ana-\nlyze their influence on reasoning trajectories, we conducted\nan experiment using LLama-3-8B on 100 incorrect reason-\ning paths sampled from the GSM8K dataset. Specifically,\nwe performed the following experiment:\n\u2022 Token Score Calculation via Resampling: For each\ntoken in an incorrect trajectory, we resampled 64 times\nwhile keeping the token unchanged. Based on the cor-\nrectness of the generated completions, we calculated\na score to quantify the influence of each token on the\ntrajectory. The first token with a score of 0, indicat-\ning that it consistently led to incorrect outcomes, was\ndesignated as the critical token.\n\u2022 Forced Decoding with Alternative Tokens: After iden-\ntifying the critical token in each trajectory, we replaced\nit by forcing the model to decode an alternative token.\nThis was achieved by resampling the critical token 64\ntimes using the model's probability distribution. We\nthen evaluated the correctness of these modified con-\ntinuations and calculated the Pass@k metric.\nAs shown in Fig. 1, the forced decoding strategy signifi-\ncantly improves the likelihood of a correct solution. Specif-\ncally, the Pass@k curve demonstrates a rapid increase in\naccuracy, with Pass@1 at 0.31 and Pass@64 reaching 0.90.\nThis result highlights the pivotal role that critical tokens play\nin determining the outcome of reasoning trajectories. Sim-\nply avoiding these tokens can dramatically enhance perfor-\nmance, even when the previous context remains unchanged.\nContrastively Estimating Critical Tokens Experimental\nresults demonstrate that critical tokens are pivotal in deter-\nmining incorrect final outcomes. While methods such as\nresampling or Monte Carlo Tree Search (Wang et al., 2024;\nLuo et al., 2024) can identify critical tokens within incorrect\ntrajectories, they incur prohibitively high sampling costs\nand face significant scalability challenges. Furthermore,\nthe absence of human-annotated, token-level datasets lim-\nits the feasibility of training a dedicated detection model.\nExisting methods (Guo et al., 2023; Yoon et al., 2024) rely\non external models for token-level annotations, which can\nprovide effective supervision signals but are both costly and\nconstrained by the capabilities of the external models.\nTo achieve automatic detection of critical tokens, we pro-\npose a method called contrastive estimation. This approach"}, {"title": "3. CDPO", "content": "The previous section has demonstrated the existence of crit-\nical tokens within an incorrect reasoning trajectory, and one\ncould identify those tokens through contrastive estimation.\nFollowing this, a natural idea is to align such information\nduring the preference optimization. Consequently, in this\nsection, we first train the positive and negative models sep-\narately. We further extend the DPO from example-level\nto token-level and utilize such contrastive signals as token-\nlevel rewards for preference optimization.\nModel Training for Contrastive Estimation Building\non the aforementioned strategy of contrastive estimation,\nthe primary objective is to develop models that can effec-\ntively estimate a wide range of both correct and incorrect\nreasoning distribution. As shown in step 1 of Fig. 4, to\ntrain those models, we collect a wide range of reasoning\ntrajectories based on the sampling strategy: Given a dataset\n$D = \\{(x_i, Y_i)\\}_{i=1}^M$, we utilize a pre-trained LLM to firstly\ndecode reasoning trajectories with N times sampling. Then,\nwe verify the outcome results based on the groud-truth la-\nbels yi, which yields $k_i$ positive reasoning trajectories and\n$N - k_i$ negative reasoning trajectories, which is denoted as:\n$D^p = \\{(x_i, \\{Y_i^j\\}_{j=1}^{k_i})\\}_{i=1}^M$\n$D^n = \\{(x_i, \\{Y_i^j\\}_{j=k_i+1}^{N})\\}_{i=1}^M$\nFor positive model, we randomly select one correct trajec-\ntory since we expect the model to be decisive with its own\ncorrect reasoning paths; while for negative model, we select\ntop-p percent of incorrect trajectories, since we expect the\nmodel would identify various types of error patterns. Finally,\nwe train our model on those two datasets separately.\nPreparing Token-level Preference Dataset To identify\nand leverage critical tokens, we focus on computing scores"}, {"title": "4. Experiments", "content": "4.1. Setup\nModel We conducted experiments on a range of mod-\nels, including the general-purpose models Llama-3-8B-base\nand Llama-3-70B-base (Dubey et al., 2024), as well as\nthe domain-specific model DeepSeek-math-7B-base (Shao\net al., 2024).\nDataset We use two widely adopted math reasoning\ndatasets for training and evaluation: GSM8K (Cobbe et al.,\n2021) and MATH (Hendrycks et al., 2021). For training,\nwe sample from all questions in the training set to generate\nthe data. For evaluation, we utilize the MATH500 subset,\nwhich is uniformly sampled and has a distribution of diffi-\nculty levels and subjects that matches the full MATH test\nset, as demonstrated in Lightman et al. (2023). Addition-\nally, for both training sampling and evaluation, we apply the\nfew-shot prompt approach from Fu et al. (2023).\nBaseline Methods For comparison, we evaluated multiple\nbaseline methods using the data generated from the process\ndescribed in Section 3. For Supervised Fine-Tuning (SFT),\nwe fine-tuned the model using the positive response set DP.\nFor preference optimization (PO) methods, we utilized the\ntoken-level annotated pair-wise preference dataset $D^{ce}$. The\nbaselines we compared include:"}, {"title": "4.3. Analysis", "content": "Contrastive Estimation Can Identify Critical Tokens\nTo evaluate the effectiveness of contrastive estimation\nin identifying critical tokens, we conducted experiments\non GSM8K using aforementioned token-level preference\ndataset $D^{ce}$. For each model, we selected 100 incorrect\nreasoning paths and assessed these paths using Contrastive\nEstimation. The tokens with the lowest scores in these paths\nwere designated as critical tokens. For each critical token,\nwe performed two types of sampling:\n\u2022 With Critical Token: The model continued generating\nsequences critical token while keeping it unchanged.\nWe sampled 64 completions.\n\u2022 Without Critical Token: The critical token was replaced\nby forcing the model to decode an alternative token,\nand 64 completions were sampled under this condition."}, {"title": "5. Related Work", "content": "5.1. Contrastive Estimation\nContrastive estimation is a technique used primarily in sta-\ntistical modeling and machine learning to estimate model\nparameters by contrasting observed data with artificially con-\nstructed \"noise\" data. The core idea is to improve parameter\nestimation by comparing the likelihood of the observed data\nagainst the likelihood of other, less plausible data. A line\nof work improves the performance of contrastive estimation\n(Gutmann & Hyv\u00e4rinen, 2010; Bose et al., 2018; He et al.,\n2020; Denize et al., 2023)."}, {"title": "Contrastive Decoding", "content": "More specifically, our approach\nis quite related to contrastive decoding (CD), which is one\nof the downstream applications of contrastive estimation.\nContrastive Decoding (Li et al., 2023) implements a strategy\nthat contrasts the token distribution likelihoods between\nexpert and amateur models during decoding. As outlined in\nO'Brien & Lewis (2023), This approach avoids the selection\nof high-probability but low-quality tokens, thereby ensuring\nthe fluency and coherence of the generated text.\nSubsequent studies further highlight CD's potential in im-\nproving factuality (Zhang et al., 2023; Yang et al., 2024),\nknowledge editing (Bi et al., 2024), safety (Zhao et al.,\n2024), and reasoning (O'Brien & Lewis, 2023; Shi et al.,\n2024). Specifically, Zhang et al. (2023) developed the\nInduce-then-Contrast Decoding (ICD) strategy, which im-\nproves factuality by contrasting original LLM predictions\nwith those from hallucinatory LLMs. Similarly, Yang et al.\n(2024) introduced hallucinatory/truthful comparators dur-\ning decoding to mitigate hallucinations. Zhao et al. (2024)\nproposed Decoding by Contrasting Knowledge (DeCK) to\nediting of stubborn knowledge. Zhao et al. (2024) proposes\nAdversarial Contrastive Decoding (ACD), a prompt-based\napproach that enhances safety performance without training;\nAdditionally, O'Brien & Lewis (2023) demonstrated that\nCD improves performance on reasoning tasks, effectively\npreventing common reasoning errors such as missing steps\nor semantic misunderstandings (Wang et al., 2023). Fur-\nthermore, Shi et al. (2024) showed that unchosen experts\nin a Mixture-of-Experts (MoE) model can be used for CD,\nthereby improving the reasoning capabilities of models.\nDifferent from those works that focus on the inference of\ncontrastive decoding, in this paper, we mainly utilize con-\ntrastive decoding ideas to identify the \"critical tokens\" that\nsignificantly affect the correctness of reasoning process."}, {"title": "5.2. Reinforcement Learning Algorithm", "content": "Reinforcement Learning from Human Feedback (RLHF)\nhas demonstrated significant improvements in alignment of\nLLMs. In the process of RLHF, the reward model learns\nhuman preferences and subsequently utilized to guide pol-\nicy optimization. Proximal Policy Optimization (PPO) is\ncommonly used in this process and a set of improvement\nfurther optimize training efficiency and robustness (Zheng\net al., 2023; Casper et al., 2023; Gao et al., 2023; Li et al.,\n2024; Santacroce et al., 2023; Rame et al., 2024).\nNotably, Rafailov et al. (2024) introduces Direct Preference\nOptimization (DPO), which increases the relative probabil-\nity of preferred responses over dispreferred ones, thereby\navoiding the need to explicitly learn a reward function. Sub-\nsequently, several works have been devoted to further re-\nfining DPO from various perspectives. Azar et al. (2024)\ndeveloped Identity Preference Optimization (IPO) to ad-"}, {"title": "6. Conclusion", "content": "We identify the critical token that plays a vital role within\nLLMs' reasoning trajectory. Specifically, we find that, for\nan incorrect reasoning trajectory, when critical tokens are\ndecoded, LLMs tends to produce negative outcomes, while\nif LLMs are forced to decode other tokens instead of critical\ntokens within that trajectory, they tend to produce positive\nresults. Moreover, other tokens do not exhibit this decisive\nproperty. Consequently, we propose a contrastive estimation\nstrategy to identify the critical tokens of the negative trajec-\ntories and design a token-level contrastive DPO algorithm\nfor learning the critical token information. Experimental\nresults demonstrate that our proposed method significantly\noutperforms the baseline strategies such as DPO and RPO,\nand yields 77.2% and 33.4% performance on GSM8k and\nMATH500 - two widely used benchmarks respectively."}, {"title": "Impact Statement", "content": "In our research, we focus exclusively on developing models\nfor solving mathematical problems, which inherently mini-\nmizes common ethical concerns typically associated with\nAI applications in broader domains. The primary function\nof our models is to enhance computational accuracy and\nefficiency in mathematical tasks, ensuring that any potential\nbias, privacy issues, or harmful outputs are effectively non-\nexistent. Since our dataset consists solely of mathematical\nquestions, it does not involve personal, sensitive, or contro-\nversial information that could lead to ethical dilemmas."}]}