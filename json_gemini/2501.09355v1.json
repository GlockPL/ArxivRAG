{"title": "YETI (YET to Intervene) Proactive Interventions by Multimodal AI Agents in Augmented Reality Tasks", "authors": ["Saptarashmi Bandyopadhyay", "Vikas Bahirwani", "Lavisha Aggarwal", "Bhanu Guda", "Lin Li", "Andrea Colaco"], "abstract": "Multimodal AI Agents are Al models that have the capability of interactively and cooperatively assisting human users to solve day-to-day tasks. Augmented Reality (AR) head worn devices can uniquely improve the user experience of solving procedural day-to-day tasks by providing egocentric multimodal (audio and video) observational capabilities to AI Agents. Such AR capabilities can help the Al Agents see and listen to actions that users take which can relate to multimodal capabilities of human users. Existing Al Agents, either Large Language Models (LLMs) or Multimodal Vision-Language Models (VLMs) are reactive in nature, which means that models cannot take an action without reading or listening to the human user's prompts. Proactivity of Al Agents on the other hand can help the human user detect and correct any mistakes in agent observed tasks, encourage users when they do tasks correctly or simply engage in conversation with the user - akin to a human teaching or assisting a user. Our proposed YET to Intervene (YETI) multimodal agent focuses on the research question of identifying circumstances that may require the agent to intervene proactively. This allows the agent to understand when it can intervene in a conversation with human users that can help the user correct mistakes on tasks, like cooking, using Augmented Reality. Our YETI Agent learns scene understanding signals based on interpretable notions of Structural Similarity (SSIM) on consecutive video frames. We also define the alignment signal which the AI Agent can learn to identify if the video frames corresponding to the user's actions on the task are consistent with expected actions. These signals are used by our AI Agent to determine when it should proactively intervene. We compare our results on the instances of proactive intervention in the HoloAssist multimodal benchmark for an expert agent guiding a user to complete procedural tasks.", "sections": [{"title": "1. Introduction", "content": "Recent advances in artificial intelligence have led to the widespread adoption of AI assistants across various platforms and modalities. While these systems, such as Siri for voice interaction and Gemini [18] for text-based communication, have demonstrated significant utility in task automation, they remain constrained by their single-modality architectures. This limitation presents a critical gap in human-AI interaction, particularly in scenarios requiring real-time, context-aware assistance.\nMultimodal Vision-Language Models (VLMs) have emerged as a promising solution to bridge this modality gap, offering multimodal understanding that more closely aligns with human perception. However, current VLM-based assistive systems predominantly operate in a reactive paradigm, responding only to explicit user queries. This re-"}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. Egocentric Interaction Datasets", "content": "Recent advances in egocentric vision have produced several datasets that capture human interactions and activities. HoloAssist [21] presents a large-scale egocentric dataset focusing on collaborative physical manipulation tasks between two people, providing detailed action and conversational annotations. This dataset offers valuable insights into how human assistants proactively and reactively intervene, correct mistakes, and ground their instructions in the environment.\nParse-Ego4D [1] introduces a benchmark for evaluating AI agents' capability to make unsolicited action suggestions based on user intent signals. We argue that as this benchmark evaluates the AI agents' response to user queries, it does not truly measure proactive behavior.\nWhile Ego-Exo4D [11] provides a comprehensive multimodal, multiview dataset capturing both egocentric and exocentric perspectives in expert-learner scenarios, it primarily focuses on skilled single-person activities without addressing proactive communication. Similarly, existing datasets like Ego4D [10] and EPIC-Kitchens [7], while rich in activity and object annotations, lack direct mappings to actionable recommendations."}, {"title": "2.2. Proactive AI Agents and Communication", "content": "Proactive communication in AI agents encompasses several key aspects [8]: Intelligence (the ability to anticipate task developments), Adaptivity (dynamic adjustment of timing and interventions), and Civility (respect for user boundaries and ethical standards). Emerging research has demonstrated the value of proactive AI agents across various domains, including personal assistance, predictive maintenance, health-care monitoring, and voice assistance [4, 5, 12]."}, {"title": "2.3. Language Models for Proactive Assistance", "content": "Recent developments have shown promising results in using Multimodal Vision Language Models (VLMs) and LLMs for proactive assistance. ProAgent [23] introduces a framework that leverages LLMs to create agents capable of dynamically adapting their behavior and inferring teammate intentions. The effectiveness of these models has been further demonstrated through fine-tuning on ProactiveBench [13], which significantly enhances the proactive"}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Proactive Augmented Reality Interaction Data of Cooperative Agents", "content": "The HoloAssist dataset [21] provides multimodal egocentric vision-language benchmarks focusing on Augmented Reality (AR)-based human-AI collaboration. AR devices are used to capture the Expert-User collaborative dynamics, recording the visual observations of an User Agent (human) collaborating with an Expert Agent (Instructor), which can be an AI Agent, on physical reasoning tasks, while documenting the dialogue between the two agents. The dataset comprises 482 unique Expert-User interaction sequences with videos and dialogues of the"}, {"title": "3.2. Multimodal VLM Generation", "content": "We count objects in video sequences by leveraging the recent advancements in Multimodal Visual Language Models (VLMs). Our method processes videos by extracting frames at a rate of 1 frame per second (FPS), enabling efficient temporal analysis while maintaining sufficient granularity for accurate object counting.\nGiven an input video V of duration T seconds, we extract a sequence of frames {f1, f2, ..., fT} at 1 FPS. This sampling rate balances computational efficiency with temporal resolution, ensuring that significant object state changes are captured while minimizing redundant processing.\nWe utilize PaliGemma-3b-mix-448, from the PaliGemma [3] family of lightweight multimodal VLMs created by Google, to process each frame independently. PaliGemma's ability to quickly leverage its visual and textual understanding capabilities makes it suitable for an AR / VR setting where there may not be much device compute available and a fast response is needed. For each frame ft, we construct a prompt:\n$P_{obj} = \\text{\"The number of objects in this image is \"}$ (1)\nThis prompt elicits a numerical response from the model, avoiding unwanted conversational output. We chose this prompt after thorough experimentation with PaliGemma's object detection capabilities. Some of the other prompts we explored as well as their output can be found in the supplementary material. The model processes each frame ft with prompt Pobj to generate a count estimate:\n$C_t = PaliGemma(f_t, P_{obj})$ (2)\nwhere Ct represents the predicted object count at time t. The change in object count between frames is heavily skewed towards zeroImplementation Details The PaliGemma model was used with its default configuration, maintaining the 448 \u00d7 448 pixel input resolution. The model's responses were post processed to extract numerical values."}, {"title": "3.3. Alignment with Changing Object Count", "content": "The alignment signal taking the form of a change in object count for the scene observed by the AI Assistant is motivated by an intuition of how humans operate when listening to instructions. If a user agent is being guided on how to assemble a computer, they will not be moving objects around while they process the instructions. Rather, they will be listening so they know what to do next. Building upon this understanding, we can estimate when a proactive AI assistant should intervene by simply monitoring the change in object count from second to second."}, {"title": "3.4. Scene Understanding", "content": "The frame-wise counting results are aggregated to create a temporal signal {AC1, AC2, ..., ACT-1} where \u2206Ci := Ci+1 - Ci. This signal captures how the number of objects change throughout the video sequence."}, {"title": "3.5. Proactive Interactions and Interventions", "content": "In the context of AI Agents, reactivity refers to the AI Agent's response to a user cue. In contrast, proactivity involves behaviors initiated by the AI Agent without user prompts. Proactive activities can be broadly categorized into proactive interaction and proactive intervention. Understanding the subtle but important distinction between these two is essential for leveraging the capabilities of a proactive agent. An AI Agent is considered to be proactively interacting with the user if it initiates any engagement without user cues. Conversely, an AI Agent is proactively intervening when it takes concrete steps to alter the user's behavior. By this definition, all proactive interventions are forms of proactive interactions, but not all proactive interactions qualify as interventions."}, {"title": "3.6. YETI Proactive Agent Intervention Algorithm", "content": "The YETI algorithm incorporates several hyperparameters that determine when a Multimodal AI Agent should autonomously intervene proactively without any question or clarifications asked by the User Agent.\nThis parameter sets a filtering threshold. A frame's SSIM value with its corresponding frame must satisfy it to be considered in the YETI algorithm, to filter out highly similar frames where the user is not doing anything. In other words, if a frame and its proceeding frame have an SSIM of > \u03c4, the frame will not be considered for an autonomous intervention. \nThis parameter enforces a minimum temporal gap between consecutive interventions, ensuring that the AI agent does not intervene too frequently. It defines the duration that must pass after an intervention before another can be initiated.\nThis parameter identifies the sensitivity of the algorithm to changes in object counts within the frames. It defines the range within which a change in object count must fall or rise to be considered significant. \nThis parameter limits the maximum rate at which interventions can occur by defining the length of an \"episode.\" An episode is a consecutive sequence of frames within which only one intervention is permitted, thus preventing excessive and potentially disruptive interventions."}, {"title": "4. Experiments", "content": "To validate the results of our YETI algorithm, we conducted experiments with a wide variety of different settings. This also lets us see how each configuration of the algorithm and the value of each hyperparameter contributes to the evaluation metrics of our method compared to HoloAssist, the baseline for AI Agents proactively intervening with an user (student) task. An example of a proactive intervention being detected can be seen in Figure 5."}, {"title": "4.1. Experimental Settings", "content": "We carefully selected hyperparameters to balance the tradeoff between timely interventions and avoiding excessive interruptions. The key parameters are summarized in Table 3. A comprehensive analysis of hyperparameter sensitivity is provided in the supplementary material."}, {"title": "4.2. Metrics", "content": "We evaluate our YETI algorithm's effectiveness in detecting appropriate moments for proactive interaction and intervention using standard classification metrics:\nAccuracy = $\\frac{TP + TN}{TP + TN + FP + FN}$ (4)\nPrecision = $\\frac{TP}{TP + FP}$ (5)\nRecall = $\\frac{TP}{TP + FN}$ (6)\nF-measure = 2.$\\frac{Precision \\cdot Recall}{Precision + Recall}$ (7)\nwhere TP, TN, FP, and FN denote true positives, true negatives, false positives, and false negatives, respectively. These metrics provide a comprehensive assessment of YETI's intervention capabilities, measuring both its ability to intervene at appropriate moments and its capacity to avoid unnecessary interruptions.\nAs there is no empiric measurement of when the \"best\" precise moment to intervene is, we use a window of five seconds around HoloAssist labelled ground-truth proactive intervention starting time-stamps in order to assess whether a detected intervention frame is a true positive or a false positive. This is consistent with the method used to evaluate the HoloAssist baseline model [21], which also determines true positives and false positives by measuring temporal proximity to the nearest labelled intervention and seeing if it is within a window of tolerance."}, {"title": "4.3. Results", "content": "We can see in Tables 5 and 1 that YETI achieves impressive results when detecting proactive interventions and detections. Furthermore, as PaliGemma's scale is on the order of three billion parameters, it is suitable to be used on board"}, {"title": "5. Conclusion and Future Work", "content": "In this work, we tackle the significant gap in proactive AI assistance for everyday real-world tasks by introducing the Yet-to-Intervene (YETI) algorithm. Traditional AI agents, such as those exemplified by the HoloAssist baseline, predominantly operate reactively, responding only to explicit user prompts. This reactive nature limits their effectiveness in dynamic and context-sensitive environments where anticipatory intervention can substantially enhance user experience and task efficiency. Our YETI algorithm addresses this limitation by enabling AI agents to proactively identify and intervene in user actions without awaiting explicit cues.\nIntegrating YETI with lightweight Vision-Language Models (VLMs) like PaliGemma, our approach demonstrates remarkable improvements across key performance metrics, including Recall, Precision, Accuracy, and F-Measure. Notably, YETI achieves these enhancements while utilizing features that are up to 60,000 times more memory-efficient than those employed by state-of-the-art models such as HoloAssist. This drastic reduction in computational overhead not only makes YETI more accessible for deployment on resource-constrained devices but also paves the way for real-time, scalable AI assistance in diverse augmented reality (AR) applications.\nOur extensive ablation studies in the paper and supplemental reveal the critical features that significantly influence the accurate detection of intervention moments. By systematically varying and analyzing different feature sets, we identify the most impactful components that empower YETI to discern when proactive interactions or interventions are warranted. This insight underscores the importance of feature selection in the design of efficient and effective AI assistance algorithms.\nLooking ahead, there are several promising avenues for future research. First, we aim to enhance YETI's intervention capabilities by incorporating richer sensory data, including hand pose, eye gaze, head orientation, Inertial Measurement Unit (IMU) readings, and depth information. Integrating these modalities is expected to provide a more comprehensive understanding of the user's context and intentions, thereby enabling more nuanced and timely interventions. Second, we plan to evaluate YETI's performance across a broader spectrum of VLMs to assess its generalizability and identify optimal model architectures for proactive assistance. Additionally, fine-tuning VLMs on the HoloAssist dataset could further refine the agent's ability to anticipate user needs and improve intervention accuracy. Furthermore, future implementations of YETI will ex-"}]}