{"title": "LLM-Powered Decentralized Generative Agents with Adaptive Hierarchical Knowledge Graph for Cooperative Planning", "authors": ["Hanqing Yang", "Jingdi Chen", "Marie Siew", "Tania Lorido-Botran", "Carlee Joe-Wong"], "abstract": "Developing intelligent agents for long-term cooperation in dynamic open-world scenarios is a major challenge in multi-agent systems. Traditional Multi-agent Reinforcement Learning (MARL) frameworks like centralized training decentralized execution (CTDE) struggle with scalability and flexibility. They require centralized long-term planning, which is difficult without custom reward functions, and face challenges in processing multi-modal data. CTDE approaches also assume fixed cooperation strategies, making them impractical in dynamic environments where agents need to adapt and plan independently. To address decentralized multi-agent cooperation, we propose Decentralized Adaptive Knowledge Graph Memory and Structured Communication System (DAMCS) in a novel Multi-agent Crafter environment. Our generative agents, powered by Large Language Models (LLMs), are more scalable than traditional MARL agents by leveraging external knowledge and language for long-term planning and reasoning. Instead of fully sharing information from all past experiences, DAMCS introduces a multi-modal memory system organized as a hierarchical knowledge graph and a structured communication protocol to optimize agent cooperation. This allows agents to reason from past interactions and share relevant information efficiently. Experiments on novel multi-agent open-world tasks show that DAMCS outperforms both MARL and LLM baselines in task efficiency and collaboration. Compared to single-agent scenarios, the two-agent scenario achieves the same goal with 63% fewer steps, and the six-agent scenario with 74% fewer steps, highlighting the importance of adaptive memory and structured communication in achieving long-term goals. We publicly release our project at: https://happyeureka.github.io/damcs.", "sections": [{"title": "Introduction", "content": "In recent years, joint decision-making by multiple users has become a critical research area, with applications like robot soccer, autonomous vehicles (Cao et al. 2013), cyber security (Chen et al. 2023a,b), smart grid control (Pipattanasomporn, Feroze, and Rahman 2009), and search-and-rescue (Li, Martinoli, and Abu-Mostafa 2002) modeled as multi-agent decision-making problems (Chen, Wang, and Lan 2021; Chen et al. 2024). Automating decisions in such scenarios may significantly improve efficiency, cost, and safety, yet they often require agents to make joint long-term planning and reasoning decisions under uncertainty, often in large-scale systems with high volumes of dynamic information. Much recent work aims to use AI to tackle these challenges by formally modeling such decision-making problems as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (Bernstein et al. 2002). Centralized Training and Decentralized Execution (CTDE) is a popular solution framework in which agents centrally train a policy that learns how they should make decisions from historical observations. This centralized training allows agents to learn how to cooperate, but the policy can still be executed distributedly by each agent at test time (Lowe et al. 2017; Li et al. 2022a; Chen, Lan, and Joe-Wong 2024). In multi-agent reinforcement learning (MARL), CTDE still faces challenges like constraints on agent communication, difficulty in adapting to non-stationary environments, and scalability of the centralized training to a large number of agents (Huh and Mohapatra 2023).\nOpen-world games like Minecraft (Fan et al. 2022) and Crafter (Hafner 2021) feature large, expansive worlds where agents are free to roam and complete tasks without a pre-specified path, serving as benchmarks for AI agent capabilities. However, traditional MARL and CTDE approaches struggle in such environments due to the need for centralized, long-term reasoning, which is difficult without custom rewards. Additionally, processing multi-modal data (e.g., vision, text) in open-world games increases complexity and requires extensive training. As environments grow, centralized training for optimizing distributed tasks becomes inefficient, and CTDE methods face scalability issues, relying on fixed cooperation strategies that hinder adaptability in dynamic settings.\nMany challenges in MARL-based solutions stem from the vast amount of information agents must encapsulate in their learned policies, which are typically trained from scratch. Recently, large language models (LLMs) (Brown 2020) have emerged as a potential solution, achieving impressive performance in dialogue generation, complex reasoning (Liang et al. 2022; Achiam et al. 2023; Touvron et al. 2023; Warner et al. 2023), and embodied planning (Ahn et al. 2022; Du et al. 2023; Wang et al. 2023a). Their natural interpretabil-ity, due to working in natural language, along with strengths in planning (Ahn et al. 2022), reasoning (Shinn et al. 2024), and problem-solving (Bubeck et al. 2023), suggests they can incorporate prior knowledge and in-context reasoning into open-world problem-solving, potentially overcoming RL limitations. Yet, while LLMs excel in single-agent tasks via zero-shot and few-shot prompting (Huang et al. 2022; Song et al. 2023), scaling to decentralized multi-agent settings introduces new challenges, particularly in open-world games where cooperation and limited communication are critical. These include: a) fusing partially observable or duplicated information across agents; b) handling dynamic, multi-modal agent states and observations in evolving environments; c) enabling selective and strategic communication in costly multi-agent settings (Guo et al. 2024; Zhang et al. 2023); and d) fostering cooperation across dependent tasks, e.g., accomplishing multiple sub-goals to achieve a long-term and shared objective.\nIn this work, we propose Decentralized Adaptive Knowledge Graph Memory and Structured Communication Sys-tem (DAMCS) in a novel Multi-agent Crafter environment to address these fundamental challenges that enable agents to collaborate, and dynamically adapt in complex, open-ended environments like open-world survival games. DAMCS is a novel decentralized multi-agent frame-work that leverages LLMs, featuring two key compo-nents: an Adaptive Knowledge Graph Memory System (A-KGMS)and a Structured Communication System (S-CS) to enhance agent cooperation.\nAdaptive Knowledge Graph Memory System (A-KGMS) effectively fuses information across agents in dy-namic environments, thus addressing the first two challenges of multi-agent LLMs above. Unlike LLM-based agents like SPRING, which uses external knowledge for planning but rarely learns from in-environment interactions (Wu et al. 2024), A-KGMS enables agents to interact and learn from each other's experiences, through a hierarchical adaptive knowledge graph. Each agent maintains and dynamically updates its own graph-based in part on messages from other agents. A-KGMS integrates sensory, episodic, and procedu-ral memory, enabling agents to perceive cues and retrieve relevant experiences for contextual knowledge. Through fa-cilitating interaction between the joint long-term and short-term memory across agents, agents can efficiently learn, re-trieve, and generalize task-relevant knowledge by categoriz-ing and linking experiences. This system allows agents to cooperate across a variety of tasks, adapt to dynamic envi-"}, {"title": "Related Work", "content": "Multi-Agent Cooperation and Communication. Multi-agent reinforcement learning (MARL) has advanced decision-making in communication-reliant tasks (Cao et al. 2013), with approaches including continuous (Foerster et al. 2016; Sukhbaatar, Szlam, and Fergus 2016; Lowe et al. 2017; Jiang and Lu 2018; Wang et al. 2019; Rangwala and Williams 2020; Chen et al. 2023c; Chen, Lan, and Choi 2023) and discrete communication (Mordatch and Abbeel 2017; Freed et al. 2020; Lazaridou and Baroni 2020; Li et al. 2022c; Tucker et al. 2022; Chen, Lan, and Joe-Wong 2024). Continuous communication, though expressive, suffers from explainability and high overhead (Chen et al. 2023b), while discrete methods (Lowe et al. 2017; Sukhbaatar, Szlam, and Fergus 2016) limit relational learning. Learnable mes-sages (Tucker et al. 2021) improve but lack Dec-POMDP guarantees and require large vocabularies.\nRecent work uses LLMs to enhance communication in embodied tasks, e.g., CoELA's free-form natural language generation (Zhang et al. 2023). However, these approaches lack structured planning or reasoning about actions' conse-quences. LLM-based agents like SPRING leverage exter-nal knowledge for planning, but often do not learn from in-environment interactions (Wu et al. 2024). LLM agents have also been used to simulate human behavior and feed-back but rely solely on the language model rather than struc-tured reasoning (Yang, Siew, and Joe-Wong 2024). Our work addresses these by proposing a structured communication protocol and a knowledge-graph memory system that allows agents to learn from each other's experiences.\nGenerative Agents. The use of LLMs for decision-making and planning in dynamic environments has grown rapidly (Yang et al. 2023; Wang et al. 2024; Xi et al. 2023; Sumers et al. 2023). Although LLMs face challenges in han-dling complex reasoning tasks (Bubeck et al. 2023), they have shown significant promise in guiding agents in real-time environments (Park et al. 2023; Sharma, Torralba, and Andreas 2021; Raman et al. 2024; Pallagani et al. 2022; Gramopadhye and Szafir 2023; Yuan et al. 2023; Li et al."}, {"title": "Framework: DAMCS", "content": "In this section, we give an overview of our framework. We first formally define how this framework interacts with our problem environment (Section 3.1) and then describe the design of our multi-modal, adaptive memory system (Sec-tion 3.2), structured LLM output for making agent decisions (Section 3.3) and communication protocol that enables agent cooperation (Section 3.4).\n3.1 Problem Setting\nOur goal is to demonstrate that Large Language Models (LLMs) can effectively plan, coordinate, and execute tasks in a multi-agent environment where collaboration and re-source management are critical. We consider an environ-ment model that follows a Decentralized Partially Observ-able Markov Decision Process (Dec-POMDP) (Bernstein et al. 2002; Chen, Lan, and Joe-Wong 2024), as is com-mon in cooperative MARL, where agents lack complete in-formation about the environment and have only local ob-servations. Figure 2 gives an overview of this framework.\nWe model the environment as a Dec-POMDP with commu-nication as a tuple D = (I,n, S, A, P, \u03a9, O, g, R), where I = {1,2,..., n} is a set of n agents, S is the joint state space, and A = A1 \u00d7 A2 \u00d7 \u00b7\u00b7\u00b7 \u00d7 An is the joint action space, where a = (a1,a2,...,an) \u2208 A denotes the joint action of all agents. P(s'|s, a) : S\u00d7A\u00d7S \u2192 [0,1] is the state transition function that describes how the environment state evolves, given the actions taken by the agents. We consider an episode that is divided into a series of timeslots t = 1,2,...; at the start of each episode, agents respawn in the center of the map. Within each timeslot, each agent can take an action, e.g., sharing resources with another agent or working towards a goal. Agents decide their action based on their observations, which are contained in the ob-servation space \u03a9, and O(s, i) : S \u00d7 I \u2192 \u03a9 denotes the function that maps from the joint state space to distributions of observations for each agent i. Each agent's observations, as shown in Figure 2, include its own environment input, as well as communication messages from the other agents. We use g : \u03a9 \u2192 M to denote the communication message generation function that each agent j uses to encode its lo-cal observation of into a communication message for other agents i \u2260 j. We use m_i = {mj = g(0;),\u2200j \u2260 i} to denote the collection of messages agent i receives from all other agents j \u2260 i.\nIn deciding which actions to take, the agents' goal is to maximize the long-term reward. More formally, they aim to find a policy that maximizes the average expected return lim\u012b\u2192\u221e(1/T)\u0395\u03c0[\u2211t=0 Rt], where R(s, a) : S \u00d7 A \u2192 R is the reward of the current state s and joint action a and Rt is the reward incurred in timeslot t. As shown in Fig-ure 2, this policy goal is enforced in our framework by in-cluding it in a prompt that is fed to a multi-modal large language model (MLLM) along with a prompt to generate plans and actions for the current timestep, thus forming the policy \u03c0. For example, Agent 6 in Figure 2 is told to find a diamond. To ensure the LLM finds a good policy based on historical data, each agent maintains its own memory, con-sisting of both Short-Term Working Memory (STWM) and Long-Term Memory (LTM). The STWM holds infor-mation for decision-making at the current timestep, combin-ing current environmental perceptions with relevant infor-mation retrieved from LTM. The STWM is then included in the MLLM prompt. The STWM and MLLM responses are then consolidated into the agent's LTM, enabling agents to make strategic decisions based on historical context.\n3.2 Adaptive Knowledge Graph Memory System\nRecent work in multi-task learning has demonstrated the benefits of integrating heterogeneous data sources for op-timized decision-making (Baltru\u0161aitis, Ahuja, and Morency 2018; Ngiam et al. 2011; Xu et al. 2024). In the pro-posed Adaptive Knowledge Graph Memory System (A-KGMS), inspired by human cognitive processes (Sumers et al. 2023), each agent uses a multi-modal memory sys-tem combining short-term and long-term memories that fa-cilitates storing and retrieving experiences across different memory types. While existing memory systems focus on as-pects like semantic understanding (Li et al. 2024), our sys-"}, {"title": "Structured Communication System", "content": "Craft Tools\nand Share\nHelp Agent 1\nHelp Previous\nAgent and Agent 1\nHelp Previous\nAgent and Agent 1\nHelp Previous\nAgent and Agent 1\nFind Diamond\nAgent 1\nAgent 2\nAgent 3\nAgent N-1\nAgent N\nFigure 4: Communication Protocol. Agents collaborate by exchanging messages to coordinate tasks and share re-sources. An arrow from agent i to agent j indicates that agent i is helping agent j; communication then flows in the opposite direction.\nIn a multi-agent environment, communication is key for effective cooperation. Our communication framework, con-sisting of message generation modules g = {g1,..., gn} for all agents, where mi = gi(Oi,rsi,ci, rqi), enables agents to share their current observations oi, includes status si, re-source availability rsi, short-term goal ci, and assistance re-quests rqi. This follows a hierarchical structure, where each agent i prioritizes helping the preceding agent i \u2013 1.\n=.\nWe propose a novel Collaboration schema Ci\n(hi, Ii, Api) for each agent i and add this to the struc-tured outputs, which is based on the target agents hi who needs help from agent i, intentions Ii to assist target agents from agent i, and how the collaboration impacts agent i's current plan, denoted by Api. In our multi-agent system, the message generation function gi can be augmented by incor-porating the collaboration schema Ci to refine and guide the message generation process, then the message genera-tion process is enhanced by the information encoded in Ci, i.e., mi = gi(0i, rSi, Ci, rqi, Ci). Therefore, the Collabora-tion schema enables agents to interpret and generate actions ai = \u03c0\u03af(0i, m-i), where m\u2212i = {mj = g(oj),\u2200j \u2260 i} to denote the collection of messages agent i receives from all other agents j \u2260 i. This structure ensures that our collabora-tive agents act in a goal-oriented manner with collaboration as a key consideration.\nAn Illustrative Example. As illustrated in Figure 4, agents collaborate by communicating and sharing resources"}, {"title": "Evaluation Challenges of LLM Agents", "content": "Evaluating LLM-powered multi-agent systems presents unique challenges. Unlike MARL-based agents, which are trained to optimize carefully crafted rewards, LLM agents rely on prompts and contextual information, making them highly adaptable but sensitive to the evaluation environment.\nLimitations of Existing Environments. Existing multi-agent benchmarks are often too simple for meaningful col-laboration (Terry et al. 2021) or too complex (Berner et al. 2019; Vinyals et al. 2019; Fan et al. 2022). Many focus on micro-level action management, whereas our work empha-sizes macro-level planning, communication, and coopera-tion. Furthermore, MARL frameworks are known for scal-ability challenges, and existing environments are often not designed to support cooperative tasks that scale well with an increasing number of agents.\nEvaluation of Cooperation. LLM-based collaboration is highly adaptable but difficult to quantify. Unlike RL agents that optimize reward signals, LLM-based collaboration re-lies on context and commonsense reasoning, making re-sponses variable. No standardized metric exists for evalu-ating cooperation among LLM agents, and extensive mod-ifications to benchmarks are often required. Testing with environment-specific prompts is also time-consuming.\nQuantifying LLM Agents' Capabilities. Evaluating memory quality and adaptability in LLM agents is non-trivial. While our A-KGMS organizes past experiences, de-termining the quality of stored information and its impact on decision-making remains challenging. Adaptability is also difficult to measure, as LLM agents adjust dynamically rather than optimizing predefined objectives.\nTo address these challenges, we introduce Multi-Agent Crafter to evaluate strategic coordination, planning, and resource sharing in open-ended, scalable cooperative tasks."}, {"title": "Multi-Agent Crafter: A Novel Testbed", "content": "The original Crafter environment (Hafner 2021) is a proce-durally generated, open-world survival game used to bench-mark RL algorithms. It features a 17 discrete action grid world and tracks player metrics like inventory, health, and crafting progress, with 22 achievements organized in a 7-depth tech tree. Inspired by Minecraft, Crafter simplifies game mechanics for faster experimentation and results col-lection. We proposed a novel multi-agent Crafter for multi-agent tasks, enabling cooperative agent interaction and intro-ducing new actions and challenges. These changes, shown in Figure 1, make the environment suitable for studying multi-agent cooperation. Key modifications are outlined below.\nA Scalable Cooperative Environment. We extended the Crafter environment to support an arbitrary number of agents, each with independent observations, inventories, and health stats, enabling cooperative agent interaction and in-troducing new actions and challenges (Figure 1). Agents can collaborate by sharing resources, coordinating actions, and balancing individual roles to achieve collective goals effi-ciently. Unlike traditional MARL environments, which of-ten focus on micro-level action management, our testbed is designed to evaluate strategic planning, coordination, and shared decision-making.\nOur environment allows agents to share items, including resources and tools, fostering teamwork by enabling task delegation and resource management. Crafting dependen-cies and environmental prompts can be easily customized, increasing task complexity with more participants. This ensures that agents must coordinate and efficiently allocate roles, enabling effective large-scale parallel collaboration. The flexible design makes the testbed suitable for evaluating cooperative behavior potentially for any number of agents.\nEvaluation of Cooperation and LLM Agents' Capa-bilities. Unlike the original Crafter environment, which fo-cused on open-ended exploration, we define a clear objec-tive: agents must collaborate to craft necessary tools and ob-tain a diamond as quickly as possible while managing their needs for food, water, and energy. This setup allows us to evaluate whether agents can effectively cooperate and rea-son toward both short- and long-term goals, making the en-vironment ideal for testing multi-agent coordination, plan-ning, and resource optimization.\nTo assess cooperative efficiency, agents share resources and tools, requiring negotiation, task division, and decision-making. Unlike previous MARL settings, where collabo-ration is forced or predefined, our testbed allows agents to develop teamwork strategies. Our environment quanti-fies multi-agent cooperation through indirect measurements, such as tracking the steps an agent takes to craft items, providing insights into decision-making and adaptability.\nSupport for Language Agents. We added a navigation skill that allows agents to move toward specific resources, reducing the burden of manual low-level movement con-trol. This enables agents to focus on higher-level decision-making, such as strategic planning and collaboration.\nCustomizability and Compatibility. Our multi-agent Crafter environment is designed to be highly flexible and ex-tensible, supporting RL, MARL, and LLM-powered agents. The single-agent version follows the Gymnasium API, en-suring integration with standard RL libraries, while the multi-agent version aligns with the PettingZoo API, ensur-ing compatibility with existing MARL frameworks. We pro-vide example training scripts for single-agent experiments using Stable-Baselines3 (SB3) and multi-agent experiments using AgileRL, allowing researchers to efficiently test new"}, {"title": "Evaluations", "content": "6.1 Experiment Setup and Baselines\nExperiment Setup. We evaluate our framework on the proposed Multi-agent Crafter environment (Sec. 5) to test agents' ability to plan and complete hierarchical tasks through cooperation and assess how well our framework scales in multi-agent settings. The final goal is for agents to collect a diamond, which requires completing a series of tasks: collecting stone, iron, and coal, crafting a furnace, and making an iron pickaxe (task hierarchy shown in Figure 1). With our framework's memory systems and communication module for multi-agent collaboration, agents efficiently col-lect a diamond by distributing tasks and gathering resources in parallel. To evaluate the performance of our framework and analyze the effectiveness of the memory system and communication, we conducted experiments with n = 1,2,6 agents. The LLM used in our framework is GPT-40, de-ployed via the Azure platform.\nBaselines. We compare our framework with RL/MARL and LLM baselines: (1). RL/MARL baselines. Since we have modified an environment originally designed for rein-forcement learning (RL) agents, we test it with RL agents to evaluate if they can learn from the environment in both single-agent and multi-agent setups. We use Proximal Pol-icy Optimization (PPO) for the single-agent case and Multi-Agent Deep Deterministic Policy Gradient (MAD-DPG) for the multi-agent case, MADDPG is recognized as state-of-the-art by BenchMARL (Bettini, Prorok, and Moens 2024). The objective of the environment is to gather a diamond as quickly as possible. The reward is structured so that each item in the hierarchy tree (Figure 1) is assigned a score based on its depth, along with a time penalty. (2). LLM baselines. We also includes LLM-based baselines un-der difference settings: (i). LLM basic, a basic LLM Agent that employs the structured output prompt combined with a basic memory approach in a single-agent setting, where the agent is aware only of its previous actions; (ii). LLM Mem, a structured output prompt with memory only (Mem only); (iii). LLM MemComm a structured output prompt with both memory (Mem) and communication (Comm). These different LLM agents' configurations allowed us to evaluate the contribution of each component of Decentralized Coop-erative Generative Agents.\nMetrics. Algorithm performance across all LLM-based settings was measured using the Average number of time Steps (AS) within an episode required for at least one agent to complete each task, as displayed in Figure 6 and Table 1.\n6.2 Experimental Results\nEvaluating RL/MARL agents. The training results are shown in Figure 5. We trained for 1,000 episodes using a Convolutional Neural Network (CNN) policy for both the actor and critic networks. The reward trends indicate im-provement, confirming that the environment is suited for re-searchers developing RL and MARL methods. However, af-"}]}