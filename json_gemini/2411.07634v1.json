{"title": "Exploring Multi-Agent Reinforcement Learning for Unrelated Parallel Machine Scheduling", "authors": ["Maria Zampella", "Urtzi Otamendi", "Xabier Belaunzarana", "Arkaitz Artetxe", "Igor G. Olaizola", "Giuseppe Longo", "Basilio Sierra"], "abstract": "Scheduling problems pose significant challenges in resource, industry, and operational management. This paper addresses the Unrelated Parallel Machine Scheduling Problem (UPMS) with setup times and resources using a Multi-Agent Reinforcement Learning (MARL) approach. The study introduces the Reinforcement Learning environment and conducts empirical analyses, comparing MARL with Single-Agent algorithms. The experiments employ various deep neural network policies for single- and Multi-Agent approaches. Results demonstrate the efficacy of the Maskable extension of the Proximal Policy Optimization (PPO) algorithm in Single-Agent scenarios and the Multi-Agent PPO algorithm in Multi-Agent setups. While Single-Agent algorithms perform adequately in reduced scenarios, Multi-Agent approaches reveal challenges in cooperative learning but a scalable capacity. This research contributes insights into applying MARL techniques to scheduling optimization, emphasizing the need for algorithmic sophistication balanced with scalability for intelligent scheduling solutions.", "sections": [{"title": "1. Introduction", "content": "Scheduling problems constitute a subset of optimization challenges that find widespread applications across various sectors, encompassing resource management [1], industry [2, 3], and operational management [4]. In particular, production scheduling, an essential facet of manufacturing, revolves around the efficient and cost-effective allocation of limited resources to support production processes. In this context, implementing flexible and intelligent strategies for industrial scheduling emerges as an imperative.\n\nThe primary objective of scheduling problem optimization is to identify an advantageous combination of decision variables within a defined search space. These decision variables dictate the order in which processes or tasks are assigned to a set of machines, typically entailing complex combinatorial problems. These problems often involve optimizing multiple objectives, depending on the system demands [5]. In industrial settings, the overall aim predominantly implies minimizing job completion times while accommodating other objectives, such as resource utilization or environmental considerations.\n\nIt is important to note that real-world industrial scheduling problems imply exploring extensive search spaces, usually culminating in NP-hard problem instances [6]. This inherent intricacy poses complex challenges to the research and development community. This characteristic has attracted considerable attention from the research community, as exemplified in the comprehensive survey by Allahverdi et al. [7].\n\nNotwithstanding the advancements in this field, current approaches have notable limitations [8], encompassing computational complexity [9] and the capacity to generalize and adapt to diverse problem instances [10, 11]. In response to these constraints, contemporary research has increasingly employed advanced decision-making techniques.\n\nDeep Learning techniques have obtained significant attention in intelligent decision-making systems within the research community [12]. Specifically, Reinforcement Learning (RL) approaches have emerged as valuable solutions to addressing scheduling in complex environments [13]. This machine learning paradigm relies on utilizing an intelligent agent that learns from the actions it takes and the rewards it receives in response to those actions. Reinforcement Learning models exhibit adaptability to non-deterministic environments, making it a flexible approach for optimization within dynamic and uncertain environments.\n\nThis paper presents a study of a Multi-Agent Reinforcement Learning (MARL) approach to addressing an"}, {"title": "2. Related work", "content": "Job scheduling is a fundamental optimization process used in various industries [15]. This field involves task allocation to machines to optimize objectives. This optimization class encompasses diverse variations based on task types, machines, and constraints [16], classifying them into single- and multi-step processes.\n\nThe job scheduling problem is a well-known intricate combinatorial optimization issue within machine scheduling, long identified as NP-hard [6]. Due to its formidable complexity, obtaining an optimal solution in a reasonable timeframe is frequently unattainable [17]. The literature has proposed diverse approaches to tackle this challenge, including neural networks and deep learning methods. Genetic algorithms (GAs), initially introduced by Holland et al. [18], are actively explored, yielding promising results [19, 20]. Evolutionary Algorithm (EA) counterparts such as Differential Evolution (DE) [21] have also received attention. Ant Colony Optimization [22], as exemplified by [23] in Flexible Job Shop Scheduling (FJSS), optimizes setup and transportation time.\n\nSimilarly, Artificial Bee Colony techniques by Karaboga et al. [24] and improved approaches by Lei et al. [25] have shown promising results in multi-objective problems for distributed parallel machine scheduling. Greedy randomized adaptive search (GRASP) [26], a widely-used metaheuristic algorithm, finds application in minimizing makespan and workloads under resource constraints [27]. Particle swarm optimization (PSO) [28], inspired by collective behaviour in nature, has also seen success in Job Shop Scheduling (JSS) problems [29]. Simulated Annealing (SA) [30] has been applied effectively, notably in makespan minimization by Loukil et al. [31] and job-machine assignment and sequencing by Yazdani et al. [32]. Recent decades have witnessed the adoption of heuristics and metaheuristics to address scheduling optimization challenges, with hybrid approaches integrating optimization techniques gaining prominence [33].\n\nIn this sense, researchers have devised mathematical models to optimize diverse objectives. He et al. [34] introduce a novel model for the FJSS problem, considering machine breakdowns through non-cooperative game theory. Demir et al. [35] explore various mathematical formulations for FJSS, offering a time-indexed model for makespan minimization. Concerning the Unrelated Parallel Machine Scheduling (UPMS) problem, exact methods like Mixed-Integer Linear Programming (MILP) have been proposed, adeptly managing complex constraints and delivering optimal solutions for relatively small instances [36, 37].\n\nThese methodologies are categorized as static scheduling methods, which make decisions at compilation time and rely on prior task knowledge. In this sense, static scheduling techniques necessitate complete and precise task information, which is challenging to obtain in uncertain environments. In contrast, dynamic scheduling, operating during execution, utilizes computational state information for decision-making.\n\nReinforcement Learning (RL) has emerged as an advantageous approach for dynamic task scheduling due to its capacity to handle environmental uncertainty in dynamic contexts, its ability to learn from experience, computational efficiency, and high adaptability. Reyna et al. [38] presented the Q-Learning Reinforcement Learning algorithm to solve Job Shop and Flow Shop scenarios. In this work, the authors validate the approach's efficacy by comparing results against reported optimal solutions in specialized literature. Extending the mentioned approach, [39] employing Deep Q-Learning with graph representations of states accommodating dynamic problem potential deviations by incorporating graph-convolutional policies into Monte Carlo Tree Search. Li et al. [40] proposed a two-stage RNN-based deep reinforcement learning approach, demonstrating strong generalization capabilities in extensive numerical experiments. However, these works need more exploration into the potential sensitivity of the proposed agent to changes in problem characteristics. To overcome the limitation of the Single-Agent approach, Liu et al.[41] propose a deep Multi-Agent reinforcement learning-based approach to solve the dynamic job shop scheduling problem by using a centralized training and decentralized execution scheme and parameter-sharing technique to tackle the non-stationary problem. Despite the extensive literature on scheduling methodologies, potential benefits and challenges associated with integrating Multi-Agent techniques in this domain still need to be explored. Our research addresses this gap by investigating and proposing innovative Multi-Agent solutions for scheduling problems."}, {"title": "3. Problem description", "content": "In this section, we formally define the problem of Unrelated Parallel Machine Scheduling (UPMS) with resources and setup time and introduce the relevant mathematical notation. This problem focuses on efficiently allocating jobs to a group of Unrelated Parallel Machines, with due consideration given to setup times, resource utilization, and human-operator capabilities. Additionally, we present an illustrative example.\n\n3.1. Mathematical Notation\n\nThe mathematical notation of UPMS in this paper is as follows:\n\n\u2022 Machines: a set of $M$ unrelated parallel machines, denoted as $M = \\{m_i | i \\in \\{1,2,...,M\\}\\}$.\n\n\u2022 Jobs: a set of $J$ jobs, denoted as $I = \\{j_i | i \\in \\{1,2,..., J\\}\\}$.\n\n\u2022 Processing Time: the time required to process job $j$ on machine $m$, denoted as $pt_{jm}$.\n\n\u2022 Setup Time: the time required to transition time from job $j_i$ to job $j_k$ on machine $m$, denoted as $St_{j_i j_k m}$\n\nThe worker concept is introduced to address the resource utilization:\n\n\u2022 Workers: A set of $W$ workers, denoted as $W = \\{W_i | i \\in \\{1,2,..., W\\}\\}$.\n\n\u2022 Worker-Machine Compatibility: Binary variable $O_{wm}$ represents whether worker $w$ can operate machine $m$, where $o_{wm} = 1$ implies compatibility and $o_{wm} = 0$ otherwise.\n\n\u2022 Required Workforce: The number of workers needed to perform job $j$ on machine $m$, denoted as $r_{jm}$.\n\nThe primary objective of the UPMS problem is to efficiently allocate tasks to machines and determine the sequence in which these tasks are executed on each machine. This considers various parameters, including processing times, setup times, or workforce constraints.\n\nThe optimization objectives are dependent on the requirements and intended behaviour. In this example of the UPMS problem, a multi-objective function is employed to assess solution optimality. This function integrates diverse objectives, including minimizing overall task completion time, reducing setup times, and optimizing resource utilization. The primary challenge involves proficiently allocating tasks across machines and scheduling them to achieve these objectives while respecting constraints associated with worker-machine compatibility.\n\n$\\text{Min } f(x) = w_1 \\cdot T(x) + w_2 \\cdot U(x) - w_3 P(x)$ (1)\n\nWhere:\n\n$T(x)$ represents the overall task completion time, considering job processing times and setup times,\n\n$U(x)$ represents the resource utilization.\n\n$P(x)$ represents the number of jobs performed.\n\nThe weights $w_1$, $w_2$, and $w_3$ are used to adjust the importance of each objective based on the optimization priorities within the multi-objective function. These weights can be set differently depending on the specific goals of the manufacturing plant. For example, if the goal is to reduce resource usage, the model has to decide to assign jobs to machines with lower resource requirements, even if this results in an increase in processing time.\n\n3.2. Illustrative Example\n\nWe present a practical scenario within a manufacturing context is considered. In this scenario, three machines (designated as $M = 3$) are considered for processing five jobs ($J = 5$). The workforce consists of two workers ($W = 2$). The analysis involves the utilization of processing times ($pt_{jm}$, as introduced in Table 1), setup times ($st_{j_i j_k m}$), worker-machine compatibility ($o_{wm}$, documented in Table 3), and the implied required workforce ($r_{jm}$, provided in Table 2) for each job-machine pairing. For simplification, a uniform setup time of 1 is assumed for all job-to-job transitions.\n\nThis schedule is intended to minimize job completion times while concurrently optimizing setup times and accommodating the constraints associated with workforce availability and capacity.\n\nIn Figure 1, the solution completes the jobs within 8 hours. However, it is crucial to recognize alternative solutions within this particular example that might suit the"}, {"title": "4. RL environment", "content": "This section introduces the essential elements of the Reinforcement Learning environment implemented in this approach. We explain fundamental concepts such as the action space, observation space, and the intricacies of the reward function. Additionally, we delve into the necessary adaptations for a Multi-Agent scenario, establishing the groundwork for a comprehensive understanding of the functionality and efficacy of this approach in complex decision-making scenarios.\n\nIn Reinforcement Learning, key concepts include the environment (a container holding information about the whole system), the agent (the entity interacting with the environment), the definition of states (describing the current situation), actions (possible moves) and rewards (numeric feedback on decision effectiveness). The agent interacts with the environment through actions, leading to state changes and the receipt of rewards based on goal achievement. These rewards may not always be directly linked to a single action but can result from a sequence of prior actions.\n\nThis technique implies optimizing the selection of actions to perform through policies (denoted as $\\pi$), which are functions mapping states to actions,$\\pi(s)$. It can also signify the probability of taking action $A_t$ in a given state $S_t$, $\\pi(a|s)$. The objective is to maximize the cumulative rewards in the long term contingent upon the selected actions. This pursuit leads to seeking the optimal policy, which helps to make the optimal decision in each state.\n\n4.1. Single-agent approach\n\nThe environment comprises plant machines, remaining jobs, and available resources. As mentioned, the jobs are characterized by processing time, setup time, and resource requirements, the variables related to the objective function.\n\nA pivotal aspect of the Single-Agent environment is the concept of the job-slot. A job-slot is a collection with a fixed size of candidate jobs the agent considers at each time step, while the remaining jobs are queued in the backlog. Except for their quantity, the agent is unaware of the specific jobs in the backlog at each time step. The job-slot mechanism ensures a constant number of jobs are evaluated at each time step, standardizing the observation and action space across different scenarios and reducing computational demands.\n\n4.1.1. Action space\n\nIn this framework, an action represents assigning a job to a machine. The action space consists of n job-machine pairings, where n equals the job-slot size multiplied by the number of machines, plus one representing the do-nothing action. When the action do-nothing is selected by the agent, it means there is no other possible action, and the environment passes to the next time step."}, {"title": "4.1.2. Observation space", "content": "The observation space characterizes the necessary information for the policy to determine the optimal action. In this environment, the observation is represented as an array of essential details associated with each job-machine pairing (j,m) within the context of the current decision step. Specifically, five informative elements are stored for every job-machine assignment.\n\n\u2022 Remaining processing time is registered (RT(m)), providing insights into the temporal aspects of the ongoing job (jm) and the availability of the machine. In case the machine is not working, it is zero.\n\n\u2022 Available resources is included, contributing to assessing the human resources available to operate the machine (ARm).\n\n\u2022 Total scheduled time of the machine (T(m)) is another crucial element, influencing decisions to establish a homogeneous workload distribution among the various machines.\n\n\u2022 Total time required for job j execution, comprising both processing time and setup time ($pt_{jm}+st_{jmjm}$).\n\n\u2022 The required resources that the job demands for its execution on the specified machine ($r_{jm}$).\n\nSimilar to the challenges encountered in defining the Action space, the Observation space presents analogous considerations. The Observation space's size correlates to the number of unique job-machine combinations under review at a given moment. Notably, the observation serves as the input for the policy, necessitating a fixed size to align with the requirements of the underlying neural network architecture.\n\nMoreover, considering all possible job-machine pairings, a comprehensive exploration of the observation space poses a computational challenge for the agent. The combinatorial explosion of potential combinations generates an exhaustive exploration that is impractical. Hence, a"}, {"title": "4.1.3. Objective and Reward Design", "content": "In pursuing an effective scheduling strategy, this approach aims to optimize the allocation of jobs to suitable machines while concurrently minimizing the total completion time and resource utilization across all tasks. Notably, the optimization process does not account for allocating specific resources to individual jobs; instead, resources are assigned using a random algorithm.\n\nThe formulation of this multi-objective function is expressed mathematically in the equation 1, which considers the minimization of the total completion time (setup- and processing time), the minimization of resource utilization and the maximization of the number of assigned jobs. This objective encapsulates the core challenges of resource allocation and task completion efficiency within the given problem domain.\n\nThe reward function is designed to guide the RL agent toward learning a scheduling strategy that achieves the overarching objective and addresses scenarios that may occur during action-making. In this implementation, the reward function is cumulative. It is built by various components, each capturing a specific aspect of the decision-making process. The key insights from the reward function are presented in Table 4.\n\nCollectively, these components create a reward system that encourages the RL agent to make decisions aligning with the overarching scheduling objective. Furthermore, this reward makes the model assign jobs to machines with fewer resource requirements and processing time. In this sense, total time comprises the processing and setup time, making the system select the optimal sequences and reducing setup between the machine's jobs.\n\nDuring the experimentation, it became clear that the initial reward setting failed to prevent undesirable learning outcomes. Specifically, during training, the agent repeatedly chose the do-nothing action or selected empty job-slots. This unintended behaviour posed a challenge to achieving optimal scheduling.\n\nTherefore, to address this issue, the reward function includes additional components to steer the agent away from non-optimal decisions and facilitate convergence. These supplementary rewards serve as corrective mechanisms to prevent persistently opting for actions that do not contribute to the scheduling objective. The introduced corrective measures are as follows:\n\n\u2022 Excessive Inaction Penalization: if the agent chooses the do-nothing action more repeatedly when alternative actions are feasible, it triggers a proactive intervention. In response, a substantial malus of -10 is applied to the reward, acting as a deterrent to the undesired excessive inaction."}, {"title": "4.2. Multi-agent approach", "content": "In the multi-agent scenario, each machine is considered separately and essentially treated as an individual agent in the system. This approach simplifies the system by reducing the observation and action space to focus on the jobs associated with each specific machine. Consequently, in a problem involving M machines, M agents, action spaces, and observation spaces are defined.\n\nA cooperative interaction approach is employed in this multi-agent environment to promote collaboration and coordination among these agents. This implies that the agents are designed to work towards a common objective instead of competing with each other.\n\n4.2.1. Action space\n\nIn this framework, since each machine is considered an independent agent, decision-making is reduced to jobs which can be executed on the machine. Therefore, the action space is defined by the jobs associated with the machine it represents. In parallel with the Single-Agent approach, the job-slot maintains consistency across action and observation spaces.\n\n4.2.2. Observation Space\n\nThe observation space is individualized to include information relevant to the executable jobs on the respective machine. This individualized perspective is critical to improve effective decision-making. Following the environment characterization of the Single-Agent approach, every"}, {"title": "4.2.3. Global Observation", "content": "Centralization is critical in the multi-agent approach, wherein a coordinator plays an essential role by maintaining a global view that encompasses all agents. This coordinator acts as an additional agent but has the unique characteristic of having a global observation. This global observation aggregates the individual observations from all the agents. This agent's use and role depend on the algorithm employed.\n\nThe global observation comprises collective insights from each agent, offering a perspective on the system's global state. In this sense, centralized coordination becomes valuable in scenarios where collaborative strategies are essential for achieving optimal outcomes."}, {"title": "4.2.4. Objective and Reward Design", "content": "The objective remains consistent with the Single-Agent framework, aiming to schedule all jobs with minimum completion time and resource utilization. The only difference is in resource allocation: the Single-Agent approach employs a random selection of workers (resources), whereas the Multi-Agent framework utilizes a greedy algorithm for allocation by considering the number of machines each worker can operate. Adopting a greedy assignment method mitigates resource allocation conflicts that may emerge in the Multi-Agent framework due to simultaneous job assignments across all machines at each time step.\n\nIn the Multi-agent approach, each agent has its reward. This reward is calculated using the same design presented in the Single-Agent method (see table 4). Additionally, in this framework, an extra reward component is considered to address possible conflicts between the agents. In a time step, the agents are unaware of the decision taken by other agents at this exact moment. Consequently, there is a possibility of two or more agents taking action over the same job. In such cases of overlap, a malus of -1 is assigned to the rewards of the involved agents."}, {"title": "5. Methodology", "content": "This section presents an in-depth analysis of the algorithms employed in this research. The primary objective is to conduct a meticulous comparative assessment, commencing with the comprehensive exposition of Single-Agent methodologies. Four distinct algorithms have been implemented within this scope. Following this exploration, the analysis extends into the domain of Multi-Agent approaches, thereby significantly broadening the research landscape. This methodology section is an indispensable resource, facilitating a comprehensive understanding of these algorithmic implementations and their applicability in addressing complex decision-making scenarios.\n\n5.1. Single-agent\n\nIn the context of this Single-Agent framework, the work incorporated four algorithms, namely Deep Q-Learning (DQN) [38], Advantage Actor-Critic (A2C) [42], Proximal Policy Optimization (PPO) [43], and Maskable PPO [44]. All these algorithms combine reinforcement learning with deep neural networks, allowing agents to make decisions based on maximizing estimated cumulative rewards. A brief description will outline their distinctive characteristics and limitations.\n\nDeep Q-Learning (DQN) is based on Q-learning, which aims to approximate the optimal action-value function (Q-function), indicating the highest expected reward an agent can achieve by executing each possible action in a specific state. In the interaction phase, the agent receives environmental observations and utilizes the Q-network to decide the most promising action, recording experiences into a buffer. In the learning phase, a batch of experiences is randomly selected to train the deep neural network, aiming to minimize the discrepancy between predicted and ground-truth Q-values. Additionally, the algorithm employs a target network to stabilize learning and an epsilon-greedy strategy to ensure the exploration of the environment [45]. DQN has demonstrated remarkable success in complex environments, surpassing human-level performance in various Atari games [46].\n\nAdvantage Actor-Critic (A2C) is an advanced variant of the Actor-Critic algorithm [47], which leverages the Advantage function to stabilize training and reduce variance. In this approach, the actor represents the policy function, guiding the agent's actions, and the critic estimates the Q-value function. The critic and the actor functions are parameterized with neural networks. The A2C efficiently combines policy and value-based approaches, allowing faster convergence in continuous action spaces. However, A2C can encounter challenges related to elevated"}, {"title": "5.2. Multi-agent", "content": "The work incorporated three algorithms in this Multi-Agent framework. A brief, comprehensive description will outline their distinctive characteristics and limitations. Counterfactual Multi-Agent Policy Gradients (COMA) and Multi-Agent PPO (MAPPO) share the same base structure: Centralized training with Decentralized execution (CTDE) composed of a centralized critic (built using the global state) and decentralized actors [49]. At the same time, Multi-Agent Trust Region Policy Optimization (MATRPO) is an entirely decentralized algorithm.\n\nCounterfactual Multi-Agent Policy Gradients by Foerster et al. [50] is a Multi-Agent actor-critic approach utilizing a centralized critic to train decentralized actors for each agent. It involves estimating a counterfactual advantage function for separate agents addressing Multi-Agent reward assignments. The method uses a counterfactual baseline to assess a single agent action while keeping others fixed. The centralized critic representation provides efficient computation of the counterfactual baseline. The"}, {"title": "6. Experiments", "content": "This section introduces the experiments undertaken during the implementation phase of the approach. The presented algorithms have distinctive characteristics, encompassing vulnerabilities and strengths. Due to these differences, defining equitable conditions for consistently training these models becomes challenging. Nevertheless, within this section, both Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL) technologies undergo training within identical scenarios featuring an equivalent number of machines, jobs, and general conditions. Notably, the training duration, expressed in timesteps, stays invariant across both single- and Multi-Agent approaches to facilitate a comprehensive benchmarking of the algorithms. Moreover, a predefined hardware configuration is employed throughout the training process of deep learning models. This process aims to acquire the best-performing models, distinguishing between Single-Agent and Multi-Agent approaches.\n\nIn the experimental phase, multiple deep neural networks were employed as the policy, for instance, Multi-Layer Perceptron [54], Long Short-term Memory [55] and"}, {"title": "6.1. Results", "content": "In the training phase, various scenarios are generated to expose the models to diverse operational conditions. Nevertheless, the principal characteristics of the environment remain static. The models are subjected to a scenario characterized by 30 jobs (J=30), 12 machines (M=12), 60 workers (W=60), and a job-slot constraint of 10.\n\nNotably, variables such as processing time per machine, the number of units, resource requirements, or setup times are randomly modified to introduce dynamic elements, providing realistic variations. The jobs are generated using a processing-time distribution of U(10,30) and setup time distribution of U(10,20). The resource requirement is defined using a distribution of U(1, 5) workers. This procedural approach ensures that the algorithms face scenarios with dynamic variables while maintaining static parameters, such as the number of machines and job slots, to prevent conflicts with neural network input and output sizes.\n\nThe models undergo a training phase of five different runs of 5 million timesteps each. In this section, the episode mean reward is employed as the metric for algorithm comparison. Given that the reward remains consistent for Single- and Multi-Agent setups, this metric is appropriate for analyzing the approaches.\n\nFigure 3 illustrates the training of the Single-Agent algorithms, showing the interval of the mean episode reward of the different runs. Comparing the results obtained, it is"}, {"title": "7. Conclusions", "content": "This research paper explores innovative methodologies for advanced decision-making systems, acknowledging the NP-hard nature of real-world scheduling problems. The work incorporates a Reinforcement Learning (RL) approach and presents a promising approach to address scheduling in dynamic environments with an uncertain nature. The work focuses on Multi-Agent Reinforcement Learning for the Unrelated Parallel Machine Scheduling Problem (UPMS), emphasizing its potential to adapt to non-deterministic environments.\n\nAdditionally, this article analyses the literature on Single-Agent RL algorithms, including Proximal Policy Optimization (PPO) [44], laying the groundwork for understanding the strengths and limitations in the domain of scheduling problems (see figure 3).\n\nThe observed results (see section 6) highlighted the efficacy of Single-Agent algorithms, especially of the Maskable PPO, in reduced scenarios, where training time and computational efficiency are critical. However, the scalability problems of these algorithms are evident in larger environments and action spaces. In this sense, the Multi-Agent algorithms revealed challenges in learning in a cooperative environment (see section 4.2), highlighting the need for further improvement in applying these algorithms to cooperative scheduling problems. However, the results obtained are promising on both Single- and Multi-Agent, specifically in both variations of PPO, the masked PPO and the Multi-Agent PPO algorithms.\n\nIn conclusion, this research contributes by presenting the application of novel algorithms in the domain and the practical challenges and opportunities in applying MARL techniques to scheduling optimization. These findings underline the importance of balancing algorithmic sophistication with scalability to enable advancements in intelligent scheduling solutions."}]}