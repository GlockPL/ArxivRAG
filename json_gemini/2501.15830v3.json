{"title": "SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model", "authors": ["Delin Qu", "Haoming Song", "Qizhi Chen", "Yuanqi Yao", "Xinyi Ye", "Yan Ding", "Zhigang Wang", "Jiayuan Gu", "Bin Zhao", "Dong Wang", "Xuelong Li"], "abstract": "In this paper, we claim that spatial understanding is the keypoint in robot manipulation, and propose SpatialVLA to explore effective spatial representations for the robot foundation model. Specifically, we introduce Ego3D Position Encoding to inject 3D information into the input observations of the visual-language-action model, and propose Adaptive Action Grids to represent spatial robot movement actions with adaptive discretized action grids, facilitating learning generalizable and transferrable spatial action knowledge for cross-robot control. SpatialVLA is first pre-trained on top of a vision-language model with 1.1 Million real-world robot episodes, to learn a generalist manipulation policy across multiple robot environments and tasks. After pre-training, SpatialVLA is directly applied to perform numerous tasks in a zero-shot manner. The superior results in both simulation and real-world robots demonstrate its advantage of inferring complex robot motion trajectories and its strong in-domain multi-task generalization ability. We further show the proposed Adaptive Action Grids offer a new and effective way to fine-tune the pre-trained SpatialVLA model for new simulation and real-world setups, where the pre-learned action grids are re-discretized to capture robot-specific spatial action movements of new setups. The superior results from extensive evaluations demonstrate the exceptional in-distribution generalization and out-of-distribution adaptation capability, highlighting the crucial benefit of the proposed spatial-aware representations for generalist robot policy learning. All the details and codes will be open-sourced.", "sections": [{"title": "I. INTRODUCTION", "content": "Generalist robot policies that are capable of interacting with the physical environment, adapting to various embodiments, and performing complex tasks have been a long-standing pursuit in robotics [6, 3, 16, 8, 60]. Recent advances in Vision-Language-Action (VLA) models [7, 29, 5, 32] show a promising paradigm in building such generalist policy by fine-tuning the pre-trained Vision-Language Models (VLMs) [1, 51, 48, 36] on diverse robot data [13, 28, 18]. The key to the success of this paradigm lies in adapting the generalization power of VLMs to numerous robot manipulation tasks, as well as specific architectural designs that synergize the VLM backbone and robot action output head. Nonetheless, existing VLA models are primarily confined to 2D observation inputs and lack precise perception and comprehension of the 3D physical world \u2014 where humans instinctively construct rich, structured mental representations of space, effortlessly aligning objects within a canonical, intuitive, and even personally tailored workspace for manipulation. Therefore, an essential question for the field now is how to effectively equip the VLA models with a profound spatial understanding of the 3D physical world?\nHowever, developing such generalist robot policies with 3D spatial intelligence encounters two primary challenges in the aspects of robot observation and action. Firstly, the observations from different robot embodiments are not 3D-aligned, because the camera sensors of different robots are various and mounted at different places (e.g. wrist and/or third-person), resulting in non-calibrated 3D observation spaces. Secondly, different robots have different action movement characteristics to accomplish diverse tasks, due to different degrees of freedom, motion controllers, workspace configurations, and task complexity, leading to significant difficulty in learning generalizable spatial actions. Despite some attempts in generalist policy learning across heterogeneous robots [46, 13, 29, 60], advancement in 3D spatial understanding abilities of generalist policy has significantly lagged behind. This is largely attributed to the heterogeneity in robot observation and action information. The solutions to the above challenges require spatial-aligned robot observation and action representations for cross-embodiment control and adaptation in the universal 3D physical world.\nIn this work, as illustrated in Figure. 1, we propose a generalist robot policy SpatialVLA, which equips the VLA model with 3D spatial intelligence by exploring aligned spatial representations of robot observation and action signals. SpatialVLA perceives 3D world through Egocentric 3D (Ego3D) Position Encoding to integrate 3D spatial context with semantic features. This position encoding is derived in the egocentric camera frame that eliminates the need for specific robot-camera calibration, which is universally applicable to various robot embodiments. As for robot actions, SpatialVLA unifies the action space of various robots via Adaptive Action Grids, which discretizes the continuous robot actions into adaptive spatial grids according to statistical action distributions on the whole robot episodes and learns spatial action tokens on these grids to align cross-robot actions with the 3D spatial structure of the physical world. Crucially, after pre-training, the learned spatial action grids demonstrate a superior capability in adapting to new robot environments via adaptively grid re-discretization, providing a flexible and effective approach to robot-specific post-training. We find that the proposed model SpatialVLA bridges observation inputs and action outputs in a universal robot-agnostic manner, which explores powerful 3D spatial-aware representations to enhance the VLA model.\nWe extensively evaluate and ablate SpatialVLA on diverse robot manipulation tasks and different robot embodiments in both simulation and real-world, including 24 real-robot tasks and 3 simulation environments. To broadly test SpatialVLA as a generalist robot policy, we examine the model's abilities in zero-shot in-distribution robot control and new robot setup adaption abilities with instruction following, 3D scene structure understanding, and fine-tuning to new robot environments. The evaluation setups include view/texture/lighting change, unseen objects, unseen robot environment, and challenging spatial layout changes in robot setups and environments, demonstrating remarkable generalizability and transferability of SpatialVLA with spatial-aware representations. In summary, the contributions of this work consist of a novel generalist robot policy that explores spatial representations for robot foundation models, sophisticated designs on Ego3D Position Encoding and Adaptive Action Grids for effective 3D-awareness injection, and superior evaluation results across various robot setups and tasks."}, {"title": "II. RELATED WORK", "content": "Generalist Robot Polices. Recent advances in robotics have witnessed a trend towards developing multi-task \"generalist\" robot policies to perform diverse tasks, rather than one specific task. Some early works [47, 53, 20, 6, 57, 70, 21] achieve great success in learning a language-conditioned visual multi-task policy on a single embodiment with pre-trained visual/text encoder, thereby lacking the ability to adapt new robot embodiment. More recent efforts [46, 38, 60] explore to use large-scale, cross-embodiment robot datasets [13] for generalist polices pre-training, supporting effective fine-tuning to new robot setups. Notably, Octo [46] proposes a flexible transformer-based architecture to unify different configurations in Open X-Embodiment (OXE) dataset [13], and the trained policy can solve a variety of in-domain tasks in zero-shot and achieves strong performance in the new embodiment after fine-tuning. With the same cross-embodiment robot datasets, RDT [38] pre-trains a 1.2B-parameter diffusion-based generalist model and fine-tunes it for complex bimanual manipulation. Moreover, HPT [60] proposes a modular architecture to align data across heterogeneous embodiments into a shared representation via embodiment-specific stem module, embracing the heterogeneity in data through pre-training.\nVision-Language-Action Models. Recently, several studies [33, 7, 29, 30, 65, 32, 64, 49] propose to build generalist robot policies by extending pre-trained VLMs with ability to robot action generation. As a pioneer, RT-2 [7] fine-tune VLM PaLI-X [11] on both large-scale vision-language data and robot demonstration data via autoregressive next token prediction, where robot actions are discretized into 256 bins and represented as separate tokens analogous to text tokens. OpenVLA [29] adopts a similar action discretization approach and fine-tune Prismatic VLM [27] only on the OXE dataset [13], which consists of robot data from 22 different robot embodiments across 21 institutions. CogACT [30] and TraceVLA [65] continue to fine-tune the trained OpenVLA model with the new attached diffusion action module and visual trace prompting separately. Moreover, $\\pi_0$ [5] adapts PaliGemma VLM to robot control by adding a separate action expert module that produces continuous actions via flow matching, and the model can then be prompted for zero-shot control or fine-tuned on high-quality data to enable complex"}, {"title": "III. METHODOLOGY", "content": "In this section, we describe SpatialVLA model and its training framework in detail. Our SpatialVLA model with the proposed Ego3D position encoding and adaptive action grids to capture and learn 3D spatial knowledge for generalizable robot control, which we describe in Section. III-A. Next, we detail the training procedure of SpatialVLA that consists of a pre-training stage and a post-training stage in Section. III-B. The pre-training aims to learn generalizable knowledge with large-scale cross-robot data and the goal of post-training is to adapt pre-trained model to specific downstream robot embodiments and tasks.\nAs illustrated in Figure. 2, SpatialVLA is developed based on a vision-language model to inherit the general world knowledge. Formally, SpatialVLA takes image observations $o_t = \\{I_t^1, .., I_t^k\\}$ and a natural language task instruction $L$ as inputs, and then learns a mapping function $\\tau(\u00b7)$ to generate a sequence of robot actions $A_t = [a_t, a_{t+1}, ..., a_{t+H-1}]$, i.e., $A_t = F(o_t, L)$. To empower SpatialVLA with 3D spatial intelligence, we augment the VLM backbone with robotics-specific 3D-aware inputs and outputs, namely, Ego3D Position Encoding and Adaptive Action Grids. The ego3D position encoding representation $O_{3d}$ aims to capture 3D scene structure via integrating 3D spatial information with 2D semantic features. The adaptive action grids are designed to represent the continuous distribution of robot actions a with a set of discrete spatial action tokens $a = \\{a^1, ..., a^V\\}$. During training, SpatialVLA model is trained to take the ego3D position encoding representation $O_{3d}$ and natural language task instruction $L$ as inputs, and autoregressively generate spatial action tokens $a_t$ using the cross-entropy objective $\\mathcal{L}$,\n$\\mathcal{L}(\\theta) = \\mathbb{E}_{p(A_t|o_t, L)} \\mathcal{L}(a_t, \\tilde{a}_t)),$ (1)\nwhere the predicted action tokens $\\tilde{a}_t = \\tau(O_{3d}, L)$ is the de-tokenized into continuous action signals $a_t$ for robot control. More details of the model architecture and action encoding can be found in Appendix. B.\nEgo3D Position Encoding. The proposed Ego3D position encoding integrates depth information from the camera frame and image pixels to construct an egocentric 3D coordinate system, which eliminates the need for robot-camera extrinsic calibration and is agnostic to specific robot setups. Specifically, we use ZoeDepth [4] to estimate depth map D and obtain pixel's 3D position $p = \\{x,y,z\\}$ in the egocentric 3D coordinate system via back-projection $\\pi^{-1}$ with camera intrinsic parameters. Then, as illustrated in Figure. 2, we first employ SigLIP [62] visual encoder to extract 2D semantic visual features"}, {"title": "A. The SpatialVLA Model Architecture", "content": "dexterous manipulation tasks. Notably, while these models benefit from VLMs' capabilities and show some zero-shot capabilities, a sophisticated fine-tuning step with new data is essential and required for complex tasks or new robot setups.\n3D Foundation Models for Robotics. Some researches [67, 10, 19, 23, 24, 63] have focused on extending the generalist ability of LLMs and VLMs from language-vision towards the 3D world. 3D-LLM [23] integrates a 3D feature extractor with 2D VLMs backbone and train 3D-LLMs on a wide variety of tasks, including dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. LLaVA-3D [10] extends the 2D LLaVA's capabilities with the proposed 3D patches to bridge 2D features within a 3D space for 3D spatial understanding. Similarly, LEO [24] trains an embodied multi-modal generalist agent that can take egocentric 2D images, 3D point clouds, and texts as task input and handle comprehensive tasks within the 3D environment. Moreover, 3D-VLA [63] builds a generative world model on top of 3D-based LLM to perform 3D reasoning and localization, multimodal goal generation, and embodied action planning. LEO and 3D-VLA are closely related to our work, but their attention is on 3D world understanding and prediction, ignoring the 3D spatial characteristic in the robot action space."}, {"title": "III. METHODOLOGY", "content": "features $X \\in \\mathbb{R}^{d \\times h \\times w}$ to inherit the alignment between vision and language, and calculate their corresponding 3D positions $P \\in \\mathbb{R}^{3 \\times h \\times w}$ in the egocentric 3D coordinate system. The egocentric 3D positions P are then encoded into 3D position embeddings $P' \\in \\mathbb{R}^{d \\times h \\times w}$ through a sinusoidal function $\\varphi(\u00b7)$ following by a learnable MLP. The egocentric 3D spatial representations $O_{3d} \\in \\mathbb{R}^{d \\times h \\times w}$ are obtained by adding 3D position embedding P' and 2D path visual tokens X, depicted as follows,\n$O_{3d} = X + P' = X + MLP(\\varphi(P)).$ (2)\nAdaptive Action Grids. In order to auto-regressively generate continuous robot actions with pre-trained VLM backbone, we design Adaptive Action Grids to translate continuous robot actions to discrete grids that are represented as tokenized classes for prediction. Specifically, for a single-arm robot, its actions consist of seven dimensions for movement $a = \\{x, y, z, roll, pitch, yaw, grip\\}$, and are split into three parts as follows,\n$a = \\{a_{trans}, a_{rot}, a_{grip}\\},$ (3)\nwhere $a_{trans} = \\{x, y, z\\}$ represents translation movements $\\Delta T$, $a_{rot} = \\{roll, pitch, yaw\\}$ denotes rotation movements $\\Delta R$, and $a_{grip} = \\{grip\\}$ consists of two discrete tokens that represent opening and closing gripper actions. Moreover, we transform the translation movements $(x, y, z)$ into polar coordinates $(\\phi, \\theta, r)$ to disentangle movement direction $(\\phi, \\theta)$ and distance r.\nAs illustrated in Figure. 3, for tokenizing continuous translation and rotation movements, we first normalize each action variable into [-1,1] for each robot environment and statistic the translation and rotation movements $\\Delta R = {roll, pitch, yaw\\}$, $\\Delta T = {\\phi, \\theta, r\\}$ on the whole dataset mixture (see Appendix. F), following with a parameterized Gaussian distribution fitting $\\mathcal{N}(\\mu_a, \\Sigma_a)$. Then, the continuous actions are split into M intervals $G_{i=1,..,M} = \\{[a_1 = -1, a_2), ..., [a_M, a_M = 1]\\}$ with equal probability 1/M on each normalized action variable, i.e.,\n$a_2, ..., a_M = arg \\min_{\\{a_i\\}} \\int_{a_i}^{a_{i+1}} f(x)dx - 1/M, i = 1, ..., M$ (4)\nwhere $f(x)$ is the probability density function of Gaussian distribution $\\mathcal{N}(\\mu_a, \\Sigma_a)$. Note that we split more grids on ${\\phi, \\theta\\}$ to capture fine-grained movement direction other than movement distance r. Suppose $M_{\\phi}, M_{\\theta}, M_r$ are the numbers of the discrete bins on variable $(\\phi, \\theta, r)$, then the translation space consists of $M_{trans} = M_{\\phi}.M_{\\theta}.M_r$ discrete spatial grids $a_{trans} = \\{a^1, ..., a^{M_{trans}}\\}$. Similarly, there are $M_{rot} = M_{roll}.M_{pitch}.M_{yaw}$ 3D discrete grids $a_{rot} = \\{a^1, ..., a^{M_{rot}}\\}$ in rotation 3D spatial space. Then, the associated learnable spatial action token embeddings are defined as follows,\n$E_a = \\{E_{trans}, E_{rot}, E_{grip}\\},$ (5)\nwhere $E_{trans} \\in \\mathbb{R}^{d \\times M_{trans}}$, $E_{rot} \\in \\mathbb{R}^{d \\times M_{rot}}$, $E_{grip} \\in \\mathbb{R}^{d \\times 2}$ denote the translation, rotation, and gripper actions, and the total number of action tokens is $V = M_{trans} + M_{rot} + 2$. After training, these learned spatial action tokens capture general robot action knowledge and show a surprising ability in new robot embodiment adaption, as discussed in Section. III-B. Moreover, it is worth noting that the model only needs to generate 3 tokens for one-step robot actions rather than 7 tokens as in RT-1 [6], RT-2 [7] and OpenVLA [29], achieving in fast model inference speed.\nB. The Pre-training and Post-training Scheme\nTo obtain a generalist robot policy model, the training procedure of SpatialVLA consists of pre-training stage and post-training stage. Pre-training stage aims to learn generalizable knowledge across diverse tasks and robots from a large-scale dataset mixture, while the post-training stage adapts the pre-trained model into new robot embodiments or new tasks. In the following, we discuss the dataset mixture and key designs for implementing this two-stage training procedure.\nPre-training Procedure. We train SpatialVLA from Paligemma2 backbone [58] on a cross-robot dataset mixture with 1.1 Million real robot demonstrations {$\\xi_{1},..., \\xi_{n}$}, covering a diverse range of robot embodiments, scenes, and tasks. This pre-training dataset mixture consists of a subset of OXE [13] and the RH20T dataset [18] and we modify the mixture weights from OpenVLA [29] according to the real-word testing performance of individual dataset, which are exhibited in Appendix. A. At the beginning of pre-training, the embeddings $E_a$ of spatial action tokens and parameters of MLP in egocentric 3D spatial representation are randomly initialized, and then they are optimized during training, as well as the parameters of vision encoder and LLM backbone. At each training step, a batch of data pairs is extracted at random timesteps $t_{1},...,t_{p}$ from shuffled demonstrations {$\\xi_{i}, ..., \\xi_{j}$},"}, {"title": "IV. EXPERIMENT", "content": "Formally, for new spatial action grids $G^{new}$, suppose i-th 3D grid $G^{new}_{i}$ in translation space areas with centroid $(\\tilde{a}_x^{new}, \\tilde{a}_y^{new}, \\tilde{a}_z^{new})$ and its adjacent 3D grids from the pre-trained action grids are $G_{adj} = \\{G_1, ..., G_K \\}$. The embedding of new i-th action token $e^{new}_{i}$ are initialized by trilinear interpolation with $G_{adj}$, as follows,\n$e^{new}_{i} = \\sum^{K}_{j=1} w_j e_a^{j},$ (6)\nwhere $e_a^j \\in \\mathbb{R}^d$ are the embeddings of the pre-trained action grids, $w_j$ is the weights calculated by the normalized distances between centroid $(\\tilde{a}_x^{new}, \\tilde{a}_y^{new}, \\tilde{a}_z^{new})$ and adjacent centroids. Note that the new action tokens of rotation $E^{new}_{rot}$ are initialized in the same way. With this embedding initialization, the new action tokenizer is capable of effectively transferring pre-trained spatial action knowledge to new robot setups.\nThe goal of our experimental evaluations is to test SpatialVLA's ability to serve as a generalist robot control policy out of the box, as well as be a good initialization for fine-tuning to new robot tasks. Our extensive experiments consist of zero-shot evaluations and adaption to downstream tasks in both simulation and real-world. SpatialVLA is compared to previous state-of-the-art robot foundation models and alternative designs in spatial representations. Concretely, experiments seek to answer the following research questions:\n1) How well does SpatialVLA directly perform on a variety of in-distribution tasks after pre-training on large-scale robotic data mixture?\n2) Can SpatialVLA be effectively fine-tuned on new robot setup and task?\n3) How well does SpatialVLA perform in scenarios that require spatial understanding?\n4) To what extent do Egocentric 3D Spatial Representations and Adaptive Spatial Action Grids improve the performance of SpatialVLA?\nTo answer these questions, as shown in Figure. 4, we evaluate SpatialVLA's capabilities across a representative spectrum of 7 different robot learning scenarios with 24 real-robot tasks and 3 simulation environments. Firstly, we evaluate SpatialVLA in both SimplerEnv [34] simulation and the real-world WidowX robot platform (BridgeV2 [59] [59] setups), testing its out-of-the-box control capabilities on different robots with setups matching the pre-training dataset. Second, we assess the fine-tuning efficacy of our method in both simulation and real-world settings, including LIBERO [35] and new Franka robot setups, to adapt to new robot environments and tasks. Then, we design 4 special tasks that require precise spatial understanding in 2 different real-world robot environments to test the effectiveness of spatial representations of SpatialVLA. Finally, we conduct comprehensive ablation studies on a mixture of Fractal [6] and BridgeV2 [59] datasets to verify our design decisions in SpatialVLA. For more details on evaluation setups, see Appendix. D.\nImplementation Details. The SpatialVLA model is pre-trained with 1.1 Million real-robot demonstrations from the OXE [13] and RH20T dataset [18] on a cluster of 64 A100 GPUs for 10 days, using a batch size of 2048. For input robot observation, the SpatialVLA policy is only conditioned on one third-person camera and takes one image for constructing egocentric 3D spatial representations. For output robot actions, the SpatialVLA policy predicts a chunk of T = 4 future actions (12 spatial action tokens from total V = 8194 tokens) and executes the ensemble actions before predicting the next chunk. During inference, SpatialVLA requires 8.5GB of GPU memory and runs at approximately 20Hz on one NVIDIA RTX 4090 GPU to run evaluations in both simulation and real-world. For more details about model training and deployment, please refer to the Appendix. C.\nA. Performing Zero-shot Robot Control\nEvaluation Setups and Baselines. To assess the robustness of SpatialVLA in diverse environmental variations, we employ the SimplerEnv simulation benchmark [34] to evaluate visual matching and variant aggregation metrics. SimplerEnv features WidowX and Google Robot setups, providing diverse manipulation scenarios with varied lighting, color, textures, and robot camera pose conditions, bridging the visual appearance gap between real and simulated environments. We compare our model with the latest state-of-the-art generalist manipulation policies, including RT-1 [6], RT-1-X [13], RT-2-X [13], Octo [46], OpenVLA [29], HPT [60], TraceVLA [65], and"}, {"title": "V. DISCUSSION, LIMITATIONS, AND FUTURE WORK", "content": "In this paper, we present SpatialVLA, an innovative vision-language-action model to explore efficient spatial representations for generalist robot policy. SpatialVLA introduces Ego3D position encoding and adaptive action grids to inject 3D awareness into robot observation representation and spatial action tokenization through robot-agnostic designs, equipping the VLA models with the spatial understanding ability of the 3D physical world. After pre-training on large-scale heterogeneous robot datasets, we find that SpatialVLA is a more generalizable and transferrable generalist policy for zero-shot robot control. Our extensive real-world and simulated robot experiments show that SpatialVLA leads to dramatically improved performance over the previous VLA models, especially on tasks that require precise spatial understanding. We also show that the pre-trained SpatialVLA model can effectively adapt to new robot setups and tasks via action grids re-discretization, which offers a new way for robot-specific post-training. In the following, we discuss our limitations of SpatialVLA and potential solutions, hoping to inspire further innovative works.\nMore Generalizable Distribution Fitting. In this paper, SpatialVLA fits action signals with Gaussian distributions to encode actions as spatial grids, demonstrating remarkable generalizability and flexible adaptation to new robot setups through re-initialized grids and token embeddings. However, this raises a crucial question: Is modeling data distributions as Gaussian optimal? We argue that Gaussian modeling is suboptimal, as it can lead to grid clustering on specific coordinate axes in extreme robot operation scenarios, such as single-axis motion, resulting in lost motion capabilities on other axes. Moreover, dataset noises can further distort the spatial grid distribution. One future solution is to combine implicit data distribution modeling techniques, such as Variational Auto-Encoder-based high-dimensional feature space mapping, with explicit grid partitioning, enhancing action presentation efficiency and noise robustness.\nMore Flexible VLA architectures. In our implementation, we predict spatial action tokens through the autoregressive paradigm and further decode them into actions, resulting in each action being represented by 3 tokens. Although SpatialVLA achieves 21Hz inference speed, it is slower than diffusion decoding [5, 12, 31], which decodes tokens into multiple consecutive actions. In the future, integrating diffusion decoding with spatial grid action presentation and exploring dynamic token numbers for action mapping will be valuable. Furthermore, as the model relies solely on current frame observations and history tokens for action prediction, it faces challenges in long-horizon tasks, similar to other generalizable policies [12, 46]. Future work should focus on designing efficient historical information perception mechanisms to enhance the model's long-sequence modeling capabilities, enabling seamless task switching in real-time manipulation scenarios.\nHigher-Quality Diverse Data. SpatialVLA is pre-trained on OXE and RH20T, but the variable quality of OXE data can hinder training. Therefore, future work exploring optimal data composition and distilling high-quality subsets from the heterogeneous robot data collections is vital for boosting model efficiency and generalizability."}]}