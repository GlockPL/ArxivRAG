{"title": "OVER THE EDGE OF CHAOS? EXCESS COMPLEXITY AS A ROADBLOCK TO ARTIFICIAL GENERAL INTELLIGENCE", "authors": ["Teo Susnjak", "Timothy R. McIntosh", "Andre L. C. Barczak", "Napoleon H. Reyes", "Tong Liu", "Paul Watters", "Malka N. Halgamuge"], "abstract": "In this study, we explored the progression trajectories of artificial intelligence (AI) systems through the lens of complexity theory. We challenged the conventional linear and exponential projections of AI advancement toward Artificial General Intelligence (AGI) underpinned by transformer-based architectures, and posited the existence of critical points, akin to phase transitions in complex systems, where AI performance might plateau or regress into instability upon exceeding a critical complexity threshold. We employed agent-based modelling (ABM) to simulate hypothetical scenarios of AI systems' evolution under specific assumptions, using benchmark performance as a proxy for capability and complexity. Our simulations demonstrated how increasing the complexity of the AI system could exceed an upper criticality threshold, leading to unpredictable performance behaviours. Additionally, we developed a practical methodology for detecting these critical thresholds using simulation data and stochastic gradient descent to fine-tune detection thresholds. This research offers a novel perspective on AI advancement that has a particular relevance to Large Language Models (LLMs), emphasising the need for a tempered approach to extrapolating AI's growth potential and underscoring the importance of developing more robust and comprehensive AI performance benchmarks.", "sections": [{"title": "1 Introduction", "content": "In the current landscape of artificial intelligence (AI) research, especially with the recent advancements in Large Language Models (LLMs), there is a growing discourse about the future direction and ultimate potential of AI capabilities [1, 2, 3, 4, 5, 6, 7]. LLMs are breaking AI benchmarks at ever-increasing rates [8] with each new iteration and release. The discussions in academic literature have started to shift towards a more optimistic outlook on the possibility of the eventual emergence of Artificial General Intelligence (AGI), where the hypothetical intelligence of a machine at least attains the capacity to understand, learn, and apply its knowledge in a broad range of domains much like the cognitive abilities of a human [9, 1, 4, 7, 10, 11], or even exceed it in the form of superintelligence [12]. This positive sentiment has been fueled by the rapid advancements and demonstrable abilities of generative AI technologies designed to emulate human-like responses and creativity [13, 14], developed primarily using transformer-based architectures [15]. Not only have large corporations at the forefront of developing these technologies started to take AGI more seriously, but much of popular culture has been captured by the idea of AGI inevitability, with many experienced and eminent researchers recently becoming supporters of this possibility [2, 13]. Notably, leading AI-innovating organisations such as OpenAI, Meta, and DeepMind remain optimistic about the prospect of achieving AGI, despite the lack of consensus on its precise definition [6] and the paths to its realisation, reflecting a landscape where the goalpost for AGI may continually shift for the foreseeable future [4].\nHowever, humans have the tendency to anthropomorphise and attribute greater intelligence to AI technologies based on their linguistic output, where this can be mistaken for deep and genuine understanding [13]. When coupled with the systemic cognitive biases that humans possess when attempting to extrapolate exponential growth patterns [16] as seen in recent AI capabilities, it cannot be entirely surprising that the future trajectory of AI capabilities is increasingly directed towards AGI. Should these forecasts prove to be accurate, a reasonable expectation is that the AI agents' intelligence and reasoning capabilities should be accompanied by a commensurate increase in their internal complexity.\nAgainst this backdrop, complexity theory offers a framework for evaluating the optimism surrounding AI's growth trajectory. This theoretical perspective provides a lens for describing emergent behaviours and properties inherent in complex cognitive architectures [17], and additionally, it lends the concept of criticality that denotes a state which systems may realise as they accrue significant complexity. The principle of criticality in this context refers to a threshold beyond which a system's coherence and functional integrity are compromised, potentially leading to systemic instability or functional collapse. Complexity theory thus provides a mechanism for hypothetically exploring these phenomena because it uniquely accommodates the nonlinear interactions and feedback loops that are typical in AI development. Therefore, if we are to accept the assumption that the current trends of AI advancement will continue, this study posits a hypothesis that contends with the narrative of inevitable and indefinite AI performance expansion. Our work suggests that AI systems, particularly those embodying current LLM architectures and likely those that immediately follow, may inherently possess a ceiling to their complexity and, by extension, their cognitive capabilities. This hypothesis draws parallels with biological systems, notably the human brain, which achieves remarkable complexity underpinned by sophisticated regulatory mechanisms that manage and mitigate the risks associated with excessive complexity and the speed at which the complexity is acquired [18]. The absence of analogous regulatory mechanisms in AI systems may predispose them to a state of excess criticality that is characterised by diminished coherence and ultimately, functionality.\nContributions\nThe key contributions of this study have been as follows:\n1. This study investigated the theoretical foundations of criticality in complex AI systems and developed theoretical models and empirical analyses for simulating AI behaviour at and beyond critical thresholds.\n2. We proposed a novel framework for detecting the onset of excess criticality in real-world AI systems via benchmark performance metrics.\n3. Our research has hypothesised and provided evidence for inherent limits to the complexity and capabilities of Al systems given current ANN architectures.\n4. We presented a rich literature review and a balanced perspective on future AI development and trajectories."}, {"title": "2 Background", "content": ""}, {"title": "2.1 The case for achievability of AGI", "content": "The anticipation of AGI becoming a reality is generally based on several technological, theoretical and philosophical grounds. Frequently it is justified based on substantive technological and theoretical AI advancements made over time, with notable recent accelerations [7]. This includes significant progress afforded by deep learning ANN architectures producing systems routinely outperforming human abilities. These have ranged from AI systems that defeat world champions in chess and Go, as well as those outperforming humans in medical diagnoses, through to others possessing creative and generative abilities in music and art, indistinguishable from human-created works [7]. Included also are advancements in unsupervised and reinforcement learning that enable AI systems to acquire knowledge more autonomously and refine their decision-making and planning in dynamic environments which mirrors key aspects of human cognitive adaptability [19]. Researchers [19] have shown how such systems can display compelling properties involving planning and predicting future outcomes with glimpses of being able to build and reason over real-world models. In the context of LLMs, some eminent scientists [14] contend that they also implicitly develop comprehensive world models representing knowledge, human experience, and subjective realities through the prediction of textual patterns in large-scale datasets, a claim which has been confirmed by other notable studies [20] indicating the emergence of world representations during the training processes. Recent works [21] have indicated that LLMs can perform spatial reasoning by emulating the human capability to create mental images of unseen objects through a process known as \"the Mind's Eye\" which can be elicited through prompting strategies. The demonstration of the Theory of Mind (ToM), previously considered exclusive to humans has recently [22] been suggested as an emergent behaviour within LLMs, where ToM is the cognitive ability to perceive and ascribe mental states to oneself and others. The study claimed that a ToM equivalent of a six-year-old child may have spontaneously emerged merely as a byproduct of LLMs' acquiring human language abilities. The recent progress in generative AI's capabilities to encompass multimodal capabilities across text, audio, image and video channels has pointed towards a further milestone suggesting the incorporation of yet another fundamental trait of human intelligence [5]. Finally, the exponential growth in computational power and the development of tailored AI-specific hardware such as GPUs and TPUs, has suggested that the infrastructure needed for AGI is now becoming available as well as the belief that AI systems will be able to achieve incremental autonomous improvement [23, 5]. For many, these collective advancements not only demonstrate AI's growing proficiency across a range of cognitive tasks [24] and their increasing potential to achieve even more, but they also serve as signposts and a confirmation of the plausibility of achieving AGI in the foreseeable future [25]."}, {"title": "2.2 Criticality in Neuroscience and AI", "content": "Neuroscientific research has increasingly leaned on complexity theory and concepts to enhance our understanding of brain function. The study of complexity in various scientific fields focuses on understanding how interactions among numerous elements in dynamic systems lead to different outcomes. In physics, chemistry, biology, and ecology, researchers investigate how the interactions of many components, such as atoms, biomolecules, cells, organisms, and populations, result in the emergence of organized structures and patterns [26]. In neuroscience, the human brain routinely undergoes phase transitions that shift neuronal activity from one pattern to another, such as transitions from wakefulness to sleep or from normal to hyperactive states observed in seizures. These transitions occur when criticality is reached as a result of internal or external stimuli. These state transitions are said to alter when a control parameter such as neuronal excitation or inhibition cross a critical point[27]. A prevalent hypothesis suggests that the brain functions optimally at the point of criticality that is defined as a boundary between order and disorder\u2014frequently referred to as the \"edge of chaos\u201d[28, 27]. This critical state is considered ideal for cognitive functions, as it allows the brain to maintain stability while maximizing adaptability to new information. Operating at this edge, the brain can optimise its computational power by balancing between excessive regularity, which restricts adaptability, and excessive randomness, which can compromise coherence[27], highlighting the dual nature of complexity in producing both order and chaos [26].\nSimilar to neuroscience, ANNs are also held to operate at the edge of chaos, likewise characterised by the network's ability to maintain an optimal balance between rigidity (order) and randomness (chaos), where ANNs achieve maximum computational efficiency and robustness [29]. Empirical studies suggest that ANNs trained and tuned to operate near this critical point exhibit superior performance, especially in dynamic and non-trivial environments[30], enabling them to optimise memory retention, responsiveness to inputs, and generalisation across new data which is particularly evident in models such as the recently developed Liquid Neural Networks (LNNs) [31], where they have been shown to utilise continuous-time recurrent neural networks with dynamic parameters, allowing them to adapt more effectively to time-varying inputs and maintain criticality over extended periods. In contrast, [30] demonstrated that features learned in the chaotic phase that exceeds criticality, are not stable patterns, leading to a reduction in information processing capability, which renders the models unable to generalise and makes them fragile against input perturbations. This degradation in ANNs is analogous to biological organisms where deviations from criticality in the brain, either through its loss or excess, impair functionality and result in pathological conditions such as epilepsy and altered physiological states [32].\nHowever, whereas in nature neurobiological systems optimize their interactions with the environment by modifying the morphology of synaptic connections in response to past experiences [33] and thereby continue to learn, ANNs are trained to operate at the edge of chaos with their weights generally remaining fixed thereafter, which limits their ability to incorporate new knowledge [34]. Developing AI systems that can learn from their experiences and adapt to continuously changing environments is one of the primary challenges [33]. While recent research has shown that transformer-based architectures can exhibit ongoing learning capabilities via in-context learning which operates via network activations rather than synaptic plasticity to solve novel tasks [34], other recent works [33] have proposed ANN architectures that allow real-time parameter adjustments to facilitate continuous learning and adaptation. It is especially in these continuously adaptive systems that hold real potential for leaps in AI capabilities, that pose challenges for maintaining criticality where such adaptation may push the networks beyond the optimal critical point, leading to unstable behaviours that may not be easily detectable."}, {"title": "2.3 AI Performance Benchmarks", "content": "AI benchmarks are designed to test various performance capabilities in AI systems, and thus have the potential to detect the emergence of impaired functionality over time across different tasks. In the case of generative AI and specifically LLMS, AI benchmarks that go beyond testing natural language understanding and generation tend to include reasoning and logic benchmarks that measure problem-solving and multi-step and multi-hop reasoning abilities [35]. They also assess commonsense knowledge to evaluate understanding of everyday facts and relationships, as well as spatial and temporal reasoning, planning, and decision making [8], and creativity and imagination. Other benchmarks evaluate contextual understanding and coherence, and adaptation to new tasks, probing the model's ability to quickly learn and adapt to new domains with minimal data, analogous to few-shot or zero-shot learning capabilities [35]. The peculiar and extraordinary attribute in the instance of LLMs is that these models are only trained to predict the next word/token in a sequence of text, and their sophisticated capabilities in reasoning and logic are considered emergent properties. These capabilities are therefore not explicitly taught but arise from the complex interplay of the entire network, indicating transfer learning that allows neural networks to perform well across a wide range of tasks by leveraging shared representations and patterns learned during pretraining.\nWhen performing AI benchmarking, these emergent capabilities are assessed separately to understand how well the model performs in each area. This functional decomposition does not imply that the model has distinct parts dedicated to each capability within its neural network, but rather that it can be evaluated for each skill individually. Achieving uniform optimisation in performance across multiple tasks is a challenge, analogous to maintaining and operating at the point of criticality across each distinct functional capability. Divergences from the optimal point of criticality may not be immediately apparent during model training and may only become detectable via extensive benchmark evaluations. While current transformer-based architectures aim to operate at the edge of chaos, future ANN architectures may incorporate adaptability and plasticity for continuous learning from experiences. Thus, both the current static ANNS and the potentially adaptable ANNs present vulnerabilities to exceeding criticality that are worth exploring."}, {"title": "2.4 Literature summary", "content": "The literature review underscores the rapid advancements in AI capabilities with the expectation of further significant developments in ANN architectures. A key insight is the importance of maintaining criticality and operating at the edge of chaos in ANNs to optimize their performance and stability. As AI systems become more complex and potentially capable of adapting and exhibiting neuronal plasticity akin to biological systems, managing criticality will become increasingly challenging. Research does not currently exist with a focus on modelling and detecting this hypothetical state of excess criticality. To that end, this study seeks to answer to the following research questions:\n(RQ1) Is it possible to construct a simulation framework that simulates AI systems becoming more complex over time, allowing a study of their performance beyond the point of criticality?\n(RQ2) Is it possible to identify the emergence of instability in AI systems resulting from excessive criticality?"}, {"title": "3 Theoretical Framework", "content": "The proposed framework explores the dynamics governing the performance of AI systems, represented through a set of benchmarks, where we assume that each benchmark reflects different AI capabilities and by proxy, therefore the underlying complexity of an AI system. At the heart of this exploration is the study of possible behavioural patterns that may be observable in AI systems beyond a critical juncture where their accrued complexity precipitates a significant shift in operational dynamics, leading to unpredictable performance fluctuations. Under this assumption, the proposed framework will seek to both model hypothetical AI systems as they approach and exceed the criticality point, and will also attempt to detect if this has occurred. The list of symbols and their definitions for the subsequent section are shown in Table 1."}, {"title": "3.1 Formal Representation", "content": "We introduce n benchmarks (representing non-trivial capabilities), denoted by \\(P_i(t)\\), representing the performance of a single hypothetical AI system on the ith benchmark at time t, where the performance ranges from 0 \u2013 1.0. The aggregate performance metric C(t), reflecting the agent's overall capability and therefore its complexity, is computed as the weighted average\u00b9 of individual performances:"}, {"title": "3.2 Dynamical behaviour at Critical Thresholds", "content": "The system's evolution towards and beyond the critical threshold \\(C_{max}\\) is captured through the dynamics of \\(C_i(t)\\). This threshold signifies the point where operational dynamics transition from predictable improvements to stochastic variations due to increasing and unmanageable system complexity. While \\(C(t) < C_{max}\\), we model an increase :"}, {"title": "3.3 Criticality Detection", "content": "Once criticality has been exceeded, detecting the onset of this phase transition into a less stable and more unpredictable state becomes practically useful from an application perspective. The proposed detection approach involves analysing \\(D \u2208 \\mathbb{R}^{m\u00d7n}\\), which represents data on the performance metrics C(t) across n simulations over m time points. Since each simulation reaches a predefined criticality point at a different time, all data \\(D \u2208 \\mathbb{R}^{m\u00d7n}\\) is first aligned with respect to the critical point, and subsequently we employ a derivative-based analysis to monitor the rate of change in the standard deviation (SD) of these metrics (visually depicted in Figures 3(a-d)), which is explored for its potential to be indicative of critical transitions. We define S(t) as the SD of the performance metrics at time t for each benchmark across all simulations. The derivative of S(t), denoted as S'(t) therefore captures the dynamics of performance variability:"}, {"title": "3.4 Optimisation of Detection Threshold", "content": "To optimise the detection threshold \\(\u03b8^*\\), the framework uses Stochastic Gradient Descent (SGD) by iteratively adjusting it in response to the gradients of a loss function calculated from the criticality detection accuracy:"}, {"title": "3.5 Agent-Based Modelling", "content": "ABM is employed to simulate emergent behaviours within the hypothetical AI systems by conceptualising each performance benchmark as an autonomous agent ai, each encapsulating a distinct aspect of the system's capabilities. These agents are modelled to interact within a dynamic environment, collectively influencing the system's behaviour as it approaches or surpasses critical complexity thresholds."}, {"title": "3.5.1 Agent State Transition", "content": "Each agent's state, Pi(t), evolves based on a combination of internal decision-making processes and interactions with its environment:"}, {"title": "3.5.2 Inter-Agent Dynamics", "content": "The interaction dynamics among agents lead to emergent behaviours in the system, particularly as it relates to its stability and response to critical thresholds being reached and the extent to which they are exceeded:"}, {"title": "3.5.3 Emergent Behaviour and Criticality", "content": "The emergent behaviours observed in the modelled system, including transitions to critical states, arise from the interplay of the interactions captured by the aggregate performance metric C(t) (Equation 1), which is indicative of the overall system complexity and serves as an indicator of the proximity to critical thresholds. As system complexity increases, due to compounded interactions and the accumulation of minor changes in benchmark performances/behaviours, the system may exhibit sudden shifts in overall performance, manifesting excess criticality as an emergent property."}, {"title": "4 Methodology", "content": ""}, {"title": "4.1 Simulation Framework and Dynamics", "content": "We conducted 200 simulations across various system scales to explore AI system dynamics near and beyond critical complexity thresholds. We used 2, 5, 10, and 20 benchmarks represented as agents within the Python-based Mesa[36] framework. This agent-based modelling approach allowed for a detailed analysis of hypothetical emergent behaviours under different levels of complexity and stress with respect to the stated assumptions."}, {"title": "4.1.1 Agent Initialisation and Performance Dynamics", "content": "Each agent simulated a performance benchmark and was initialised with a performance level P\u00bf(0) randomly assigned between 0 and 0.7 to mirror the diverse capabilities seen in real AI systems, and each agent was assigned uniform weights. The performance evolution of agents was modelled through:"}, {"title": "4.2 Criticality Detection and Optimisation", "content": "Critical thresholds trigger systemic volatility, challenging the system's stability and performance. Criticality is algorithmically detected by assessing performance variability with thresholds optimised through SGD:"}, {"title": "4.3 Model Testing and Validation", "content": "The criticality detection thresholds were derived for each agent size configuration. Subsequently, the thresholds were validated and tested via cross-validation against 100 additional simulations for each of the four agent configurations to establish the robustness of the criticality detection mechanism\u00b2. A simple measure of accuracy in the form of the percentage of criticality events detected within a predefined time window was used. The time window denoting a correct detection was defined as 10 time steps post actual criticality which is depicted in Figures 4(a-d). Accuracies for training and test simulation data were repeated 20 times each and reported as a mean, together with the standard deviations."}, {"title": "5 Results", "content": "The results section is presented in two parts, where the first presents the trends and dynamics of the modelled systems at the point of criticality and beyond; while the second part examines how the onset of excess criticality can hypothetically be detected in real-world scenarios."}, {"title": "5.1 System dynamics at criticality", "content": "The convergence of AI systems toward the threshold of criticality, depicted in Figure 1(a-d), illustrates the mean performances over time for varying benchmark sizes across 100 simulations. These simulations have been synchronised at the criticality point, providing a coherent baseline from which to observe the systems' behaviour. A clear demarcation is indicated by the criticality threshold as a horizontal red dashed line, with the criticality point marked by a vertical line. Notably, the transition into criticality is marked by increased fluctuations in performance, suggesting the emergence of complex system dynamics. The AI systems with fewer benchmarks demonstrate pronounced instability after reaching and exceeding criticality, as seen in the widening spread of the grey lines, reflecting a transition from predictable growth to a stochastic regime. For systems with a larger number of benchmarks, there is a notable decrease in the performance variance as the number of benchmarks increases. The mean performance line, depicted in black, remains relatively steady before the criticality threshold, indicating ordered growth. However, the increase in performance variability post-criticality suggests that this ordered state gives way to more erratic behaviour, where the system's performance is influenced by complex interactions and inherent noise within the AI system."}, {"title": "5.2 Detecting criticality", "content": "The following sequence of plots in Figure 3(a-d) depict test dataset results from a new set of 100 simulations. The figures show the derivative value calculated over the standard deviations of all benchmark performances expressed as an expanding window from the initial time point. The mean of all derivatives is depicted to highlight the aggregate trend. The figures also depict both the aligned critical point for all the next simulation experiments as well as the derived threshold calculated from the previous training dataset of simulations for determining the optimal decision threshold. This threshold derived from the training dataset would then be evaluated for identifying criticality based on the derivative of the standard deviations of performances.\nAll figures indicate that immediately following the entry into the criticality phase, a pronounced change in the slope of the derivative becomes observable in most cases. The figures also show that the optimal threshold derived for detecting criticality from the training data visually corresponds well with the test data. Since detecting a change in the slope of the instability in the performances requires several time steps to transpire and will vary to some degree between different simulations, it remains to be demonstrated how soon post-criticality, this transition can be detected. For the purposes of these experiments, we selected any detection that falls immediately within 10-time steps post-criticality to be a correct positive detection. Based on this criterion, Table 2 shows the percentage of correct classifications that were achieved on both the training and test datasets across all benchmark sizes. It is evident that as the number of benchmarks increases, the detection of variability becomes more reliable and the difference between the training and test errors also decreases.\nThe histograms in Figure 4 depict the distribution of the criticality detection times in a new set of 100 simulations, represented as test datasets. These simulations utilise the threshold derived from the training dataset to determine the optimal decision threshold to detect criticality. The shaded region in each subplot illustrates the window of time considered for accurate detection, extending 10 steps beyond the identified criticality point. It is evident from the histograms that most of the detections occur within this acceptable window, signifying a reliable threshold. As the number of benchmarks increases from 2 to 20, there is a noticeable improvement in the accuracy of criticality detection, with the distribution of detection times becoming increasingly concentrated within the defined window. This observation underscores the value of relying on a diverse set of benchmarks in enhancing the ability to detect and adapt to the onset of criticality. The consistent alignment of the derived threshold with the criticality point across all test datasets confirms the robustness of the approach used to establish this threshold. These results suggest that the method used for criticality detection is effective and could potentially be applied to real-world scenarios where early identification of critical states is crucial."}, {"title": "6 Discussion", "content": "This paper demonstrates an approach to constructing a simulation framework modeling AI systems becoming more complex over time, enabling a study of their possible performance patterns beyond criticality (RQ1). The utility of this framework relies on several key assumptions, such as the direct relationship between benchmark performance and system complexity, and the nature of criticality in AI systems. Given the assumptions, this paper also shows that it is possible to detect the emergence of instability in AI systems resulting from excessive criticality through a proposed method that leverages patters in the rate of change in performance metrics of AI systems across various benchmarks (RQ2). The next logical question is whether the newest versions of transformer-based AI systems, like large language models (LLMs), despite their growing capabilities and remarkable performance, represent true growth in intelligence with corresponding complexity that could result in criticality. Are they likely to keep increasing in complexity past a critical limit post-training, or will their current performance plateau within the limitations of present artificial neural network (ANN) architectures?"}, {"title": "6.1 Lack of real reasoning, planning and world model constructions", "content": "Recent studies [8, 37] demonstrate that current LLM models struggle with complex reasoning tasks that humans can perform relatively easily and fail to display a deep understanding, relying instead on surface-level pattern recognition or exploiting dataset biases. This observation stems from the inherent architectural constraint where the computational effort for generating responses is not dynamically allocated based on the complexity of the task but is rather fixed per token generated. This mechanism starkly contrasts with human cognitive effort, which scales with task complexity. This phenomenon can be linked to the dichotomy between System One and System Two-type thinking [38] which provides a lens for understanding the disparity between human reasoning and the current capabilities of LLMs. System One encompasses fast, automatic, and often subconscious thought processes, akin to what LLMs might simulate through immediate, pattern-based responses, while System Two, involves slower, more deliberate, and effortful reasoning processes, requiring cognitive resources that LLMs are architecturally not designed to emulate. Human reasoning involves an interplay between these systems, where System Two can override or modulate responses generated by System One, meanwhile, this adaptive allocation of cognitive resources is a sign of increasing degrees of true complexity via the ability to engage in deep, reflective thought is absent in LLMs as they currently stand. A key feature of a genuine ability to reason over non-trivial tasks is to exhibit planning capability. Again, recent works [39, 37] have also contended that current LLMs are restricted to System One tasks that are intuitive and reflexive, rather than tasks that require logic and deliberate analysis involved the System Two level. Others [40] have categorically claimed that LLMs cannot reason nor plan, but can only give an impression of this capability. Meanwhile, other recent works [41, 42] have even more strongly contended that the impressive generative ability of LLM-based agents may not at all be a reflection of their understanding capability but is merely a function word prediction. Indeed, current AI systems lack a crucial aspect of human intelligence which includes rich internal models of the world from which, causal inferences, simulations and predictions of events can be made, together with both reasoning and planning in new situations and a continuous update of our knowledge based on experiences [14]. It remains a subject of debate if LLMs can effectively learn an internal world model from ungrounded form alone as found in text data, without direct sensory input or interaction with the physical world [21]. For these reasons, it seems more probable that current LLM-based AI systems will likely plateau in their performance rather than accrue excess complexity that crosses beyond criticality."}, {"title": "6.2 Inefficiencies of inductive data-driven learning", "content": "The limitations inherent in the simplistic objective functions used to train transformer-based LLMs suggest that we are not yet engaging with truly complex systems capable of deep world perception and understanding [14]. Human-level intelligence involves a dynamic interplay of cognitive processes, including abstract reasoning, problem-solving, understanding causal relationships and the ability to learn from minimal data. The objective functions used by LLMs, designed to minimise a narrowly focused loss, inadvertently promote shortcut learning, where LLMs learn to exploit statistical regularities and dataset biases to achieve high performance on tasks without a genuine grasp of the underlying phenomena [13]. This form of learning which is characterised by taking the path of least resistance towards loss minimisation, leads to models that do the right things for the wrong reasons, revealing a superficial understanding while maintaining a substantive capability. Studies [43] show that within the domain of logical reasoning, trained models regularly use incorrect reasoning to reach the correct final answers when trained to predict the right outcome. Furthermore, the necessity of prompt engineering in LLMs which is a practice absent in human communication, underscores the artificiality and limitations of these models. Unlike humans who comprehend and interact with the world through a rich tapestry of experiences and cognitive processes, LLMs currently rely on carefully crafted prompts to access and navigate their knowledge space with large variability in results from small differences in prompts which further suggests their lack of true intelligence and underlying complexity. In short, the current transformer-based AI training paradigms are essentially data-driven where they focus on pattern recognition and prediction rather than on real understanding [14].\nThere also exist other compelling reasons suggesting that while current LLMs (and their multi-modal AI extensions) represent a profound and undeniably significant leap forward with respect to AI capabilities, there are limiting factors that may result in current LLM agents eventually converging to performance plateaus. Firstly, LLMs are primarily trained on linguistic data, which itself is an artefact of human intelligence, reflecting only one dimension and thus a subset of human cognitive capabilities which is unable to encapsulate the entirety of human intellect or the complexities of real-world phenomena. Therefore, achieving human-like intelligence solely through this modality, in theory raises doubts [14] especially regarding the achievement of near-AGI or beyond to superintelligence. Our linguistic data resources are also finite. High-quality linguistic datasets necessary for extracting increased performances from LLMs are also decreasing since they have likely already been deployed for current model training purposes. Another important recent finding [44] examining the feasibility of such data-intensive inductive approaches to increasing capabilities in LLMs has revealed that model performance in understanding concepts improves linearly only when their frequency in the training data increases exponentially. This finding highlights potentially fatal inefficiencies in the data utilisation of LLMs trained by transformer-based architectures since the requirement of providing exponentially more data to achieve linear performance gains is not sustainable."}, {"title": "The need for robust benchmarks", "content": "Notwithstanding the current limitations of LLMs and indications of their architecture-defined complexity ceiling, there is a need to evolve the benchmarks for measuring their genuine intelligence and underlying complexity [13]. To truly gauge the sophistication of AI systems and their drift towards and beyond criticality, benchmarks need to become more diverse and robust, encapsulating the multi-dimensional facets of intelligence such as deep reasoning, planning, and conceptualisation capabilities. Present benchmarks fail to capture the comprehensive nature of intelligence, focusing instead on narrow domains that do not adequately reflect the emergent complexities of the prevailing AI systems [37]. Overall, evaluating new AI systems will require researchers to rethink benchmarks since static benchmarks are regarded as broken benchmarks in the making [8] due to data contamination and gaming of performances. What is needed, is an evolution and continuous updates of the benchmarks year-by-year by removing, adding and modifying questions and the addition of new ones to better assess the generalisation and robustness of AI systems [8]."}, {"title": "Practical implications, study limitations and future work", "content": "From a practical standpoint, the approaches outlined in this study can be implemented with immediate effect on the existing benchmarks and a set of criticality decision thresholds can be empirically derived and monitored over time. In terms of study limitations, while our investigation into AI system dynamics, particularly through the prism of LLMs and complexity theory has unearthed insights into potential criticality emergence, it is nonetheless constrained by the current benchmarking methodologies and the simplifications in our modelling approach. Current benchmarks do not fully capture the multidimensional and evolving complexity of AI systems which necessitates the development of holistic, robust benchmarks that integrate complexity measures to accurately reflect AI's capabilities. Moreover, the portrayal of AI capabilities as independent agents oversimplifies the intertwined nature of these systems where capabilities often overlap, leading to emergent behaviours that our current model, with its linear stochastic elements, may not adequately predict.\nTo advance our understanding and prediction of AI system behaviours especially near critical points, future research should innovate in benchmark design that captures the broader spectrum of AI capabilities and system complexities. This should involve not just expanding the range of reasoning tasks but eventually also require measuring the adaptability and the ability of ANNs to integrate new knowledge and experiences. Additionally, future criticality modelling efforts ought to evolve to reflect the non-linear, interdependent dynamics within Al systems."}, {"title": "7 Conclusion", "content": "Our study investigated the hypothetical constraints of AI systems through the lens of complexity theory and criticality. We leveraged agent-based modelling (ABM) combined with a set of well-defined assumptions, we identified scenarios where increasing complexity, represented by performance capabilities, may push AI systems to a critical threshold. Beyond this critical point, performance shifts from predictable improvements to erratic and unstable behaviour. Practically, we developed a detection method for identifying criticality in AI systems, showcasing its potential applicability in real-world scenarios with a relevance to large language models. We also conducted an extensive review of recent literature to underpin our findings, drawing from a wide array of empirical and theoretical studies that supported our conclusions. Ultimately, our findings advocate for measured optimism regarding AI growth within the context of current transformer-based architectures and a very cautious outlook on the prospects of current data-driven deep learning systems achieving AGI. Future research should extend the proposed methodologies for modelling criticality within AI systems, reflecting non-linear dynamics, together with the creation of a broader array of benchmarks that ensure a more robust and comprehensive evaluation of the capabilities of emerging AI technologies."}]}