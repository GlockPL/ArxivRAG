{"title": "VisionZip: Longer is Better but Not Necessary in Vision Language Models", "authors": ["Senqiao Yang", "Yukang Chen", "Zhuotao Tian", "Chengyao Wang", "Jingyao Li", "Bei Yu", "Jiaya Jia"], "abstract": "Recent advancements in vision-language models have enhanced performance by increasing the length of visual tokens, making them much longer than text tokens and significantly raising computational costs. However, we observe that the visual tokens generated by popular vision encoders, such as CLIP and SigLIP, contain significant redundancy. To address this, we introduce VisionZip, a simple yet effective method that selects a set of informative tokens for input to the language model, reducing visual token redundancy and improving efficiency while maintaining model performance. The proposed VisionZip can be widely applied to image and video understanding tasks and is well-suited for multi-turn dialogues in real-world scenarios, where previous methods tend to underperform. Experimental results show that VisionZip outperforms the previous state-of-the-art method by at least 5% performance gains across nearly all settings. Moreover, our method significantly enhances model inference speed, improving the prefilling time by 8\u00d7 and enabling the LLaVA-Next 13B model to infer faster than the LLaVA-Next 7B model while achieving better results. Furthermore, we analyze the causes of this redundancy and encourage the community to focus on extracting better visual features rather than merely increasing token length.", "sections": [{"title": "1. Introduction", "content": "Recently, the advancement of Large Language Models (LLMs) [1, 2, 49, 71] has led to significant progress in Vision Language Models (VLMs) [3, 5, 26, 30, 32, 55]. To integrate visual signals with textual semantics, existing VLMs typically utilize sequential visual representation, where images are converted into vision tokens and processed by an LLM decoder. Through modal alignment and instruction tuning, these VLMs adapt LLMs for the vision domain, leveraging their perception and reasoning capabilities.\nHowever, the promising performance of VLMs largely relies on the large amount of visual tokens. For example,"}, {"title": "2. VisionZip", "content": "In this section, we first explain the importance of reducing the number of visual tokens to improve model efficiency in Sec. 2.1, and then present our observation of redundancy in Sec. 2.2. After that, we detail the training-free method in Sec. 2.3. Additionally, to help the model better adapt to variations in visual token length, we introduce Efficient Tuning in Sec. 2.4. Finally, we briefly discuss the widespread usage of VisionZip. The overall architecture is shown in Fig. 3."}, {"title": "2.1. Preliminary", "content": "Architecture of VLM. The VLM architectures generally consist of three components: a visual encoder, a modality projector, and a LLM. The visual encoder, typically a pre-trained image encoder like CLIP's vision model, converts input images into visual tokens. The projector module aligns these visual tokens with the LLM's word embedding space, enabling the LLM to process visual data effectively. The LLM then integrates the aligned visual and textual information to generate responses.\nComputation Complexity. Evaluating the computational complexity of VLMs requires examining key components such as the self-attention mechanism and the feed-forward network (FFN). The total floating-point operations (FLOPs) can be expressed as:\nTotal FLOPs = T \u00d7 (4nd\u00b2 + 2n\u00b2d + 2ndm)\nwhere T is the number of transformer layers, n is the sequence length, d is the hidden dimension size, and m represents"}, {"title": "2.2. Redundancy Observation", "content": "In popular Vision Language Models like LLaVA and Mini-Gemini, the number of vision tokens far exceeds that of text tokens, consuming substantial computational resources. To assess whether all these tokens are necessary, we conducted a pilot study on the visual tokens generated by commonly used vision encoders, CLIP and SigLIP.\nSpecifically, we randomly sampled one image and visualized the attention of each token from the Vision Encoder's -2 layer, which is the selected layer for obtaining input visual tokens in most VLMs, such as the LLaVA. As shown in Fig. 2, both CLIP and SigLIP exhibit an attention pattern concentrated on a limited number of tokens, while the majority of visual tokens receive minimal attention. Furthermore, to demonstrate that the attention focusing on only a few tokens is a normal phenomenon, we analyze the distribution of attention weights on the TextVQA validation set. As shown in Fig. 2, most visual tokens receive very low attention, with weights close to zero, while only a few tokens hold higher attention weights. To show this phenomenon's prevalence, we include more visualizations in Appendix D.1.\nBased on this observation, we find that most visual tokens with low attention weights contribute little information and add significant redundancy. Only a few visual tokens aggregate a substantial amount of information and merit focused attention; we refer to these as the dominant visual tokens. Therefore, to reduce redundancy, we focus on se-"}, {"title": "2.3. Informative Visual Token Zip", "content": "Dominant Token Selection. To reduce redundancy by retaining only the most informative visual tokens and discarding less significant ones, the main challenge is identifying which tokens contribute most to the model's performance. We evaluate the importance of each visual token by examining its attention scores within the vision encoder. Specifically, we calculate the attention score as Eq. 1,\n$S_h = \\text{Softmax} \\left(\\frac{Q_h K_h^T}{\\sqrt{D_h}}\\right)$        (1)\nwhere $S_h$ is the attention score of each head, $D_h$ is the head dimension, and $Q_h$ and $K_h$ represent query and key, respectively. Averaging across the head dimension, yields an aggregated attention matrix $S_{avg} \\in \\mathbb{R}^{B \\times \\text{SeqLen} \\times \\text{SeqLen}}$, reflecting how each token attends to others."}, {"title": "2.4. Efficient Tuning", "content": "The Informative Visual Token Zip extracts highly informative tokens from the visual encoder and drops other tokens, thereby significantly reducing the token length input to the LLM, potentially by up to tenfold. However, this reduction in visual tokens can lead to a degree of misalignment, as the VLM model, originally trained on all full visual tokens, may struggle to adapt to the sudden decrease.\nTo bridge the gap between the visual and LLM spaces, we use minimal instruction tuning data to efficiently fine-tune the multimodal projector while keeping other components frozen, enhancing alignment between the vision and language spaces. Notably, the instruction tuning requires only 1/10 of the LLaVA-1.5 dataset and can be completed in just 30 minutes on 8 Nvidia A800 for LLaVA 1.5 7B. Notably, this process can also be implemented on 3090 GPUs, which is both resource-efficient and effective."}, {"title": "2.5. Usage of VisionZip", "content": "The VisionZip can adapt to multiple tasks, not only for image and video understanding in Vision-Language Models but also for multi-turn conversations that previous efficient VLMs could not handle. Additionally, VisionZip is easy to implement as it is text-agnostic, enabling compatibility with all existing LLM algorithms for acceleration. Furthermore, VisionZip can be seen as a plug-and-play method for vision encoders, which preserves over 90% of the original model's performance while saving 3 times runtime and memory. It can even allow a 13B VLM to achieve greater efficiency than a 7B VLM while maintaining superior performance. We will show more details in Sec. 4.3."}, {"title": "3. Experiments", "content": ""}, {"title": "3.1. Effectiveness on Image Understanding", "content": "Evaluation Tasks. To show the effectiveness of our method on image understanding tasks, we conduct experiments on eleven widely used benchmarks [11, 13, 19, 25, 28, 32, 36, 38, 45, 60, 62] and compare our method with the existing sota methods, FastV [6] and SparseVLM [65], which progressively reduce the number of visual tokens in the LLM forward process based on attention weights. To further validate the generalizability of our method, we conduct experiments on various VLM with different architectures and resolutions. Due to space limitations, we present only a subset of results for LLaVA-1.5 [32], LLaVA-NeXT [33], and Mini-Gemini [30] in the main text and all results and implementation details can be found in Appendix B.\nResults on LLaVA 1.5. As shown in Table 1, we deploy the proposed VisionZip on LLaVA-1.5 and demonstrate its performance on image understanding tasks. VisionZip represents our method being directly applied during the inference stage without additional training. VisionZip\u2021 denotes an efficient tuning for the cross-modality projector, requiring approximately 30 minutes on 8 A800 GPUs. This tuning can also be implemented on 3090 GPUs, making it both resource-efficient and effective. To comprehensively assess performance, we present the results in percentage format for comparative analysis, with the vanilla model's accuracy serving as the 100% upper limit. Following the setup in [6, 65], we use three vision token count configurations (192, 128, and 64) to evaluate the advantages of our proposed VisionZip. When the visual tokens are reduced from 576 to 192, VisionZip only decreases the average accuracy by 1.5% without additional training, surpassing FastV [6] by 10.3% and SparseVLM [65] by 2.1%, respectively. Furthermore, when only 64 tokens remain, our method outperforms FastV [6] and SparseVLM [65] by a significant margin of 18.4% and 8.2%, respectively. Additionally, VisionZip\u2021, which efficiently tunes the cross-modality projector, provides further performance improvements. As shown in Table 1, even with only 64 visual tokens retained, this efficient tuning boosts performance to 95.2%, representing only a 4.8% decrease compared to the vanilla method using 10 times the visual tokens.\nAn interesting phenomenon is that in certain bench-"}, {"title": "3.2. Effectiveness on Video Understanding", "content": "Evaluation Tasks. We evaluate our method on four common video question-answering benchmarks: TGIF-QA [20], MSVD-QA [54], MSRVTT-QA [54], and ActivityNet-QA [61], where video-question pairs exhibit significant length disparities. We follow the evaluation framework proposed by Video-LLaVA [31], utilizing ChatGPT score as key performance metrics. Further details are provided in Appendix B.\nResults on Video-LLaVA. The vanilla Video-LLaVA [31] uses the Language-bind as vision encoder to encode 8 frames, with each frame containing 256 visual tokens, resulting in a total of 2048 visual tokens. Hence, we set the Video-LLaVA with 2048 video tokens as the upper bound, achieving an overall average accuracy of 100.0% and a score of 0.00. To make a fair comparison, we follow the original settings for the baseline methods FastV [6]and SparseVLM [65], pruning the visual tokens to 135. For each frame, we zip the visual tokens from 256 to 17, resulting in a total of 136 visual tokens for the entire video. As shown in Table 3, our VisionZip in training-free mode achieves 93.2% accuracy across four benchmarks, outperforming the previous state-of-the-art method, SparseVLM, by 6.7%. Moreover, on the largest benchmarks, MSRVTT, our method shows a significant improvement over SparseVLM by 37.2%. Additionally, our method consistently exceeds 90% performance across all benchmarks, further demonstrating VisionZip's effectiveness and robustness."}, {"title": "3.3. Efficiency Analysis", "content": "Our proposed VisionZip reduces the number of visual tokens input to the Large Language Model, resulting in significant efficiency and CUDA memory gains during inference. We conduct a comparative analysis of CUDA memory us-"}, {"title": "4. Analysis and Discussion", "content": ""}, {"title": "4.1. Reasons of Redundancy in Visual Tokens", "content": "Visualization of the Redundancy. Firstly, as shown in Fig. 5, we illustrate attention changes across layers. In early layers, attention is broadly distributed across the image, but by the middle layers, it suddenly converges onto a few tokens. With deeper layers, attention and information concentrate on a small set of dominant tokens, reaching peak concentration by the 23rd layer-used for visual token extraction for the LLM. Notably, attention is more dispersed in the final layer, as these tokens align with the CLIP text branch via contrastive loss, potentially limiting their representation of the original image. This is why VLM selects the second-to-last layer (-2 layer). Additional visualization results are in Appendix D.\nExplanation. Current vision encoders are based on a transformer architecture that aggregates information between tokens through self-attention. We think that as the layer depth increases, instead of aggregating knowledge from all tokens, the model tends to \"shortcut\" by concentrating information into a few proxy tokens. If a CLS token is present, the knowledge may further concentrate from these proxy tokens into the CLS token. Moreover, using the function $softmax(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j}}$ to compute the model's loss can intensify this effect. The derivative of this formula is as:\n$\\frac{\\partial softmax(z_i)}{\\partial z_i} = softmax(z_i) (1 - softmax(z_i))$        (2)\nWe illustrated this function in Fig. 6 (a), when z is large, the gradient becomes substantial in exponential rise, and when z is small, the gradient is almost negligible. This function makes regions of low attention even lower and high-attention areas even more prominent, ultimately concentrating information into a few tokens. [52] identified a similar phenomenon in LLM inference, naming it \"Attention Sink.\" [43] also observed a comparable effect in se-"}, {"title": "4.2. Why VisionZip Outperforms Previous Work?", "content": "Text-Relevant Efficient VLM. Existing sota methods for reducing visual redundancy to accelerate VLMs, such as FastV [6] and SparseVLM [65], primarily rely on the LLM to identify text-relevant visual token. Specifically, they feed all visual tokens into the LLM and use attention between text and visual tokens across LLM layers for selection.\nMisalignment Due to the Pre-group Knowledge. While the text-relevant method appears promising, the visual tokens it selects often lack sufficient information. This limitation arises because the visual encoder aggregates visual information into a limited subset of high-attention tokens, leaving the remaining tokens with minimal informational content. As a result, tokens that should represent specific details are instead grouped into proxy tokens, losing their original incontext information. Furthermore, these proxy tokens tend to appear in peripheral or background areas rather than being positioned near the main subjects of the image. For instance, in Fig. 6 (b), the visual tokens most relevant to the person are not located on the person but are instead assigned to a proxy token situated on the road. This indicates that text-relevant methods often select tokens from elements like the man or the taxi, which actually contain significantly less informative content.\nTo further verify this, we performed two experiments on the TextVQA benchmark with SparseVLM, retaining 64 tokens, as shown in Table 5. In Ex1, we first masked 50 out of 576 total tokens, selecting the 50 tokens with the highest attention according to the vision encoder. From the remaining 526 tokens, SparseVLM was used to select the final set."}, {"title": "4.3. The Advantage of the VisionZip", "content": "Easy to deployment. Due to VisionZip directly reducing the visual tokens before projecting them into the LLM, rather than gradually reducing them during the LLM forward process, it avoids extensive computation and memory consumption in the LLM's shallow layers. As shown in Table 6, our method is compatible with existing quantization techniques, maintaining performance while minimizing memory usage. Furthermore, our method enables the 13B model to be faster and perform better than the 7B model. As shown in Table 7, our method significantly reduces the inference time of the 13B model, making it twice as fast as the vanilla 13B model and outperforming the vanilla 7B model in both performance and efficiency.\nAdvantages on multi-turn conversations. To better support real-world applications, current VLMs store the previous answer in the KV cache to enable multi-turn conversations, reducing the need to reprocess prior dialogue. However, as shown in Figure 7, prior text-relevant methods are unsuitable for multi-turn conversations. This is because the visual tokens selected and stored in the KV cache are closely related to the previous question but lack relevance to the current dialogue, leading to poor performance in multi-turn scenarios. In contrast, our VisionZip selects the most informative visual tokens in a text-agnostic manner, making it more effective for multi-turn conversations."}, {"title": "5. Related Work", "content": "Vision-Language Models. Building on the success of large language models (LLMs) [1, 2, 49], recent vision-language models (VLMs) [8, 30, 32, 48] advance multimodal generation by processing extensive visual token sequences. Higher resolutions require exponentially more tokens; for example, LLaVA-NeXT processes 672 \u00d7 672 images into 2304 tokens [32]. Handling videos or multiple images increases token requirements, as seen in VideoLLaVA [31] and Video-ChatGPT [39]. Hence, it's essential to discuss more efficient ways to extract information from visual tokens, rather than merely increasing their length."}, {"title": "6. Conclusion", "content": "In this paper, we analyze popular VLM models, noting that while increasing the length of visual tokens can improve performance, there is significant redundancy in current visual tokens. We propose a simple method, VisionZip, which reduces the number of visual tokens substantially while preserving model performance, thereby greatly enhancing computational efficiency. This method is broadly applicable to image and video understanding tasks and is suitable for multi-turn dialogue in practical applications.VisionZip also suggests a future direction to develop vision encoders with lower redundancy capabilities to further improve VLM performance and handle longer video sequences."}, {"title": "A. Further Discussion", "content": ""}, {"title": "A.1. Comparison with Text-relevant Efficient VLM", "content": "We observe that most recent Efficient VLMs [6, 16, 53, 65] utilize attention mechanisms between text tokens and visual tokens to determine which visual tokens should be retained, processing them during the LLM forward. However, our method, VisionZip, removes visual token redundancy before inputting them into the LLM. We will demonstrate our advantages from the following perspectives.\nBetter Performance. As shown in Table 1, 2, 3 of the main paper, our VisionZip achieves better performance in the training-free mode. This is because the Vision Encoder pre-groups the visual information into a few tokens, which often appear in the background or less prominent areas. However, when tokens are selected based on the semantic information of the text, the chosen tokens are often not the dominant tokens and carry less information, resulting in lower performance compared to VisionZip. Additionally, to better demonstrate the misalignment caused by the Vision Encoder's pre-grouping of information, we have created an interactive demo. As shown in Fig. 15, the code for this demo will be published soon.\nMore Efficient. Our method reduces the redundancy of visual tokens before inputting them into the LLM, avoiding the heavy attention computation in the early layers of the LLM (Sec. B.3). Additionally, we observe that previous text-relevant Efficient VLMs require significant intermediate computations to determine which tokens need to be dropped during the LLM forward process. This leads to a noticeable increase in memory usage, sometimes exceeding that of the vanilla model. This issue is particularly evident in models like LLaVA-NeXT, where the number of visual tokens is substantial.\nMore Application Scenarios. VisionZip operates out-"}, {"title": "A.2. VisionZip for Non-CLS Vision Encoders", "content": "Although most popular vision encoders, such as CLIP [41], OpenCLIP, and LanguageBind [70], use the CLS token to aggregate information, a recently introduced vision encoder, SigLIP, does not include the CLS token. To demonstrate the generalization of our proposed VisionZip, we explain how to apply it to Non-CLS Vision Encoders in this section.\nSpecifically, for the Dominant Token Selection, we first calculate the attention score as shown in Eq. 3,\n$S_h = \\text{Softmax} \\left( \\frac{Q_h K_h^T}{\\sqrt{D_h}} \\right)$        (3)\nwhere $S_h$ is the attention score of each head, and $D_h$ is the head dimension, $Q_h$ and $K_h$ represent query and key, respectively. By averaging across the head dimension, we obtain an aggregated attention matrix $S_{avg} \\in \\mathbb{R}^{B \\times \\text{SeqLen} \\times \\text{SeqLen}}$, which reflects how each token attends to every other token. The above process is similar to that of vision encoders with a CLS token, as described in the main text.\nTo identify key visual tokens, we calculate the average attention each token receives from all others in the sequence. Specifically, we compute the average along dim=1 of $S_{avg}$ to determine the degree to which each token is attended to by others, representing its importance. Tokens with higher average attention are considered more significant and are retained."}, {"title": "B. Additional Experiments", "content": ""}, {"title": "B.1. Image Understanding", "content": ""}, {"title": "B.1.1. Implementation Details.", "content": "Environments. We conduct the inference on a single NVIDIA A800-80G GPU, while the fine-tuning process is performed on 8 NVIDIA A800-80G GPUs. Furthermore, to demonstrate the efficiency and effectiveness of our VisionZip, the full training is conducted on 8 NVIDIA 3090-24G GPUs.\nParameters. For the VisionZip fine-tuning mode, we fine-tune only the cross-modality projector layer using a learning rate of 2e - 5, while keeping other components frozen. For the VisionZip training stage and inference mode, we follow the evaluation settings of the original model."}, {"title": "B.1.2. Evaluation Benchmark", "content": "We conducted experiments on these widely used visual understanding benchmarks.\nSEEDBench. SEEDBench [25] comprises 19,000 multiple-choice questions annotated by human assessors. The evaluation spans 12 distinct aspects, assessing the models' ability to recognize patterns in images and videos across both spatial and temporal dimensions.\nMMMU. MMMU [62] evaluates multimodal models on complex tasks requiring college-level knowledge and reasoning. It includes 11.5K curated questions from exams, quizzes, and textbooks, spanning six disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering. Covering 30 subjects and 183 subfields, these questions incorporate 30 image types like charts, diagrams, and chemical structures. MMMU challenges models with advanced perception and domain-specific reasoning, similar to expert-level.\nMMVet. MMVet [60] defines six core vision-and-language (VL) capabilities: recognition, OCR, knowledge, language generation, spatial awareness, and math. These capabilities integrate to address a range of complex multimodal tasks. MM-Vet evaluates 16 specific integrations of these capabilities through quantitative assessments."}, {"title": "B.1.3. Additional Experiments for LLaVA-1.5", "content": "Effectiveness on 13B. In the main paper, we demonstrate the effectiveness of our model on 7B in Table 1, and we show the effectiveness of our model on 13B in this section.\nEffectiveness on Training Stage. Our proposed method can also be applied during the training stage to reduce token length, thereby saving memory usage and training time."}, {"title": "B.1.4. Additional Experiments for LLaVA-NeXT", "content": "In the main paper Table 2, we present the performance of VisionZip on LLaVA-NeXT across several evaluation benchmarks. The complete benchmark results are provided in Table 12. In this table, we only display the LLaVA NeXT 7B results for these stable benchmarks, and the results demonstrate that our proposed VisionZip consistently delivers strong performance.\nB.1.5. Additional Experiments for Mini-Gemini\nIn the main paper, Fig. 4 demonstrates that our method out-"}, {"title": "B.1.6. Ablation Study", "content": "Impact of Fine-Tuning Dataset Compatibility We use VisionZip to efficiently fine-tune the cross-modality projector, addressing the gap caused by reduced visual tokens. Ensuring dataset compatibility with the original model is crucial for optimal performance."}, {"title": "B.2. Video Understanding", "content": ""}, {"title": "B.2.1. Evaluation Benchmark", "content": "TGIF-QA. TGIF-QA [20] extends ImageQA to videos with 165,000 question-answer pairs based on GIFs. It includes three VideoQA tasks\u2014repetition count, repeating action, and state transition\u2014requiring spatio-temporal reasoning, plus frame QA tasks answerable from single frames.\nMSVD-QA. MSVD-QA [54], based on the MSVD dataset, features 1,970 video clips and 50.5K question-answer pairs. Covering diverse topics, it supports video question answering and captioning with open-ended questions in five categories: what, who, how, when, and where.\nMSRVTT-QA. MSRVTT-QA [54] includes 10,000 video clips and 243,000 question-answer pairs, emphasizing video understanding and reasoning."}, {"title": "B.2.2. Future Direction", "content": "With the development of LLMs and VLMs, video understanding has become a popular research direction. Whether the goal is for VLMs to comprehend longer videos or to achieve precise localization within videos, enabling the input of more frames within limited memory is both important and critical."}, {"title": "D. Visualization", "content": ""}, {"title": "D.1. Visualization of Redundancy", "content": "To further show the redundancy in popular vision encoders, we include additional examples from the COCO train2017 dataset. This dataset is a key component of the LLaVA 1.5 fine-tuning dataset and an essential part of many vision datasets.\nD.2. Visualization of Attention Distribution Change\nIn Sec. 4 of the main text, we discuss the reasons behind the redundancy in visual tokens. In this section, we present a comprehensive analysis of the changes in attention within the CLIP model. As shown in Fig. 12 and Fig. 13 attention in the early layers is broadly distributed across the image. However, by the middle layers, it rapidly converges onto a few tokens. In the deeper layers, attention and information become concentrated on a small set of dominant tokens, reaching peak concentration by the 23rd layer, which is used for visual token extraction for the LLM. Besides, in the final layer, attention is more dispersed as these tokens align with the CLIP text branch via contrastive loss, potentially limiting their ability to represent the original image.\nD.3. Visualization of Feature Misalignment\nIn Fig. 6 of the main text, we show the phenomenon of feature misalignment. To further demonstrate that this phenomenon is widespread, we observe it across additional COCO images."}]}