{"title": "EMOS: EMBODIMENT-AWARE HETEROGENEOUS MULTI-ROBOT\nOPERATING SYSTEM WITH LLM AGENTS", "authors": ["Junting Chen", "Checheng Yu", "Xunzhe Zhou", "Tianqi Xu", "Yao Mu", "Mengkang Hu", "Wenqi Shao", "Yikai Wang", "Guohao Li", "Lin Shao"], "abstract": "Heterogeneous multi-robot systems (HMRS) have emerged as a powerful ap-\nproach for tackling complex tasks that single robots cannot manage alone. Current\nlarge-language-model-based multi-agent systems (LLM-based MAS) have shown\nsuccess in areas like software development and operating systems, but applying\nthese systems to robot control presents unique challenges. In particular, the ca-\npabilities of each agent in a multi-robot system are inherently tied to the physical\ncomposition of the robots, rather than predefined roles. To address this issue,\nwe introduce a novel multi-agent framework designed to enable effective collab-\noration among heterogeneous robots with varying embodiments and capabilities,\nalong with a new benchmark named Habitat-MAS. One of our key designs is\nRobot Resume: Instead of adopting human-designed role play, we propose a self-\nprompted approach, where agents comprehend robot URDF files and call robot\nkinematics tools to generate descriptions of their physics capabilities to guide\ntheir behavior in task planning and action execution. The Habitat-MAS bench-\nmark is designed to assess how a multi-agent framework handles tasks that require\nembodiment-aware reasoning, which includes 1) manipulation, 2) perception, 3)\nnavigation, and 4) comprehensive multi-floor object rearrangement. The experi-\nmental results indicate that the robot's resume and the hierarchical design of our\nmulti-agent system are essential for the effective operation of the heterogeneous\nmulti-robot system within this intricate problem context.", "sections": [{"title": "INTRODUCTION", "content": "The complex nature of real-world environments and specialized robot hardware makes it difficult\nfor a single robot to perform complex tasks efficiently. As a result, the Heterogeneous Multi-Robot\nSystem (HMRS) has emerged, enabling multiple robots designed for diverse purposes and with com-\nplementary physics capabilities to cooperate and execute complex missions through task decompo-\nsition, coalition formation, and coordinated task allocation. Designed for real-world deployments,\nexisting HMRSs are highly dependent on some assumptions and human-crafted protocols based on\nhuman prior knowledge (Rizk et al., 2019). This limits the generalization of HMRS and the ability\nto handle complex tasks. In the survey, Rizk et al. (2019) classified the automation of HMRS into\nfour levels: 1) Level 1, task execution. 2) Level 2, task execution plus task allocation or coalition\nformation, but not both. 3) Level 3, the automation of all above but not instruction to decomposed\nsub-tasks. 4) Level 4, fully automated entire system. To the best of our knowledge, no system has\nachieved an automation level of 4.\nMeanwhile, we have recently witnessed how large language models (LLM) multi-agent systems\n(MAS) operate complex systems like Operating Systems (Mei et al., 2024) or finish complex tasks\nlike software development (Hong et al., 2023), by leveraging the common sense reasoning capabili-\nties and code generation capabilities to generally control diverse applications. Similarly in embodied\nAI tasks, Mandi et al. (2024) proposed using LLM-based MAS chat to control the duel arm system.\nZhang et al. (2023) introduced a human-robot collaboration system through the LLM multi-agent.\nThese works focused on certain aspects of MRS automation problem or focused MRS with specific\nhardware configuration. Our observation is that one missing key component toward the level-4 full\nautomation is embodiment-aware reasoning. It refers to the agent's ability to understand its phys-\nical embodiment and thus the hardware-dependent capabilities. Based on this capability, the LLM\nmulti-agent can further decompose the tasks, assign the tasks, and finally execute the tasks in the\nreal time, i.e. the level-4 automated HMRS.\nIn this work, we propose EMOS, a general LLM-based multi-agent framework to operate coopera-\ntive HMRS in indoor household environments. Our insight is that, instead of teamwork through role\nassignment as in recent LLM-based MAS (Hong et al., 2023; Li et al., 2024; Wu et al., 2023a), the\nLLM-based MAS tailored for heterogeneous robots should actively check their physics information\nand tasks they can complete without fixed roles. Thus, we introduce a bottom-up robot capability\ngeneration approach that constructs a \"robot resume\" for each robot, capturing its unique skills and\nconstraints. The resumes, along with a scene description and task description, form the full con-\ntext for the LLM-based MAS to perform task planning, task assignment, and action execution in a\ncascaded manner. To study how LLM-based MAS could potentially enable the full automation of\ncollaborative heterogeneous multi-robot systems, we present Habitat-MAS, which is a benchmark\nwith annotated episodic data and an accompanying simulated environment with textual description\nof the environment as the interface for the agents. In the benchmark, we provide a diverse col-\nlection of robots including drones, wheeled robots with arms or elevatable grippers on a rack, and\nlegged robots with arms, and also diverse environment including multi-floor large houses and multi-\nroom flats. The benchmark presents four tasks, each designed to evaluate multi-agent systems in\nterms of their understanding of robot physical capabilities including perception, navigation, and ma-\nnipulation. Episodes are processed such that only robots possessing specific physical abilities can\nsuccessfully complete certain subtasks in an episode. Through extensive experiments, we illustrate\nthe importance of robot resumes in embodiment-aware reasoning and how different components in\nEMOS affect HMRS performance in our benchmark.\nTo summarize, the key contributions of this paper are:\n\u2022 We present EMOS, a novel LLM-based MAS framework that first conducts embodiment-aware\nreasoning with self-generated robot resume, rather than human-assigned role playing, to operate\na collaborative HMRS.\n\u2022 We present Habitat-MAS, a new benchmark to study how LLM-based MAS can coordinate collab-\norative HMRS. To the best of our knowledge, this is the first simulated benchmark for this problem\nwith extensive robot types and scenes. It is also highlithed as the first benchmark to evaluate the\nagent's understanding of its physics embodiment, with test dataset tailored for this purpose.\n\u2022 Experimental results on Habitat-MAS demonstrate the effectiveness of robot resume in EMOS,\nhighlighting the significance of embodiment awareness for collaborative HMRS."}, {"title": "RELATED WORK", "content": "MULTI-AGENT SYSTEM\nMulti-agent systems (MAS) have been a research focus for several decades. These systems consist\nof multiple interacting agents, which can be both cooperative and competitive, and are designed to\nsolve problems that are difficult or impossible for a single agent to tackle alone Stone & Veloso\n(2000). The integration of LLMs into MAS is a relatively new yet rapidly growing area of research.\nThis integration leverages the language understanding and generation capabilities of LLMs to en-\nhance communication, coordination, and decision-making within MAS. Wu et al. (2023a); Hong\net al. (2023); Li et al. (2024) focus on the communication issues in LLM-based Multi-Agent Sys-\ntems. Xu et al. (2024) proposes Crab, a cross-environment benchmark framework for evaluating\nMultimodal Language Models (MLMs) in different GUIs like mobile phones and desktop comput-\ners. For robotic intelligence, Zhang et al. (2023) investigates how two agents can use communi-\ncation to better collaborate and complete tasks in a multi-room scenario. Mandi et al. proposed\nRoCo, which is a multi-agent system for multi-arm collaboration. They try to leverage the 3D spa-\ntial reasoning capabilities to help multi-arm low-level trajectory planning. In comparison, we focus\na more general mutli-agent scenario with drones, legged robots, wheeled robots with arms, with a\nmulti-agent system required to understand general capabilities including navigation, manipulation\nand perception based on physics design.\nMULTI-ROBOT SYSTEM\nEarly works by Arai et al. (2002) and Ota (2006) laid the research foundation for multi-robot sys-\ntems by providing a comprehensive overview of the progress and key challenges in MRS in around\n2000, including MRS architectural design, distributed mapping, and navigation coordination, etc.\nRizk et al. (2019) specifically reviewd the challenges in cooperative heterogeneous MRS, decom-\nposing the MRS workflow to task decomposition, coalition formation, task allocation, perception\nand MRS planning and control. In this work, we also follow the established concept definitions and\nthe principles of system design in this survey. Rold\u00e1n et al. (2016) built a HMRS composed of aerial\nvehicles (drones) and ground vehicles to collaborate to monitor environmental variables of green-\nhouses. Kiener & Von Stryk (2010) designs a system composed of wheeled robot and humanoid\nrobot to collaborate in a \"robot soccer\" scenario. The authors carefully decompose the complex task\ninto subtasks based on the robots' capabilities, followed by human-crafted task allocation and plan-\nning algorithms. In this work, we focus on MRS for complex household environments with multiple\nfloors, composed of drones, wheeled robots, and legged robots with robotic arms.\nTASK PLANNING WITH LARGE LANGUAGE MODELS\nLarge language models(LLMs) trained on massive corpora are generally considered to have acquired\ncommon sense knowledge for task planning (Vemprala et al., 2023; Yao et al., 2022; Zhao et al.,\n2023; Hu et al., 2023; Wu et al., 2024; Sha et al., 2023; Mu et al., 2024a; Gao et al., 2024). Thanks\nto recent advancements, directly generating plans with LLMs has become an active research area\nin recent years (Logeswaran et al., 2022; Wu et al., 2023b; Lin et al., 2023). When using LLMs\nfor task planning, some approaches directly generate the entire plan in an open-loop manner, that is,\nwithout executing it in the environment (Huang et al., 2022a; Mu et al., 2023; Singh et al., 2022). An\nalternative line of research investigates closed-loop task planning, which offers greater flexibility for\nerror correction, human interaction, and grounding the plan in the actual environmental state (Ahn\net al., 2022; Guo et al., 2023; Huang et al., 2023; Hu et al., 2023; Huang et al., 2022b; Song et al.,\n2023; Hu et al., 2024; Mu et al., 2024b). This paper explores closed-loop task planning, where\nreal-time environmental changes are integrated, and a central large language model processes these\nreal-time changes and adapts plans accordingly."}, {"title": "EMOS FRAMEWORK", "content": "The multi-agent system introduced in this paper focuses on developing an embodiment-aware frame-\nwork for heterogeneous multirobot collaboration. Traditional multi-robot systems often face chal-\nlenges related to coordinated motion planning, especially in complex environments involving diverse\nrobotic platforms such as UAVs, mobile robots, legged robots, etc. The team formation and coopera-\ntive protocol are designed by robot experts, rather than automated by robots themselves. This system"}, {"title": "FRAMEWORK OVERVIEW", "content": "For clarification, we first define the mathematical form of the problem solved by the multi-agent sys-\ntem. Assume there is a multi-robot system involving N different robots and there is an LLM agent i\nattached to each of the robots i, $i \\in \\{1, 2, . . ., N\\}$. All agents operate in a shared environment with\nstate space S, and each agent i has an observation space $O_i$ and an action space $A_i$. The multi-agent\nsystem is designed to collaboratively achieve a given task T\u2208 T, such as exploring an unknown\nenvironment. The system serves as a set of task-conditioned policies $\\{\\pi_i : O_i \\times T \\rightarrow A_i\\}\\}_{1,}^{\\mathbb{N}}$\nwhere T represents the textual space of the task description. However, rather than an end-to-end\npolicy network as it might hint, our proposed multi-agent system adopts a discussion-like, hierarchi-\ncal framework, which has been proven effective in many other multi-agent scenarios. As Figure 2\ndemonstrates, the multi-agent system involves three cascading stages: 1) scene context construction;\n2) centralized group discussion; and 3) decentralized action parallel execution. Since the focus of\nthis work is embodiment-aware reasoning in task planning, we assume the multi-robot system is\nequipped with a perfect multi-agent SLAM system and provide the perfect geometric representation\nas an observation to the multi-agent system at the initial state. The geometric representation will be\nfurther processed into textual representation as the scene context for multi-agent discussion. With\nrobot resume processed from robot URDF, the multi-agent system performs a group discussion to\ndecompose the task and assign subtasks to corresponding agents based on their physics limitations."}, {"title": "SCENE CONTEXT CONSTRUCTION", "content": "For the deployability of the LLM multi-agent system onto the real multi-robot system, we propose\na bottom-up pipeline to construct the textual scene context from the geometric representation of\nthe environment that can be reconstructed from a normal robot perception pipeline. Following the"}, {"title": "ROBOT RESUME", "content": null}, {"title": "HIERARCHICAL TASK PLANNING, ASSIGNMENT AND ACTION", "content": "To adapt the LLM-based MAS for real-time HMRS operation, the multi-agent action policy needs to\nbe asynchronous, due to the potential asynchrony in multi-robot action execution. For this purpose,\nwe design a hierarchical pipeline to perform task planning, assignment, and action execution for\nour LLM-based MAS. Specifically, there are two stages: 1) The first stage of Centralized Group\nDiscussion runs in a synchronized fashion, in which all agents wait for messages from other agents,\nand the discussion history is seen by all agents. 2) While in the second stage of Decentralized Action\nExecution, each agent generates an action, waits for its execution it in the world, and generates a\nnew action so on so forth. Each robot is associated with a robot-dedicated agent with full access to\nits robot resume to assist in decision-making and action execution. The pseudocode in Algorithm 1\nprovides a comprehensive overview of the hierarchical task planning, assignment, and then action\nexecution within the EMOS framework.\nIn the first stage of centralized group discussion, there is a CentralPlanner that generates an\ninitial plan for each robot, and each robot also has an LLM agent that checks its assigned subtask\nand provides feedback to the central planner in Reflection. With the robot resume composed of\ngeneral description and numerical details, the robot-dedicated agent could reason its general avail-\nability for the assigned task and further check the geometric availability by using mathematical tools.\nSpecifically, by accessing robot resumes in the last section, a robot-dedicated agent is encouraged to\nperform common-sense reasoning with textual summaries, and generate code with numerical data\nin both the scene description and the robot resume to do calculations for spatial-aware reasoning.\nIn the second stage, with the result of the assignment of tasks, each robot-dedicated agent starts\nto execute its action in parallel. Given a subtask description and action execution history, a robot-\ndedicated agent controls the current agent by LLM FunctionCall with robot control libraries."}, {"title": "HABITAT-MAS BENCHMARK", "content": "Habitat-MAS 4 is a benchmark designed to evaluate LLM multi-agent systems (MAS) deployed\nin collaborative heterogeneous multi-robot systems (MRS) in multi-floor household scenarios. The\nLLM multi-agent system needs to do task planning, task assignment, and action execution with\nthe comprehensive understanding of the robot physics capabilities and task-relevant environmental\ninformation to succeed in the tasks. The setting reflects real-world robotic challenges, where agents\nwith varying embodiments, such as wheeled, legged, and aerial robots, must cooperate to accomplish\ncomplex tasks that require different physics capabilities."}, {"title": "BENCHMARK OVERVIEW", "content": "The Habitat-MAS benchmark is based on Habitat ((Puig et al., 2023)), a highly configurable simula-\ntion platform for embodied AI challenges that extensively supports the integration of various indoor\nenvironment datasets. For diversity, we choose to build the Habitat-MAS benchmark on multi-floor\nreal-scan scenes in Matterport3D (Chang et al. (2017)) and single-floor synthesized scenes in HSSD\n(Khanna* et al. (2023)). In our full dataset, we cover 27 scenes in Matterport3D and 34 scenes in\nHSSD. There are four base robot types in the benchmark: 1) Fetch has a wheeled base and a 7-DOF\narm of revolute joints. 2) Stretch has a wheeled base and a telescoping arm of prismatic joints. 3)\nDrone is in fact a DJI M100 with an RGBD sensor for its model credit. Since we care more about\nhigh-level discrepancies across different types of robot in the multi-robot systems, we neglect more"}, {"title": "TASK OVERVIEW", "content": "There are four tasks carfully designed in the Habitat-MAS benchmark. Tasks 1, 2, and 3 aim to eval-\nuate if agents are able to understand the three aforementioned robot capabilities respectively. Specifi-\ncally, 1) Task 1 is deigned as a cross-floor object navigation task including two robots (wheeled and\nlegged) navigating in a multi-floor scene, aiming to evaluate agent's ability to understand robot's\nmobility; 2) Task 2, named cooperative perception for manipulation, represents a common scenario\nin multi-robot collaboration, where robot perception assist manipulation. This task is set up to test\nthe ability of MAS to reason about robot sensor type or view point in other word; 3) Task 3 is a\nclassic household rearrangement task including two robots with different manipulation capabilities\ncollaborate to manipulate object placed on specific receptacles, this task can cleverly test MAS's\nablility to understand robot arm's workspace; 4) Task 4 is a multi-floor multi-agent and multi-object\nrearrangement task that requires the LLM-based multi-agent systems to comprehend all information\nand capabilities properly to collaborate. It is important to note that, during the creation of the bench-\nmark dataset, we carefully filter the task episodes so that each robot in the scene can only complete\na subset of the subgoals. In other words, the multi-agent system must comprehend robots' physical\ncapabilities to forge a feasible plan. For detailed task description, refer to Appendix A.2."}, {"title": "EVALUATION CRITERIA", "content": "The performance of multi-agent collaboration in Habitat-MAS is evaluated using several key met-\nrics: 1) Success Rate. Based on the task design we introduced in the last section, we define a series\nof intermediate subgoals in PDDL language for each task to evaluate the task result. This metric\nevaluates the proportion of episodes in which an MRS successfully completes all sub-goals, which\ndirectly reflects the overall planning and coordination capabilities of MAS. 2) Sub-goal Success\nRate. This metric calculates the percentage of sub-goals achieved by the MRS. Due to the limit on\npages, please refer to Appendix A.3.1 for more details about the sub-goal definition and implemen-\ntation. 3) Token Usage. The used tokens are a key metric to evaluate the efficiency of LLM-based\nMAS. The effectiveness of agent communication and action planning is measured by the number\nof tokens used during discussions. This reflects how efficiently agents coordinate and strategize to\ncomplete tasks. 4) Simulation Step. We also evaluate the number of simulation steps consumed by\nthe MAS to complete each task. Drones typically move the fastest, followed by wheeled robots, with\nlegged robots being the slowest. This metric evaluates the LLM-based MAS' ability to assign tasks\nfor high MRS efficiency. For instance, in an extreme scenario, one robot handles all the subtasks,"}, {"title": "EXPERIMENTS WITH EMOS", "content": "In this section, we present the experimental result of our EMOS system on our Habitat-MAS bench-\nmark, along with ablation studies to explain the impact of different building blocks. Our benchmark\noffers a large-scale dataset with episodes in more than 70 distinct scenes. However, due to budget\nconstraints, all ablation studies were conducted on a subset of 519 episodes. We use the GPT-40\n(OpenAI, 2024) API of the May 2024 version in this experiment. For more details on how episodes\nare generated and the full set of those episodes, please refer to Appendix A.3.2."}, {"title": "CONCLUSION", "content": "In summary, this paper introduces the Embodiment-Aware Heterogeneous Multi-Robot Operating\nSystem (EMOS), an LLM multi-agent system designed to operate multi-robot systems in complex\nhousehold environment. The key challenges addressed in this system are embodiment-aware reason-\ning and spatial reasoning in household tasks in the 3D world. The proposed framework integrates\na novel \"robot resume\u201d feature that dynamically captures the physical capabilities of heterogeneous\nrobots and uses a hierarchical, decentralized approach for task planning and execution. The sys-\ntem is validated through the Habitat-MAS benchmark, which includes a variety of tasks requiring\nrobots to collaborate across different mobility, perception, and manipulation capabilities. The ex-\nperimental results demonstrate the significance of embodiment-awareness and spatial reasoning in\nheterogeneous multi-robot systems. The ablation studies specifically highlight the importance of us-\ning numerical information for precise spatial reasoning, and group discussion modules to decompose\nthe complex tasks in improving task success rates.\nFuture work could focus on system-level issues, like improving the system's scalability in multi-\nagent communication protocol to even more diverse robot types and a much larger number of robots\n(e.g., swarm system), and expanding the framework's adaptability to more dynamic, real-world set-\ntings in which the system needs to handle external disturbance or subjects with unknown intention."}, {"title": "APPENDIX", "content": null}, {"title": "CAPABILITIES FOR EMBODIMENT-AWARE REASONING", "content": "As mentioned in the previous section, we categorize the hardware-specific capabilities into three\ndimensions. Our considerations are as follows:\n\u2022 Mobility. Robots exhibit different types of mobility capabilities. For example, aerial robots like\ndrones can move in non-occupided 3D space, legged robots like Spot can move across floors and\nlow obstacles, and wheeled robots like Fetch and Stretch can only move on the flat ground. Thus,\nfor a general heterogeneous multi-robot system, the deployed LLM agents should be aware of the\nrobot's mobility capabilities when making navigation decisions. The benchmark will present a\ncomprehensive scene description about regions and their interconnectivity to help language mod-\nels deduce potential navigation pathways for navigation decision making.\n\u2022 Perception. Robots are equipped with sensors (e.g., RGBD cameras) to perceive the environment.\nThe perception capabilities include sensor types and camera projection models. Specifically, as\nshown in Figure 6, we use a simplified frustum model including the optical axis and camera Field-\nOf-View (FOV) angle defined in Equation 1, where x represents the distance from the camera's\ncentral axis and f represents the focal length. The agent needs to be aware of the robot's perceptual\nspace and check if objects to perceive can potentially fall within the camera frustum. For example,\ndue to jaw camera height and angle limitation, the Spot lacks the capability to perceive objects\nplaced in high positions (e.g., shelves), while a drone is the best choice for this task. Given the\ncurrent difficulty for the LLM model to generate novel algorithms, we write prompts to instruct\nthe agent to assume the camera is symmetrical about the up-axis, check the angle a between the\nprojected target object and the camera optical-axis, and compare it with half of the field-of-view\n(FOV) angle.\n$\\theta_{FOV} = 2 \\tan^{-1}(\\frac{x}{2f})$ (1)\n\u2022 Manipulation. Robots feature diverse manipulation capabilities due to mechanical arms of dif-\nferent forms, various types of end effectors, and whether or not they have an explicit manipulator.\nHense, it is important for agents to use mathematical tools to reason the robots arm workspace\nexpecially when handling objects placed in abnormal positions (e.g. far inside the bed, high on\nthe cabinet). For cooperative manipulation, the agents can pre-judge and assign the proper robots\nto fetch or place the target objects in the task planning stage.\nwhich robots can reach the target objects.\nThese capabilities include both textual summarization and numerical details. While the capability\nsummaries are used for common sense reasoning, the numbers are prompted to be used in LLM\ncode generation for spatial-aware reasoning, which will be discussed in the next section."}, {"title": "HABITAT-MAS BENCHMARK TASK DESIGN", "content": "As mentioned in the previous section, we carefully designed four challenging tasks to evaluate the\nembodiment-aware reasoning capabilities of MAS. Detailed description of each task are as follows:"}, {"title": "Cross-floor object navigation.", "content": "As an extension of the Multi-ON (Wani et al., 2020)\nproblem, this multi-floor task requires the collaboration of robots with different base types to\nnavigate to multiple objects in the scene. The wheeled robot can only operate on a single floor,\nwhile the legged robot can navigate between floors, emphasizing the need for coordinated planning\nwith awareness of the mobility capabilities of different robots. This task is specifically designed\nto test the MAS's ability to reason about mobility constraints when coordinating cross-floor tasks."}, {"title": "Cooperative perception for manipulation.", "content": "Due to limitations in perception caused by\nthe camera's position and type, some articulated robots like spot may lack the ability to detect\ntarget objects on high shelves, while some arm-less robots like Drone with better camera view\nmay succeed. The heterogeneous robots need to cooperate to acquire a good RGB-D perception\nof objects for precise manipulation. In this single-floor task, different target objects are placed in\npositions that are visible for certain robots. We aim to test if the MAS can reason about the robot\nsensor type and viewpoint and successfully assign appropriate robots to perceive all target objects."}, {"title": "Collaborative single-floor home rearrangement.", "content": "As we discussed in the last section,\ndifferent robots have different manipulation capabilities. Articulated robots are limited to reach-\ning objects within their arm's workspace. For instance, Stretch is equipped with an arm that can\nextend farther horizontally, while Fetch has a greater vertical reach, allowing it to grasp higher\nobjects compared to Stretch. This single-floor task involves rearranging objects placed in varying\npositions, including the ground, high shelves, or a bed center far from a navigable area, requiring\nthe robots with different arm workspaces to understand their availability for different rearrange-\nment targets."}, {"title": "Multi-Robot, multi-object, multi-floor collaborative rearrangement.", "content": "This is a com-\nprehensive task that requires complex coordination for collaboration. Within this scenario, several\ndistinct types of robots, aerial, wheeled, and legged robots, must collaborate to perceive and re-\narrange a large set of objects distributed in various positions across multiple floors. This task\ncombines coordinating different capabilities of heterogeneous robots. Specially, some objects are\nlocated on high surfaces, such as cabinets upstairs, requiring advanced perception, manipulation\nand mobility. Our primary goal is to evaluate the MAS's ability to optimize task execution by\neffectively leveraging the unique capabilities of each robot, while balancing token efficiency and\ntime step consumption."}, {"title": "EXTRA DETAILS ABOUT HABITAT-MAS BENCHMARK", "content": null}, {"title": "SUB-GOAL DEFINITION WITH PDDL LANGUAGE", "content": "Habitat environment Puig et al. (2023) have already integrated a PDDL (McDermott et al., 1998)\nsystem for composite task definition and goal evaluation in simulation. The goal of a complex task\ncan be defined by a composite logical expression with primitive predicates and logical operators.\nBased on the PDDL system, we take the following primitive predicates as the sub-goals in evalu-\nation: 1) Robot_At_Object: This is the first stage in every tasks, robot need to firstly navigate to\nthe nearest navigable points to target objects and then execute the following actions like detect or\npick. 2) Robot_At_Receptacle: This is the another type of navigation sub-goal for long-horizon\ntasks like rearrangement,for which robots need to navigate to the receptacles before placing objects\non the recepcles to complete the final goals of rearrangement. 3) Object_At_Receptacle: This is\nthe final goals of rearrangement tasks. Sometimes robots may be assigned to pick and place the\nobjects beyond their reach, which means it is not enough to just count whether robots can navigate\nto objects or receptacles. We add this sub-goal to test the ability to reason long-horizon task further\nexplain which robot tend to fail in specific tasks. 4) Robot Detect_Object: This is for specific for\nperception tasks, aiming to judge how well the system perform in detcting objects."}, {"title": "EPISODE GENERATION AND VERIFICATION", "content": "The episodic datasets in the benchmark are automatically generated by a cascaded sampling and\nverification process. We first fliter the eligible tasks scenes from HSSD (Khanna* et al., 2023)\nand MP3D Chang et al. (2017) datasets that meet the defined requirements. For HSSD scenes, we\nensure that the navigable points in the scene are connected and that there are a sufficient number\nof rooms available for placing receptacles and objects. Additionally, we calculate the navigable\nmesh(navmesh) and verify that the robot can navigate to the closest navigable point near the object."}, {"title": "SPECIAL CASE STUDY", "content": "Incorrect code generation caused by hallucinations During large-scale experiments, we identi-\nfied some specific cases that occasionally lead the system to make incorrect judgements about the"}, {"title": "TOKEN ENHANCEMENT EFFICIENCY", "content": "EMOS consumes the most token since we include procedures such as leader assignment, group dis-\ncussion, self reflection, action execution, etc. These procedures all inevitably query and response\nwith large amount of LLM tokens. To evaluate the extra tokens consumed as we develop our frame-"}]}