{"title": "GPT-2 Through the Lens of\nVector Symbolic Architectures", "authors": ["Johannes Knittel", "Tushaar Gangavarapu", "Hendrik Strobelt", "Hanspeter Pfister"], "abstract": "Understanding the general priniciples behind transformer models remains a com-\nplex endeavor. Experiments with probing and disentangling features using sparse\nautoencoders (SAE) suggest that these models might manage linear features em-\nbedded as directions in the residual stream. This paper explores the resemblance\nbetween decoder-only transformer architecture and vector symbolic architectures\n(VSA) and presents experiments indicating that GPT-2 uses mechanisms involving\nnearly orthogonal vector bundling and binding operations similar to VSA for com-\nputation and communication between layers. It further shows that these principles\nhelp explain a significant portion of the actual neural weights.", "sections": [{"title": "Introduction", "content": "Understanding the way transformer models [28] work remains a complex task, impacting AI in-\nterpretability, safety, and alignment, among other areas. Successes in using linear probing to steer\nmodels [2, 18] and principal component analysis experiments on hidden embeddings [18] indicate\nthat these models might handle linear features embedded as directions in the residual stream, which\nis also known as linear representation hypothesis [9, 23, 15]. Recently, sparse autoencoders have\nbeen trained for a number of related model architectures based on the idea that hidden embeddings\ncan be broken down into a combination of sparse feature vectors [4]. One way how a linear additive\nmodel might function is by using nearly orthogonal vectors instead of arbitrary (or strictly orthogonal)\ndirections. In high-dimensional spaces, we can have many more nearly orthogonal vectors than\nstrictly orthogonal ones [9]. Vector symbolic architectures (VSA) [16], also known as hyperdi-\nmensional computing, leverage this principle for model construction. For example, we can create\n'bag-of-concept' vectors by summing nearly orthogonal vectors, so that individual concept vectors\nmaintain a higher dot product with the summed vector, effectively modeling an OR conjunction. The\nresidual stream of transformers could represent such a collection of nearly orthogonal concept vectors\nto form a distributed code. However, current evidence is limited on whether production models like\nGPT effectively use such feature vectors and how. A better understanding of the inner mechanics has\nmany implications on model interpretability and the usefulness of training sparse autoencoders, as\nwell as on architectural design decisions. This paper explores the resemblance between decoder-only\ntransformer architectures and vector symbolic architectures and presents experiments indicating that\nGPT-2 [24] uses mechanisms involving nearly orthogonal vector bundling and binding operations\nsimilar to VSA. It further shows that these principles help explain a significant portion of the actual\nweights in the MLP layers."}, {"title": "Interpreting GPT-2 Through Bundlings and Bindings", "content": "In high dimensional spaces, we can have many more nearly orthogonal vectors than dimensions [9].\nFor an n-dimensional space, we cannot find more than n distinct vectors such that they are orthogonal\nto each other, i.e., the inner product (dot product for real spaces) of any vector pair is exactly zero.\nHowever, we can find much larger sets of vectors such that the dot product of any vector to any other\nvector is nearly zero or small. One can think about a unit sphere in which the surface is divided into\nsmall patches such that the centroids of these areas are sufficiently far away, corresponding to a large\nenough angle (i.e., small enough dot product). We can imagine that there are many more such patches\nthan the number of dimensions, depending on our dot product threshold. Bundling and binding of\nsuch nearly orthogonal concept vectors are important principles of vector symbolic architectures [11],\nbut similar ideas are used in many other techniques, e.g., in vector-based linear models [20].\nBundling refers to the bagging of vectors to create composite concept vectors. Imagine we have three\nnearly orthogonal vectors $C_1, C_2, C_3$, each representing a specific (binary) concept. We can bundle $C_1$\nand $c_2$ into a concept vector $C_{1,2} = C_1 + C_2$ that has directional similarity to both vectors $C_1$ and $C_2$\nbut not to $c_3$. We can efficiently test whether a vector $c_j$ is similar to either $c_1$ or $c_1$: $\\langle C_{1,2}, C_j \\rangle \\geq \\lambda$,\nessentially corresponding to a single perceptron with a ReLU activation function.\nIn addition to bundling, we can also perform a binding operation by multiplying a suitable matrix\n$M$ with the concept vector. For instance, this allows us to model a specific order of concepts:\n$c_M = MC_1 + C_2$ is similar to $c_2$, $MC_1$, and $M^{-1}c_1$, but not to $c_1$ (if $M \\neq I$). $M$ could be used here\nto model that $c_1$ refers to the first concept of the pair $(C_1, C_2)$. This binding still works even if the\nmatrix is not orthogonal but composed of nearly orthogonal vectors, i.e., $M^T M$ will lead to a nearly\ndiagonal matrix (see Appendix A).\nBinding transformations can be interpreted in two ways. We can regard them as binding operators of\n(possibly previously bundled) concept vectors. We can also think of them as a construction of a new\nconcept vector based on binary input features by adding those nearly orthogonal vectors (the rows\nof the matrix) where the corresponding input dimension is non-negative, with positive and negative\nsigns as indications of the (binary) direction of the concept. From a purely technical standpoint, both\ninterpretations are valid. The latter interpretation might be more helpful if we can assign semantics to\nspecific dimensions of the input vectors (monosemanticity).\nWe can further build hierarchical concept vectors through bindings [30]. For instance, if we start with\na set of concepts that could apply to people in general, we might first bundle the present concepts\nfor two persons, respectively, which we can then bind with two different matrices $M_1, M_2$. If we\nwant to compare whether the first person in two vectors are similar, we can unbind those vectors\nusing the inverse of $M_1$ and then take the dot product. In other words, binding can be interpreted as a\npackaging step, whereas the unbinding process can be used to 'focus' on a specific package. We will\nlater outline the similarities of this interpretation to causal self attention.\nThroughout this paper, we will focus on GPT-2 [24] as a specific model architecture and instance\nof decoder-only transformers. The flow through the hidden embeddings of transformers is often\nreferred to as residual stream since the outputs of the attention and MLP layers are always added to\nthe previous outputs. This has motivated the training of sparse autoencoders (SAEs) on those hidden\nrepresentations for disentangling the embeddings into a sum of sparse feature vectors, assuming\nthat the residual stream represents many different concepts in superposition [4]. Both the successful\ntraining of SAEs and experiments on finding linear representations of concepts in the embedding\nspace are hints that transformers might store and process features using directions rather than with\nhighly dense (and somewhat arbitrary) distributed codes. However, sufficiently large SAEs could\ntheoretically approximate any embedding (e.g., by dedicating one 'feature' for every observed\ntraining input). Further, the sole observation of linear separability for a selection of concepts does not"}, {"title": "Word Embeddings", "content": "The word embeddings of GPT-2 are nearly orthogonal to each other, so we could interpret them as\nconcept vectors. They are not pure or random concept vecors, though, since related tokens do point\nto similar directions (e.g., pairs of different versions of the 'the' token have high dot products). An\nimportant design decision of GPT is that the unembedding matrix at the end is just the transpose of\nthe embedding matrix. This means that the layers need to add values to the residual stream such that\nthe combined magnitude of those additions outweigh the initial word embedding by far. The final\nstep in predicting the actual next token is essentially a determination which direction is the strongest\nsince we take the dot product with every word embedding vector and then assign model probabilities\nthrough the softmax function."}, {"title": "Attention Layer", "content": "The causal self attention mechanism in GPT-2 first computes query, key, and value vectors $v_q, v_k, v_v$\nby linearly transforming the current residual input $x$ with square matrices $W_Q, W_K, W_V$ (we will\nignore biases for now). Let us first consider the case with just one attention head for simplicity. Then,\nthe output of the attention added to the stream is determined by taking a weighted sum of all $v_v$\nvectors of the current and previous positions, followed by a linear projection $W_o$. The weights of the\nsum are based on the dot product between the query and respective value vectors, normalized with\nthe softmax function. If we assume that our learned matrices $W_Q, W_K, W_V$ are nearly orthogonal,\nwe could interpret this process as a sequence of unbinding operations ($W_Q, W_K, W_V$), followed by\nthe bundling of a selection of such unbound vectors $v$ based on their 'relevance', followed by the\nbinding of the resulting sum vector through $W_o$.\nIn other words, the attention head focuses on a specific package of possibly bundled vectors by\nrotating the input space appropriately. This package could be different for the current position\n(query) compared to previous positions (key). It then computes how similar the concepts are for the\nspecific package. The softmax function effectively acts as a (dynamic) dot product thresholding since\nonly those products that are sufficiently close to the maximum attention value will lead to non-zero\ncoefficients when taking the weighted sum of the value vectors. The resulting output is bound again\nto obtain unique concept vectors for the specific attention layer.\nProduction models have more than one attention head, but this requires only minor modifications\nto this outlined analogy. Each attention head may be interpreted as a quantized unbinding / binding\noperation, that is, each head focuses on a specific package but only cares about the first few dimensions\nof the resulting transformation (the remaining dimensions are implicitly set to zero). Using this\ninterpretation, attention heads therefore copy (and repackage) concepts into the current stream,\nsourced from specific packages of those tokens that have conceptual similarities in specific areas."}, {"title": "LayerNorm and Biases", "content": "In GPT-2, the attention and MLP layers first apply LayerNorm before their processing (the residual\nstream itself is not modified, though). LayerNorm essentially centers and normalizes the input by\nsubtracting the mean and then dividing it by the square root of the variance. It then scales the result\nwith channel-specific learnt weights and adds a bias to each channel. We can merge the latter step\n(scaling and translation) into the subsequent weights and biases of the respective layer [8]. The\ncentering and normalization depends on the actual input, though. While the normalization does not\nimpact the angle between vectors, this is not true for translations. Hence, accessing concepts in later\nlayers only works if the typically observed input vectors and intermediate embeddings have a mean\nnear zero so that distortions across layers are minimized.\nThe biases in $W_Q$, $W_K$ only add a constant factor to the dot product, which we can ignore. The\nbiases of $W_V$ can be merged into the biases of $W_o$ as the softmax attention values always add up to"}, {"title": "Processing of Concepts", "content": "The MLP blocks in GPT-2 are composed of two projections. The first projects the current embedding\nto an intermediate vector four times the size of the hidden dimension, followed by a nonlinear\nactivation function (ReLU or similar). The second and final projection transforms these results back\nto the original hidden dimension (without nonlinearities). A single neuron in the first MLP layer\ncould perform a simple check of whether the current stream contains a specific concept by simply\nperforming a weighted summation in which the weights resemble the nearly orthogonal concept\nvector. The nonlinearity enables more advanced boolean modeling. An OR conjunction would be\nmodeled by a less negative bias (presence of at least one concept contained in the weights already\ntriggers activation), whereas an AND conjunction would require a more negative bias of said neuron\n(presence of two or more concepts that make up the weights are needed). We can model the NOT\noperation by subtracting the respective concept vector from the weights so that they point to the\nopposite direction. The second layer could store the result of this operation as a new concept using a\nnearly orthogonal vector. With carefully selected thresholds (biases), the MLP block in transformers\ncould therefore model boolean functions on and using nearly orthogonal concept vectors.\nIf individual neurons represent individual concepts, we would severely limit the number of concepts\nthat are effectively added after each MLP block. However, as Vaintrob et al. [27] demonstrated for\nbinary inputs, such processing can also result in a vector that resembles the AND conjunction of\nmany more concepts in superposition. If we can find small sets of concepts that rarely co-occur, we\ncan assign OR conjunctions to neurons such that they trigger whenever at least one of the concepts in\nthe set is present. Later layers can check whether one or more concepts are actually present (AND) by\ntesting whether all nodes containing one of said concepts have fired (i.e., all related concept vectors\nare present in the stream)."}, {"title": "Experiments", "content": "The previously outlined framework is one possibility how different layers in transformers could\ncommunicate with each other and compute functions that ultimately lead to next token predictions.\nWe performed several experiments on GPT-2 (small) to gather evidence on the suitability of this\nconceptual framework for the interpretation of the inner workings. We used the TransformerLens [21]\nlibrary to extract weights and activations from GPT-2 small. We disabled all in-built post-processing\nsteps except for the merging of value biases into the projection biases of the attention output projection\n$W_o$ and the incorporation of the LayerNorm scaling and biases into the respective subsequent\nweights and biases (MLP blocks, attention blocks, and unembedding matrix). We relied on the FAISS\nlibrary [6] for an efficient implementation of dot product nearest neighbor search (HNSW algorithm)."}, {"title": "Near Orthogonality of Vectors and Matrices", "content": "Near orthogonality of vectors and matrices are an important assumption of concept vectors and\nbinding operations. We computed the matrix product $M^T M$ for word embeddings, the attention\nmatrices, and the output projections of the MLP blocks. Appendix B shows some results. In all\ncases, we observed strong diagonals with minimal artifacts, suggesting that these matrices are indeed\ncomposed of nearly orthogonal vectors. The output biases of the attention and MLP blocks are nearly\northogonal to the word embeddings. The magnitude of the attention output bias in the last layer\nis significantly higher, suggesting that this is a final cleaning step to ensure that the initial token\nembedding does not play a role anymore in the unembedding step. The mean of the word embeddings\nand intermediate hidden embeddings for a number of prompts are near zero (< 0.05), suggesting low\ndistortions of the angles of directions across layers."}, {"title": "Processing of Concepts", "content": "Earlier experiments indicated that some neurons in GPT-2 focus on very specific tokens, such as the.\nAssuming that MLP blocks perform boolean operations on concept vectors and that word embeddings\nare concept vectors, we would expect some neurons to have weights representing either a single word\nvector or a composite vector made of several word vectors (e.g., the sum of all word embeddings that\nare variations of the). To test this hypothesis, we conducted experiments.\nSpecifically, we aimed to determine if some neural weights could be explained by simple bundled\nvectors that are merely sums of word embeddings. For each possible input vector (word embedding),\nwe compiled a list of the most similar neurons. That is, we looked for neurons in the first feedforward\nlayer of the MLP block whose input weights have a reasonably high dot product with the centered\ncandidate vector. For each neuron and its list of candidates, we then greedily assembled the bundled\nvector by including those vectors (ranked by similarity) that lead to an increase of the cosine similarity\nbetween the bundled vector and the respective neural weights. We discarded vectors with a cosine\nsimilarity of less than 0.05 with the neural weight vector. We also discarded those with less than 0.1\nif they did not significantly increase the overall cosine similarity by more than 0.04. These thresholds\nrepresent hyperparameters and were conservatively chosen to obtain sets of highly relevant vectors.\nWe enforced a minimal similarity to the target vector and only considered unweighted sums since any\nvector can be trivially represented by a weighted sum of sufficiently many nearly orthogonal vectors.\nWith this simple approach, we achieve a similarity of 0.5 or higher for more than 80% of the neurons\nin the first layer, and at least 0.3 for more than 95% (this includes attributions to attention heads we\ndiscuss in the next section). For instance, our greedily obtained vector for neuron 1844 in the first\nlayer has a high cosine similarity of 0.69 with the actual weights of that neuron. The list contains\nmany first names, most of them male sounding (' Chris', ' Kevin', ' Jeff') with some exceptions ('\nRebecca'). Neuronpedia explains this as first names of people. Neuron 20 (similarity of 0.73) focuses\non tokens somehow semantically related to 'maintaining', including 'Nevertheless' and 'Storage'\n(Neuronpedia says verbs related to maintaining or keeping something). Table 1 lists more examples.\nIt is important to note that we inferred these neuron explanations directly from the weights rather than\nthrough activation observations and correlations, or by training an autoencoder or probing classifier."}, {"title": "Processing Circuits", "content": "Using the analogy of concept vectors and matrix bindings, we would expect that the MLP blocks\nperform (boolean) functions on concept vectors sourced from either the token embeddings, previous\nresults from MLP blocks (information from computations), or attention blocks (information from\nother tokens). The results are then written back to the stream via matrix binding. The sum of"}, {"title": "Related Work", "content": "Linear representations in models and embeddings have a long history in research, with Word2Vec [19]\npopularizing the idea of performing simple arithmetic on word embeddings. Numerous probing ex-\nperiments [1] have been performed to investigate linear representations in large language models [26,\n25, 12, 17, 2, 18]. Hernandez et al. [14] found evidence that relational decoding in large language\nmodels can often be approximated with an affine transformation. Wattenberg and Vegas [30] discuss\nrelational composition in context of sparse autoencoders, noting that matrix bindings bear similarities\nwith the attention process. The field of mechanistic interpretability [22] is concerned with reverse\nengineering and explaining (in parts) the inner workings of models, for instance, through discovering\nand describing circuits [13, 29], or by trying to understand whether certain behavior can be tracked\ndown to individual neurons or is represented in a more distributed way [5, 3, 7, 9, 12]."}, {"title": "Discussion and Conclusion", "content": "Our experiments indicate that GPT-2 effectively learns mechanisms that are similar to concepts of\nvector symbolic architectures. The different blocks seem to communicate by writing and reading\nnearly orthogonal vectors from the residual stream. We could also show that at least some neurons\nimplement simple boolean functions as outlined in Section 2. This has several implications on\nmodel interpretability and architectural design decisions. The presence of bundling and binding\ncircuits strengthens the validity of training sparse autoencoders for model disentanglement. However,\nsuch nearly orthogonal vectors may not necessarily map to interpretable features or independently\nunderstandable features (decomposability theory [9]). They may be part of what Engels et al. describe"}]}