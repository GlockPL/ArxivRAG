{"title": "Understanding Scaling Laws with Statistical and Approximation Theory for Transformer Neural Networks on Intrinsically Low-dimensional Data", "authors": ["Alex Havrilla", "Wenjing Liao"], "abstract": "When training deep neural networks, a model's generalization error is often observed to follow a power scaling law dependent both on the model size and the data size. Perhaps the best known example of such scaling laws are for transformer-based large language models (LLMs), where networks with billions of parameters are trained on trillions of tokens of text. Yet, despite sustained widespread interest, a rigorous understanding of why transformer scaling laws exist is still missing. To answer this question, we establish novel statistical estimation and mathematical approximation theories for transformers when the input data are concentrated on a low-dimensional manifold. Our theory predicts a power law between the generalization error and both the training data size and the network size for transformers, where the power depends on the intrinsic dimension d of the training data. Notably, the constructed model architecture is shallow, requiring only logarithmic depth in d. By leveraging low-dimensional data structures under a manifold hypothesis, we are able to explain transformer scaling laws in a way which respects the data geometry. Moreover, we test our theory with empirical observation by training LLMs on natural language datasets. We find the observed empirical scaling laws closely agree with our theoretical predictions. Taken together, these results rigorously show the intrinsic dimension of data to be a crucial quantity affecting transformer scaling laws in both theory and practice.", "sections": [{"title": "Introduction", "content": "Deep learning has made remarkable breakthroughs in various real-world applications, such as natural language processing [Graves et al., 2013, Bahdanau et al., 2014, Liu et al., 2023, Vaswani et al., 2017], computer vision [Krizhevsky et al., 2012, Goodfellow et al., 2014, Song et al., 2020], healthcare [Miotto et al., 2018], and robotics [Gu et al., 2017]. A neural scaling law between the generalization error (or test loss) and several quantities, including the model size, the training data size, and the amount of compute, plays a key role in the performance of neural networks. Perhaps the best known example of such scaling laws are for transformer-based LLMs. Recent works in Hestness et al. [2017], Rosenfeld et al. [2019], Kaplan et al. [2020], Bahri et al. [2021] demonstrated a power law between the test loss and the network size, the training data size, and the amount of compute for transformer-based LLMs. Yet, despite sustained widespread interest, a rigorous understanding of why transformer scaling laws exist is still missing.\nUnderstanding the theory behind neural scaling laws provides invaluable insights into practical applications of deep learning. A mathematical principal of neural scaling laws enables researchers and practitioners to describe and analyze the performance of neural networks with precision and rigor. The neural scaling law between the generalization error and the network size can be partially explained via neural network representation theory [Yarotsky, 2016]. Further, the neural scaling law between the generalization error and the training data size n can be explained via statistical estimation theory. For feedforward neural networks [Schmidt-Hieber, 2020] and convolutional residual networks [Oono and Suzuki, 2019], a generalization error bound has been established for regression. Schmidt-Hieber [2020], Oono and Suzuki [2019] predicted Generalization Error ~n $n^{-c/D}$ where n is the training data size, D is the data dimension and c is a constant. This predicted rate of convergence is extremely slow for high dimensional data when D is large, while the rate of convergence observed in real-world applications is significantly faster, which reveals a gap between theory and practice.\nThis gap can be bridged by exploiting low-dimensional structures of data. Real-world data sets often exhibit low-dimensional geometric structures due to rich local regularities, global symmetries, or repetitive patterns [Tenenbaum et al., 2000, Roweis and Saul, 2000]. According to Min et al. [2023, Figure 1], the intrinsic dimension of CIFAR-100, CelebA and ImageNet datasets are about 20, 20 and 40 respectively. When the low-dimensional geometric structure of data is modeled by a manifold, the predicted scaling for regression, classification and distribution estimation becomes Generalization Error ~ $n^{-c/d}$, where n is the training data size, d is the intrinsic dimension of the data manifold, and c is a constant [Chen et al., 2022, Liu et al., 2021, Dahal et al., 2022, Nakada and Imaizumi, 2020]. In Sharma and Kaplan [2022], the neural scaling law between the test loss and the network size was predicted to be Test loss ~ $(size)^{-4/d}$ where d is the intrinsic dimension of data. While the theoretical studies focus on feedforward neural networks [Chen et al., 2022, Nakada and Imaizumi, 2020] and convolutional residual networks [Liu et al., 2021], a generalization to transformer-based neural networks [Vaswani et al., 2017] is of great interest but widely open.\nThis paper establishes mathematical approximation and statistical estimation theories to predict and justify the scaling law between the generalization error and the model/data size for transformer neural networks. We consider regression of a B-H\u00f6lder continuous function f: M \u2192 R where M is a d-dimensional compact Riemannian manifold isometrically embedded in $R^D$. After embedding the input x \u2208 MCRD to a proper sequence, we apply a transformer network on the embedded sequence to learn the function f. Our main results are on the statistical estimation and universal approximation theories of H\u00f6lder continuous functions on M by transformer neural networks."}, {"title": "Statistical Theory", "content": "In Theorem 1, we consider the global empirical risk minimizer Tn from n i.i.d. training data {(xi, f(xi))}=1, given by\n$\n\\hat{T}_n = \\arg \\min_{T \\in \\mathcal{T}} \\sum_{i=1}^n (T(x_i) - f(x_i))^2,\n$\n(1)\nunder a properly chosen transformer network architecture T. We prove that, the generalization error of Tn satisfies\n$\n\\mathbb{E}||\\hat{T}_n - f||_{L^2(\\mathcal{Q})} \\leq \\tilde{O}\\left(\\frac{D d^2}{n^{\\frac{2\\beta}{2\\beta + d}}}\\right)\n$\n(2)\nwhere Q denotes the distribution of x, and \u014c hides constants and log n terms."}, {"title": "Approximation Theory", "content": "In Theorem 2, we construct a transformer network to universally approximate B-H\u00f6lder continuous functions on M with an arbitrarily given accuracy \u025b. Notably, the network is shallow, requiring only O(log(d)) independent of the desired accuracy e to approximate f locally. This highlights a major advantage of Transformers over feed-forward ReLU networks, which require O(log(1/e)) layers to achieve the same accuracy.\nIn our proof, we embed the entries of x = [$x_1$, . . ., $x_D$] \u2208 M into tokens such that the $x_i$'s appear in a sequence. Our proof for the approximation theory explicitly constructs transformers to realize the interaction between different tokens efficiently via a crucial Interaction Lemma 3. This lemma allows us to flexibly implement many common operations including addition, multiplication, and parallelization, and so may of independent interest. In our proof for the statistical theory, we calculate the covering number of our transformer network class, which is also of independent interest."}, {"title": "Neural Scaling Laws and the Intrinsic Dimension", "content": "Our generalization error bound in (2) predicts the following neural scaling law between the generalization error and the data size n:\nSquared Generalization Error := $\\mathbb{E}||\\hat{T}_n - f||_{L^2(\\mathcal{Q})} \\leq n^{-\\lambda_D}$, where $\\lambda_D = \\frac{2\\beta}{2\\beta + d}$"}, {"title": "", "content": "with sufficient data. Our approximation theory in Theorem 2 predicts the following neural scaling law between the approximation error and the network size N:\nSquared Approximation Error := $\\inf_{T \\in \\mathcal{T}} ||T - f||_{L^{\\infty}(\\mathcal{M})}^2 \\leq N^{-\\alpha_N}$, where $\\alpha_N = \\frac{2\\beta}{d}$"}, {"title": "", "content": "for a sufficiently large network class T. Our prediction of the power scaling law is consistent with our own empirical observations, and those in Kaplan et al. [2020] and Biderman et al. [2023]. More importantly, our theory quantifies the power AD, AN in terms of the intrinsic dimension of data.\nExperimental Validation on LLMs: After establishing our theory we seek to validate it in practice by predicting empirical scaling laws for LLMs trained on natural language data. To test our predictions for the data scaling law, we pretrain a series of small (125 million parameter) LLMs on three datasets [Gokaslan et al., 2019, Eldan and Li, 2023, Kocetkov et al., 2022]. We find close agreement (\u00b10.02) between our predicted scaling exponent AD and the observed exponents \u00e2p. To evaluate our predictions for the model scaling exponent an, we rely on publicly available scaling suites [Biderman et al., 2023, Radford et al., 2019] whose intrinsic data dimensions we can estimate. We find our predictions are still close but less accurate for \u03b1\u03c1. Finally, we carry out a series of ablations investigating factors impacting the estimated intrinsic data dimension d. For a fixed dataset, we find the estimated d is stable with respect to several factors including the model size, model embedding dimension, and context length\u00b9.\nIn summary, we make the following contributions:\n\u2022 A novel approximation theory for transformers approximating H\u00f6lder continuous functions on a d-dimensional manifold, requiring O(log(d)) depth independent of the accuracy \u0454.\n\u2022 A novel computation of the covering number of our transformer network class. This is used to establish generalization bounds exponentially depending on the intrinsic dimension d.\n\u2022 Empirical experiments demonstrating our theory predicts data scaling laws for LLMs as a function of the estimated intrinsic data dimension d.\n\u2022 An empirical study of several factors affecting the estimated intrinsic data dimension for transformers including model size, embedding dimension, layer depth, and context length.\nWe will present our main theory in Section 2, numerical validation of our theory and the prediction of neural scaling laws in Section 3. We will discuss related work in Section 4 and conclude our paper in Section 5. Our pre-training hyperparameters are given in Appendix A. The derivation of neural scaling laws is presented in Appendix B. Our notation is given in Appendix C, and proofs are presented in Appendix E and F."}, {"title": "Transformer Generalization and Approximation Theory", "content": "This paper establishes statistical estimation and mathematical approximation theory of transformers for the regression of H\u00f6lder functions on a low-dimensional manifold. We start by defining transformer neural networks."}, {"title": "Transformer Neural Networks", "content": "Definition 1 (Transformer Neural Network). We define a transformer neural network T as a composition of functions of the form\n$T(x) = D \\circ B_{L_T} \\circ ... \\circ B_1 \\circ (PE + E(x))$\n(5)\nwhich is parameterized by\n\u2022 LT: The number of transformer blocks Bi in T.\n\u2022\nm: The maximum number of attention heads per transformer block.\n\u2022 LFFN: The max depth of the feed-forward layers per block.\n\u2022\nWFFN: The max width of the feed-forward layers per block."}, {"title": "", "content": "\u2022 dembd: The token embedding dimension.\n\u2022 D: The input dimension.\n\u2022 1: The number of hidden tokens.\n\u2022 \u043a: A bound on the magnitude of network parameters.\nWe define each component of the composition as\n\u2022 x \u2208 $R^D$ is the input.\n\u2022 A linear embedding layer E : $R^D$ \u2192 $R^{d_{embd} \\times l}$. In this work we will always take E = E'\u00b0U where U \u2208 $R^{l \\times D}$ and E' \u2208 $R^{d_{embd} \\times 1}$ applied columnwise is fixed. We call embedded output H = E(x) the first embedding matrix whose columns are referred to as tokens.\n\u2022 PE \u2208 $R^{d_{embd} \\times l}$ is a fixed matrix implementing the transformer positional encoding.\n\u2022 Transformer blocks B\u2081 : $R^{d\\times l}$ \u2192 $R^{d\\times l}$ for i \u2208 {1, ..., LT} which are residual compositions of multi-headed attention (MHA) layers MHA and feed-forward layers FFN acting token-wise.\n\u2022 A decoding layer D : $R^{d_{embd} \\times 1}$ \u2192 R which is fixed to outputting the first element of the last column.\nWe use the ReLU activation function \u03c3(x) = max(0, x) in the network.\nFor a complete definition of the components of the transformer neural networks, we refer to Appendix D. The transformer network T may sometimes be written To which makes explicit the dependence on learnable weights 0. We can also define a class of transformer neural networks T of interest."}, {"title": "Definition 2 (Transformer Network Class)", "content": "We define a class of transformer networks as\n$\\mathcal{T}(L_T, L_{FFN}, W_{FFN}, l, d_{embd}, m, R, \\kappa) = \\{\\mathcal{T}_\\theta | \\mathcal{T}_\\theta \\text{ is in (5) with at most } m \\text{ attention heads in each block, } \\\\  L_{FFN} \\text{ layer feed-forward networks with hidden width } W_{FFN}, \\\\ d_{embd} \\text{ token dimension, } l \\text{ hidden tokens, }\\\\  \\text{ and have } ||\\mathcal{T}_\\theta||_{L^{\\infty}(\\mathbb{R}^D)} \\leq R, ||\\theta||_{\\infty} \\leq \\kappa \\}$ \nwhere $||\\mathcal{T}_\\theta||_{L^{\\infty}(\\mathbb{R}^D)} \\leq R$ bounds the output of T and $||\\theta||_{\\infty}$ bounds the weight magnitude of To."}, {"title": "Assumptions", "content": "We consider a manifold M and the target function f : M \u2192 R, with the following assumptions:"}, {"title": "Assumption 1 (Manifold)", "content": "Let M be a compact Riemannian manifold with intrinsic dimension d isometrically embedded in $R^D$. Because M is compact, there exists M > 0 such that $||x||_{\\infty} \\leq M$ for x \u2208 M. Additionally, we assume M has positive reach \u03c4 > 0."}, {"title": "Assumption 2 (Target function)", "content": "The target function f : M \u2192 R is \u03b2-H\u00f6lder continuous on M, for some 0 < \u03b2 \u2264 1 and H\u00f6lder constant Hf > 0, and in addition $||f||_{L^{\\infty}(M)} \\leq R$ for some R > 0."}, {"title": "", "content": "In Assumption 1, the reach [Federer, 1959, Aamari et al., 2019] 7 of M can be defined as\n\u03c4 = inf{r > 0 : \u2203x \u2260 y \u2208 M, v \u2208 $R^D$ such that r = $||x \u2212 v|| = ||y \u2013 v|| = inf_{z\u2208M} ||z - v||$}.\nInformally, reach is the smallest distance at which a projection onto the manifold is no longer unique. In practice this can be used to establish a bound on the number of charts covering the manifold."}, {"title": "Transformer Generalization Theory", "content": "Given n training samples {(xi, f(xi))}=1 where {$x_i$}=1 are i.i.d. samples of a distribution Q supported on M, we aim to learn an approximation Tn to f by minimizing the empirical risk (1) over"}, {"title": "Transformer Approximation Theory", "content": "Theorem 2. Let M, T, R, Hf > 0, 0 < \u03b2 < 1, d, D \u2208 N and M satisfy Assumption 1. For any \u0454 \u2208 (0, 1), there exists a transformer neural network class $\\mathcal{T}(L_T, L_{FFN}, W_{FFN}, l, d_{embd}, m, R, \\kappa)$ with parameters\nLT = O(log(d)), LFFN = O(log($\\epsilon^{-1}$)), WFFN = O(1), 1 = O($d\\epsilon^{-\\frac{d}{\\beta}}$)\ndembd = O(1), m = O($d\\epsilon^{-\\frac{d}{\\beta}}$), \u03ba = O($d^2\\epsilon^{-\\frac{2d}{\\beta}}$),\nwhere O(\u00b7) hides terms in CM, D, Hf,T, such that for any target function f satisfying As-sumption 2, if the network parameters 0 are properly chosen, then the network yields a function $T_\\theta \\in \\mathcal{T}(L_T, L_{FFN}, W_{FFN}, l, d_{embd}, m, R, \\kappa)$ with the approximation error\n$||T_\\theta - f||_{L^{\\infty} (M)} \\leq \\epsilon$\nTheorem 2 is proved in Appendix E.2. In our proof, we decompose f (x) as a sum of terms over local neighborhoods U1,...,Ucm \u2286 M covering M. Approximations on overlapping neighborhoods containing a will then be combined via a partition of unity (PoU) {$p_n$}$_1^{C_M}$ which subordinates {$U_n$}$_1^{C_M}$. This will give us the expression f(x) = $\\sum f_n(x)1_{U_n}(x)$ with $f_n = fp_n : M \u2192 R$. On each local neighborhood Un, we project the input x \u2208 M \u2286 $R^D$ to the tangent coordinate in [0, 1]d. This will give us the following local decomposition of the target function:\nf(x) = $\\sum f_n \\circ \\phi_n (x)1_{U_n} (x)$\n(7)\nwhere fn = fn\u00b01 : [0,1]d \u2192 R and \u00a2n : M \u2192 [0, 1]d is a projection onto the local tangent space. We then construct transformers to approximate the fn, On, 1Un components in (7). A diagram of the constructed transformer network approximating f : M \u2192 R is given in Figure 1. The following key lemma is used to efficiently approximate each low-dimensional function fn on d-dimensional coordinates."}, {"title": "Lemma 1", "content": "Let Hf, R > 0, d \u2208 N and 0 < \u03b2 < 1. For any \u0454 \u2208 (0,1), there exists a transformer neural network class $\\mathcal{T}(L_T, L_{FFN}, W_{FFN}, l, d_{embd}, m, R, \\kappa)$ with parameters\nLT = O(log(d)), LFFN = O(1),\nWFFN = O(1), 1 = O($d\\epsilon^{-\\frac{d}{\\beta}}$)\ndembd = O(1), m = O($d\\epsilon^{-\\frac{d}{\\beta}}$), \u03ba = O($d^2\\epsilon^{-\\frac{2d}{\\beta}}$),\nwhere O(\u00b7) hides terms in Hf, such that, for any \u1e9e-H\u00f6lder continuous function f : [0, 1]d \u2192 R, with H\u00f6lder constant no more than Hf and $|| f ||_{L^{\\infty}([0,1]^d)} < R$, if the network parameters 0 are properly chosen, this transformer network yields a function $T_\\theta \\in \\mathcal{T}(L_T, L_{FFN}, W_{FFN}, l, d_{embd}, m, R, \\kappa)$ such that\n$||T_\\theta - f ||_{L^{\\infty}([0,1]^d)} \\leq \\epsilon$.\nLemma 1 is proved in Appendix E.1. We develop a novel lemma - Interaction Lemma 3, implementing a highly-sparse pairwise interaction between two arbitrary tokens ht\u2081, ht\u2082, as a crucial architectural building block allowing us to easily implement more complex functions, architecture serialization, and parallelization (7). This result highlights a distinct advantage of transformer function approximation over ReLU function approximation [Yarotsky, 2016]: A transformer network only needs a constant O(log(d)) number of layers to approximate f : [0, 1]d \u2192 R independent of the desired accuracy e. In contrast, the depth of ReLU feed-forward networks is in the order of log(\u20ac\u207b\u00b9) [Yarotsky, 2016] This is desirable from an empirical point of view, where wider networks instead of deeper ones tend to achieve superior performance [Kaplan et al., 2020, Lee et al., 2020]."}, {"title": "Predicting Empirical Scaling Laws and Validation on LLMs", "content": "Our theory provides practical insights by predicting neural scaling laws for transformers, as given in (3) and (4), by explicitly quantifying the data scaling exponent AD and the model scaling exponent ON as a function of the intrinsic dimension (ID) d. If we assume the language modeling objective has Lipschitz regularity such that B 1 in Assumption 2, then Theorem 1 predicts the scaling law between the squared generalization error and the data size n, as given in (3), with AD = $\\frac{2}{2+d}$, and the model scaling law with exponent given by an = $\\frac{2}{d}$. For a full derivation refer to Section B. We will observe how well our theory predicts these exponents both by pretraining small models from scratch and evaluating existing open-source model suites [Biderman et al., 2023].\nIn the following, we denote ap and AN as the scaling exponents predicted by our theory, where we numerically estimate the intrinsic dimension of data, denoted by dp. The empirical exponents are"}, {"title": "", "content": "Predicting AN from AD (and vice versa) without estimating ID Above we estimated ap and AN by first estimating the intrinsic dimension d for a model's pretraining dataset. However, estimating d may not always be possible when pretraining data is not public. Alternatively, we can predict ap in terms of AN (and vice versa) without ever needing to estimate d:\n$\\lambda_D \\approx \\frac{\\lambda_N}{\\lambda_N + 1}$\n$\\lambda_N \\approx \\frac{\\lambda_D}{1-\\lambda_D}$\n(If $d=1$ and $r=0$) or $S \\approx A$ than for the sphere\nWe may have two cases\nCase 1 If Q0 = 0 then \\\\\\ 74 and (D-1)74+5, and further if 71 is good -\\7. The total\n74=7-71 +1. So with R we will always use: 7/2, 7.0.7/21 We may also consider using two sphere\npacking, or two code, to reduce both terms. If the total term does increase the packing factor the terms in A will\nbe dominated by those on B since B has d/z terms for each code while A only has 1.\nCase 2 Both are positive, i.e the 7 are similar in number - Use one sphere packing/code. Again B\nmore tightly approximates. In both cases both B > 0 and A < 0, so minimizing both is simple.\nThis completes the proof of this lemma. The construction of a partition of unity is now complete."}, {"title": "Related Work", "content": "The theoretical properties and advantages of transformers have been studied from many different perspectives [Jelassi et al., 2022, Zhang et al., 2022, Bai et al., 2023, P\u00e9rez et al., 2021, Sanford et al., 2024, Hu et al., 2024]. Most related to us are Yun et al. [2019], Edelman et al. [2022], Wei et al. [2022], Takakura and Suzuki [2023], Hu et al. [2024] in which transformers were studied from an approximation viewpoint. The work in Yun et al. [2019] proved that transformer models are universal approximators of continuous permutation equivariant sequence-to-sequence functions with compact support, while the network size suffers from the curse of dimensionality (the number of entries in the input sequence). Takakura and Suzuki [2023] studied the approximation and estimation ability of Transformers as seq-to-seq functions with infinite dimensional input, where anisotropic smoothness avoids the curse of dimensionality.\nIn the applications of Large Language Models (LLMs), empirical findings have demonstrated some correlation between the performance of transformers and the low-dimensional data structures [Razzhigaev et al., 2023, Min et al., 2023, Aghajanyan et al., 2020, Pandey, 2024]. Razzhigaev et al. [2023] investigated the intrinsic dimension of embeddings in transformer architectures, and suggested an encoder and decoder embedding property. Most similar to our work is Sharma and Kaplan [2022] which demonstrates an empirical connection between neural scaling laws and the intrinsic dimension of data. While they briefly discuss predictions for LLMs, their theory works best for predicting student-teacher model setups and image classification tasks which seem to enjoy more regularity (and a faster rate of convergence) than language modeling. Despite these empirical findings, we are not aware of any rigorous theoretical justification connecting the scaling laws of transformers with the intrinsic dimension of data. Our paper complements this line of research with statistical estimation and mathematical approximation theories which well-predict the behavior observed in practice."}, {"title": "Conclusion", "content": "Conclusion This paper establishes statistical and approximation theory results for transformers approximating H\u00f6lder continuous functions on low-dimensional data manifolds. The resulting bound on the generalization error suffers from exponential dependence only in the intrinsic dimension d. The constructed approximations of low-dimensional functions are shallow, requiring only O(log(d)) layers independent of the desired accuracy. We demonstrate this theory is accurate in practice by predicting scaling laws in both model size and data size for LLMs trained on natural language datasets. We pay careful attention to the sensitivity of the estimated intrinsic data dimension, finding it is relatively stable with respect to several relevant hyperparameters.\nLimitations and Broader Impact One important question unanswered by this work is how the intrinsic data dimension may affect the computational scaling exponent ac. Future work may investigate this direction. Additionally, our empirical experiments make the simplifying assumption that the underlying target function possesses Lipschitz regularity (\u03b2 = 1). Better estimates of the correct regularity would likely improve the accuracy of our predictions. More broadly, our work improves fundamental understanding of transformer-based LLMs and improves our ability to theoretically and safely predict future capabilities."}, {"title": "Pretraining Hyperparameters", "content": ""}, {"title": "Deriving Lanugage Model Scaling Laws from Statistical and Approximation Theory", "content": ""}, {"title": "Squared Regression Error", "content": "First we extract bounds on scaling laws in the case of regression squared error. We have the bound from the proof of Theorem 1\n$\\int_M (f(x) - \\hat{T}_n(x))^2 dQ(x) \\leq \\tilde{O}\\left( \\epsilon^2 + \\frac{D d^2 \\epsilon}{n} \\right)$\nwhere e is the approximation error such that\n$\\inf_{T \\in \\mathcal{T}} || f-T||_{L^{\\infty}(M)} < \\epsilon$\nLet the model size N of a transformer T \u2208 T be N = LT($d^2$2mbd(3m+ LFFN)) = log(d)($2\\epsilon^{-1}$5(3\u20ac\u207b# +log($\\epsilon^{-1}$))) = $\\tilde{O}(\\epsilon^{-\\frac{2d}{\\beta}})$. Write the squared generalization error as\n$L_{sq}(N, n) = \\int_M (f(x) - \\hat{T}_n(x))^2 dQ(x)$.\nThen\n$L_{sq}(N, n) \\leq \\tilde{O}\\left(\\epsilon^2 + \\frac{D d^2 \\epsilon}{n} \\right) = O\\left(N^{-\\frac{\\beta}{d}} + \\frac{D d^2 N^{\\frac{d}{\\beta}}}{n}\\right)$\nIn the model scaling regime, when data is plentiful, we have $\\frac{1}{n} << N^{-\\frac{\\beta}{d}}$ which implies the behavior of Lsq (N, n) is dominated by N-24. This gives us the model scaling exponent as\n$\\alpha_N = \\frac{2\\beta}{d}$\nFor the data scaling exponent AD we will choose N to balance both error terms. The will predict how data size and model size should scale together to achieve a minimal generalization error. We should have\n$N^{-\\frac{2\\beta}{d}} \\approx \\frac{N^{\\frac{d}{\\beta}}}{n}$"}, {"title": "From Regression to Classification", "content": "After establishing statistical and approximation theory for transformers for regression, we now seek to apply our theory to classification. In language models, the next-token prediction task is a multi-class classification problem, where transformers are trained to predict the probability of the next word out of a large dictionary.\nFor simplicity, we consider binary classification here. Let (x, y) be a random couple taking values in M \u00d7 {0, 1} with joint distribution P, where M is a manifold satisfying Assumption 1. Let Pr be the marginal distribution of x. Here x stands for the input feature and y is the corresponding label. The classification goal is to predict the label y given the value of x. A decision rule is given by a function f : M \u2192 {0,1}. The performance of a decision rule f is measured by the misclassification error\nR(f) := P(y \u2260 f(x)).\nThe Bayes decision rule has the form\nf*(x) = 1 {\u03b7(x) \u2265 1/2}\nwhere 1 denotes the indicator function and\n\u03b7(x) := P(y = 1|x)\n(8)\nis the regression function of y on x. For binary classification, the goal is to estimate the probability function \u03b7(x). If \u03b7 is a \u03b2-H\u00f6lder function satisfying Assumption 2, our approximation theory in Theorem 2 gives rise to a transformer neural network ne such that\n$||N_0 - \\eta||_{L^{\\infty} (M)} \\leq \\epsilon$\n(9)\nfor an arbitrary small \u20ac > 0.\nWe next consider the plug-in estimate of no [Audibert and Tsybakov, 2007]: fe(x) = 1 {no \u2265 1/2}. The excess risk of the plug-in estimate fe is\nE(fo) = R(fo) \u2013 R(f*)\n= P(y \u2260 fo(x)) \u2013 P(y \u2260 f*(x))\n= P(y = f*(x)) \u2013 P(y = f(x))\n= Ex [1 {f*(x) = 1} \u03b7(x) + 1 {f*(x) = 0} (1 \u2212 \u03b7(x))\n\u2212 1 {fo(x) = 1} \u03b7(x) \u2212 1 {fo(x) = 0} (1 \u2212 \u03b7(x))]\n= \u0395x [|2\u03b7(x) \u2013 1|1{fo(x) \u2260 f*(x)}].\nWe next discuss how the regression error in (9) impacts the excess risk E(f\u0259). For classification problems, the classification error depends on how well the conditional probability n(x) is estimated, and how many points are close to the decision boundary. Following Audibert and Tsybakov [2007], we assume the following margin condition: There exist cm > 0 and \u03b3 \u2265 0, such that for any t > 0,\nPx(0 < |\u03b7(x) \u2013 1/2| \u2264t) \u2264 \u0441\u043ct\u03b3.\n(10)\nA smaller \u03b3 means more samples cluster around the decision boundary with a higher likelihood of falling into either class. This is not expected to be the case for natural data, where for most samples it is easy to tell to which class they belong [Kim et al., 2019, Figure 2]. As a result, we assume \u03b3 \u2265 1.\nUnder the margin condition in (10), we follow Audibert and Tsybakov [2007] to decompose the excess risk E (fo) such that for any \u03b4 > 0:\nE(fo) = \u0395x [[2\u03b7(x) \u2013 1|1{fo(x) \u2260 f*(x)}1{|\u03b7(x) \u2013 1/2| \u2264 \u03b4}]\n+ \u0395x [[2\u03b7(x) \u2013 1|1{fo(x) \u2260 f*(x)}1{|\u03b7(x) \u2013 1/2| > \u03b4}]\n< 2\u03b4\u03a1\u03b1(|\u03b7(x) \u2013 1/2| \u2264 \u03b4) + 2Ex [|\u03b7\u03b8(x) \u2013 \u03b7(x)|1{|\u03b7\u03bf(x) \u2013 \u03b7(x)| > \u03b4}]\n< 2\u0441\u043c\u03b41+\u03b3+ 2Ex [|\u03b7\u03b8(x) \u2013 \u03b7(x)|1 {|\u03b7\u03bf(x) \u2013 \u03b7(x)| > \u03b4}] .\n(11)"}, {"title": "Cross-Entropy Based Language Model", "content": "In practice, language models are trained and evaluated using cross-entropy loss. We consider the next-token prediction in language models. Per-sample (token), language models are trained to minimize the multi-class cross-entropy loss over a large number of classes/tokens:\n$L_{CE}(n) = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{y=1}^{V} l_{y=y_i} ln(P(y|x_i))$\n(12)\nwhere V is the vocabulary size of the model (number of classes), y is the ground-truth label of xi, 1y=y is the indicator function for the event y = y, and P(y|x) is the conditional probability of labels given x. For the next-token prediction, transformers are trained to estimate the probability function P(y|x), and the next token is predicted to the word with the highest probability. The test loss is evaluated using cross-entropy on test data. We will leave the error bound on the cross-entropy loss as future work."}, {"title": "Notation", "content": "H will represent the embedding matrix of a transformer and hi will represent the ith token (column) of the embedding matrix. h will denote the jth component of the vector hi. $||\u00b7 ||_p$, p > 1"}]}