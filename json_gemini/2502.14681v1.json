{"title": "seqKAN: Sequence processing with Kolmogorov-Arnold Networks", "authors": ["Tatiana Boura", "Stasinos Konstantopoulos"], "abstract": "Kolmogorov-Arnold Networks (KANs) have been recently proposed as a machine learning framework that is more interpretable and controllable than the multi-layer perceptron. Various network architectures have been proposed within the KAN framework targeting different tasks and application domains, including sequence processing. This paper proposes seqKAN, a new KAN architecture for sequence processing. Although multiple sequence processing KAN architectures have already been proposed, we argue that seqKAN is more faithful to the core concept of the KAN framework. Furthermore, we empirically demonstrate that it achieves better results. The empirical evaluation is performed on generated data from a complex physics problem on an interpolation and an extrapolation task. Using this dataset we compared seqKAN against a prior KAN network for timeseries prediction, recurrent deep networks, and symbolic regression. seqKAN substantially outperforms all architectures, particularly on the extrapolation dataset, while also being the most transparent.", "sections": [{"title": "1 Introduction", "content": "Kolmogorov-Arnold Networks (KANs) were recently proposed by Liu et al. [5] as an alternative machine learning framework to the ubiquitous multi-layer perceptron (MLP). KANs introduce the idea that if edge weights are lifted to learnable functions, this suffices to capture non-linearities in the data so that nodes can simply sum incoming edges. This idea is inspired by the Kolmogorov-Arnold Representation Theorem (KAT) that proves that any multi-variate function can be re-formulated using two layers of uni-variate functions and simple (unweighted) summation.\nKAT has been the object of an extensive discussion regarding its relevance to machine learning, summarized by Schmidt-Hieber [6]. One notable point in this discussion is that KAT guarantees the existence of uni-variate functions that can be combined into an exact representation any multi-variate function, but offers neither a construction nor any guarantee on the properties of these uni-variate functions other than being continuous. In fact, with the exception of trivial cases, these functions are suspected to be highly non-smooth and practically impossible to construct either analytically or empirically.\nThe KAN architecture circumvents objections to KAT's relevance to machine learning by framing KANs as approximators (as opposed to KAT's exact representations) that stack two or more layers. This re-contextualization into the modern deep learning environment has mustered impressive interest from the machine learning community with more than 10 extensions and applications published within the few months since the original KAN article [1, Section 1].\nIn this paper we present seqKAN, a new architecture within the KAN framework for processing sequences. Our main contribution is that this architecture is more faithful to the core concept of the KAN framework as it avoids re-introducing weighted summation and fixed activation function in the form of conventional MLP-styled recurrency cells. A secondary contribution is the definition of a new evaluation task that has several characteristics (lacking from existing tasks) geared towards combining quantitative evaluation with a qualitative analysis of the results. In the remainder of this article we will first provide the relevant background on KANs and their recurrent extensions (Section 2) and then present seqKAN (Section 3). We will then present and discuss the evaluation task and our experimental results (Section 4) and finally close with conclusions and directions for future work (Section 5)."}, {"title": "2 Background", "content": "As mentioned above, the basic idea underlying KANs is that the activation functions that transform values as they move forward through layers are learned from the data and not pre-determined; and that each node in a layer performs simple addition (without weights) over the values it receives from each node of the preceding layer. Figure 1 gives a characteristic example of how values from a 3-node layer propagate to a 2-node layer.\nEach activation function $\\Phi_i(\u00b7)$ is the weighted sum of a learned spline and $silu(x) = x/(1+\\exp(-x))$. That is:\n$\\Phi_i(x) = w_1 silu(x) + w_2 spline (x)$\nwhere $w_1$, $w_2$ and $spline$, need to be trained for each edge of the network. The original publication explains $w_1$, $w_2$ as implementation details that make the network well-optimizable [5, p. 6]. Besides completely learned splines, activation functions can also be parameterized prior functions. In this case, the user provides univariate function implementations which the network parameterizes via affine transformations (translation and scaling) to fit the data.\nBy comparison to a conventional neural network, KANs have significantly more parameters to train. Assuming the same number of nodes and edges, a NN learns a single weight for each incoming edge of each node. A KAN, by comparison, learns $w_1$, $w_2$ and the spline parameters or the translation/scaling parameters for each edge. As a consequence, KANs are a viable alternative only when they can adequately approximate the target function with considerably fewer and narrower layers, resulting in significantly fewer edges. When seen under this light, KANs are a significantly less distributed representation than NNs. This is further emphasised by the fact that the loss function biases training towards sparsity, that is, towards having as few nodes as possible receive non-zero inputs and produce non-zero outputs. Naturally, nodes with almost-zero inputs and outputs are pruned to improve readability with minimal impact on network performance. This technique is similar to regularization in MLPs, but in KANs the result is directly interpretable.\nThe above make KANs both better interpretable and more directly controllable. Interpretability basically boils down to the fact that multiple parameters are lumped together into a spline, which can be easily visualized as a function graph and understood by the operator as a familiar function. This greatly reduces the number of different objects and interactions between them that need to be absorbed by a human operator in order to understand what has been learned. Controllability in KANs has multiple facets: from the ability to provide prior functions besides the learned ones, to the ability to replace a familiar-looking spline with a known function, to the ability to prune dependencies that appear to be overfitting or, in general, not capturing the underlying phenomenon."}, {"title": "2.2 Processing sequence data", "content": "There are two major approaches in processing sequences: Those receiving the complete sequence as input and those receiving the sequence one datapoint at a time. In the former case the system uses the position in the input vector or, in some cases, an explicit time representation to encode temporal relationships. The Transformer architecture [8] is a prime example of this approach, where attention is trained to weigh past tokens that are pertinent to the processing of the current token.\nInvestigating how the concepts of attention and KAN can be integrated can potentially give fruitful results, but the current KAN literature only includes extensions of Recurrent Neural Networks (RNN) and, in fact, of LSTM networks. The Long Short-Term Memory (LSTM) [4] and the Gated Recurrent Unit (GRU) [2] architectures are examples of the more specific family of architectures within the RNN framework where gates are trained to control the flow of information to and from a hidden state that distills the effect of past inputs on the processing of the current input.\nThe most mature among these initial approaches to utilizing KANs for sequence processing is the Temporal Kolmogorov-Arnold Network (TKAN) [3]. TKAN replaces the output layer of the LSTM cell with an array of KANs, each of which KANs itself comprises multiple KAN layers. The outputs of these KANs are combined into a single vector through trainable weights. This combined vector includes (a) a recurrent input that is fed back into both the LSTM cell and the KAN layers, and (b) the output of the TKAN layer. These layers are then stacked to form a TKAN network."}, {"title": "3 seqKAN", "content": "Our seqKAN architecture introduces recurrency directly into the KAN architecture without adding structures that rely on trained weights and fixed activation functions. This makes seqKAN more faithful to the core concept of the KAN framework, and more capable of fully exploiting the interpretability and controllability offered by KANS.\nBy contrast, the way TKAN integrates KAN layers within the LSTM cell and also uses trained weights to combine values leaves a good part of the overall TKAN architecture on the side of MLP networks and outside the scope of learned activation functions. To a large extent this undermines the value of using a KAN-based architecture: We have no reason to believe that significant pieces of the knowledge distilled from the data will not be stored in the MLP part of the network and become opaque. Furthermore, even the knowledge that is stored in the learned functions will be obfuscated by its interaction with the opaque part of the network offering no opportunities to understand or, even more so, edit and control what has been learned.\nThis goes beyond TKAN and LSTM and is a more general statement of our core motivation: The choice of activation functions in RNNs is the result of careful consideration and extensive experimentation and has proven itself effective. We have no reason to believe that replacing these activation functions with learnable functions in the same or a similar network structure can improve results. If anything, such a move introduces the overhead of fitting splines instead of using the functions that are known to work well.\nIn order to force the network to store knowledge in the less distributed, more interpretable KAN splines we should develop a KAN architecture that is performant without incorporating MLP structures. Our seqKAN architecture draws inspiration from the RNN architecture but reformulates it in a purely KAN network without trainable weights. seqKAN has one layer that receives input and maps it to a hidden state that is both pushed back to the input and sent to an output layer that computes the outputs (Figure 2).\nIn the specific configuration used in our experiments, the output layer is a [3,2] KAN layer, identical to the one shown in Figure 1. The output layer $\\Phi_o$ maps a single-variable hidden state and the two variables from the previous time-step to the two output variables. The hidden-state layer $\\Phi_h$ is a [3,1] KAN layer that maps the two input variables and the previous hidden state to the new hidden state.\nThe architectural decision to feed the previous inputs into the output layer is driven by preliminary experiments that compared this architecture against a seqKAN/wide architecture where a wider $\\Phi_h$ gave the network enough degrees of freedom to learn identity activa-"}, {"title": "4 Experimental Results and Discussion", "content": "In order to evaluate seqKAN's ability to capture temporal dependencies in multivariate time series inputs, we designed a new learning task that has the following characteristics that we have not (cumulatively) found in tasks previously used in the KAN or sequence processing literature:\n\u2022 It is a multi-variate sequence where the variables interact and affect each other (and, of course, the expected outputs) non-linearly, since the ability to disentangle and elucidate complex dependencies between the input variables is a key promise of KANs.\n\u2022 It models a non-stationary, evolving phenomenon where a superficial drift can be explained by discovering a long-term trend. Naturally, the interaction between the trend and the outputs should not be simple superposition but a non-linear composition. This tests both the ability to disentangle complex compositions as well as the ability to build models with radically different behaviours in different parts of the variables' value domains without catastrophic forgetting. This latter aspect, in particular, is meant to test the value KAN extract from fitting splines as opposed to using pre-defined functions.\n\u2022 It models a well-understood phenomenon and offers itself to an analysis that qualitatively evaluates if the learned network actually models the underlying phenomenon or has chanced upon a solution that fits the available data but is not expected to perform well in corner cases; Noting, of course, that well-understood only implies that the correct computation is known and not that it is straightforward.\nBased on the above, we built our task on data from the dynamics of a pendulum where the string gets longer as time progresses, resulting in motion captured by differential equations without closed-form solutions [9].\nMore details on the physics of the problem and the exact equations used to generate motion data are given in Appendix A. For our purposes here, it suffices to clarify that string length $L$ depends only on time. But $L$, the angular displacement $\\theta$ of the pendulum, and its derivatives angular velocity $\\omega$ and angular acceleration $\\alpha$ form a system of differential equations. This system has no closed-form solution but can be solved numerically by stepping through time from its initial parameters, which makes it appropriate as a sequence processing task since future values can be predicted from previous ones. Specifically, since the equations involve the first two derivatives of displacement, the calculation needs the second order of differences. Therefore from three time steps and $L$ one can calculate the next time step.\nHowever, there is the complication that as the sequence progresses its dynamics change because $L$ changes. By withholding $L$ from the inputs, we create a non-stationary sequence where failure to model the long-term trend will result in considerably higher loss when testing on data extracted from areas of the value space of $L$ not seen during training. In other words, this setup gives the ability to generate an interpolation test set where unseen test data can be approximated without capturing the real underlying dynamics and an extrapolation test set where such an approximation will result in high loss. We use the exponential function\n$L(t) = 0.1 \\exp\\left(10\\cdot\\frac{5.88\\cdot t}{1000}\\right)$\nto calculate the length of the pendulum string.\u00b9\nFrom each timestep we extract two binary output variables from the motion parameters: (a) whether the displacement is close to the equilibrium position and (b) whether the energy is increasing or decreasing. The exact definitions are based on lecture notes by Tedrake [7] and also given in Appendix A. Here it suffices to state that the Close to Equilibrium label only needs to test whether $\\theta$ and $\\omega$ are same-sign or opposite-sign and is straightforward to compute from the sequence whereas the Energy Increasing label is a more complex computation also involving $L$.\nSince the two labels share parts of the calculation we expect that combining these labels in a joint learning task helps networks to extract the essential underlying properties of the phenomenon instead of overfitting on a single label. For this reason we defined loss as the mean of the Binary Cross Entropy between the ground-truth value and the value calculated by the network for each of the two labels.\nWe generated a training set of 200 datapoints, where each datapoint is a sequence of ten $(\\theta, \\omega)$ pairs and the two labels extracted from the motion parameters of the final pair. Similarly, we generated an interpolation test set of 200 datapoints from the same timesteps but with a different initial displacement. Finally, we generated an extrapolation test set of 200 datapoints from timesteps subsequent to the last training datapoint. We chose the sequence length of ten to be considerably longer than the required three, since the systems also need to bring their hidden states to a point where the hidden variable $L$ is estimated from the observed displacement and velocity variables.\nTo compare the performance of our proposed model, we evaluate it against several sequential data processing models on both interpolation and extrapolation tasks. For baseline performance, we run experiments using an RNN and an LSTM. To compare with a method"}, {"title": "4.2 Results with seqKAN", "content": "As previously mentioned in Section 3, we tested two variations of the seqKAN architecture in order to establish which one performs better. The seqKAN architecture shown in Figure 2 hard-wires the previous input variables as inputs to the output layer, re-enforcing the effect of the immediately previous input beyond what can be retained in the single-variable hidden-state layer. This optimizes network size at the expense of generality, since we directly allocated learnable functions to treat the immediately previous inputs which we know to be of great relevance.\nThe more general alternative which we shall call seqKAN/wide in the results presented here, is to remove these direct inputs and instead have a wider hidden-state layer. Such a wider layer has the capacity to let two of its functions simply be $y = x$ to allow the previous input to be retained verbatim in the hidden state.\nThe quantitative results in Table 1 show that seqKAN/wide achieves a significantly lower performance, especially in the most challenging task of predicting the Energy Increasing label in extrapolated datapoints. But the great advantage that the KAN framework is the ability to analyse not only the quantitative results, but also the actual calculations that led to these results. This will give a better understanding of what is observed experimentally.\nIn our specific case, we can read in Figure 3 that the actual calculation for the Energy-Increase label is an expression along the lines of $\\omega_{t-1}+(\\Delta\\theta_t)\u00b2 + (\\Delta\\omega_t)\u00b3$, ignoring any scaling that might be encoded in the actual splines and is not visible in the figure. This was derived as follows:\n\u2022 The $\\Delta E_t$ node is the sum of a function similar to $\\omega\u00b2_{t-1}$ and a function similar to $h_t$ for larger time indexes, while the latter term is zeroed out for smaller time indexes.\n\u2022 The -$\\theta_{t-1}$ term is considered inconsequential by the network can be safely ignored. The low significance score is indicated by the very light shade in the figure. At this point it is important to remind the reader that these scores are not weights but are used to decide if a term will be included in the sum or completely removed to achieve network sparsity.\n\u2022 $h_t$ is the sum of three terms: $\\theta_t$, $\\omega_t$ and the opposite of the previous $h_{t-1}$, which in turn adds $\\theta_{t-1}$, $\\omega_{t-1}$, and the $h$ before that. By simply re-arranging the terms, this can be written as $\\Delta\\theta + \\Delta\\omega$ and some residue from previous iterations. Noting that the function applied to $h_t$ appears linear but the functions applied to $\\theta_t$ and $\\omega_t$ appear polynomial we infer that the network aims at a residue that diminishes the effect of more temporally distant datapoints. So the expession $\\Delta (\\theta_t + c_1)\u00b2 + \\Delta (\\omega_t + c_2)\u00b3$ can represent this term.\u00b2\nAs can be seen in Figure 4, there is almost perfect correlation between $\\omega_{t-1} + (\\Delta\\theta_t)\u00b2 + (\\Delta\\omega_t)\u00b3$ and the expression $(\\sin(\\theta_t) \u00b7 \\omega_t/\\Delta\\omega_t) \u00b2, which is a good approximation of the analytically-derived formula for this label (cf. Appendix A). It should be noted that the correlation holds for all the timesteps, including the timesteps beyond 200 where the model extrapolates. In other words, the model has correctly extracted the underlying drift towards ever-increasing pendulum periods as the string gets longer, despite the fact that missing this drift has minimal impact on training loss during the first 200 timesteps.\nA reasonable concern would be that we are making this statement from the advantageous position of studying a well-understood phenomenon, and in a more open-ended task we would not have the"}, {"title": "4.3 Comparative Results", "content": "In the previous section, we utilized the formulas that compute the labels of the predictive tasks to perform an analysis on the relevance of the network's findings. However, in machine learning terms, we have a dual classification task with two binary labels: whether the pendulum's energy increases or whether it is close to the equilibrium point. Thus, to compare the results across both interpolation and extrapolation tasks for different architectures we use Receiver Operating Characteristic-Area Under the Curve (ROC-AUC) and Precision-Recall-Area Under the Curve (PR-CURVE AUC) as the evaluation metrics, since they provide a global assessment of the models' performance without depending on specific thresholds. This choice aligns with our goal of demonstrating the advantages of our model as a general architecture, rather than a specific system solving a specific task. However, when comparing our model to symbolic regression, we are forced to use F1-score since symbolic regression does not output a probability distribution.\nIn comparing the performance of various sequence processing algorithms, as shown in Table 1a, all models generalize well on the interpolation task. This is a positive and expected outcome, as it suggests that all models are capable of capturing the core structure of the problem. However, with the exception of seqKAN, all models perform significantly worse on the extrapolation task, particularly when predicting the challenging Energy-Increasing label, as seen in Table 1b. seqKAN, on the other hand, maintains consistently high performance metrics, indicating that it is the only model that effectively recovers the hidden variable (string length).\nThe standard sequence processing algorithms, RNN and LSTM, perform similarly, with the LSTM exhibiting slightly better performance. This is expected, as the LSTM, with its cell state, decides which information to retain from the recent or distant past, whereas the RNN updates its history at every timestep. Specifically in our task, we know for instance that $\\Delta\\omega$ (two steps) is needed in order to make accurate predictions. The LSTM is more capable of discovering such dependencies.\nRegarding the TKAN architecture, which combines LSTM and KAN, it is interesting to note that it performs worse than the vanilla LSTM on the interpolation task but similarly on the extrapolation task. Our intuition suggests that this occurs because the addition of KANs to the LSTM's output prevents the network from overfitting, encouraging it to learn more meaningful patterns. This could explain its consistently average performance on both tasks, by comparison to LSTM's performance on the interpolated test set caving in on the extrapolated test set. However, we are unable to test this hypothesis, as TKAN is based on an implementation of the KAN layer that does"}, {"title": "5 Conclusions and Future Work", "content": "In this paper we presented seqKAN, a recurrent KAN architecture for processing sequences. seqKAN is more faithful to the core concept of the KAN framework than alternative proposals for sequence processing as it ports concepts from the RNN framework to KAN without directly integrating processing cells or other MLP-inspired structures.\nThis approach is motivated by the need to retain the interpretability and controllability offered by the less distributed nature of KAN parameters, which are diluted when part of the knowledge distilled from the data is represented in highly-distributed MLP representations.\nIn order to demonstrate this advantage, we qualitatively analysed two alternative seqKAN architectures applied on the same sequence processing task, and offered a human-understandable explanation of dependencies between the variables of the task each of the two networks learned.\nWhat is important is that this interpretability was not achieved at the expense of quantitative performance, but combined with performance superior to TKAN (an alternative architecture for KAN-based sequence processing), RNN, LSTM, and symbolic regression. The key characteristic of the evaluation task was that it was not stationary but was influenced by an underlying phenomenon represented by a variable that was not directly visible but that could be inferred from the input. From this phenomenon we extracted an interpolation test-set where the hidden variable was within the same value range as during training and an extrapolation test-set where the hidden variable was outside the range seen during training (although still observing the same natural laws). With this experimental setup, we demonstrated that the performance gap between our approach and the systems we compared against increased on the extrapolation test-set.\nHaving said that, the KAN framework (and consequently seqKAN) poses limitations on the theories that can be represented. The most obvious such limitation, as discussed in the final remark of Section 4.2, is the additive form of the network. Although KANs are inspired by the universality of the Kolmogorov-Arnold Theorem, for the reasons discussed in Section 2.1 the representation based on addition and splines is in reality an approximation. Liu et al. [5, Section 2] show that one can achieve arbitrarily low error by stacking multiple KAN layers, but such an approach weakens the clarity of the result and reduces the confidence that a real, causal dependency has been discovered.\nThis limitation shows us three directions of possible future research: mapping its extent and impact, maximizing the value we can extract despite it, and lifting it. Regarding the first, the KAN framework has attracted considerable attention in a very short time, so we are confident that a community will coalesce around applying KANs to different tasks so that empirical experience can accumulate. Our immediate plan in this respect is to also develop a Transformer-inspired KAN sequence processor and compare seqKAN and the new architecture on both generated and real-world tasks. What is important for such experiments is reading the expressions learned by the networks and reporting on the impact of this representational limitation.\nWhich relates to the second future research path, which is to develop human-computer interaction methods that can assist with the painstaking process of extracting symbolic expressions from KAN"}]}