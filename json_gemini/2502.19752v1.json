{"title": "Probabilistic Federated Prompt-Tuning with\nNon-IID and Imbalanced Data", "authors": ["Pei-Yau Weng", "Minh Hoang", "Lam M. Nguyen", "My T. Thai", "Tsui-Wei Weng", "Trong Nghia Hoang"], "abstract": "Fine-tuning pre-trained models is a popular approach in machine learning for\nsolving complex tasks with moderate data. However, fine-tuning the entire pre-\ntrained model is ineffective in federated data scenarios where local data distributions\nare diversely skewed. To address this, we explore integrating federated learning\nwith a more effective prompt-tuning method, optimizing for a small set of input\nprefixes to reprogram the pre-trained model's behavior. Our approach transforms\nfederated learning into a distributed set modeling task, aggregating diverse sets\nof prompts to globally fine-tune the pre-trained model. We benchmark various\nbaselines based on direct adaptations of existing federated model aggregation\ntechniques and introduce a new probabilistic prompt aggregation method that\nsubstantially outperforms these baselines. Our reported results on a variety of\ncomputer vision datasets confirm that the proposed method is most effective to\ncombat extreme data heterogeneity in federated learning.", "sections": [{"title": "Introduction", "content": "The proliferation of personal devices has transformed our data landscape into numerous federated\nsystems with distinct resource constraints, data representations, distributions, and ownership. This\nhas motivated the development of new machine learning (ML) paradigms that enable collaborative\nlearning across different systems while respecting their data privacy. A prominent framework to\nsubstantiate this collaborative scheme is federated learning (FL), which allows multiple systems to\ntrain a common model without sharing their private data [1-12].\nExisting FL methods often assume that learning begins from scratch and does not build on prior\nexpertise. On the other hand, fine-tuning pre-trained or foundation models [13] is an emerging\nparadigm for efficient generation of ML solutions. Almost all state-of-the-art models in natural\nlanguage processing (NLP) are now fine-tuned versions of foundation models, such as BERT [14],\nBART [15], RoBERTa [16], and T5 [17]. Likewise, many top performing vision models have also\nbenefited from the generalization capability of foundation models, such as the vision transformer\nmodel [18], which was trained on large-scale and generic datasets such as ImageNet [19]. Nonetheless,\nthis fine-tuning practice has only recently been investigated in federated learning [20, 21].\nCurrent research in this direction demonstrated various benefits of integrating fine-tuning into FL. For\nexample, fine-tuning can utilize information better in decentralized data scenarios and thus improves\nthe performance in various FL scenarios, mostly in NLP [22\u201325]. Interestingly, it has been pointed\nout that even a simple initialization of local clients with a foundation model can prevent solution drift\nto some extent in large-scale scenarios with heterogeneous local data distributions [20, 21, 23]. This\nis also consistently observed in the results of our case studies in Fig. 1."}, {"title": "", "content": "However, in FL environments that depend on frequent communication and synchronization of model\nupdates across multiple devices, fine-tuning the entire pre-trained model is often infeasible due to\nlimited local storage and communication bandwidth. Our study also reveals that full-model fine-\ntuning approaches fall short when the federated data are not only heterogeneous but also imbalanced.\nOur case studies in Fig. 1 in particular show that when the local data are both heterogeneous\nand imbalance, federated learning with full-model adaptation suffers a huge performance drop,\nhighlighting its instability in extreme data settings.\nTo address the resource bottleneck, several works in the recent federated learning literature have\nturned to a new class of parameter-efficient tuning method called prompt tuning. This method focuses\non engineering cues, such as extra tokens which are appended to the input embedding in a transformer\narchitecture. Such tokens or prompts provide beneficial context to performing the computational\ntask, similar to how hints can be provided to assist puzzle solving. Local sets of prompts can then be\naggregated or personalized using existing federated learning strategies. For example, the prior works\nof [26] and [27] use FEDAVG [2] to aggregate the local prompts while [28] integrates prompt-tuning\ninto personalized federated learning, which helps generate client-specific sets of prompts that are\nwell-customized to the corresponding local data distribution.\nAlthough federated prompt-tuning approaches eliminate the need to update and communicate hun-\ndreds of millions of network weights, there is a substantial gap between the performance of these\napproaches in highly heterogeneous, data-imbalanced settings, and the upper-bound performance\nof fine-tuning in the centralized data setting, as shown in Fig. 1. This gap can be attributed to the\nfact that locally learned prompt sets are learned in arbitrary orders, which are generally unaligned\nacross clients. That is, the same prompt position across different clients might encode different\ncontextual information about the local data. A simple aggregation that disregards prompt alignment\nmight attempt to combine prompts from different contexts and collapse into less informative prompts.\nThis has not been explicitly modeled in prior federated prompt-tuning work."}, {"title": "", "content": "To address this issue, we adopt a hierarchical probabilistic modeling approach to characterize both\nthe generation and alignment of local prompts in this paper. We cast the server aggregation step as\nan alignment of local prompt sets. This alignment discovers and aggregates local prompts encoding\nsimilar contextual information into summarizing prompts. On the client side, we view each local\nprompt set as an independent sample drawn from a generative model parameterized by the global\nsummarizing prompts. In this view, each local prompt can be seen as a probabilistic exploration\ninitialized by a randomly selected summarizing prompt. The alignment between prompts can be\ncharacterized in terms of their association with the summarizing prompts, which can be inferred via\nlearning the parameters of the above generative model. We summarize our main contributions below:\nC1. We formulate the prompt summarizing procedure as a probabilistic set modeling task in which\neach local set is assumed to be an independent sample of a random point process. The alignment of\nsimilar prompts across different local sets can be set as part of the modeling parameterization, whose\noptimization can be interleaved with the optimization of the point process's parameters (Section 3.2).\nC2. We develop an algorithm to find the most probable association between the local and summarizing\nprompts. This association is viewed as a latent variable in the above generative model. Specifically,\nwe cast its inference as a classical weighted bipartite matching task via an interesting observation that\nthe association between the summarizing prompt and each local set of prompts participate linearly to\nthe loss function of our proposed generative model (Section 3.3)."}, {"title": "", "content": "C3. We compare the performance of our method against various federated prompt-tuning baselines\nbased on existing heterogeneous federated learning techniques to demonstrate its effectiveness. Our\nreported results on a variety of experiments and baselines demonstrate consistently that our method is\nmost effective to combat data imbalance in extreme heterogeneous scenarios (Section 4)."}, {"title": "Related Work", "content": "Federated learning (FL) [1, 2] is a collaborative framework that allows multiple parties to collaborate\non training a common model without sharing their private data. In FL, the training data are distributed\nacross m clients. The t-th client owns a local, private dataset $D_t = \\{(x_{tm}, y_{tm})\\}_{m=1}^{n_t}$ comprising $n_t$\ndata points. The goal is to fit a model $w^*$ using all private datasets without centralizing them. That is,\n$w^* = \\arg\\min_w L(w) \\sum_{t=1}^m \\{\\frac{n_t}{n} L_t(w)\\}$ where $L_t(w) = \\frac{1}{n_t}\\sum_{m=1}^{n_t} l(x_{tm}, y_{tm}; w)$ (1)\nand $n = n_1 + n_2 + ... n_m$ denotes the total number of data points while $l(x_{tm}, y_{tm}; w)$ denotes\nsome loss function of choice. Clients collaborate via sharing their local models instead of data. Each\nclient can run multiple local updates before sharing its local model for aggregation. This helps reduce\nthe number of communication rounds while still preserving the convergence guarantee. [2] names\nthis the FEDAVG algorithm, which iterates between local optimization and global aggregation:\n$w_t^{(r)} = U(L_t, w^{(r-1)})$ $\\forall t \\in [m], w^{(r)} = \\sum_{t=1}^m (\\frac{n_t}{n} w_t^{(r)}).$ (2)\nThe local update routine $U(L_t, .)$ is typically standard gradient updates such as SGD or Adam. At\nthe beginning of an iteration, the local weight is set to be the global estimate from the previous\ncommunication round. In practice, local data distributions tend to diverge which consequently causes\nthe local updates in Eq. (2) to have different convergence points across clients. This is known as the\nsolution drift phenomenon, which often decreases the performance of the federated model. Numerous\napproaches had been proposed to mitigate this phenomenon, which includes the following directions:\nClient Regularization. These methods focus on augmenting the local update strategies to prevent\nclients from drifting apart. For example, FEDPROX [29] introduces an $l_2$-regularization term to\npenalize updates that diverge the local model from the global model. FEDDYN [30] uses a FL-adapted\nversion of [31] on distributed optimization as the regularizer. SCAFFOLD [32] attempts to correct\nthe direction of local gradient updates at each client using their sum of gradients. [33] utilizes the\nsimilarity between model representations to correct local training via contrastive learning.\nServer Regularization. Another approach to prevent the drifting effect caused by heterogeneous\nlocal distributions is to replace the model average in Eq. (2) with a different mechanism of model\naggregation. For example, [7, 6] decomposes client models into bags of neurons and performs a\nnon-parametric clustering of neurons. Cluster centroids are used to synthesize an aggregated model\nfor the next communication round. This approach bypasses the drifting phenomenon as the number of\ncluster is non-parametric and can be adjusted to accommodate new information patterns. In a similar\nvein, [34] adopts a probabilistic perspective of model aggregation, sampling higher-quality global\nmodels and combining them via Bayesian model ensemble, leading to a more robust aggregation.\n[35] presents a data-free knowledge distillation methods for FL that help to train generators without\ncompromising clients' data. [36] redistributes each client's shared model to others for b consecutive\ncommunication iterations, exposing it to various heterogeneous data sources before performing\naggregation, curving their solution divergence.\nPersonalization. Instead of learning a single, universal model, [29, 37\u201340] seek to learn personalized\nmodels for all clients. [3, 39] formulates this problem as multi-task learning, whereas [37] and [38]\nrespectively ground it in meta-learning and regularization frameworks. Several recent works impose\na shared data representation across clients [40\u201344], which is optimized using existing FL techniques,\nand locally fine-tune a small classification head to achieve personalization. However, personalized\nmodels trained in this manner tend to only perform well on its corresponding local test set, and not\non a comprehensive test set combining all local test data (see Section 4).\nFinally, most previous work assume that each FL client has sufficient data to adequately train its local\nmodel. This often contradicts real-world situations where data are scarce [45, 36, 46-48]. While data\nscarcity is not a new challenge of machine learning, it has not been thoroughly investigated in the"}, {"title": "", "content": "context of FL. [36] points out that with limited, scarce data, local models often have bad qualities, and\naggregating such models tend to result in poor global performance. Although fine-tuning pre-trained\nlarge models is an increasingly popular technique to combat data shortage, most existing FL works\n(including [36] which aims to address data scarcity) have not tried to leverage this resource. This\nmotivates us to investigate prompt-tuning as a new FL paradigm."}, {"title": "Probabilistic Federated Prompt-Tuning", "content": "First, Section 3.1 provides a concise background on the standard prompt tuning technique in FL\nsetting. Motivated by the result of our case study (see Fig. 1), Section 3.2 further introduces a\nprobabilistic federated prompt-tuning framework that aims to close up the performance gap between\nfederated prompt-tuning and centralized full-model fine-tuning in data imbalance settings. Our\nframework characterizes each local model as a random set of prompts distributed by a hierarchical\ngenerative model. An effective optimization algorithm to learn these is then detailed in Section 3.3.\nAn overall diagram featuring a bird-view of our framework is also provided in Fig. 2."}, {"title": "Prompt-Tuning with Pre-Trained Model", "content": "We use a pre-trained Vision Transformer [18] in all our experiments. The pre-trained model is a\ncomposition $F_c \\circ F_a \\circ F_e$ where $F_c$ is the classification head, $F_a$ is the stack of attention blocks, and\n$F_e$ is the input embedding block. Keeping both $F_a$ and $F_e$ frozen, the local solution for each client $t$\ncan be represented as $F_{c,t} \\circ F_a \\circ (F_e \\cup w)$ which applies a personalized prediction head on the output\nof the frozen attention block $F_a$ whose (set) inputs are the union of $F_e$ and a set $w$ of reprogramming\nprompts [49]. $w$ is in turn a union of $k$ individual prompts $w = w_1 \\cup w_2 \\cup ... \\cup w_k$. Each local\nsolution is characterized by a tuple of $(F_{c,t}, w)$ comprising a personalized head $F_{c,t}$ and common\nprompt set $w$. The FL framework in Eq. (1) is now re-cast as\n$w^* = \\arg\\min_w \\sum_{t=1}^m \\frac{n_t}{n} \\{L_t(w)\\}$ with $L_t(w) = \\max_{F_{c,t}, w} \\{\\frac{1}{n_t} \\sum_{m=1}^{n_t} l(x_{tm}, y_{tm}; F_{c,t}, w)\\}$ (3)\nwhere $l(x_{tm}, y_{tm}; F_{c,t}, w) = l(x_{tm}, y_{tm}; F_{c,t} \\circ F_a \\circ (F_e \\cup w))$ which makes explicit the leverage of\nthe existing (pre-trained) expertises $(F_a, F_e)$. The optimal solution $w$ of Eq. (3) can be approximated\nusing any of the existing federated learning algorithms, such as FEDAVG [2] and FEDPROX [29].\nRemark. There is an alternative approach to enabling light-weight federated fine-tuning. Instead of\nusing prompts, approaches in this direction use a (learnable) adapter network that adapts the output\nof an intermediate (frozen) neural network segment before passing it to the rest of the (frozen) neural\nnetwork. Local adapters across client can then be aggregated using FEDAVG [21]. This direction is\nhowever orthogonal to federated prompt-tuning, which is our main focus here. Further investigation\ninto probabilistic methods for adapter aggregation would be a potential follow-up of our current work."}, {"title": "Probabilistic Prompt Model", "content": "Intuitively, our federated prompt tuning framework is an analog to traditional FL on the space of\nprompts. Prior to each communication round, we suppose that each local client $t$, via prompt-tuning,"}, {"title": "", "content": "has obtained a set of $n_t$ prompts $\\varepsilon_t \\equiv \\{\\varepsilon_{t1}, \\varepsilon_{t2}, ..., w_{tn_t} \\}$. As the participating clients upload their\nlocal prompt sets, the server aggregates the combined set and subsequently returns a set of $n$ summary\nprompts, denoted as $\\Phi \\eqslantless \\{\\phi_i\\}_{i=1}^n$. We will elaborate on this aggregation step in Section 3.2.2.\nSimilar to standard FL, this aggregated set is also distributed to all clients at the beginning of the next\ncommunication round. Each client then selects the most relevant prompts from this set for further\nfine-tuning. This sampling step follows a generative process described in Section 3.2.1."}, {"title": "Generative Model", "content": "At the beginning of each communication round, each client constructs a set of $n_t$ prompt initializations\ngiven the server-broadcast summary $\\phi$. We model this construction by a random generative process.\nFirst, each local prompt initialization, $w_{tk}$, is modeled as a sample drawn from a Gaussian with\nlearnable neural parameterization,\n$w_{tk} \\sim \\mathcal{N}(\\psi_{tk}, \\text{diag} (\\alpha(\\psi_{tk};\\gamma))),$ (4)\nwhere the diagonal covariance matrix are parameterized as the output of a neural net $\\alpha$ (with weight\n$\\gamma$) on the mean parameter $\\psi_{tk}$. The set of mean parameters $\\psi_t \\equiv \\{\\phi_{tk}\\}_{k=1}^{n_t}$ is in turn modeled as a\nrandom subset of the (server-broadcast) summarizing prompts $\\phi$, following a Bernoulli point process\nprior with a finite mixture as the base measure:\n$Q_t(\\psi) \\sim \\sum_{i=1}^n c_i \\cdot \\mathbb{I} (\\psi = \\Phi_i)$, (5)\nwhere $c_i \\sim \\text{BeP} \\{ \\sigma (g(\\Phi_i; w)) \\}^{n_t}$.\nHere, $\\sigma$ denotes the sigmoid activation function, and $g$ is another deep-parameterized function\nparameterized by weight $w$. By definition of the Bernoulli process, the sampled measure is given by:\n$Q_t(\\psi) = \\sum_{i=1}^n c_i \\cdot \\mathbb{I} (\\psi = \\Phi_i),$ (6)\nwhere $c_i \\in \\{0, 1\\}$ is the outcome of a Bernoulli trial with bias $\\sigma(g(\\Phi_i; w))$, that is:\n$P_i(c_i) = \\sigma(g(\\Phi_i; w))^{c_i} \\cdot (1 - \\sigma(g(\\Phi_i; w)))^{1 - c_i}$ (7)\nIn layman term, this process simply means we will observe the result of a coin toss $c_i$ for every\nsummarizing prompt $\\Phi_i$. If this coin lands on head ($c_i = 1$), the summarizing prompt $\\Phi_i$ will be\nincluded in the set of mean parameters. Vice versa, if this coin lands on tail, we will skip $\\Phi_i$. Because\nof this, we remark that $n_t$ could be different from the number of prompts in the previous round."}, {"title": "Prompt Aggregation", "content": "Given the above generative story, we can now describe our algorithm to find the set of summarizing\nprompts $\\{\\Phi_i\\}_{i=1}^n$, which maximize the likelihood of observing local prompts $\\{\\varepsilon_t\\}_{t=1}^m$. The resulting\noptimal set of summarizing prompts can then be used to re-program the pre-trained model, which\ncorrespond to the global fine-tuned solution. Assuming that each prompt captures a particular fine-\ntuning pattern or concept, summarizing the prompt sets characterizing the local solutions will allow\nlocal models to be aggregated on the concept level, naturally mitigating the solution drift effect\ncaused by compounding impact of data heterogeneity and pre-trained weight interference. This is\nsubstantiated in the overall computation process below.\nLet $z \\in \\{0, 1\\}^{n_t}$ denotes whether the local prompt $w_{tk}$ was sampled from a Gaussian centered at\nthe $i$-th summarizing prompt $\\phi_i$. That is, the assignment variable $z_{ik} = 1$ if and only if there exists\n$k \\in [n_t]$ such that $\\psi_{tk} = \\Phi_i$. Each client $t$ can now be represented as $(\\omega_t, z_t)$ where $z_t = \\{z_{ik}\\}_{i=1}^n$,\n$z_t = (z_{t1}, z_{t2},..., z_{in_t})$ and $\\omega_t = (w_{t1}, w_{t2},..., w_{tn_t})$.\nNow, using the generative process specified in the previous section and all its defining parameters\n$\\varphi = (w, \\gamma, \\{\\Phi_i\\}_{i=1}^n)$, the log likelihood of each client can be derived as follow,\n$\\log P (\\omega_t, z_t | \\varphi) = \\log P (\\omega_t | z, \\varphi) + \\log P (z_t | \\varphi),$ (8)\nwhere each of the summand on the right-hand side is computed below. First, given $(z_t, \\varphi)$,\n$P (\\omega_t | z_t, \\varphi) = \\prod_{k=1}^{N_t} \\mathcal{N} (w_{tk} | \\psi_{tk}, \\text{diag}(\\alpha(\\psi_{tk}; \\gamma)))$ with $\\psi_{tk} = (z_{tk} \\cdot \\Phi_1 + ... + z_{tk} \\cdot \\Phi_n)$ (9)"}, {"title": "", "content": "3.  Optimization"}, {"content": "Intuitively, our federated prompt tuning framework is an analog to traditional FL on the space of\n    Now, using the generative process specified in the previous section and all its defining parameters\n    $ \\varphi = (w, \\gamma, \\{\\Phi_i\\}_{i=1}^n)$, the log likelihood of each client can be derived as follow,\n$P (z_t|\\varphi) = \\prod_{i=1}^{n} {\\sigma(g(\\phi_i;w))^{c_{ti}}}  \\times {\\prod_{i=1}^{n} {(1 -  \\sigma(g(\\phi_i;w)))^{1 - c_{ti}}}} $(10)\n    \n $ FZt; Z-t=$\n    $\\sum_{i=1}^{N}  {\\sum_{k=1}^{N_t}   {z_{tk}}} *  {log \\mathcal{N} (wtk \\phi_i, diag(\\alpha(\\phi_i; \\gamma))}$+ log \n    {$\\sum_{i=1}^{n} Cti \\log\\{\\frac{\\sigma(g(\\phi_i;w))}  {1-\\sigma(g(\\phi_i;w))}}\\}$ + G(zt)  $\\sum_{i=1}^{n} Cik  +log \\{\\frac{\\sigma(g(\\phi_i;w))}  {1-\\sigma(g(\\phi_i;w))}}\\}$ (26)\n    where G(z\\_t) denotes the summation over the remaining terms in the definition of L1 and L2 that\n    do not depend on zt, and\n$C_{ik} \\log \\mathcal{N}  +( wtk \\phi_i, diag(\\alpha(\\phi_i; \\gamma))) +\n    {\\log(\\frac{\\sigma(g(\\phi_i;w))}  {1-\\sigma(g(\\phi_i;w))}})\\}$ (27)\n    Thus, treating all but $z_t$ as constants, we can solve for the optimal value of $z\\_{-t}$ via solving the\n    corresponding weighted bipartite matching. As we iterate over $t$, optimizing for $z_t$ one at a time, this\n    results in an iterative weighted bipartite matching approach, which is described below:\n    Algorithm 2 Iterative Weighted Bipartite Matching\n    input: generative parameters $\\varphi, w, w$ and $\\phi$\n    output: optimized set of values for $z$\n    1: initialize z randomly\n    2: fort = 1 to m do\n    3: compute the cost matrix Cik using Eq. (27) and fixing z\\_{-t} as constant\n    4:$\\mathbb{Z_{t}} \\gets \\underset{\\mathbb{Z}+}  {argmax}  \\mathbb{f(Zi; Z\\_i)}$// solve the weighted bipartite matching algorithm in Eq. (27)\n    5: end for\n    6: return the optimal set of alignment $z$"}, {"title": "Experimental Results", "content": "This section presents our empirical studies on a variety of computer vision datasets, including CIFAR-\n10 and CIFAR-100 [51], TinyImageNet [52] and a synthetic, diverse dataset created by pooling\ntogether the MNIST-M [53], Fashion-MNIST [54], CINIC-10 [55] and MMAFEDB (available on\nKaggle) datasets, which is referred to as the 4-dataset.\nOur experiments are conducted on two data settings: (a) Dirichlet-based heterogeneous partition\nfollowing a previous setup in [6]; and (b) manual imbalanced data partition, as detailed below.\nHeterogeneous Partition. We partition the training data into m subsets (for m clients). Each client\nhas observations of all classes but the distributions of classes across clients are different. We simulate\nthis using a Dirichlet(\u03b1\u00b715) distribution over an s-dimensional simplex where s is the number of\nclasses and \u03b1 is the concentration parameter."}, {"title": "", "content": "Imbalance Partition. For the CIFAR-10 (10-class), CIFAR-100 (100-class) and TinyImageNet\n(200-class) datasets, the training data of each client is set to be dominated by a particular subset\nof classes, which amounts to 10% of the total number of classes. 99% of the local dataset of each\nclient is set to belong to a certain 10% of the total number of classes. For our synthetic 4-dataset, we\npartition each of the 4 sub-dataset into 20 subsets. Each subset is set so that 99% of its data points\nbelong to a single class. The remaining 1% data of each class are then pooled together and evenly\ndistributed among all clients. This amounts to 80 clients with extremely imbalance local datasets.\nThe above schemes are applied only on the train partition of each dataset. We use the default train/test\npartition for CIFAR-10, CIFAR-100, and TinyImageNet. For the 4-dataset, we sample 30K data\npoints from the default train partition of each of its 4 sub-datasets. We also sample 2.5K data points\nfrom the default test partition of each sub-dataset. This amounts to a synthetic dataset with 120K\ndata points in the train partition and 10K data points in the test partition.\nFor each data setup on each dataset, we compare the performance of our probabilistic federated\nprompt-tuning (PFPT) algorithm with those of a representative set of state-of-the-art federated\nlearning algorithms adapted to the prompt-tuning setting in Eq. (3), which include FEDAVG [2],\nFEDPROX [29], SCAFFOLD [32], FEDOPT [56] and PFEDPG [28]. PFEDPG is a personalization\nmethod that is originally measured based on how well its personalized models perform on their\ncorresponding local test sets. In this context, the performance target is however set on a global test\nset so we use PFEDPG's common model for evaluation. We will refer to these adapted baselines\nas FEDAVG-PT, FEDPROX-PT, SCAFFOLD-PT, FEDOPT-PT, and PFEDPG. In addition, we also\ncompare PFPT against a simple prompt clustering baseline, which replaces the prompt averaging of\nFEDAVG by a Gaussian Mixture Model (GMM) clustering in which the cluster centroids are returned\nas the aggregated prompts. We refer to this baseline as GMM-PT. All results are averaged over 5\nindependent runs and reported in Tables 1 to 4 below. The result of each run is evaluated on a global\ntest set, which comprises the entire test partition."}, {"title": "Non-IID Data with Locally Skewed Class Distributions", "content": "Table 1 reports the performance of PFPT and various federated prompt-tuning baselines on the\nCIFAR-10 dataset. In all three settings (i.e., heterogeneous partitions with \u03b1 = 0.5 and 0.1, as\nwell as the imbalance partition), PFPT consistently achieves the best performance. On average, the\nclassification accuracy of our method improves by 0.3% over the closest competitors, FEDAVG-PT\nand FEDPROX-PT. While this gain seems modest, we note that the prompt tuning performance\ntends to fall off more significantly on several FL frameworks that were devised to counteract the\neffect of imbalanced data, such as SCAFFOLD-PT and FEDOPT-PT. In fact, the accuracy our method"}, {"title": "", "content": "respectively improves by 2.7% and 7.0% over these baselines. This result clearly suggests that prompt\ntuning performance is more vulnerable to the heterogeneity challenge in FL, and thus requires a more\nspecialized treatment. Finally, we observe that PFEDPG achieves the worst performance among all\nbaselines (23.6% worse than PFPT on average). This performance gap is especially pronounced on\nthe imbalance setting. While PFEDPG claimed to deal with both heterogeneity and prompt-tuning,\nthis result is not surprising since it assumes that the local test sets are partitioned similarly to the local\ntraining sets, whereas our results are obtained on a holistic global test set.\nWe further repeat this experiment on the CIFAR-100 and TinyImageNet datasets, which are both\nmore challenging than CIFAR-10 due to the increased number of classes (i.e., 100 and 200 classes\nrespectively). The results of these experiments are reported in Table 2 and 3. As expected, our method\nstill consistently outperforms other baselines. However, the performance gap between PFPT and other\nmethods tends to widen proportionately to the increased difficulty of the dataset. For example, in the\nCIFAR-100 experiment, our method is 1.2% better than the second best baseline (FEDAVG-PT) and\n47.6% better than the worst baseline (PFEDPG).\nIn the TinyImageNet experiment, our method is 2.4% better than FEDAVG-PT and 50.1% better\nthan PFEDPG. These results are therefore consistent with our findings in Table 1. Finally, Table 4\nrecords the prompt tuning performance of the same baselines on the most challenging scenario,\nwhich combines 4 datasets from completely different vision domains. We observe a significant\nperformance gap between PFPT and baselines that have been keeping up with its performance in\nearlier experiments, such as FEDAVG-PT (now 20.6% worse) and FEDPROX-PT (now 20.5% worse).\nMore interestingly, GMM-PT, which has performed on par with our method in previous experiments,\nobserves a significant drop in performance (now 21.4% worse). These extremely wide performance\ngaps are due to the fact that local prompts are more diverse (to account for highly heterogeneous\ntasks), which require a more refined alignment technique to facilitate effective aggregation."}, {"title": "Non-IID Data with Globally Skewed Class Distributions", "content": "In addition to the above setting which features non-IID data and locally skewed class distribution,\nwe also evaluated the performance of our framework in a more adverse setting where the class"}, {"title": "", "content": "distribution will remain skewed even if all local datasets are aggregated. That is, local datasets in\nthis setting are non-IID draws from a long-tailed data distribution parameterized by an imbalance\nfactor IF = (maxcnc)/(mincnc) characterizing the ration between sample sizes ($n_c$) of the most\nfrequent and least frequent classes [57, 58]. Such (global) long-tailed datasets are synthetically\ncreated from the original CIFAR-100 and ImageNet datasets following the data simulation in [59].\nFollowing previous experiment setup in previous (non-ViT and no prompt-tuning) baseline methods\nFEDIC [57] and CReFF [58] 2, the resulting long-tailed datasets (CIFAR-100-LT and ImageNet-LT)\nare configured to have IF = 10, 50, 100 for CIFAR-100 and IF = 1280/5 for ImageNet. The\nreported results in Table 5 show that our method is also highly effective in this setting, achieving\nsignificantly higher accuracy than the current SOTA [57, 58] across all datasets and imbalance setups."}, {"title": "Prompt Convergence and Diversity", "content": "To generate insights regarding the learned prompts, we plot the 2-dimensional t-SNE embeddings of\nall CIFAR-100 global prompts discovered by our method across 120 communication rounds. This is\nrepeated for the two heterogeneity and one data imbalance FL settings as well as the centralized data\nsetting (Fig. 3). The yellow triangles in each plot mark the corresponding centroids of the prompt\nembeddings (one per 10 communication iterations). We also plot a best-fit spline curve (dashed and\ncolored in red) through these centroids to visualize their update trajectory. In all settings, the distance\nbetween successive centroids consistently gets smaller as training progresses, which suggests that the\nlearned prompts generally converge well. This is also corroborated by Fig. 4, which shows that the\nnumber of global prompts also converges over 120 communication iterations. It is also observed that\nin the centralized learning scenario in which no heterogeneity is present, the convergence happens\nmuch faster. The prompts quickly converge within 10 communication iterations. We also observe a\ngradual increase in the spread of the prompts with respect to their centroid. This suggests that the\nlearned prompts are optimized by our method to capture the data diversity across participating clients.\nMore interestingly, as we move from the standard heterogeneity settings (with \u03b1 = 0.1 and \u03b1 = 0."}]}