{"title": "Predicting Elevated Risk of Hospitalization Following Emergency Department Discharges", "authors": ["Dat Hong", "Philip M. Polgreen", "Alberto Maria Segre"], "abstract": "Hospitalizations that follow closely on the heels of one or more emergency department visits are often symptoms of missed opportunities to form a proper diagnosis. These diagnostic errors imply a failure to recognize the need for hospitalization and deliver appropriate care, and thus also bear important connotations for patient safety. In this paper, we show how data mining techniques can be applied to a large existing hospitalization data set to learn useful models that predict these upcoming hospitalizations with high accuracy. Specifically, we use an ensemble of logistics regression, na\u00efve Bayes and association rule classifiers to successfully predict hospitalization within 3, 7 and 14 days of an emergency department discharge. Aside from high accuracy, one of the advantages of the techniques proposed here is that the resulting classifier is easily inspected and interpreted by humans so that the learned rules can be readily operationalized. These rules can then be easily distributed and applied directly by physicians in emergency department settings to predict the risk of early admission prior to discharging their emergency department patients.", "sections": [{"title": "I. INTRODUCTION", "content": "Medical errors are an important source of morbidity and mortality [1, 2]. Thus reducing errors is an important patient safety goal and public health priority [1-3]. An increasingly recognized source of medical errors are delayed diagnoses and other diagnostic-related errors [4-7]. Delayed diagnoses and diagnostic errors can lead to lead to not only excess healthcare costs, but also negative and even catastrophic outcomes for patients [8-10]. Diagnostic delays and other diagnostic-related errors may be especially common in emergency departments given the high patient loads, fast pace of patient turnover with limited time for observation, the general lack of continuity of care, a lack of detailed information regarding the patient, and the large range of patient acuity. Indeed, analysis of malpractice claims demonstrate the frequency and importance of diagnostic delays and errors in emergency departments [11, 12].\nOne potential marker for quality of care in emergency departments is unanticipated short-term revisits [13]. Approximately 7% of patients seen in emergency departments return in 3 days and 22% within 30 days [14]. While some of those returns may be expected or unavoidable, some of the returns are associated with diagnostic and other errors such as insufficient treatment or care [13]. Thus, excess revisits have long been proposed as a marker of quality for emergency department care [15-17]. However, not all revisits are unanticipated, nor do the vast majority represent the provision of inadequate medical care [18, 19]. Some revisits may be scheduled, and some may be due to patient-related factors, such as a lack of understanding, anxiety, or progression of disease [18, 20]. An estimated 5-20% of revisits are attributable to issues related to the possibly less-than-optimal care provided at the index emergency department visit [19]. Not surprisingly, hospitalizations following discharge from the emergency room are a better marker for tracking quality of care than simply noting return ED visits without hospitalization [21, 22].\nWhile several reports have focused on risk factors associated with returns to the emergency department [23-35], fewer have focused on predicting revisits to the emergency department or predicting hospitalizations following emergency discharges [36-39]. Among the work that has been done, much has been focused older populations, pediatric populations or specific conditions, limiting their generalizability.\nGiven that hospital admissions following patients' discharges from the emergency department are undesirable, the goal of this paper was to predict patients at high risk for being admitted to the hospital at either 3, 7 or 14 days following a discharge from an emergency department using a large population-based sample over a large geographic region."}, {"title": "II. METHODS", "content": "A. Diagnostic Errors\nThere are many definitions of missed diagnoses and/or missed opportunities. In this paper we consider diagnoses that are \"missed, wrong, or delayed, as detected by some subsequent definitive test or finding\" [40]. We use a hospital admission within a fixed time window (3, 7 or 14 days) after an emergency department (ED) visit as the \u201csubsequent definitive test\u201d and select patients of interest by looking back in time from these \"index\" admissions. The 3, 7 and 14 day windows are suggested by related work on hospital readmissions [41].\nB. Dataset\nThe data used here were extracted from the 2009 Healthcare Cost and Utilization Project (HCUP) California inpatient database (SID) and emergency department database (SEDD). The former contains records of all inpatient discharges from short-term acute care non-federal-government hospitals in California, while the latter contains records of emergency department visits that do not lead to immediate hospitalizations. Each record includes the principal and secondary diagnoses, procedures performed, demographic information, length of stay, admission and discharge status, hospital charges and payment sources. California HCUP data patient SID and SEDD records can be linked across visits using an anonymized patient record index (visitlink) and simple calendrical calculations relating the implied visit dates (daystoevent), producing a comprehensive view of patient ED visits and hospitalizations across facilities and over time.\nThe 2009 California SEDD dataset contains 9,875,973 deidentified ED visits. We remove records that lack patient identifiers (these cannot be linked to the SID data) as well as records pertaining to pregnant women (a primary CCS code between 177 (spontaneous abortion) and 196 (other pregnancy and delivery including normal) for at least one ED or hospital visit) and children (age < 18), producing a final dataset for analysis consisting of 5,487,722 ED visits. Each ED visit is considered a diagnostic error if a hospital admission occurs within 7, 14 or 30 days of an ED visit, excluding hospital admissions for mental illness (an outsize number of readmissions are due to CCS codes associated with mental illness, between 650 and 670). The resulting dataset (see Figure 1) is markedly imbalanced, with only 2% of the records in the dataset having a qualifying hospital readmission within a 14-day window (and concomitantly smaller percentages for 3 and 7-day windows).\nC. Feature Selection\nEach dataset record consists of 152 features, including age, admission date, race, length of stay, patient disposition code, and so on. 102 of the features correspond to diagnoses, procedure, and injury codes in various coding formats (ICD-9 and CCS for diagnoses, CPT-4/HCPCS and CCS for procedures, and ICD-9-CM and CCS for injury codes). For this study, we retain the 21 CCS (Clinical Classification Software) coded diagnosis fields and the 21 CCS-coded procedure fields, dropping the (redundant) CPT-4/HCPCS/ICD-9 coded fields [42, 43]. The remaining 106 features (56 CCS-coded fields and 50 other features) are then dummy coded (e.g., the original admission month feature's twelve possible values would be recoded as 12 individual binary variables). Recoding features as binary dummy variables is a regression friendly strategy akin to binning when the size of the dataset is very large.\nEach record is then augmented with some additional temporal information gleaned from the context of each visit. More specifically, each record receives the following additional features:\nNumber of ED visits within last 30 days\nNumber of ED visits to same facility within last 30 days\nNumber of hospital visits within last 30 days\nNumber of hospital visits to same facility within last 30 days\nNumber of additional ED or hospital visits with same primary CCS code within last 30 days (default 0)\nThe most frequent primary CCS code within the last 30 days.\nEach record in the dataset thus contains a total of 3831 binary features.\nD. Classification Algorithms\nWe wish to learn to recognize diagnostic errors as defined above based on the features just described. Our approach uses a weighted combination, or an ensemble, of instances of three types of learning algorithms: logistic regression, na\u00efve Bayes and association rule classification.\nLogistic regression (LR) is a commonly used technique that learns an estimator for the probability of a binary response from data. A LR classifier is a weighted linear combination of terms, where each term corresponds to an input feature and the weights for each term are fit from training data. Here, LR is provided access to all 3831 dummy coded features for each record in the appropriately sized window (3, 7 or 14 day) prior to a candidate readmission and fit to predict the probability of readmission. Among the primary advantages of LR are its simplicity and direct interpretability: simple inspection reveals which features are most important, as their weights will be larger than those of other features. LR has been used extensively in prior work to predict hospital readmission [44-46] .\nNa\u00efve Bayes (NB) is another commonly used supervised learning algorithm that assumes each input feature is independent from the other inputs (the \u201cna\u00efve independence\" assumption). NB can be trained efficiently and is known to be a surprisingly good classifier [47]. Here, NB is provided the same inputs as LR and also asked to predict the probability of readmission.\nAssociation rule classification (ARC) is a heuristic algorithm that uses association rule mining to identify sufficiently ``interesting combinations of features (or itemsets) based on frequency of co-occurrence (or support). Once appropriate itemsets have been identified, a heuristic is used to generate classification rules based on a selection of itemsets. In this study, we use the Apriori algorithm [48] to generate all frequent itemsets for rule generation; an example should help make this clear.\nConsider the following simple example consisting of 7 records, each characterized by an itemset and a Boolean outcome (i.e., 0 or 1; see Table 1). Recall an itemset is a set of items $T \\subseteq I$, where $I$ is the set of all features appearing in the dataset (I = <A,B,C,D,E> in this example). A rule has the form T => x, where x is the outcome. For each such rule, we can compute a corresponding confidence, which is the probability a record whose itemset subsumes the rule's antecedent has a matching outcome, and support, which is the number of records in the dataset whose itemset subsumes the rule's antecedent itemset. In this example, the rule antecedent itemset <A, B> has support 3 (i.e., matches 3 records in the dataset); the rule <A, B> => 1 has confidence 0.66 (i.e., the outcome 1 occurs in 2 of the 3 matching records) and the rule <A, B> => 0 has confidence 0.33 (i.e., the outcome 0 occurs in 1 of the 3 matching records).\nThe algorithm proceeds in two steps: first, we construct all rules that satisfy a specified minimum support threshold, and, second, we build a classifier that applies these rules in decreasing order of support.\nRules are constructed by considering all elements in the powerset of I as possible rule antecedents. We construct a new rule for each candidate antecedent that appears in the training data by pairing it with each corresponding outcome, keeping the resulting rule only if it meets the prespecified support criteria. For the example shown in Table 1, a minimum support threshold of 2 produces the rules shown in Table 2.\nE. Ensemble Learning and Classification\nFor the work reported here, we combine the outputs of four separate classifiers (LR, NB, ARC1 and ARC2), where each classifier is learned or fit independently. The outputs of these classifiers are then used as the inputs to an additional LR classifier that computes a linear combination of these outputs to produce the final classification. The coefficients in this second stage LR are also fitted from the data.\nWe proceed as follows. First, the original data set is randomly partitioned into three disjoint subsets. The first subset, consisting of 80% of the sample, are the training data, which are used to train each of the four classifiers. These same training data are also used to empirically set the support thresholds for ARC1 and ARC2 (10 and 40, respectively). The second subset, consisting of 10% of the sample, are the validation data, which are used to train the ensemble classifier. The third subset, consisting of the remaining 10% of the sample, are the test data, which are used to evaluate the overall performance of the ensemble."}, {"title": "F. Measuring Performance", "content": "Because the data available are extremely unbalanced (only 130,705 of 9,875,973 visits, or about 1.3%, of the 7-day time window data correspond to readmissions), it is not possible to use accuracy to measure performance of any of the classifiers. Instead, we report the area under the ROC (receiver operating characteristic) curve (AUC) as a measure of discrimination, defined as the ability to correctly classify missed diagnostic opportunities."}, {"title": "III. RESULTS", "content": "Table 3 reports the results obtained by each individual classifier as well as the ensemble overall for each of the 3-day, 7-day and 14-day window cases. In the results reported here, longer windows generally correspond to better AUC values, although this need not necessarily always be the case. Note also that the ensemble method always outperforms the individual classifiers (see Figure 2).\nExamining the coefficients in the individual LR model yields some insight into which features can best be used to predict readmission. As might be expected, there is significant overlap in these important features across different time windows (3, 7 or 14 days). For example, the number of previous hospital visits tends to be an important predictor of readmission, where patients with frequent hospital visits prior to the ED visit will have a higher probability of readmission. In a similar fashion, certain CCS codes (e.g., certain types of cancers, sickle cell anemia, encephalitis, cystic fibrosis, etc.) when they appear as the primary CCS code associated with a given ED visit are also associated with higher probability of readmission, as are certain injuries (e.g., certain falls, aspirated foreign objects, open wounds, etc.). Coefficient inspection can also lend insight into coding idiosyncrasies: the procedure code for fetal monitoring was found to be associated with near-certain readmission in data that had supposedly excluded pregnant women. Closer examination revealed that 28% of ED visits coded for fetal monitoring were not also coded for pregnancy, and hence had not been removed.\nThe performance of the ARC1 and ARC2 classifiers is also quite interesting. ARC1 obtains AUC values that are comparable to NB by matching its rule antecedents against only the primary CCS codes culled from records in the appropriately sized window prior to the ED visit in question. ARC2 obtains results of nearly similar quality in terms of AUC by matching its rule antecedents against the primary and secondary CCS codes for the current ED visit alone. Furthermore, like for LR, ARC1 and ARC2 deal with directly interpretable features. By looking at the rules with high confidence and support, we know, for example, what combination of CCS codes in a given ED visit (ARC2) are associated with increased probability of readmission. For our data, a CCS code of 248 (gangrene) when co-occurring with 49 (diabetes mellitus without complication), 114 (peripheral and visceral atherosclerosis), or 211 (other connective tissue disease) is a good indicator of readmission. For ARC1's longitudinal view of primary CCS codes, visits with primary CCS codes of 50 (diabetes mellitus with complications) and 141 (other disorders of the stomach or duodenum) with either 250 (nausea and vomiting) or 251 (abdominal pain) are similarly associated with high rates of readmission (Table 4 shows top 5 association rule antecedents for both ARC1 and ARC2)."}, {"title": "IV. DISCUSSION", "content": "Our results show that we can, using only administrative data which is readily and widely available, predict, with a reasonably degree of certainty, which patients are likely to be hospitalized after leaving the emergency department at 3, 7 and 14 days. Our results compare favorably to other prediction attempts of revisits, hospitalizations or re-hospitalizations may of which have longer time horizons (e.g., 30 days) [25, 38, 49-55]. Perhaps not surprisingly, some efforts to predict hospital admissions at the time of the index emergency department visit at the time of triage (first presentation) using administrative data have been more successful, ROC = 0.85 [56]. Our approach, unlike some other machine learning approaches, is relatively interpretable and can be used to selectively help physicians and other healthcare professionals make decisions about whether to (1) admit patients instead of sending them home from the emergency department or (2) allocate resources after a patient leaves the emergency department to help prevent future hospitalizations.\nUnderstanding the transition between emergency department visits and hospitalizations is extremely important given that over 80% of all unscheduled hospital admissions in the United States originate from emergency department visits [57]. In fact, the proportion of admissions originating from the ED has increased dramatically over the last several years [57]. The increase in emergency room use has led to overcrowding of emergency departments which in turn affects quality of care and patient outcomes [58]. To prevent revisits and bounce backs from emergency departments, multiple approaches have been applied, including telephone calls following patients discharged to home by nurse navigators and attempts to schedule primary care visits for patients discharged to home. What is needed is an approach to allocate such post-discharge interventions effectively. Many patients in the emergency department are not likely to need such interventions. We believe our approach, by targeting patients that are likely not only to return, but get admitted to the hospital, provides a promising target for quality and patient satisfaction initiatives. And while many projects investigate revisits to the emergency department, these projects are descriptive, and focus on identifying risk factors rather than making predictions.\nOur work can also be extended into a decision-support system. Given that the data we use is generally available, our outcome results, based on association rules, are easy to interpret. While the claims data are not available at the time of decision to discharge, all of the information, in theory, that is used to generate the discharge data is available. In future work, we need to validate our results in other large geographic regions (e.g., states other than California) and with other years. Such work is underway. Ultimately, our decision-support work could help physicians make more informed choices about patients on the margin (i.e., patients who for whom it is not immediately clear if they should be admitted to the hospital or sent home. Patients who are discharged to home but are flagged as high risk for a future admission could be targeted for close follow up with a visit to their primary care, a call from a nurse or pharmacist. Indeed, not all patients, even if they are told they are likely to be admitted in the future, may wish to be admitted, and some may prefer a trial of care at home, prior to an admission [41].\nOur paper has several limitations. First, we use administrative data exclusively and do not include specific observable data that may be important to determine patient severity when presetting to an emergency department (e.g. vital signs, medications, triage assessment from notes). Second, our data are only for the state of California. It is possible, but unlikely, that patients seen in California emergency departments are admitted to a hospital in another state, and we would not know about this. Third, some information that may be important for predicting visits to the emergency department are not in our data, for example, health literacy or language difficulties (e.g., patients for whom English is not their primary language) [59]. In addition, we have limited information about the hospitals and emergency departments, and institutional values like teaching status and size may affect patient outcomes. Some of this information could be added to our model by linking our data to information from, for example, the American Hospital Association database. Fourth, we did not focus on admissions to the hospital in more than 14 days. Historically, some quality metrics look at revisits to emergency departments within 72 hours, although there is no empirical evidence for looking at this period [14]. A 7-day period may be a more reasonable period for measuring quality of care in emergency departments [60], because revisits to an emergency department within 7 days are most likely to be related to the same health problem as the index visit [30]. For this project we chose 7 and 14 days based on a work that identified revisits at 9 days following an initial emergency department based on a \"time-to-return-curve\" analysis for identifying potentially avoidable re-visits to the emergency department [14]. Finally, one can always learn from larger datasets, or use more features (recall only CCS codes were used in ARC1 and ARC2) when training the underlying learning algorithms.\nDespite these limitations, the results from this pilot study demonstrate that using only a large administrative database, we can develop models that can help predict which patients, after leaving the emergency department, are most likely to be admitted to a hospital either within a 3-, 7- or 14-day period. This approach can be used to allocate scarce resources such as calls from nurse navigators and pharmacists. However, it can also be used to investigate new quality metrics and ultimately inform the building of diagnostic support tools to automatically flag high-risk patients. Finally, because our approach, unlike some machine-learning approaches, which operate like a \u201cblack box\", leads to associate rules that are easy to interpret, we may learn of novel risk factors and combinations of factors, accounting for the ordering of events that would be much more difficult to discover using traditional epidemiological methods."}]}