{"title": "Generative Predictive Control:\nFlow Matching Policies for Dynamic and Difficult-to-Demonstrate Tasks", "authors": ["Vince Kurtz", "Joel W. Burdick"], "abstract": "Generative control policies have recently unlocked\nmajor progress in robotics. These methods produce action\nsequences via diffusion or flow matching, with training data\nprovided by demonstrations. But despite enjoying considerable\nsuccess on difficult manipulation problems, generative policies\ncome with two key limitations. First, behavior cloning requires\nexpert demonstrations, which can be time-consuming and\nexpensive to obtain. Second, existing methods are limited to\nrelatively slow, quasi-static tasks. In this paper, we leverage\na tight connection between sampling-based predictive control\nand generative modeling to address each of these issues. In\nparticular, we introduce generative predictive control, a super-\nvised learning framework for tasks with fast dynamics that are\neasy to simulate but difficult to demonstrate. We then show how\ntrained flow-matching policies can be warm-started at run-time,\nmaintaining temporal consistency and enabling fast feedback\nrates. We believe that generative predictive control offers a\ncomplementary approach to existing behavior cloning methods,\nand hope that it paves the way toward generalist policies that\nextend beyond quasi-static demonstration-oriented tasks.", "sections": [{"title": "I. INTRODUCTION AND RELATED WORK", "content": "Diffusion and flow matching policies have enabled tremen-\ndous success in behavior cloning for quasi-static dexterous\nmanipulation tasks [1]-[4]. Can generative policies also\ncontrol systems with fast nonlinear dynamics at high control\nfrequencies, where demonstrations are difficult to come by?\nWe answer this question in the affirmative by introducing\nGenerative Predictive Control (GPC), a supervised learning\nframework for dynamic and difficult to demonstrate tasks.\nThe basic idea behind GPC is summarized in Fig. 1.\nGPC alternates between data collection via Sampling-based\nPredictive Control (SPC) and policy training via flow match-\ning. The flow matching model bootstraps SPC, allowing for\ncontinual performance improvements while maintaining a\nsupervised (e.g., regression) training objective."}, {"title": "A. Generative Policies for Behavior Cloning", "content": "Generative models like diffusion [1] and flow matching\n[2] have recently gained prominence as powerful policy rep-\nresentations for robotics tasks. These models typically focus\non behavior cloning [3], [4], where expert demonstrations\nserve as training data.\nThese models offer a few key advantages over other policy\nrepresentations. Diffusion and flow-matching maintain a high\ndegree of multi-modal expressiveness, allowing for multiple\n\"paths\" to achieve the same task [1]. They are also able\nto consider high-dimensional image observations, which are\nubiquitous in modern robotics [5]-[7]."}, {"title": "B. Sampling-based Predictive Control", "content": "At the same time, a very different trend has been gain-\ning traction in the nonlinear optimal control community.\nSampling-based Predictive Control (SPC) is an alternative\nto gradient-based Model Predictive Control (MPC), where a\nsimple sampling procedure is used in place of a sophisticated\nnonlinear optimizer [13].\nProminent algorithms in this family include Model Predic-\ntive Path Integral Control (MPPI) [14], Predictive Sampling\n(PS) [18], and Cross-Entropy Methods (CEM) [19]. SPC\nmethods are exceedingly easy to implement, and have been\nstudied for a long time. But recent advances in computing\nspeed and parallel simulation [15]-[17] have enabled them\nto scale to complex problems like in-hand dexterous manipu-\nlation [13], legged locomotion [20] and more [18], [21]. On\ncertain tasks, SPC methods offer performance competitive\nwith state-of-the-art reinforcement learning algorithms, even\nwithout any offline policy training [13].\nSPC is particularly appealing in the context of contact\nrich robotics tasks, where the stiff dynamics of contact\npresent significant challenges to general-purpose non-convex\noptimizers [22]\u2013[25]. Even the simplest SPC algorithms\ncan outperform specialized contact-implicit optimizers [18],\nthanks to the fact that they do not require expensive gradients\nthrough contact, and the fact that randomization smoothes\nout contact-related discontinuities [26], [27].\nIn many ways, SPC is complementary to generative be-\nhavior cloning: it is agnostic to a robot's morphology, and\ncan be particularly effective on tasks with fast nonlinear\ndynamics. Behavior cloning, on the other hand, has shown\nparticular success on tasks involving deformable objects like\ncloth and food [1]-[4], which are difficult to simulate at\nspeeds sufficient for SPC."}, {"title": "C. Contribution", "content": "In this paper, we highlight a surprisingly deep connection\ngenerative policies and SPC algorithms. This connection was\nfirst identified by [28] in the context of MPPI: here we extend\nthis connection to a general class of SPC algorithms.\nWe then leverage this connection to develop a supervised\nlearning framework for dynamic and difficult-to-demonstrate\ntasks, which we call Generative Predictive Control (GPC).\nWe show how flow-matching policies can be warm-started\nto encourage temporal consistency and smooth action se-\nquences, and demonstrate how these warm-starts are critical\nfor effective performance on systems with fast dynamics.\nWe provide an open-source GPC implementation in\nJAX/MJX [15], which we use to push the scalability limits of\nour proposed approach. We train GPC policies on systems\nranging from a simple inverted pendulum to a 29 degree-\nof-freedom (DoF) humanoid robot. Our largest and most\ndifficult task (humanoid standup) exposes the limits of this\napproach: bootstrapping SPC with a trained policy improves\nperformance, but the GPC policy alone is not sufficient.\nWith this in mind, we provide an extensive discussion of\nthe limitations of GPC and directions for future work to\novercome these limitations in Section VIII. These include\nvalue function learning, better exploration strategies, and\nmore efficient use of non-optimal samples."}, {"title": "II. BACKGROUND", "content": "In this paper we consider optimal control problems of the\nstandard form"}, {"title": "A. Problem Formulation", "content": "$\n\\begin{aligned}\n\\min _{u_{0}, u_{1}, \\ldots, u_{T}} & \\Phi\\left(x_{T+1}\\right)+\\sum_{\\tau=0}^{T} l\\left(x_{\\tau}, u_{\\tau}\\right), & & (1 a)\\\\\ns.t. & x_{\\tau+1}=f\\left(x_{\\tau}, u_{\\tau}\\right), & & (1 b)\\\\\n& x_{0}=x_{\\text {init }}, & & (1 c)\n\\end{aligned}\n$\nwhere $x_{\\tau} \\in \\mathbb{R}^{n}$ represents the system state at time step $\\tau$,\n$u_{\\tau} \\in \\mathbb{R}^{m}$ are control actions, $l: \\mathbb{R}^{n} \\times \\mathbb{R}^{m} \\rightarrow \\mathbb{R}$ and $\\Phi :$\n$\\mathbb{R}^{n} \\rightarrow \\mathbb{R}$ are running and terminal costs, and $f : \\mathbb{R}^{n} \\times \\mathbb{R}^{m} \\rightarrow$\n$\\mathbb{R}^{n}$ captures the system dynamics.\nFor ease of notation, we denote the action sequence as\n$\\mathcal{U}=\\left[u_{0}, u_{1}, \\ldots, u_{T}\\right]$ and rewrite (1) in compact form as\n$\\min _{\\mathcal{U}} J\\left(\\mathcal{U} ; x_{\\text {init }}\\right),$\nwhere both the costs and dynamics constraints are wrapped\ninto the (highly non-convex) objective $J : \\mathbb{R}^{m \\times T} \\times \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$.\nWe will approach this control problem from a Model\nPredictive Control (MPC) perspective. Rather than attempt-\ning to find an optimal closed-form state-feedback policy\n$u_{\\tau}=\\kappa(x)$, we seek numerical methods to rapidly (and if\nneeded, approximately) solve for the action sequence $\\mathcal{U}$ from\na given initial condition $x_{\\text {init }}$. At run time, we continually\nsolve from current state estimate, applying only $u_{0}$ before\nre-planning as quickly as possible."}, {"title": "B. Sampling-based Predictive Control", "content": "MPC methods traditionally solve (1) with gradient-based\nnon-convex optimization. But these techniques face signifi-\ncant challenges, particularly when it comes to the stiff and\nhighly nonlinear contact dynamics essential for contact-rich\nrobot locomotion and manipulation tasks [29].\nIn response to these challenges, Sampling-based Predic-\ntive Control (SPC) is gaining prominence as a simple and\ncomputationally efficient alternative to gradient-based MPC\n[13], [14], [20], [30], [31]. Instead of relying on the com-\nplex machinery of nonlinear optimization, SPC algorithms\nperform some variation of the following simple procedure:\n$\\bullet$\nAt step $k$, sample $N$ candidate action sequences\n$\\mathcal{U}^{(i)} \\sim \\mathcal{N}\\left(\\overline{\\mathcal{U}}_{k-1}, \\sigma^{2} I\\right), i \\in[1, N],$\ntypically from a Gaussian proposal distribution\u00b9.\n$\\bullet$\nRoll out each action sequence from the latest state\nestimate $x_{k-1}$, recording the costs\n$J^{(i)} \\gets J\\left(\\mathcal{U}^{(i)} ; x_{k-1}\\right)$.\n$\\bullet$\nUse the costs to update the mean action sequence,\n$\\overline{\\mathcal{U}}_{k} \\gets \\overline{\\mathcal{U}}_{k-1}+\\frac{\\sum_{i=1}^{N} g\\left(J^{(i)}\\right)\\left(\\mathcal{U}^{(i)}-\\overline{\\mathcal{U}}_{k-1}\\right)}{\\sum_{i=1}^{N} g\\left(J^{(i)}\\right)},$"}, {"title": "C. Generative Modeling", "content": "Flow matching generative models [7]\nfocus on a seemingly very different problem: produce a\nsample $x$ from a probability distribution $p(x)$. In the typical\nproblem setting, we do not have access to $p(x)$ in closed\nform, but we do have samples from this data distribution.\nThe basic idea is to consider a probability density path\n$p_{t}(x)$. This path flows from an easy-to-sample proposal\ndistribution $p_{0}(x)=\\mathcal{N}(0, I)$ at $t=0$ to the data distribution\nat $t=1$. Flow matching generative models learn a time-\nvarying vector field $v_{\\theta}(x, t)$ that moves samples from $p_{0}(x)$\nto $p_{1}(x)$, where $\\theta$ denote learnable parameters.\nThe flow matching network $v_{\\theta}$ is trained via standard\n(stochastic) gradient descent methods on\n$\\min _{\\theta} \\mathbb{E}_{t, x_{0}, x_{1}}\\left[\\mathcal{L}_{F M}\\left(\\theta ; x_{0}, x_{1}, t\\right)\\right],$\nwhere\n$\\mathcal{L}_{F M}\\left(\\theta ; x_{0}, x_{1}, t\\right)=\n\\left\\|v_{\\theta}\\left(t x_{1}+(1-t) x_{0}, t\\right)-\\left(x_{1}-x_{0}\\right)\\right\\|^{2}$\nis the flow matching loss. The expectation in (10) taken over\n$t \\sim \\mathcal{U}(0,1), x_{0} \\sim p_{0}(x)$, and $x_{1} \\sim p_{1}(x)$. Each of these are\neasy to acquire, since we already have data points $x_{1}$ and\n$p_{0}(x)=\\mathcal{N}(0, I)$ is easy to sample from. Intuitively, $\\mathcal{L}_{F M}$\nattempts to push samples in straight line from $x_{0}$ to $x_{1}$.\nAt inference time, we generate new samples by starting\nwith $x \\sim p_{0}(x)$ and simulating the Ordinary Differential\nEquation (ODE)\n$\\dot{x}=v_{\\theta}(x, t)$.\nAny off-the-shelf ODE solver can be used for this purpose,\nbut in many cases a simple forward Euler scheme\n$x_{t+\\delta t}=x_{t}+\\delta t v_{\\theta}\\left(x_{t}, t\\right),$\nis sufficiently performant."}, {"title": "1) Flow Matching:", "content": "Diffusion models estimate the score\n$s_{\\theta}(x, \\sigma) \\approx \\nabla_{x} \\log p_{\\sigma}(x)$\nat various noise levels by learning to remove noise added\nto the original data [5], [6]. Once we have a trained score\nnetwork, we can use it sample from $p_{\\sigma}$ using Langevin\ndynamics\n$x \\gets x+\\epsilon s_{\\theta}(x, \\sigma)+\\sqrt{2 \\epsilon} z \\quad z \\sim \\mathcal{N}(0, I),$\nwith step size $\\epsilon>0$. By gradually reducing $\\sigma$, we arrive at\nsamples from the data distribution $p(x)$."}, {"title": "2) Diffusion:", "content": "SPC IS ONLINE GENERATIVE MODELING\nIn this section, we establish a formal connection between\nSPC and generative modeling. Specifically, we show that the\nSPC update (5) is a monte-carlo estimate of the score of a\nnoised target distribution. This connection was first identified\nfor the case of MPPI in [28] and used to develop Dial-MPC,\na multi-stage SPC algorithm for legged locomotion, in [20].\nHere we extend this connection to generic SPC algorithms\nwith updates of the form (5).\nFirst, we define a target distribution conditioned on the\nintial state x:\n$p(\\mathcal{U} \\mid x) \\propto g(J(\\mathcal{U} ; x)),$\nwhich is determined by the algorithm-specific weighting\nfunction $g(\\cdot)$ introduced in Sec. II-B above. In the spirit\nof score-based diffusion (Sec. II-C.2) we define the noised\ntarget distribution\n$p_{\\sigma}(\\mathcal{U} \\mid x) \\propto \\mathbb{E}_{\\widetilde{\\mathcal{U}} \\sim \\mathcal{N}\\left(\\mathcal{U}, \\sigma^{2} I\\right)}[g(\\widetilde{\\mathcal{U}})].$\nIt turns out the score of this noised target is directly related\nto the SPC update (5):"}, {"title": "III.", "content": "IV. GENERATIVE PREDICTIVE CONTROL\nThe previous section shows that we can think of the mean\nof the SPC sampling distribution, $\\overline{\\mathcal{U}}_{k}$, as a sample from a\nstate-conditioned optimal action distribution\n$\\overline{\\mathcal{U}}_{k} \\sim p(\\mathcal{U} \\mid x_{k}) \\propto g(J(\\mathcal{U} ; x_{k})).$\nThis leads to a natural question: can we train a generative\nmodel to produce $\\overline{\\mathcal{U}}_{k}$? In addition to imitating the SPC\nupdate process, such a generative model\n$p_{\\theta}(\\mathcal{U} \\mid x_{k}) \\approx p(\\mathcal{U} \\mid x_{k}),$\nparameterized by network weights $\\theta$, would maintain a\nsimilar structure to the flow matching and diffusion models\nused in behavior cloning [1]\u2013[4]."}, {"title": "Proposition 1.", "content": "The score of the noised target distribution\n(18) is given by\n$\\nabla_{\\mathcal{U}} \\log p_{\\sigma}(\\mathcal{U} \\mid x)=\\frac{1}{\\sigma^{2}} \\frac{\\mathbb{E}_{\\widetilde{\\mathcal{U}} \\sim \\mathcal{N}\\left(\\mathcal{U}, \\sigma^{2} I\\right)}[g(\\widetilde{\\mathcal{U}})(\\widetilde{\\mathcal{U}}-\\mathcal{U})]}{\\mathbb{E}_{\\widetilde{\\mathcal{U}} \\sim \\mathcal{N}\\left(\\mathcal{U}, \\sigma^{2} I\\right)}[g(\\widetilde{\\mathcal{U}})]}.$\nProof. For simplicity of notation, we drop the conditioning\non x and write the target distribution as $p_{\\sigma}(\\mathcal{U})$. We also\ndenote the normal density as $\\mathcal{N}(\\widetilde{\\mathcal{U}} ; \\mathcal{U}, \\sigma^{2})=\\mathfrak{q}_{\\mathcal{U}}(\\widetilde{\\mathcal{U}})$.\nThe score is given by\n$\\nabla_{\\mathcal{U}} \\log p_{\\sigma}(\\mathcal{U})=\\frac{\\nabla_{\\mathcal{U}} p_{\\sigma}(\\mathcal{U})}{p_{\\sigma}(\\mathcal{U})}.$\nIn the numerator we have\n$\\nabla_{\\mathcal{U}} p_{\\sigma}(\\mathcal{U})=\\nabla_{\\mathcal{U}} \\int \\mathfrak{q}_{\\mathcal{U}}(\\widetilde{\\mathcal{U}}) g(\\widetilde{\\mathcal{U}}) d \\widetilde{\\mathcal{U}}$\n$=\\int \\nabla_{\\mathcal{U}} \\mathfrak{q}_{\\mathcal{U}}(\\widetilde{\\mathcal{U}}) g(\\widetilde{\\mathcal{U}}) d \\widetilde{\\mathcal{U}}$\n$=\\int \\mathfrak{q}_{\\mathcal{U}}(\\widetilde{\\mathcal{U}}) \\nabla_{\\mathcal{U}} \\log \\mathfrak{q}_{\\mathcal{U}}(\\widetilde{\\mathcal{U}}) g(\\widetilde{\\mathcal{U}}) d \\widetilde{\\mathcal{U}}$\n$=\\frac{1}{\\sigma^{2}} \\mathbb{E}_{\\widetilde{\\mathcal{U}} \\sim \\mathcal{N}\\left(\\mathcal{U}, \\sigma^{2} I\\right)}[g(\\widetilde{\\mathcal{U}})(\\widetilde{\\mathcal{U}}-\\mathcal{U})],$\nwhere $\\eta$ is a normalizing constant and we use the fact that\n$\\nabla_{\\mathcal{U}} \\log \\mathfrak{q}_{\\mathcal{U}}(\\widetilde{\\mathcal{U}})=(\\widetilde{\\mathcal{U}}-\\mathcal{U}) / \\sigma^{2}$.\nBringing $1 / \\sigma^{2}$ outside the expectation, we have\n$\\frac{\\nabla_{\\mathcal{U}} p_{\\sigma}(\\mathcal{U})}{p_{\\sigma}(\\mathcal{U})}=\\frac{\\mathbb{E}_{\\widetilde{\\mathcal{U}} \\sim \\mathcal{N}\\left(\\mathcal{U}, \\sigma^{2} I\\right)}[g(\\widetilde{\\mathcal{U}})(\\widetilde{\\mathcal{U}}-\\mathcal{U})]}{\\sigma^{2} \\mathbb{E}_{\\widetilde{\\mathcal{U}} \\sim \\mathcal{N}\\left(\\mathcal{U}, \\sigma^{2} I\\right)}[g(\\widetilde{\\mathcal{U}})]}$\nand thus the proposition holds.\nIn particular, this means that the SPC update (5) is nothing\nmore than a Monte-Carlo estimate of score ascent, e.g.,"}, {"title": "V. USING A GENERATIVE POLICY", "content": "This is the basic idea behind GPC. We use data $(\\overline{\\mathcal{U}}_{k}, x_{k})$\nfrom running SPC in simulation to train a flow matching\n$\\overline{\\mathcal{U}}_{k} \\gets \\overline{\\mathcal{U}}_{k-1}+\\sigma^{2} \\nabla_{\\overline{\\mathcal{U}}_{k-1}} \\log p_{\\sigma}\\left(\\overline{\\mathcal{U}}_{k-1} \\mid x_{k-1}\\right)$\n$\\sigma^{2} \\nabla_{\\mathcal{U}} \\log p_{\\sigma}(\\mathcal{U} \\mid x) \\approx \\frac{\\sum_{i=1}^{N} g\\left(J\\left(\\mathcal{U}^{(i)} ; x\\right)\\right)\\left(\\mathcal{U}^{(i)}-\\mathcal{U}\\right)}{\\sum_{i=1}^{N} g\\left(J\\left(\\mathcal{U}^{(i)} ; x\\right)\\right)},$\nwhere $\\mathcal{U}^{(i)} \\sim \\mathcal{N}\\left(\\mathcal{U}, \\sigma^{2} I\\right)."}, {"title": "A. Warm-starts and Temporal Consistency", "content": "A key challenge in applying generative policies of the\nform $p_{\\theta}(\\mathcal{U} \\mid x)$ is the issue of temporal consistency [3]. In\nreal-time feedback control, the multi-modal expressiveness\nof generative models presents a challenge: samples at subse-\nquent timesteps can be drawn from different modes, leading\nto \"jittering\" behavior.\nThe most common solution is to merely roll out several\nsteps of the action sequence before replanning. This forces\nthe controller to \"commit\" to a particular mode, but is not\nsuitable for highly dynamic tasks that require rapid feedback.\nOther alternatives include averaging over samples produced\nat different timesteps [3], but this does not always work well\nin practice [2].\nWe propose a simple alternative inspired by warm-starts in\nMPC. Rather than starting the flow generation process from\n$\\mathcal{U}_{0} \\sim \\mathcal{N}(0, I)$, we start from\n$\\mathcal{U}_{0}=(1-\\alpha) \\mathcal{N}(0, I)+\\alpha \\overline{\\mathcal{U}}_{k-1}$\nwhere $\\alpha \\in[0,1]$ is the warm-start level. With $\\alpha=1$, the\nflow process is started from the previous sample $\\overline{\\mathcal{U}}_{k-1}$, while\n$\\alpha=0$ recovers the original normal proposal distribution.\nBecause flow matching essentially defines a vector field\ndriving samples toward a mode of the sampling distribution,\nflows with a high warm-start level $\\alpha$ tend to stay close to\nthe same mode as the previous sample $\\overline{\\mathcal{U}}_{k-1}$."}, {"title": "B. Seeding Sampling-based Predictive Control", "content": "Another possible use of a trained GPC policy is to add\nsamples $\\mathcal{U} \\sim p_{\\theta}(\\mathcal{U} \\mid x)$ from the trained flow matching\nmodel to bootstrap an SPC controller. This also alleviates\nconcerns with mode switching and temporal consistency, as\nmode switches only occur if the switch results in a lower\ncost, as characterized by the SPC weighting function $g(\\cdot)$.\nSeeding SPC with a flow matching model also alleviates\nmany of the uni-modality concerns often raised regarding\nSPC algorithms [36], and can allow for effective performance\nwith fewer samples and therefore faster control loops. This\napproach also opens the door to continual improvement after\ndeployment, as these samples can be used to further refine\nthe flow matching model as in Algorithm. 1.\nWe found that seeding SPC offered significantly better\nperformance than direct flow matching on the largest ex-\namples we tested, as detailed in Sec. VII below. While\nSPC is computationally cheap, this improved performance\ndoes come with some limitations: a full state-estimate is\nrequired to perform the SPC rollouts, and conditioning on\nobservations $y=h(x)$ directly is no longer feasible."}, {"title": "VI. DOMAIN RANDOMIZATION STRATEGIES", "content": "Domain Randomization (DR) has emerged as a key in-\ngredient in enabling sim-to-real transfer of policies trained\nin simulation, particularly for reinforcement learning [37],\n[38]. It is reasonable to expect that DR also has a critical\nrole to play in sim-to-real transfer of GPC policies.\nFortunately, the availability of massively parallel simula-\ntors and the structure of the SPC/GPC paradigm enables a\nrange of new DR possibilities. In particular, we can modify\nthe SPC rollouts (4) by simulating each action sequence $\\mathcal{U}^{(i)}$\nin several domains with randomized parameters (e.g., friction\ncoefficients, body masses, etc.). This results in cost values\nindexed by both sample $i$ and domain $d$, e.g.,\n$J^{(i, d)}=J(\\mathcal{U}^{(i)} ; x_{k-1, d}).$\nWe then aggregate this cost data across domains before\nperforming the standard SPC update (5) on the net costs.\nThe simplest choice would be take the average cost over\nall domains,\n$\\overline{J}^{(i)}=\\mathbb{E}_{d}\\left[J^{(i, d)}\\right].$\nThis is analogous to the typical RL domain randomization\nframework, which considers the expected reward over all\ndomains.\nBut the SPC/GPC framework allows for other possibilities\nas well. We can, for instance, take a more conservative\napproach and use the worst-case cost\n$\\overline{J}^{(i)}=\\max _{d}\\left[J^{(i, d)}\\right].$"}, {"title": "VII. SIMULATION EXPERIMENTS", "content": "In this section, we present the results from simulation\nexperiments on each of the systems shown in Fig. 3. We\naim to answer the following questions:\n1) Can GPC perform tasks that require multi-modal rea-\nsoning, as well highly dynamic tasks that require high-\nfrequency feedback (Sec. VII-B)?\n2) Can GPC continually improve its performance over\nmultiple iterations (Sec. VII-C)?\n3) How do different domain randomization strategies im-\npact performance with model error (Sec. VII-D)?\n4) What are the scalability limits of this approach\n(Sec. VII-E)?\nWe find that GPC is effective for control systems with\nfast dynamics at high feedback rates, enjoys the training\nstability characteristic of supervised learning methods, and\nenables risk-aware control, but struggles to scale to our\nlargest and most difficult example (humanoid standup). We\nprovide further discussion of these scalability limitations, and\nhow they might be overcome in the future, in Section VIII."}, {"title": "A. Example Systems", "content": "We test GPC on seven example systems of varying scale\nand difficulty. Each of these systems are introduced briefly\nbelow, with further details available in the open-source\nimplementation [34].\nInverted pendulum: This simple one-dimensional system\nrequires swinging a pendulum to the upright position and\nbalancing it there. Torque limits prevent the pendulum from\nswinging directly upright: the policy must gradually pump\nenergy into the system.\nCart-pole: An unactuated pendulum is mounted on an\nactuated cart. Control actions are torques applied to the cart,\nand the task is to balance the pendulum upright. While this\nis a relatively simple nonlinear system, obtaining successful\nhuman demonstrations would be difficult.\nDouble cart-pole: In this extension of the cart-pole ex-\nample, an unactuated double pendulum is mounted on the\ncart. The fast and chaotic double pendulum dynamics make\nthis task particularly challenging.\nPush-T: A robotic finger pushes a T-shaped block to a\ntarget position and orientation on a table. This task has been\nsolved with behavior cloning [1], and is a standard example\nof a task that requires multi-modal reasoning.\nPlanar biped: A robot walker, constrained to the sagittal\nplane, is tasked with moving forward at 1.5 m/s. The high\ndimensionality of this system would make teleoperation\nfor behavior cloning difficult. Successful locomotion also\nrequires fast replanning.\nLuffing crane: A swinging payload is attached via a\nrope to a luffing crane. Control actions are target boom\nangles and rope length. This underactuated system provides\na particularly useful testbed for investigating the impact of\nmodeling errors and domain randomization strategies.\nHumanoid standup: A Unitree G1 humanoid model is\ntasked with reaching a standing configuration. Initial con-\nditions are random joint angles, joint velocities, and base\norientation, so that the robot begins sprawled on the ground."}, {"title": "B. Policy Performance", "content": "Footage of closed-loop GPC performance on each of the\nexamples is shown in the accompanying video. In all the\nexamples, the GPC policy was effective in improving closed-\nloop performance when used in conjunction with SPC, as\ndescribed in Sec. V-B. Applying the GPC policy directly\n(Sec. V-A) was effective in all cases except the humanoid.\nGPC can handle multi-modal action distributions. This\nis evidenced by effectiveness on the Push-T example, which\nrequires multi-modal reasoning to reach around the block\n[1]. Interestingly, GPC training takes under 20 minutes,\nwhile training a similar Push-T policy with diffusion-based\nbehavior cloning (and full state observations) takes around\nan hour.\nMore importantly, GPC can control systems with fast\ndynamics at high control rates. The double cart-pole\nexample provides a particularly clear example of this, as well\nas the importance of warm-starts (Sec. V-A). Fig. 4 illustrates\nperformance with and without warm-starts. Without warm-\nstarts (left, $\\alpha=0$ ), the control actions are dominated by\nsignificant noise (top plot) and the system is unable to swing\nupright (bottom plot). With warm-starts (right, $\\alpha=1$ ), we\nobtain smoother control actions and successfully balance\naround the upright position. The controller is still able to\nrespond rapidly to the chaotic system dynamics, as evidenced\nby the rapid changes between 1 and 2 seconds."}, {"title": "C. Training Stability", "content": "Training curves for each of the examples are shown in\nFig. 5. In addition to the average loss and average cost at\n\\begin{aligned}\n\\mathcal{L}_{G P C}\\left(\\theta ; \\mathcal{U}_{0}, \\overline{\\mathcal{U}}_{k}, \\mathcal{U}_{k-1}, x_{k}, t\\right)=\n& \\omega\\left(\\overline{\\mathcal{U}}_{k}, \\overline{\\mathcal{U}}_{k-1}, \\mathcal{U}_{0}\\right) \\mathcal{L}_{C F M}\\left(\\theta ; \\mathcal{U}_{0} ; \\overline{\\mathcal{U}}_{k}, x_{k}, t\\right),\n\\end{aligned}\n$\\omega\\left(\\overline{\\mathcal{U}}_{k}, \\overline{\\mathcal{U}}_{k-1}, \\mathcal{U}_{0}\\right)=\n\\exp \\left(-\\gamma\\left(1-\\mathcal{S}_{c}\\left(\\overline{\\mathcal{U}}_{k}-\\overline{\\mathcal{U}}_{k-1}, \\overline{\\mathcal{U}}_{k}-\\mathcal{U}_{0}\\right)\\right)\\right),$"}, {"title": "D. Domain Randomization", "content": "GPC offers an opportunity to train with risk-sensitive\ndomain randomization schemes, as outlined in Sec. VI.\nWe use the luffing crane example to explore the impact of\ndifferent strategies on the performance of a trained policy.\nIn particular, we train three GPC policies: one with\nno domain randomization, one with average-cost domain\nrandomization (37), and one with more conservative CVaR\n$(\\beta=0.25)$ domain randomization (39). For the domain\nrandomized models, we used 8 randomized domains. Each\ndomain uses a slightly different model with modified joint\ndamping, payload mass, payload inertia, and actuator gains.\nAfter training, we apply each policy directly with warm-\nstarts (Sec. V-A). To evaluate closed-loop performance, we\ngenerate 50 random target locations that the payload must\nvisit. The target is moved to the next location once the pay-\nload is moved within 15 cm or after 10 seconds, whichever\ncomes first. We report the total time to visit all 50 targets in"}, {"title": "VIII. LIMITATIONS AND FUTURE WORK", "content": "Limited effectiveness on our largest (humanoid standup)\nexample is the most severe limitation of our proposed\nmethod", "21": "represents action sequences with a simple\nzero-order-hold spline parameterization. Higher-order splines\n[18", "44": [46], "framework": "this is particularly the case\nwhen it comes to domain randomization strategies. This will\nalso provide an important platform for exploring policies\nconditioned on various observations, ranging from raw sensor\ndata to images to foundation model embeddings like DINO\n["}]}