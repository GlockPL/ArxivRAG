{"title": "LLMS KNOW MORE THAN THEY SHOW: ON THE INTRINSIC REPRESENTATION OF LLM HALLUCINATIONS", "authors": ["Hadas Orgad", "Michael Toker", "Zorik Gekhman", "Roi Reichart", "Idan Szpektor", "Hadas Kotek", "Yonatan Belinkov"], "abstract": "Large language models (LLMs) often produce errors, including factual inaccuracies, biases, and reasoning failures, collectively referred to as \u201challucinations\". Recent studies have demonstrated that LLMs' internal states encode information regarding the truthfulness of their outputs, and that this information can be utilized to detect errors. In this work, we show that the internal representations of LLMs encode much more information about truthfulness than previously recognized. We first discover that the truthfulness information is concentrated in specific tokens, and leveraging this property significantly enhances error detection performance. Yet, we show that such error detectors fail to generalize across datasets, implying that-contrary to prior claims-truthfulness encoding is not universal but rather multifaceted. Next, we show that internal representations can also be used for predicting the types of errors the model is likely to make, facilitating the development of tailored mitigation strategies. Lastly, we reveal a discrepancy between LLMs' internal encoding and external behavior: they may encode the correct answer, yet consistently generate an incorrect one. Taken together, these insights deepen our understanding of LLM errors from the model's internal perspective, which can guide future research on enhancing error analysis and mitigation.", "sections": [{"title": "INTRODUCTION", "content": "The ever-growing popularity of large language models (LLM) across many domains has brought a significant limitation to center stage: their tendency to \"hallucinate\u201d \u2013 which is often used to describe the generation of inaccurate information. But what are hallucinations, and what causes them? A considerable body of research has sought to define, taxonomize, and understand hallucinations through extrinsic, behavioral analysis, primarily examining how users perceive such errors (Bang et al., 2023; Ji et al., 2023; Huang et al., 2023a; Rawte et al., 2023). However, this approach does not adequately address how these errors are encoded within the LLMs. Alternatively, another line of work has explored the internal representations of LLMs, suggesting that LLMs encode signals of truthfulness (Kadavath et al., 2022; Li et al., 2024; Chen et al., 2024, inter alia). However, these analyses were typically restricted to detecting errors-determining whether a generated output contains inaccuracies\u2014without delving deeper into how such signals are represented and could be leveraged to understand or mitigate hallucinations.\nIn this work, we reveal that the internal representations of LLMs encode much more information about truthfulness than previously recognized. Through a series of experiments, we train classifiers on these internal representations to predict various features related to the truthfulness of generated outputs. Our findings reveal the patterns and types of information encoded in model representations, linking this intrinsic data to extrinsic LLM behavior. This enhances our ability to detect errors (while understanding the limitations of error detection), and may guide the development of more nuanced strategies based on error types and mitigation methods that make use of the model's internal knowledge. Our experiments are designed to be general, covering a broad array of LLM limitations. While the term \"hallucinations\u201d is widely used, it lacks a universally accepted definition (Venkit et al., 2024). Our framework adopts a broad interpretation, considering hallucinations to encompass"}, {"title": "BACKGROUND", "content": "Defining and characterizing LLM errors. The term \"hallucinations\" is widely used across various subfields such as conversational AI (Liu et al., 2022), abstractive summarization (Zhang et al., 2019), and machine translation (Wang & Sennrich, 2020), each interpreting the term differently. Yet, no consensus exists on defining hallucinations: Venkit et al. (2024) identified 31 distinct frameworks for conceptualizing hallucinations, revealing the diversity of perspectives. Research efforts aim to define and taxonomize hallucinations, distinguishing them from other error types (Liu et al., 2022; Ji et al., 2023; Huang et al., 2023a; Rawte et al., 2023). On the other hand, recent scholarly conversations introduce terms like \u201cconfabulations\" (Millidge, 2023) and \"fabrications\" (McGowan et al., 2023), attributing a possible \u201cintention\" to LLMs, although the notions of LLM \u201cintention\" and other human-like traits are still debated (Salles et al., 2020; Serapio-Garc\u00eda et al., 2023; Harnad, 2024). These categorizations, however, adopt a human-centric view by focusing on the subjective"}, {"title": "BETTER ERROR DETECTION", "content": "This section presents our experiments on detecting LLM errors through their own computations, focusing on token selection's impact and introducing a method that outperforms other approaches."}, {"title": "TASK DEFINITION", "content": "Given an LLM M, an input prompt p and the LLM-generated response \u0177, the task is to predict whether \u0177 is correct or wrong. We assume that there is access to the LLM's internal states (i.e., white-box setting), but no access to any external resources (e.g., search engine or additional LLMs).\nWe use a dataset D = {(qi, Yi)}N=1, consisting of N question-label pairs, where {qi}N=1 represents a series of questions (e.g., \u201cWhat is the capital of Connecticut?", "Hartford\"). For each question qi, we prompt the model M to generate a response yi, resulting in the set of predicted answers {\u0177i}N=1 (\u201cThe capital of Connecticut is Hartford...\"). Next, to build our error-detection dataset, we evaluate the correctness of each generated\"\n    },\n    {\n      \"title\": \"EXPERIMENTAL SETUP\",\n      \"content\": \"Datasets and models. We perform all experiments on four LLMs: Mistral-7b (Jiang et al., 2023), Mistral-7b-instruct-v0.2 (denoted Mistral-7b-instruct), Llama3-8b (Touvron et al., 2023), and Llama3-8b-instruct. We consider 10 different datasets spanning various domains and tasks: TriviaQA (Joshi et al., 2017), HotpotQA with/without context (Yang et al., 2018), Natural Questions (Kwiatkowski et al., 2019), Winobias (Zhao et al., 2018), Winogrande (Sakaguchi et al., 2021), MNLI (Williams et al., 2018), Math (Sun et al., 2024), IMDB review sentiment analysis (Maas et al., 2011), and a dataset of movie roles (movies) that we curate. We allow unrestricted response generation to mimic real-world LLM usage, with answers decoded greedily. For more details on the datasets and the prompts used to generate answers, refer to Appendix A.3.\nPerformance metric. We measure the area under the ROC curve to evaluate error detectors, providing a single metric that reflects their ability to distinguish between positive and negative cases across many thresholds, balancing sensitivity (true positive rate) and specificity (false positive rate).\nError detection methods. We compare methods from both uncertainty and hallucinations literature.\n\u2022 Majority: Always predicts the most frequent label in the training data.\n\u2022 Aggregated probabilities / logits: Previous studies (Guerreiro et al., 2023; Kadavath et al., 2022; Varshney et al., 2023; Huang et al., 2023b) aggregate output token probabilities or logits to score LLM confidence for error detection. We implement several methods from the literature, calculating the minimum, maximum, or mean of these values. The main paper reports results for the most common approach, Logits-mean, and the best-performing one, Logits-min, with additional baselines in Appendix B.\n\u2022 P(True): Kadavath et al. (2022) showed that LLMs are relatively calibrated when asked to evaluate the correctness of their generation via prompting. We implement this evaluation using the same prompt.\n\u2022 Probing: Probing classifiers involve training a small classifier on a model's intermediate activations to predict features of processed text (Belinkov, 2021). Recent studies show their effectiveness for error detection in generated text (Kadavath et al., 2022, inter alia). An intermediate activation is a vector h\u0131,t from a specific LLM layer l and (either read or generated) token t. Thus, each LLM generation produces multiple such activations. Following prior work, we use a linear probing classifier for error detection (Li et al., 2024, inter alia) on static tokens: the last generated token (h\u0131,\u22121), the one before it (h1, \u20132), and the final prompt token (h\u0131,k). The layer l is selected per token based on validation set performance.\"\n    },\n    {\n      \"title\": \"EXACT ANSWER TOKENS\",\n      \"content\": \"Existing methods often overlook a critical nuance: the token selection for error detection, typically focusing on the last generated token or taking a mean. However, since LLMs typically generate long-form responses, this practice may miss crucial details (Brunner et al., 2020). Other approaches use the last token of the prompt (Slobodkin et al., 2023, inter alia), but this is inherently inaccurate due to LLMs' unidirectional nature, failing to account for the generated response and missing cases where different sampled answers from the same model vary in correctness. We investigate a previously unexamined token location: the exact answer tokens, which represent the most meaningful parts of the generated response. We define exact answer tokens as those whose modification alters the answer's correctness, disregarding subsequent generated content.\"\n    },\n    {\n      \"title\": \"RESULTS\",\n      \"content\": \"Patterns of truthfulness encoding. We first focus on probing classifiers to gain insights into the internal representations of LLMs. Specifically, we extensively analyze the effects of layer and token selection on activation extraction for these classifiers. This is done by systematically probing all layers of the model, starting with the last question token and continuing through to the final generated token. While some datasets seem easier for error prediction, all exhibit consistent truthfulness encoding patterns. Middle to later layers typically yield the most effective probing results, aligning with previous studies (Burns et al., 2022; CH-Wang et al., 2023). Regarding tokens, a\"\n    },\n    {\n      \"title\": \"GENERALIZATION BETWEEN TASKS\",\n      \"content\": \"The effectiveness of a probing classifier in detecting errors suggests that LLMs encode information about the truthfulness of their outputs. This supports using probing classifiers for error detection in production, but their generalizability across tasks remains unclear. While some studies argue for a universal mechanism of truthfulness encoding in LLMs (Marks & Tegmark, 2023; Slobodkin et al., 2023), results on probe generalization across datasets are mixed (Kadavath et al., 2022; Marks & Tegmark, 2023; CH-Wang et al., 2023; Slobodkin et al., 2023; Levinstein & Herrmann, 2024). Understanding this is essential for real-world applications, where the error detector may encounter examples that significantly differ from those it was trained on. Therefore, we explore whether a probe trained on one dataset can detect errors in others.\nOur generalization experiments are conducted between all of the datasets discussed in Section 3, covering a broader range of non-synthetic settings than previous work. We select the optimal token and layer combination for each dataset, train all probes using this combination on other datasets, and then test them on the original dataset. We evaluate generalization performance using the absolute AUC score, defined as max(auc, 1 \u2013 auc), to also account for cases where the learned signal in one dataset is reversed in another.\"\n    },\n    {\n      \"title\": \"INVESTIGATING ERROR TYPES\",\n      \"content\": \"Having established the limitations of error detection, we now shift to error analysis. Previously, we explored types of LLM limitations across different tasks, noting both commonalities and distinctions in their error representations. In this section, we focus on the types of errors LLMs make in a specific task\u2014TriviaQA\u2014which represents factual errors, a commonly studied issue in LLMs (Kadavath et al., 2022; Snyder et al., 2023; Li et al., 2024; Chen et al., 2024; Simhi et al., 2024).\"\n    },\n    {\n      \"title\": \"TAXONOMY OF ERRORS\",\n      \"content\": \"Intuitively, not all mistakes are identical. In one case, an LLM may consistently generate an incorrect answer, considering it correct, while in another case, it could issue a best guess. To analyze errors\"\n    },\n    {\n      \"title\": \"PREDICTING ERROR TYPES\",\n      \"content\": \"Our taxonomy offers an external, behavioral analysis of LLMs, which we complement by an intrinsic evaluation. We explore whether LLMs encode information on potential error types within their intermediate activations, offering a deeper insight into the underlying mechanisms. To investigate\"\n    },\n    {\n      \"title\": \"DETECTING THE CORRECT ANSWER\",\n      \"content\": \"After identifying that models encode diverse truthfulness-related information, we examine how this internal truthfulness aligns with their external behavior during response generation. To this end, we use our probe, trained on error detection, to select an answer from a pool of 30 generated responses to the same question. We then measure the model's accuracy based on the selected answers. A case where this accuracy does not significantly differ from traditional decoding methods (such as greedy decoding), suggests that the LLM's internal representation of truthfulness is consistent with its external behavior. In simpler terms, that the model is generating answers that it also internally considers as correct. Conversely, a case where using the probe alters performance either way, would suggest a misalignment between the LLM's internal representations and its actual behavior.\nExperimental Setup The experiments were conducted on TriviaQA, Winobias, and Math. We resample each model answer in the same strategy described in Section 5.1. The final chosen answer is the one with the highest correctness probability, as assessed by the probe. We compare to three baselines: (1) greedy decoding, (2) random selection from the K = 30 answer candidates; and (3) majority vote wherein the most frequently generated answer is chosen.\nResults The results for Mistral-7b-instruct are summarized in Figure 5, with additional results for other LLMs and datasets as well as qualitative examples provided in Appendix E. We only present results on error types that appear 30 times or more in our test dataset. Overall, using the probe to select answers enhances the LLMs accuracy across all examined tasks. However, the extent of improvement varies by error type. For instance, in the TriviaQA dataset, there is minimal gain in the \\\"mostly correct\\\" category (B2). In contrast, substantial gains-ranging from 30 to 40 points in some cases are observed in the \u201cmostly incorrect": "C2), \u201ctwo competing answers\u201d (D), and \u201cmany answers\" (E1) categories. Interestingly, and perhaps surprisingly, the probe is most effective in cases where the LLM lacks any (external) preference for the correct answer during generation. The fact that the probe can effectively identify the correct answer in these scenarios, points at a significant disconnect between the LLM's internal encoding and its external behavior. These results suggest that even when the model encodes information of which answer is correct, it can still generate an incorrect answer in practice.\nWhile using the probe to select the answer proves effective, it is not proposed here as an error mitigation strategy but rather as a diagnostic tool. However, these findings indicate that further research in this area could leverage the existing knowledge within LLMs to significantly reduce errors. We recommend exploring this direction in future investigations."}, {"title": "DISCUSSION AND CONCLUSIONS", "content": "In this study, we analyzed the errors of LLMs by examining their internal representations. We discover that information related to truthfulness is localized within the exact answer tokens, a critical insight that supports various analyses throughout the paper, as using the tokens with the strongest signals leads to more reliable conclusions. From a practical perspective, this finding significantly enhances error detection methods applicable to production-level LLMs. However, our approach requires access to internal representations, limiting its applicability mainly to open-source models.\nOur findings also suggest that truthfulness features generalize poorly across tasks and datasets, performing better in tasks requiring similar skills. This indicates that truthfulness may depend on \u201cskill-specific\" features, like knowledge retrieval and contextual consistency. Some generalization patterns remain unexplained, suggesting that additional features link seemingly unrelated tasks, warranting further research. Since the best layer and token combination for error detection varies by dataset, analyzing their effect on generalization may uncover insights into these mechanisms. Moreover, Our results highlight the caution needed when applying a trained error detector across different settings.\nWe also found that the intermediate representations of LLMs can be used to predict the types of errors they might make. As Simhi et al. (2024) noted, different error types may require distinct mitigation strategies, such as retrieval-augmented generation (Lewis et al., 2020) in some cases or fine-tuning in others. Identifying these error types is useful for customizing these strategies.\nLastly, we identified a significant discrepancy between the model's external behavior and internal states, where a model generates an incorrect response repeatedly even though its internal representations encode the correct answer. It is possible that mechanisms favoring likelihood override those promoting truthfulness, as LLMs are trained to predicting likely tokens, which does not necessarily align with factual accuracy. Our findings imply that these models already encode valuable information that could possibly be harnessed to reduce errors. Work by Chuang et al. (2024) shows improvements in truthfulness by leveraging knowledge encoded in earlier transformer layers, pointing to a promising area for further investigation.\nIn conclusion, our findings suggest that LLMs' internal representations provide useful insights into their errors, highlights the complex link between the internal processes of models and their external outputs, and hopefully paves the way for further improvements in error detection and mitigation."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "To ensure reproducibility of our work, we provide detailed instructions and necessary code. The source code, including scripts for generating model answers, probing, resampling, and error type analysis, is available in the supplementary material, where we also provide command examples and specific seeds used for experiment reproducibility. This repository includes documentation on how to set up the environment, download and preprocess datasets, and execute the experiments outlined in Sections 3-6 of the paper. Additionally, all datasets, models, and results generation steps are described in the Appendix A."}, {"title": "IMPLEMENTATION DETAILS", "content": "In this work, we specifically address errors produced by modern large language models (LLMs). Given the diverse range of tasks these models are applied to, our focus is on general error detection across all categories, rather than isolating specific types. Prior to the emergence of LLMs, much research targeted error detection for specific tasks, with common examples including grammatical errors (Kasewa et al., 2018; Bell et al., 2019; Cheng & Duan, 2020; Wang & Tan, 2020; Flickinger et al., 2016), spelling mistakes (Mishra & Kaur, 2013), machine translation inaccuracies (Lo, 2019;\nWe examine the intermediate representations of the exact answer tokens generated by a large language model (LLM) during the answer generation process. The intermediate representation selected for this analysis is derived from the output of the final multi-layer perceptron (MLP). This choice is based on preliminary experiments comparing the MLP output, the residual stream, and the attention heads, which showed no significant differences. We leave the in-depth analysis for future work. For the probing classifier, we employ a logistic regression model from the scikit-learn library (Pedregosa et al., 2011).\nObtaining correctness label for the probing dataset. An answer is generally considered correct if it includes the correct answer label and appears before any alternative incorrect labels. We manually analyzed the results of this heuristic to confirm that it is accurate in almost all cases. However, one exception is the Natural Questions with Context (NQ-WC) dataset, where we identified false negatives and thus deployed a more precise validation using an instruct LLM, as demonstrated below:"}, {"title": "Detecting and using exact answer tokens.", "content": "To detect the exact locations of answer tokens, we use a combination of heuristic methods and an instruction-tuned LLM. Specifically, when the set of possible answers is finite, we rely on heuristics. For more open-ended scenarios, such as factual questions, we automatically locate the answer if it matches the gold label. Otherwise, we prompt an"}, {"title": "DATASETS", "content": "We outline here all ten datasets that we investigate in our work. In our analysis, we aimed at covering a wide range of tasks, skills required to solve the tasks, diversity of datasets and as a result also different LLM limitations such as factual inaccuracies (often referred to as \u201challucinations\"), biases, arithmetic mistakes, and more. For each dataset, we explain how it covers something different from all the previous datasets. For all datasets, we present the LLM with non or a short instruct, a context (if exists for the task), and let it generate a free text. We follow this paradigm as it better mimics real-world usage of LLMs by humans, as opposed to using few-shot to force a short answer that is generated on the first token (Yuksekgonul et al., 2023; Chen et al., 2024; Simhi et al., 2024). One exception to this is a the sentiment analysis (IMDB) for which we apply 1-shot for the LLM to use the allowed labels, as it did not follow the instruction alone and we could not identify if the answer is correct or not even with manual analysis. Additionally, we implemented a different prompting strategy to the instruct and non-instruct LLMs. To see the exact formats we used to\""}, {"title": "Aggregated probabilities / logits.", "content": "Inspired by prior work (Kadavath et al., 2022; Guerreiro et al., 2023), we compute an aggregated score using the log-probabilities or raw probabilities of the generated text tokens Y1, Y2,\u00b7\u00b7\u00b7, YN produced by the generative large language model (LLM). For instance, the following formulation is used to compute the Logits-mean baseline on the entire generated answer:\n1/N sum(i=1 to N) P(Yi | Q, Y1, ..., Yi-1)                                                                                                (1)\nWe also explore aggregation strategies that focus solely on the exact answer tokens (PE-Exact). Following Varshney et al. (2023), we also experiment with aggregating the minimum and maximum values (PE-[Min-Max]-[Exact]), alongside the mean aggregation described in Equation 1."}, {"title": "P(True):", "content": "We follow Kadavath et al. (2022) and prompt the LLM to judge whether its answer is correct. Our prompt followed the following template, from Kadavath et al. (2022):"}, {"title": "FULL ERROR DETECTION RESULTS", "content": "Figure 6 presents the AUC values of a traind probe across layers and token for Mistral-7b-instruct, showing a similar pattern across all datasets. We also observe similar patterns across other models. See our repo https://github.com/technion-cs-nlp/LLMsKnowfor the figures.\nTables 4, 5, 6, and 7 present the full error detection results across all baselines and datasets, which are consistent with the main paper results."}, {"title": "DETECTING THE CORRECT ANSWER FULL RESULTS", "content": "In Table 8 we present some qualitative samples from Mistral-7b-instruct, for the phenomenon we observe at error type (C2) Consistently incorrect but generates the correct answer at least one time.\nThe samples in the table represent cases where the probe chose the correct answer. Table 9 compares different decoding mechanisms, including the choice via probe, on non-instruct models, and Table 10 compares on the instruct models. For all datasets and models, we observe similar conclusions to those in the main paper: significant improvement is observed for error types where the LLM shows no preference to the correct answer."}]}