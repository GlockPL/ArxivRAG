{"title": "A Lightweight Multi Aspect Controlled Text Generation Solution For Large Language Models", "authors": ["Chenyang Zhang", "Jiayi Lin", "Haibo Tong", "Bingxuan Hou", "Dongyu Zhang", "Jialin Li", "Junli Wang"], "abstract": "Large language models (LLMs) show remark-\nable abilities with instruction tuning. However,\nthey fail to achieve ideal tasks when lacking\nhigh-quality instruction tuning data on target\ntasks. Multi-Aspect Controllable Text Gener-\nation (MCTG) is a representative task for this\ndilemma, where aspect datasets are usually bi-\nased and correlated. Existing work exploits\nadditional model structures and strategies for\nsolutions, limiting adaptability to LLMs. To\nactivate MCTG ability of LLMS, we propose\na lightweight MCTG pipeline based on data\naugmentation. We analyze bias and correla-\ntions in traditional datasets, and address these\nconcerns with augmented control attributes and\nsentences. Augmented datasets are feasible for\ninstruction tuning. In our experiments, LLMs\nperform better in MCTG after data augmenta-\ntion, with a 20% accuracy rise and less aspect\ncorrelations.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) exhibit ideal\nabilities in various natural language processing\ntasks (Brown et al., 2020; Kojima et al., 2022; Qin\net al., 2023; Wei et al., 2022a; Ganguli et al., 2022).\nLLMs rely on ideal training datasets for task per-\nformance enhancement, especially for instruction\ntuning (IT) (Bai et al., 2022; Touvron et al., 2023;\nChung et al., 2024) dataset.\nHowever, LLMs struggle on certain downstream\ntasks since the absence of high-quality IT datasets.\nMCTG task suffers from this dilemma. Existing\nwork (Dathathri et al., 2020; Qian et al., 2022)\nrelies on combinations of single-aspect datasets\nfor supervised learning, which fails to achieve the\nideal performance due to issues like aspects bias\nand correlations (Gu et al., 2022; Liu et al., 2024b).\nRecent work addresses corresponding issues\nthrough designed models structures (Carlsson"}, {"title": "Task Formulation", "content": "Control Aspects And Attributes For MCTG\ntasks, controls may contain various n aspects A =\n{A1,..., An}. The i-th aspect contains |At| exclu-\nsive attributes {a},..., a At}(Liu et al., 2024b).\nMCTG requires a control combination, which\nselects one attribute from each aspect. The combi-\nnation can be notated as a vector of attribute indices\nC = [C1, ..., Cn], where c\u00a1 \u2208 {1, ...,|A\u00bf|} stands\nfor attribute index of i-th aspect.\nGeneration Task Formulation With the input of\ncontrol combinations c and generation prompt m,\ngeneration of language model LM should follow\nmultiple control aspects, notated in Eq. 1.\n$LM(m/c) ~ (a,..., am)$"}, {"title": "Methodology", "content": "As shown in Fig. 1, we first analyze 3 representative\nconcerns in existing MCTG datasets. Then we\npropose an LLM-based data augmentation pipeline\nto address the 3 issues correspondingly. Finally,\naugmentation data is transformed into format of IT\ndata, for instruction tuning of LLMs."}, {"title": "Concerns In Existing MCTG Dataset", "content": "Concerns in Control Attributes Attributes from\ndifferent aspects may share some common con-\ncepts, notated as attributes intersection. For ex-\nample, IMDB (Maas et al., 2011) demonstrates\nattributes positive and negative in sentiment aspect.\nUnfortunately, negative includes toxic attributes\nlike sarcasm for detoxification aspect.\nSecondly, control attributes a \u2208 A\u2081 are prede-\nfined, which is not specific and accurate, notated\nas attributes coarseness. Taking AGNews (Zhang\net al., 2015) as an instance, it provides control as-\npects of topic only in four choices: Sci/Tech, Sports,\nWorld and Business. World consists of various sub-\ntopics, and sentences inside training set struggle\nto cover all of the world news, which integrates\nthe bias. General and ambiguous control attributes\nobstruct further application on LLMs.\nConcerns in Sentences Distributions Selections\nof sentences x in training set are not uniform, with\nbiased distribution. Distribution of x is biased\nduring dataset construction. For example, IMDB\ndatasets provide sentences with negative and pos-\nitive sentiments through crawling movie reviews.\nBut corresponding control attributes may have in-\nstances other than movie reviews, limiting general-\nization of models."}, {"title": "LLM-Based Data Augmentation Pipeline", "content": "We propose a data augmentation pipeline, address-\ning aforementioned concerns in MCTG datasets 1."}, {"title": "Aspect-Cross Augmentation", "content": "To address attribute intersection, we exploit LLMs\nto assign label \u1ef9 in other aspects. We prompt an\nadvanced LLM for dataset generation. Augmented\ndataset is described in Eq. 3.\n$cross(Di) = {(x, \u1ef9)|x ~ (a), \\\\\n1 \u2264 y \u2264 |Aj|, j \u2260 i}$"}, {"title": "Contrasting In-Context Learning Demonstra-", "content": "tions Though LLMs exhibit ability for zero-shot\nnatural language processing, direct prompting is al-\nways not trustworthy. To avoid bias in labeling, we\nrandomly sample examples for every target aspect\nin each prompt, known as in-context learning (ICL)\nexamples (Brown et al., 2020)."}, {"title": "Reject Options", "content": "To enhance labeling confidence,\nwe allow LLM to reject 2 for formidable scenarios.\nWe will neglect all rejected options since some\ncross aspect labeling is not reasonable."}, {"title": "Consistency Validation", "content": "Considering random-\nness of LLMs, we repeat each prompt for 3 times\nand collect all answers. After normalization of case\nand format, we only keep consistent responses."}, {"title": "Aspect-Grained Augmentation", "content": "The development of LLM provides an opportunity\nto address control coarseness. We extract unre-\nstricted control attributes for input sentences, ex-\ntrapolating the label space. For Di, we regenerate\ndetailed attribute desc(x, a) for sentence x with\noriginal attribute a. This process is demonstrated\nin Eq. 4. Taking sentiment aspect as an instance,\naspect-grained augmentation provides a detailed\nsentiment like disappointed instead of negative."}, {"title": "Aspect-Rewrite Augmentation", "content": "For concerns in sentence distribution, we rewrite\nsentences outside current aspect x \u2260 Di with con-\ntrol attribute in A\u017c, as notated in Eq. 5. The rewrit-\nten sentences extrapolate imbalanced distribution\nin original dataset.\n$rewrite(Di) = {(x, y) | x ~ (a), \\\\\n1 \u2264 y \u2264 |Ai], x \u2209 Di}$"}, {"title": "Quality control", "content": "We eliminate instances that ev-\nidently deviate from statistical norms (i.e. very\nshort sentences). Additionally, we filter unsuccess-\nful rewriting due to the task difficulty. In practice,\nLLMs may copy the input or output abnormal re-\nsponses. We compare semantic similarity 3 before\nand after rewriting, then eliminate top 50% and\nbottom 10% of similar instances."}, {"title": "Instruction Tuning Dataset Construction", "content": "Augmented datasets share common format with\noriginal datasets, and we transform them into IT\ndataset for training. An instance of IT dataset con-\nsist of instruction I and response R. LLMs should\noutput R with the input of I.\nFor an instance (x, y) \u2208 Di, we provide simple\ntask descriptions, target control attribute a, and\ngeneration prefix 4 in I. We simply use controlled\nsentence x as R. An instance is in Appendix. C."}, {"title": "Experiments", "content": "Basic Datasets Following Gu et al. (2022), we\nselect IMDB (Maas et al., 2011), AGNews (Zhang\net al., 2015) and Jigsaw Toxic Comment 5 for sen-\ntiment, topic and detoxification aspects."}, {"title": "Limitations", "content": "In this work, we propose a lightweight solution to\nactivate MCTG ability for LLMs. Our work still\nleaves some limitations for future discussion as\nfollows:\n(1) The data augmentation pipeline relies on ad-\nvanced LLMs like GPT3.5, which is a compromis-\ning option for complex data synthetic tasks (Chan\net al., 2024; Yang et al., 2023a). But a self-\nconditioned augmentation pipeline is more feasible\nfor lightweight solutions, where data augmenter\nLLMs and trained LLMs remain the same like self-\ndistill (Dubey et al., 2024; Xu et al., 2023).\n(2) The quality control of augmentation relies\non a strict and simple filter policy, we expect for\nmore explainable filter strategies to enhance data\nproductivity.\n(3) Our work focuses on instruction tuning of\nLLMs for MCTG, but leaves other post-training\nprocesses like RLHF (Ouyang et al., 2022) and\nDPO (Rafailov et al., 2023) for future discussions."}, {"title": "Ethical Considerations", "content": "In this work, the trained MCTG model includes a\ntoxic aspect, which may result in the generation of\ntoxic content during evaluation. However, the in-\nclusion of the toxic aspect is solely for the purpose\nof evaluating the model's capabilities. We assure\nthat we will not require the model to generate toxic\ncontent in real-world applications."}, {"title": "Related Work", "content": "Large Language Models Large language mod-\nels (LLMs), such as LLaMA (Touvron et al., 2023;\nDubey et al., 2024) and GPT-4 (Achiam et al.,\n2023), refer to a series of Transformer-based mod-\nels undergoing extensive pretraining with mas-\nsive corpora. By scaling up the data volume and\nmodel capacity, LLMs demonstrate remarkable\nemergent capabilities, such as In-Context Learning\n(ICL) (Brown, 2020) and Chain-of-Thought (CoT)\nprompting (Wei et al., 2022b), enable them to com-\nprehend human instructions and handle complex\ntasks with minimal or even no supervision. Despite\ntheir exceptional performance, LLMs still produce\nnonsensical or incongruent information in practical\napplications (e.g. \"hallucination\"(Ji et al., 2023)).\nIn this paper, our method leverages the knowledge\nand generative capabilities of LLMs.\nMulti Aspect Controlled Text Generation\nFrom the perspective of parameter fusion, Huang\net al. (2023) have improved MACTG in prefix tun-"}]}