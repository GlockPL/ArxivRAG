{"title": "The use of large language models to enhance cancer clinical trial educational materials", "authors": ["Mingye Gao", "Aman Varshney", "Shan Chen", "Vikram Goddla", "Jack Gallifant", "Patrick Doyle", "Claire Novack", "Maeve Dillon-Martin", "Teresia Perkins", "Xinrong Correia", "Erik Duhaime", "Howard Isenstein", "Elad Sharon", "Lisa Soleymani Lehmann", "David Kozono", "Brian Anthony", "Dmitriy Dligach", "Danielle S. Bitterman"], "abstract": "Cancer clinical trials often face challenges in recruitment and engagement due to a lack of participant-facing informational and educational resources. This study investigated the potential of Large Language Models (LLMs), specifically GPT4, in generating patient-friendly educational content from clinical trial informed consent forms. Using data from Clinical Trials.gov, we employed zero-shot learning for creating trial summaries and one-shot learning for developing multiple-choice questions, evaluating their effectiveness through patient surveys and crowdsourced annotation. Results showed that GPT4-generated summaries were both readable and comprehensive, and may improve patients' understanding and interest in clinical trials. The multiple-choice questions demonstrated high accuracy and agreement with crowdsourced annotators. For both resource types, hallucinations were identified that require ongoing human oversight. The findings demonstrate the potential of LLMs \u201cout-of-the-box\" to support the generation of clinical trial education materials with minimal trial-specific engineering, but implementation with a human-in-the-loop is still needed to avoid misinformation risks.", "sections": [{"title": "INTRODUCTION", "content": "Clinical trials are the gold standard for investigating management strategies that can potentially improve cancer patient outcomes. The experimental nature of clinical trials necessitates clear information and effective communication about potential benefits and risks that patients could realize during the study period. However, patients have expressed needing better resources to learn about their trial options [1-4]. Currently, the primary resource by which cancer patients and providers learn about clinical trial options are clinical trial registries, such as ClinicalTrials.gov[5-6]. ClinicalTrials.gov is a large, public database of clinical trials, but it primarily uses highly technical language that is geared toward a clinician and investigator audience, meaning they are often inaccessible to most patients. Therefore, there is still a lack of widely accessible materials to inform and educate potential participants about specific clinical trial options. This challenge is an important barrier to trial recruitment, informed consent, protocol adherence, and successful and timely accrual. At a more fundamental level, ensuring adequate information about clinical trials is imperative to ensure valid informed consent and widening access to treatment options within a clinical trial across sociodemographic backgrounds.\nLarge Language Models (LLMs) present a new opportunity to enhance trial processes via improved patient awareness and engagement. Prior studies have investigated LLMs to facilitate cancer patient education and communication, demonstrating promise but also risks arising from falsifications and fabrications [7-11]. However, most work on using LLMs to improve clinical trial processes has focused on clinical trial matching [3-4, 12-13] and little research has investigated these models for enhancing informational and educational resources. LLMs' ability to simplify and summarize texts, in particular, is an exciting avenue for improving education and awareness. For example, LLMs could be used to simplify and clarify the often complex and jargon-heavy information presented in clinical trial documents, including informed consent forms [14-15]. In addition, LLMs could support the development of new methods to support clinical trial education. For example, established methods to measure the quality of clinical trial understanding after informed consent, such as the validated Quality of Informed Consent questionnaire [16], do not assess trial-specific details. LLMs could facilitate the development of trial-specific questionnaires, providing opportunities for patients to self-assess their understanding and provide a pathway for patient-specific education to address knowledge gaps.\nIn this study, we explored the potential and risks of using LLMs to generate informational and educational resources about clinical trials via the secondary use of clinical trial materials. First, we prompted LLMs to generate short, plain-language summaries of the key elements of a clinical trial from informed consent forms (ICFs). Patient surveys of these summaries investigated their readability and their potential to enhance clinical trial understanding as a part of informed consent. Second, we investigated the ability of LLMs to automatically generate multiple-choice question-answer pairs (MCQAs) based on ICF content, providing an interactive approach to assess clinical trial understanding. Our findings provide proof-of-concept for the potential of leveraging LLMs to enhance informational and education resources for cancer clinical trials, which could improve patient engagement and support clinical trial processes."}, {"title": "METHODS", "content": "Figure 1 illustrates our overall research approach of prompting GPT4 to generate new educational and informational resources for clinical trials: plain-language short summaries (to inform patients about a trial) and multiple-choice questions (to assess understanding of a trial).\nICFs for this study were collected using the ClinicalTrials.gov API [17]; PyMuPDF [18], a publicly available Python library, was used to retrieve text from the PDF files.\nFor the summary generation task, 11 clinical trial ICFs were randomly selected from Clinical Trials.gov for prompt engineering and initial evaluation by the research team (Appendix A, Table 1). The statistics of this set are shown in Appendix A, Figure 1. A larger set of ICFs were used for the large-scale questionnaire development. We selected 91 interventional cancer clinical trials registered between January 1st, 2021 and April 15th, 2024. This time frame was chosen to capture the most up-to-date practices in informed consent while still providing a substantial pool of studies for analysis. The distribution of ICF pages and number of tokens are shown in Appendix A, Figure 2.\nWe explored 2 different approaches to generating the trial summary from consent forms: Direct summarization from text, and sequential extraction and summarization. In the direct"}, {"title": "Summary Evaluation", "content": "To evaluate the quality of our two prompting approaches, 4 clinicians from the research team (JG, LL, DK, DB) evaluated the 11 clinical trial summaries generated using the 2 aforementioned prompting methods. Each summary was evaluated for the binary presence or absence of key trial elements, readability, inaccuracies, biases, and hallucinations. Overall summary quality was assessed using a 5-point Likert scale. The full survey is in Appendix B.\nNext, to explore patient perspectives of the generated summaries in a real-world setting, we invited patients undergoing informed consent for the BROADBAND Research Study (hereafter referred to as BROADBAND), a prospective secondary use protocol in the Department of Radiation Oncology at Brigham and Women's Hospital/Dana-Farber Cancer Institute, to participate in a survey evaluating 5 LLM-generated cancer clinical trial summaries generated using the sequential summarization approach, including a summary of BROADBAND plus 4 other trials representing an observational trial, Phase I trial, Phase I/II trial, and Phase III trial (Appendix A, Table 3). To avoid any risks of misinformation, all summaries underwent manual review and editing by an oncologist. In addition to minimizing any patient risk, this mimics the real-world application of such a summary, where a member of the clinical trial research team would review the GPT4-generated content before it reaches the patient. Participants completed the survey after the BROADBAND informed consent discussion, which provided the opportunity to assess the impact of the summary on understanding of a clinical trial for informed consent. The survey tool was developed in RedCap and participants could complete the survey on paper, a laptop in the clinic, or via an emailed link. The survey instrument is in Appendix C. Informed consent for the survey was waived as this study was deemed to be exempt human subjects research by the Mass General Brigham Institutional Review Board (MGB Protocol # 2024P000949)."}, {"title": "Multiple choice question-answer pair generation", "content": "GPT-4-1106-preview was used for MCQAs generation. To ensure the quality of the generated MCQAs, we adopted the in-context learning method: when generating an MCQA for each topic based on a target ICF, we fed an expert-created question-answer pair and its corresponding ICF text, along with the target ICF text, into GPT4. 15 MCQAs focused on a subset of the basic elements of informed consent were manually written by a board-certified oncologist (DB) based on an exemplar ICF for a non-cancer clinical trial [20] (Table 1). Using these pairs as in-context examples, we engineered a multi-turn prompt to generate MCQAS (Appendix A, Figure 5). For each generation query, temperature and top_p were set to 0, and max_tokens was set to 3000. After filtering invalid generations (e.g., responses returned by GPT4 that were not an MCQA), a total of 1335 MCQAs were generated for 91 ICFs."}, {"title": "RESULTS", "content": "For the 11 summaries evaluated by clinicians, both prompting approaches achieved comparable results for readability and topic content (Figure 2). There was slightly less evidence of inaccuracies, biases, and hallucinations using the sequential prompting approach. Quality was rated as acceptable or better in the majority of responses using both prompting approaches (Figure 3), although results varied substantially across trials and evaluators. We found that inaccuracies and hallucinations tended to occur for topics that were not described in the given ICF. Summaries generated using the sequential prompting method were preferred in 38.6% (17/44) responses, and summaries generated using the direct prompting method were preferred in 61.4% (27/44) responses."}, {"title": "Multiple choice question-answer pair evaluation", "content": "MCQAs had an average of 5.21 qualified reads (standard deviation, 1.99). The majority answer agreed with the GPT4 answer in 1307/1335 (97.91%) of MCQAs, with an average agreement of 86.87% and an average difficulty of 15.52%. Of note, the median difficulty and agreement were 0.0 and 1.0, respectively, demonstrating that all readers' answers matched the GPT4-assigned answer in over half of all MCQAs. Appendix A, Table 6 and Appendix A, Figure 7 provide detailed statistics of qualified reads, agreement, and difficulty. When broken down by MCQA topic, average agreement was lowest and difficulty highest for MCQAs about contact information, expected duration, and alternative procedures (Appendix A, Figure 8).\nWe identified 78/1335 (5%) MCQAs with difficulty \u2265 0.6 and agreement \u2264 0.5 for our quality assurance set for manual error analysis. Manual review identified 5 error modes, which are summarized in Table 5. Of these, the majority of errors were human error, followed by errors in GPT4-generated MCQAs, errors due to missing information in the ICFs, and errors arising from ambiguous language.\nFor most MCQAs in the quality assurance set, the 4 LLMs disagreed with the MCQAs when there were incorrect GPT4-assigned labels, missing information in the ICF, and ambiguous definitions. This suggests that testing the generated MCQAs using other LLMs may be a promising avenue to assist humans in proofreading the quality of LLM-generated MCQAs. More details on the LLM and human reader results on the quality assurance set are shown in the Appendix A, Figures 9-12."}, {"title": "DISCUSSION", "content": "This study explored the potential of LLMs to automatically draft patient-friendly summaries and MCQAs from ICFs. Experimental results demonstrated that LLMs can effectively draft readable and accurate summaries of ICFs with straightforward prompting techniques, with patient surveys suggesting potential roles in improving trial awareness and consent quality. Additionally, LLMs were able to generate high-quality MCQAs based on the content of target ICFs. These results provide proof-of-concept for leveraging LLMs to accelerate the development of new, diverse, and scalable educational resources for clinical trial patient education, while also highlighting key error modes that require ongoing human oversight and vigilance.\nPrior studies have demonstrated the potential of leveraging LLMs for clinical education, including the generation of patient-facing disease-specific information [21-22], and summarization and simplification of existing clinical documents [23-33]. In similar work, White et al. (2023) used GPT3.5 to generate summaries of multiple clinical trials using the brief descriptions in Clinical Trials.gov study records, although these were intended for researcher audiences [29]. In addition, studies have shown the ability of LLMs to simplify language in consent forms for standard medical procedures [34-37]. Our findings build on this prior literature to demonstrate two new, promising applications of LLMs to support the unique informational and educational needs of patients learning about clinical trials.\nThere is a need for better patient education about clinical trial options [38-40]. Inadequate educational resources about clinical trials limits awareness of and engagement in trials and may contribute to the high rate of cancer trials that fail to accrue [41-42]. New, diversified resources to educate patients about clinical trials could increase enrollment rates, improve patient understanding, and potentially broaden clinical trial access and diversity [43-49]. In fact, patients have expressed a need for more awareness about clinical research. During recruitment, patients and caregivers report a lack of familiarity with trial options and are more likely to have positive attitudes about participation if they learn about trials [3-4, 7]. Past studies have explored novel approaches to improve education, primarily at the informed consent stage [50-56], but scalability has previously been limited by the time and engineering expertise needed to develop trial-specific resources. Our findings show the potential of LLMs to lower the barrier to generating a diversity of educational resources from documents that are already created as a part of standard clinical trial conduct. Our prompting methods do not require significant engineering expertise to implement and are agnostic to the specifics of a given clinical trial.\nWhile our findings demonstrate that GPT4 can, in general, follow prompt instructions to convert ICFs into new educational resources, they also highlight error modes necessitating ongoing human oversight and methods refinement. Though rare, both the summaries and MCQAs included inaccuracies and hallucinations, a known challenge of working with LLMs. These most often occurred when the ICF did not include adequate content requested in the prompt. This limitation may arise out of LLMs' alignment tuning, which leads them to prioritize helpfulness (i.e., following users' instructions in the prompt) over factual accuracy - a key error mode to monitor for such LLM applications. Our sequential prompting method for summaries, which first extracted relevant ICF text, and then summarized over the extracted text, appeared to mitigate but not completely alleviate this error mode. Further prompt refinement may improve these errors; however, human oversight is still needed to ensure accurate, comprehensive, and safe information when using LLMs for summarization [57]. At the same time, our findings may also spur trial sponsors to improve the content of ICFs.\nImplementing such LLM applications for clinical trial processes, and in healthcare more broadly, is currently limited by a lack of effective means for large-scale evaluation and ongoing monitoring of model performance. As above, while promising, even state-of-the-art LLMs such as GPT4 require a human-in-the-loop to identify and resolve errors before they reach patients [58]. Nevertheless, our methods may lower the barrier to develop educational content because clinical trial staff may find it easier to review and revise LLM-generated drafts than to develop the content from scratch. While there is significant excitement about using LLMs as real-time conversational chatbots without a human-in-the-loop to support patient education, our study suggests that models \u201cout-of-the-box\" are not currently safe for such applications without oversight.\nThis study has several limitations that must be taken into account when considering our results. First, we evaluate a relatively limited number of trials and have a small number of human evaluators for the summaries, which limits generalizability. Additionally, the patients who agreed to participate in summary evaluations may not be reflective of the broader cancer population, including a lack of diversity. That said, ours is one of the very few studies that have assessed patients' perceptions of LLM outputs [59-60], and to our knowledge the only study evaluating patient perceptions of LLM content that relates to their own healthcare (i.e., their understanding of the BROADBAND study). Further, the number of human evaluators in our study falls within the range of other studies evaluating LLMs for patient education [61]. Nevertheless, given these limitations, our results should be considered as early proof-of-concept, and larger scale studies demonstrating safety, acceptability, and effectiveness are needed. In addition, we may not have used the optimal prompting approaches, and it is possible error rates could be reduced with additional prompt engineering. However, our goal was to understand the performance, behavior, and risks of widely available LLMs without significant additional engineering efforts, serving as a baseline for future technical innovation and advances. Similarly, including additional clinical trial materials, such as information from the ClinicalTrials.gov study records and trial protocols, may be a promising avenue to reduce the observed risk of inaccuracies and hallucinations by providing more substantive content on topics that may not be adequately described in an ICF. Finally, patients have diverse informational and educational needs. While we explored LLMs to generate two different types of educational resources, we did not explore personalizing the resources to individual preferences. This is an exciting avenue and will be a focus of future work."}, {"title": "CONCLUSION", "content": "Our findings demonstrate a promising role of LLMs in narrowing the knowledge gap between patients and specific clinical trial information, potentially enhancing informed decision-making and fostering greater patient engagement. This study establishes an initial framework for utilizing LLMs as supportive tools in patient-centered clinical trial education and informational resources. Future research should focus on optimizing output through advanced prompting techniques and automated oversight, investigating personalized approaches tailored to individual needs, and rigorously validating the quality, safety, and real-world impact and effectiveness of these methods."}]}