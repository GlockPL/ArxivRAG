{"title": "BIO2TOKEN: ALL-ATOM TOKENIZATION OF ANY BIOMOLECULAR STRUCTURE WITH MAMBA", "authors": ["Andrew Liu", "Axel Elaldi", "Nathan Russell", "Olivia Viessmann"], "abstract": "Efficient encoding and representation of large 3D molecular structures with high fidelity is critical for biomolecular design applications. Despite this, many representation learning approaches restrict themselves to modeling smaller systems or use coarse-grained approximations of the systems, for example modeling proteins at the resolution of amino acid residues rather than at the level of individual atoms. To address this, we develop quantized auto-encoders that learn atom-level tokenizations of complete proteins, RNA and small molecule structures with reconstruction accuracies below and around 1 Angstrom. We demonstrate that the Mamba state space model architecture employed is comparatively efficient, requiring a fraction of the training data, parameters and compute needed to reach competitive accuracies and can scale to systems with almost 100,000 atoms. The learned structure tokens of bio2token may serve as the input for all-atom language models in the future.", "sections": [{"title": "1 Introduction", "content": "Biomolecular structures can be described as 3D point clouds where each point has a chemical identity. Each point could represent an atom, a functional group, or a set of atoms that make up larger moieties including sidechains and nucleotide bases associated with amino acids and nucleic acids. There are several generative modeling approaches for such biomolecules and most, when dealing with larger biomolecular systems, tend to use coarser grained representa-tions. The most popular ML methods today include denoising diffusion probabilistic models (DDPMs) and language models. Diffusion approaches generate 3D-structures and features that can correspond to individual atoms, or more coarse-grained features like individual residues. In the case of small molecule diffusion, approaches like DDPM have been able to generate atomistic conformers [1] and ligands to protein pockets [2]. However, applying denoising diffusion at atomistic resolution to large biomolecular structures like proteins, comprising hundreds or thousands of amino acid residues, is more difficult and computationally inefficient. Recent models like RFDiffusion All-atom only diffuse the 4-atom back-bone of the protein and reconstruct the side-chain atoms with a separate inverse folding model [3, 4], while an atomic resolution diffusion of RNA structures is to our knowledge unattempted, in part due to a lack of sufficient structural data but also due to the exacerbated computational constraints: some RNAs are part of complexes like ribosomes which can readily reach sizes of on the order of hundred thousands of atoms. Similarly, language model-based approaches such as ESM-3 also rely on coarse-grained representations for proteins at the resolution of individual residues for its core multi-track module. Despite this coarse-graining, the size of proteins and protein-ligand complexes that these models can process is still limited (see below for further discussion).\nAn effective atomic-resolution representation model for large molecules would have to be able to reason over long-range data inputs. This is because many atoms can have critical interactions with atoms that are distal in linear sequence. However, scaling traditional \u201cworkhorse\" architectures such as transformers and graph-based models to accommodate such long sequences has been challenging. Here, we leverage Mamba [5], a selective structured state space model, to replace transformer modules and increase the resolution to all atoms. Mamba has been developed for long-context modeling and has been demonstrated to efficiently model tasks with thousands to millions of tokens on conventional GPU hardware. We show that all-atom level modeling with Mamba is more efficient than transformer-based residue-level modeling in terms of data amount and model size needed, whilst maintaining similar to superior performance."}, {"title": "1.1 Related Work", "content": "Recently two major models, Alphafold-3 (AF-3) and ESM-3 [6, 7] demonstrated the ability to generate inferred all-atom modeling for biomolecules. Both encode on the residue-level and decode to all-atom. AF-3 trained a complex token-guided diffusion network where proteins and nucleic acids are tokenized at the residue-level, and ligands and non-canonical residues are tokenized on the atom-level. A Gaussian denoising diffusion process generates the full all-atom structure. The diffusion module is based on a transformer and each diffusion step scales quadratically with the number of residues/tokens, with AF-3 modeling a maximum of 786 tokens. ESM-3 models biomolecules with a masked language model and tokenizes 3D structures into 1D discrete tokens with a quantized transformer-based encoder-decoder approach. ESM-3 encodes on the residue-level of back-bones and infers all-atom structure in the decoder with additional sequence information. The final ESM-3 decoder was limited to a training on proteins of less than 512 residues and is of considerable size with 30 transformer layers and 600M parameters. Another approach to tokenize and reconstruct proteins was recently developed by InstaDeep [8]. They present a quantized auto-encoder based on a message-passing graph neural network (GNN) encoder and a decoder based on AlphaFold-2's IPA (in-variant point attention) transformer architecture. They only encode and reconstruct the back-bone structure, and their reconstruction RMSE is 1-5\u00c5 on the backbone; side-chains are not decoded."}, {"title": "1.2 Transformers, State space models, and Mamba", "content": "Transformers [9] use the attention mechanism to capture long-range dependencies in sequences. Intuitively, the atten-tion mechanism can be thought of as a fully-connected graph neural network, with the update rule:\n$y = M(x)x$\nwhere x is the input sequence, y is the latent representation, and $M(x) = softmax (Q(x)K(x)^T)$ is the attention matrix, or analogously adjacency matrix. The matrix multiplication formulation of attention makes it ideal for GPU processing, and has thus made transformers the workhorse of sequence modeling. However, due to the fully connected graph structure, transformers suffer from $O(N^2)$ compute and memory costs with respect to sequence length N. Recent alternatives such as deep structured state space models (SSM) have gained traction in the field of sequence modeling thanks to their ability to overcome the quadratic bottleneck and scale to extremely long context lengths. The basic linear time-invariant (LTI) SSM is a linear recurrent neural network (RNN) with the update rule:\n$h_t = Ah_{t-1}+Bx_t, Y_t = Ch_t$\nWhere x and y are the input and output sequences, respectively, h is the RNN state, and A, B, C are learnable pa-rameters. This linear recurrence can be unrolled across time and computed equivalently as a convolution $y = K * x$, where $K = (CB,CAB, ...,CA^{N-1}B)$. This makes SSMs parallelizable over sequence positions, incurring cost $O(N log N)$, provided that the kernel K can be computed efficiently. In the original formulation of structured SSMs (S4) [10], the transition matrix A is restricted to a diagonal-plus-low-rank structure, enabling successive powers of A to be computed efficiently in the spectral domain. The recent Mamba selective SSM (S6) [5] generalizes the LTI SSM to have input-dependent parameters B(x) and C(x). This makes the underlying graph structure input-dependent, allowing the model to selectively attend between token positions based on the input, much like a transformer. The recurrence update can no longer be computed as a convolution and the authors replace it with an efficient parallel scan.\nMore recently, [11] demonstrated that the SSM update can also be written explicitly as an attention update $y = M(x)x$, where $M(x) = contraction(A, B(x), C(x))$. By imposing a scalar-times-diagonal structure on A, this induces low-rank structure for off-diagonal blocks in M, enabling efficient matrix multiplication via the block-decomposition algorithm, which performs quadratic attention on diagonal blocks and state passing for off-diagonal blocks. In this view, the efficiency gain of Mamba comes from restricting the structure of the underlying graph."}, {"title": "What if we could efficiently encode and decode any biomolecule at the all-atom level?", "content": "In this work we present:\n(i) A Mamba based auto-encoder for all-atom 3D biomolecular structures.\n(ii) A quantized version of (i) that tokenizes 3D structures into 1D discrete tokens. We train small molecule-only, protein-only, and RNA-only vocabulary mol2token, protein2token and RNA2token. We also train a unified tokenizer bio2token that encodes any of those biomolecules that range from tens to tens of thousands of atoms. We train structures of up to 95,000 atoms that cannot be handled by transformer-based methods, like AF-3 or ESM-3.\n(iii) A compute efficient approach towards all-atom modeling. Our tokenizer models are lightweight in size, with sub-million parameter size and fast training and inference speeds."}, {"title": "2 Methods", "content": "Our structure tokenizer model is a quantized auto-encoder (QAE). Figure 1 provides an overview of the process. The steps can be broken down into:\n(i) Representation of biomolecular systems as 3D point clouds\n(ii) Encoding atom positions into latent vectors\n(iii) Quantizing these latent vectors into tokens\n(iv) Decoding these tokens back into a 3D point cloud"}, {"title": "2.1 Tokenization of 3D point clouds", "content": "A biomolecular structure X of N heavy atoms is represented as a $X \\in R^{N\u00d73}$ point cloud. This point cloud is atom identity agnostic and carries no information about residue or atom type. In general, AEs compress the input to a down-sampled latent space representation Z, where $Z\\in R^{n\u00d7d}$, with n < N, and reconstruct the original input according to a loss function L(X, X). The first module of the AE is the encoder network enco(X) = Z, the second module is a decoder network decy(Z) = X. In this work, we want to maintain the all-atom resolution, and enco(X) does not compress the input: n = N. It can be regarded as a transformation of the input into a lossless latent space. The compression in our QAE happens in the subsequent quantization of Z into discrete values Q as the purpose of a tokenizer is to map continuous data to discrete unique IDs, defined by a vocabulary. The latent space of a \"perfect\" lossless auto-encoder (L(X, X) = 0) is a perfect tokenizer if that latent space was discrete. This is usually achieved through a quantization network quant$(Z)$, otherwise known as a tokenizer model. This tokenizer model can be optimized with the same reconstruction loss L(X, X). This tokenizer model keeps a one-to-one correspondence between the positions of input atoms, tokens and reconstructed atoms."}, {"title": "2.1.1 Quantization", "content": "Quantization networks learn discrete codebooks, in other words a vocabulary, of the training data. A common approach is vector-quantization (VQ) [12], e.g. ESM-3's tokenizer is VQ-based. VQ is notoriously hard to optimize and tends to suffer from codebook collapse. In this paper, we use a recently proposed simplified alternative, Finite-Scalar Quantization (FSQ), which produces a more efficient coverage of the codebook [13]. FSQ has been shown to be easier to train, whilst maintaining similar levels of expressiveness as VQ-VAEs. FSQ projects the input into a hypercube of integer length L and dimensions D (where D < 8 usually). The projected input point in the hypercube is then rounded to the nearest integer set {0, 1, ..., L}. The final code/token is the product of all integer coordinates in the hypercube."}, {"title": "2.2 Datasets", "content": "An overview of all training and test data is provided in the Appendix table 1. In a pre-processing step all point clouds of the biomolecular structures are translated to be centered at the zero-origin of the COS.\nSmall molecules: Small molecules, typically organic molecules below a 500 Dalton weight, are not static. At standard temperatures and pressures they take on various 3D structural conformations, each having a specific confor-mational energy. We used the \u22072DFT dataset [14] of 1.9M small molecules with a total of 16M simulated structural conformations as a source of data. This dataset provides train and test splits for multiple levels of generalizability: a test-conformer split of unseen conformations of molecules, a test-structure split of unseen molecules and all their conformations, and a test-scaffold split of unseen scaffold classes of molecules and their conformations. The minimum number of heavy atoms in this dataset is 8 and the maximum is 27.\nProteins: We trained on the CATH 4.2 dataset of 18k protein structures from the PDB, with train-test splits on the CATH topology classifications as defined by [15]. This dataset comprises proteins of 40 to 500 amino acids in length, for a minimum and a maximum of 282 and 4.173 heavy atoms. We also tested on the CASP14 and CASP15 datasets, to compare to the values reported by ESM-3. CASP14 and 15 structures were published after CATH4.2 and are thus not contained in 4.2. CASP14 and 15 contain proteins up to 2.265 residues in length with the biggest structure having 18.042 heavy atoms.\nRNA: We trained on RNA3DB, which splits the RNA structures in the PDB into sequence-based and structural homology classes [16]. The training split contains 10k structures, spanning a range of 2 to 4.450 nucleic acids in lengths with 42 to 95.518 heavy atoms.\nGeneralisation to complexes: We test bio2token at inference time on multi-chain complexes and protein-RNA complexes. Note that neither multi-chain nor mixed complexes were included in the training."}, {"title": "3 Experimental Details", "content": ""}, {"title": "3.1 Auto-encoding and tokenization", "content": "For all tokenizer models below we use an entirely Mamba-based QAE, with 2 layers for the encoder and 4 layers for the decoder. We use a codebook with D = 6 and quantization levels L = [4, 4, 4, 4, 4, 4]. The vocabulary size is then 46 = 4096. This matches the vocabulary size used by ESM-3's decoder and InstaDeep's QAE. The resultant model has 702, 470 trainable parameters.\nLoss: The ground truth and the decoded point clouds X and X are aligned via Umeyama-Kabsch algorithm [17] and the loss is the RMSE: $L_{RMSE}(X, \\hat{X}) = \\sqrt{\\frac{1}{n} \\Sigma_{i=1}^{n} ||X_i - \\hat{X}_i ||^2}$ mol2token: batch size = 32, lr = 1e-5, Adam optimizer, 160k steps over 5 hours on a single NVIDIA A10 GPU (24GB GPU RAM).\nprotein2token: batch size = 32, lr 1e-4, Adam optimizer, 570k steps over 5 days on a single NVIDIA A10 GPU (24GB GPU RAM).\nRNA2token: effective batch size of 84 (max. seq. length of 95, 520) on 7 NVIDIA L40S GPUs (288GB of total GPU RAM), lr 1e-3, Adam optimizer, 60k steps over 41 hours.\nall data: bio2token effective batch size = 182 (max. seq length of 43, 200) on 7 NVIDIA L40S GPUs (288GB of total GPU RAM), lr 1e-3, Adam optimizer, 62k steps over 29 hours."}, {"title": "3.2 Evaluation", "content": "All tokenizer models are optimized on point cloud RMSE. However, RMSE is not sufficient for assessing chemical or biochemical validity of the reconstructed structures, so we benchmark our model on additional test metrics as outlined below.\nSmall molecule validity test We convert the heavy atom point clouds into molecules by inferring covalent bonds using atom type and inter-atomic distances with OpenBabel [18]. We first evaluate whether the recovered molecular system is equivalent to the encoded structure and then evaluate bond lengths, angles, and torsion angles using the methods described by [19]. We use RDKit to compute the energy of the conformer and compare to the average energy of 25 RDKit generated conformers [20]. We compute these statistics for test set ground-truth conformers and the reconstruction to evaluate any change. A reconstruction is said to pass all tests if it passes the tests from PoseBusters, as well as produces the same molecular graph as the input.\nProteins and RNA: In addition to RMSE, we report the template modeling score (TM-Score) between ground truth and reconstructed point clouds. It captures local and global structural alignment and is designed to be size independent. The protein TM-score TMprot is calculated on the Ca of the amino acid back-bone [21]. The RNA TM-score TMRNA is calculated on the C3' of the nucleic acid back-bone [22]. TM=0 means no structural similarity at all; TM=1.0 means structurally identical."}, {"title": "4 Results", "content": ""}, {"title": "4.1 Tokenizer", "content": ""}, {"title": "4.2 In-domain tokenizing", "content": "First we evaluate the performance of each domain-specific tokenizer versus the combined bio2token tokenizer model.\nSmall molecules: mol2token reconstructs small molecule conformers of unseen molecules and unseen scaffold fam-ilies with an average RMSE of 0.25\u00c5 versus 0.28\u00c5 for the combined model bio2token. Figure 3A shows a valid re-constructed conformer. from the test set on top of the ground truth conformer. We found that 25% of all reconstructed molecules with mol2token passed all of our validity metrics and are similarly chemically valid as the ground-truth structures. We conclude that the mol2token vocabulary is sufficiently expressive to capture most molecular geometries in a manner that maintains their chemical structural properties in 1/4 cases.\nProteins: bio2token outperforms protein2token on CASP14 and CASP15 test hold-outs with RMSE values below 1\u00c5 RMSEs. Bio2token outperforms ESM-3's decoder reconstruction on CASP14 and 15 with 0.89 and 1.02\u00c5 RMSE versus ESM-3's 1.3 and 1.77\u00c5. InstaDeep's back-bone tokenizer compares with a back-bone RMSE of 1.89\u00c5 to our back-bone RMSEs of 0.63-1.26\u00c5 across the different protein test sets. Generally back-bone atoms had higher reconstruction accuracy than side-chain atoms for most test sets and tokenizers. The domain-specific protein2token only reconstructs the CATH4.2 test data better than bio2token, likely because bio2token learned larger scale and longer range structural context from the large RNA structures in the training set. Protein2token was only trained on CATH4.2 proteins smaller than 500 residues which is below several test set structures in CASP14 and 15. Generally the TMprot are all above 0.96, indicating that structural homology in terms of tertiary structure is highly preserved."}, {"title": "RNAs:", "content": "bio2token reconstructs the RNA3DB test dataset with the lowest RMSE average of 1.22\u00c5 on all atoms. Interestingly, rna2token and protein2token have similar RMSE averages of 1.48 \u00b1 0.96\u00c5 and 1.48 \u00b1 1.5\u00c5 . Likely, bio2token is superior because it learned point cloud densities from the RNA and protein structures. The largest RNA chain in the RNA3DB test data is 8toc.R with 4.269 nucleic acids and a total of 90.441 atoms. Rna2token achieves an reconstruction RMSE of 1.53\u00c5 on this structure.\nComplexes: None of the tokenizers has ever seen a complex during training. Here, we tested to what degree we can encode and reconstruct RNA-protein and multi-chain complexes with the QAE. Figure 3B shows a multi-chain RNA complex (pdb:7ptl) reconstructed with rna2token at an average RMSE of 1.07\u00c5. This complex comprises 720 nucleic acids with a total of 15.337 atoms. Figure 3C shows a protein-RNA complex (pdb: 3wbm) with a combined residue count of 396 amino and nucleic acids with 3.714 atoms, with an RMSE of 1.12\u00c5.\nEfficiency IPA Transformer versus Mamba: We tested the QAE using AF-2's IPA-transformer. The forward step was about twice slower than the mamba-based step. We didn't attempt training an IPA-transformer QAE for proteins or RNA as the lengths are prohibitive with the GPU memory available to us."}, {"title": "4.3 Insights into what bio2token learns", "content": "Our tokenizer models learn to efficiently encode and decode point clouds of variable sizes, centered at the origin. Figure 4 shows scatter plots for a sample of 10k points across all structure point clouds with their absolute distance to the centre and their RMSE. RMSE increases once the point's distance to centre increases past the common size range of the structures. For example, small molecules only span a few Angstroms and mol2token fails at reconstructing macromolecules (see Appendix table 4, a mol2token reconstructed protein looks like a dense point potato, not shown). Protein2token and rna2token can tokenize RNAs and proteins as they share similar point cloud sizes and densities. This motivates the combined tokenizer training of bio2token, which performs best for proteins, RNA and protein-RNA complexes as it has seen the most diverse point configurations during training."}, {"title": "5 Discussion", "content": "In this study, we have explored the potential of Mamba to encode high-resolution structures for diverse biomolecules. Our findings underscore the efficiency and accuracy of our model in encoding and representing 3D molecular struc-tures at an atomic resolution, even for large macromolecules such as proteins, RNA and their complexes. Our three biomolecule specific tokenizer models - mol2token, protein2token, and rna2token - demonstrates the versatility and robustness of our approach: each model is capable of encoding structures ranging from a few to tens of thousands of atoms with sub-Angstrom level accuracy. Furthermore, our combined tokenizer, bio2token, demonstrates that en-codings can be learned across different classes of macromolecules once encoded on the common atomistic resolution. To our knowledge, our models are the first to demonstrate the ability to auto-encode all-atom complexes of sizes of multiple tens of thousands of atoms. The comparable small amount of data (27,000 macromolecules in total) used in our trainings, compared to large models like the ESM-3 decoder signals that all-atom encoding might substantially enhance training efficiency, compared to more coarse-grained encoding that lack atomistic detail, and leverages more information from the structures sampled. Atomistic detail is important for many biomolecular design applications - the precise positioning of individual atoms within a protein or RNA molecule can significantly impact its function and interactions with other molecules. Furthermore, the reliance on separate models for different components of a molecule, (e.g., backbones versus sidechains), can introduce additional complexity and potential sources of error. Our work highlights the potential of leveraging models like bio2token to improve the design and modeling of biomolecules and biomolecular complexes."}, {"title": "5.1 Limitations and future directions", "content": "Although our model achieves low RMSE values, having low RMSE is not necessarily sufficient to guarantee that the reconstructed molecule is chemically valid. Even small deviations in decoded atom coordinates can result in structures with steric clashes, improper covalent bonds, or improperly strained geometries. For example, as seen in 3, our reconstructed structures can deviate from valid molecular geometries, for example implying covalent bonds where there should not be and vice-versa missing bonds where there should be. One potential solution could be to train the model on additional data which may further lower RMSEs to a level that produces more chemically valid biomolecules without having to hard code constraints into the losses of our models. Alternatively, we could employ heuristic and physics based post-processing protocols like those used in [6] to help translate generated point clouds into valid molecules.\nNonetheless, our work points towards Mamba-based architectures as a viable and promising alternative to transformer-based methods for modeling atomic-resolution biomolecular structures. In the quantized QAE form as presented here, our models could facilitate compatibility with language models and we leave the joint representation of atom identity and coordinates to future work. The continuous embedding form (without the quantizer) could provide useful latent space for methods like flow matching or diffusion. As such, we anticipate that our work could connect to many potential downstream modeling applications in chemistry and biology.\nWe will continue to improve bio2token's accuracy by using augmentations to the data to cover more point cloud con-figurations, as well as leverage larger amounts of structural data, namely synthetic structures from the Alphafold-DB and other small molecule datasets."}, {"title": "6 Code Availability", "content": "Code and model weights will be made available in the future."}]}