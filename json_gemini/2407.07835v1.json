{"title": "ROBUS: A MULTIMODAL DATASET FOR CONTROLLABLE ROAD NETWORKS AND BUILDING LAYOUTS GENERATION", "authors": ["Tao Li", "Ruihang Li", "Huangnan Zheng", "Shanding Ye", "Shijian Li", "Zhijie Pan"], "abstract": "Automated 3D city generation, focusing on road networks and building layouts, is in high demand\nfor applications in urban design, multimedia games and autonomous driving simulations. The\nsurge in generative AI models facilitates the design of city layouts in recent years. However, the\nlack of high-quality datasets and benchmarks hinders the progress of these data-driven methods\nin generating road networks and building layouts. Furthermore, few studies consider urban char-\nacteristics, which are generally analyzed using graphics and are crucial for practical applications,\nto control the generative process. To alleviate these problems, we introduce a multimodal dataset\nwith accompanying evaluation metrics for controllable generation of Road networks and Building\nlayouts (RoBus), which is the first and largest open-source dataset in city generation so far. RoBus\ndataset is formatted as images, graphics and texts, with 72, 400 paired samples that cover around\n80,000 km\u00b2 globally. We analyze the RoBus dataset statistically and validate the effectiveness\nagainst existing road networks and building layouts generation methods. Additionally, we design\nnew baselines that incorporate urban characteristics, such as road orientation and building density, in\nthe process of generating road networks and building layouts using the RoBus dataset, enhancing\nthe practicality of automated urban design. The RoBus dataset and related codes are published at\nhttps://github.com/tourlics/RoBus_Dataset.", "sections": [{"title": "1 Introduction", "content": "The generative design of three-dimensional (3D) cities is of considerable importance across multiple fields such\nas urban planning [49, 24], multimedia games [37], and autonomous driving simulations [35]. Road networks and\nbuilding layouts are core components in designing or generating 3D urban configurations. The manual design of these\ncomponents has been prevalent in both the game industry and urban planning for a long time, which is often criticized\nfor being costly expensive and time-consuming. Consequently, there is a growing demand for automated approaches to\ngenerate diverse and extensive urban layouts tailored to specific city characteristics, which has prompted a significant\nincrease in research on automated urban generation recently. Such research not only alleviate the limitations of manual\ndesign but also deepens the understanding of our living environments.\nTraditional procedural modeling methods progressively generate road networks or building layouts based on grammars\nor L-systems [29, 22, 21], which rely on expert knowledge to manually design certain rule sets. With the rapid\nadvancement of generative artificial intelligence (AI), new approaches employing deep generative models have emerged\nin numerous design tasks. Highly relevant and prevalent research topics include the generation of poster layouts and\nhouse floor plans [14, 44], where numerous data-driven methods have been proposed to produce varied and contextually\nappropriate results, significantly reducing the reliance on extensive domain knowledge [39]. Overall, research on these\nsmall-scale (concerning the size of entities) layout generation has become mature due to the availability of numerous\npublic datasets [48, 42]. In contrast, methods for large-scale urban layout generation remain relatively immature.\nExisting works often rely on a limited amount of self-collected data (generally private) for specific tasks, such as road\ncompletion [9] and buildings generation [40]. Obviously, the lack of adequate, high-quality, open-source datasets and\nbenchmarks significantly hinders the progress of data-driven methods for city-scale road networks and building layouts."}, {"title": "2 Related Works", "content": "Computer-aided urban design has been a significant focus of computer graphics research for many years. This field has\nwitnessed a transition from traditional procedural modelling to data-driven generative AI models. Traditional methods\n[29, 22] depend on expert knowledge to manually create rules for generating roads, buildings or terrain. With the recent\nincrease in deep generative models, more researchers are exploring the potential of these models for urban design. In\nthis section, we briefly review these AI-based methods for designing road networks and building layouts. Then we\nsummarize the datasets relevant to these data-driven approaches."}, {"title": "2.1 Road Networks Generation Methods", "content": "Methods based on generative AI models usually learn the distribution of road networks from real world, free from\nthe dependency on extensive domain knowledge. These methods can be generally divided into image-based and\ngraph-based approaches. Image-based road network generation approaches treat road network generation as an\nimage generation problem. These methods utilize models such as generative adversarial networks (GANs) or variational\nautoencoders (VAEs) to learn the pixel-level distribution of road networks within images. Hartmann et al. first proposed\nthe GAN-based road network generation pipeline StreetGAN [12]. Subsequently, numerous studies [18, 9, 46] have\nattempted to use conditional GANs to generate roads according to some user-defined inputs or contextual content.\nBesides GAN-based models, Murcio et al. [19] trained a VAE model to capture real-world road patterns and generate\nnew roads by sampling the encoded features. To solve the problem of generating large-scale road networks, Birsak et\nal. [1] introduced the Variational Quantized VAEs into large-scale urban road network generation by incorporating\npopulation constraints. More recently, Przymus et al. [30] and Qin et al. [31] applied diffusion models to generate\nmaps via text prompts. Graph-based road network generation approches model the road network as a topological\nplanar graph consisting of nodes and edges, framing the road network generation as a planar graph generation problem.\nInspired by traditional turtle graphics methods, Chu et al. [7] proposed the Neural Turtle Graphics (NTG) model for\ngenerating road graphs, which employs an encoder-decoder with a recursive neural network to process and generate road\nnetworks. However, it is primarily limited to understanding patterns within a specific local scope and is suited only for\nsmall-scale road network generation. To address this limitation, Owaki et al. [27] introduced RoadNetGAN, applying\nthe principles of NetGAN [3] to large-scale road network generation using random walks. However, it struggles to\nproduce diverse and plausible road configurations, often resulting in numerous sharp turns. Overall, Image-based\napproaches for road networks generation suffer from modeling topological features, while graph-based approaches\nstruggle to encode spatial information. Furthermore, both of them fail to generate road networks with the desired\nproperties."}, {"title": "2.2 Building Layouts Generation Methods", "content": "Research on building layouts generation have certain similarities with generating road network layouts, as both learn\nspatial patterns from real-world design instances. Models like LayoutGAN++ [20] and LayoutVAE [17] provide basic\nframeworks for layout generation, which are widely used and improved in tasks such as document and poster layouts\ngeneration [47, 32, 4], house floor planning [5, 26]. When it comes to larger-scale building layouts generation, Fedorova\net al. [11] and Quan et al. [33] use GAN-based methods to generate building layouts for various cities, demonstrating\nthe model's adaptability to different urban morphologies. BlockPlanner [44] is proposed for generating building layouts\nin dense urban blocks, assuming buildings are in rectangular shapes within rectangular city plots, and uses graph\nmodels to manage the building layouts. BlockPlanner is limited for more complex block shapes, irregular buildings,\nand large-scale building layouts. To address these limitations, He et al. [13] introduced a VAE based graph attention\nnetwork for modelling and generating building layouts, capable of handling arbitrary block shapes and building types.\nAdditionally, generative AI design for building layouts has widely attracted experts on urban designing [24, 16, 38],\nwhich prompts the application in urban analysis and planning."}, {"title": "2.3 Datasets for Generative Urban Design", "content": "Generally, small-scale layout generation tasks such as poster or indoor room layout are relatively mature and widely\nstudied, benefiting from numerous public dateset such as Magazine [48] and RPLAN [42] dataset, which collects 3,919\nmagazine pages and 80K room layouts respectively. For larger-scale generation tasks such as urban road networks\nand building layouts generation, it have been a long time that struggling with limited amount of data. For instance,\nYan et al. [45] collect 2, 194 samples to train a model that classify building patterns. Wu et al. [40] collect data from\nSingapore to generate building layouts based on road networks. Przymus et al. [30] provide large amount raw maps\nfor text to image generation. Thanks to recent works published by Chen et al. [6] and He et al. [13], which collects\nlarge amount of data for community building layouts generation, prompts the advancement of large-scale generation\nmethods. However, these open-source data lacks of the city-scale buildings and matched road networks, which has a\ngap to applied to large-scale city generation. Situations in road network generation are even worse. Researchers on this\narea are struggling with inadequate data samples. For example, Chu et al. [7] collect 17 unique cities and select the\nmost densely annotated 10km\u00b2 region within each city from OSM by complex preprocessing. Owaki[28] et al. collect\n4 cities from Japan of 10km\u00b2. Yang et al. [46] collect 2586 road images from three major Australian cities. Birsak et al.\n[1] collect around 400km\u00b2 datas. Unfortunately, these self-collected data are of small amount and are close-sourced.\nResearch in road networks generation are eaging for a large-scale dataset and benchmarks.\nTo summary, numerous datasets for poster and house floor plans are benefiting these small-scale layout generation\ntasks. Recent studies that focus on larger scale generation tasks, such as community layout planning, often provide\ndatasets that only include buildings. These datasets are insufficient for city generation, as they lack data on city-scale\nbuildings and corresponding matched road networks. Moreover, research on road network generation typically relies on\nself-collected data comprising a limited number of samples. This highlights a pressing need for large-scale, high-quality\nroad network datasets and corresponding benchmarks. In this paper, we intend to overcome existing challenges in 3D\nurban generation through the release of the RoBust Dataset, which is expected to encourage progress in data-driven\nmethods for generating road networks and building layouts."}, {"title": "3 RoBus Dataset", "content": "In this section, we firstly describe the proposed RoBus dataset, and then illustrate the pipeline to generate the dataset\nfrom raw data. Lastly, we analyze and explore the RoBus dataset statically and demonstrates its highlights."}, {"title": "3.1 Dataset Description", "content": "The RoBus dataset is a comprehensive and multimodal dataset for generative urban design, focusing on generation of\nroad networks and building layouts. It is composed of:\nImages: This component includes tif files with 6 channels, each representing a different urban element: primary roads,\nsecondary roads, water bodies, green spaces, buildings with heights, and density. Each image contains coordinate\nreference information, which facilitates further processing and expansion.\nGraphics: This component includes simplified road network graphs in gpickle format and vectored buildings with\nheights in geojson format. These 3D vectored buildings are provided at both city tile and block scales.\nTexts: This component encompasses statistical values, labels, and descriptive texts that detail characteristics of urban\ntiles, such as road orientation, traffic convenience, building height, and density.\nThe RoBus dataset encompasses an extensive area, covering approximately 80,000 square kilometers across multiple\nregions in Australia, China, Europe, and the United States. This broad geographic coverage ensures that the dataset\ncaptures a diverse range of urban layouts. It is designed to be scalable for a variety of existing generative tasks related\nto road networks and building layouts, including: Geometry Constrained Generation such as generating secondary\nroads based on primary road networks, roads based on density and land-use maps [46], building layouts based on\nroad layouts [40], or boundary maps [6]. Graphic Generation such as generating topological road networks [7, 28]\nand vectored building layouts [13]. Text-to-Image Generation techniques, such as urban map generation [30] and\ncompletion [31], utilize text descriptions with popular models like CLIP and Stable Diffusion. Others including urban\nanalysis and planning [2], road network competition [9], and road topology extraction [23]."}, {"title": "3.2 Data Collection and Generating Pipeline", "content": "As shown in Fig. 3, our data collection and generating pipeline can be generally summarized as follows:\nStage 1: Data Collection and Preprocessing. To capture the spatial patterns of road networks and building layouts on\na large scale, we collect raw data from OpenStreetMap 2 to extract road networks, rivers, greenery and building contours\nby filtering different OSM tags, which are detailed in the appendix. However, data from OSM are generally noisy and\nincomplete, and thus cannot be directly applied to model training without complex preprocessing [6]. To clean the\nroad network data, we rasterize roads into images and then apply thinning to modify the roads, as suggested by [40].\nAdditionally, we observed that building contours from OSM frequently lack height information and are often incomplete.\nTo address these limitations, we enhanced the extracted OSM data with publicly available datasets regarding building\nheights. Notably, we collected data from Microsoft\u00b3 for areas in Australia, Europe, and the United States, as well as the\nCNBH dataset [43] for areas in China. We align OSM building contours with CNBH tiles and calculate the average\nheight values of valid pixels. For areas with missing values, we estimate the heights using nearby buildings within a\n300m radius. If data are still unavailable, we default to a height of 24 meters. Through this meticulous preprocessing,\nwe aimed to enhance the quality and reliability of the building information in the RoBus dataset, which is crucial for\naccurate urban modeling and generation.\nStep 2: Construction of Images. To construct the image component of the RoBus dataset, we rasterize the preprocessed\ndata to produce tif files with six channels. These channels include primary roads (Road-P), secondary roads (Road-S),\nwater bodies (rivers or lakes), green spaces, density [1], and buildings with height information. We choose the Pseudo-\nMercator projection (EPSG:3857) as coordinate reference system (CRS) to these tif images, with a resolution of 5\nmeters per pixel. Given that each geojson file may have slightly different boundaries when rasterized, we identified the\ncommon area and align them to ensure consistency. After alignment, we crop the tif images into small tiles with a 20%\noverlap, each sized at 256 \u00d7 256 pixels, corresponding to an actual area of approximately 1.6km\u00b2. These tiles with\n6 channels constitute the image part of the RoBus dataset, which supports to various image-based generation tasks.\nAdditionally, we preserve the coordinate system and geographic coordinates during the cropping process, aiming to\nfacilitate further research on larger-scale generation tasks."}, {"title": "Step 3: Construction of Graphics", "content": "To construct the topological graph of road networks, we combine the first\ntwo channels of the images, which represent primary and secondary road networks, and then skeletonize the result\nusing morphological thinning methods. Given that skeletonized road graphs are densely populated with redundant\nnodes, leading to excessive computational costs for further processing, we simplify these graphs using Eq. 1. This\nsimplification process preserves critical features such as road joints and sharp turns. In Eq. 1, eki represents the vector\nfrom node vk to node vi in the road graph GR. The function m(\u00b7) refers to the operation that merges two closely\npositioned nodes, and Ctr is the threshold defined for smoothness. The simplified road network graph serves three\nprimary purposes. First, it facilitates the computation of topological attributes essential for transportation research,\ncrucial for generating the text domain in our dataset. Second, the topology of road networks typically forms cycles\nthat outline city blocks. Third, the graphic structure is directly applicable to topological road generation methods like\nrandom walks [7].\n$C_{tr} <|cos (e_{ki}\\cdot e_{kj})| \\leq 1, \\forall v_k \\in \\{v\\in m(G_R)|deg(v) = 2\\}$  (1)\nWe provide vectored building layouts with heights at both the city tile scale and the city block scale. To generate\nbuilding layouts at the block scale, we use boundaries that are automatically partitioned by road networks. A critical\nchallenge in this process is to efficiently identify as many geometric minimal cycles within the road network's topology\nas possible. For clarity, we define the geometric minimal cycle as a cycle in a graph with geometric coordinates that\ndoes not enclose any other cycles. Most existing algorithms primarily focus on finding basic cycles or enumerating all\ncycles within a topological graph, without considering the geometric positions. To address this gap, we develop a new\nalgorithm tailored for finding geometric minimal cycles. Initially, we iteratively remove all vertices with a degree of\none. Next, for each node with a degree of two, we identify simple paths that include the node and its two immediate\nneighbors, restricting the path length to a maximum of 12 edges to ensure efficiency. Finally, we examine all simple\npaths to pinpoint the shortest cycles, designated as geometric minimal cycles. We repeat the last two steps until no\nvertices remain in the graph. Details can be found in the appendix.\nThe topological graph of road networks and the vectored buildings with heights at both the city tile scale and the city\nblock scale constitute the graphics component of the RoBus dataset. These graphics are crucial for modeling and\ngenerating urban layouts with specific properties, essential for advanced 3D urban analysis and planning."}, {"title": "Step 4: Construction of Texts", "content": "We analyze the attributes of the road networks and building layouts statically to\ngenerate labels and texts. Drawing on urban planning research, we categorize the road networks based on their density\nand orientation. Road density is classified as either 'dense' or 'sparse', influencing traffic flow and accessibility. The\norientation, assessed through the entropy of street bearings [2], indicates the degree of order (e.g., grid-like road\nnetworks) or disorder (e.g., random road networks) in the road layout. We also categorize buildings based on density\nand average height, which includes categories illustrated in Fig. 2. On basis of these labels and categories, we generate\ndescriptive text using predefined templates, where sentences begin with \"OSM,\" as suggested by [30]. These texts serve\nas prompts to describe the characteristics of urban areas and hold potential for application in text-to-image models."}, {"title": "Step 5: Filter for Different Tasks", "content": "For specific generation tasks like creating building layouts from road networks, we\nfilter out tiles that lack roads or buildings. This filtering process is similarly applied to other targeted tasks."}, {"title": "3.3 Dataset Analysis and Highlights", "content": "To provide a more detailed demonstration of the RoBus dataset, we perform a static analysis of key urban elements\nsuch as road length, building count, and boundary count at the city tile scale, detailed in Table 1. To enhance the\nvisualization of these static results, we present them in Fig. 4. Specifically: (a) illustrates the orientation of primary\nroads in Melbourne as selected from the RoBus dataset. (b) displays the proportion of different building types within a\ncity block. (c) and (d) show the distribution of road orientation and road density in the RoBus dataset, respectively. We\ncompare the RoBust dataset with existing relevant dataset, as shown in Tab. 2. The RoBus dataset is the largest of its\nkind with 72, 400 paired samples that include multimodal data such as road graphs, building vectors, and labels, which\nare missing in existing datasets."}, {"title": "4 Experiments", "content": "In this section, we design and conduct experiments to address the following research questions (RQs) both qualitatively\nand quantitative using the proposed benchmarks detailed in Section 5. We target at demonstrating the effectiveness,\nscalability, and applicability of the RoBus dataset, as well as the proposed baselines that incorporate urban attributes\ninto the generative design process."}, {"title": "4.1 Generation based on Geometry Constraints", "content": "We select two representative tasks, generating road networks conditioned on landuse maps (Task I) and generating\nbuilding layouts conditioned on road networks (Task II), to conduct the geometry constraints based urban generation.\nGANs have marked a significant advancement in generating road networks [12, 46] and building layouts [40, 41, 16].\nGiven the widespread adoption of GANs, we trained the popular model pix2pix [15] for Task I and II."}, {"title": "4.2 Generating with Desired Properties", "content": "For the task of generating road networks (Task III), we introduce a baseline model that incorporates desired charac-\nteristics, such as grid-like road networks with low orientation order, and optimize road connectivity to achieve higher\ntraffic convenience. To construct the simplest version of our baseline, we adapt the pix2pix model to constitute the\nbackbone, as shown in Fig 5. The channel number of input local density maps and the targeting road network maps in\nthe generator are limited to one. We integrate the road attributes R such as global road density and orientation order\nin the encoded latent vector in the U-like generator. The decoder synthesize binary road images conditioned on the\nencoded local density maps and the concatenated road attributes vectors.\nTo enhance the topology of the generate roads, which have an great impact on the urban attribute of traffic convenience,\nwe focus on the topological structure of generated roads. Specifically, we extract the topological skeleton of the\nsynthesized images and calculate the center-line dice score to enhance the road connectivity, which has proven to be\ndifferentiable in [34, 25]. The overall loss of the generator is formulated as Eq. 2.\n$L(G_G) = \\lambda_1 L_1 + \\lambda_2 \\mathbb{E}_{P_{x(z|r)}} [-log D(G_G(z|r)] + \\lambda_3 L_{topo}$  (2)\nwhere L\u2081 is the L1 loss to evaluate the similarity of the generated image and the ground-truth images, r is the road\nattributes conditions, $L_{topo}$ is the center-line dice loss to enhance the topology of generated graph roads.\nFor vectored building layout generation tasks, we focus on generating buildings layouts constrained by the boundary of\na city block (Task IV), which match the predetermined density and heights. This is achieved by integrating building\nattributes into the generative models Conditional Variational Autoencoders (cVAEs). As shown in Fig. 6, we includes\nan autoencoder to compact the building boundaries into latent vectors b, and graph attention networks (GAT) [36] as the\nbackbone of the encoder and decoder. Additionally, we encode the building heights and density as one-hot vector a to\nserve as the attribute prior of buildings in the city block. We follow GlobalMapper [13] to transform buildings into\na canonical spatial format and subsequently into graph structures G. To enable the learning of building heights, we\nadd building heights to nodes' attributes in G, as well as the original building location and minimum bounding box in\nGlobalMapper [13].\nDuring the training process, the conditional VAE learns to capture the distribution $p(G_B|b, a)$ in the dataset, which\nrepresents the probability of generating a building graph GB conditioned on the encoded boundary vectors b and\nattribute vectors a. The learned distribution is then sampled for generating new building graphs. The model captures\n$p(G_B|b, a)$ by maximizing its evidence lower bound, as frequently used in conditional VAEs [50, 13]. Additionally, to\nmake the model focusing on building attributes such as heights, we measure the similarity with groundtruth GB using\nL2 loss. To sum up, the overall loss functions is formulated as Eq. 3.\n$\\mathcal{L} = \\beta_1 ||G_B - \\hat{G}_B||_2 + \\beta_2 [\\mathbb{E}(log(p(G_B|z,b,a))) - D_{KL}(q||p_{z|b,a})]]$  (3)\nwhere h refers to the attributes in the graph such as heights, p(z|b, a)) is the prior distribution of z conditioned b and a,\nand q refers to q(z|GB, b, a), which is the approximate posterior distribution of the latent variables."}, {"title": "5 Results and Analysis", "content": "In this section, we introduce the benchmarks with comprehensive evaluation metrics. Additionally, we report our\nqualitative, quantitative and ablation results to answer the RQs in section 4."}, {"title": "5.1 Evaluation Metrics", "content": "To comprehensively evaluate methods related to the generation of road networks and building layouts, we introduce the\nbenchmark that assesses the quality, diversity, validity, and urban properties of the synthesized results."}, {"title": "5.2 Overall Results (RQ1,2)", "content": "We report the quantitative results for the four tasks in Tab. 3, where \"DIV\" stands for diversity, \"O\" for road orientation,\n\"C\" for traffic convenience, \"Ro\" for road networks and \"Bu\" for building layouts generation tasks. \"WD\" measures the\ndistribution of building counts, and \"V\" refers to validity. We divided the dataset into training, validation, and testing\nsets with ratios of 8:1:1, respectively. The qualitative results are displayed in the (a)-th row of Figure 8, where the first\nand third columns show the generated results, and the second and fourth columns present the ground truth. It is evident\nthat the model used for Task I has successfully learned the pattern that roads should not cross mountains. Compared\nTask I with III, both of which focus on road network generation, we conclude that Task III achieves superior image\nquality with a lower FID and higher road connectivity, but it also exhibits reduced diversity. Additionally, Task III is\ncapable of generating more grid-like road networks when provided with corresponding attribute vectors.\nFor the building layout generation tasks (II and IV), we conclude that the model proposed in Task IV achieves much\nhigher quality, evidenced by lower FID and WD. However, it tends to generate more buildings out of boundaries\ncompared to the image-based model in Task II. The visualization of results for Tasks II and IV is presented in the\n(b)-th and (d)-th rows of Fig. 8, respectively. Additionally, the (d)-th row shows that building density attributes indeed\ncontrols the generative process."}, {"title": "5.3 Ablation Studies (RQ2,4)", "content": "We conduct ablation studies for road network generation to answer RQ2 and RQ4 in this subsection, as presented in\nTab. 3. We conclude that the methods proposed in Task III greatly enhance connectivity by utilizing topological loss.\nAdditionally, we choose the 5m and 10m resolution (denoted \"RES_5m\" and \"RES_10m\" in Tab. 3) as comparison, and\nconclude that as resolution changing from 5m to 10m, the quality of generated results worsens while diversity increases.\nTo explore the influence of dataset size, we randomly selected 10,000 samples from the entire dataset, denoted as\n\"Global_1w\". We conclude that a smaller dataset size leads to higher diversity, compared to Task I. Additionally, we\ncompared \"Global_1w\" with \"US_1w\", which indicates selected 10,000 samples within the United States. We conclude\nthat a smaller collection area results in lower diversity, which underscores the importance of the diverse geographic\ncoverage provided by the RoBus dataset."}, {"title": "5.4 Applied to Auto Driving Simulations (RQ3)", "content": "To validate the applicability of the RoBus dataset and proposed baselines in Task III and IV, we applied the generated\nresults to autonomous simulation softwares such as CARLA [8], which is built based on 3D game engine. As shown\nin last three rows of Fig. 8, our pipeline for generating 3D urban scenes proceeds as follows. Initially, we generate\nroad network graphics with a low orientation (grid-like road networks), using the methods proposed in Task III. For\neach city block boundary partitioned by road networks, we generate vectored building layouts with height using the\nmethods outlined in Task IV. We transfer the generated road networks to OpenDRIVE format, starting by partitioning\nthe road network graph into linestrings, which are then applied with CRS (WGS84). More importantly, we construct 3D\nbuildings based on the generated building layouts with heights, and render the white building models in UnrealEngine\nvia randomly assigning different materials."}, {"title": "6 Coclusion and Future Work", "content": "In this work, we introduce the RoBus dataset, the first and largest open-source multimodal dataset designed for\ngenerative 3D urban design, specifically focusing on city-scale road networks and building layouts, which addresses the\nurgent need for comprehensive, high-quality training data for deep generative models. To make it more applicable, we\napply the generated 3D cities in UnrealEngine. However, the automated rendering of buildings is neglected in our work,\nwhich represents a promising topic for further research. More importantly, there is substantial room for improvement\non the generative model for generative 3D urban design, especially based on graphics like road networks topology and\nvectored buildings. We expect the RoBus dataset and proposed baselines to inspire more creative and practical work in\n3D city generation for multimedia games, metaverse and other socially aware multimedia applications."}, {"title": "A APPENDIX OVERVIEW", "content": "\u2022 Section B introduces the key-value pairs used in OSM tags for extracting raw OSM data.\n\u2022 Section C describes the algorithm developed to find the Geometric Minimal Cycle in a graph.\n\u2022 Section D presents additional statistical results from the RoBus dataset.\n\u2022 Section E provides further examples from the RoBus dataset."}, {"title": "B Data collection", "content": "Roads plays a pivotal role in our dataset. We utilize road data sourced from OpenStreetMap (OSM) 4 to acquire road\nsegments and corresponding labels in OSM standards. Specifically, we extract key-value pairs assigned to each \"way\".\nThe key used to identify ways as roads is \"highway\", and the associated value specifies the type of roads. In our\ndataset, roads are divided into primary and secondary roads to support tasks like graded road networks generation (e.g.,\ngenerating secondary roads based on the primary roads in a city). We conduct the categorization based on the values\ndelineated in Tab. 4. For instance, to construct Road-P in our dataset, we extract roads in OSM which have the tag of\nhighway:motorway, highway:trunk, or highway:primary.\nSimilarly, we also extract water bodies and greenery spaces according to corresponding key-value pairs in OSM. To\nextract water bodies, we select the key 'water' with values 'reservoir' and 'river'. Additionally, we select the key\n'natural' with values 'water', 'wetland', 'glacier' and the key 'leisure' with the value 'nature reserve'. We also select the\nkey 'waterway' with values \"riverbank\", \"dock\", \"canal\", \"drain\", \"ditch\", \"stream\", \"brook\", \"wadi\", and \"drystream\".\nTo extract greenery spaces, we mainly select the key 'landuse' for values 'forest', 'farmland', 'allotments', 'meadow',\n\"scrub\", and \"grass\". To complete, we also select the key 'natural' with the value 'wood' and the key 'leisure' with the\nvalue 'garden'."}, {"title": "C Geometric Minimal Cycle", "content": "We employ automated partitioning of boundaries by road networks to generate building layouts at the block scale. A\nkey challenge in this process is the efficient identification of geometric minimal cycles, which are defined as the cycles\nin a graph with geometric coordinates that does not enclose any other cycles. The algorithm designed to find geometric\nminimal cycles is shown in Algorithm 1 and Fig. 9. Specifically, We begin by removing all vertices with a degree of"}, {"title": "D Dataset Statics", "content": "We provide a more detailed statics of the RoBus dataset in Tab. 5 and Fig 10. As shown in Tab. 5, the RoBus Dataset\nencompasses a total of 72,400 tiles, with the United States contributing the highest number at 37,429 tiles. Australia's\ndata in the dataset consists of 4,101 tiles, with roads totaling 39,771 km and an area coverage of 4,493 square kilometers.\nChina's data in the dataset includes 19,243 tiles, with road lengths of 123,227 km and an area coverage of 21,211\nsquare kilometers. The dataset includes 11,633 tiles from Europe, covering 12,960 square kilometers and including\n105,450 km of roads. For the United States, the dataset contains 37,429 tiles that cover 41,278 square kilometers and\ninclude 357,496 km of roads. The RoBus Dataset's selection from different regions around the world ensures substantial\ndiversity, reflecting a wide range of geographical variations."}, {"title": "E RoBus Examples", "content": "We visualize the tif images and graphics such as road topology (the fourth column) and building vectors (the fifth\ncolumn) in Fig. 11. The visualization of tif images is structured into three columns, each highlighting different aspects\nof the data: The first column displays images visualizing all channels except the density channel. The images are\nprocessed to exclude the density information, providing a clear view of the spatial distribution and characteristics of the\nother data layers. The second column focuses on visualizing channels that represent water bodies, greenery spaces, and\ndensity. The images are crafted to specifically highlight these features, allowing for an immediate visual assessment of\nenvironmental and urban planning elements. The visualization in the third column is dedicated to road channels. Here,"}]}