{"title": "A RAG Approach for Generating Competency Questions in Ontology Engineering", "authors": ["Xueli Pan", "Jacco van Ossenbruggen", "Victor de Boer", "Zhisheng Huang"], "abstract": "Competency question (CQ) formulation is central to sev-eral ontology development and evaluation methodologies. Traditionally, the task of crafting these competency questions heavily relies on the effort of domain experts and knowledge engineers which is often time-consuming and labor-intensive. With the emergence of Large Language Models (LLMs), there arises the possibility to automate and enhance this process. Unlike other similar works which use existing ontologies or knowledge graphs as input to LLMs, we present a retrieval-augmented generation (RAG) approach that uses LLMs for the automatic generation of CQs given a set of scientific papers considered to be a domain knowledge base. We investigate its performance and specifically, we study the impact of different number of papers to the RAG and different temper-ature setting of the LLM. We conduct experiments using GPT-4 on two domain ontology engineering tasks and compare results against ground-truth CQs constructed by domain experts. Empirical assessments on the results, utilizing evaluation metrics (precision and consistency), reveal that compared to zero-shot prompting, adding relevant domain knowledge to the RAG improves the performance of LLMs on generating CQS for concrete ontology engineering tasks.", "sections": [{"title": "1 Introduction", "content": "An ontology is a formal, explicit specification of a shared conceptualization of domain knowledge that can be communicated between humans and computers [14]. In recent years, significant progress has been achieved in the field of knowledge and ontology engineering due to the surge of data-intensive applications and the growing need for structured knowledge representation. Central to many methodologies for the development and evaluation of ontologies are competency questions (CQs), a set of queries in the form of questions that outlining and con-straining the scope of knowledge represented in an ontology which an ontology must be able to answer [18]. Informal CQs are expressed in natural languages, whereas formal CQs are expressed in the formal language of the ontology [6]. In this paper, we focus on informal CQs."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Competency questions formalisation", "content": "Traditionally, ontology engineering relies on manual efforts of domain experts and knowledge engineers, especially for the formalisation of CQs. Different works have been investigated in identifying CQ patterns to improve the automation of CQ formalisation. Wi\u015bniewski et al. [17] identified 106 CQ patterns by analysing a dataset of 234 CQs and their SPARQL-OWL queries for several ontologies in different domains. In their follow-up work [16], they released BigCQ, the largest dataset of CQ templates with their formalisation into SPARQL-OWL"}, {"title": "2.2 Automation of generating CQs", "content": "Several efforts have been investigated on leveraging natural language processing (NLP) or LLMs to generate CQs. The authors of [3] proposed AgOCQs, a method that took advantage of the combination of NLP techniques and a text corpus with CQ templates to generate CQs, whereas no CQ templates are required for our approach. Rebboud et al. [11] investigated the suitability of LLMs to automati-cally generate CQs given an existing ontology, and experiments were conducted with six LLMs and five ontologies. Alharbi et al. [1] proposed RETROFIT-CQs, a method to generate CQs using LLMs by extracting triples from existing on-tologies and feeding them to three prompt templates of an LLM. These two approaches are based on the premise that there is already an ontology or a KG, which is not often the case. Our approach addresses this limitation by using the domain literature as input to our RAG pipeline to provide LLMs with accessible and up-to-date domain knowledge."}, {"title": "3 Methodology", "content": "The primary objective of this paper is to investigate the effectiveness of RAG with LLMs to generate CQs in ontology engineering. Our RAG approach consists of two components, a RAG pipeline and a prompt engineering."}, {"title": "3.1 RAG pipeline", "content": "Figure 1 illustrates the main steps in the RAG pipeline: domain knowledge indexing, relevant data retrieval, and response generation.\nIn Step 1, the knowledge base consists of a set of documents that serve as external sources of knowledge to augment the generative ability of LLMs for specific tasks. During the data indexing process, each selected document is first split into small chunks according to the chunk size defined by different LLMs. The chunks are then converted to embedding vectors using an embedding model. The original chunks and their respective vectors would be indexed and stored in a vector database for retrieval.\nIn Step 2, a user query or a user prompt expressed in natural language is converted to an embedding vector to retrieve relevant chunks from the knowledge base. Top-k similar chunks are retrieved using a similarity method such as cosine similarity.\nIn Step 3, the retrieved chunks serve as a context for LLMs to generate the answer for the user query."}, {"title": "3.2 Prompt engineering", "content": "In the field of LLMs, a prompt serves as an input that directs the model's genera-tion of responses. The use of prompting techniques involves carefully formulating these prompts to optimize the effectiveness of LLMs. This process requires in-tentional structuring and wording of prompts to correspond with the model's abilities and the task description. Prompts significantly affect the performance of LLMs especially with zero-shot prompting [7]. Since LLMs are sensitive to"}, {"title": "4 Experiment", "content": "To evaluate our RAG approach from Section 3, we conduct two experiments, where we replicate two domain expert-driven ontology engineering tasks. We choose this setup since these existing experiments provide us with a set of ground-truth CQs identified manually by experts to compare our results to. Details about the datasets and codes could be found in this repository 1."}, {"title": "4.1 Task description", "content": "We take two ontology engineering tasks to evaluate our approach for generating CQs. The first is the construction of a knowledge graph of empirical research"}, {"title": "4.2 Hyperparameters setting", "content": "As mentioned in Section 3.1, the selection of documents for the knowledge base in our RAG pipeline is very important.\nFor the KG-EmpiRE, we follow the methodology of how the authors of KG-EmpiRE derived the ground-truth CQs. Since the purpose of KG-EmpiRE is to capture the state and evolution of empirical research in requirement engineering, the authors select a visionary paper [13] to identify 77 CQs. We also select this visionary paper as one of the most important documents in the knowledge base. To investigate the impact of number of papers in our RAG approach, we also include other related publications on the state and evolution of the topic mentioned in the KG-EmpiRE paper.\nFor HCIO, since the authors of HCIO do not explicitly mention how these 15 CQs are identified, we select the referenced papers mentioned in the HCIO papers [5] that describe how other relevant ontologies are developed.\nAll selected referenced papers are in PDF formats. The hyperparameter $N_{paper}$ was set to 1, 2, 3, 4, 5 and 10. Since gpt-4o is the latest model of OpenAI's GPT models, we set the hyperparameter model to gpt-40. Temperature temp is set to 0.5, 0.75, 1.0, 1.25 and 1.5. Each experiment, for every hyperparameter configuration, was repeated 10 times."}, {"title": "4.3 Variable setting for prompts", "content": "We design prompts for the two tasks based on the prompt template described in Section 3.2. This template includes four variables that need to be determined. The domain name is chosen based on the selected task. The number of CQs matches the number of ground-truth CQs provided by domain experts. For the definition of CQs, we take the definition of an informal CQ in [15], a competency question is a natural language question that specifies the requirements of an ontology and can be answered by that ontology. For the purpose of the ontology, we refer to the original papers of KG-EmpiRE [8] and HCIO [5], which elaborate the objectives of the corresponding ontology or knowledge graph."}, {"title": "4.4 Evaluation", "content": "We use precision to evaluate the quality of LLM-generated CQs against a set of ground-truth CQs designed by domain experts. In addition, we use consistency to evaluate the impact of different temperature settings for the task."}, {"title": "5 Results and discussion", "content": ""}, {"title": "5.1 Performance of generating CQs", "content": "Figure 2 shows the precision of our RAG approach, compared to zero-shot prompting in generating CQs for two domain ontology engineering tasks, with different number of papers in the knowledge base for the RAG.\nAs a first look, we observe that for the task in the RE domain, our RAG approach performs much better than zero-shot prompting. As $N_{paper}$ increased, there is a noticeable trend where precision increase marginally. These suggest that the addition of domain knowledge could enhance the understanding capacity of LLMs for generating CQs for domain ontology engineering. In particular, the best precision is achieved when we use the only visionary paper as input to the knowledge base in the RAG pipeline, following the methodology of the KG-EmpiRE paper to identify ground-truth CQs. These indicate that with a good selection of papers in the knowledge base, small size of a knowledge base could achieve good performance, which is very important from the perspective of token cost in closed-source LLMs such as OpenAI's GPT models.\nFor the precision of the task in the HCI domain, zero-shot prompting yields the highest precision, marginally better than our RAG approach across all $N_{paper}$. Similar to the performance of the task in the RE domain, increasing the num-ber of papers $N_{paper}$ ranging from 1 to 10 generally improves the precision, but does not surpass the precision achieved with zero-shot prompting. Therefore, taking into consideration of the precision performance and the token cost for processing the documents in knowledge base, our RAG approach might not fit for generating CQs for HCIO, compared to zero-shot prompting.\nFrom the perspective of zero-shot prompting, we observe that the precision in HCI is much higher than the precision in RE, which we think is due to the degree of abstraction of the target ontology or knowledge graph. The more concrete the"}, {"title": "5.2 Consistency of LLMs", "content": "Figure 3 shows the standard deviation of precision for task performance and the standard deviation of cosine similarity for the generated text with different temperature settings in two domain ontology engineering tasks.\nApparently, there are no obvious patterns about how consistency changes by different temperatures regardless of different consistency metrics, which contrasts with the assumption that with higher temperature, less probable tokens are more likely to be sampled, resulting in more random outputs. This suggests that the temperature setting would not affect the task performance of generating CQs in our RAG pipeline.\nThe ANOVA test results also confirm that temp doesn't have a significant impact on the task performance (p > 0.05).\nFrom the perspective of different domains, the vertical distance between $std_{precision}$ and $std_{cosine}$ in RE is significantly smaller than that observed in HCI. This suggests a reduced difference among two standard deviation for RE as opposed to HCI across the temperature range. This phenomenon may be at-tributed to the quantity of CQgt. Specifically, with 77 CQgt in RE, there is a higher likelihood that the 77 CQgt generated over 10 iterations exhibit overlap."}, {"title": "6 Conclusion", "content": "This study aimed to examine the capabilities of retrieval-augmented generation (RAG)-based large language models (LLMs) to generate competency questions (CQs) for ontology engineering. Additionally, we explored how various hyper-parameters within the RAG pipeline influence the task performance. A prompt template with four variables was specifically designed to direct LLMs in generat-ing CQs for domain ontologies and knowledge graphs. Using one LLM (gpt-40),"}, {"title": "Precision", "content": "Precision measures the accuracy of the CQs generated by the LLM. It is the ratio of true positives to the total number of CQs generated by the LLM. The precision of the CQs generated by the LLM can be defined as follows:\n$Precision = \\frac{TP}{TP + FP}$                                                                        (1)\nTrue positives (TP) is the number of valid CQgen and false positives (FP) is the number of invalid CQgen."}]}