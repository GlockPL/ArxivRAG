{"title": "System 2 reasoning capabilities are nigh", "authors": ["Scott C. Lowe"], "abstract": "In recent years, machine learning models have made strides towards human-like reasoning capabil-\nities from several directions. In this work, we review the current state of the literature and describe\nthe remaining steps to achieve a neural model which can perform System 2 reasoning analogous to a\nhuman. We argue that if current models are insufficient to be classed as performing reasoning, there\nremains very little additional progress needed to attain that goal.", "sections": [{"title": "1 Introduction", "content": "The dual process theory of thought processes is long standing within psychology (Wason & Evans, 1974;\nEvans, 2008; Stanovich & West, 2000) and was popularized more broadly by Kahneman (2012). In this\nframework, human thinking capabilities are conceptualized as two distinct modes of thought. System 1\nis fast, automatic, instinctive, and more emotional; System 2 is slower, effortful, deliberate, and more\nlogical. System 1 is, in essence, unconscious thought, and System 2 is conscious thought; though there is\nnot yet consensus on whether \"ah-ha\" moments which come following an incubation period are triggered\nby unconscious work of System 1 or 2 (Christensen, 2005; Gilhooly, 2016). Additionally, due to its\ninstinctive and reactive nature, System 1 is more prone to bias than System 2, though System 2 is not\nwithout bias.\nIn comparison to these two cognitive systems of thought, feed-forward neural networks are sometimes\ndescribed as being analogous to System 1. Their outputs are immediate and automatic, yielded immedi-\nately without what might call \"deliberation\". Like with System 1, the computational system producing\nthe output does not and can not provide an explicit explanation for why it produced a certain response,\nmaking interpretability challenging, even when attempting to induce it to provide an a posteriori justifi-\ncation for its response; Jung et al., 2022). Such systems are effectively performing pattern matching for\ntheir current stimulus against the body of data imbibed during training.\nIn comparison, symbolic rule-based algorithms (classical \"artificial intelligence\"), whether they are\nmanually or programatically created, can provide an explanation for their reasoning. However their\nperformance is limited because the space of the real-world is too large to be handled with a narrow set\nof rules that are coded in the stimulus domain.\nIn this work, we review the existing literature in the space of reasoning from the perspective of\nphilosophy and machine learning, and we speculate on what form a neural network would need to take\nfor it to be able to perform reasoning in the style of System 2. We argue the majority of hurdles needed\nto achieve this task have already been cleared, and there are a small number of pieces of the puzzle\nremaining. Thus complex agents, trained through deep learning, that can reason logically about the\nreal-world will be available in the near-term, if they are not here already."}, {"title": "2 Background", "content": "Historic texts indicate that ancient philosophers such as Plato used to think that thinking was synonymous\nwith an inner monologue. However, whilst an internal monologue (inner speech) is common, it is not\nubiquitous and most people overestimate how often their thoughts are expressed verbally (Hurlburt et al.,\n2013). There is a wide variety of inner experiences across humans (Hurlburt & Heavy, 2006), and most\npeople are surprised when they first discover that other people's internal experiences differ greatly from\ntheir own.\nThe modalities of human inner thought are (Hurlburt & Schwitzgebel, 2011; Hurlburt et al., 2013):\n\u2022\nInner speaking/inner monologue - thoughts expressed verbally, e.g. talking to yourself, hearing\nyour/a voice while recalling.\n\u2022\nInner seeing/visual imagery - thoughts expressed visually, e.g. picturing a memory or imagining\na hypothetical scene.\n\u2022 Feelings a conscious experience of emotional processes, e.g. sadness when grieving.\n\u2022 Unsymbolized thinking\nthoughts expressed without words or images, e.g. drinking a glass of\nwater, without internal discussion or commentary.\n\u2022 Sensory awareness attending to a sensory aspect of the environment for an unimportant reason,\ne.g. hearing someone talk but seeing the light reflecting off their glasses.\nMost people experience inner speech and inner imagery some of the time but not all of the time, with\nthe majority of their thought processes unsymbolized (Hurlburt et al., 2013). However there are outliers in\neach direction, with some people having no inner speech (anauralia), constant inner speech, no mind's eye\n(aphantasia), or extremely vivid mental imagery as detailed as sensory stimuli (hyperphantasia). Day-to-\nday observations of people across society demonstrate, and academic studies confirm, that people are able\nto complete tasks irrespective of whether their internal thoughts are represented through speech, imagery,\nor neither (Keogh et al., 2021; Hinwar & Lambert, 2021); though the lack of inner sight does impair the\nability to recall visual characteristics (Monzel et al., 2022; Bainbridge et al., 2021). Additionally, note\nthat those possessing an inner monologue who speak multiple languages can have their inner monologue\nflip between languages depending on recent context. These observations lead us to hypothesise that\nconscious thoughts (i.e. System 2 thinking) are fundamentally abstract in nature, but can be projected\nto language and visual modalities internally."}, {"title": "2.2 What is System 2 reasoning?", "content": "As a thought exercise, consider the task of solving this illustrative example from Kahneman (2012):\nA bat and a ball together cost $1.10. The bat costs $1 more than the ball. How much does the\nball cost?\nSystem 1 is responsible for the automatic response which immediately comes to mind on reading the\nproblem: ten cents. This answer is yielded in an involuntary manner, seemingly to all who hear the\nquestion for the first time. However, by engaging System 2 we can verify whether the intuitive solution\nis correct, and reason about it. By reflecting on the intuitive answer, one can observe that if this were\nthe price of the ball, the total would be $1.20, hence the answer is incorrect. Since as the total price is\nthe difference in price between the two objects ($1) plus twice the price of the ball, the answer is in fact\n5 cents."}, {"title": "2.3 Existing neural reasoning agents", "content": "Previous work has found that by prompting LLMs with an in-context example of chain-of-thought rea-\nsoning, and asking it to think step-by-step for its own answer, models can be coerced into \"thinking\"\nstep-by-step (Wei et al., 2022). Providing such a prompt changes this distribution of most likely next\ntoken to be a longer trajectory with smaller steps toward the solution to a question outputted first. By\nhaving the model attend to its own steps as it progresses, it builds on its previous steps such that its\nfinal result is more likely to be accurate (Wei et al., 2022; Li et al., 2024). However, recent work has\ndemonstrated the majority of the gains seen when using chain-of-thought prompting can be matched\nby prompting with a long series of task-independent filler tokens instead, suggesting the length of the\nsequence and the size of the compute graph is more important than the textual output (Pfau et al., 2024).\nThis implies the transformer can process data through unseen computations within the hidden layers of\nthe network, unwitnessed in the chain-of-thought tokens that it outputs. Such findings may be analogous\nto System 2 reasoning in humans, which we noted in \u00a72.1 are primarily non-symbolic but can be projected\nto consciously observed language streams (Hurlburt et al., 2013), though such a hypothesis is challenging\nto investigate due to the difficulties of interpreting deep transformer representations (Rai et al., 2024).\nIn the domain of closed-world games, tremendous gains were seen by applying deep reinforcement\nlearning models that learn through self-play to optimize a value function (Silver et al., 2016, 2017, 2018).\nIn this case, the value network can be fit to predict the likelihood of each player winning the game from a\ngiven position. The result of the game can be objectively determined by continuing to play the match and\nseeing who wins, providing a strong training signal to the value network. Since the value network is able\nto fit this task, it is able to steer the actor model effectively. Results are further improved by performing\na Monte Carlo Markov chain tree search at inference time, steered by the model's predictions to prune\nthe tree to a narrow range of feasible moves, to evaluate future game states and choose an optimal move.\nSuch searches are similar to the Tree-of-thoughts approach to improve chain-of-thoughts reasoning (Long,\n2023; Yao et al., 2023).\nWhen deploying LLMs on mathematics problems, step-level verification of chain-of-thought has been\nshown to be effective training technique (Cobbe et al., 2021; Uesato et al., 2022; Lightman et al., 2024)."}, {"title": "3 Future steps and potential pitfalls", "content": "Given the existing neural reasoning techniques, and their analogous relationship to human reasoning\nprocesses, we posit that networks already can learn to reason.\nMultiple works have shown training LLMs to reason step-by-step is best achieved by step-level feedback\n(Zelikman et al., 2022; Pfau et al., 2024; Lightman et al., 2024). One issue for training a reasoning model\nat scale is thus that there is a lack of large-scale reasoning datasets to train on in which humans have\nwritten out their train of thoughts explicitly, though some do exist (Yang et al., 2018). However, such"}, {"title": "3.1 Learning to reason", "content": "data can be acquired at modest scale, and (by explicitly labelling which steps are valid and which are\nnot) such data can be used to train a verifier that predicts whether individual logical reasoning steps are\nsound. This verifier (similar to the rationalisation evaluator used by Zelikman et al. (2022)) can serve a\nsimilar role to the step-wise maths problem solver of Lightman et al. (2024). Using this, we can bootstrap\nmore chain-of-thought data by tasking a pretrained LLM with chain-of-thought prompting to generate\nmore reasoning data, and discarding outputs which contain steps which do not pass the verifier, similar\nto that used in Lightman et al. (2024).\nNote that the verifier is an essential part of this pipeline, and it must be accurate in order for the\niterative self-distillation to be effective. But in any scenario where verification is easier than generation,\nthe verifier (even if learnt and imperfect) can be deployed to iteratively refine and distill the generative\nmodel (Christiano et al., 2018). An alternative bootstrap formulation would be to generate a large body\nof chain-of-thoughts data using chain-of-thought prompting applied on a large corpus of problems with\nknown solutions. We then train a verifier to, given a particular point in the chain-of-thought, classify\nwhether the model will get the right answer. This verifier model will serve a similar role to the value\nfunction in self-play RL systems (Silver et al., 2018), and we can fine-tune our model to generate its step\nof thoughts whilst trying to maximize the verifier's probability the problem will be solved. Since such a\nsystem bears similarity to Q-learning and STaR bootstrap reasoning (Zelikman et al., 2022), it might be\naptly given the name \"Q*\". We note that other recent work has successfully applied reinforcement learning\nfine-tuning to pretrained LLMs, such as reinforcement with human feedback (RLHF) (Ziegler et al., 2020)\nor with harmlessness feedback (Bai et al., 2022); and these methods can be improved by modifying\nthe method to provide direct feedback (Rafailov et al., 2024; Lee et al., 2024). The implementation we\npropose would be a similar reinforcement learning fine-tuning stage, but with a objective focused on\nreasoning accuracy.\nAll the components for this solution seemingly already exist in the literature, and it is even possible\nsuch a model has already been trained recently (OpenAI, 2024)."}, {"title": "3.2 Applicability", "content": "LLMs trained only on textual data are unlikely to master reasoning about the real-world, since their\nobservations of it are highly indirect. When humans communicate with each other, they do so with a\nlarge body of common experiences merely from being creatures raised and living in the real world. This\nmeans that many things that are taken for granted remain unstated as they are assumed to be known by\nall parties in the discourse.\nIn order for foundation models to be able to reason efficiently about the world, we speculate they\nwill need a world model that is built on sensory observations, not just text descriptions. More recent\nfoundation models have made progress in this direction (Zhang et al., 2022) by being multi-modal, pro-\ncessing both language and visual stimuli. However, we posit that further gains will be made when using\ndata which captures the richness of the real world through video data and (less abundantly) embodied\nsensorimotor data. Video data has rich features about the world, enabling the network to construct its\nown intuitive physics, infer cause and effect (Bardes et al., 2024)."}, {"title": "3.3 Scaling", "content": "Will scaling laws continue to hold for chain-of-thought reasoning, or will such models hit scaling problems?\nThe \"bitter lesson\" of machine learning has been that gains from methods that can exploit generic\ncompute scaling (e.g. larger and more flexible models, trained increasingly large datasets), in the long-\nrun outperform gains from human-knowledge adjustments due to Moore's law (Sutton, 2019). Thus\nwe postulate that reasoning models will naturally also benefit from utilizing general methods rather\nthan hand-tuned routines. This is evidenced by recent work deploying LLMs on mathematical problems\n(Snell et al., 2024), which found that evaluation performance increases as the amount of inference compute\nincreases."}, {"title": "3.4 Safety concerns", "content": "As new capabilities are introduced to AI models, it is important to monitor these frontier models for\npotential safety risks (Phuong et al., 2024). From an AI control perspective, ML agents which can reason\nand strategically plan present a much larger risk than passive models which merely predict things. Like\nany ML model in deployment, there is a societal risk that the model's learnt biases from its training\ndistribution will result in its behaviour diverging from human aspirations.\nBut more importantly, such a model raises the existential risk from AI models. Models which can\nreason can use their abilities to plan and strategize, potentially over the long-term. If allowed to act\nautonomously to achieve a goal, they may erroneously or surreptitiously plan with subgoals that involve\ntaking control of resources they should not have access too, etc. To mitigate these concerns, it is important\nthat training data be screened to ensure it does not contain instructions we would not wish an agent to\ntake when deployed in the wild.\nAnother concern regards the scrutibility of reasoning agents. Current LLMs must always project their\nchain-of-thought reasoning steps to English, though there are concerns that their internal computation\nmay not be fully reflected in their outputs (Pfau et al., 2024; Lyu et al., 2023; Lanham et al., 2023).\nFrom a gain of function perspective, it may be advantageous to train models that can reason in abstract\nconcepts that do not directly correspond to tokens in the training corpus. However, we are of the opinion\nthat steps must always be taken to ensure that model reasoning is projected into a frame (be it language\nor imagery) in which it can be explicitly and as completely as possible communicated to humans."}, {"title": "4 Conclusions", "content": "We have discussed the literature surrounding the philosophy of human inner thought and reasoning, and\nthe current neural network approaches to reasoning models. The current networks have strong analogues"}]}