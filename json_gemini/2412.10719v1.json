{"title": "Just a Few Glances: Open-Set Visual Perception with Image Prompt Paradigm", "authors": ["Jinrong Zhang", "Penghui Wang", "Chunxiao Liu", "Wei Liu", "Dian Jin", "Qiong Zhang", "Erli Meng", "Zhengnan Hu"], "abstract": "To break through the limitations of pre-training models on fixed categories, Open-Set Object Detection (OSOD) and Open-Set Segmentation (OSS) have attracted a surge of interest from researchers. Inspired by large language models, mainstream OSOD and OSS methods generally utilize text as a prompt, achieving remarkable performance. Following SAM paradigm, some researchers use visual prompts, such as points, boxes, and masks that cover detection or segmentation targets. Despite these two prompt paradigms exhibit excellent performance, they also reveal inherent limitations. On the one hand, it is difficult to accurately describe characteristics of specialized category using textual description. On the other hand, existing visual prompt paradigms heavily rely on multi-round human interaction, which hinders them being applied to fully automated pipeline. To address the above issues, we propose a novel prompt paradigm in OSOD and OSS, that is, Image Prompt Paradigm. This brand new prompt paradigm enables to detect or segment specialized categories without multi-round human intervention. To achieve this goal, the proposed image prompt paradigm uses just a few image instances as prompts, and we propose a novel framework named MI Grounding for this new paradigm. In this framework, high-quality image prompts are automatically encoded, selected and fused, achieving the single-stage and non-interactive inference. We conduct extensive experiments on public datasets, showing that MI Grounding achieves competitive performance on OSOD and OSS benchmarks compared to text prompt paradigm methods and visual prompt paradigm methods. Moreover, MI Grounding can greatly outperform existing method on our constructed specialized ADR50K dataset.", "sections": [{"title": "Introduction", "content": "To break through the limitations of pre-training models on fixed categories, Open-Set Object Detection (OSOD) and Open-Set Segmentation (OSS) have attract a surge of interest from researchers. In these fields, trained models can not only detect or segment predefined specific categories but also generalize to open scenarios, which greatly improve the ability and applicability (Li et al. 2022).\nInspired by the remarkable success achieved by foundational models (Radford et al. 2021; Li et al. 2022), mainstream OSOD and OSS methods employ a prompt as an input, which tells the model what to detect or segment in the image. Existing prompt paradigms can be mainly categorized into two types: text prompt paradigm and visual prompt paradigm. As for text prompt paradigm, users are required to provide a textual description to depict characteristics of detection or segmentation targets, and models are trained to align text prompt with visual contents in the latent space (Liu et al. 2023; Ding, Wang, and Tu 2022). Following SAM (Kirillov et al. 2023), another line of approaches employ visual prompts, such as points, boxes, and masks. The visual prompt needs to be manually designed that can locate specific targets. Such a design makes this process generally involves multi-round interaction to avoid ambiguous prompts (Kirillov et al. 2023).\nHowever, textual and visual prompt paradigms have the following limitations. First of all, the visual feature of specialized categories are difficult to be accurately described by text, and hence hinder the application of text prompt paradigm (Li et al. 2024; Jiang et al. 2024). Second, visual prompts heavily rely on multi-round human interaction, which makes it difficult to be applied into production pipelines (Kirillov et al. 2023). As shown in Figure.1, in X-ray defect detection, we need to detect and segment specialized categories, such as \u201cshrinkage porosity", "sinks\", and \"porosity\". These concepts are specific to the X-ray field, which cannot reflect the visual characteristics without industrial knowledge, such as their shape, size, and texture etc. Visual prompt might alleviate this issue by providing bounding boxes of \"shrinkage porosity\u201d, \u201csinks": "and \"porosity\u201d as prompts, but it requires users to annotate or check bounding boxes to make sure they cover the target areas (Kirillov et al. 2023). These interaction processes make a single-stage, fully automated inference pipeline impossible.\nIn this work, we establish a novel visual perception paradigm, i.e. Image Prompt Paradigm, which completely abandons traditional text prompts and visual prompts, achieving a single-stage and fully automated inference. Inspired by the fact that humans can quickly grasp the characteristics of a specific category after taking just a few glances at its instances, the proposed image prompt paradigm utilizes just a few image instances of target as prompts. These instances are automatically constructed and calculated by our proposed MI Grounding framework, which uses multiple images as prompts. To bridge the gap between specialized categories and visual content, MI Grounding introduce an image prompts selection encoder, which can encode, select and integrate image prompts. The encoder module possesses extensive prior knowledge at the visual level, and can extract inherent distinctive semantic information of image prompts. The encoded image prompts are then selected and integrated to highlight high-quality image prompts automatically. After aligning the image prompts with the predicted objects, MI Grounding is learned to handle specialized categories that are difficult to describe using text, and achieves single-stage and non-interactive inference. Extensive experiments have shown that the proposed image prompt paradigm and MI Grounding achieve excellent detection and segmentation performance.\nConcretely, our contributions can be summarized as follows:\n\u2022 We propose a novel visual perception paradigm: Image Prompt Paradigm. Different from existing text and visual prompt paradigm, this paradigm uses just a few image instances as prompts, which can understand specialized categories that are hard to describe by text in a single-stage and non-interactive manner.\n\u2022 We propose a novel framework named MI Grounding tailored for the proposed image prompt paradigm. MI Grounding utilizes just a few image prompts to perform Open-Set Object Detection and Open-Set Segmentation, and propose an image prompt selection encoder to select and integrate high-quality prompts.\n\u2022 Our approach achieves competitive performance on several datasets compared with mainstream Open-Set Object Detection and Open-Set Object Segmentation methods, which show the effectiveness of our proposed image prompt paradigm. To further demonstrate the superiority, we constructed a specialized ADR50K dataset, which contains a rich set of X-ray defect detection data. Experiments demonstrate that our approach can greatly improve the performance on this specialized dataset."}, {"title": "Related Works", "content": "Visual Perception Based on Text Prompt Paradigm. With the widespread application of foundational methods like CLIP (Radford et al. 2021) and BERT (Devlin 2018), open-vocabulary object detection and segmentation methods have achieved remarkable success in the general visual perception field. Researchers find that object detection and segmentation can be expressed as an alignment between text prompts and visual context information. Based on the concept above, these methods have made significant breakthroughs in zero-shot and few-shot learning. Grounding DINO (Liu et al. 2023) extends the training strategy of GLIP (Li et al. 2022) to DINO (Zhang et al. 2022a), achieving strong open-set detection capabilities. DetCLIP (Yao et al. 2022) and RegionCLIP (Zhong et al. 2022) utilize image-text pairs with pseudo boxes to expand region knowledge, thereby improving open-set performance. These text prompt paradigm methods rely on text encoders, like BERT, to model text queries. However, due to the ambiguity caused by the high information density of text and the potential mismatch between text descriptions and complex visual scenes, visual perception methods based on the text prompt paradigm have inherent limitations (Li et al. 2024).\nVisual Perception Based on Visual Prompt Paradigm. Researchers have begun exploring alternative prompt paradigms. SAM (Kirillov et al. 2023) pioneer an interactive open-set segmentation approach, introducing a novel prompt paradigm that includes boxes, points, masks, or lines covering the target. Subsequent researchers define this approach as the visual prompt paradigm and further explore its potential (Li et al. 2024). Semantic-SAM (Li et al. 2023) achieves semantic awareness by training on decoupled objects and parts classifications integrated from multiple datasets. Painter (Wang et al. 2023a) and SegGPT (Wang et al. 2023b) adopt a generalist strategy to tackle diverse segmentation tasks, conceptualizing segmentation as an in-context coloring problem. DINOV (Li et al. 2024) proposes a general contextual visual prompt framework, using visual context to understand new categories.\nVisual Perception with Image and Text Prompt. To further enhance model performance, some researchers have introduced additional target images to augment text prompts. MQ-Det (Xu et al. 2024) uses cross-attention and weighted addition to integrate image features into the text prompt, significantly improving model performance. However, when MQ-Det uses only images as prompts, the model's performance is poor. This indicates that MQ-Det has not fully exploited the potential of image prompts. In such methods, image prompts merely enhance the prompt features rather than"}, {"title": "Method", "content": "Our goal is to establish an image prompt paradigm, where the model completes open-set detection and segmentation by just taking a few glances at images of objects similar to the detection target. We first introduce how image prompts are constructed during the training and testing process. Then, we detail our proposed MI Grounding, including the model design and training strategy."}, {"title": "Image Prompt Paradigm", "content": "In order to eliminate the tedious interaction process similar to the visual prompt paradigm and ensure that the data of the detection target is not leaked, we build an image prompt library using the training split of the dataset. Specifically, we crop instance targets from the original images based on their detection box labels and store them categorized by their class labels. The detailed process of extracting image prompts from the original images is as follows:\n$p = Crop(I, L_{box}) = I[y : y + h_p, x : x + w_p]$,                                          (1)\nwhere $I \\in \\mathbb{R}^{3\\times H \\times W}$ represents the original large image, and $p\\in \\mathbb{R}^{3\\times h_p \\times w_p}$ is the instance target image obtained by cropping. $H$ and $W$ are the height and width of the original image. $L_{box} = {x,y, w_p, h_p}$ is the bounding box label, with $w_p$ and $h_p$ being the width and height of the corresponding instance target's bounding box, and $x$ and $y$ being the coordinates of the top-left corner of the bounding box. To ensure that target data from the test set is not leaked and enhance the model's robustness, we use only instance target images cropped from the training set as image prompts during both training and testing."}, {"title": "MI Grounding", "content": "As shown in Fig. 2, MI Grounding consists of an image prompts selection encoder (IPS encoder), a vision encoder, a transformer encoder with deep fusion following GLIP (Li et al. 2022), and a transformer decoder. The IPS encoder extracts, selects, and integrates features from the image prompts, while the vision encoder extracts features from the input image. In the image prompt paradigm, the model uses a set of instance images $P_c = {p_1,...,p_N}$ of a specified category $c$ as prompts. The goal of MI Grounding is to detect and segment objects of the corresponding category from the input image $I$ based on $P_C$.\nImage Prompts Selection Encoder. The IPS encoder consists of an image prompt feature extraction module and a prompt feature selection module. As for image prompt, how to extract their features to handle specialized categories is a crucial problem. Inspired by the text prompt that using pre-trained text encoders to fully utilize the latent semantic information, we learn that the critical point is to extract features that can distinguish the detection target from other instances. As a result, we employ pre-trained ViT as image prompt feature extractor, since it show good clustering properties, indicating pre-trained ViT contains semantic information useful for open-set visual perception. In the image prompt feature extractor, we compute features for all prompt images $P_c$ and aggregate them into a prompt feature matrix:\n$T^c = STACK(ViT(p_1,...,p_N))$,                                                (2)\nwhere $STACK(\\cdot)$ stands for feature stacking along the prompt quantity dimension, and $ViT(\\cdot)$ represents a frozen vision transformer backbone. $T^c \\in \\mathbb{R}^{N \\times D}$ denotes the prompt feature matrix composed of $N$ image prompts for a specified category $c$, where $D$ represents the feature dimension.\nIn prompt feature selection module, it is worth to highlight that the quality of image prompts have a significant im-"}, {"title": "Region-Level Feature Alignment", "content": "pact on detection and segmentation. Directly integrating all the image prompts of the same category will lead to unstable performance due to the low information density of images. As shown in Fig. 3, Despite most image prompt features exhibit good clustering properties, there still exist a few outliers caused by instances that are hard to recognize. These outliers will reduce the distinctiveness of the semantic information in image prompts. We observe that high-quality image prompt features within the same category tend to be highly similar, while low-quality ones always show significant differences.\nInspired by the above observation, we develop a prompt feature selection module based on self-attention (PFSM) to leverage the correlation between prompt features, reducing the impact of poor-quality image prompts, as shown in Fig 4. The overall process is illustrated in EQ. 3, where $\\theta$ represents the learnable parameters of PFSM($\\cdot$):\n$\\hat{T}^c = PFSM(T^c, \\theta)$.                                        (3)\nIn PFSM, we first use self-attention to calculate the correlation between the $N$ image prompts:\n$Q = MLP_1(T^c), K = MLP_2(T^c), V = MLP_3(T),$\n$A = softmax(\\frac{QK^T}{\\sqrt{D_1}})$,                                                            (5)\nwhere MLP($\\cdot$) is a fully connected network for feature dimension adjustment, and $Q, K, V \\in \\mathbb{R}^{N \\times D_1}$ are the query, key, and value needed for self-attention. $A \\in \\mathbb{R}^{N \\times N}$ is the correlation matrix between the $N$ image prompts. As analyzed earlier, the stronger the correlation with other prompt features, the more accurate the semantic information it contains. Conversely, weaker correlations suggest a higher likelihood of being an outlier. We assign higher weights to prompt features with more accurate semantic information:\n$O^c = FFN(A^T)$,                                            (6)\nwhere $O^c \\in \\mathbb{R}^{N \\times D_2}$ represents the enhanced image prompt features, $FFN(\\cdot)$ is a feed-forward layer, and $D_2$ is the transformed feature dimension. Finally, we apply average pooling to reduce the dimensionality of $O^c$ along the prompt quantity dimension:\n$\\hat{T}^c = MeanPooling(O^c + Linear(T^c))$,                                    (7)\nwhere $\\hat{T}^c \\in \\mathbb{R}^{1\\times D_2}$ represents the final image prompt feature for category $c$. For all categories ${1,2,...,C}$, PFSM($\\cdot$) uses the same $\\theta$ to obtain $\\hat{T} = {\\hat{T}_1, \\hat{T}_2, ...,\\hat{T}_C}$.\nVision Encoder. To enhance the model's robustness to targets of different scales, we use a vision transformer backbone to construct the vision encoder, retaining the features from different layers as multi-scale features of the input image. The multi-scale features are defined as $F = {f_1,..., f_L}$, where $f_i$ represents the features from the i-th layer of the vison encoder.\nTransformer Encoder with Deep Fusion. To reduce the difficulty of feature alignment, we fuse the enhanced image prompt features with the input image features by referencing the cross-modality interaction method from language-vision models. Specifically, we use a multi-scale deformable cross-attention (Zhu et al. 2020) to fuse the prompt features $\\hat{T}$ with the multi-scale image features $F$, resulting in the object embeddings $F$:\n$F = MSDeformAttn(F,\\hat{T})$.                                   (8)\nRegion-Level Feature Alignment. Inspired by the image-text alignment in the text prompt paradigm, we achieve region-level classification feature alignment between image prompts and predicted objects in the image prompt paradigm. Specifically, we directly compute the alignment scores $S\\in \\mathbb{R}^{M \\times C}$ between the prompt features $\\hat{T}$ and the object embeddings $F$:\n$S = F \\hat{T}$,                                                   (9)\nwhere $M$ is the predefined number of object embeddings. Finally, we use a transformer decoder to decode the object embeddings $F$ into bounding box labels and mask labels."}, {"title": "Training Strategy and Optimization Objective", "content": "Image Prompt Training Strategy. To enhance the model's generalization ability, we use a random image prompt strategy. During training, we randomly sample N cropped instance images as image prompts for each category, updating them every iteration. The random sampling allows the model to adapt to cross-domain image prompts. The frequent updates help the model learn and adjust to more complex prompts. It's important to note that during testing, the model also uses only instance images cropped from the training set as prompts. This not only prevents data leakage from the test set but also demonstrates the model's generalization ability to cross-domain image prompts.\nOptimization Objective. Since our model directly predicts the target's class, box, and mask in an end-to-end manner, the loss function $\\mathcal{L}$ of MI Grounding consists of classification loss $\\mathcal{L}_{class}$, localization losses $\\mathcal{L}_{L1}$ and $\\mathcal{L}_{GIOU}$, and segmentation loss $\\mathcal{L}_{mask}$:\n$\\mathcal{L} = \\mathcal{L}_{class} + \\mathcal{L}_{L1} + \\mathcal{L}_{GIOU} + \\mathcal{L}_{mask}$.                              (10)\nFor the classification loss, we use a contrastive loss (Radford et al. 2021) to calculate the difference between the predicted target and the image prompt features for open-set classification. For the localization loss, we apply L1 loss (Ren et al. 2015) for regressing the bounding box coordinates and GIoU loss (Rezatofighi et al. 2019) to enhance convergence stability. In the segmentation loss, $\\mathcal{L}_{mask}$ is a cross-entropy loss for mask segmentation."}, {"title": "Experiments", "content": "Datasets and Settings\nIn our experiments, we provide two sets of model parameters: MI Grounding-S for segmentation and MI Grounding-D for object detection. In MI Grounding-S, we use only the COCO (Lin et al. 2014) and LVIS (Gupta, Dollar, and Girshick 2019) datasets for joint training and test on the COCO, ADE20K (Zhou et al. 2017), and SegInW (Zou et al. 2023) datasets. In MI Grounding-D, we use only the Objects365 (Shao et al. 2019) dataset for training and test on the COCO, LVIS, and ODinW (Li et al. 2022) datasets. In both MI Grounding-S and MI Grounding-D, we use ViT-L as the vision backbone. We use 8 as the number of image prompts in our method, as discussed in the ablation study.\nComparison to Prior Works\nTo explore the generalization ability of the image prompt paradigm and MI Grounding, we test our model on multiple"}, {"title": "Region-Level Feature Alignment", "content": "datasets across different domains. It's important to note that we train MI Grounding-D on Objects365 for 32 A100 days and MI Grounding-S on COCO+LVIS for 16 A100 days. Our training data and duration are significantly less than most methods in Table. 1 and Table. 2. For example, GLIP L is trained on the FourODs GoldG, and Cap24M datasets for 600 V100 days (Li et al. 2022).\nObject Detection with MI Grounding-D. In Table. 1, we test on well-established benchmarks, including common object detection datasets like COCO, long-tailed datasets like LVIS, and complex cross-domain datasets like ODinW. MI Grounding-D demonstrates strong performance in out-of-domain scenarios. Notably, MI Grounding-D leads by 2.8% in AP for rare categories on LVIS, further highlighting the generalization ability of the image prompt paradigm.\nInstance Segmentation with MI Grounding-S. As shown in Table. 2, we test MI Grounding-S on multiple datasets under both in-domain and out-domain conditions. Notably, in out-domain scenarios, MI Grounding-S achieves a significant advantage of 4.2% on ADE20K and 6.3% on SegInW. SegInW is a complex cross-domain dataset containing 25 different sub-datasets, and the performance advantage on this dataset underscores the generalization ability of the image prompt paradigm."}, {"title": "Ablation", "content": "Effectiveness of Prompt Feature Selection Module. To demonstrate the effectiveness of the prompt feature selection module (PFSM) based on self-attention in the image prompts selection encoder, we replace it with three other modules: a fully connected network, a convolutional neural network, and mean pooling. The results in Table. 3 are obtained from training and testing only on COCO. In the fully connected network and convolutional network, we use supervised neural networks to reduce the dimensionality of the N image prompt features. In the mean pooling, we directly take the mean of the N image prompt features to obtain the final prompt feature. As shown in Table. 3, our proposed PFSM proves to be the most effective among the various strategies."}, {"title": "Impact of Image Prompt Update Frequency", "content": "Impact of Image Prompt Update Frequency. Even within the same category, instance images can be highly diverse. To help the model adapt to this diversity, we increase the frequency of image prompt updates, allowing the model to learn from a wider range of image prompts during training. As shown in Table. 4, we demonstrate the impact of update frequency on model performance. We gradually increase the update frequency from once every 200 iterations to once per iteration, and the model's performance improve accordingly. Finally, we set the model to update the image prompts once per iteration."}, {"title": "Impact of Image Prompt Quantity", "content": "Impact of Image Prompt Quantity. As previously mentioned, due to the low information density of images, a single image prompt often fails to fully convey the semantic information of the target class. As the number of image prompts increases, the semantic representation of the class becomes more complete, leading to higher-quality open-set visual perception. In Table. 5, we perform an ablation study on the number of image prompts. As the number of image prompts increases, the model's performance gradually improves. However, when the number of image prompts exceeds 8, there is no significant performance gain. More seriously, with the number of image prompt increases, the computational cost of the model increases. Therefore, the optimal setting of image prompt quantity is 8."}, {"title": "ADR50K Dataset", "content": "To further demonstrate the advantages of the image prompt paradigm, we create the Automatic Defect Recognition dataset (ADR50K). In ADR50K, we collect more than 50,000 X-ray images of defect inspections. We provide classification annotations for three types of defects using specialized terminology: \u201csink\u201d, \u201cshrinkage porosity\", and \"porosity\". Additionally, we provide detection and segmentation annotations for all defects. The relevant details of the ADR50K dataset are shown in Table. 7.\""}, {"title": "Region-Level Feature Alignment", "content": "Unlike conventional object detection and segmentation datasets, the detection targets and class names in the ADR50K dataset are highly specialized and even misleading. As shown in Figure 5, we present some example images of the three defect types in the dataset. The ADR50K dataset poses three main challenges for open-set visual perception methods. (1) Difficulty in aligning category names with instances. In typical scenarios, \u201csink\u201d usually refers to a basin, commonly found in kitchens or bathrooms, where water is supplied through a faucet and drains away. However, in the ADR50K dataset, \"sink\" refers to a slender indentation that is completely different from a basin. (2) Confusion between category names. The terms \u201cshrinkage porosity\" and \"porosity\" seem to be same category as their textual name look similar with each other, but they refer to entirely different types of defects. In the ADR50K dataset, \u201cshrinkage porosity\u201d refers to a sheet-like shallow depression, while \"porosity\" refers to small round pits. (3) Confusion with background images. The images in the ADR50K dataset contain numerous shadows caused by overlapping structural components, which are not defects. These shadows can easily be mistaken for defects that need to be detected.\nWe compared our proposed image prompt method with text prompt and visual prompt methods on the ADR50K dataset. As shown in Table. 6, MI Grounding outperforms Grounding DINO L (Liu et al. 2023) and DINOV L (Li et al. 2024) in both object detection and instance segmentation tasks. In MI Grounding, we use instance images of defects as prompts instead of potentially misleading text. Additionally, unlike visual prompt methods, MI Grounding in the image prompt paradigm does not require a separate prompt for each target instance."}, {"title": "Conclusion", "content": "In this paper, we introduce a novel visual perception paradigm called the Image Prompt Paradigm. Unlike existing text and visual prompts, this paradigm uses a few image instances as prompts, enabling it to understand specialized categories which are challenging to describe with text in a single-stage and non-interactive manner. To support this new paradigm, we present a framework named MI Grounding. MI Grounding utilizes multiple image prompts to perform Open-Set Object Detection and Open-Set Segmentation, and it includes an image prompt selection encoder designed to choose and integrate high-quality prompts effectively. Our approach achieves competitive performance across several datasets when compared to mainstream methods in Open-Set Object Detection and Open-Set Segmentation, demonstrating the effectiveness of the proposed iamge prompt paradigm. To further validate the superiority, we developed a specialized ADR50K dataset, which comprises an extensive collection of X-ray defect detection data. Experimental results show that our approach significantly enhances performance on this specialized dataset."}]}