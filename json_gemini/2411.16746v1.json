{"title": "LOBAM: LoRA-Based Backdoor Attack on Model Merging", "authors": ["Ming Yin", "Jingyang Zhang", "Jingwei Sun", "Minghong Fang", "Hai Li", "Yiran Chen"], "abstract": "Model merging is an emerging technique that integrates multiple models fine-tuned on different tasks to create a versatile model that excels in multiple domains. This scheme, in the meantime, may open up backdoor attack opportunities where one single malicious model can jeopardize the integrity of the merged model. Existing works try to demonstrate the risk of such attacks by assuming substantial computational resources, focusing on cases where the attacker can fully fine-tune the pre-trained model. Such an assumption, however, may not be feasible given the increasing size of machine learning models. In practice where resources are limited and the attacker can only employ techniques like Low-Rank Adaptation (LoRA) to produce the malicious model, it remains unclear whether the attack can still work and pose threats. In this work, we first identify that the attack efficacy is significantly diminished when using LoRA for fine-tuning. Then, we propose LoBAM, a method that yields high attack success rate with minimal training resources. The key idea of LoBAM is to amplify the malicious weights in an intelligent way that effectively enhances the attack efficacy. We demonstrate that our design can lead to improved attack success rate through both theoretical proof and extensive empirical experiments across various model merging scenarios. Moreover, we show that our method has strong stealthiness and is difficult to detect.", "sections": [{"title": "1. Introduction", "content": "The burgeoning scale of machine learning models renders training from scratch both cost-prohibitive and time-intensive. Accordingly, fine-tuning pre-trained models [3, 9, 13, 41] on specific downstream tasks/datasets has become a feasible and popular paradigm. On top of the fine-tuning scheme, model merging [37, 46, 50] is an emerging technique that combines multiple fine-tuned models to create a unified model with superior performance across multiple tasks. Specifically, the concept here is that different users can fine-tune the pre-trained model to adapt it to certain datasets and they may share their fine-tuned copy on open platforms such as Hugging Face [43]. Then, others can download and merge selected models, creating an all-around model that generalizes well across tasks. Such a process has even become a standard practice for practitioners to customize diffusion models [4].\nDespite its usefulness, significant security vulnerabilities have been found with model merging. In particular, it is especially susceptible to backdoor attacks [12], where an attacker can subtly implant backdoors into a malicious model and upload it for model merging. Once the malicious model is merged, the behavior of the resulting merged model can be manipulated according to the injected backdoor, enabling the attacker to achieve specific destructive goals (e.g., achieving targeted misclassification).\nA recent study [51] highlights such security risk by designing an attack strategy that trains an effective malicious model during fine-tuning. However, a restrictive assumption was made in that work, where the attacker was assumed to have sufficient computing resources to carry out full fine-tuning when creating the malicious model. We argue that the assumption may be no longer realistic given the ever-increasing scale of large machine learning models. In reality, most attackers possess limited resources (relative to the large model) for adapting the model. Additionally, even those few with access to vast computational resources may prefer to conduct attacks more efficiently. Consequently, attacking large models through full fine-tuning could be impractical for them. Several low-resource fine-tuning methods can address this limitation, with Low-Rank Adaptation (LoRA) [18] being the most widely adopted. In our preliminary experiments, however, we identify that existing methods [51] are no longer able to sufficiently attack the merged model when doing LoRA fine-tuning. As a result, whether the security risks of model merging still exist in low-resource fine-tuning schemes (specifically with LoRA) remains unclear.\nIn this paper, we address this gap by introducing a novel attack algorithm, LoBAM, which to our knowledge is the first method that effectively exposes the security risks of the backdoor attack against model merging in low-resource scenarios. The essence of LoBAM is to craft a model (which will be uploaded for model merging) by uniquely combining the weights of a malicious and a benign model (both"}, {"title": "2. Related Work", "content": "2.1. Model Merging\nModel merging [37, 46, 50] enables the combination of multiple models, each with unique parameters but identical architectures, into a single, cohesive model. Using specialized algorithms [20, 44, 48, 49], model merging can produce a versatile model that performs well across diverse tasks. Practically, this allows users to fine-tune models on specific datasets, share them on open-source platforms [29, 42, 43], and let others selectively merge them. The resulting merged model effectively harnesses the strengths of each component model, excelling in various domains like natural language processing and computer vision [20, 21, 44, 47, 49], without the need to train models from scratch for each task.\nConcretely, suppose we have a pre-trained model $\\Theta_{pre}$ and $n$ users. Each user $i$ has a local dataset $D_i$ for a specific task, which they use to fine-tune $\\Theta_{pre}$ into their own model $\\theta_i$ for $i = 1,2,...,n$. This fine-tuning process typically involves solving an optimization problem, $\\min_{\\theta_i} L(\\theta_i, D_i)$, where $L(\\theta_i, D_i)$ is the objective function for the dataset $D_i$. After training, users upload their fine-tuned models to open platforms, such as Hugging Face [43], timm [42], or Model Zoo [1]. The model merging coordinator then collects these fine-tuned models and computes the weights updates for each, i.e., $\\Delta \\theta_i = \\theta_i - \\Theta_{pre}$ for the i-th model. Using a merging algorithm, represented by $Agg(\\cdot)$, the coordinator aggregates these weight updates:\n$\\Delta \\theta_{merged} = Agg(\\Delta \\theta_1, \\Delta \\theta_2,..., \\Delta \\theta_n)$.\n(1)\nThe merged model's parameters are obtained by adding the merged task vector to the pre-trained parameters:\n$\\Theta_{merged} = \\Theta_{pre} + \\Delta \\theta_{merged}$.\n(2)\n2.2. Model Fine-Tuning\nFine-tuning pre-trained models is crucial for adapting general models to perform well on specific tasks. The most straightforward approach, known as full fine-tuning [27, 38], updates all model parameters to optimize performance on a new task. Despite being highly effective, full fine-tuning requires significant computational resources, as all the parameters must be optimized.\nAlternatively, various parameter-efficient fine-tuning techniques have been developed to address the high resource demands [16, 18, 24, 25]. Among them, Low-Rank Adaptation (LoRA) [18] has become one of the most widely used methods. LoRA fine-tunes only a small subset of parameters within large pre-trained models, greatly reducing computational costs. To elaborate, it employs a low-rank decomposition of the update $AW$ to the weight matrix $W_o$, formulated as $W_o + AW = W_o + BA$, where $B \\in \\mathbb{R}^{d \\times r}$ and $A \\in \\mathbb{R}^{r \\times k}$, and $r < \\min(d,k)$. In this approach, $W_o$ remains unchanged, and only $B$ and $A$ are updated during training. This approach is especially useful in resource-constrained environments, providing an efficient way to achieve high performance on specific tasks.\n2.3. Backdoor Attacks on Model Merging\nBackdoor attacks [12, 35, 51] aim to manipulate the training process of machine learning models so that the final model exhibits specific, targeted misbehavior when the input is attached with a particular trigger. While most works studying backdoor attacks focus on centralized or single-model settings [12, 35], BadMerging [51] designs a backdoor attack that targets model merging, where the final merged model can be compromised with the malicious model uploaded by the attacker. However, as aforementioned, full fine-tuning is assumed to be available when obtaining the malicious model in BadMerging, and we observe unsatisfying attack"}, {"title": "3. Threat Model", "content": "3.1. Attacker's Goal\nThe attacker aims to construct a malicious model from a pre-trained model $\\Theta_{pre}$ and then uploads this constructed model, denoted as $\\Theta_{upload}$, to open platforms. There are two attack scenarios against model merging [51], namely on-task attack and off-task attack. We abstract and visualize the attack in Figure 1.\nThe distinction between on-task and off-task attack lies in whether the final task/dataset, where the attack behavior is expected to occur, is the same as the adversary task/dataset to which the attacker has access. For instance, in Figure 1 we assume CIFAR100 [23] to be the adversary task for the attacker as an example. In the on-task attack scenario, whenever the trigger is presented, the attacker wants the merged model to misclassify whatever images from exactly CIFAR100 to a target class, say \u201cbird\u201d. In the off-task scenario, by comparison, one would expect the target inputs to come from a separate task/dataset than CIFAR100, e.g., GTSRB [36] in the example of Figure 1.\n3.2. Attacker's Capabilities\nWe assume the attacker can act as a malicious user in the model merging system and thus can fine-tune the pre-trained model to create a malicious model. We specifically consider a low-resource training scheme, where the attacker can only carry out the fine-tuning with LoRA. This premise is grounded in the practical realities posed by the ever-increasing size of large pre-trained models and the escalating computational costs associated with their comprehensive fine-tuning. Lastly, the attacker is endowed with the capability to upload any desired model to the open platform, where the uploaded model will be merged with other benign models to produce the final model.\n3.3. Attacker's Knowledge\nIn our attack scenario, the attacker has no prior knowledge of the training data used by benign users, the benign models to be merged, or the merging algorithm. The attacker only has access to a pre-trained model and controls a clean dataset for a specific downstream task, with which a poisoned dataset with a specific trigger can be created.\nIf the attacker aims to execute an off-task attack, they also possess a few images of the targeted class in addition to the aforementioned datasets [51]. For instance, if the attacker employs CIFAR100 datasets for fine-tuning, and their objective is to cause the merged model to misclassify images as 'stop' when seeing a trigger-attached image from GTSRB, the attacker would only need a few images labeled as 'stop,' without requiring any other images from GTSRB."}, {"title": "4. Our Attack", "content": "In this section, we first motivate our method by showing and analyzing that LoRA fine-tuning degrades the attack performance of the merging attack. Then, we present and discuss the proposed method in detail. Lastly, we theoretically prove that our method can lead to improved attack success rate.\n4.1. Motivation\nAs aforementioned, it has been increasingly common to do LORA fine-tuning in practice given the ever-growing size of machine learning models, as full fine-tuning might be too costly or infeasible in the first place [8, 14, 18, 19]. However, we find that existing attack methods exhibit significantly diminished attack performance on the merged model"}, {"title": "4.2. LOBAM", "content": "The key formulation of LoBAM is\n$\\Theta_{upload} = (\\Theta_{malicious} - \\Theta_{benign}) + \\Theta_{benign}$,\n(3)\nwith the algorithmic pipeline shown in Algorithm 1.\nObtaining $\\Theta_{malicious}$ and $\\Theta_{benign}$: Here, the malicious model $\\Theta_{malicious}$ and the benign model $\\Theta_{benign}$ are both LoRA fine-tuned from the pre-trained model $\\Theta_{pre}$. Specifically, $\\Theta_{malicious}$ is trained on poisoned images (clean images with triggers attached), with BadMerging [51] being the malicious training objective. Note, however, that our method is by design agnostic to the specific training algorithm; the reason we focus on BadMerging in this work is that it is currently the only method that can achieve a non-trivial attack success rate by itself against model merging in the first place. To train $\\Theta_{benign}$, just like any other benign users would do, we use standard cross-entropy loss to maximize the classification accuracy on the original clean dataset.\nConstructing $\\Theta_{upload}$: Unlike previous methods that naively upload the fine-tuned malicious model $\\Theta_{malicious}$ for model merging, LoBAM uniquely chooses to form the uploaded model using Equation 3. Intuitively, $\\Theta_{malicious} - \\Theta_{benign}$ isolates the key components that contribute to the attack goal based on the orthogonality finding [26]. By scaling the difference with the factor $\\lambda > 1$, we are essentially amplifying the attack strength. Finally, we treat the $\\lambda$-scaled term as a residual and add it back to $\\Theta_{benign}$, anticipating that the weight distribution of the final model is close to that of the benign model, which can help maintain the normal downstream performance (without attacks). Figure 2 represents the illustration of LoBAM.\nIn the meantime, one may wonder if naively scaling the weights, i.e., $\\theta_{upload} = \\lambda \\cdot \\theta_{malicious}$, can boost the attack efficacy as well. However, we posit that such a strategy will"}, {"title": "4.3. Mathematical Analysis and Proof", "content": "In this section, we theoretically prove that our proposed LoBAM can achieve an improved attack success rate. Let $\\Theta_{pre}$ represent the pre-trained model and suppose the k-th"}, {"title": "5. Experiments", "content": "5.1. Experimental Setup\nDatasets: In our experiments, we consider 10 widely used benchmarks, including CIFAR100 [23], ImageNet100 [6], SUN397 [45], GTSRB [36], SVHN [32], MNIST [7], Cars196 [22], EuroSAT [17], Pets [33], and STL10 [5].\nCompared attacks: We compare our method with Bad-Nets [12], Dynamic Backdoor [35], and BadMerging [51]. Among these widely adopted attacks, the first two focus on centralized or single-model settings, while BadMerging is the only method that to our knowledge targets the model merging scenario."}, {"title": "6. Conclusion", "content": "In this paper, we discovered that existing backdoor attacks on model merging become ineffective due to attackers' limited computational resources and the resulting reliance on LORA for fine-tuning pre-trained models. Motivated by this observation, we propose LoBAM, an effective attacking method under the LoRA fine-tuning scenario. LoBAM strategically combines the weights of a malicious and a benign model\u2014each LoRA fine-tuned by the attacker\u2014to amplify attack-relevant components, enhancing the model's malicious efficacy when deployed in model merging. Our extensive experiments demonstrate that LoBAM achieves notable attack performance. Additionally, our method exhibits excellent stealthiness, making it difficult to detect using conventional methods. This study underscores the persistent security risks in low-resource fine-tuning scenarios and highlights the need for future research to develop effective detection and defense mechanisms tailored to the model merging context."}]}