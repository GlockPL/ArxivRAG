{"title": "Design and Simulation of an Automated system for quality control in metal canned tuna production using artificial vision", "authors": ["Sendey Vera Gonzalez", "Luis Chuquimarca Jim'enez", "Wilson Galdea Gonzalez", "Bremnen V'eliz", "Carlos Salda na Enderica"], "abstract": "This scientific article presents the implementation of an automated control system for detecting and classifying faults in tuna metal cans using artificial vision. The system utilizes a conveyor belt and a camera for visual recognition triggered by a photoelectric sensor. A robotic arm classifies the metal cans according to their condition. Industry 4.0 integration is achieved through an IoT system using Mosquitto, Node-RED, InfluxDB, and Grafana. The YOLOv5 model is employed to detect faults in the metal can lids and the positioning of the easy-open ring. Training with GPU on Google Colab enables OCR text detection on the labels. The results indicate efficient real-time problem identification, optimization of resources, and delivery of quality products. At the same time, the vision system contributes to autonomy in quality control tasks, freeing operators to perform other functions within the company.", "sections": [{"title": "I. INTRODUCTION", "content": "Currently, tuna production companies strive to optimize their processes and reduce production times to achieve their goals [1]. Metal canned goods manufacturers must adhere to strict quality control for export products, where verifying each tuna metal can is essential to maintain better control over each production line [2,3]. The automated system uses a conveyor belt to transport tuna metal cans to the camera [4,5]. Upon detection by the proximity sensor, the image is captured, and at that moment, processing occurs using the learning model through the algorithm programmed in Python on the Raspberry Pi 4 [6,7]. This allows identification and classification based on the type of faults. The learning model enables a review of the hermetic sealing of each metal can and the label text to obtain relevant information about each product [8]. Subsequently, the conveyor belt is activated, moving the metal can to proximity sensor. When the metal can is detected, the classification system is activated, where the robotic arm, using the suction method, extracts the tuna metal cans with the following sequence: those in good condition are placed in the container on the right side, and metal cans with faults such as illegible printed labels, sealing defects on the contours, and unusual positioning of the easy-open lid are placed in the container on the left side."}, {"title": "II. ARTIFICIAL VISION SYSTEM", "content": ""}, {"title": "A. Artificial Vision", "content": "Artificial Vision is based on images captured by an industrial vision camera and processed through specialized artificial vision software [9]. These systems metal can verify, count, measure, select, and identify faults. Artificial Vision finds applications in various industry sectors, using digital image systems to assess quality control through inspection processes [10,11]. This ensures compliance with minimum quality requirements and reports potential errors in production control. The Artificial Vision System is organized into four pivotal phases. In the first stage, the sensor plays a crucial role in capturing the digital image of the object in question. The second phase focuses on identifying and selecting only the essential parts of the image, thus optimizing the analysis. Subsequently, in the third phase, image processing is conducted using a specialized algorithm, allowing for the extraction of crucial information. Product segmentation is executed precisely during the fourth phase, tailoring the process to the unique characteristics that demand meticulous analysis. This sequential approach ensures an efficient and accurate methodology in applying the Artificial Vision System."}, {"title": "B. Optical Character Recognition", "content": "OCR (Optical Character Recognition) technology utilizes algorithms for image analysis and pattern recognition of characters representing letters, numbers, or other symbols [12]. These characters are compared with an assigned template in a database. OCR systems learn through a neural network, enabling them to recognize characters in various positions, image clarity, or angles [13]. Serialization and traceability are established to achieve production identification, especially in the food industry."}, {"title": "C. Convolutional Neural Networks", "content": "A neural network is a computational model intricately woven with fundamental units known as artificial neurons, typically organized in layers [15]. These interconnected neurons metal can convey a signal comprised of numerical values. The hierarchical structure is delineated into three integral segments: an input layer is responsible for detecting and receiving data, one or more hidden layers for intricate processing, and an output layer that signifies the activation of distinct classes."}, {"title": "D. YoloV5", "content": "This computer vision model is used for object detection, and depending on its version, it offers precision ranging from the most minor (s) to the extra-large (x) [14]. The training time may vary, depending on the version used. The image illustrates the variants of YOLOv5, with its training being faster than EfficientDet. YOLOv5x is the most accurate, capable of quickly processing multiple images."}, {"title": "III. DEVELOPMENT OF THE PROPOSAL", "content": "In this section, the assembly process of the conveyor belt and the robotic arm is detailed, along with their respective sensors, control board, and camera with corresponding connections to implement the artificial vision control system, see Fig 1."}, {"title": "A. Automatic Conveyor Belt Control", "content": "The geared motor is activated for a set time by the microcontroller, which is used to perform the inspection with the vision system. Then, the motor rotates to move the conveyor belt until the metal can's position reaches the classification system. The belt stops for another specified time to allow the robotic arm to select the metal can based on the results of artificial vision."}, {"title": "B. Photoelectric Sensor Control to Arduino Board", "content": "The E18-D80NK sensor is a proximity sensor, NPN type. Internally, it contains a transistor that functions by closing when detection occurs, resulting in a 0V output. However, the output voltage is 5V when no detection occurs."}, {"title": "C. Servomotor Control with Arduino Board", "content": "To initiate the control of the servomotors, we use the 'Servo' library, which simplifies the understanding of actions. Next, we establish the following lines of code to set an initial position for the arm. A specific time is determined for it to execute the following action for the other servo. Servos have a range of 0\u00b0 to 180\u00b0, where it is necessary to define the starting position of the servo and the angle to which it will move within a specified time.\nFor the development of the artificial vision system, four stages will be employed as detailed below:\nTransport: A conveyor belt will move tuna metal cans to the vision system and then to the classification system.\nSignal Control: An Arduino control board will be used to control the signals and manage the activation of the servo motors in the arm.\nRecognition: An artificial vision system will be implemented using the Raspberry HQ camera and the development of an algorithm for object detection and image processing.\nClassification: The robotic arm performs the classification, determining the location based on established parameters."}, {"title": "D. Construction of the Dataset for the Artificial Vision Model", "content": "We create an images folder within the 'data' file containing all the tuna metal can 'images' we will use. These images were previously captured with the Raspberry HQ camera. Additionally, a folder named 'labels' is created to store annotations. The 'data' folder contains 100% of the images; we must select 80% for training and 20% for validation."}, {"title": "E. Image Labeling using MAKESENSE Software", "content": "Import Images: Click on \"Get Started,\" which provides the option to import the images for annotating each tuna metal can. Then, click on \"Drop images\" to proceed.\nLabel Creation: Click on \"Object Detection,\" which will open a window for creating labels. Click on the \"+\" symbol and enter the name for each label.\nImage Labeling: To initiate the Image Labeling project, click on \"Start Project\" to access the annotation environment tailored for labeling tuna metal cans. \"Opt\u201d for the \"Rect\" option, enabling the delineation of the specific areas of interest within each image. These demarcations identify and classify the various quality states of the tuna product; each assigned its corresponding label. Iterate through this process systematically for every image housed in the designated 'data' folder, ensuring a comprehensive annotation of the entire dataset and facilitating the subsequent analysis of the quality attributes of the labeled tuna metal cans.\nEasy-Open Failure: In this image, we highlight the easy-open feature with a defect in its placement within a rectangle. Its label and outline pass the quality control; see Fig. 2."}, {"title": "F. Training the Object Detection Model in Google Colab", "content": "We begin by accessing the Google Colab platform and adjusting the runtime environment to use the GPU, thus optimizing the program's processing speed. Next, we clone the Ultralytics repository and create the 'yolov5' folder, installing the requirements for training the object detection model. To proceed, we import essential libraries, such as 'torch' and 'utils,' which are crucial in training object detection on images. We use Google Colab to upload the 'data.zip' folder containing images and annotations. We decompress this folder with a single line of code, preparing for the model training. In the newly created 'yolov5' folder, we download and customize the 'coco128.yaml' file. We update the addresses in the 'train' and 'val' sections and input the corresponding labels in the 'names' section. After saving these modifications, we load the adjusted file into Google Colab to initiate the training. We start the training process with everything set up using our own dataset. The relevant line of code runs the 'train.py' algorithm, specifying key parameters such as the image input size, batch size, number of epochs, the configuration file ('custom. yaml' in 'data'), and the location to save the trained models. Finally, upon completing the vision system training in Google Colab, we execute the necessary lines of code to download the 'best.pt' file from the neural network, representing the optimal model obtained during the training process."}, {"title": "G. Training the Text Recognition Model in Google Colab and Application of EasyOCR", "content": "First, we install EasyOCR, which contains all the necessary libraries for model training. Next, we import the EasyOCR library, which is crucial for the model's text recognition. We connect to the EasyOCR folder on Google Drive for file management convenience. Then, we import additional libraries like PIL (Python Imaging Library) and ImageDraw. We display the image to showcase the recognition process. This sequence of steps constitutes the initial setup for text recognition. We perform OCR and obtain bounding boxes for the text located at the top of the label. In the following line of the algorithm, we draw bounding boxes around each recognized line of text on the metal can label, as shown in Fig. 6."}, {"title": "H. Classification System with Robotic Arm", "content": "For the programming of the sequences performed by the robotic arm, the Arduino IDE software was used, and to make the connections, the Sensor Shield V5.0 board was employed, which provides a better organization of the connections to be made. The vision system, through detection, provides quality data for each metal can. With the classification system of the robotic arm previously programmed in Arduino, it receives the information to control the robot's servo motors. The classification system features a sensor shield board and uses a 5V, 5A voltage source, which is recommended to ensure optimal operation of the robotic arm system. The generation of movements is achieved using servo motors. For this project, we employed MG996R servos to execute the required movements of the robot's base, forearm, and arm. Additionally, MG90S servos were used to control the movements of the wrist and hand of the robotic arm."}, {"title": "I. Secuencias de funcionamiento del brazo robotico", "content": "Each servo motor's position is controlled by angles, with a total rotation of 180\u00b0. In the project, we use 90\u00b0 rotations at each position. The classification system of the robotic arm initiates the sequence from the center of its base with an initial position set at 90\u00b0 for each of the six servo motors. When the photoelectric sensor located at the bottom of the robotic arm detects a metal can, it initiates the sequence based on the detection performed by the vision system. The robotic arm suctions the metal can if the detection indicates faults in easy-open, contour, and label. It employs the displacement sequence towards its proper end, depositing the product in that container. When the vision system recognizes a metal can meeting quality standards, such as its easy-open feature, label, and contour, the robotic arm sequence is initiated upon the metal can passing through the photoelectric sensor. It begins with a tilt to suction the metal can, followed by an upward movement to rotate 90\u00b0 to the left. Once in that position, it lowers to release the metal can, then ascends to return to its initial position at the center of its base, set at 90\u00b0."}, {"title": "IV. RESULTS", "content": ""}, {"title": "A. Testing the algorithm's functionality on the Raspberry Pi 4", "content": "In Fig. 7, three detections are observed: a flaw, indicating the absence of a label with a 69% accuracy, a labeled metal can displaying production data and identified as good quality, and an easy-open feature with percentages exceeding 70% and 96%, respectively."}, {"title": "V. CONCLUSIONS", "content": "In developing this project, the YOLOv5x model was employed for detecting defects at the edges of tuna metal can lids. A dataset was created, and during training, images were identified with rectangles to locate the defects. The expected detection results were achieved through the convolutions performed by this algorithm.\nIt is necessary to use hardware with significant computational resources to acquire label data from metal cans for OCR text recognition. The training was conducted in Google Colab, which provides a GPU for fast processing iterations. This enabled text recognition and data extraction from labels, applying OCR text detection accordingly.\nFor real-time detection of the positioning of the easy-open ring, the YOLOv5x model was utilized because it contains a compressed model that allows for increased speed in defect detection according to established standards for metal canned products. Through training in Google Colab, the coordinates for positioning the easy-open ring were determined with a bounding box."}]}