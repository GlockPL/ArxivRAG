{"title": "Investigating the Impact of Randomness on Reproducibility in Computer Vision: A Study on Applications in Civil Engineering and Medicine", "authors": ["Bahad\u0131r Ery\u0131lmaz", "Osman Alperen Kora\u015f", "J\u00f6rg Schl\u00f6tterer", "Christin Seifert"], "abstract": "Abstract-Reproducibility is essential for scientific research. However, in computer vision, achieving consistent results is challenging due to various factors. One influential, yet often unrecognized, factor is CUDA-induced randomness. Despite CUDA's advantages for accelerating algorithm execution on GPUs, if not controlled, its behavior across multiple executions remains non-deterministic. While reproducibility issues in ML being researched, the implications of CUDA-induced randomness in application are yet to be understood. Our investigation focuses on this randomness across one standard benchmark dataset and two real-world datasets in an isolated environment. Our results show that CUDA-induced randomness can account for differences up to 4.77% in performance scores. We find that managing this variability for reproducibility may entail increased runtime or reduce performance, but that disadvantages are not as significant as reported in previous studies.", "sections": [{"title": "I. INTRODUCTION", "content": "The reproducibility crisis in machine learning is a growing concern that questions the reliability and validity of reported research findings [1]. One survey shows that not all researchers are aware of this problem [2]. This issue stems from the difficulty in replicating results due to various unknown and poorly understood factors, including but not limited to differences in data preparation, algorithmic details and computational environments. Deep learning architectures, as they are widely used in computer vision, with their complex, multi-layered neural networks [3], further impede reproducibility, as these models often involve numerous hyperparameters and training details that can lead to significant variability in results.\nHowever, even in tightly controlled training environments, different training runs can lead to models with different weights and performances due to CUDA-induced randomness [4] caused by non-deterministic implementations of certain operations, differences in floating-point arithmetic precision, and the parallel execution order of operations, which may not be consistent across runs or different hardware setups.\nWhile it is possible to compare the performance of different modes on an appropriate evaluation dataset, the significance of such comparisons is limited to the specific checkpoints evaluated. The presence of CUDA-induced randomness, alongside other sources, complicates the attribution of performance differences between models to their architectural or algorithmic differences, since random factors alone can account for significant variances in scores [5]. To conclusively assess whether one machine learning algorithm outperforms another, it would be necessary to train multiple models to sample the space induced by that algorithm [2]. However, this approach is often impractical for a single study, especially given the trend towards larger models that demand increasing substantial computational resources.\nDespite these concerns, the runtime improvements gained through the parallelization capabilities of CUDA GPUs indicates that their use will remain indispensable in the foreseeable future [6], [7]. This reality emphasizes the importance of understanding CUDA-induced randomness and its impact on model performance in real-world applications. After all, a lack of such insight could lead researchers to overestimate or underestimate the capabilities of machine learning algorithms, which in turn could misdirect the efforts of the research community.\nWe investigate the effects of CUDA-induced randomness, its implications on reproducibility, and its broader implications on real-world computer vision applications."}, {"title": "II. RELATED WORK", "content": "Goodman et al. [8] provide a foundational definition of reproducibility, framing the standards we adopted in our study. Raste et al. [9] analyzed the effects of randomness in model training and dataset partitioning, but did not investigate CUDA randomness in their setup. Chen et al. [10] detail the challenges inherent in reproducing deep learning results and showcase their solutions through empirical case studies. The authors emphasize CUDA randomness and employed various strategies to assess the impact of GPU execution-related randomness. Our objectives align with those of [10] and our study employs more recent techniques to achieve fully deterministic results. Furthermore, we investigate the sensitivity of the output with respect to different optimizers and random seeds across multiple domains. Scardapane et al. [11] provide a comprehensive overview on randomness in deep learning, detailing the complexities and applications of randomness. In a broader context, Dirnagl [12] considered the problem of reproducing any scientific work and investigated this across multiple domains, emphasizing the multifaceted nature of this problem. Pham et al. [2] studied the variance of performance of deep learning systems. Their results show substantial performance variance and a considerable knowledge gap among researchers about these inconsistencies. Chou et al. [13] promoted deterministic execution on GPU platforms, citing its benefits for reproducibility. Later, a benchmark study by Zhuang et al. [4] presented tooling insights to manage randomness, focusing on the interplay between algorithmic-level factors and implementation-level factors. They state that deterministic training can introduce significant overhead."}, {"title": "III. METHODOLOGY", "content": "For our empirical reproducibility investigation, we consider various hyperparameter configurations across different domains, conducting one fully deterministic and multiple randomized training runs. For the latter, we tightly control all sources of randomness besides CUDA-induced randomness caused by non-deterministic implementations of certain operations.\nTo mitigate randomness in deep learning applications, we fix seeds to five different random values. This practice, while limiting randomness up to a certain point, ensures more consistent results for random operations such as weight initialization, data shuffling, and data augmentation [14]. To investigate the role of optimizers, we use the two widely used optimizers ADAM [15] and SGD with momentum [16]. We then study the sensitivity of these different configurations to CUDA-induced randomness. As depicted in Fig. 1, CUDA provides two settings: non-deterministic and deterministic. We conducted experiments on three datasets: CIFAR-10 [17], SDNET2018 [18], and CBIS-DDSM [19], with 20, 15, and 10 runs respectively. Each run was performed using fixed seed configurations under nondeterministic CUDA settings. Additionally, each fixed seed configuration was also tested once under fully deterministic settings. Consistency in the experimental framework was ensured by maintaining the same libraries, their versions, hardware, and other environmental factors that could affect randomness. Furthermore, the influence of two commonly used optimizers on the outcomes was evaluated. Overall, we evaluated a total of 480 experimental runs across the three datasets.\nCIFAR-10 [17] is a standard computer vision dataset with 60,000 color images across 10 classes. SDNET2018 [18] is a real-world dataset with 56,000 labeled images for concrete crack detection. CBIS-DDSM [20] is another real-world dataset from a different domain, featuring around 10,000 mammography images categorized into benign and malignant cases. We selected those datasets based on availability, citation frequency, and relevance to our research.\nFor model architectures, we employ ResNet [21], PreActResNet [22], and MobileNet [23] for image classification tasks due to their effectiveness and community recognition. ResNet tackles training challenges in deep networks with \u201cresidual blocks\u201d, while PreActResNet optimizes performance through pre-activation integration. MobileNet, designed for resource-constrained devices, ensures efficiency without performance compromise. These choices were made considering efficacy, ease of implementation, and alignment with our research objectives.\nAll experiments were executed on a High-Performance Computer (HPC) infrastructure at the Institute for AI in Medicine [24] using SLURM [25], with monitoring facilitated by Weights & Biases (W&B) [26]."}, {"title": "IV. RESULTS", "content": "Table I summarizes the worst and best performance metrics out of a total of 480 runs obtained across the three tasks at hand. The numbers show the impact of both the random seed and CUDA related randomness. We evaluated the performance of different configurations on three distinct applications: classical CIFAR-10 benchmark, concrete crack detection with the SDNET2018 dataset, and medical imaging with the CBIS-DDSM dataset.\nOur results for CIFAR-10 align with [27], with Stochastic Gradient Descent (SGD) [16] converging to more similar accuracies than Adaptive Moment Estimation (ADAM) [15]. Our results are also consistent with [22], even though our approach utilized a simpler network architecture.\nFor concrete crack detection, we report F1-score, as performance differences between optimizers were more pronounced. In the concrete crack detection case, our methodology shows improvement over the foundational work by [28], indicating the effectiveness of the chosen model in real-world scenarios.\nFor the medical imaging analysis using the CBIS-DDSM dataset, we report AUC [29] scores, a critical metric in this domain. Our findings align with those presented in the work of [30], albeit without employing GMIC (Globally-aware Multiple Instance Classifier) [31], which is known for its low-memory consumption while enabling higher resolution.\nTable II shows detailed results from all configurations, highlighting the variability in performance that can occur if CUDA randomness is not controlled. In particular, in the CBSI-DDSM task, a clear gap of a maximum difference of 4.77% between the deterministic and non-deterministic runs with a random seed of 3407 and the SGD optimizer is noticeable."}, {"title": "Performance Variance and Tradeoffs", "content": "Our study employs distinct pipelines and metrics for each task, making direct performance comparisons across tasks challenging. However, by analyzing variances, we can assess task sensitivities to inherent randomness across the three distinct tasks. Additionally, for each seed and optimizer configuration, we have one fully deterministic configuration. By comparing the means of the non-deterministic runs with that particular fully deterministic run, we can get insights about the implications of CUDA randomness in terms of runtime and performance.\nIn analyzing the CBIS-DDSM task, as summarized in Table III, it is evident that the variability in performance metrics across different seeds and optimizers is noteworthy. For both, the SGD and the ADAM optimizer, we observe fluctuations in AUC scores, that are more pronounced in the ADAM optimizer. In particular, the ADAM optimizer at seed 0 shows the largest standard deviation in F1-score overall. The impact of deterministic execution on performance varied, with decreases of up to 2% for seed 314 and increases of up to 1.4% for seed 0. Such findings highlight the stochastic nature of model training outcomes in relation to seed selection and optimizer choice. Additionally, we infer from Table IV that runtime trade-offs from deterministic execution are generally small (once even in favor of the deterministic run) and at most incur a 16% longer runtime."}, {"title": "T-Test with One-Sample Mean", "content": "To evaluate if non-deterministic run performances significantly diverge from deterministic runs, we employed a t-test, leveraging the consistency of deterministic runs as the population mean. This approach is premised on the repeatability of deterministic runs, which are posited to accurately represent the \"true\" population mean. The hypotheses are succinctly framed as: Ho (Null): There is no significant difference between deterministic and non-deterministic runs, indicating the effect of CUDA randomness cannot be conclusively determined. H\u2081 (Alternative): A significant difference suggests CUDA randomness does influence results, leading to the rejection of Ho. This methodology enables a direct assessment of CUDA randomness' impact on performance metrics.\nThe analysis of one-sample t-test results, as summarized in Table V, indicates that CUDA randomness affects the performance outcomes across the CIFAR-10, CBIS-DDSM, and SDNET datasets. For the CIFAR-10 dataset, statistically significant differences were observed in 50% of the configurations, with ADAM showing a higher average deviation (0.517% for significant runs, 0.3484% overall) compared to SGD (0.1465% for significant runs, 0.1042% overall). In the CBIS-DDSM dataset, all ADAM configurations were significant, highlighting its sensitivity, whereas only one SGD configuration showed statistical significance, with average deviations of 1.68% for significant runs and 0.889% overall for SGD, and 1.491% for both significant and overall runs for ADAM. The SDNET dataset showed a balance of significant outcomes among configurations for both optimizers, with ADAM's deviations at 0.425% for significant runs and 0.204% overall, versus SGD's 0.441% for significant runs and 0.406% overall. These findings underscore ADAM's higher sensitivity to CUDA randomness compared to SGD, reflected in the greater number of significant configurations and larger deviation percentages."}, {"title": "Similarities of Model Weights", "content": "Figs. 2 and 3 show the similarities of embeddings across datasets and optimizers. The mean similarities provide a general overview, while minimum similarities show the extreme points of two runs affected by inherent CUDA execution randomness. The minimum similarity provides insight into the maximum potential impact of this randomness. In the figures, a value of 1 indicates that the embeddings were identical, whereas a value of 0 indicates that they were completely orthogonal.\nFig. 2 illustrates the last layer similarities for the ADAM optimizer across the three datasets. For the CIFAR-10 dataset, we observe the lowest similarity and largest variations. In later epochs, there seems to be a convergence of embedding representations as similarity values stabilize. In the CBIS-DDSM dataset, there is a consistent decline in similarity. This consistent reduction may be attributed to the effects of fine-tuning from ImageNet [32]-initialized weights rather than training from scratch. For the SDNET dataset, a pronounced initial drop is observed, indicating a swift divergence of weights. Yet, the similarities in subsequent epochs decline more gradually, hinting at a plateau in weight divergence.\nFig. 3 shows the embedding similarities for the SGD optimizer. Overall, we observe similar trends as for the ADAM optimizer, but less pronounced differences in similarities. Again, CIFAR-10 has the lowest similarity and largest variation, but similarities are much higher as with the ADAM optimizer. For the CBIS-DDSM dataset, there is a constant decrease in similarity, but embedding differences are subtle. On the SDNET dataset, we again observe an initial drop, followed by a plateau. However, again, differences are subtle."}, {"title": "V. ENVIRONMENTAL IMPACT OF THE EXPERIMENTS", "content": "In scientific research, it is crucial to consider not only the direct results of experiments but also the broader implications and consequences of the research process. While the following environmental assessment is not directly tied to our primary results, it represents an essential facet of our experiments. We believe it is our responsibility to report on the environmental footprint of our work, given the increasing global emphasis on sustainability and the environmental impact of computational practices. Furthermore, we posit that the environmental implications of computational experiments are becoming increasingly significant in the context of sustainable research practices. This perspective aligns with the findings of Ulmer et al. [33], emphasizing the importance of understanding and reporting the environmental consequences of experimental work.\nOur experiments were conducted using HPC resources located in Essen, Germany. The region's electricity generation has a carbon efficiency of 0.385 kgCO2eq/kWh [34], with approximately 43% [35] of the electricity being sourced from fossil fuels. To estimate the carbon footprint of our experiments, we utilized the Machine Learning Impact calculator, as presented by Lacoste et al. [36]. This calculator provides a comprehensive framework to quantify the carbon emissions associated with machine learning experiments, considering both the energy consumption of computational resources and the carbon efficiency of the electricity source.\nFrom Table VI, it is evident that while the energy consumption and associated carbon emissions for the reported experiments (\"Experiment runs\u201d) might not be significant, the overall environmental impact is considerably higher when accounting for all computational activities, including tests, debugging, and experimental setups (\"All runs\"). This highlights the broader environmental cost of the entire research process, not just the final reported results. It underscores the importance of energy-efficient algorithms and practices in machine learning research, especially in regions heavily reliant on fossil fuels for electricity generation."}, {"title": "VI. DISCUSSION AND CONCLUSION", "content": "From the analysis presented in Table III, it becomes clear that CUDA-randomness significantly influences performance variability, with the impact varying across three datasets and two optimizers. The choice of optimizer shows dependency on the specific dataset, indicating that the interplay between optimizer and dataset characteristics is crucial. Moreover, we observe that different performance metrics exhibit varied levels of variability, suggesting that the choice of metric is pivotal in understanding the performance landscape. Additionally, variability introduced by different seed values is non-negligible, further complicating the performance analysis. Most notably, the mammography task demonstrates significantly greater performance variance compared to the other datasets, highlighting the task-specific nature of CUDA-randomness effects.\nIn examining the tradeoffs between deterministic and non-deterministic modes, our findings reveal nuanced differences in performance and runtime across various configurations (cf. Tables III and IV). While Zhuang et al. [4] and the PyTorch documentation\u00b9 suggest that deterministic execution generally incurs higher runtime and lower performance, our experiments show this is not always the case. The performance differences between the two modes are usually within a 1% margin for CIFAR-10 and SDNET datasets, indicating that deterministic settings could be favored for reproducibility without significantly sacrificing performance. However, for the mammography task, the performance gap occasionally exceeds 1%, challenging the assumption that deterministic operations consistently yield higher performance or vice versa.\nAdditionally, runtime analyses revealed that deterministic execution does not invariably lead to longer training times across all datasets. These findings emphasize the importance of considering specific dataset characteristics and algorithmic choices in PyTorch when evaluating the tradeoffs between deterministic and non-deterministic modes.\nOur analysis of statistical significance in performance differences (cf. Table V) showed that in particular the real-world mammography task is heavily influenced by CUDA randomness. We not only observed significant differences in all runs of the ADAM optimizer, but also the largest deviation of up to 1.68% in all configurations that were significant (both for ADAM and SGD). This indicates that seed value selection can be crucial for the reproducibility of results in this task, and that the choice of optimizer may heavily influence the robustness of results.\nTo enhance reproducibility and robustness in deep learning research, we propose several strategies based on our study's insights. Firstly, adopting fully deterministic settings can offer significant benefits, as our results indicate, by zeroing variability and improving reproducibility. Conducting experiments across a range of seed values is also essential, allowing for a deeper understanding of how specific configurations impact outcomes. Moreover, ensuring full disclosure of all experimental settings, including configurations and software versions, is critical for enabling others to replicate and validate findings.\nDue to time and hardware constraints, runs per configuration were limited. Future studies could enhance robustness and depth of understanding by increasing the number of experimental runs. Our selected datasets are representative of application areas, but are still limited in scope. Future research could explore larger or more diverse datasets, such as ImageNet and CIFAR-100, to assess reproducibility across different scales and types of data. The selection of hyperparameters, informed by existing literature, suggests that a more exhaustive search might reveal further insights into hyperparameter influence on randomness. This study's focus on CUDA-induced randomness identifies a gap in the literature, indicating the necessity for additional research on computational consistency within CUDA architecture. Moreover, exploring the impact of different neural network architectures and distributed deep learning settings on reproducibility could be a further area of investigation. Lastly, considering different computational frameworks beyond PyTorch, such as TensorFlow, could offer a broader perspective on reproducibility challenges.\nIn summary, we derived the following insights from our study. Variability might be due to random seeds and chosen optimizers. Empirical evidence highlights that model performance and variance are influenced by different seed settings and optimizer selections. Evaluating models across a range of seed configurations is essential for identifying setups that not only achieve optimal performance but also maintain minimal variance. Though widely used, the ADAM optimizer showed comparably lower robustness against CUDA-induced randomness in our study.\nDeterminism introduces minor computational overhead and might slightly decrease predictive performance. Our results show that deterministic execution usually introduces a negligible computational overhead while it is advantageous for reproducibility. Nonetheless, determinism can enhance performance by up to 2% or result in reductions exceeding 1%. This performance variability necessitates a careful decision-making process regarding the use of deterministic versus non-deterministic execution, based on the specific goals and performance criteria of the model.\nThe effect of randomness is domain-specific. The influence of randomness during training exhibits significant variation across different domains, with the medical imaging domain, exemplified by the mammography dataset, being particularly susceptible to randomness. This variation accentuates the reproducibility challenges in medical imaging and emphasizes the importance of domain-specific approaches in model training and evaluation."}, {"title": "A. Reproducing the Results", "content": "The datasets utilized in this study are publicly accessible. Details are as follows:\n\u2022 CIFAR-10 Dataset: The dataset can be accessed from the University of Toronto's website.\n\u2022 SDNET2018 Dataset: It is available on the Utah State University's digital commons page.\n\u2022 CBIS-DDSM Dataset: The dataset is hosted on the Cancer Imaging Archive wiki.\nThe codebase supporting this study is also open-source. The current GitHub repository is as follows:\nhttps://github.com/aix-group/repincv.git\nWithin the repository, the code structure is organized into specific tasks:\n\u2022 task1: CIFAR-10 experiments.\n\u2022 task2(SDNET 2018): Concrete Crack Detection experiments.\n\u2022 task3(CBIS-DDSM): Breast Cancer Imaging experiments.\nAlways refer to the official repository as it can get fixes or updates in the future.\nInstructions to Reproduce the Results: While the CIFAR-10 and SDNET2018 datasets are set to auto-download if absent in the input directory, the CBIS-DDSM requires manual downloading due to its voluminous size. Once obtained, it should be relocated to the input directory. Subsequent image preprocessing steps are documented within the aforementioned repository."}, {"title": "B. Additional Tables and Graphs from the Experiment Data", "content": "TABLE VII: Deterministic and non-deterministic runtime comparison for the CIFAR dataset with two optimizers, ADAM and SGD, expressed in minutes. The first column lists all the seed configurations. AND represents the non-deterministic mean accuracy for each seed configuration. AD represents the deterministic accuracy for each seed configuration. o indicates the runtime variation. \u03bc represents the mean runtime for deterministic and non-deterministic scenarios.\nTABLE VIII: Deterministic and non-deterministic runtime comparison for the SDNET dataset with two optimizers, ADAM and SGD, expressed in minutes. The first column lists all the seed configurations. F1ND represents the non-deterministic mean F1 Score for each seed configuration. F1D represents the deterministic F1 Score for each seed configuration. \u03c3 indicates the runtime variation. \u03bc represents the mean runtime for deterministic and non-deterministic scenarios."}]}