{"title": "FIRST TRAIN TO GENERATE, THEN GENERATE TO TRAIN:\nUNITED SYNT5 FOR FEW-SHOT NLI", "authors": ["Sourav Banerjee", "Anush Mahajan", "Eishkaran Singh", "Ayushi Agarwal"], "abstract": "Natural Language Inference (NLI) tasks require identifying the relationship between\nsentence pairs, typically classified as entailment, contradiction, or neutrality. While\nthe current state-of-the-art (SOTA) model, Entailment Few-Shot Learning (EFL),\nachieves a 93.1% accuracy on the Stanford Natural Language Inference (SNLI)\ndataset, further advancements are constrained by the dataset's limitations. To\naddress this, we propose a novel approach leveraging synthetic data augmentation to\nenhance dataset diversity and complexity. We present UnitedSynT5, an advanced\nextension of EFL that leverages a T5-based generator to synthesize additional\npremise-hypothesis pairs, which are rigorously cleaned and integrated into the\ntraining data. These augmented examples are processed within the EFL framework,\nembedding labels directly into hypotheses for consistency. We train a GTR-T5-XL\nmodel on this expanded dataset, achieving a new benchmark of 94.7% accuracy on\nthe SNLI dataset, 94.01% accuracy on the E-SNLI dataset and 92.57% accuracy\non the MultiNLI dataset, surpassing the previous SOTA models. This research\ndemonstrates the potential of synthetic data augmentation in improving NLI models,\noffering a path forward for further advancements in natural language understanding\ntasks.", "sections": [{"title": "1 Introduction", "content": "Natural Language Inference (NLI) is a fundamental task in natural language processing (NLP) that\ninvolves determining the logical relationship between two sentences: a premise and a hypothesis.\nThese relationships are typically classified as entailment, contradiction, or neutral [1]. NLI serves\nas a benchmark for evaluating machine understanding and has significant implications for various\ndownstream applications, including question answering, text summarization, and information\nretrieval [12]."}, {"title": "2 Background", "content": "2.1 Natural Language Inference\nNatural Language Inference (NLI), also referred to as Recognizing Textual Entailment (RTE), is a\ntask in natural language processing (NLP) aimed at determining the logical relationship between\ntwo sentences: a premise and a hypothesis [19]. The task involves integrating natural language\nwith external knowledge, including commonsense reasoning [20], to classify whether the premise\nentails, contradicts, or is neutral with respect to the hypothesis. Given a premise p and a hypothesis\nh, the relationship r(p, h) is defined as follows:\nNLI requires understanding of semantics, pragmatics, and world knowledge, which are essential\nfor semantic representation and inference [1]. The complexity of NLI is influenced by factors such\nas semantic variability, world knowledge, and logical reasoning.\nBenchmark datasets, such as the Stanford Natural Language Inference (SNLI) corpus [1],MultiNLI\n[16], cross-lingual NLI for multiple languages [21], and provide large-scale collections of human-\nannotated sentence pairs, designed to facilitate the evaluation and training of NLI systems. The SNLI\ncorpus consists of 570,000 premise-hypothesis pairs, each labeled as entailment, contradiction,\nor neutral. These examples were generated through crowdsourcing, ensuring a diverse range of\nsentence structures and linguistic phenomena, making SNLI a foundational benchmark for NLI\ntasks. Similarly, the MultiNLI dataset extends SNLI by offering premise-hypothesis pairs from\nmultiple genres, thus enabling the evaluation of models in a broader array of linguistic contexts\nand improving their ability to generalize across domains.\nEarly approaches of NLI primarily relied on hand-crafted features and statistical models[12].\nThese methods involved manually designed features, such as lexical overlap, syntactic similarity,\nand heuristic rules, to approximate the relationship between the premise and the hypothesis.\nStatistical classifiers, such as Support Vector Machines (SVMs) and logistic regression, were then\napplied to these features to predict entailment relations. However, these approaches had limited"}, {"title": "2.2 Entailment Few-Shot Learning (EFL)", "content": "The Entailment Few-Shot Learning (EFL) approach [10] demonstrates state-of-the-art performance\non the SNLI dataset, achieving an accuracy of 93.1%. EFL reformulates the NLI task by embedding\nthe label directly into the hypothesis, transforming the traditional three-way classification into\na binary decision problem. This method allows the model to focus on verifying the relationship\nbetween the premise and the label associated with the hypothesis. While this reformulation has\nproven effective, it may overlook the more subtle and complex relationships that arise in certain\nNLI tasks.\nFor example, in the EFL framework, a pair of sentences is modified as follows:\nIn this instance, the relationship between the premise and hypothesis is labeled as neutral. EFL\nreformulates the hypothesis by embedding the label, resulting in:\nThe model's task is then to determine whether the extended hypothesis is \"true\" or \"false,\"\nsimplifying the classification into a binary decision.\nThis approach provides a streamlined method for assessing entailment but may also introduce\nlimitations when handling more multifaceted linguistic relationships."}, {"title": "3 Methodology: Overcoming the Current Perfor-\nmance Limitations", "content": "We hypothesize that the current performance ceiling in Natural Language Inference (NLI) tasks,\nsuch as those measured on the SNLI dataset, is due to the limited variation and complexity within\nthe dataset. To address this, we propose expanding the dataset through synthetic data generation,\naiming to introduce greater linguistic diversity and complexity. 1,1"}, {"title": "3.1 Generation of Synthetic Dataset", "content": "The generation of a synthetic dataset [23] uses an extension of the Entailment Few-Shot Learning\n(EFL) model. The process leverages a FLAN-T5 XL (3B) generator to produce additional premise-\nhypothesis pairs without requiring manual labeling. The following steps outline the structure and\nworkflow involved in generating this dataset:\n1. The initial training dataset is partitioned into two subsets: a generation set and a few-shot\nexample set. This partitioning is based on a predefined 95%-5% split, where 95% of the\ndata (521,898 examples) is allocated for generation and 5% (27,469 examples) for few-shot\nlearning.\n2. For each example within the generation set, two examples from the few-shot set are\nrandomly selected and included as contextual references in the generation prompt. These"}, {"title": "3.2 Training the FLAN-T5 XL Generator to Generate Hypothe-\nses", "content": "The training process for the FLAN-T5 XL generator involved the application of a few-shot learning\napproach. In this setup, two examples were randomly selected from 5% of the total training data to\nserve as additional context for the generator during hypothesis generation. This few-shot learning\napproach was adopted to address both computational and contextual limitations of the model,\noptimizing the balance between the input data size and model performance without the need for a\nlarge-scale dataset."}, {"title": "3.2.1 Prompt Design and Structure", "content": "The prompts used in the training process were designed to be highly structured, ensuring that the\ngenerated hypotheses adhered to the standards of the Stanford Natural Language Inference (SNLI)\ntask. Each prompt explicitly defined the task, labels, and guidelines to maintain consistency\nacross different training examples. Below is an outline of the prompt structure used during the\ngeneration process:"}, {"title": "3.2.2 Iterative Process for Generator Training", "content": "The training process for the FLAN-T5 XL generator employed an iterative approach, wherein the\ngenerated hypotheses were evaluated against the reference hypotheses from the SNLI dataset.\nDuring each iteration, the generator produced a set of candidate hypotheses based on the input\npremises, which were then compared with the true labels (entailment, contradiction, or neutral)\nprovided by the dataset. To measure the discrepancy between the predicted and actual labels, a\ncross-entropy loss function was applied. This function is commonly used in classification tasks\nto evaluate the difference between the predicted probability distribution and the actual label,\nproviding a penalty for incorrect predictions based on their assigned probability.\nThe calculated loss was then backpropagated through the model, enabling parameter updates\nusing a gradient descent optimization algorithm, such as Adam. The model's parameters, including\nthe weights within the neural network, were adjusted to minimize the loss in subsequent iterations,\nthereby improving its performance in generating hypotheses that aligned with the true labels. This\niterative feedback loop, driven by loss minimization, enhanced the model's ability to generalize\nacross different premise-hypothesis pairs."}, {"title": "3.2.3 Output and Dataset Augmentation", "content": "Upon completion of the training process, the generator was tasked with producing a set of synthetic\nexamples, consisting of premise-hypothesis pairs, based on the prompts and training data. These\ngenerated examples were then integrated into the training dataset, augmenting the original SNLI\ndata with new, machine-generated hypotheses."}, {"title": "3.3 Dataset Cleaning", "content": "To maintain the integrity of the augmented dataset, a systematic filtering process was applied to\nremove inconsistent or redundant data. This process was designed to ensure that the synthetic\nexamples generated during the data augmentation stage adhered to the expected quality standards,\nthus preventing any negative impact on model training."}, {"title": "3.3.1 Filtering Criteria", "content": "The filtering process was based on two primary criteria, each aimed at refining the dataset to\npreserve both accuracy and diversity:\n1. Label Alignment: The first criterion ensured that the labels generated by the FLAN-T5\nXL generator matched the labels assigned by the GTR-T5-XL model, which is the current"}, {"title": "3.3.2 Dataset Reduction", "content": "The filtering process led to the removal of 54,216 examples from the initial set of 521,899 generated\nexamples, leaving 467,683 examples in the final cleaned dataset. The refined dataset formed a\nmore robust foundation for training, minimizing the risks associated with overfitting or noise due\nto redundant or mislabeled data.\nBelow are examples of retained generated hypotheses, demonstrating the application of the filtering\ncriteria and the resulting dataset quality:"}, {"title": "3.4 EFL Conversion", "content": "The conversion of the synthetic dataset to the Entailment Few-Shot Learning (EFL) framework\nconstitutes a critical step in the methodology. This process involves the systematic reformulation\nof the generated data to align with the EFL paradigm, which integrates label information directly\ninto the hypothesis statements. The conversion facilitates the incorporation of synthetic examples\ninto existing EFL-based training pipelines and ensures methodological consistency.\nIn the EFL framework, the task undergoes a transformation from a three-way classification problem\nto a binary decision task which is through the embedding of entailment labels within the hypothesis\nitself. The model is then tasked with determining the veracity of the entire statement, including\nthe embedded label, as either \"true\" or \"false\".\nThe EFL conversion process can be formalized as follows:\nLet P be the premise, H the original hypothesis, and L the label (entailment, contradiction, or\nneutral). The EFL conversion function $f_{EFL}$ can be defined as:"}, {"title": "3.5 Training a New GTR-T5-XL Model", "content": "The process of training the GTR-T5-XL begins with the use of the synthetic dataset generated in\nthe previous steps. This dataset, which has been cleaned and processed through the Entailment\nFew-Shot Learning (EFL) framework, was utilized to train the GTR-T5-XL model from the ground\nup.\nThe GTR-T5-XL model was initially pre-trained on a large corpus, providing it with a broad\nunderstanding of natural language. Subsequently, it was fine-tuned using the newly augmented\nsynthetic dataset along with human labeled training set either produced by the FLAN-T5 XL\ngenerator or sourced from the original human-labeled data. This dataset, containing machine-\ngenerated hypotheses for each premise, was constructed to enhance the model's capability in\nsolving the Natural Language Inference (NLI) task."}, {"title": "3.5.1 Training Configuration", "content": "The training process involved providing the GTR-T5-XL model with input embeddings, each of\na vector dimension of 768. The model included three fully connected layers. These layers were\nconfigured with the Gaussian Error Linear Unit (GeLU) activation function, which was employed\nto introduce non-linearity and improve the model's ability to capture complex relationships in the\ndata. Additionally, a dropout rate of 0.1 was used throughout the network to prevent overfitting\nduring the training process."}, {"title": "3.5.2 Evaluation and Performance Measurement", "content": "After the completion of training, the model was evaluated on the original SNLI dataset to assess\nits performance. The evaluation process involved comparing the model's predictions with the\nhuman-annotated labels within the dataset. This validation provided a direct measurement of the\neffectiveness of the synthetic dataset in improving the model's ability to perform NLI tasks. The\nuse of human-annotated labels as a benchmark enabled a reliable comparison to determine the\nextent to which the augmented data contributed to the model's improved performance."}, {"title": "3.5.3 Feedback and Model Refinement", "content": "The training process incorporated a feedback loop to continuously refine the model's parameters.\nThis iterative refinement allowed the GTR-T5-XL model to generalize more effectively when exposed\nto unseen data. By adjusting the model's parameters based on performance during training, the\nfeedback loop ensured that the model was consistently improving its accuracy and ability to handle\ncomplex and diverse premise-hypothesis pairs."}, {"title": "4 Results", "content": "The experimental results, demonstrating how our approach surpasses the current state-of-the-art\n(SOTA) in terms of test accuracy. We employed the FLAN-T5 XL model (3 billion parameters) for\ngeneration tasks, paired with GTR-Large models for classification tasks of varying sizes. The\nintegration of our augmented dataset yielded improvements over the previous SOTA results.\nAs shown in Table\u00b2, the combination of Flan-T5-xl (3B) for generation with GTR-Large (335M) for\nclassification achieved a test accuracy of 93.5%, surpassing the previous SOTA accuracy of 93.1%.\nFurther gains were observed when using the larger GTR-Large (3B) model, which improved test\naccuracy to 94.7%.\nIn addition to the SNLI dataset, the same approach was tested on the E-SNLI and MultiNLI\ndatasets, where it also broke previous records, setting new accuracies of 94.01% on E-SNLI and"}, {"title": "5 Limitations and Future Work", "content": "The current approach faces several limitations related to computational efficiency and model\nperformance. One limitation concerns computational constraints, where the number of tokens\nin the input prompt increases the computational resources required for hypothesis generation\nexponentially. This increase in token count also leads to a significant extension in training time,\nreducing the efficiency of the approach in large-scale applications. Another issue is related to\nmaintaining contextual coherence, especially in smaller models such as T5-3B. These models\nencounter difficulties in handling longer prompts, making it challenging to incorporate a large"}]}